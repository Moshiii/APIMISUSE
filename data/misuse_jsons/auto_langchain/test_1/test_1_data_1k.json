[
    {
        "commit_hash": "eb8d5d6946bf4950026a92ccdc0d77d5870b4b93",
        "index": "68ca9d3e..1a12067a 100644",
        "commit_message": "fixed saved-model format\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def tf_function(*, num_args):",
            "results = function(self, **kwargs, **params_kwargs)",
            "return results",
            "",
            "-                function_graphs[graph_params] = tf.function(",
            "+                function_graphs[str(graph_params)] = tf.function(",
            "func=function_graph, input_signature=graph_signature.to_list(), autograph=False",
            "# experimental_implements=None, experimental_autograph_options=None,",
            "# experimental_relax_shapes=False, experimental_compile=None",
            ")",
            "",
            "# Apply function graph",
            "-            return function_graphs[graph_params](*graph_args)",
            "+            return function_graphs[str(graph_params)](*graph_args)",
            "",
            "# TensorFlow make_decorator",
            "return decorated"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=subscript), node=('call', None), position=2, insert_id=2224343)",
            "Insert(target_node=ASTNode(type=subscript), node=('call', None), position=2, insert_id=2224344)",
            "Insert(target_node=IN(type=call), node=('identifier', 'str'), position=0, insert_id=2224345)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2224346)",
            "Insert(target_node=IN(type=call), node=('identifier', 'str'), position=0, insert_id=2224347)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2224348)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2224349)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=graph_params), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2224350)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2224351)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=graph_params), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2224352)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 12,
        "number": 3373,
        "neg_line": [
            "-function_graphs[graph_params] = tf.function(",
            "-return function_graphs[graph_params](*graph_args)"
        ],
        "pos_line": [
            "+function_graphs[str(graph_params)] = tf.function(",
            "+return function_graphs[str(graph_params)](*graph_args)"
        ],
        "core_change": "-function_graphs[graph_params] = tf.function( +function_graphs[str(graph_params)] = tf.function( -return function_graphs[graph_params](*graph_args) +return function_graphs[str(graph_params)](*graph_args)",
        "core_API": "function"
    },
    {
        "commit_hash": "5f4c6e830f603830117877b8990a0e65a2386aa6",
        "index": "a1ba99d82..16708eb24 100644",
        "commit_message": "Quick fix :) (#606)\n\n* Changing the name\n\n* style + quality\n\n* update doc and logo\n\n* clean up\n\n* circle-CI on the branche for now\n\n* fix daily dialog dataset\n\n* fix urls\n\nCo-authored-by: Quentin Lhoest <lhoest.q@gmail.com>\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "Returns:",
            "\"\"\"",
            "",
            "",
            "-class Rouge(nlp.Metric):",
            "+class Rouge(datasets.Metric):",
            "def _info(self):",
            "-        return nlp.MetricInfo(",
            "+        return datasets.MetricInfo(",
            "description=_DESCRIPTION,",
            "citation=_CITATION,",
            "inputs_description=_KWARGS_DESCRIPTION,",
            "-            features=nlp.Features(",
            "+            features=datasets.Features(",
            "{",
            "-                    \"predictions\": nlp.Value(\"string\", id=\"sequence\"),",
            "-                    \"references\": nlp.Value(\"string\", id=\"sequence\"),",
            "+                    \"predictions\": datasets.Value(\"string\", id=\"sequence\"),",
            "+                    \"references\": datasets.Value(\"string\", id=\"sequence\"),",
            "}",
            "),",
            "codebase_urls=[\"https://github.com/google-research/google-research/tree/master/rouge\"],"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 4,
        "minus_line": 4,
        "AST_diff_line": 17,
        "number": 3376,
        "neg_line": [
            "-class Rouge(nlp.Metric):",
            "-return nlp.MetricInfo(",
            "-features=nlp.Features(",
            "-\"predictions\": nlp.Value(\"string\", id=\"sequence\"),",
            "-\"references\": nlp.Value(\"string\", id=\"sequence\"),"
        ],
        "pos_line": [
            "+class Rouge(datasets.Metric):",
            "+return datasets.MetricInfo(",
            "+features=datasets.Features(",
            "+\"predictions\": datasets.Value(\"string\", id=\"sequence\"),",
            "+\"references\": datasets.Value(\"string\", id=\"sequence\"),"
        ],
        "core_change": "-class Rouge(nlp.Metric): +class Rouge(datasets.Metric): -return nlp.MetricInfo( +return datasets.MetricInfo( -features=nlp.Features( +features=datasets.Features( -\"predictions\": nlp.Value(\"string\", id=\"sequence\"), -\"references\": nlp.Value(\"string\", id=\"sequence\"), +\"predictions\": datasets.Value(\"string\", id=\"sequence\"), +\"references\": datasets.Value(\"string\", id=\"sequence\"),",
        "core_API": "MetricInfo"
    },
    {
        "commit_hash": "5f4c6e830f603830117877b8990a0e65a2386aa6",
        "index": "1ac1918c1..df6a9e311 100644",
        "commit_message": "Quick fix :) (#606)\n\n* Changing the name\n\n* style + quality\n\n* update doc and logo\n\n* clean up\n\n* circle-CI on the branche for now\n\n* fix daily dialog dataset\n\n* fix urls\n\nCo-authored-by: Quentin Lhoest <lhoest.q@gmail.com>\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "def benchmark_iterating():",
            "]",
            "with tempfile.TemporaryDirectory() as tmp_dir:",
            "print(\"generating dataset\")",
            "-        features = nlp.Features({\"list\": nlp.Sequence(nlp.Value(\"float32\")), \"numbers\": nlp.Value(\"float32\")})",
            "+        features = datasets.Features(",
            "+            {\"list\": datasets.Sequence(datasets.Value(\"float32\")), \"numbers\": datasets.Value(\"float32\")}",
            "+        )",
            "dataset = generate_example_dataset(",
            "os.path.join(tmp_dir, \"dataset.arrow\"),",
            "features,"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=nlp), value='datasets')",
            "Update(target_node=ASTNode(type=identifier, text=nlp), value='datasets')",
            "Update(target_node=ASTNode(type=identifier, text=nlp), value='datasets')",
            "Update(target_node=ASTNode(type=identifier, text=nlp), value='datasets')"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 3378,
        "neg_line": [
            "-features = nlp.Features({\"list\": nlp.Sequence(nlp.Value(\"float32\")), \"numbers\": nlp.Value(\"float32\")})"
        ],
        "pos_line": [
            "+features = datasets.Features(",
            "+{\"list\": datasets.Sequence(datasets.Value(\"float32\")), \"numbers\": datasets.Value(\"float32\")}",
            "+)"
        ],
        "core_change": "-features = nlp.Features({\"list\": nlp.Sequence(nlp.Value(\"float32\")), \"numbers\": nlp.Value(\"float32\")}) +features = datasets.Features( +{\"list\": datasets.Sequence(datasets.Value(\"float32\")), \"numbers\": datasets.Value(\"float32\")} +)",
        "core_API": "TemporaryDirectory"
    },
    {
        "commit_hash": "586f66b06cb21af15126759767f834ac4425ba9d",
        "index": "9e6577ffd6..d336be45fb 100644",
        "commit_message": "small format fixes\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def unique_all(x: Tensor) -> Tuple[Tensor, Tensor, Tensor, Tensor]:",
            "",
            "flat_tensor = tf.reshape(x, [-1])",
            "values, inverse_indices, counts = tf.unique_with_counts(flat_tensor)",
            "-    # values = tf.cast(values, 'float64') if values.dtype not in [tf.float32, tf.float64] else values",
            "tensor_list = flat_tensor.numpy().tolist()",
            "if (",
            "x.dtype.is_floating"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 3379,
        "neg_line": [
            "-# values = tf.cast(values, 'float64') if values.dtype not in [tf.float32, tf.float64] else values"
        ],
        "pos_line": [],
        "core_change": "-# values = tf.cast(values, 'float64') if values.dtype not in [tf.float32, tf.float64] else values",
        "core_API": "reshape"
    },
    {
        "commit_hash": "adec3c565f15b23c850bad33abd5269fe2dccd58",
        "index": "4178ed2d5..33e6d3812 100644",
        "commit_message": "fix random seed for testing\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_get_rtf(ch):",
            "normalized=False,",
            "onesided=True,",
            ")",
            "+    torch.random.manual_seed(0)",
            "x = random_speech[..., :ch]",
            "n = torch.rand(2, 16, ch, dtype=torch.double)",
            "ilens = torch.LongTensor([16, 12])"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=4, insert_id=144007)",
            "Insert(target_node=ASTNode(type=module), node=('ERROR', None), position=5, insert_id=144008)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=144009)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=identifier, text=x), position=0)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type==, text==), position=1)",
            "Move(target_node=ASTNode(type=expression_statement), node=ASTNode(type=subscript), position=0)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=144010)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=144011)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=144012)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=144013)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'manual_seed'), position=2, insert_id=144014)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=144015)",
            "Insert(target_node=IN(type=argument_list), node=('integer', '0'), position=1, insert_id=144016)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=144017)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=144018)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=144019)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'random'), position=2, insert_id=144020)",
            "Delete(target_node=ASTNode(type=assignment))"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 18,
        "number": 3380,
        "neg_line": [],
        "pos_line": [
            "+torch.random.manual_seed(0)"
        ],
        "core_change": "+torch.random.manual_seed(0)",
        "core_API": "manual_seed"
    },
    {
        "commit_hash": "c9a0671477b06780840ab2f5ff39006e0952be3d",
        "index": "d7412a878..968e2d346 100644",
        "commit_message": "[`bnb`] fix `bnb` decoders bug (#21688)\n\n* fix `bnb` decoders bug\n\n* make fixup\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class MixedInt8T5Test(unittest.TestCase):",
            "`flan-t5-small` uses `T5DenseGatedActDense` whereas `t5-small` uses `T5DenseReluDense`. We need to test",
            "both cases.",
            "\"\"\"",
            "+        import bitsandbytes as bnb",
            "+",
            "from transformers import T5ForConditionalGeneration",
            "",
            "# test with `t5-small`",
            "model = T5ForConditionalGeneration.from_pretrained(self.model_name, load_in_8bit=True, device_map=\"auto\")",
            "+",
            "+        # there was a bug with decoders - this test checks that it is fixed",
            "+        self.assertTrue(isinstance(model.decoder.block[0].layer[0].SelfAttention.q, bnb.nn.Linear8bitLt))",
            "+",
            "encoded_input = self.tokenizer(self.input_text, return_tensors=\"pt\").to(0)",
            "_ = model.generate(**encoded_input)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 3,
        "minus_line": 0,
        "AST_diff_line": 17,
        "number": 3383,
        "neg_line": [],
        "pos_line": [
            "+import bitsandbytes as bnb",
            "+",
            "+",
            "+# there was a bug with decoders - this test checks that it is fixed",
            "+self.assertTrue(isinstance(model.decoder.block[0].layer[0].SelfAttention.q, bnb.nn.Linear8bitLt))",
            "+"
        ],
        "core_change": "+import bitsandbytes as bnb + + +# there was a bug with decoders - this test checks that it is fixed +self.assertTrue(isinstance(model.decoder.block[0].layer[0].SelfAttention.q, bnb.nn.Linear8bitLt)) +",
        "core_API": "from_pretrained"
    },
    {
        "commit_hash": "1ab2f1d30208a972fbb331da35de4f899580ea5d",
        "index": "377e0add..b8de4362 100644",
        "commit_message": "Tutorial fixing (#635)\n\n* TF bug fixing in Tutorials\n\n* Error fix in #476\n\n* Issue with Flags in Tutorials Fixed\n\n* Missing import fixed\n\n* Changelog Update\n\n* VGG19 import error fix\n\n* Error fixing in VGG tutorials\n\n* TFRecord Shape Error Fix\n\n* Sess Initialization Error Fix\n\n* Squeezenet model loading from \"models\" dir\n\n* PTB tutorials import issue fixed\n\n* mobilenet load from dir \"models\"\n\n* YAPF error fix\n\n* Missing Import fixed\n\n* Various Fixes on Tutorials\n\n* YAPF error correct\n\n* Update CHANGELOG.md\n\n* update VGG16 tutorial, auto download model\n\n* Python 3 Unicode Encoding Error\n\n* Deprecation Warning Fix\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "models",
        "change": [
            "sess = tf.InteractiveSession()",
            "network.print_params(False)",
            "",
            "saver = tf.train.Saver()",
            "-if not os.path.isfile(\"inception_v3.ckpt\"):",
            "+if not os.path.isfile(MODEL_PATH):",
            "raise Exception(",
            "\"Please download inception_v3 ckpt from https://github.com/tensorflow/models/tree/master/research/slim\"",
            ")",
            "",
            "try:  # TF12+",
            "-    saver.restore(sess, \"./inception_v3.ckpt\")",
            "+    saver.restore(sess, MODEL_PATH)",
            "except Exception:  # TF11",
            "-    saver.restore(sess, \"inception_v3.ckpt\")",
            "+    saver.restore(sess, MODEL_PATH)",
            "print(\"Model Restored\")",
            "",
            "y = network.outputs"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'MODEL_PATH'), position=1, insert_id=2677303)",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2677304)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'saver'), position=0, insert_id=2677305)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2677306)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'restore'), position=2, insert_id=2677307)",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'MODEL_PATH'), position=3, insert_id=2677308)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'MODEL_PATH'), position=3, insert_id=2677309)",
            "Delete(target_node=ASTNode(type=string, text=\"inception_v3.ckpt\"))",
            "Delete(target_node=ASTNode(type=string, text=\"./inception_v3.ckpt\"))",
            "Delete(target_node=ASTNode(type=identifier, text=saver))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=restore))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=string, text=\"inception_v3.ckpt\"))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 15,
        "number": 3384,
        "neg_line": [
            "-if not os.path.isfile(\"inception_v3.ckpt\"):",
            "-saver.restore(sess, \"./inception_v3.ckpt\")",
            "-saver.restore(sess, \"inception_v3.ckpt\")"
        ],
        "pos_line": [
            "+if not os.path.isfile(MODEL_PATH):",
            "+saver.restore(sess, MODEL_PATH)",
            "+saver.restore(sess, MODEL_PATH)"
        ],
        "core_change": "-if not os.path.isfile(\"inception_v3.ckpt\"): +if not os.path.isfile(MODEL_PATH): -saver.restore(sess, \"./inception_v3.ckpt\") +saver.restore(sess, MODEL_PATH) -saver.restore(sess, \"inception_v3.ckpt\") +saver.restore(sess, MODEL_PATH)",
        "core_API": "InteractiveSession"
    },
    {
        "commit_hash": "2a6d7b4f6fd41b12a9a451005ed8867eb505672a",
        "index": "38413393ed..68e1cc817a 100644",
        "commit_message": "superset fix for `inv()` for `linalg.py` (#4988)\n\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def matmul(",
            "*,",
            "transpose_a: bool = False,",
            "transpose_b: bool = False,",
            "-    out: Optional[torch.Tensor] = None,",
            "+    out: Optional[torch.Tensor] = None",
            ") -> torch.Tensor:",
            "if transpose_a is True:",
            "x1 = torch.t(x1)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 3386,
        "neg_line": [
            "-out: Optional[torch.Tensor] = None,"
        ],
        "pos_line": [
            "+out: Optional[torch.Tensor] = None"
        ],
        "core_change": "-out: Optional[torch.Tensor] = None, +out: Optional[torch.Tensor] = None",
        "core_API": "t"
    },
    {
        "commit_hash": "dc82068a6dc5e934f2c14741564869dfc5d003e3",
        "index": "964b415b3..e6fe49244 100644",
        "commit_message": "FIX E523,E541,E741\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Decoder(BatchScorerInterface, torch.nn.Module):",
            "logp, states = self.forward_one_step(ys, ys_mask, xs, cache=batch_state)",
            "",
            "# transpose state of [layer, batch] into [batch, layer]",
            "-        state_list = [[states[l][b] for l in range(n_layers)] for b in range(n_batch)]",
            "+        state_list = [[states[i][b] for i in range(n_layers)] for b in range(n_batch)]",
            "return logp, state_list"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=l), value='i')",
            "Update(target_node=ASTNode(type=identifier, text=l), value='i')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 3388,
        "neg_line": [
            "-state_list = [[states[l][b] for l in range(n_layers)] for b in range(n_batch)]"
        ],
        "pos_line": [
            "+state_list = [[states[i][b] for i in range(n_layers)] for b in range(n_batch)]"
        ],
        "core_change": "-state_list = [[states[l][b] for l in range(n_layers)] for b in range(n_batch)] +state_list = [[states[i][b] for i in range(n_layers)] for b in range(n_batch)]",
        "core_API": "forward_one_step"
    },
    {
        "commit_hash": "06a1991d4153fda633af6228542f172c8cf8a9fd",
        "index": "a9c41a17..7d6bd067 100644",
        "commit_message": ":elephant: Remove warnings during testing (#1401)\n\n* fix warnings during testing\n\n* Update test/geometry/transform/test_imgwarp.py\n\n* remove warning in clahe due to //\n\n* Apply suggestions from code review\n\nCo-authored-by: Luis Ferraz <luisferrazc@gmail.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def rgb_to_hsv(image: torch.Tensor, eps: float = 1e-8) -> torch.Tensor:",
            "if len(image.shape) < 3 or image.shape[-3] != 3:",
            "raise ValueError(f\"Input size must have a shape of (*, 3, H, W). Got {image.shape}\")",
            "",
            "-    # TODO: remove if/else statement once pytorch 1.6 is deprecated and keep first branch",
            "-    max_rgb, argmax_rgb = _compute_max_argmax(image)",
            "+    max_rgb, argmax_rgb = image.max(-3)",
            "min_rgb, argmin_rgb = image.min(-3)",
            "deltac = max_rgb - min_rgb"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=413901)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=image), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=413902)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'max'), position=2, insert_id=413903)",
            "Insert(target_node=ASTNode(type=argument_list), node=('unary_operator', '-3'), position=1, insert_id=413904)",
            "Delete(target_node=ASTNode(type=identifier, text=_compute_max_argmax))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 6,
        "number": 3389,
        "neg_line": [
            "-# TODO: remove if/else statement once pytorch 1.6 is deprecated and keep first branch",
            "-max_rgb, argmax_rgb = _compute_max_argmax(image)"
        ],
        "pos_line": [
            "+max_rgb, argmax_rgb = image.max(-3)"
        ],
        "core_change": "-# TODO: remove if/else statement once pytorch 1.6 is deprecated and keep first branch -max_rgb, argmax_rgb = _compute_max_argmax(image) +max_rgb, argmax_rgb = image.max(-3)",
        "core_API": "max"
    },
    {
        "commit_hash": "05cf0d1a44430230e75339ff7cfdd26bdf554502",
        "index": "3dd5fe9..fee5e93 100644",
        "commit_message": "Export single output only (#7259)\n\n* Update\n\n* Update\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "yolov5.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Detect(nn.Module):",
            "y = torch.cat((xy, wh, conf), 4)",
            "z.append(y.view(bs, -1, self.no))",
            "",
            "-        return x if self.training else (torch.cat(z, 1), x)",
            "+        return x if self.training else (torch.cat(z, 1),) if self.export else (torch.cat(z, 1), x)",
            "",
            "def _make_grid(self, nx=20, ny=20, i=0):",
            "d = self.anchors[i].device"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=conditional_expression), node=('conditional_expression', None), position=4, insert_id=1294988)",
            "Insert(target_node=IN(type=conditional_expression), node=('tuple', None), position=0, insert_id=1294989)",
            "Insert(target_node=IN(type=conditional_expression), node=('if', 'if'), position=1, insert_id=1294990)",
            "Insert(target_node=IN(type=conditional_expression), node=('attribute', None), position=2, insert_id=1294991)",
            "Insert(target_node=IN(type=conditional_expression), node=('else', 'else'), position=3, insert_id=1294992)",
            "Move(target_node=IN(type=conditional_expression), node=ASTNode(type=tuple), position=4)",
            "Insert(target_node=IN(type=tuple), node=('(', '('), position=0, insert_id=1294993)",
            "Insert(target_node=IN(type=tuple), node=('call', None), position=1, insert_id=1294994)",
            "Insert(target_node=IN(type=tuple), node=(',', ','), position=2, insert_id=1294995)",
            "Insert(target_node=IN(type=tuple), node=(')', ')'), position=3, insert_id=1294996)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=1294997)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1294998)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'export'), position=2, insert_id=1294999)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1295000)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1295001)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1295002)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1295003)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cat'), position=2, insert_id=1295004)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1295005)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'z'), position=1, insert_id=1295006)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=1295007)",
            "Insert(target_node=IN(type=argument_list), node=('integer', '1'), position=3, insert_id=1295008)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=1295009)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 23,
        "number": 3392,
        "neg_line": [
            "-return x if self.training else (torch.cat(z, 1), x)"
        ],
        "pos_line": [
            "+return x if self.training else (torch.cat(z, 1),) if self.export else (torch.cat(z, 1), x)"
        ],
        "core_change": "-return x if self.training else (torch.cat(z, 1), x) +return x if self.training else (torch.cat(z, 1),) if self.export else (torch.cat(z, 1), x)",
        "core_API": "cat"
    },
    {
        "commit_hash": "5f4c6e830f603830117877b8990a0e65a2386aa6",
        "index": "81aa55427..8d684560e 100644",
        "commit_message": "Quick fix :) (#606)\n\n* Changing the name\n\n* style + quality\n\n* update doc and logo\n\n* clean up\n\n* circle-CI on the branche for now\n\n* fix daily dialog dataset\n\n* fix urls\n\nCo-authored-by: Quentin Lhoest <lhoest.q@gmail.com>\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "def benchmark_indices_mapping():",
            "functions = (select, sort, shuffle, train_test_split, shard)",
            "with tempfile.TemporaryDirectory() as tmp_dir:",
            "print(\"generating dataset\")",
            "-        features = nlp.Features({\"text\": nlp.Value(\"string\"), \"numbers\": nlp.Value(\"float32\")})",
            "+        features = datasets.Features({\"text\": datasets.Value(\"string\"), \"numbers\": datasets.Value(\"float32\")})",
            "dataset = generate_example_dataset(",
            "os.path.join(tmp_dir, \"dataset.arrow\"), features, num_examples=SPEED_TEST_N_EXAMPLES",
            ")"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=nlp), value='datasets')",
            "Update(target_node=ASTNode(type=identifier, text=nlp), value='datasets')",
            "Update(target_node=ASTNode(type=identifier, text=nlp), value='datasets')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 3,
        "number": 3394,
        "neg_line": [
            "-features = nlp.Features({\"text\": nlp.Value(\"string\"), \"numbers\": nlp.Value(\"float32\")})"
        ],
        "pos_line": [
            "+features = datasets.Features({\"text\": datasets.Value(\"string\"), \"numbers\": datasets.Value(\"float32\")})"
        ],
        "core_change": "-features = nlp.Features({\"text\": nlp.Value(\"string\"), \"numbers\": nlp.Value(\"float32\")}) +features = datasets.Features({\"text\": datasets.Value(\"string\"), \"numbers\": datasets.Value(\"float32\")})",
        "core_API": "TemporaryDirectory"
    },
    {
        "commit_hash": "786be2a3f8b8160640614638f2fddc11572e2bc8",
        "index": "c6454c71b..c9745c3e8 100755",
        "commit_message": "Should be fixed finally\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def main():",
            "# train",
            "logging.info('backend = ' + args.backend)",
            "if args.backend == \"chainer\":",
            "-        from espnet.lmchainer.asr_chainer import train",
            "+        from espnet.asr.chainer.asr_chainer import train",
            "train(args)",
            "elif args.backend == \"pytorch\":",
            "-        from espnet.lmpytorch.asr_pytorch import train",
            "+        from espnet.asr.pytorch.asr_pytorch import train",
            "train(args)",
            "else:",
            "raise ValueError(\"Only chainer and pytorch are supported.\")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=import_from_statement), node=ASTNode(type=dotted_name), position=3)",
            "Update(target_node=ASTNode(type=identifier, text=lmpytorch), value='asr')",
            "Insert(target_node=ASTNode(type=dotted_name), node=('identifier', 'pytorch'), position=4, insert_id=178794)",
            "Insert(target_node=ASTNode(type=dotted_name), node=('.', '.'), position=5, insert_id=178795)",
            "Insert(target_node=ASTNode(type=import_from_statement), node=('dotted_name', None), position=3, insert_id=178796)",
            "Update(target_node=ASTNode(type=identifier, text=lmchainer), value='asr')",
            "Insert(target_node=ASTNode(type=dotted_name), node=('identifier', 'chainer'), position=4, insert_id=178797)",
            "Insert(target_node=ASTNode(type=dotted_name), node=('.', '.'), position=5, insert_id=178798)",
            "Insert(target_node=IN(type=dotted_name), node=('identifier', 'train'), position=0, insert_id=178799)",
            "Delete(target_node=ASTNode(type=identifier, text=train))",
            "Delete(target_node=ASTNode(type=dotted_name))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 11,
        "number": 3395,
        "neg_line": [
            "-from espnet.lmchainer.asr_chainer import train",
            "-from espnet.lmpytorch.asr_pytorch import train"
        ],
        "pos_line": [
            "+from espnet.asr.chainer.asr_chainer import train",
            "+from espnet.asr.pytorch.asr_pytorch import train"
        ],
        "core_change": "-from espnet.lmchainer.asr_chainer import train +from espnet.asr.chainer.asr_chainer import train -from espnet.lmpytorch.asr_pytorch import train +from espnet.asr.pytorch.asr_pytorch import train",
        "core_API": "info"
    },
    {
        "commit_hash": "f67a1affcd5660c39dad0868cd69cf83479976ea",
        "index": "35696576..2a375372 100644",
        "commit_message": "improve logging and fix wd computation in inference\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def regularize_cost_from_collection(name='regularize_cost'):",
            "\"\"\"",
            "regularization_losses = set(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))",
            "ctx = get_current_tower_context()",
            "+    if not ctx.is_training:",
            "+        # Currently cannot build the wd_cost correctly at inference,",
            "+        # because ths vs_name used in inference can be '', therefore the",
            "+        # variable filter will fail",
            "+        return None",
            "+",
            "if len(regularization_losses) > 0:",
            "# NOTE: this collection doesn't grow with towers.",
            "# It is only added with variables that are newly created."
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=4, insert_id=2295517)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=2295518)",
            "Insert(target_node=IN(type=if_statement), node=('not_operator', None), position=1, insert_id=2295519)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=2295520)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=2295521)",
            "Insert(target_node=IN(type=not_operator), node=('not', 'not'), position=0, insert_id=2295522)",
            "Insert(target_node=IN(type=not_operator), node=('attribute', None), position=1, insert_id=2295523)",
            "Insert(target_node=IN(type=block), node=('return_statement', None), position=0, insert_id=2295524)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'ctx'), position=0, insert_id=2295525)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2295526)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'is_training'), position=2, insert_id=2295527)",
            "Insert(target_node=IN(type=return_statement), node=('return', 'return'), position=0, insert_id=2295528)",
            "Insert(target_node=IN(type=return_statement), node=('none', 'None'), position=1, insert_id=2295529)"
        ],
        "plus_line": 5,
        "minus_line": 0,
        "AST_diff_line": 13,
        "number": 3396,
        "neg_line": [],
        "pos_line": [
            "+if not ctx.is_training:",
            "+# Currently cannot build the wd_cost correctly at inference,",
            "+# because ths vs_name used in inference can be '', therefore the",
            "+# variable filter will fail",
            "+return None",
            "+"
        ],
        "core_change": "+if not ctx.is_training: +# Currently cannot build the wd_cost correctly at inference, +# because ths vs_name used in inference can be '', therefore the +# variable filter will fail +return None +",
        "core_API": "get_collection"
    },
    {
        "commit_hash": "02c38f26d938cf6cdad4a09ada754ed950d024d7",
        "index": "19b55ea2..c188e44a 100644",
        "commit_message": "Fix TF version of imagenet loader (fix #1085)\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def fbresnet_mapper(isTrain):",
            "return image",
            "",
            "def lighting(image, std, eigval, eigvec):",
            "-        v = tf.random_uniform(shape=[3]) * std * eigval",
            "+        v = tf.random_normal(shape=[3], stddev=std) * eigval",
            "inc = tf.matmul(eigvec, tf.reshape(v, [3, 1]))",
            "image = tf.cast(tf.cast(image, tf.float32) + tf.reshape(inc, [3]), image.dtype)",
            "return image"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=IN(type=root), node=ASTNode(type=binary_operator), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=std), value='eigval')",
            "Update(target_node=ASTNode(type=identifier, text=random_uniform), value='random_normal')",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=2277594)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=2277595)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'stddev'), position=0, insert_id=2277596)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2277597)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'std'), position=2, insert_id=2277598)",
            "Delete(target_node=ASTNode(type=*, text=*))",
            "Delete(target_node=ASTNode(type=identifier, text=eigval))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 3400,
        "neg_line": [
            "-v = tf.random_uniform(shape=[3]) * std * eigval"
        ],
        "pos_line": [
            "+v = tf.random_normal(shape=[3], stddev=std) * eigval"
        ],
        "core_change": "-v = tf.random_uniform(shape=[3]) * std * eigval +v = tf.random_normal(shape=[3], stddev=std) * eigval",
        "core_API": "random_uniform"
    },
    {
        "commit_hash": "aa750becf439224df59f80eb57aef5737cf11337",
        "index": "fd323e24..927eed7c 100644",
        "commit_message": "fix Average metric (#4624)\n\n* fix Average metric\n\n* try spawn\n\n* try again\n\n* oops\n\n* clean up, fix evalb too\n\n* use different start method for GPU vs CPU tests\n\n* add comment\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Average(Metric):",
            "_total_value = list(self.detach_tensors(value))[0]",
            "_count = 1",
            "if is_distributed():",
            "-            device = torch.device(\"cpu\")",
            "+            device = torch.device(\"cuda\" if dist.get_backend() == \"nccl\" else \"cpu\")",
            "count = torch.tensor(_count).to(device)",
            "total_value = torch.tensor(_total_value).to(device)",
            "dist.all_reduce(count, op=dist.ReduceOp.SUM)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('conditional_expression', None), position=1, insert_id=7193)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=2, insert_id=7194)",
            "Insert(target_node=IN(type=conditional_expression), node=('string', '\"cuda\"'), position=0, insert_id=7195)",
            "Insert(target_node=IN(type=conditional_expression), node=('if', 'if'), position=1, insert_id=7196)",
            "Insert(target_node=IN(type=conditional_expression), node=('comparison_operator', None), position=2, insert_id=7197)",
            "Insert(target_node=IN(type=conditional_expression), node=('else', 'else'), position=3, insert_id=7198)",
            "Move(target_node=IN(type=conditional_expression), node=ASTNode(type=string, text=\"cpu\"), position=4)",
            "Insert(target_node=IN(type=comparison_operator), node=('call', None), position=0, insert_id=7199)",
            "Insert(target_node=IN(type=comparison_operator), node=('==', '=='), position=1, insert_id=7200)",
            "Insert(target_node=IN(type=comparison_operator), node=('string', '\"nccl\"'), position=2, insert_id=7201)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=7202)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=7203)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dist'), position=0, insert_id=7204)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=7205)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'get_backend'), position=2, insert_id=7206)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=7207)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=), text=)), position=1)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 3401,
        "neg_line": [
            "-device = torch.device(\"cpu\")"
        ],
        "pos_line": [
            "+device = torch.device(\"cuda\" if dist.get_backend() == \"nccl\" else \"cpu\")"
        ],
        "core_change": "-device = torch.device(\"cpu\") +device = torch.device(\"cuda\" if dist.get_backend() == \"nccl\" else \"cpu\")",
        "core_API": "detach_tensors"
    },
    {
        "commit_hash": "bc09e75b78d05e9e5f16ef61e2b99dd58e1a84c2",
        "index": "dd0f29fc9..5200da39b 100644",
        "commit_message": "[RLlib] Fix 3 flakey test cases. (#15785)\n\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "torch, _ = try_import_torch()",
            "class TestDDPG(unittest.TestCase):",
            "@classmethod",
            "def setUpClass(cls) -> None:",
            "+        np.random.seed(42)",
            "+        torch.manual_seed(42)",
            "ray.init()",
            "",
            "@classmethod"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('block', None), position=0, insert_id=1116906)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=1116907)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=1, insert_id=1116908)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=1116909)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=1116910)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1116911)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1116912)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1116913)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1116914)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1116915)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1116916)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'seed'), position=2, insert_id=1116917)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1116918)",
            "Insert(target_node=IN(type=argument_list), node=('integer', '42'), position=1, insert_id=1116919)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1116920)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1116921)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1116922)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'manual_seed'), position=2, insert_id=1116923)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1116924)",
            "Insert(target_node=IN(type=argument_list), node=('integer', '42'), position=1, insert_id=1116925)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1116926)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'np'), position=0, insert_id=1116927)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1116928)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'random'), position=2, insert_id=1116929)",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 2,
        "minus_line": 0,
        "AST_diff_line": 25,
        "number": 3402,
        "neg_line": [],
        "pos_line": [
            "+np.random.seed(42)",
            "+torch.manual_seed(42)"
        ],
        "core_change": "+np.random.seed(42) +torch.manual_seed(42)",
        "core_API": "seed"
    },
    {
        "commit_hash": "92970c0cb9826be8b935e3b50b9c0ad0e2f6c62f",
        "index": "d06376aa4..63ddd7997 100644",
        "commit_message": "Enabling multilingual models for translation pipelines. (#10536)\n\n* [WIP] Enabling multilingual models for translation pipelines.\n\n* decoder_input_ids -> forced_bos_token_id\n\n* Improve docstring.\n\n* Rebase\n\n* Fixing 2 bugs\n\n- Type token_ids coming from `_parse_and_tokenize`\n- Wrong index from tgt_lang.\n\n* Fixing black version.\n\n* Adding tests for _build_translation_inputs and add them for all\ntokenizers.\n\n* Mbart actually puts the lang code at the end.\n\n* Fixing m2m100.\n\n* Adding TF support to `deep_round`.\n\n* Update src/transformers/pipelines/text2text_generation.py\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n\n* Adding one line comment.\n\n* Fixing M2M100 `_build_translation_input_ids`, and fix the call site.\n\n* Fixing tests + deep_round -> nested_simplify\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Pipeline(_ScikitCompat):",
            "Return:",
            ":obj:`Dict[str, torch.Tensor]`: The same as :obj:`inputs` but on the proper device.",
            "\"\"\"",
            "-        return {name: tensor.to(self.device) for name, tensor in inputs.items()}",
            "+        return {",
            "+            name: tensor.to(self.device) if isinstance(tensor, torch.Tensor) else tensor",
            "+            for name, tensor in inputs.items()",
            "+        }",
            "",
            "def check_model_type(self, supported_models: Union[List[str], dict]):",
            "\"\"\""
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"\"\"\n        return {name: tensor.to(self.device) for name, tensor in inputs.items()}\n\ndef check_model_type(self, supported_models: Union[List[str], dict]):\n\"\"\"), value='\"\"\"\\n        return {\\n            name: tensor.to(self.device) if isinstance(tensor, torch.Tensor) else tensor\\n            for name, tensor in inputs.items()\\n        }\\n\\ndef check_model_type(self, supported_models: Union[List[str], dict]):\\n\"\"\"')"
        ],
        "plus_line": 4,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 3403,
        "neg_line": [
            "-return {name: tensor.to(self.device) for name, tensor in inputs.items()}"
        ],
        "pos_line": [
            "+return {",
            "+name: tensor.to(self.device) if isinstance(tensor, torch.Tensor) else tensor",
            "+for name, tensor in inputs.items()",
            "+}"
        ],
        "core_change": "-return {name: tensor.to(self.device) for name, tensor in inputs.items()} +return { +name: tensor.to(self.device) if isinstance(tensor, torch.Tensor) else tensor +for name, tensor in inputs.items() +}",
        "core_API": "to"
    },
    {
        "commit_hash": "7fb3792845b474a41de7c2fe47141dcc37243ef6",
        "index": "d685f72b..3ad2be95 100644",
        "commit_message": "Fix einsum transpose (#2532)\n\nSummary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2532\n\nReviewed By: myleott\n\nDifferential Revision: D32049520\n\nPulled By: sshleifer\n\nfbshipit-source-id: 9036c6db48c15e8a04a27a7d3660bdb2a248f0a5\n\n",
        "file": "fairseq.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TransformerDecoderLayerBase(nn.Module):",
            "if self.c_attn is not None:",
            "tgt_len, bsz = x.size(0), x.size(1)",
            "x = x.view(tgt_len, bsz, self.nh, self.head_dim)",
            "-            x = torch.einsum('tbhd,h->tbdh', x, self.c_attn)",
            "+            x = torch.einsum('tbhd,h->tbhd', x, self.c_attn)",
            "x = x.reshape(tgt_len, bsz, self.embed_dim)",
            "if self.attn_ln is not None:",
            "x = self.attn_ln(x)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text='tbhd,h->tbdh'), value=\"'tbhd,h->tbhd'\")"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 3404,
        "neg_line": [
            "-x = torch.einsum('tbhd,h->tbdh', x, self.c_attn)"
        ],
        "pos_line": [
            "+x = torch.einsum('tbhd,h->tbhd', x, self.c_attn)"
        ],
        "core_change": "-x = torch.einsum('tbhd,h->tbdh', x, self.c_attn) +x = torch.einsum('tbhd,h->tbhd', x, self.c_attn)",
        "core_API": "size"
    },
    {
        "commit_hash": "07f95c19194bab69d13c55d3b942717343e44fc6",
        "index": "ba63e9dc..042d3a1a 100644",
        "commit_message": "fix docs of distributed\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tensorflow",
        "change": [
            "class LoadCheckpoint(session_run_hook.SessionRunHook):",
            "def after_create_session(self, session, coord):",
            "if not self._loaded:",
            "self._loaded = True",
            "-            self._saver.restore(self._checkpoint)",
            "\\ No newline at end of file",
            "+            self._saver.restore(self._checkpoint)"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=4, insert_id=2455895)",
            "Move(target_node=IN(type=expression_statement), node=ASTNode(type=call), position=0)",
            "Insert(target_node=ASTNode(type=assignment), node=('ERROR', None), position=2, insert_id=2455896)",
            "Insert(target_node=ASTNode(type=assignment), node=('identifier', 'file'), position=3, insert_id=2455897)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=true, text=True), position=0)",
            "Insert(target_node=IN(type=ERROR), node=('ERROR', ' '), position=1, insert_id=2455898)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'No'), position=2, insert_id=2455899)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'newline'), position=3, insert_id=2455900)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'at'), position=4, insert_id=2455901)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'end'), position=5, insert_id=2455902)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'of'), position=6, insert_id=2455903)",
            "Delete(target_node=ASTNode(type=ERROR, text= ))",
            "Delete(target_node=ASTNode(type=identifier, text=No))",
            "Delete(target_node=ASTNode(type=identifier, text=newline))",
            "Delete(target_node=ASTNode(type=identifier, text=at))",
            "Delete(target_node=ASTNode(type=identifier, text=end))",
            "Delete(target_node=ASTNode(type=identifier, text=of))",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=identifier, text=file))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 20,
        "number": 3405,
        "neg_line": [
            "-self._saver.restore(self._checkpoint)"
        ],
        "pos_line": [
            "+self._saver.restore(self._checkpoint)"
        ],
        "core_change": "-self._saver.restore(self._checkpoint) +self._saver.restore(self._checkpoint)",
        "core_API": "restore"
    },
    {
        "commit_hash": "1a19939ea3ea02fb977f08975c2b344ed0c333c1",
        "index": "494b69e5..5829cbe0 100644",
        "commit_message": "Replace some fs operations by tf.gfile for other fs support. (fix #416)\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def get_checkpoint_path(model_path):",
            "logger.warn(",
            "\"Checkpoint path {} is auto-corrected to {}.\".format(model_path, new_path))",
            "model_path = new_path",
            "-    assert os.path.isfile(model_path) or os.path.isfile(model_path + '.index'), model_path",
            "+    assert tf.gfile.Exists(model_path) or tf.gfile.Exists(model_path + '.index'), model_path",
            "return model_path"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=isfile), value='Exists')",
            "Update(target_node=ASTNode(type=identifier, text=isfile), value='Exists')",
            "Update(target_node=ASTNode(type=identifier, text=os), value='tf')",
            "Update(target_node=ASTNode(type=identifier, text=path), value='gfile')",
            "Update(target_node=ASTNode(type=identifier, text=os), value='tf')",
            "Update(target_node=ASTNode(type=identifier, text=path), value='gfile')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 3406,
        "neg_line": [
            "-assert os.path.isfile(model_path) or os.path.isfile(model_path + '.index'), model_path"
        ],
        "pos_line": [
            "+assert tf.gfile.Exists(model_path) or tf.gfile.Exists(model_path + '.index'), model_path"
        ],
        "core_change": "-assert os.path.isfile(model_path) or os.path.isfile(model_path + '.index'), model_path +assert tf.gfile.Exists(model_path) or tf.gfile.Exists(model_path + '.index'), model_path",
        "core_API": "warn"
    },
    {
        "commit_hash": "c317aaa3de289fc0a69e6b1001ffc586bb4d9c9a",
        "index": "7158d3a6..cc51c1a5 100644",
        "commit_message": "Refactor library namespaces [pre-release][0.6-rc1] (#1412)\n\n* flake fixes\n\n* initial flake8 fixeS\n\n* remove top level from kornia.color\n\n* kornia filters\n\n* kornia losses\n\n* kornia features\n\n* geomtry and all ok\n\n* removed jit module\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* apply formatting and few fixes\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add keep block for isort\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* skip init\n\n* fix the docs\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* remove old code\n\n* simplify ci workflow\n\n* fix circular dependency\n\n* few format fixes\n\n* fix code format test\n\n* remove kornia.jit imports\n\n* final fixes\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* ci fixes\n\n* add versioneer\n\n* fix pnp import\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* exclude version files from pre-commit\n\n* exclude files precommit\n\n* update to pytorch 1.10 and add fixes\n\n* Update tests_cpu.yml\n\n* Update setup_dev_env.sh\n\n* Update tests_cpu.yml\n\n* undo versioneer\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix no_grad\n\n* Apply suggestions from code review\n\n* fix skip windows tests\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Update test_integrated.py\n\n* Apply suggestions from code review\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def get_projective_transform(center: torch.Tensor, angles: torch.Tensor, scales:",
            "raise AssertionError(center.dtype, angles.dtype)",
            "",
            "# create rotation matrix",
            "-    angle_axis_rad: torch.Tensor = K.deg2rad(angles)",
            "-    rmat: torch.Tensor = K.angle_axis_to_rotation_matrix(angle_axis_rad)  # Bx3x3",
            "-    scaling_matrix: torch.Tensor = K.eye_like(3, rmat)",
            "+    angle_axis_rad: torch.Tensor = deg2rad(angles)",
            "+    rmat: torch.Tensor = angle_axis_to_rotation_matrix(angle_axis_rad)  # Bx3x3",
            "+    scaling_matrix: torch.Tensor = eye_like(3, rmat)",
            "scaling_matrix = scaling_matrix * scales.unsqueeze(dim=1)",
            "rmat = rmat @ scaling_matrix.to(rmat)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=angle_axis_to_rotation_matrix), position=0)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=eye_like), position=0)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=deg2rad), position=0)",
            "Delete(target_node=ASTNode(type=identifier, text=K))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=identifier, text=K))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=identifier, text=K))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 12,
        "number": 3407,
        "neg_line": [
            "-angle_axis_rad: torch.Tensor = K.deg2rad(angles)",
            "-rmat: torch.Tensor = K.angle_axis_to_rotation_matrix(angle_axis_rad)  # Bx3x3",
            "-scaling_matrix: torch.Tensor = K.eye_like(3, rmat)"
        ],
        "pos_line": [
            "+angle_axis_rad: torch.Tensor = deg2rad(angles)",
            "+rmat: torch.Tensor = angle_axis_to_rotation_matrix(angle_axis_rad)  # Bx3x3",
            "+scaling_matrix: torch.Tensor = eye_like(3, rmat)"
        ],
        "core_change": "-angle_axis_rad: torch.Tensor = K.deg2rad(angles) -rmat: torch.Tensor = K.angle_axis_to_rotation_matrix(angle_axis_rad)  # Bx3x3 -scaling_matrix: torch.Tensor = K.eye_like(3, rmat) +angle_axis_rad: torch.Tensor = deg2rad(angles) +rmat: torch.Tensor = angle_axis_to_rotation_matrix(angle_axis_rad)  # Bx3x3 +scaling_matrix: torch.Tensor = eye_like(3, rmat)",
        "core_API": "deg2rad"
    },
    {
        "commit_hash": "439d7609dfcbca5bafa13b7b6098c4d0e9ecddf3",
        "index": "012fa961..224d8e95 100644",
        "commit_message": "GH-462: fix cuda errors in classifier and unit tests\n\n",
        "file": "flair.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TextClassifier(flair.nn.Model):",
            "self.document_embeddings.embed(sentences)",
            "",
            "text_embedding_list = [sentence.get_embedding().unsqueeze(0) for sentence in sentences]",
            "-        text_embedding_tensor = torch.cat(text_embedding_list, 0)",
            "+        text_embedding_tensor = torch.cat(text_embedding_list, 0).to(flair.device)",
            "",
            "label_scores = self.decoder(text_embedding_tensor)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=240637)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=240638)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=240639)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to'), position=2, insert_id=240640)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=240641)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=240642)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=240643)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'flair'), position=0, insert_id=240644)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=240645)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=240646)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 3408,
        "neg_line": [
            "-text_embedding_tensor = torch.cat(text_embedding_list, 0)"
        ],
        "pos_line": [
            "+text_embedding_tensor = torch.cat(text_embedding_list, 0).to(flair.device)"
        ],
        "core_change": "-text_embedding_tensor = torch.cat(text_embedding_list, 0) +text_embedding_tensor = torch.cat(text_embedding_list, 0).to(flair.device)",
        "core_API": "embed"
    },
    {
        "commit_hash": "cf28445f10e4a6b6fbc9456d45128f163f66f764",
        "index": "0835f692..67a4683a 100644",
        "commit_message": "fix cpu usage\n",
        "file": "stable-diffusion-webui.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def reconstruct_cond_batch(c: ScheduledPromptBatch, current_step):",
            "break",
            "res[i] = cond_schedule[target_index].cond",
            "",
            "-    return res.to(shared.device)",
            "+    return res"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=return_statement), node=ASTNode(type=identifier, text=res), position=1)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=to))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=shared))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=device))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 12,
        "number": 3410,
        "neg_line": [
            "-return res.to(shared.device)"
        ],
        "pos_line": [
            "+return res"
        ],
        "core_change": "-return res.to(shared.device) +return res",
        "core_API": "to"
    },
    {
        "commit_hash": "df3f60706458029550c67aeace9264ca86016c3a",
        "index": "41a03f4b9..5f89abaeb 100644",
        "commit_message": "fix gpu decoding\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def recog_v2(args):",
            "for idx, name in enumerate(js.keys(), 1):",
            "logging.info('(%d/%d) decoding ' + name, idx, len(js.keys()))",
            "batch = [(name, js[name])]",
            "-            enc = model.encode(load_inputs_and_targets(batch)[0][0])",
            "+            feat = load_inputs_and_targets(batch)[0][0]",
            "+            enc = model.encode(torch.as_tensor(feat).to(device))",
            "nbest_hyps = beam_search(",
            "x=enc,",
            "sos=model.sos,"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=5, insert_id=169034)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=169035)",
            "Update(target_node=ASTNode(type=identifier, text=enc), value='feat')",
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=subscript), position=2)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'enc'), position=0, insert_id=169036)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=169037)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=call), position=2)",
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=1, insert_id=169038)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=169039)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=169040)",
            "Insert(target_node=IN(type=attribute), node=('call', None), position=0, insert_id=169041)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=169042)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to'), position=2, insert_id=169043)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=169044)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'device'), position=1, insert_id=169045)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=169046)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=169047)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=169048)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=169049)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=169050)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'as_tensor'), position=2, insert_id=169051)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=169052)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'feat'), position=1, insert_id=169053)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=169054)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 24,
        "number": 3415,
        "neg_line": [
            "-enc = model.encode(load_inputs_and_targets(batch)[0][0])"
        ],
        "pos_line": [
            "+feat = load_inputs_and_targets(batch)[0][0]",
            "+enc = model.encode(torch.as_tensor(feat).to(device))"
        ],
        "core_change": "-enc = model.encode(load_inputs_and_targets(batch)[0][0]) +feat = load_inputs_and_targets(batch)[0][0] +enc = model.encode(torch.as_tensor(feat).to(device))",
        "core_API": "keys"
    },
    {
        "commit_hash": "0ba7472da92a89af9aca84de5b01a228d18340a2",
        "index": "fc6d31c16..55d6f940b 100644",
        "commit_message": "[Testing] Fix LINT/sphinx errors. (#8874)\n\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class TorchRNNModel(TorchRNN):",
            "name,",
            "fc_size=64,",
            "lstm_state_size=256):",
            "+        nn.Module.__init__(self)",
            "super().__init__(obs_space, action_space, num_outputs, model_config,",
            "name)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=2, insert_id=1506378)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1506379)",
            "Move(target_node=ASTNode(type=expression_statement), node=ASTNode(type=call), position=0)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=identifier, text=lstm_state_size), position=0)",
            "Insert(target_node=IN(type=assignment), node=('ERROR', None), position=1, insert_id=1506380)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=:, text=:), position=2)",
            "Insert(target_node=IN(type=assignment), node=('type', None), position=3, insert_id=1506381)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type==, text==), position=0)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=integer, text=256), position=1)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=), text=)), position=2)",
            "Insert(target_node=IN(type=type), node=('call', None), position=0, insert_id=1506382)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1506383)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1506384)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1506385)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1506386)",
            "Insert(target_node=IN(type=attribute), node=('identifier', '__init__'), position=2, insert_id=1506387)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1506388)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'self'), position=1, insert_id=1506389)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1506390)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'nn'), position=0, insert_id=1506391)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1506392)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Module'), position=2, insert_id=1506393)",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=assignment))"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 24,
        "number": 3418,
        "neg_line": [],
        "pos_line": [
            "+nn.Module.__init__(self)"
        ],
        "core_change": "+nn.Module.__init__(self)",
        "core_API": "__init__"
    },
    {
        "commit_hash": "f31437d8dbcfe4a4bfedd66f06a294b7e69f42ca",
        "index": "147b48983..13a6a9b29 100644",
        "commit_message": "Fixed error: Expected object of device type cuda but got device type cpu for argument\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DecoderRNNT(torch.nn.Module):",
            "normscore = recog_args.score_norm_transducer",
            "",
            "z_list, c_list = self.zero_state(h.unsqueeze(0))",
            "-        eys = torch.zeros((1, self.embed_dim))",
            "+        eys = to_device(self, torch.zeros((1, self.embed_dim)))",
            "",
            "_, (z_list, c_list) = self.rnn_forward(eys, None)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('identifier', 'to_device'), position=0, insert_id=165073)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=165074)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=165075)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'self'), position=1, insert_id=165076)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=165077)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=call), position=3)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=165078)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 3419,
        "neg_line": [
            "-eys = torch.zeros((1, self.embed_dim))"
        ],
        "pos_line": [
            "+eys = to_device(self, torch.zeros((1, self.embed_dim)))"
        ],
        "core_change": "-eys = torch.zeros((1, self.embed_dim)) +eys = to_device(self, torch.zeros((1, self.embed_dim)))",
        "core_API": "zero_state"
    },
    {
        "commit_hash": "27a1eacee85dd7beabbdc38f26cd495871069b34",
        "index": "480f5914..2ff65ad3 100644",
        "commit_message": "bugfix\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def parse_npz(f):",
            "",
            "adj = sp.csr_matrix((f['adj_data'], f['adj_indices'], f['adj_indptr']),",
            "f['adj_shape']).tocoo()",
            "-    edge_index = torch.tensor([adj.row, adj.col])",
            "+    edge_index = torch.tensor([adj.row, adj.col], dtype=torch.long)",
            "edge_index, _ = remove_self_loops(edge_index)",
            "edge_index = to_undirected(edge_index, x.size(0))  # Internal coalesce."
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1061245)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1061246)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=1061247)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1061248)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=1061249)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1061250)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1061251)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'long'), position=2, insert_id=1061252)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 3421,
        "neg_line": [
            "-edge_index = torch.tensor([adj.row, adj.col])"
        ],
        "pos_line": [
            "+edge_index = torch.tensor([adj.row, adj.col], dtype=torch.long)"
        ],
        "core_change": "-edge_index = torch.tensor([adj.row, adj.col]) +edge_index = torch.tensor([adj.row, adj.col], dtype=torch.long)",
        "core_API": "csr_matrix"
    },
    {
        "commit_hash": "ce7d75497956c6c2544e352eda36f0173a917989",
        "index": "d4531a1..cd60011 100644",
        "commit_message": "fix(data): fix sampler bug for single device (#22)\n\nCo-authored-by: liusongtao <liusongtao@megvii.com>\n",
        "file": "YOLOX.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Exp(MyExp):",
            "",
            "if is_distributed:",
            "batch_size = batch_size // dist.get_world_size()",
            "-            sampler = InfiniteSampler(",
            "-                len(self.dataset), seed=self.seed if self.seed else 0",
            "-            )",
            "-        else:",
            "-            sampler = torch.utils.data.RandomSampler(self.dataset)",
            "+",
            "+        sampler = InfiniteSampler(",
            "+            len(self.dataset), seed=self.seed if self.seed else 0",
            "+        )",
            "",
            "batch_sampler = YoloBatchSampler(",
            "sampler=sampler,"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=else))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=identifier, text=sampler))",
            "Delete(target_node=ASTNode(type=type))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=utils))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=data))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=RandomSampler))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=dataset))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 3,
        "minus_line": 5,
        "AST_diff_line": 25,
        "number": 3423,
        "neg_line": [
            "-sampler = InfiniteSampler(",
            "-len(self.dataset), seed=self.seed if self.seed else 0",
            "-)",
            "-else:",
            "-sampler = torch.utils.data.RandomSampler(self.dataset)"
        ],
        "pos_line": [
            "+",
            "+sampler = InfiniteSampler(",
            "+len(self.dataset), seed=self.seed if self.seed else 0",
            "+)"
        ],
        "core_change": "-sampler = InfiniteSampler( -len(self.dataset), seed=self.seed if self.seed else 0 -) -else: -sampler = torch.utils.data.RandomSampler(self.dataset) + +sampler = InfiniteSampler( +len(self.dataset), seed=self.seed if self.seed else 0 +)",
        "core_API": "get_world_size"
    },
    {
        "commit_hash": "324c84e8d9038dc9c707112dda7b29f5e5e36dd6",
        "index": "ccb536196..1aa0f3b37 100644",
        "commit_message": "[Audio datasets] Adapting all audio datasets (#3081)\n\n* up\n\n* up\n\n* up\n\n* up\n\n* up\n\n* Update src/datasets/utils/resources/readme_structure.yaml\n\nCo-authored-by: Albert Villanova del Moral <8515462+albertvillanova@users.noreply.github.com>\n\n* correct\n\n* correct 2\n\n* Update datasets/covost2/README.md\n\nCo-authored-by: Albert Villanova del Moral <8515462+albertvillanova@users.noreply.github.com>\n\n* Fix typo\n\nCo-authored-by: Albert Villanova del Moral <8515462+albertvillanova@users.noreply.github.com>\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class LJSpeech(datasets.GeneratorBasedBuilder):",
            "example = {",
            "\"id\": uid,",
            "\"file\": os.path.join(wav_path, filename),",
            "+                    \"audio\": os.path.join(wav_path, filename),",
            "\"text\": text,",
            "\"normalized_text\": norm_text,",
            "}"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=dictionary), node=('pair', None), position=5, insert_id=1782486)",
            "Insert(target_node=ASTNode(type=dictionary), node=(',', ','), position=6, insert_id=1782487)",
            "Insert(target_node=IN(type=pair), node=('string', '\"audio\"'), position=0, insert_id=1782488)",
            "Insert(target_node=IN(type=pair), node=(':', ':'), position=1, insert_id=1782489)",
            "Insert(target_node=IN(type=pair), node=('call', None), position=2, insert_id=1782490)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1782491)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1782492)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1782493)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1782494)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'join'), position=2, insert_id=1782495)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1782496)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'wav_path'), position=1, insert_id=1782497)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=1782498)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'filename'), position=3, insert_id=1782499)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=1782500)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'os'), position=0, insert_id=1782501)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1782502)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'path'), position=2, insert_id=1782503)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 18,
        "number": 3425,
        "neg_line": [],
        "pos_line": [
            "+\"audio\": os.path.join(wav_path, filename),"
        ],
        "core_change": "+\"audio\": os.path.join(wav_path, filename),",
        "core_API": "join"
    },
    {
        "commit_hash": "37e98945728f9961fd33d598a954358d79988c28",
        "index": "6cc8e655..3d284311 100644",
        "commit_message": "fix flake8 style in tensorpack/\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def LeakyReLU(x, alpha, name=None):",
            "if name is None:",
            "name = 'output'",
            "return tf.maximum(x, alpha * x, name=name)",
            "-    #alpha = float(alpha)",
            "-    #x = ((1 + alpha) * x + (1 - alpha) * tf.abs(x))",
            "+    # alpha = float(alpha)",
            "+    # x = ((1 + alpha) * x + (1 - alpha) * tf.abs(x))",
            "# return tf.mul(x, 0.5, name=name)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 3426,
        "neg_line": [
            "-#alpha = float(alpha)",
            "-#x = ((1 + alpha) * x + (1 - alpha) * tf.abs(x))"
        ],
        "pos_line": [
            "+# alpha = float(alpha)",
            "+# x = ((1 + alpha) * x + (1 - alpha) * tf.abs(x))"
        ],
        "core_change": "-#alpha = float(alpha) -#x = ((1 + alpha) * x + (1 - alpha) * tf.abs(x)) +# alpha = float(alpha) +# x = ((1 + alpha) * x + (1 - alpha) * tf.abs(x))",
        "core_API": "maximum"
    },
    {
        "commit_hash": "ac200a54eb65ae6b902be200d5f631ee26be10b4",
        "index": "fd87d7b..39a160e 100644",
        "commit_message": "Fixed typos and spacing according to PEP 8\n\n",
        "file": "Tensorflow-Project-Template.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class ExampleModel(BaseModel):",
            "",
            "",
            "def init_saver(self):",
            "-        # here you initalize the tensorflow saver that will be used in saving the checkpoints.",
            "+        # here you initialize the tensorflow saver that will be used in saving the checkpoints.",
            "self.saver = tf.train.Saver(max_to_keep=self.config.max_to_keep)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 3429,
        "neg_line": [
            "-# here you initalize the tensorflow saver that will be used in saving the checkpoints."
        ],
        "pos_line": [
            "+# here you initialize the tensorflow saver that will be used in saving the checkpoints."
        ],
        "core_change": "-# here you initalize the tensorflow saver that will be used in saving the checkpoints. +# here you initialize the tensorflow saver that will be used in saving the checkpoints.",
        "core_API": "Saver"
    },
    {
        "commit_hash": "95dd025ae7f13a0d26f2fdb66cef4c078cbf02a3",
        "index": "c90cebeb..e7b2dc22 100755",
        "commit_message": "Double variable fetch fixed, deprecated arg in softmax renamed.\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Categorical(Distribution):",
            "state_value = tf.reduce_logsumexp(input_tensor=logits, axis=-1)",
            "",
            "# Softmax for corresponding probabilities",
            "-        probabilities = tf.nn.softmax(logits=logits, dim=-1)",
            "+        probabilities = tf.nn.softmax(logits=logits, axis=-1)",
            "",
            "# Min epsilon probability for numerical stability",
            "probabilities = tf.maximum(x=probabilities, y=util.epsilon)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=dim), value='axis')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 3430,
        "neg_line": [
            "-probabilities = tf.nn.softmax(logits=logits, dim=-1)"
        ],
        "pos_line": [
            "+probabilities = tf.nn.softmax(logits=logits, axis=-1)"
        ],
        "core_change": "-probabilities = tf.nn.softmax(logits=logits, dim=-1) +probabilities = tf.nn.softmax(logits=logits, axis=-1)",
        "core_API": "reduce_logsumexp"
    },
    {
        "commit_hash": "badc5517ff1ec433c1f417fcdd90b467e650cedb",
        "index": "ad738a27..1a61891c 100644",
        "commit_message": "fix small bug\n\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class UNetUnconditionalModel(ModelMixin, ConfigMixin):",
            "prev_output_channel = output_channel",
            "",
            "# out",
            "-        self.conv_norm_out = nn.GroupNorm(num_channels=block_channels[0], num_groups=32, eps=1e-5)",
            "+        self.conv_norm_out = nn.GroupNorm(num_channels=block_channels[0], num_groups=32, eps=resnet_eps)",
            "self.conv_act = nn.SiLU()",
            "self.conv_out = nn.Conv2d(block_channels[0], out_channels, 3, padding=1)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=keyword_argument), node=('identifier', 'resnet_eps'), position=2, insert_id=1330252)",
            "Delete(target_node=ASTNode(type=float, text=1e-5))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 3431,
        "neg_line": [
            "-self.conv_norm_out = nn.GroupNorm(num_channels=block_channels[0], num_groups=32, eps=1e-5)"
        ],
        "pos_line": [
            "+self.conv_norm_out = nn.GroupNorm(num_channels=block_channels[0], num_groups=32, eps=resnet_eps)"
        ],
        "core_change": "-self.conv_norm_out = nn.GroupNorm(num_channels=block_channels[0], num_groups=32, eps=1e-5) +self.conv_norm_out = nn.GroupNorm(num_channels=block_channels[0], num_groups=32, eps=resnet_eps)",
        "core_API": "GroupNorm"
    },
    {
        "commit_hash": "43043ee4d5c672f7fbc22ac9559e8164731fd053",
        "index": "6a99bc9c7f..74f272f872 100644",
        "commit_message": "[RLlib] Tf2x preparation; part 2 (upgrading `try_import_tf()`). (#9136)\n\n* WIP.\n\n* Fixes.\n\n* LINT.\n\n* WIP.\n\n* WIP.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* WIP.\n\n* Fixes.\n\n* Test\n\n* Fix.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* LINT.\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Trainer(Trainable):",
            "logging.getLogger(\"ray.rllib\").setLevel(self.config[\"log_level\"])",
            "",
            "def get_scope():",
            "-            if tf and not tf.executing_eagerly():",
            "-                return tf.Graph().as_default()",
            "+            if tf1 and not tf1.executing_eagerly():",
            "+                return tf1.Graph().as_default()",
            "else:",
            "return open(os.devnull)  # fake a no-op scope"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tf), value='tf1')",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=2145661)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=argument_list), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=tf), value='tf1')",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2145662)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=2145663)",
            "Update(target_node=ASTNode(type=identifier, text=tf), value='tf1')",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 10,
        "number": 3435,
        "neg_line": [
            "-if tf and not tf.executing_eagerly():",
            "-return tf.Graph().as_default()"
        ],
        "pos_line": [
            "+if tf1 and not tf1.executing_eagerly():",
            "+return tf1.Graph().as_default()"
        ],
        "core_change": "-if tf and not tf.executing_eagerly(): -return tf.Graph().as_default() +if tf1 and not tf1.executing_eagerly(): +return tf1.Graph().as_default()",
        "core_API": "getLogger"
    },
    {
        "commit_hash": "95b6d985ffb231d9a2047c039c95660913063b9d",
        "index": "0aece37b..70a73b2d 100644",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class PassThroughEncoder(Seq2SeqEncoder):",
            "else:",
            "# We should mask out the output instead of the input.",
            "# But here, output = input, so we directly mask out the input.",
            "-            return inputs * mask.unsqueeze(dim=-1).float()",
            "+            return inputs * mask.unsqueeze(dim=-1)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=float))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 3436,
        "neg_line": [
            "-return inputs * mask.unsqueeze(dim=-1).float()"
        ],
        "pos_line": [
            "+return inputs * mask.unsqueeze(dim=-1)"
        ],
        "core_change": "-return inputs * mask.unsqueeze(dim=-1).float() +return inputs * mask.unsqueeze(dim=-1)",
        "core_API": "unsqueeze"
    },
    {
        "commit_hash": "01daad80cfebc778e9912e62f4241048348ef1cd",
        "index": "d58ee65ba0..9d41021c7f 100644",
        "commit_message": "small fix as zeta of torch does support native out\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def zeta(",
            "*,",
            "out: Optional[torch.Tensor] = None,",
            ") -> torch.Tensor:",
            "-    return torch.special.zeta(x, q)",
            "+    return torch.special.zeta(x, q, out=out)",
            "",
            "",
            "-zeta.support_native_out = False",
            "+zeta.support_native_out = True",
            "",
            "",
            "def gradient("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('true', 'True'), position=2, insert_id=277990)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=277991)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=277992)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'out'), position=0, insert_id=277993)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=277994)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'out'), position=2, insert_id=277995)",
            "Delete(target_node=ASTNode(type=false, text=False))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 3437,
        "neg_line": [
            "-return torch.special.zeta(x, q)",
            "-zeta.support_native_out = False"
        ],
        "pos_line": [
            "+return torch.special.zeta(x, q, out=out)",
            "+zeta.support_native_out = True"
        ],
        "core_change": "-return torch.special.zeta(x, q) +return torch.special.zeta(x, q, out=out) -zeta.support_native_out = False +zeta.support_native_out = True",
        "core_API": "zeta"
    },
    {
        "commit_hash": "f62cb8313c2d7051e38f845823c1f4a7307aac3e",
        "index": "1073d4bfe..9817065ab 100644",
        "commit_message": "Adds CLIP to models exportable with ONNX (#18515)\n\n* onnx config for clip\n\n* default opset as 14\n\n* changes from the original repo\n\n* input values order fix\n\n* outputs fix\n\n* remove unused import\n\n* ran make fix-copies\n\n* black format\n\n* review comments: forward ref, import fix, model change revert, .to cleanup\n\n* make style\n\n* formatting fixes\n\n* revert groupvit\n\n* comment for cast to int32\n\n* comment fix\n\n* make .T as .t() for onnx conversion\n\n* ran make fix-copies\n\n* remove unneeded comment\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n\n* fix copies\n\n* remove comment\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class GroupViTTextTransformer(nn.Module):",
            "",
            "# text_embeds.shape = [batch_size, sequence_length, transformer.width]",
            "# take features from the eot embedding (eot_token is the highest number in each sequence)",
            "-        pooled_output = last_hidden_state[torch.arange(last_hidden_state.shape[0]), input_ids.argmax(dim=-1)]",
            "+        # casting to torch.int for onnx compatibility: argmax doesn't support int64 inputs with opset 14",
            "+        pooled_output = last_hidden_state[",
            "+            torch.arange(last_hidden_state.shape[0]), input_ids.to(torch.int).argmax(dim=-1)",
            "+        ]",
            "",
            "if not return_dict:",
            "return (last_hidden_state, pooled_output) + encoder_outputs[1:]"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('call', None), position=0, insert_id=1193261)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1193262)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1193263)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=input_ids), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1193264)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to'), position=2, insert_id=1193265)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1193266)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=1193267)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1193268)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1193269)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1193270)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'int'), position=2, insert_id=1193271)"
        ],
        "plus_line": 4,
        "minus_line": 1,
        "AST_diff_line": 12,
        "number": 3441,
        "neg_line": [
            "-pooled_output = last_hidden_state[torch.arange(last_hidden_state.shape[0]), input_ids.argmax(dim=-1)]"
        ],
        "pos_line": [
            "+# casting to torch.int for onnx compatibility: argmax doesn't support int64 inputs with opset 14",
            "+pooled_output = last_hidden_state[",
            "+torch.arange(last_hidden_state.shape[0]), input_ids.to(torch.int).argmax(dim=-1)",
            "+]"
        ],
        "core_change": "-pooled_output = last_hidden_state[torch.arange(last_hidden_state.shape[0]), input_ids.argmax(dim=-1)] +# casting to torch.int for onnx compatibility: argmax doesn't support int64 inputs with opset 14 +pooled_output = last_hidden_state[ +torch.arange(last_hidden_state.shape[0]), input_ids.to(torch.int).argmax(dim=-1) +]",
        "core_API": "arange"
    },
    {
        "commit_hash": "abd7110e21102467448035ffdbf6b208a05ac80b",
        "index": "fdf2f1924..00eb03925 100644",
        "commit_message": "gradient norm clipping should be done right before calling the optimiser - fixing run_glue and run_ner as well\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "def train(args, train_dataset, model, tokenizer, labels, pad_token_label_id):",
            "if args.fp16:",
            "with amp.scale_loss(loss, optimizer) as scaled_loss:",
            "scaled_loss.backward()",
            "-                torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)",
            "else:",
            "loss.backward()",
            "-                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)",
            "",
            "tr_loss += loss.item()",
            "if (step + 1) % args.gradient_accumulation_steps == 0:",
            "+                if args.fp16:",
            "+                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)",
            "+                else:",
            "+                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)",
            "+",
            "scheduler.step()  # Update learning rate schedule",
            "optimizer.step()",
            "model.zero_grad()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=7, insert_id=1546610)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=1546611)",
            "Move(target_node=IN(type=if_statement), node=ASTNode(type=comparison_operator), position=1)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=1546612)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=1546613)",
            "Insert(target_node=IN(type=block), node=('if_statement', None), position=0, insert_id=1546614)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=1546615)",
            "Insert(target_node=IN(type=if_statement), node=('attribute', None), position=1, insert_id=1546616)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=1546617)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=1546618)",
            "Insert(target_node=IN(type=if_statement), node=('else_clause', None), position=4, insert_id=1546619)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'args'), position=0, insert_id=1546620)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1546621)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'fp16'), position=2, insert_id=1546622)",
            "Move(target_node=IN(type=block), node=ASTNode(type=expression_statement), position=0)",
            "Insert(target_node=IN(type=else_clause), node=('else', 'else'), position=0, insert_id=1546623)",
            "Insert(target_node=IN(type=else_clause), node=(':', ':'), position=1, insert_id=1546624)",
            "Insert(target_node=IN(type=else_clause), node=('block', None), position=2, insert_id=1546625)",
            "Move(target_node=IN(type=block), node=ASTNode(type=expression_statement), position=0)",
            "Delete(target_node=ASTNode(type=if, text=if))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=block, text=))",
            "Delete(target_node=ASTNode(type=if_statement))"
        ],
        "plus_line": 4,
        "minus_line": 2,
        "AST_diff_line": 23,
        "number": 3442,
        "neg_line": [
            "-torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)",
            "-torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)"
        ],
        "pos_line": [
            "+if args.fp16:",
            "+torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)",
            "+else:",
            "+torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)",
            "+"
        ],
        "core_change": "-torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm) -torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm) +if args.fp16: +torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm) +else: +torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm) +",
        "core_API": "scale_loss"
    },
    {
        "commit_hash": "3fa34b0f8b1c5e4247d2ee39521038a3ee74ff41",
        "index": "61da50bea..11f5e3a31 100644",
        "commit_message": "fix python3 pytorch0.4 errors\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def train(args):",
            "",
            "# Save attention weight each epoch",
            "if args.num_save_attention > 0 and args.mtlalpha != 1.0:",
            "-        data = sorted(valid_json.items()[:args.num_save_attention],",
            "+        data = sorted(list(valid_json.items())[:args.num_save_attention],",
            "key=lambda x: int(x[1]['input'][0]['shape'][1]), reverse=True)",
            "data = converter_kaldi([data], device=gpu_id)",
            "trainer.extend(PlotAttentionReport(model, data, args.outdir + \"/att_ws\"), trigger=(1, 'epoch'))"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('identifier', 'list'), position=0, insert_id=184214)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=184215)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=184216)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=call), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=184217)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 3443,
        "neg_line": [
            "-data = sorted(valid_json.items()[:args.num_save_attention],"
        ],
        "pos_line": [
            "+data = sorted(list(valid_json.items())[:args.num_save_attention],"
        ],
        "core_change": "-data = sorted(valid_json.items()[:args.num_save_attention], +data = sorted(list(valid_json.items())[:args.num_save_attention],",
        "core_API": "items"
    },
    {
        "commit_hash": "c402309470740e17973eeee22a7b90123f85b310",
        "index": "d6824280c..9a931aa73 100644",
        "commit_message": "fix CI error\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def trans(args):",
            "names = [name for name in names if name]",
            "batch = [(name, js[name]) for name in names]",
            "feats = load_inputs_and_targets(batch)[0]",
            "-                nbest_hyps = model.translate_batch(",
            "-                    feats,",
            "-                    args,",
            "-                    train_args.char_list,",
            "-                )",
            "+                nbest_hyps = model.translate_batch(feats, args, train_args.char_list)",
            "",
            "for i, nbest_hyp in enumerate(nbest_hyps):",
            "name = names[i]"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 1,
        "minus_line": 5,
        "AST_diff_line": 1,
        "number": 3446,
        "neg_line": [
            "-nbest_hyps = model.translate_batch(",
            "-feats,",
            "-args,",
            "-train_args.char_list,",
            "-)"
        ],
        "pos_line": [
            "+nbest_hyps = model.translate_batch(feats, args, train_args.char_list)"
        ],
        "core_change": "-nbest_hyps = model.translate_batch( -feats, -args, -train_args.char_list, -) +nbest_hyps = model.translate_batch(feats, args, train_args.char_list)",
        "core_API": "translate_batch"
    },
    {
        "commit_hash": "ac9304bfb28abac37efb9f56005eb9051eb69ba9",
        "index": "1cf80acc..7c08939d 100644",
        "commit_message": "Improve mnist examples and fix bugs. (#382)\n\n* simplified mnist example\n\n* simplied mnist example\n\n* simplied mnist example\n\n* simplied mnist example\n\n* simplied mnist example\n\n* IMPORTANT : fixed D_TYPE bug / as we splited the layers into many file, the D_TYPE in core.py cant change in other files\n\n* update layer config.\n\n* format code\n\n* decouple set_keep.\n\n* fix hao comments.\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class PReluLayer(Layer):",
            "",
            "# with tf.name_scope(name) as scope:",
            "with tf.variable_scope(name):",
            "-            alphas = tf.get_variable(name='alphas', shape=w_shape, initializer=a_init, dtype=D_TYPE, **a_init_args)",
            "+            alphas = tf.get_variable(name='alphas', shape=w_shape, initializer=a_init, dtype=LayersConfig.tf_dtype, **a_init_args)",
            "try:  # TF 1.0",
            "self.outputs = tf.nn.relu(self.inputs) + tf.multiply(alphas, (self.inputs - tf.abs(self.inputs))) * 0.5",
            "except Exception:  # TF 0.12"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=keyword_argument), node=('attribute', None), position=2, insert_id=2264189)",
            "Update(target_node=ASTNode(type=identifier, text=D_TYPE), value='LayersConfig')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=D_TYPE), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2264190)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf_dtype'), position=2, insert_id=2264191)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 3452,
        "neg_line": [
            "-alphas = tf.get_variable(name='alphas', shape=w_shape, initializer=a_init, dtype=D_TYPE, **a_init_args)"
        ],
        "pos_line": [
            "+alphas = tf.get_variable(name='alphas', shape=w_shape, initializer=a_init, dtype=LayersConfig.tf_dtype, **a_init_args)"
        ],
        "core_change": "-alphas = tf.get_variable(name='alphas', shape=w_shape, initializer=a_init, dtype=D_TYPE, **a_init_args) +alphas = tf.get_variable(name='alphas', shape=w_shape, initializer=a_init, dtype=LayersConfig.tf_dtype, **a_init_args)",
        "core_API": "name_scope"
    },
    {
        "commit_hash": "c5938f8fbc9d942986718b0973afe27eaedaa8bc",
        "index": "1740d529c..98cad39a0 100644",
        "commit_message": "Fix torchelastic detection with non-distributed installations (#13142)\n\n* Fix torchelastic detection under Mac\n\n* CHANGELOG\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TorchElasticEnvironment(ClusterEnvironment):",
            "def detect() -> bool:",
            "\"\"\"Returns ``True`` if the current process was launched using the torchelastic command.\"\"\"",
            "if _TORCH_GREATER_EQUAL_1_9_1:",
            "-            return torch.distributed.is_torchelastic_launched()",
            "+            # if not available (for example on MacOS), `is_torchelastic_launched` is not defined",
            "+            return torch.distributed.is_available() and torch.distributed.is_torchelastic_launched()",
            "required_env_vars = {\"RANK\", \"GROUP_RANK\", \"LOCAL_RANK\", \"LOCAL_WORLD_SIZE\"}",
            "return required_env_vars.issubset(os.environ.keys())"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=return_statement), node=('boolean_operator', None), position=1, insert_id=507605)",
            "Insert(target_node=IN(type=boolean_operator), node=('call', None), position=0, insert_id=507606)",
            "Insert(target_node=IN(type=boolean_operator), node=('and', 'and'), position=1, insert_id=507607)",
            "Move(target_node=IN(type=boolean_operator), node=ASTNode(type=call), position=2)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=507608)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=507609)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=507610)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=507611)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'is_available'), position=2, insert_id=507612)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=507613)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=507614)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=507615)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=507616)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'distributed'), position=2, insert_id=507617)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 14,
        "number": 3456,
        "neg_line": [
            "-return torch.distributed.is_torchelastic_launched()"
        ],
        "pos_line": [
            "+# if not available (for example on MacOS), `is_torchelastic_launched` is not defined",
            "+return torch.distributed.is_available() and torch.distributed.is_torchelastic_launched()"
        ],
        "core_change": "-return torch.distributed.is_torchelastic_launched() +# if not available (for example on MacOS), `is_torchelastic_launched` is not defined +return torch.distributed.is_available() and torch.distributed.is_torchelastic_launched()",
        "core_API": "is_torchelastic_launched"
    },
    {
        "commit_hash": "f41d5f021acdf29726c05f32abfbcca78edcdf21",
        "index": "91f1f4f1..5d117723 100644",
        "commit_message": "PEP8 fix.\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def cast(x, dtype):",
            "# you need to assign it.",
            ">>> input = K.cast(input, dtype='float16')",
            ">>> input",
            "-        <tf.Tensor 'Cast_2:0' shape=(2, 3) dtype=float16>",
            "+        <tf.Tensor 'Cast_2:0' shape=(2, 3) dtype=float16>",
            "```",
            "\"\"\"",
            "return tf.cast(x, dtype)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 3459,
        "neg_line": [
            "-<tf.Tensor 'Cast_2:0' shape=(2, 3) dtype=float16>"
        ],
        "pos_line": [
            "+<tf.Tensor 'Cast_2:0' shape=(2, 3) dtype=float16>"
        ],
        "core_change": "-<tf.Tensor 'Cast_2:0' shape=(2, 3) dtype=float16> +<tf.Tensor 'Cast_2:0' shape=(2, 3) dtype=float16>",
        "core_API": "cast"
    },
    {
        "commit_hash": "95b6d985ffb231d9a2047c039c95660913063b9d",
        "index": "f3d49ba0..fced4ef4 100644",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestFeedforwardEncoder(AllenNlpTestCase):",
            ")",
            "",
            "# mask should work",
            "-        mask = torch.LongTensor([[1, 1, 1], [1, 0, 0]])",
            "+        mask = torch.BoolTensor([[True, True, True], [True, False, False]])",
            "output = encoder(tensor, mask)",
            "target = feedforward(tensor) * mask.unsqueeze(dim=-1).float()",
            "numpy.testing.assert_array_almost_equal("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=LongTensor), value='BoolTensor')",
            "Move(target_node=ASTNode(type=list), node=ASTNode(type=,, text=,), position=2)",
            "Insert(target_node=ASTNode(type=list), node=('true', 'True'), position=1, insert_id=19812)",
            "Insert(target_node=ASTNode(type=list), node=('true', 'True'), position=4, insert_id=19813)",
            "Move(target_node=ASTNode(type=list), node=ASTNode(type=,, text=,), position=5)",
            "Insert(target_node=ASTNode(type=list), node=('true', 'True'), position=6, insert_id=19814)",
            "Move(target_node=ASTNode(type=list), node=ASTNode(type=], text=]), position=7)",
            "Insert(target_node=ASTNode(type=list), node=('true', 'True'), position=1, insert_id=19815)",
            "Insert(target_node=ASTNode(type=list), node=('false', 'False'), position=4, insert_id=19816)",
            "Insert(target_node=ASTNode(type=list), node=(',', ','), position=5, insert_id=19817)",
            "Insert(target_node=ASTNode(type=list), node=('false', 'False'), position=6, insert_id=19818)",
            "Insert(target_node=ASTNode(type=list), node=(']', ']'), position=7, insert_id=19819)",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=integer, text=0))",
            "Delete(target_node=ASTNode(type=integer, text=0))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 20,
        "number": 3464,
        "neg_line": [
            "-mask = torch.LongTensor([[1, 1, 1], [1, 0, 0]])"
        ],
        "pos_line": [
            "+mask = torch.BoolTensor([[True, True, True], [True, False, False]])"
        ],
        "core_change": "-mask = torch.LongTensor([[1, 1, 1], [1, 0, 0]]) +mask = torch.BoolTensor([[True, True, True], [True, False, False]])",
        "core_API": "LongTensor"
    },
    {
        "commit_hash": "d144c46a593665ff06fa6c0b1492ffafeffdec17",
        "index": "55062c32..e4cedbff 100644",
        "commit_message": "[UNet2DConditionModel, UNet2DModel] pass norm_num_groups to all the blocks (#442)\n\n* pass norm_num_groups to unet blocs and attention\n\n* fix UNet2DConditionModel\n\n* add norm_num_groups arg in vae\n\n* add tests\n\n* remove comment\n\n* Apply suggestions from code review\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SpatialTransformer(nn.Module):",
            "self.d_head = d_head",
            "self.in_channels = in_channels",
            "inner_dim = n_heads * d_head",
            "-        self.norm = torch.nn.GroupNorm(num_groups=32, num_channels=in_channels, eps=1e-6, affine=True)",
            "+        self.norm = torch.nn.GroupNorm(num_groups=num_groups, num_channels=in_channels, eps=1e-6, affine=True)",
            "",
            "self.proj_in = nn.Conv2d(in_channels, inner_dim, kernel_size=1, stride=1, padding=0)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=keyword_argument), node=('identifier', 'num_groups'), position=2, insert_id=104410)",
            "Delete(target_node=ASTNode(type=integer, text=32))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 3465,
        "neg_line": [
            "-self.norm = torch.nn.GroupNorm(num_groups=32, num_channels=in_channels, eps=1e-6, affine=True)"
        ],
        "pos_line": [
            "+self.norm = torch.nn.GroupNorm(num_groups=num_groups, num_channels=in_channels, eps=1e-6, affine=True)"
        ],
        "core_change": "-self.norm = torch.nn.GroupNorm(num_groups=32, num_channels=in_channels, eps=1e-6, affine=True) +self.norm = torch.nn.GroupNorm(num_groups=num_groups, num_channels=in_channels, eps=1e-6, affine=True)",
        "core_API": "GroupNorm"
    },
    {
        "commit_hash": "351753aae55894591dafa81814eaa82a59687f09",
        "index": "18f141d095..5b9328c3c4 100644",
        "commit_message": "[rllib] Remove dependency on TensorFlow (#4764)\n\n* remove hard tf dep\n\n* add test\n\n* comment fix\n\n* fix test\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tensorflow",
        "change": [
            "class LSTM(Model):",
            "",
            "@override(Model)",
            "def _build_layers_v2(self, input_dict, num_outputs, options):",
            "+        import tensorflow.contrib.rnn as rnn",
            "+",
            "cell_size = options.get(\"lstm_cell_size\")",
            "if options.get(\"lstm_use_prev_action_reward\"):",
            "action_dim = int("
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('block', None), position=0, insert_id=2454464)",
            "Insert(target_node=IN(type=block), node=('import_statement', None), position=0, insert_id=2454465)",
            "Insert(target_node=IN(type=import_statement), node=('import', 'import'), position=0, insert_id=2454466)",
            "Insert(target_node=IN(type=import_statement), node=('aliased_import', None), position=1, insert_id=2454467)",
            "Insert(target_node=IN(type=aliased_import), node=('dotted_name', None), position=0, insert_id=2454468)",
            "Insert(target_node=IN(type=aliased_import), node=('as', 'as'), position=1, insert_id=2454469)",
            "Insert(target_node=IN(type=aliased_import), node=('identifier', 'rnn'), position=2, insert_id=2454470)",
            "Insert(target_node=IN(type=dotted_name), node=('identifier', 'tensorflow'), position=0, insert_id=2454471)",
            "Insert(target_node=IN(type=dotted_name), node=('.', '.'), position=1, insert_id=2454472)",
            "Insert(target_node=IN(type=dotted_name), node=('identifier', 'contrib'), position=2, insert_id=2454473)",
            "Insert(target_node=IN(type=dotted_name), node=('.', '.'), position=3, insert_id=2454474)",
            "Insert(target_node=IN(type=dotted_name), node=('identifier', 'rnn'), position=4, insert_id=2454475)",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 13,
        "number": 3466,
        "neg_line": [],
        "pos_line": [
            "+import tensorflow.contrib.rnn as rnn",
            "+"
        ],
        "core_change": "+import tensorflow.contrib.rnn as rnn +",
        "core_API": "get"
    },
    {
        "commit_hash": "4d5da4b663d7a2210a9fe4965ab942ad7557efb0",
        "index": "9b151cac..8a45d9e3 100644",
        "commit_message": "fix travis + pylint tests\n\n",
        "file": "TTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class WaveRNN(nn.Module):",
            "",
            "def forward(self, x, mels):",
            "bsize = x.size(0)",
            "-        h1 = torch.zeros(1, bsize, self.rnn_dims).to(x.device)",
            "+        h1 = torch.zeros(1, bsize, self.rnn_dims).to(x.device)",
            "h2 = torch.zeros(1, bsize, self.rnn_dims).to(x.device)",
            "mels, aux = self.upsample(mels)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 3468,
        "neg_line": [
            "-h1 = torch.zeros(1, bsize, self.rnn_dims).to(x.device)"
        ],
        "pos_line": [
            "+h1 = torch.zeros(1, bsize, self.rnn_dims).to(x.device)"
        ],
        "core_change": "-h1 = torch.zeros(1, bsize, self.rnn_dims).to(x.device) +h1 = torch.zeros(1, bsize, self.rnn_dims).to(x.device)",
        "core_API": "size"
    },
    {
        "commit_hash": "ddfb38efa733f52ced8d02b03c9fd913e5d7e044",
        "index": "8752228a..564b5657 100644",
        "commit_message": "add pytorch 1.1.0 SyncBN support (#577)\n\n* add pytorch 1.1.0 SyncBN support\n\n* change BatchNorm2d to _BatchNorm and call freeze after train\n\n* add freeze back to init function\n\n* fixed indentation typo in adding freeze\n\n* use SyncBN protect member func to set ddp_gpu_num\n\n* Update README.md\n\nupdate pytorch version to 1.1\n\n",
        "file": "mmdetection.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class ResNet(nn.Module):",
            "",
            "def train(self, mode=True):",
            "super(ResNet, self).train(mode)",
            "+        self._freeze_stages()",
            "if mode and self.norm_eval:",
            "for m in self.modules():",
            "# trick: eval have effect on BatchNorm only",
            "-                if isinstance(m, nn.BatchNorm2d):",
            "+                if isinstance(m, _BatchNorm):",
            "m.eval()"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=1427315)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=1427316)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1427317)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1427318)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=1427319)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1427320)",
            "Insert(target_node=IN(type=attribute), node=('identifier', '_freeze_stages'), position=2, insert_id=1427321)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1427322)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1427323)",
            "Update(target_node=ASTNode(type=identifier, text=nn), value='_BatchNorm')",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=nn), position=3)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=BatchNorm2d))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 14,
        "number": 3469,
        "neg_line": [
            "-if isinstance(m, nn.BatchNorm2d):"
        ],
        "pos_line": [
            "+self._freeze_stages()",
            "+if isinstance(m, _BatchNorm):"
        ],
        "core_change": "+self._freeze_stages() -if isinstance(m, nn.BatchNorm2d): +if isinstance(m, _BatchNorm):",
        "core_API": "_freeze_stages"
    },
    {
        "commit_hash": "b96a734ec3dd981d6a1f1aa3855d89cd3c49926f",
        "index": "c29833f6..b4a91a6c 100644",
        "commit_message": "batchnorm refactored, bug fixed, get_variable_with_initializer changed\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Input(Layer):",
            "logging.info(\"Input  %s: %s\" % (self.name, str(shape)))",
            "",
            "shape_without_none = [_ if _ is not None else 1 for _ in shape]",
            "-        self.outputs = self.forward(tf.initializers.random_normal()(shape_without_none))",
            "+        self.outputs = self.forward(tf.compat.v1.initializers.random_normal()(shape_without_none))",
            "",
            "def __call__(self, prev_layer):",
            "# FIXME: better exception raising"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=2258850)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=2258851)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2258852)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2258853)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'v1'), position=2, insert_id=2258854)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=tf), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'compat'), position=2, insert_id=2258855)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 3470,
        "neg_line": [
            "-self.outputs = self.forward(tf.initializers.random_normal()(shape_without_none))"
        ],
        "pos_line": [
            "+self.outputs = self.forward(tf.compat.v1.initializers.random_normal()(shape_without_none))"
        ],
        "core_change": "-self.outputs = self.forward(tf.initializers.random_normal()(shape_without_none)) +self.outputs = self.forward(tf.compat.v1.initializers.random_normal()(shape_without_none))",
        "core_API": "info"
    },
    {
        "commit_hash": "ff06e9580b67852a475dd7a7a47aa78b19c5bca4",
        "index": "e390458c4..dc681f84e 100644",
        "commit_message": " Fixing Monorepo CI  (#5642)\n\n* Updating GitHub CI workflow files\n\n* Fixing issue with MNIST dataset and torch downgrade from SyMPC\n\n* Fixed Syft and Grid pre-commit to be isolated\n\n* Bumped SyMPC to torch 1.8.1 branch\n\n* Moved MNIST to our own mirror on GitHub\n\n* Run PyGrid MCFL tests without random and waiting on finish app start\n\n* Added retry to grid_connect to fix CI random refused connections\n",
        "file": "PySyft.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "if TORCHVISION_VERSION < version.parse(\"0.9.1\"):",
            "\"ec29112dd5afa0611ce80d1b7f02629c\",",
            "),",
            "]",
            "-",
            "+else:",
            "+    torchvision.datasets.MNIST.mirrors.insert(0, URL)",
            "",
            "torchvision.datasets.MNIST(get_root_data_path(), train=True, download=True)",
            "torchvision.datasets.MNIST(get_root_data_path(), train=False, download=True)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=1799287)",
            "Insert(target_node=ASTNode(type=module), node=('ERROR', None), position=4, insert_id=1799288)",
            "Insert(target_node=ASTNode(type=ERROR), node=('else', 'else'), position=3, insert_id=1799289)",
            "Insert(target_node=ASTNode(type=ERROR), node=(':', ':'), position=4, insert_id=1799290)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=1799291)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'torchvision'), position=0, insert_id=1799292)",
            "Insert(target_node=IN(type=ERROR), node=('.', '.'), position=1, insert_id=1799293)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1799294)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1799295)",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1799296)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1799297)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1799298)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'insert'), position=2, insert_id=1799299)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1799300)",
            "Insert(target_node=IN(type=argument_list), node=('integer', '0'), position=1, insert_id=1799301)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=1799302)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'URL'), position=3, insert_id=1799303)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=1799304)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'datasets'), position=0, insert_id=1799305)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1799306)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'MNIST'), position=2, insert_id=1799307)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1799308)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'mirrors'), position=2, insert_id=1799309)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 24,
        "number": 3471,
        "neg_line": [
            "-"
        ],
        "pos_line": [
            "+else:",
            "+torchvision.datasets.MNIST.mirrors.insert(0, URL)"
        ],
        "core_change": "- +else: +torchvision.datasets.MNIST.mirrors.insert(0, URL)",
        "core_API": "parse"
    },
    {
        "commit_hash": "92f122e0df7e233f3a8b7873c7294155afbbf852",
        "index": "8b392e5a1..c5d361364 100644",
        "commit_message": "Fix average_precision metric (#2319)\n\n* Fixed average_precision metric, parenthesis were missing. Added test test that failed with the old implementation\n\n* Modified CHANGELOG.md\n\n* Update CHANGELOG.md\n\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\n\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def average_precision(",
            "# Return the step function integral",
            "# The following works because the last entry of precision is",
            "# guaranteed to be 1, as returned by precision_recall_curve",
            "-    return -torch.sum(recall[1:] - recall[:-1] * precision[:-1])",
            "+    return -torch.sum((recall[1:] - recall[:-1]) * precision[:-1])",
            "",
            "",
            "def dice_score("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=ERROR), node=('parenthesized_expression', None), position=5, insert_id=578424)",
            "Insert(target_node=ASTNode(type=ERROR), node=('ERROR', None), position=8, insert_id=578425)",
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=identifier, text=dice_score), position=9)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('(', '('), position=0, insert_id=578426)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('binary_operator', None), position=1, insert_id=578427)",
            "Insert(target_node=IN(type=parenthesized_expression), node=(')', ')'), position=2, insert_id=578428)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=subscript), position=0)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=), text=)), position=1)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=def, text=def), position=2)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=subscript), position=0)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=-, text=-), position=1)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=subscript), position=2)",
            "Delete(target_node=ASTNode(type=ERROR))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 13,
        "number": 3474,
        "neg_line": [
            "-return -torch.sum(recall[1:] - recall[:-1] * precision[:-1])"
        ],
        "pos_line": [
            "+return -torch.sum((recall[1:] - recall[:-1]) * precision[:-1])"
        ],
        "core_change": "-return -torch.sum(recall[1:] - recall[:-1] * precision[:-1]) +return -torch.sum((recall[1:] - recall[:-1]) * precision[:-1])",
        "core_API": "sum"
    },
    {
        "commit_hash": "6d247bd4fd76b45998747ecc3367daab5f5e0b82",
        "index": "c9b6711b7..1efa721b1 100644",
        "commit_message": "Fix shards in IterableDataset.from_generator (#5233)\n\n* correctly pass the gen_kwards in Generator builder\n\n* docs\n\n* tests\n\n* style\n\n* more docs\n\n* typo\n\n* typo2\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class Generator(datasets.GeneratorBasedBuilder):",
            "return datasets.DatasetInfo(features=self.config.features)",
            "",
            "def _split_generators(self, dl_manager):",
            "-        return [datasets.SplitGenerator(name=datasets.Split.TRAIN, gen_kwargs={})]",
            "+        return [datasets.SplitGenerator(name=datasets.Split.TRAIN, gen_kwargs=self.config.gen_kwargs)]",
            "",
            "-    def _generate_examples(self):",
            "-        for idx, ex in enumerate(self.config.generator(**self.config.gen_kwargs)):",
            "+    def _generate_examples(self, **gen_kwargs):",
            "+        for idx, ex in enumerate(self.config.generator(**gen_kwargs)):",
            "yield idx, ex"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=2, insert_id=1775331)",
            "Insert(target_node=ASTNode(type=parameters), node=('dictionary_splat_pattern', None), position=3, insert_id=1775332)",
            "Insert(target_node=IN(type=dictionary_splat_pattern), node=('**', '**'), position=0, insert_id=1775333)",
            "Insert(target_node=IN(type=dictionary_splat_pattern), node=('identifier', 'gen_kwargs'), position=1, insert_id=1775334)",
            "Move(target_node=ASTNode(type=keyword_argument), node=ASTNode(type=attribute), position=2)",
            "Insert(target_node=ASTNode(type=dictionary_splat), node=('identifier', 'gen_kwargs'), position=1, insert_id=1775335)",
            "Delete(target_node=ASTNode(type={, text={))",
            "Delete(target_node=ASTNode(type=}, text=}))",
            "Delete(target_node=ASTNode(type=dictionary))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 9,
        "number": 3477,
        "neg_line": [
            "-return [datasets.SplitGenerator(name=datasets.Split.TRAIN, gen_kwargs={})]",
            "-def _generate_examples(self):",
            "-for idx, ex in enumerate(self.config.generator(**self.config.gen_kwargs)):"
        ],
        "pos_line": [
            "+return [datasets.SplitGenerator(name=datasets.Split.TRAIN, gen_kwargs=self.config.gen_kwargs)]",
            "+def _generate_examples(self, **gen_kwargs):",
            "+for idx, ex in enumerate(self.config.generator(**gen_kwargs)):"
        ],
        "core_change": "-return [datasets.SplitGenerator(name=datasets.Split.TRAIN, gen_kwargs={})] +return [datasets.SplitGenerator(name=datasets.Split.TRAIN, gen_kwargs=self.config.gen_kwargs)] -def _generate_examples(self): -for idx, ex in enumerate(self.config.generator(**self.config.gen_kwargs)): +def _generate_examples(self, **gen_kwargs): +for idx, ex in enumerate(self.config.generator(**gen_kwargs)):",
        "core_API": "DatasetInfo"
    },
    {
        "commit_hash": "592c161032bfd697ef6f2f334c5cd007062bec4b",
        "index": "cdb78f982..61fcb2a11 100644",
        "commit_message": "[RLlib] Issue 12118: LSTM prev-a/r should be separately configurable. Fix missing prev-a one-hot encoding. (#12397)\n\n* WIP.\n\n* Fix and LINT.\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def one_hot(x, depth=0, on_value=1, off_value=0):",
            "x = x.astype(np.int)",
            "depth = 2",
            "",
            "+    # If depth is not given, try to infer it from the values in the array.",
            "if depth == 0:",
            "depth = np.max(x) + 1",
            "assert np.max(x) < depth, \\"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 17,
        "number": 3479,
        "neg_line": [],
        "pos_line": [
            "+# If depth is not given, try to infer it from the values in the array."
        ],
        "core_change": "+# If depth is not given, try to infer it from the values in the array.",
        "core_API": "astype"
    },
    {
        "commit_hash": "05978e06215d31234c15c5306d57bd4dca69598a",
        "index": "5efdcd03..c012c273 100644",
        "commit_message": "functionality for saving optimizer state (#257)\n\n* cleanup\n\n* remove print statement\n\n* fix weird merge nonsense\n\n* fix weird merge nonsense part 2\n\n* py3 tab nonsense\n\n* more tests\n\n* fl8\n\n*  py3 blah\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class MaskedLinear(nn.Linear):",
            "Constructor",
            "\"\"\"",
            "super(MaskedLinear, self).__init__(in_features, out_features, bias)",
            "-        self.register_buffer('mask', mask)",
            "+        self.register_buffer('mask', mask.data)",
            "",
            "def forward(self, _input):",
            "-        masked_weight = self.weight * self.mask",
            "+        masked_weight = self.weight * torch.autograd.Variable(self.mask)",
            "return F.linear(_input, masked_weight, self.bias)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=3, insert_id=764253)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=mask), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=764254)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'data'), position=2, insert_id=764255)",
            "Insert(target_node=ASTNode(type=binary_operator), node=('call', None), position=2, insert_id=764256)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=764257)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=764258)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=764259)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=764260)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Variable'), position=2, insert_id=764261)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=764262)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=attribute), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=764263)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=764264)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=764265)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'autograd'), position=2, insert_id=764266)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 16,
        "number": 3481,
        "neg_line": [
            "-self.register_buffer('mask', mask)",
            "-masked_weight = self.weight * self.mask"
        ],
        "pos_line": [
            "+self.register_buffer('mask', mask.data)",
            "+masked_weight = self.weight * torch.autograd.Variable(self.mask)"
        ],
        "core_change": "-self.register_buffer('mask', mask) +self.register_buffer('mask', mask.data) -masked_weight = self.weight * self.mask +masked_weight = self.weight * torch.autograd.Variable(self.mask)",
        "core_API": "register_buffer"
    },
    {
        "commit_hash": "ef5feac7ba104011175448e21ff660f64965ac2d",
        "index": "dfca76f46..8b65ca143 100644",
        "commit_message": "fix version + yapf (#6999)\n\n\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def atomic_save(checkpoint, filepath: str):",
            "# Can't use the new zipfile serialization for 1.6.0 because there's a bug in",
            "# torch.hub.load_state_dict_from_url() that prevents it from loading the new files.",
            "# More details can be found here: https://github.com/pytorch/pytorch/issues/42239",
            "-    if LooseVersion(torch.__version__).version[:3] == [1, 6, 0]:",
            "+    if Version(torch.__version__).release[:3] == (1, 6, 0):",
            "torch.save(checkpoint, bytesbuffer, _use_new_zipfile_serialization=False)",
            "else:",
            "torch.save(checkpoint, bytesbuffer)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=comparison_operator), node=('tuple', None), position=2, insert_id=537779)",
            "Insert(target_node=IN(type=tuple), node=('(', '('), position=0, insert_id=537780)",
            "Move(target_node=IN(type=tuple), node=ASTNode(type=integer, text=1), position=1)",
            "Move(target_node=IN(type=tuple), node=ASTNode(type=,, text=,), position=2)",
            "Move(target_node=IN(type=tuple), node=ASTNode(type=integer, text=6), position=3)",
            "Move(target_node=IN(type=tuple), node=ASTNode(type=,, text=,), position=4)",
            "Move(target_node=IN(type=tuple), node=ASTNode(type=integer, text=0), position=5)",
            "Insert(target_node=IN(type=tuple), node=(')', ')'), position=6, insert_id=537781)",
            "Update(target_node=ASTNode(type=identifier, text=version), value='release')",
            "Update(target_node=ASTNode(type=identifier, text=LooseVersion), value='Version')",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=list))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 13,
        "number": 3484,
        "neg_line": [
            "-if LooseVersion(torch.__version__).version[:3] == [1, 6, 0]:"
        ],
        "pos_line": [
            "+if Version(torch.__version__).release[:3] == (1, 6, 0):"
        ],
        "core_change": "-if LooseVersion(torch.__version__).version[:3] == [1, 6, 0]: +if Version(torch.__version__).release[:3] == (1, 6, 0):",
        "core_API": "load_state_dict_from_url"
    },
    {
        "commit_hash": "caf3746678111e906182416603e8f26d7eab1094",
        "index": "d9a8a4c8c..63db82723 100644",
        "commit_message": "fix indentation issue (#4941)\n\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class PyTorchBenchmark(Benchmark):",
            ")",
            "torch.cuda.reset_max_memory_cached()",
            "",
            "-                        # run forward",
            "-                        _forward()",
            "+                    # run forward",
            "+                    _forward()",
            "elif not self.args.no_tpu and is_torch_tpu_available():",
            "# tpu",
            "raise NotImplementedError("
        ],
        "hunk_index": 2,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 3485,
        "neg_line": [
            "-# run forward",
            "-_forward()"
        ],
        "pos_line": [
            "+# run forward",
            "+_forward()"
        ],
        "core_change": "-# run forward -_forward() +# run forward +_forward()",
        "core_API": "reset_max_memory_cached"
    },
    {
        "commit_hash": "90fbed81af2debf6893f0b12f25d462d4f54629b",
        "index": "b548f82..fdb21db 100644",
        "commit_message": "fix errors\n\n",
        "file": "nebullvm.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "models",
        "change": [
            "class MultiCompilerOptimizer(BaseOptimizer):",
            ")",
            "if return_all:",
            "return optimized_models",
            "-        optimized_models.sort(key=lambda x: x[1], ascending=True)",
            "+        optimized_models.sort(key=lambda x: x[1], reverse=False)",
            "return optimized_models[0][0]"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=ascending), value='reverse')",
            "Insert(target_node=ASTNode(type=keyword_argument), node=('false', 'False'), position=2, insert_id=2671099)",
            "Delete(target_node=ASTNode(type=true, text=True))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 3,
        "number": 3490,
        "neg_line": [
            "-optimized_models.sort(key=lambda x: x[1], ascending=True)"
        ],
        "pos_line": [
            "+optimized_models.sort(key=lambda x: x[1], reverse=False)"
        ],
        "core_change": "-optimized_models.sort(key=lambda x: x[1], ascending=True) +optimized_models.sort(key=lambda x: x[1], reverse=False)",
        "core_API": "sort"
    },
    {
        "commit_hash": "568710f2f2484a3fc2d255e7c3393ee0640e3929",
        "index": "053a153ad..dd39ddb35 100644",
        "commit_message": "Fix tests failing on a single GPU (#11753)\n\nCo-authored-by: akihiro@grid.ai <akihiro@grid.ai>\nCo-authored-by: Carlos Mocholi <carlossmocholi@gmail.com>\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_quantization(tmpdir, observe: str, fuse: bool, convert: bool):",
            "# todo: make it work also with strict loading",
            "qmodel2 = RegressionModel.load_from_checkpoint(model_path, strict=False)",
            "quant2_score = torch.mean(torch.tensor([mape(qmodel2(x), y) for x, y in dm.test_dataloader()]))",
            "-    assert torch.allclose(org_score, quant2_score, atol=0.45)",
            "+    assert torch.allclose(org_score, quant2_score, atol=0.47)",
            "",
            "# test without and with QAT callback",
            "trainer_args.update(max_epochs=curr_epoch + 1)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=float, text=0.45), value='0.47')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 3491,
        "neg_line": [
            "-assert torch.allclose(org_score, quant2_score, atol=0.45)"
        ],
        "pos_line": [
            "+assert torch.allclose(org_score, quant2_score, atol=0.47)"
        ],
        "core_change": "-assert torch.allclose(org_score, quant2_score, atol=0.45) +assert torch.allclose(org_score, quant2_score, atol=0.47)",
        "core_API": "load_from_checkpoint"
    },
    {
        "commit_hash": "d669a74623f273f74213a88b5233964d1ab3ea08",
        "index": "43cebc7..2e794c3 100644",
        "commit_message": "Detect.py supports running against a Triton container (#9228)\n\n* update coco128-seg comments\n\n* Enables detect.py to use Triton for inference\n\nTriton Inference Server is an open source inference serving software\nthat streamlines AI inferencing.\nhttps://github.com/triton-inference-server/server\n\nThe user can now provide a \"--triton-url\" argument to detect.py to use\na local or remote Triton server for inference.\nFor e.g., http://localhost:8000 will use http over port 8000\nand grpc://localhost:8001 will use grpc over port 8001.\nNote, it is not necessary to specify a weights file to use Triton.\n\nA Triton container can be created by first exporting the Yolov5 model\nto a Triton supported runtime. Onnx, Torchscript, TensorRT are\nsupported by both Triton and the export.py script.\n\nThe exported model can then be containerized via the OctoML CLI.\nSee https://github.com/octoml/octo-cli#getting-started for a guide.\n\n* added triton client to requirements\n\n* fixed support for TFSavedModels in Triton\n\n* reverted change\n\n* Test CoreML update\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Update ci-testing.yml\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Use pathlib\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Refacto DetectMultiBackend to directly accept triton url as --weights http://...\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Deploy category\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Update detect.py\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Update common.py\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Update common.py\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Update predict.py\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Update predict.py\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Update predict.py\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Update triton.py\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Update triton.py\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Add printout and requirements check\n\n* Cleanup\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* triton fixes\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fixed triton model query over grpc\n\n* Update check_requirements('tritonclient[all]')\n\n* group imports\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix likely remote URL bug\n\n* update comment\n\n* Update is_url()\n\n* Fix 2x download attempt on http://path/to/model.pt\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\nCo-authored-by: glennjocher <glenn.jocher@ultralytics.com>\nCo-authored-by: Gaz Iqbal <giqbal@octoml.ai>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "yolov5.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def run(",
            "seen, windows, dt = 0, [], (Profile(), Profile(), Profile())",
            "for path, im, im0s, vid_cap, s in dataset:",
            "with dt[0]:",
            "-            im = torch.from_numpy(im).to(device)",
            "+            im = torch.from_numpy(im).to(model.device)",
            "im = im.half() if model.fp16 else im.float()  # uint8 to fp16/32",
            "im /= 255  # 0 - 255 to 0.0 - 1.0",
            "if len(im.shape) == 3:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=1, insert_id=1291900)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'model'), position=0, insert_id=1291901)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1291902)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=device), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 3492,
        "neg_line": [
            "-im = torch.from_numpy(im).to(device)"
        ],
        "pos_line": [
            "+im = torch.from_numpy(im).to(model.device)"
        ],
        "core_change": "-im = torch.from_numpy(im).to(device) +im = torch.from_numpy(im).to(model.device)",
        "core_API": "from_numpy"
    },
    {
        "commit_hash": "3dda5ac9fcf390d8b83eb855249360711707c11c",
        "index": "d3a976ff..e7fd2c18 100644",
        "commit_message": "for discussion: incorporate black code formatter (#3308)\n\n* setup files\n\n* run black\n\n* undo\n\n* update CONTRIBUTING.md\n\n* fix quotes in test_other_modules\n\n* make flake8 happy\n\n* set black to 100 characters per line\n\n* move type: ignore to where mypy wants them\n\n* more flake8\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Scheduler:",
            "\"\"\"",
            "Returns the state of the scheduler as a ``dict``.",
            "\"\"\"",
            "-        return {key: value for key, value in self.__dict__.items() if key != 'optimizer'}",
            "+        return {key: value for key, value in self.__dict__.items() if key != \"optimizer\"}",
            "",
            "def load_state_dict(self, state_dict: Dict[str, Any]) -> None:",
            "\"\"\""
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text='optimizer'), value='\"optimizer\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 3493,
        "neg_line": [
            "-return {key: value for key, value in self.__dict__.items() if key != 'optimizer'}"
        ],
        "pos_line": [
            "+return {key: value for key, value in self.__dict__.items() if key != \"optimizer\"}"
        ],
        "core_change": "-return {key: value for key, value in self.__dict__.items() if key != 'optimizer'} +return {key: value for key, value in self.__dict__.items() if key != \"optimizer\"}",
        "core_API": "items"
    },
    {
        "commit_hash": "a9fc87029ce450ea851a5fa3f810962684632d67",
        "index": "4f2e643e..43b34c61 100644",
        "commit_message": "fixed trpo\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class PGModel(Model):",
            "self.dist = self.policy.get_distribution()",
            "",
            "self.baseline_value_function = LinearValueFunction()",
            "-        self.saver = tf.train.Saver()",
            "+       # self.saver = tf.train.Saver()",
            "",
            "def get_action(self, state, episode=1):",
            "\"\"\""
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=saver))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=train))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=Saver))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 18,
        "number": 3496,
        "neg_line": [
            "-self.saver = tf.train.Saver()"
        ],
        "pos_line": [
            "+# self.saver = tf.train.Saver()"
        ],
        "core_change": "-self.saver = tf.train.Saver() +# self.saver = tf.train.Saver()",
        "core_API": "get_distribution"
    },
    {
        "commit_hash": "ade4956483b34a295a0555886ee853f12998f8ff",
        "index": "4bc11c47..b65f7946 100644",
        "commit_message": "breaking changes in independent act mode, other fixes\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class StatefulLayer(TemporalLayer):",
            "",
            "# def tf_apply(self, x, **internals):",
            "",
            "-    #     # optimization = tf.math.logical_not(x=Module.retrieve_tensor(name='optimization'))",
            "+    #     # optimization = tf.math.logical_not(x=Module.retrieve_tensor(name='???'))",
            "",
            "#     # def true_fn():",
            "#     batch_size = tf.shape("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 3499,
        "neg_line": [
            "-#     # optimization = tf.math.logical_not(x=Module.retrieve_tensor(name='optimization'))"
        ],
        "pos_line": [
            "+#     # optimization = tf.math.logical_not(x=Module.retrieve_tensor(name='???'))"
        ],
        "core_change": "-#     # optimization = tf.math.logical_not(x=Module.retrieve_tensor(name='optimization')) +#     # optimization = tf.math.logical_not(x=Module.retrieve_tensor(name='???'))",
        "core_API": "logical_not"
    },
    {
        "commit_hash": "ed63b7ee9e68ba120893aaaaca0d4525a56ed484",
        "index": "c0da1166..1b157d12 100644",
        "commit_message": "upgrade to pytorch 0.4.0 (#1126)\n\n* bump pytorch to 0.4 + fix sanitize\n\n* remove check for Variable in block_orthogonal\n\n* rename parameter split_size=\n\n* remove checks for Variable\n\n* fix more tests\n\n* fixes\n\n* get tests to pass\n\n* fix warnings\n\n* get rid of some of the Variables\n\n* more tests passing\n\n* more elimination of variables\n\n* finish removing all Variables\n\n* pylint and such\n\n* a few fixes\n\n* move torch.no_grad into model.forward_on_instances\n\n* more pytorch 0.4 changes\n\n* detach() -> data\n\n* pylint\n\n* fix bad tensor creation\n\n* fix types\n\n* remove print statement\n\n* more 0.4 goodness\n\n* factor out is_tensor\n\n* add no_grad to elmo command\n\n* cleanup\n\n* remove TODO\n\n* address PR feedback\n\n* replace all() with item()\n\n* more cleanup\n\n* further cleanup\n\n* remove Variable\n\n* really fix merge conflict\n\n* fix pylint\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestDotProductSimilarityFunction(AllenNlpTestCase):",
            "a_vectors = numpy.random.rand(5, 4, 3, 6, 7)",
            "b_vectors = numpy.random.rand(5, 4, 3, 6, 7)",
            "desired_result = numpy.sum(a_vectors * b_vectors, axis=-1)",
            "-        result = dot_product(Variable(torch.from_numpy(a_vectors)),",
            "-                             Variable(torch.from_numpy(b_vectors))).data.numpy()",
            "+        result = dot_product(torch.from_numpy(a_vectors),",
            "+                             torch.from_numpy(b_vectors)).data.numpy()",
            "assert result.shape == (5, 4, 3, 6)",
            "# We're cutting this down here with a random partial index, so that if this test fails the",
            "# output isn't so huge and slow."
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=call), position=3)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=), text=)), position=4)",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=identifier, text=Variable))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=), text=)))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 9,
        "number": 3500,
        "neg_line": [
            "-result = dot_product(Variable(torch.from_numpy(a_vectors)),",
            "-Variable(torch.from_numpy(b_vectors))).data.numpy()"
        ],
        "pos_line": [
            "+result = dot_product(torch.from_numpy(a_vectors),",
            "+torch.from_numpy(b_vectors)).data.numpy()"
        ],
        "core_change": "-result = dot_product(Variable(torch.from_numpy(a_vectors)), -Variable(torch.from_numpy(b_vectors))).data.numpy() +result = dot_product(torch.from_numpy(a_vectors), +torch.from_numpy(b_vectors)).data.numpy()",
        "core_API": "rand"
    },
    {
        "commit_hash": "510c8506512b8e9873ffbab0abdc1a668b022022",
        "index": "d0fadd0bf1..11f6b17f22 100644",
        "commit_message": "[RLlib] SAC add discrete action support. (#7320)\n\n* Exploration API (+EpsilonGreedy sub-class).\n\n* Exploration API (+EpsilonGreedy sub-class).\n\n* Cleanup/LINT.\n\n* Add `deterministic` to generic Trainer config (NOTE: this is still ignored by most Agents).\n\n* Add `error` option to deprecation_warning().\n\n* WIP.\n\n* Bug fix: Get exploration-info for tf framework.\nBug fix: Properly deprecate some DQN config keys.\n\n* WIP.\n\n* LINT.\n\n* WIP.\n\n* Split PerWorkerEpsilonGreedy out of EpsilonGreedy.\nDocstrings.\n\n* Fix bug in sampler.py in case Policy has self.exploration = None\n\n* Update rllib/agents/dqn/dqn.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* WIP.\n\n* Update rllib/agents/trainer.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* WIP.\n\n* Change requests.\n\n* LINT\n\n* In tune/utils/util.py::deep_update() Only keep deep_updat'ing if both original and value are dicts. If value is not a dict, set\n\n* Completely obsolete syn_replay_optimizer.py's parameters schedule_max_timesteps AND beta_annealing_fraction (replaced with prioritized_replay_beta_annealing_timesteps).\n\n* Update rllib/evaluation/worker_set.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* Review fixes.\n\n* Fix default value for DQN's exploration spec.\n\n* LINT\n\n* Fix recursion bug (wrong parent c'tor).\n\n* Do not pass timestep to get_exploration_info.\n\n* Update tf_policy.py\n\n* Fix some remaining issues with test cases and remove more deprecated DQN/APEX exploration configs.\n\n* Bug fix tf-action-dist\n\n* DDPG incompatibility bug fix with new DQN exploration handling (which is imported by DDPG).\n\n* Switch off exploration when getting action probs from off-policy-estimator's policy.\n\n* LINT\n\n* Fix test_checkpoint_restore.py.\n\n* Deprecate all SAC exploration (unused) configs.\n\n* Properly use `model.last_output()` everywhere. Instead of `model._last_output`.\n\n* WIP.\n\n* Take out set_epsilon from multi-agent-env test (not needed, decays anyway).\n\n* WIP.\n\n* Trigger re-test (flaky checkpoint-restore test).\n\n* WIP.\n\n* WIP.\n\n* Add test case for deterministic action sampling in PPO.\n\n* bug fix.\n\n* Added deterministic test cases for different Agents.\n\n* Fix problem with TupleActions in dynamic-tf-policy.\n\n* Separate supported_spaces tests so they can be run separately for easier debugging.\n\n* LINT.\n\n* Fix autoregressive_action_dist.py test case.\n\n* Re-test.\n\n* Fix.\n\n* Remove duplicate py_test rule from bazel.\n\n* LINT.\n\n* WIP.\n\n* WIP.\n\n* SAC fix.\n\n* SAC fix.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* FIX 2 examples tests.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* Fix.\n\n* LINT.\n\n* Renamed test file.\n\n* WIP.\n\n* Add unittest.main.\n\n* Make action_dist_class mandatory.\n\n* fix\n\n* FIX.\n\n* WIP.\n\n* WIP.\n\n* Fix.\n\n* Fix.\n\n* Fix explorations test case (contextlib cannot find its own nullcontext??).\n\n* Force torch to be installed for QMIX.\n\n* LINT.\n\n* Fix determine_tests_to_run.py.\n\n* Fix determine_tests_to_run.py.\n\n* WIP\n\n* Add Random exploration component to tests (fixed issue with \"static-graph randomness\" via py_function).\n\n* Add Random exploration component to tests (fixed issue with \"static-graph randomness\" via py_function).\n\n* Rename some stuff.\n\n* Rename some stuff.\n\n* WIP.\n\n* update.\n\n* WIP.\n\n* Gumbel Softmax Dist.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP\n\n* WIP.\n\n* WIP.\n\n* Hypertune.\n\n* Hypertune.\n\n* Hypertune.\n\n* Lock-in.\n\n* Cleanup.\n\n* LINT.\n\n* Fix.\n\n* Update rllib/policy/eager_tf_policy.py\n\nCo-Authored-By: Kristian Hartikainen <kristian.hartikainen@gmail.com>\n\n* Update rllib/agents/sac/sac_policy.py\n\nCo-Authored-By: Kristian Hartikainen <kristian.hartikainen@gmail.com>\n\n* Update rllib/agents/sac/sac_policy.py\n\nCo-Authored-By: Kristian Hartikainen <kristian.hartikainen@gmail.com>\n\n* Update rllib/models/tf/tf_action_dist.py\n\nCo-Authored-By: Kristian Hartikainen <kristian.hartikainen@gmail.com>\n\n* Update rllib/models/tf/tf_action_dist.py\n\nCo-Authored-By: Kristian Hartikainen <kristian.hartikainen@gmail.com>\n\n* Fix items from review comments.\n\n* Add dm_tree to RLlib dependencies.\n\n* Add dm_tree to RLlib dependencies.\n\n* Fix DQN test cases ((Torch)Categorical).\n\n* Fix wrong pip install.\n\nCo-authored-by: Eric Liang <ekhliang@gmail.com>\nCo-authored-by: Kristian Hartikainen <kristian.hartikainen@gmail.com>\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFPolicy(Policy):",
            "",
            "# build output signatures",
            "output_signature = self._extra_output_signature_def()",
            "-        for i, a in enumerate(tf.nest.flatten(self._sampled_action)):",
            "+        for i, a in enumerate(tree.flatten(self._sampled_action)):",
            "output_signature[\"actions_{}\".format(i)] = \\",
            "tf.saved_model.utils.build_tensor_info(a)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tf), value='tree')",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=tf), position=0)",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Delete(target_node=ASTNode(type=identifier, text=nest))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 3505,
        "neg_line": [
            "-for i, a in enumerate(tf.nest.flatten(self._sampled_action)):"
        ],
        "pos_line": [
            "+for i, a in enumerate(tree.flatten(self._sampled_action)):"
        ],
        "core_change": "-for i, a in enumerate(tf.nest.flatten(self._sampled_action)): +for i, a in enumerate(tree.flatten(self._sampled_action)):",
        "core_API": "_extra_output_signature_def"
    },
    {
        "commit_hash": "f89bd4832df2d41df8b776a8055d33ee37d78f87",
        "index": "99ce9b82..0aa387f4 100644",
        "commit_message": "minor mwt fixes\n\n",
        "file": "stanza.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DictTrainer(BaseTrainer):",
            "",
            "def load(self, filename):",
            "try:",
            "-            checkpoint = torch.load(filename)",
            "+            checkpoint = torch.load(filename, lambda storage, loc: storage)",
            "except BaseException:",
            "print(\"Cannot load model from {}\".format(filename))",
            "exit()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1151036)",
            "Insert(target_node=ASTNode(type=argument_list), node=('lambda', None), position=3, insert_id=1151037)",
            "Insert(target_node=IN(type=lambda), node=('lambda', 'lambda'), position=0, insert_id=1151038)",
            "Insert(target_node=IN(type=lambda), node=('lambda_parameters', None), position=1, insert_id=1151039)",
            "Insert(target_node=IN(type=lambda), node=(':', ':'), position=2, insert_id=1151040)",
            "Insert(target_node=IN(type=lambda), node=('identifier', 'storage'), position=3, insert_id=1151041)",
            "Insert(target_node=IN(type=lambda_parameters), node=('identifier', 'storage'), position=0, insert_id=1151042)",
            "Insert(target_node=IN(type=lambda_parameters), node=(',', ','), position=1, insert_id=1151043)",
            "Insert(target_node=IN(type=lambda_parameters), node=('identifier', 'loc'), position=2, insert_id=1151044)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 3506,
        "neg_line": [
            "-checkpoint = torch.load(filename)"
        ],
        "pos_line": [
            "+checkpoint = torch.load(filename, lambda storage, loc: storage)"
        ],
        "core_change": "-checkpoint = torch.load(filename) +checkpoint = torch.load(filename, lambda storage, loc: storage)",
        "core_API": "load"
    },
    {
        "commit_hash": "f38aaa5be05104ddfc3452249eeb521f139ded99",
        "index": "15281c72..1652b701 100755",
        "commit_message": "improved global tensor handling, various other fixes\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class MultiStep(MetaOptimizer):",
            "deltas = [delta1 + delta2 for delta1, delta2 in zip(deltas, step_deltas)]",
            "return deltas",
            "",
            "-            deltas = tf.while_loop(",
            "+            deltas = self.while_loop(",
            "cond=util.tf_always_true, body=body, loop_vars=(deltas,),",
            "maximum_iterations=(self.num_steps - 1)",
            ")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tf), value='self')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 3507,
        "neg_line": [
            "-deltas = tf.while_loop("
        ],
        "pos_line": [
            "+deltas = self.while_loop("
        ],
        "core_change": "-deltas = tf.while_loop( +deltas = self.while_loop(",
        "core_API": "while_loop"
    },
    {
        "commit_hash": "754148e80c26d7c45abd26ef6c49b84139d5053a",
        "index": "5c4a7670..7a22d62b 100644",
        "commit_message": "Fix minor errors in european_option pricing module for heston models.\n\nPiperOrigin-RevId: 406103349\n\n",
        "file": "tf-quant-finance.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def european_option_price(",
            "0.0, dtype=dtype, name='discount_rates')",
            "",
            "if dividend_rates is None:",
            "-      dividend_rates = tf.convert_to_tensor(",
            "-          0.0, dtype=dtype, name='dividend_rates')",
            "+      dividend_rates = 0.0",
            "+    dividend_rates = tf.convert_to_tensor(",
            "+        dividend_rates, dtype=dtype, name='dividend_rates')",
            "",
            "if discount_factors is None:",
            "discount_factors = tf.exp(-discount_rates * expiries)  # pylint: disable=invalid-unary-operand-type"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=ASTNode(type=module), node=ASTNode(type=expression_statement), position=1)",
            "Insert(target_node=ASTNode(type=block), node=('expression_statement', None), position=0, insert_id=2336113)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=2336114)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'dividend_rates'), position=0, insert_id=2336115)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=2336116)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=float, text=0.0), position=2)",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'dividend_rates'), position=1, insert_id=2336117)"
        ],
        "plus_line": 3,
        "minus_line": 2,
        "AST_diff_line": 7,
        "number": 3508,
        "neg_line": [
            "-dividend_rates = tf.convert_to_tensor(",
            "-0.0, dtype=dtype, name='dividend_rates')"
        ],
        "pos_line": [
            "+dividend_rates = 0.0",
            "+dividend_rates = tf.convert_to_tensor(",
            "+dividend_rates, dtype=dtype, name='dividend_rates')"
        ],
        "core_change": "-dividend_rates = tf.convert_to_tensor( -0.0, dtype=dtype, name='dividend_rates') +dividend_rates = 0.0 +dividend_rates = tf.convert_to_tensor( +dividend_rates, dtype=dtype, name='dividend_rates')",
        "core_API": "convert_to_tensor"
    },
    {
        "commit_hash": "62e3d1dbdc88af9ed8afb37f7e8ecc011f5486a3",
        "index": "6532c617..d0a1c12a 100644",
        "commit_message": "Scope TokenIndexer output by indexer name (#3597)\n\n* Indexer tests are now passing, at least\n\n* Fixed some masking issues, and padding keys for one config file\n\n* TextFieldEmbedder tests pass\n\n* Fixing fixtures, some more tests passing\n\n* Fixed weird ordering bug\n\n* All TokenEmbedder tests passing?\n\n* Update fixtures\n\n* Fix more hard-coded references\n\n* fix field tests\n\n* fix dataset reader tests\n\n* Fix iterator tests\n\n* More tests passing\n\n* fix hotflip and some other tests\n\n* more tests\n\n* more test fixes\n\n* more tests\n\n* most tests passing; I think the remaining ones are spacy model changes\n\n* hard-code POS tag test\n\n* last test, I think\n\n* black\n\n* updated black\n\n* flake8\n\n* mypy\n\n* black again\n\n* fix training configs\n\n* remove reference to embedder_to_indexer_map\n\n* Other fixes from PR comments\n\n* fix breakage from incorrect merge during rebase\n\n* flake, some docstring formatting\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ElmoTokenEmbedder(TokenEmbedder):",
            "The ELMo representations for the input sequence, shape",
            "``(batch_size, timesteps, embedding_dim)``",
            "\"\"\"",
            "-        elmo_output = self._elmo(inputs, word_inputs)",
            "+        elmo_output = self._elmo(tokens, word_inputs)",
            "elmo_representations = elmo_output[\"elmo_representations\"][0]",
            "if self._projection:",
            "projection = self._projection"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 3509,
        "neg_line": [
            "-elmo_output = self._elmo(inputs, word_inputs)"
        ],
        "pos_line": [
            "+elmo_output = self._elmo(tokens, word_inputs)"
        ],
        "core_change": "-elmo_output = self._elmo(inputs, word_inputs) +elmo_output = self._elmo(tokens, word_inputs)",
        "core_API": "_elmo"
    },
    {
        "commit_hash": "504d63d25cfd42e650834d8db6d976103e5f4baa",
        "index": "715aedd..2cf733b 100644",
        "commit_message": "Develop (#171)\n\n* merge (#128)\n\n* 0.2.6 (#126)\n\n* 0.2.5 setup.py (#111)\n\n* [WIP] Attempts to Fix Memory Error  (#112)\n\n* Add Website Badge in README.md, apply timeout in search function in search.py\n\n* Add timeout in maximize_acq function in search.py\n\n* Update unit test to allow timeout to raise TimeoutError\n\n* Add unit test for timeout resume\n\n* Remove TimeoutError from expectation\n\n* Check Timeout exception in search() in search.py\n\n* 0.2.5 setup.py (#110)\n\n* Prevent gpu memory copy to main process after train() finished\n\n* Cast loss from tensor to float\n\n* Add pass() in MockProcess\n\n* [MRG] Search Space limited to avoid out of memory (#121)\n\n* limited the search space\n\n* limited the search space\n\n* reduce search space\n\n* test added\n\n* [MRG]Pytorch mp (#124)\n\n* Change multiprcoessing to torch.multiprocessing\n\n* Replace multiprocessing.Pool with torch.multiprocessing.Pool in tests\n\n* 0.2.6 (#125)\n\n* new release\n\n* auto deploy\n\n* auto deploy of docs\n\n* fix the docs auto deploy\n\n* Create CNAME\n\n* deploy docs fixed\n\n* update\n\n* bug fix (#127)\n\n* setup.py\n\n* rm print\n\n* Issue#37 and Issue #79 Save keras model/autokeras model (#122)\n\n* Issue #37 Export Keras model\n\n* Issue #79 Save autokeras model\n\n* Issue #37 and Issue#79 Fixed comments\n\n* Issue #37 and Issue #79\n\n* Issue #37 and Issue #79\n\n* Issue #37 and Issue #79 Fixed pytests\n\n* Issue #37 and Issue #79\n\n* quick fix test\n\n* Progbar (#143)\n\n* 0.2.6 (#126)\n\n* 0.2.5 setup.py (#111)\n\n* [WIP] Attempts to Fix Memory Error  (#112)\n\n* Add Website Badge in README.md, apply timeout in search function in search.py\n\n* Add timeout in maximize_acq function in search.py\n\n* Update unit test to allow timeout to raise TimeoutError\n\n* Add unit test for timeout resume\n\n* Remove TimeoutError from expectation\n\n* Check Timeout exception in search() in search.py\n\n* 0.2.5 setup.py (#110)\n\n* Prevent gpu memory copy to main process after train() finished\n\n* Cast loss from tensor to float\n\n* Add pass() in MockProcess\n\n* [MRG] Search Space limited to avoid out of memory (#121)\n\n* limited the search space\n\n* limited the search space\n\n* reduce search space\n\n* test added\n\n* [MRG]Pytorch mp (#124)\n\n* Change multiprcoessing to torch.multiprocessing\n\n* Replace multiprocessing.Pool with torch.multiprocessing.Pool in tests\n\n* 0.2.6 (#125)\n\n* new release\n\n* auto deploy\n\n* auto deploy of docs\n\n* fix the docs auto deploy\n\n* Create CNAME\n\n* deploy docs fixed\n\n* update\n\n* bug fix (#127)\n\n* setup.py\n\n* contribute guide\n\n* Add Progress Bar\n\n* Update utils.py\n\n* Update search.py\n\n* update constant (#145)\n\n* [WIP] Issue #158 Imageregressor (#159)\n\n* Develop (#146)\n\n* merge (#128)\n\n* 0.2.6 (#126)\n\n* 0.2.5 setup.py (#111)\n\n* [WIP] Attempts to Fix Memory Error  (#112)\n\n* Add Website Badge in README.md, apply timeout in search function in search.py\n\n* Add timeout in maximize_acq function in search.py\n\n* Update unit test to allow timeout to raise TimeoutError\n\n* Add unit test for timeout resume\n\n* Remove TimeoutError from expectation\n\n* Check Timeout exception in search() in search.py\n\n* 0.2.5 setup.py (#110)\n\n* Prevent gpu memory copy to main process after train() finished\n\n* Cast loss from tensor to float\n\n* Add pass() in MockProcess\n\n* [MRG] Search Space limited to avoid out of memory (#121)\n\n* limited the search space\n\n* limited the search space\n\n* reduce search space\n\n* test added\n\n* [MRG]Pytorch mp (#124)\n\n* Change multiprcoessing to torch.multiprocessing\n\n* Replace multiprocessing.Pool with torch.multiprocessing.Pool in tests\n\n* 0.2.6 (#125)\n\n* new release\n\n* auto deploy\n\n* auto deploy of docs\n\n* fix the docs auto deploy\n\n* Create CNAME\n\n* deploy docs fixed\n\n* update\n\n* bug fix (#127)\n\n* setup.py\n\n* rm print\n\n* Issue#37 and Issue #79 Save keras model/autokeras model (#122)\n\n* Issue #37 Export Keras model\n\n* Issue #79 Save autokeras model\n\n* Issue #37 and Issue#79 Fixed comments\n\n* Issue #37 and Issue #79\n\n* Issue #37 and Issue #79\n\n* Issue #37 and Issue #79 Fixed pytests\n\n* Issue #37 and Issue #79\n\n* quick fix test\n\n* Progbar (#143)\n\n* 0.2.6 (#126)\n\n* 0.2.5 setup.py (#111)\n\n* [WIP] Attempts to Fix Memory Error  (#112)\n\n* Add Website Badge in README.md, apply timeout in search function in search.py\n\n* Add timeout in maximize_acq function in search.py\n\n* Update unit test to allow timeout to raise TimeoutError\n\n* Add unit test for timeout resume\n\n* Remove TimeoutError from expectation\n\n* Check Timeout exception in search() in search.py\n\n* 0.2.5 setup.py (#110)\n\n* Prevent gpu memory copy to main process after train() finished\n\n* Cast loss from tensor to float\n\n* Add pass() in MockProcess\n\n* [MRG] Search Space limited to avoid out of memory (#121)\n\n* limited the search space\n\n* limited the search space\n\n* reduce search space\n\n* test added\n\n* [MRG]Pytorch mp (#124)\n\n* Change multiprcoessing to torch.multiprocessing\n\n* Replace multiprocessing.Pool with torch.multiprocessing.Pool in tests\n\n* 0.2.6 (#125)\n\n* new release\n\n* auto deploy\n\n* auto deploy of docs\n\n* fix the docs auto deploy\n\n* Create CNAME\n\n* deploy docs fixed\n\n* update\n\n* bug fix (#127)\n\n* setup.py\n\n* contribute guide\n\n* Add Progress Bar\n\n* Update utils.py\n\n* Update search.py\n\n* update constant (#145)\n\n* Update setup.py (#147)\n\n* Update setup.py\n\n* Update setup.py\n\n* Update setup.py (#155)\n\n* requirements\n\n* Issue #158 Export ImageRegressor model\n\n* Memory (#161)\n\n* aa\n\n* limit memory\n\n* refactor to_real_layer to member functions\n\n* bug fix (#166)\n\n* doc string changed for augment (#170)\n\nI added proper documentation for class ImageSupervised  arg 'augment'. It is 'None' by default. However, if it is 'None', then it uses Constant.DATA_AUGMENTATION which is 'True'. This is misleading when trying things out.\n\n* Update constant.py\n\n",
        "file": "autokeras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "layers",
        "change": [
            "class TorchModel(torch.nn.Module):",
            "self.graph = graph",
            "self.layers = []",
            "for layer in graph.layer_list:",
            "-            self.layers.append(to_real_layer(layer))",
            "+            self.layers.append(layer.to_real_layer())",
            "if graph.weighted:",
            "for index, layer in enumerate(self.layers):",
            "set_stub_weight_to_torch(self.graph.layer_list[index], layer)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2561852)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=layer), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2561853)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=to_real_layer), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 3511,
        "neg_line": [
            "-self.layers.append(to_real_layer(layer))"
        ],
        "pos_line": [
            "+self.layers.append(layer.to_real_layer())"
        ],
        "core_change": "-self.layers.append(to_real_layer(layer)) +self.layers.append(layer.to_real_layer())",
        "core_API": "append"
    },
    {
        "commit_hash": "0c166dcfb8b9e9d79d013b7967a59dfb1ac4b804",
        "index": "e6628761..b45b37e4 100644",
        "commit_message": "define and apply YAPF formatting  (#1039)\n\n* define YAPF\n\n* yapf kornia -rip\n\n* yapf examples -rip\n\n* yapf test -rip\n\n* Update CONTRIBUTING.rst\n\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\n\n* update config\n\n* fix 4lint\n\n* mypy\n\n* mypy\n\n* mypy\n\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def psnr(input: torch.Tensor, target: torch.Tensor, max_val: float) -> torch.Ten",
            "if input.shape != target.shape:",
            "raise TypeError(f\"Expected tensors of equal shapes, but got {input.shape} and {target.shape}\")",
            "",
            "-    return 10. * torch.log10(max_val ** 2 / mse(input, target, reduction='mean'))",
            "+    return 10. * torch.log10(max_val**2 / mse(input, target, reduction='mean'))",
            "",
            "",
            "def psnr_loss(input: torch.Tensor, target: torch.Tensor, max_val: float) -> torch.Tensor:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 3512,
        "neg_line": [
            "-return 10. * torch.log10(max_val ** 2 / mse(input, target, reduction='mean'))"
        ],
        "pos_line": [
            "+return 10. * torch.log10(max_val**2 / mse(input, target, reduction='mean'))"
        ],
        "core_change": "-return 10. * torch.log10(max_val ** 2 / mse(input, target, reduction='mean')) +return 10. * torch.log10(max_val**2 / mse(input, target, reduction='mean'))",
        "core_API": "log10"
    },
    {
        "commit_hash": "3a27ba3d18b9691926b296b0a6a483313b0299ba",
        "index": "567e08d36..9ede3cabb 100644",
        "commit_message": "Fix opt softmax small nit (#19243)\n\n* fix opt softmax nit\n\n- Use the same logic as 1eb09537550734a783c194e416029cb9bc4cb119 for consistency\n\n* Update src/transformers/models/opt/modeling_opt.py\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "functional",
        "change": [
            "class OPTAttention(nn.Module):",
            "attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask",
            "attn_weights = torch.max(attn_weights, torch.tensor(torch.finfo(attn_weights.dtype).min))",
            "attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)",
            "-            dtype_attn_weights = attn_weights.dtype",
            "",
            "# upcast to fp32 if the weights are in fp16. Please see https://github.com/huggingface/transformers/pull/17437",
            "-        if dtype_attn_weights == torch.float16:",
            "-            attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(dtype_attn_weights)",
            "+        if attn_weights.dtype == torch.float16:",
            "+            attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(torch.float16)",
            "else:",
            "attn_weights = nn.functional.softmax(attn_weights, dim=-1)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=comparison_operator), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=ASTNode(type=comparison_operator), node=('attribute', None), position=3, insert_id=1754695)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1754696)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1754697)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'float16'), position=2, insert_id=1754698)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=attribute), position=1)",
            "Delete(target_node=ASTNode(type=identifier, text=dtype_attn_weights))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=identifier, text=dtype_attn_weights))",
            "Delete(target_node=ASTNode(type=identifier, text=dtype_attn_weights))"
        ],
        "plus_line": 2,
        "minus_line": 3,
        "AST_diff_line": 12,
        "number": 3513,
        "neg_line": [
            "-dtype_attn_weights = attn_weights.dtype",
            "-if dtype_attn_weights == torch.float16:",
            "-attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(dtype_attn_weights)"
        ],
        "pos_line": [
            "+if attn_weights.dtype == torch.float16:",
            "+attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(torch.float16)"
        ],
        "core_change": "-dtype_attn_weights = attn_weights.dtype -if dtype_attn_weights == torch.float16: -attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(dtype_attn_weights) +if attn_weights.dtype == torch.float16: +attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(torch.float16)",
        "core_API": "view"
    },
    {
        "commit_hash": "95b6d985ffb231d9a2047c039c95660913063b9d",
        "index": "01f69524..4becbfef 100644",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SpanBasedF1Measure(Metric):",
            "possible roles associated with it).",
            "\"\"\"",
            "if mask is None:",
            "-            mask = torch.ones_like(gold_labels)",
            "+            mask = torch.ones_like(gold_labels).bool()",
            "",
            "predictions, gold_labels, mask, prediction_map = self.detach_tensors(",
            "predictions, gold_labels, mask, prediction_map"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=20236)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=20237)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=20238)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'bool'), position=2, insert_id=20239)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=20240)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=20241)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 3514,
        "neg_line": [
            "-mask = torch.ones_like(gold_labels)"
        ],
        "pos_line": [
            "+mask = torch.ones_like(gold_labels).bool()"
        ],
        "core_change": "-mask = torch.ones_like(gold_labels) +mask = torch.ones_like(gold_labels).bool()",
        "core_API": "ones_like"
    },
    {
        "commit_hash": "b7fdbdf18c9170ae01dd7ecfdc8ac0d9596cb6da",
        "index": "e69ff88b..aee5d68d 100644",
        "commit_message": "JAX: Fix RNNScratch b_h param\n\n",
        "file": "d2l-en.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class RNNScratch(nn.Module):",
            "(self.num_inputs, self.num_hiddens))",
            "self.W_hh = self.param('W_hh', nn.initializers.normal(self.sigma),",
            "(self.num_hiddens, self.num_hiddens))",
            "-        self.b_h = self.param('b_h', nn.initializers.zeros, (num_hiddens))",
            "+        self.b_h = self.param('b_h', nn.initializers.zeros, (self.num_hiddens))",
            "",
            "def __call__(self, inputs, state=None):",
            "\"\"\"Defined in :numref:`sec_rnn-scratch`\"\"\""
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=parenthesized_expression), node=('attribute', None), position=1, insert_id=1319645)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=1319646)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1319647)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=num_hiddens), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 3519,
        "neg_line": [
            "-self.b_h = self.param('b_h', nn.initializers.zeros, (num_hiddens))"
        ],
        "pos_line": [
            "+self.b_h = self.param('b_h', nn.initializers.zeros, (self.num_hiddens))"
        ],
        "core_change": "-self.b_h = self.param('b_h', nn.initializers.zeros, (num_hiddens)) +self.b_h = self.param('b_h', nn.initializers.zeros, (self.num_hiddens))",
        "core_API": "param"
    },
    {
        "commit_hash": "9724581249baa695df07829ff25be361f3456966",
        "index": "bba0972735..f60dca6824 100644",
        "commit_message": "lintfixbot: Auto-commit fixed lint errors in codebase\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "eigvals.support_native_out = False",
            "",
            "",
            "def adjoint(",
            "-        x: torch.Tensor,",
            "-        /,",
            "-        *,",
            "-        out: Optional[torch.Tensor] = None,",
            "+    x: torch.Tensor,",
            "+    /,",
            "+    *,",
            "+    out: Optional[torch.Tensor] = None,",
            ") -> torch.Tensor:",
            "_check_valid_dimension_size(x)",
            "return torch.adjoint(x).resolve_conj()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 4,
        "minus_line": 4,
        "AST_diff_line": 17,
        "number": 3521,
        "neg_line": [
            "-x: torch.Tensor,",
            "-/,",
            "-*,",
            "-out: Optional[torch.Tensor] = None,"
        ],
        "pos_line": [
            "+x: torch.Tensor,",
            "+/,",
            "+*,",
            "+out: Optional[torch.Tensor] = None,"
        ],
        "core_change": "-x: torch.Tensor, -/, -*, -out: Optional[torch.Tensor] = None, +x: torch.Tensor, +/, +*, +out: Optional[torch.Tensor] = None,",
        "core_API": "adjoint"
    },
    {
        "commit_hash": "e553aa176c20b81a52bf8ce4a6a67b7729f68876",
        "index": "51e291fecb..ad523e95b1 100644",
        "commit_message": "Fixed vsplit in the experimental API (#10210)\n\nRemoved out argument as the function returns a list of arrays\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def vsplit(",
            "ary: torch.Tensor,",
            "indices_or_sections: Union[int, Tuple[int]],",
            "/,",
            "-    *,",
            "-    out: Optional[torch.Tensor] = None,",
            "-) -> torch.Tensor:",
            "+) -> List[torch.Tensor]:",
            "return torch.vsplit(ary, indices_or_sections)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=function_definition), node=('->', '->'), position=3, insert_id=269591)",
            "Move(target_node=ASTNode(type=function_definition), node=ASTNode(type=type), position=4)",
            "Update(target_node=ASTNode(type=identifier, text=Optional), value='List')",
            "Delete(target_node=ASTNode(type=*, text=*))",
            "Delete(target_node=ASTNode(type=keyword_separator))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=out))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=none, text=None))",
            "Delete(target_node=ASTNode(type=typed_default_parameter))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=->, text=->))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=Tensor))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=type))"
        ],
        "plus_line": 0,
        "minus_line": 2,
        "AST_diff_line": 18,
        "number": 3523,
        "neg_line": [
            "-*,",
            "-out: Optional[torch.Tensor] = None,",
            "-) -> torch.Tensor:"
        ],
        "pos_line": [
            "+) -> List[torch.Tensor]:"
        ],
        "core_change": "-*, -out: Optional[torch.Tensor] = None, -) -> torch.Tensor: +) -> List[torch.Tensor]:",
        "core_API": "vsplit"
    },
    {
        "commit_hash": "9385ff85bdb7882265b3e345b84f301712dac4b7",
        "index": "e7b0b12a..ab99e211 100644",
        "commit_message": "Fix the is_tensor check in Keras remove_squeezable_dimensions.\n\ntf.is_tensor only returns true for Tensor and native TF CompositeTensors, like RaggedTensor and SparseTensor. For user custom CompositeTensors, it returns false. Change the if condition to check both native TF types and extension types.\n\nPiperOrigin-RevId: 427492662\n\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def remove_squeezable_dimensions(",
            "Tuple of `labels` and `predictions`, possibly with last dim squeezed.",
            "\"\"\"",
            "with backend.name_scope(name or 'remove_squeezable_dimensions'):",
            "-    # tf.is_tensor returns True if predictions is a tensor or composite tensor.",
            "-    if not tf.is_tensor(predictions):",
            "+    if not tf_utils.is_tensor_or_extension_type(predictions):",
            "predictions = tf.convert_to_tensor(predictions)",
            "-    if not tf.is_tensor(labels):",
            "+    if not tf_utils.is_tensor_or_extension_type(labels):",
            "labels = tf.convert_to_tensor(labels)",
            "predictions_shape = predictions.shape",
            "predictions_rank = predictions_shape.ndims"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tf), value='tf_utils')",
            "Update(target_node=ASTNode(type=identifier, text=is_tensor), value='is_tensor_or_extension_type')",
            "Update(target_node=ASTNode(type=identifier, text=tf), value='tf_utils')",
            "Update(target_node=ASTNode(type=identifier, text=is_tensor), value='is_tensor_or_extension_type')"
        ],
        "plus_line": 2,
        "minus_line": 3,
        "AST_diff_line": 4,
        "number": 3525,
        "neg_line": [
            "-# tf.is_tensor returns True if predictions is a tensor or composite tensor.",
            "-if not tf.is_tensor(predictions):",
            "-if not tf.is_tensor(labels):"
        ],
        "pos_line": [
            "+if not tf_utils.is_tensor_or_extension_type(predictions):",
            "+if not tf_utils.is_tensor_or_extension_type(labels):"
        ],
        "core_change": "-# tf.is_tensor returns True if predictions is a tensor or composite tensor. -if not tf.is_tensor(predictions): +if not tf_utils.is_tensor_or_extension_type(predictions): -if not tf.is_tensor(labels): +if not tf_utils.is_tensor_or_extension_type(labels):",
        "core_API": "name_scope"
    },
    {
        "commit_hash": "726aba089d12503249d824bbaf4070f47d0fe44d",
        "index": "7cf1da44..1130d3d9 100644",
        "commit_message": "[Pytorch] pytorch only timesteps (#724)\n\n* pytorch timesteps\n\n* style\n\n* get rid of if-else\n\n* fix test\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ScoreSdeVpScheduler(SchedulerMixin, ConfigMixin):",
            "self.discrete_sigmas = None",
            "self.timesteps = None",
            "",
            "-    def set_timesteps(self, num_inference_steps):",
            "-        self.timesteps = torch.linspace(1, self.config.sampling_eps, num_inference_steps)",
            "+    def set_timesteps(self, num_inference_steps, device: Union[str, torch.device] = None):",
            "+        self.timesteps = torch.linspace(1, self.config.sampling_eps, num_inference_steps, device=device)",
            "",
            "def step_pred(self, score, x, t, generator=None):",
            "if self.timesteps is None:"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=4, insert_id=104110)",
            "Insert(target_node=ASTNode(type=parameters), node=('typed_default_parameter', None), position=5, insert_id=104111)",
            "Insert(target_node=IN(type=typed_default_parameter), node=('identifier', 'device'), position=0, insert_id=104112)",
            "Insert(target_node=IN(type=typed_default_parameter), node=(':', ':'), position=1, insert_id=104113)",
            "Insert(target_node=IN(type=typed_default_parameter), node=('type', None), position=2, insert_id=104114)",
            "Insert(target_node=IN(type=typed_default_parameter), node=('=', '='), position=3, insert_id=104115)",
            "Insert(target_node=IN(type=typed_default_parameter), node=('none', 'None'), position=4, insert_id=104116)",
            "Insert(target_node=IN(type=type), node=('subscript', None), position=0, insert_id=104117)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'Union'), position=0, insert_id=104118)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=104119)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'str'), position=2, insert_id=104120)",
            "Insert(target_node=IN(type=subscript), node=(',', ','), position=3, insert_id=104121)",
            "Insert(target_node=IN(type=subscript), node=('attribute', None), position=4, insert_id=104122)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=5, insert_id=104123)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=104124)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=104125)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=104126)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=6, insert_id=104127)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=7, insert_id=104128)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=104129)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=104130)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=2, insert_id=104131)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 22,
        "number": 3531,
        "neg_line": [
            "-def set_timesteps(self, num_inference_steps):",
            "-self.timesteps = torch.linspace(1, self.config.sampling_eps, num_inference_steps)"
        ],
        "pos_line": [
            "+def set_timesteps(self, num_inference_steps, device: Union[str, torch.device] = None):",
            "+self.timesteps = torch.linspace(1, self.config.sampling_eps, num_inference_steps, device=device)"
        ],
        "core_change": "-def set_timesteps(self, num_inference_steps): -self.timesteps = torch.linspace(1, self.config.sampling_eps, num_inference_steps) +def set_timesteps(self, num_inference_steps, device: Union[str, torch.device] = None): +self.timesteps = torch.linspace(1, self.config.sampling_eps, num_inference_steps, device=device)",
        "core_API": "linspace"
    },
    {
        "commit_hash": "06938835ebf47df6aacab30aa7c0eb049bc5f228",
        "index": "ccb50856..a68dc1af 100644",
        "commit_message": "Support fp32 gradaccum for bf16 model (#2566)\n\n* allow bf16 model with fp32 gradient accumulation datatype\n\n* allow fp32 gradient accumulation and bfloat16 model in amp mode\n\n* alternative fix for grad accumulation type mismatch.  In the case of zero optimizer we should have grad accum type == model data type\n\nCo-authored-by: Olatunji Ruwase <olruwase@microsoft.com>\n",
        "file": "DeepSpeed.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DeepSpeedEngine(Module):",
            "model_dtype = torch.bfloat16",
            "",
            "if self._config.grad_accum_dtype == None:",
            "-            if model_dtype == torch.bfloat16:",
            "+            if model_dtype == torch.bfloat16 and not self.zero_optimization():",
            "grad_accum_dtype = torch.float32",
            "else:",
            "grad_accum_dtype = model_dtype"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=if_statement), node=('boolean_operator', None), position=1, insert_id=76492)",
            "Move(target_node=IN(type=boolean_operator), node=ASTNode(type=comparison_operator), position=0)",
            "Insert(target_node=IN(type=boolean_operator), node=('and', 'and'), position=1, insert_id=76493)",
            "Insert(target_node=IN(type=boolean_operator), node=('not_operator', None), position=2, insert_id=76494)",
            "Insert(target_node=IN(type=not_operator), node=('not', 'not'), position=0, insert_id=76495)",
            "Insert(target_node=IN(type=not_operator), node=('call', None), position=1, insert_id=76496)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=76497)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=76498)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=76499)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=76500)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'zero_optimization'), position=2, insert_id=76501)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=76502)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=76503)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 13,
        "number": 3533,
        "neg_line": [
            "-if model_dtype == torch.bfloat16:"
        ],
        "pos_line": [
            "+if model_dtype == torch.bfloat16 and not self.zero_optimization():"
        ],
        "core_change": "-if model_dtype == torch.bfloat16: +if model_dtype == torch.bfloat16 and not self.zero_optimization():",
        "core_API": "zero_optimization"
    },
    {
        "commit_hash": "6a683c74b9d0fbd417f370d9165ecb7dd93cdac9",
        "index": "1f132864b..6d160e883 100644",
        "commit_message": "docs: minor refactoring and docs for MorphoTagger\n\n* docs: add docs for MorphoTagger, minor refactoring\n\n* docs: morpho_tagger docstrings fixed\n\n* docs: fix morpho_tagger docstrings\n\n* docs: update morpho_tagger docstrings\n\n",
        "file": "DeepPavlov.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def csoftmax_for_slice(input):",
            "p = q_list[1] * (1.0 - mass_) / tf.reduce_sum(q_list[1])",
            "p_new = tf.dynamic_stitch(condition_indices, [q_list[0], p])",
            "",
            "-        # verification of the condition and modification of masks",
            "-        less_mask = tf.cast(tf.less(u, p_new), tf.int32)  # 0 when u bigger than p, 1 when u less than p",
            "+        # condition verification and mask modification",
            "+        less_mask = tf.cast(tf.less(u, p_new), tf.int32)  # 0 when u is bigger than p, 1 when u is less than p",
            "condition_indices = tf.dynamic_partition(tf.range(tf.shape(p_new)[0]), less_mask,",
            "-                                                 2)  # 0 when u bigger",
            "-        #  than p, 1 when u less than p",
            "+                                                 2)  # 0 when u is bigger than p, 1 when u is less than p",
            "",
            "split_p_new = tf.dynamic_partition(p_new, less_mask, 2)",
            "split_u = tf.dynamic_partition(u, less_mask, 2)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 3,
        "minus_line": 4,
        "AST_diff_line": 17,
        "number": 3535,
        "neg_line": [
            "-# verification of the condition and modification of masks",
            "-less_mask = tf.cast(tf.less(u, p_new), tf.int32)  # 0 when u bigger than p, 1 when u less than p",
            "-2)  # 0 when u bigger",
            "-#  than p, 1 when u less than p"
        ],
        "pos_line": [
            "+# condition verification and mask modification",
            "+less_mask = tf.cast(tf.less(u, p_new), tf.int32)  # 0 when u is bigger than p, 1 when u is less than p",
            "+2)  # 0 when u is bigger than p, 1 when u is less than p"
        ],
        "core_change": "-# verification of the condition and modification of masks -less_mask = tf.cast(tf.less(u, p_new), tf.int32)  # 0 when u bigger than p, 1 when u less than p +# condition verification and mask modification +less_mask = tf.cast(tf.less(u, p_new), tf.int32)  # 0 when u is bigger than p, 1 when u is less than p -2)  # 0 when u bigger -#  than p, 1 when u less than p +2)  # 0 when u is bigger than p, 1 when u is less than p",
        "core_API": "reduce_sum"
    },
    {
        "commit_hash": "dac779a5473a40a8231b9dcf95587bcb15400c1a",
        "index": "972b357..d7a7963 100644",
        "commit_message": "Fix pylint errors\n\n",
        "file": "seq2seq.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class ParallelDataProvider(data_provider.DataProvider):",
            "",
            "# Optionally shuffle the data",
            "if shuffle:",
            "-      shuffle_queue = data_flow_ops.RandomShuffleQueue(",
            "+      shuffle_queue = tf.RandomShuffleQueue(",
            "capacity=common_queue_capacity,",
            "min_after_dequeue=common_queue_min,",
            "dtypes=[tf.string, tf.string],",
            "seed=seed)",
            "enqueue_ops = []",
            "enqueue_ops.append(shuffle_queue.enqueue([data_source, data_target]))",
            "-      queue_runner.add_queue_runner(",
            "-          queue_runner.QueueRunner(shuffle_queue, enqueue_ops))",
            "+      tf.train.add_queue_runner(",
            "+          tf.train.QueueRunner(shuffle_queue, enqueue_ops))",
            "data_source, data_target = shuffle_queue.dequeue()",
            "",
            "# Decode source items"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=2159267)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=2159268)",
            "Update(target_node=ASTNode(type=identifier, text=queue_runner), value='tf')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=queue_runner), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'train'), position=2, insert_id=2159269)",
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=2159270)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=2159271)",
            "Update(target_node=ASTNode(type=identifier, text=data_flow_ops), value='tf')",
            "Update(target_node=ASTNode(type=identifier, text=queue_runner), value='tf')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=queue_runner), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'train'), position=2, insert_id=2159272)"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 13,
        "number": 3536,
        "neg_line": [
            "-shuffle_queue = data_flow_ops.RandomShuffleQueue(",
            "-queue_runner.add_queue_runner(",
            "-queue_runner.QueueRunner(shuffle_queue, enqueue_ops))"
        ],
        "pos_line": [
            "+shuffle_queue = tf.RandomShuffleQueue(",
            "+tf.train.add_queue_runner(",
            "+tf.train.QueueRunner(shuffle_queue, enqueue_ops))"
        ],
        "core_change": "-shuffle_queue = data_flow_ops.RandomShuffleQueue( +shuffle_queue = tf.RandomShuffleQueue( -queue_runner.add_queue_runner( -queue_runner.QueueRunner(shuffle_queue, enqueue_ops)) +tf.train.add_queue_runner( +tf.train.QueueRunner(shuffle_queue, enqueue_ops))",
        "core_API": "RandomShuffleQueue"
    },
    {
        "commit_hash": "4c59718f40a36cc1da102ec4d01c5ed6bbed407d",
        "index": "597928c2..4ad95a1a 100755",
        "commit_message": "fix initializer mode\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Model(ModelDesc):",
            "defs, block_func = cfg[DEPTH]",
            "",
            "with argscope(Conv2D, nl=tf.identity, use_bias=False,",
            "-                      W_init=tf.variance_scaling_initializer(scale=2.0, mode='FAN_OUT')), \\",
            "+                      W_init=tf.variance_scaling_initializer(scale=2.0, mode='fan_out')), \\",
            "argscope([Conv2D, MaxPooling, GlobalAvgPooling, BatchNorm], data_format='NCHW'):",
            "convmaps = (LinearWrap(image)",
            ".Conv2D('conv0', 64, 7, stride=2, nl=BNReLU)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text='FAN_OUT'), value=\"'fan_out'\")"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 3537,
        "neg_line": [
            "-W_init=tf.variance_scaling_initializer(scale=2.0, mode='FAN_OUT')), \\"
        ],
        "pos_line": [
            "+W_init=tf.variance_scaling_initializer(scale=2.0, mode='fan_out')), \\"
        ],
        "core_change": "-W_init=tf.variance_scaling_initializer(scale=2.0, mode='FAN_OUT')), \\ +W_init=tf.variance_scaling_initializer(scale=2.0, mode='fan_out')), \\",
        "core_API": "variance_scaling_initializer"
    },
    {
        "commit_hash": "ed63b7ee9e68ba120893aaaaca0d4525a56ed484",
        "index": "8652d274..7c3acbf1 100644",
        "commit_message": "upgrade to pytorch 0.4.0 (#1126)\n\n* bump pytorch to 0.4 + fix sanitize\n\n* remove check for Variable in block_orthogonal\n\n* rename parameter split_size=\n\n* remove checks for Variable\n\n* fix more tests\n\n* fixes\n\n* get tests to pass\n\n* fix warnings\n\n* get rid of some of the Variables\n\n* more tests passing\n\n* more elimination of variables\n\n* finish removing all Variables\n\n* pylint and such\n\n* a few fixes\n\n* move torch.no_grad into model.forward_on_instances\n\n* more pytorch 0.4 changes\n\n* detach() -> data\n\n* pylint\n\n* fix bad tensor creation\n\n* fix types\n\n* remove print statement\n\n* more 0.4 goodness\n\n* factor out is_tensor\n\n* add no_grad to elmo command\n\n* cleanup\n\n* remove TODO\n\n* address PR feedback\n\n* replace all() with item()\n\n* more cleanup\n\n* further cleanup\n\n* remove Variable\n\n* really fix merge conflict\n\n* fix pylint\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ProductionRuleField(Field[ProductionRuleArray]):  # type: ignore",
            "@overrides",
            "def as_tensor(self,",
            "padding_lengths: Dict[str, int],",
            "-                  cuda_device: int = -1,",
            "-                  for_training: bool = True) -> ProductionRuleArray:",
            "+                  cuda_device: int = -1) -> ProductionRuleArray:",
            "# pylint: disable=unused-argument",
            "if self.is_global_rule:",
            "-            tensor = Variable(torch.LongTensor([self._rule_id]), volatile=not for_training)",
            "+            tensor = torch.LongTensor([self._rule_id])",
            "else:",
            "tensor = None",
            "return (self.rule, self.is_global_rule, tensor)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=call), position=2)",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=for_training))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=identifier, text=bool))",
            "Delete(target_node=ASTNode(type=type))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=true, text=True))",
            "Delete(target_node=ASTNode(type=typed_default_parameter))",
            "Delete(target_node=ASTNode(type=identifier, text=Variable))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=volatile))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=not, text=not))",
            "Delete(target_node=ASTNode(type=identifier, text=for_training))",
            "Delete(target_node=ASTNode(type=not_operator))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 2,
        "minus_line": 3,
        "AST_diff_line": 21,
        "number": 3540,
        "neg_line": [
            "-cuda_device: int = -1,",
            "-for_training: bool = True) -> ProductionRuleArray:",
            "-tensor = Variable(torch.LongTensor([self._rule_id]), volatile=not for_training)"
        ],
        "pos_line": [
            "+cuda_device: int = -1) -> ProductionRuleArray:",
            "+tensor = torch.LongTensor([self._rule_id])"
        ],
        "core_change": "-cuda_device: int = -1, -for_training: bool = True) -> ProductionRuleArray: +cuda_device: int = -1) -> ProductionRuleArray: -tensor = Variable(torch.LongTensor([self._rule_id]), volatile=not for_training) +tensor = torch.LongTensor([self._rule_id])",
        "core_API": "LongTensor"
    },
    {
        "commit_hash": "14f015d8bf01e8c870b26a28aebbe97460cc398f",
        "index": "cccb3d7..3c9915c 100644",
        "commit_message": "Add pyre typeshed information for Tensor.ndim and nn.ConvTranspose2d\n\nSummary: Adding some appropriate methods into pyre typeshed. Removing corresponding pyre-ignore and pyre-fixme messages.\n\nDifferential Revision: D22949138\n\nfbshipit-source-id: add8acdd4611ab698954868832594d062cd58f88\n\n",
        "file": "pytorch3d.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TexturesVertex(TexturesBase):",
            "\"\"\"",
            "if isinstance(verts_features, (tuple, list)):",
            "correct_shape = all(",
            "-                # pyre-fixme[16]: `Tensor` has no attribute `ndim`.",
            "-                (torch.is_tensor(v) and v.ndim == 2)",
            "-                for v in verts_features",
            "+                (torch.is_tensor(v) and v.ndim == 2) for v in verts_features",
            ")",
            "if not correct_shape:",
            "raise ValueError("
        ],
        "hunk_index": 2,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 17,
        "number": 3542,
        "neg_line": [
            "-# pyre-fixme[16]: `Tensor` has no attribute `ndim`.",
            "-(torch.is_tensor(v) and v.ndim == 2)",
            "-for v in verts_features"
        ],
        "pos_line": [
            "+(torch.is_tensor(v) and v.ndim == 2) for v in verts_features"
        ],
        "core_change": "-# pyre-fixme[16]: `Tensor` has no attribute `ndim`. -(torch.is_tensor(v) and v.ndim == 2) -for v in verts_features +(torch.is_tensor(v) and v.ndim == 2) for v in verts_features",
        "core_API": "is_tensor"
    },
    {
        "commit_hash": "e94abf66783f5e24ab37adc754ac902e827a4b4d",
        "index": "196f7743..ce341290 100755",
        "commit_message": "fix TF deprecation of concat/split/pack/unpack\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Model(ModelDesc):",
            "if OUT_CH == 1:",
            "output = tf.image.grayscale_to_rgb(output)",
            "fake_output = tf.image.grayscale_to_rgb(fake_output)",
            "-        viz = (tf.concat(2, [input, output, fake_output]) + 1.0) * 128.0",
            "+        viz = (tf.concat_v2([input, output, fake_output], 2) + 1.0) * 128.0",
            "viz = tf.cast(tf.clip_by_value(viz, 0, 255), tf.uint8, name='viz')",
            "tf.summary.image('input,output,fake', viz, max_outputs=max(30, BATCH))"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=concat), value='concat_v2')",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=3, insert_id=2308260)",
            "Insert(target_node=ASTNode(type=argument_list), node=('integer', '2'), position=4, insert_id=2308261)",
            "Delete(target_node=ASTNode(type=integer, text=2))",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 3544,
        "neg_line": [
            "-viz = (tf.concat(2, [input, output, fake_output]) + 1.0) * 128.0"
        ],
        "pos_line": [
            "+viz = (tf.concat_v2([input, output, fake_output], 2) + 1.0) * 128.0"
        ],
        "core_change": "-viz = (tf.concat(2, [input, output, fake_output]) + 1.0) * 128.0 +viz = (tf.concat_v2([input, output, fake_output], 2) + 1.0) * 128.0",
        "core_API": "grayscale_to_rgb"
    },
    {
        "commit_hash": "5f4c6e830f603830117877b8990a0e65a2386aa6",
        "index": "26e70fc32..653c8b051 100644",
        "commit_message": "Quick fix :) (#606)\n\n* Changing the name\n\n* style + quality\n\n* update doc and logo\n\n* clean up\n\n* circle-CI on the branche for now\n\n* fix daily dialog dataset\n\n* fix urls\n\nCo-authored-by: Quentin Lhoest <lhoest.q@gmail.com>\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class DownloadManager(object):",
            "\"\"\"",
            "Ship the files using Beam FileSystems to the pipeline temp dir.",
            "\"\"\"",
            "-        from nlp.utils.beam_utils import upload_local_to_remote",
            "+        from datasets.utils.beam_utils import upload_local_to_remote",
            "",
            "remote_dir = pipeline._options.get_all_options().get(\"temp_location\")",
            "if remote_dir is None:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=nlp), value='datasets')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 3546,
        "neg_line": [
            "-from nlp.utils.beam_utils import upload_local_to_remote"
        ],
        "pos_line": [
            "+from datasets.utils.beam_utils import upload_local_to_remote"
        ],
        "core_change": "-from nlp.utils.beam_utils import upload_local_to_remote +from datasets.utils.beam_utils import upload_local_to_remote",
        "core_API": "get_all_options"
    },
    {
        "commit_hash": "fa062a40c3d972444a65f4550f2494c7488fbf7f",
        "index": "429469ca..5cbda80f 100644",
        "commit_message": "`LightGCN`: Initialize embeddings via `xavier_uniform` (#4083)\n\n* initalized embedding with xavier uniform\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* move to reset_parameters()\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class LightGCN(torch.nn.Module):",
            "self.reset_parameters()",
            "",
            "def reset_parameters(self):",
            "-        self.embedding.reset_parameters()",
            "+        torch.nn.init.xavier_uniform_(self.embedding.weight)",
            "for conv in self.convs:",
            "conv.reset_parameters()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1490288)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1490289)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1490290)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'xavier_uniform_'), position=2, insert_id=1490291)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=attribute), position=1)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1490292)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1490293)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'init'), position=2, insert_id=1490294)",
            "Update(target_node=ASTNode(type=identifier, text=reset_parameters), value='weight')",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1490295)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1490296)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'nn'), position=2, insert_id=1490297)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 12,
        "number": 3547,
        "neg_line": [
            "-self.embedding.reset_parameters()"
        ],
        "pos_line": [
            "+torch.nn.init.xavier_uniform_(self.embedding.weight)"
        ],
        "core_change": "-self.embedding.reset_parameters() +torch.nn.init.xavier_uniform_(self.embedding.weight)",
        "core_API": "reset_parameters"
    },
    {
        "commit_hash": "0db2046b0a35d5ce7a411489a9d7d14ee254d15d",
        "index": "78e268ee08..a7919f1f82 100644",
        "commit_message": "[RLlib] Policy.compute_log_likelihoods() and SAC refactor. (issue #7107) (#7124)\n\n* Exploration API (+EpsilonGreedy sub-class).\n\n* Exploration API (+EpsilonGreedy sub-class).\n\n* Cleanup/LINT.\n\n* Add `deterministic` to generic Trainer config (NOTE: this is still ignored by most Agents).\n\n* Add `error` option to deprecation_warning().\n\n* WIP.\n\n* Bug fix: Get exploration-info for tf framework.\nBug fix: Properly deprecate some DQN config keys.\n\n* WIP.\n\n* LINT.\n\n* WIP.\n\n* Split PerWorkerEpsilonGreedy out of EpsilonGreedy.\nDocstrings.\n\n* Fix bug in sampler.py in case Policy has self.exploration = None\n\n* Update rllib/agents/dqn/dqn.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* WIP.\n\n* Update rllib/agents/trainer.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* WIP.\n\n* Change requests.\n\n* LINT\n\n* In tune/utils/util.py::deep_update() Only keep deep_updat'ing if both original and value are dicts. If value is not a dict, set\n\n* Completely obsolete syn_replay_optimizer.py's parameters schedule_max_timesteps AND beta_annealing_fraction (replaced with prioritized_replay_beta_annealing_timesteps).\n\n* Update rllib/evaluation/worker_set.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* Review fixes.\n\n* Fix default value for DQN's exploration spec.\n\n* LINT\n\n* Fix recursion bug (wrong parent c'tor).\n\n* Do not pass timestep to get_exploration_info.\n\n* Update tf_policy.py\n\n* Fix some remaining issues with test cases and remove more deprecated DQN/APEX exploration configs.\n\n* Bug fix tf-action-dist\n\n* DDPG incompatibility bug fix with new DQN exploration handling (which is imported by DDPG).\n\n* Switch off exploration when getting action probs from off-policy-estimator's policy.\n\n* LINT\n\n* Fix test_checkpoint_restore.py.\n\n* Deprecate all SAC exploration (unused) configs.\n\n* Properly use `model.last_output()` everywhere. Instead of `model._last_output`.\n\n* WIP.\n\n* Take out set_epsilon from multi-agent-env test (not needed, decays anyway).\n\n* WIP.\n\n* Trigger re-test (flaky checkpoint-restore test).\n\n* WIP.\n\n* WIP.\n\n* Add test case for deterministic action sampling in PPO.\n\n* bug fix.\n\n* Added deterministic test cases for different Agents.\n\n* Fix problem with TupleActions in dynamic-tf-policy.\n\n* Separate supported_spaces tests so they can be run separately for easier debugging.\n\n* LINT.\n\n* Fix autoregressive_action_dist.py test case.\n\n* Re-test.\n\n* Fix.\n\n* Remove duplicate py_test rule from bazel.\n\n* LINT.\n\n* WIP.\n\n* WIP.\n\n* SAC fix.\n\n* SAC fix.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* FIX 2 examples tests.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* Fix.\n\n* LINT.\n\n* Renamed test file.\n\n* WIP.\n\n* Add unittest.main.\n\n* Make action_dist_class mandatory.\n\n* fix\n\n* FIX.\n\n* WIP.\n\n* WIP.\n\n* Fix.\n\n* Fix.\n\n* Fix explorations test case (contextlib cannot find its own nullcontext??).\n\n* Force torch to be installed for QMIX.\n\n* LINT.\n\n* Fix determine_tests_to_run.py.\n\n* Fix determine_tests_to_run.py.\n\n* WIP\n\n* Add Random exploration component to tests (fixed issue with \"static-graph randomness\" via py_function).\n\n* Add Random exploration component to tests (fixed issue with \"static-graph randomness\" via py_function).\n\n* Rename some stuff.\n\n* Rename some stuff.\n\n* WIP.\n\n* WIP.\n\n* Fix SAC.\n\n* Fix SAC.\n\n* Fix strange tf-error in ray core tests.\n\n* Fix strange ray-core tf-error in test_memory_scheduling test case.\n\n* Fix test_io.py.\n\n* LINT.\n\n* Update SAC yaml files' config.\n\nCo-authored-by: Eric Liang <ekhliang@gmail.com>\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class SACModel(TFModelV2):",
            "shift_and_log_scale_diag = tf.keras.Sequential([",
            "tf.keras.layers.Dense(",
            "units=hidden,",
            "-                activation=getattr(tf.nn, actor_hidden_activation),",
            "+                activation=getattr(tf.nn, actor_hidden_activation, None),",
            "name=\"action_hidden_{}\".format(i))",
            "for i, hidden in enumerate(actor_hiddens)",
            "] + ["
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=2147275)",
            "Insert(target_node=ASTNode(type=argument_list), node=('none', 'None'), position=5, insert_id=2147276)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 3549,
        "neg_line": [
            "-activation=getattr(tf.nn, actor_hidden_activation),"
        ],
        "pos_line": [
            "+activation=getattr(tf.nn, actor_hidden_activation, None),"
        ],
        "core_change": "-activation=getattr(tf.nn, actor_hidden_activation), +activation=getattr(tf.nn, actor_hidden_activation, None),",
        "core_API": "Sequential"
    },
    {
        "commit_hash": "f3c3d2ce5d85ba77336a9d0a87c6a446732cdda6",
        "index": "a52aae9..bedbee1 100644",
        "commit_message": "Merge `develop` branch into `master` (#3518)\n\n* update ci-testing.yml (#3322)\n\n* update ci-testing.yml\n\n* update greetings.yml\n\n* bring back os matrix\n\n* update ci-testing.yml (#3322)\n\n* update ci-testing.yml\n\n* update greetings.yml\n\n* bring back os matrix\n\n* Enable direct `--weights URL` definition (#3373)\n\n* Enable direct `--weights URL` definition\n\n@KalenMike this PR will enable direct --weights URL definition. Example use case:\n```\npython train.py --weights https://storage.googleapis.com/bucket/dir/model.pt\n```\n\n* cleanup\n\n* bug fixes\n\n* weights = attempt_download(weights)\n\n* Update experimental.py\n\n* Update hubconf.py\n\n* return bug fix\n\n* comment mirror\n\n* min_bytes\n\n* Update tutorial.ipynb (#3368)\n\nadd Open in Kaggle badge\n\n* `cv2.imread(img, -1)` for IMREAD_UNCHANGED (#3379)\n\n* Update datasets.py\n\n* comment\n\nCo-authored-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* COCO evolution fix (#3388)\n\n* COCO evolution fix\n\n* cleanup\n\n* update print\n\n* print fix\n\n* Create `is_pip()` function (#3391)\n\nReturns `True` if file is part of pip package. Useful for contextual behavior modification.\n\n```python\ndef is_pip():\n    # Is file in a pip package?\n    return 'site-packages' in Path(__file__).absolute().parts\n```\n\n* Revert \"`cv2.imread(img, -1)` for IMREAD_UNCHANGED (#3379)\" (#3395)\n\nThis reverts commit 21a9607e00f1365b21d8c4bd81bdbf5fc0efea24.\n\n* Update FLOPs description (#3422)\n\n* Update README.md\n\n* Changing FLOPS to FLOPs.\n\nCo-authored-by: BuildTools <unconfigured@null.spigotmc.org>\n\n* Parse URL authentication (#3424)\n\n* Parse URL authentication\n\n* urllib.parse.unquote()\n\n* improved error handling\n\n* improved error handling\n\n* remove %3F\n\n* update check_file()\n\n* Add FLOPs title to table (#3453)\n\n* Suppress jit trace warning + graph once (#3454)\n\n* Suppress jit trace warning + graph once\n\nSuppress harmless jit trace warning on TensorBoard add_graph call. Also fix multiple add_graph() calls bug, now only on batch 0.\n\n* Update train.py\n\n* Update MixUp augmentation `alpha=beta=32.0` (#3455)\n\nPer VOC empirical results https://github.com/ultralytics/yolov5/issues/3380#issuecomment-853001307 by @developer0hye\n\n* Add `timeout()` class (#3460)\n\n* Add `timeout()` class\n\n* rearrange order\n\n* Faster HSV augmentation (#3462)\n\nremove datatype conversion process that can be skipped\n\n* Add `check_git_status()` 5 second timeout (#3464)\n\n* Add check_git_status() 5 second timeout\n\nThis should prevent the SSH Git bug that we were discussing @KalenMike\n\n* cleanup\n\n* replace timeout with check_output built-in timeout\n\n* Improved `check_requirements()` offline-handling (#3466)\n\nImprove robustness of `check_requirements()` function to offline environments (do not attempt pip installs when offline).\n\n* Add `output_names` argument for ONNX export with dynamic axes (#3456)\n\n* Add output names & dynamic axes for onnx export\n\nAdd output_names and dynamic_axes names for all outputs in torch.onnx.export. The first four outputs of the model will have names output0, output1, output2, output3\n\n* use first output only + cleanup\n\nCo-authored-by: Samridha Shrestha <samridha.shrestha@g42.ai>\nCo-authored-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Revert FP16 `test.py` and `detect.py` inference to FP32 default (#3423)\n\n* fixed inference bug ,while use half precision\n\n* replace --use-half with --half\n\n* replace space and PEP8 in detect.py\n\n* PEP8 detect.py\n\n* update --half help comment\n\n* Update test.py\n\n* revert space\n\nCo-authored-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Add additional links/resources to stale.yml message (#3467)\n\n* Update stale.yml\n\n* cleanup\n\n* Update stale.yml\n\n* reformat\n\n* Update stale.yml HUB URL (#3468)\n\n* Stale `github.actor` bug fix (#3483)\n\n* Explicit `model.eval()` call `if opt.train=False` (#3475)\n\n* call model.eval() when opt.train is False\n\ncall model.eval() when opt.train is False\n\n* single-line if statement\n\n* cleanup\n\nCo-authored-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* check_requirements() exclude `opencv-python` (#3495)\n\nFix for 3rd party or contrib versions of installed OpenCV as in https://github.com/ultralytics/yolov5/issues/3494.\n\n* Earlier `assert` for cpu and half option (#3508)\n\n* early assert for cpu and half option\n\nearly assert for cpu and half option\n\n* Modified comment\n\nModified comment\n\n* Update tutorial.ipynb (#3510)\n\n* Reduce test.py results spacing (#3511)\n\n* Update README.md (#3512)\n\n* Update README.md\n\nMinor modifications\n\n* 850 width\n\n* Update greetings.yml\n\nrevert greeting change as PRs will now merge to master.\n\nCo-authored-by: Piotr Skalski <SkalskiP@users.noreply.github.com>\nCo-authored-by: SkalskiP <piotr.skalski92@gmail.com>\nCo-authored-by: Peretz Cohen <pizzaz93@users.noreply.github.com>\nCo-authored-by: tudoulei <34886368+tudoulei@users.noreply.github.com>\nCo-authored-by: chocosaj <chocosaj@users.noreply.github.com>\nCo-authored-by: BuildTools <unconfigured@null.spigotmc.org>\nCo-authored-by: Yonghye Kwon <developer.0hye@gmail.com>\nCo-authored-by: Sam_S <SamSamhuns@users.noreply.github.com>\nCo-authored-by: Samridha Shrestha <samridha.shrestha@g42.ai>\nCo-authored-by: edificewang <609552430@qq.com>\n",
        "file": "yolov5.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def _create(name, pretrained=True, channels=3, classes=80, autoshape=True, verbo",
            "cfg = list((Path(__file__).parent / 'models').rglob(f'{name}.yaml'))[0]  # model.yaml path",
            "model = Model(cfg, channels, classes)  # create model",
            "if pretrained:",
            "-                attempt_download(fname)  # download if not found locally",
            "-                ckpt = torch.load(fname, map_location=torch.device('cpu'))  # load",
            "+                ckpt = torch.load(attempt_download(fname), map_location=torch.device('cpu'))  # load",
            "msd = model.state_dict()  # model state_dict",
            "csd = ckpt['model'].float().state_dict()  # checkpoint state_dict as FP32",
            "csd = {k: v for k, v in csd.items() if msd[k].shape == v.shape}  # filter"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=type), node=('identifier', 'ckpt'), position=0, insert_id=1298447)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=call), position=0)",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=state_dict), position=2)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=argument_list), position=1)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=1298448)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1298449)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1298450)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=call), position=1)",
            "Delete(target_node=ASTNode(type=identifier, text=ckpt))",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=identifier, text=fname))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 18,
        "number": 3555,
        "neg_line": [
            "-attempt_download(fname)  # download if not found locally",
            "-ckpt = torch.load(fname, map_location=torch.device('cpu'))  # load"
        ],
        "pos_line": [
            "+ckpt = torch.load(attempt_download(fname), map_location=torch.device('cpu'))  # load"
        ],
        "core_change": "-attempt_download(fname)  # download if not found locally -ckpt = torch.load(fname, map_location=torch.device('cpu'))  # load +ckpt = torch.load(attempt_download(fname), map_location=torch.device('cpu'))  # load",
        "core_API": "load"
    },
    {
        "commit_hash": "8982e4d78c6d32e1dabc6f40bf1633c018b430a7",
        "index": "d81e600ab4..13e947dc5f 100644",
        "commit_message": "Revert \"[Ray Dataset] fix the type infer of pd.dataframe (when dtype is object)\" (#25809)\n\nThis reverts commit f61f60f70857888751c214d8ec280b85aeeb5932.\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def convert_pandas_to_tf_tensor(",
            "# them. If the columns contain different types (for example, `float32`s",
            "# and `int32`s), then `tf.concat` raises an error.",
            "dtype: np.dtype = np.find_common_type(df.dtypes, [])",
            "-",
            "-            # if the columns are `ray.data.extensions.tensor_extension.TensorArray`,",
            "-            # the dtype will be `object`. In this case, we need to set the dtype to",
            "-            # none, and use the automatic type casting of `tf.convert_to_tensor`.",
            "-            if isinstance(dtype, object):",
            "-                dtype = None",
            "-",
            "except TypeError:",
            "# `find_common_type` fails if a series has `TensorDtype`. In this case,",
            "# don't cast any of the series and continue."
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=ERROR), node=('typed_default_parameter', None), position=3, insert_id=2136265)",
            "Insert(target_node=ASTNode(type=ERROR), node=('identifier', 'except'), position=4, insert_id=2136266)",
            "Move(target_node=IN(type=typed_default_parameter), node=ASTNode(type=identifier, text=dtype), position=0)",
            "Move(target_node=IN(type=typed_default_parameter), node=ASTNode(type=:, text=:), position=1)",
            "Move(target_node=IN(type=typed_default_parameter), node=ASTNode(type=type), position=2)",
            "Move(target_node=IN(type=typed_default_parameter), node=ASTNode(type==, text==), position=3)",
            "Move(target_node=IN(type=typed_default_parameter), node=ASTNode(type=call), position=4)",
            "Delete(target_node=ASTNode(type=if, text=if))",
            "Delete(target_node=ASTNode(type=identifier, text=isinstance))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=dtype))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=object))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=identifier, text=dtype))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=none, text=None))",
            "Delete(target_node=ASTNode(type=except, text=except))",
            "Delete(target_node=ASTNode(type=ERROR))"
        ],
        "plus_line": 0,
        "minus_line": 5,
        "AST_diff_line": 22,
        "number": 3556,
        "neg_line": [
            "-",
            "-# if the columns are `ray.data.extensions.tensor_extension.TensorArray`,",
            "-# the dtype will be `object`. In this case, we need to set the dtype to",
            "-# none, and use the automatic type casting of `tf.convert_to_tensor`.",
            "-if isinstance(dtype, object):",
            "-dtype = None",
            "-"
        ],
        "pos_line": [],
        "core_change": "- -# if the columns are `ray.data.extensions.tensor_extension.TensorArray`, -# the dtype will be `object`. In this case, we need to set the dtype to -# none, and use the automatic type casting of `tf.convert_to_tensor`. -if isinstance(dtype, object): -dtype = None -",
        "core_API": "find_common_type"
    },
    {
        "commit_hash": "c531eb7cc6462eaa7960f2dc5f65db60a0b57b52",
        "index": "4abba3401..1fae98a36 100644",
        "commit_message": "Fixed MNIST dataset in integration test\n\n",
        "file": "PySyft.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "\"train_kwargs = {\\n\",",
            "\"    \\\"batch_size\\\": args[\\\"batch_size\\\"],\\n\",",
            "\"}\\n\",",
            "-    \"train_data_ptr = remote_torchvision.datasets.MNIST('../data', train=True, download=True, transform=transforms)\\n\",",
            "+    \"train_data_ptr = remote_torchvision.datasets.MNIST(str(get_root_data_path()), train=True, download=True, transform=transforms)\\n\",",
            "\"train_loader_ptr = remote_torch.utils.data.DataLoader(train_data_ptr,**train_kwargs)\"",
            "]",
            "},"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"train_data_ptr = remote_torchvision.datasets.MNIST('../data', train=True, download=True, transform=transforms)\\n\"), value='\"train_data_ptr = remote_torchvision.datasets.MNIST(str(get_root_data_path()), train=True, download=True, transform=transforms)\\\\n\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 3557,
        "neg_line": [
            "-\"train_data_ptr = remote_torchvision.datasets.MNIST('../data', train=True, download=True, transform=transforms)\\n\","
        ],
        "pos_line": [
            "+\"train_data_ptr = remote_torchvision.datasets.MNIST(str(get_root_data_path()), train=True, download=True, transform=transforms)\\n\","
        ],
        "core_change": "-\"train_data_ptr = remote_torchvision.datasets.MNIST('../data', train=True, download=True, transform=transforms)\\n\", +\"train_data_ptr = remote_torchvision.datasets.MNIST(str(get_root_data_path()), train=True, download=True, transform=transforms)\\n\",",
        "core_API": "MNIST"
    },
    {
        "commit_hash": "d215aa258e77f43b6c36950bc9a484d5d849eb4f",
        "index": "182dfdae..f03e0b92 100644",
        "commit_message": "fix torch.nonzero usage in pytorch 1.5 (#2602)\n\n\n",
        "file": "mmdetection.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class GuidedAnchorHead(AnchorHead):",
            "bbox_deltas = bbox_anchors.new_full(bbox_anchors.size(), 0)",
            "bbox_deltas[:, 2:] += shape_pred",
            "# filter out negative samples to speed-up weighted_bounded_iou_loss",
            "-        inds = torch.nonzero(anchor_weights[:, 0] > 0).squeeze(1)",
            "+        inds = torch.nonzero(",
            "+            anchor_weights[:, 0] > 0, as_tuple=False).squeeze(1)",
            "bbox_deltas_ = bbox_deltas[inds]",
            "bbox_anchors_ = bbox_anchors[inds]",
            "bbox_gts_ = bbox_gts[inds]"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=638790)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=638791)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'as_tuple'), position=0, insert_id=638792)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=638793)",
            "Insert(target_node=IN(type=keyword_argument), node=('false', 'False'), position=2, insert_id=638794)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 3558,
        "neg_line": [
            "-inds = torch.nonzero(anchor_weights[:, 0] > 0).squeeze(1)"
        ],
        "pos_line": [
            "+inds = torch.nonzero(",
            "+anchor_weights[:, 0] > 0, as_tuple=False).squeeze(1)"
        ],
        "core_change": "-inds = torch.nonzero(anchor_weights[:, 0] > 0).squeeze(1) +inds = torch.nonzero( +anchor_weights[:, 0] > 0, as_tuple=False).squeeze(1)",
        "core_API": "new_full"
    },
    {
        "commit_hash": "e94abf66783f5e24ab37adc754ac902e827a4b4d",
        "index": "e004b650..e028c0c4 100755",
        "commit_message": "fix TF deprecation of concat/split/pack/unpack\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Model(ModelDesc):",
            "# For visualization in tensorboard",
            "padded1 = tf.pad(sampled1, [[0, 0], [HALF_DIFF, HALF_DIFF], [HALF_DIFF, HALF_DIFF], [0, 0]])",
            "padded2 = tf.pad(sampled2, [[0, 0], [HALF_DIFF, HALF_DIFF], [HALF_DIFF, HALF_DIFF], [0, 0]])",
            "-        img_orig = tf.concat(1, [image[:, :, :, 0], image[:, :, :, 1]])  # b x 2h  x w",
            "-        transform1 = tf.concat(1, [padded1[:, :, :, 0], padded1[:, :, :, 1]])",
            "-        transform2 = tf.concat(1, [padded2[:, :, :, 0], padded2[:, :, :, 1]])",
            "-        stacked = tf.concat(2, [img_orig, transform1, transform2], 'viz')",
            "+        img_orig = tf.concat_v2([image[:, :, :, 0], image[:, :, :, 1]], 1)  # b x 2h  x w",
            "+        transform1 = tf.concat_v2([padded1[:, :, :, 0], padded1[:, :, :, 1]], 1)",
            "+        transform2 = tf.concat_v2([padded2[:, :, :, 0], padded2[:, :, :, 1]], 1)",
            "+        stacked = tf.concat_v2([img_orig, transform1, transform2], 2, 'viz')",
            "tf.summary.image('visualize',",
            "tf.expand_dims(stacked, -1), max_images=30)",
            "",
            "-        sampled = tf.concat(3, [sampled1, sampled2], 'sampled_concat')",
            "+        sampled = tf.concat_v2([sampled1, sampled2], 3, 'sampled_concat')",
            "logits = (LinearWrap(sampled)",
            ".apply(symbf.batch_flatten)",
            ".FullyConnected('fc1', out_dim=256, nl=tf.nn.relu)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=concat), value='concat_v2')",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=3, insert_id=2308276)",
            "Insert(target_node=ASTNode(type=argument_list), node=('integer', '1'), position=4, insert_id=2308277)",
            "Update(target_node=ASTNode(type=identifier, text=concat), value='concat_v2')",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=3, insert_id=2308278)",
            "Insert(target_node=ASTNode(type=argument_list), node=('integer', '1'), position=4, insert_id=2308279)",
            "Update(target_node=ASTNode(type=identifier, text=concat), value='concat_v2')",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=3, insert_id=2308280)",
            "Insert(target_node=ASTNode(type=argument_list), node=('integer', '1'), position=4, insert_id=2308281)",
            "Update(target_node=ASTNode(type=identifier, text=concat), value='concat_v2')",
            "Insert(target_node=ASTNode(type=argument_list), node=('integer', '2'), position=4, insert_id=2308282)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=5, insert_id=2308283)",
            "Update(target_node=ASTNode(type=identifier, text=concat), value='concat_v2')",
            "Insert(target_node=ASTNode(type=argument_list), node=('integer', '3'), position=4, insert_id=2308284)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=5, insert_id=2308285)",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=integer, text=2))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=integer, text=3))",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 5,
        "minus_line": 5,
        "AST_diff_line": 25,
        "number": 3559,
        "neg_line": [
            "-img_orig = tf.concat(1, [image[:, :, :, 0], image[:, :, :, 1]])  # b x 2h  x w",
            "-transform1 = tf.concat(1, [padded1[:, :, :, 0], padded1[:, :, :, 1]])",
            "-transform2 = tf.concat(1, [padded2[:, :, :, 0], padded2[:, :, :, 1]])",
            "-stacked = tf.concat(2, [img_orig, transform1, transform2], 'viz')",
            "-sampled = tf.concat(3, [sampled1, sampled2], 'sampled_concat')"
        ],
        "pos_line": [
            "+img_orig = tf.concat_v2([image[:, :, :, 0], image[:, :, :, 1]], 1)  # b x 2h  x w",
            "+transform1 = tf.concat_v2([padded1[:, :, :, 0], padded1[:, :, :, 1]], 1)",
            "+transform2 = tf.concat_v2([padded2[:, :, :, 0], padded2[:, :, :, 1]], 1)",
            "+stacked = tf.concat_v2([img_orig, transform1, transform2], 2, 'viz')",
            "+sampled = tf.concat_v2([sampled1, sampled2], 3, 'sampled_concat')"
        ],
        "core_change": "-img_orig = tf.concat(1, [image[:, :, :, 0], image[:, :, :, 1]])  # b x 2h  x w -transform1 = tf.concat(1, [padded1[:, :, :, 0], padded1[:, :, :, 1]]) -transform2 = tf.concat(1, [padded2[:, :, :, 0], padded2[:, :, :, 1]]) -stacked = tf.concat(2, [img_orig, transform1, transform2], 'viz') +img_orig = tf.concat_v2([image[:, :, :, 0], image[:, :, :, 1]], 1)  # b x 2h  x w +transform1 = tf.concat_v2([padded1[:, :, :, 0], padded1[:, :, :, 1]], 1) +transform2 = tf.concat_v2([padded2[:, :, :, 0], padded2[:, :, :, 1]], 1) +stacked = tf.concat_v2([img_orig, transform1, transform2], 2, 'viz') -sampled = tf.concat(3, [sampled1, sampled2], 'sampled_concat') +sampled = tf.concat_v2([sampled1, sampled2], 3, 'sampled_concat')",
        "core_API": "pad"
    },
    {
        "commit_hash": "e750fe7819ce00cb3585aea628a038ed67ad364f",
        "index": "8c1752710..961cc164e 100644",
        "commit_message": "fix test for dtype\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class LengthBonus(ScorerInterface):",
            "self.n = n_vocab",
            "",
            "def score(self, y, state, x):",
            "-        return torch.tensor([1.0], device=y.device).expand(self.n), None",
            "+        return torch.tensor([1.0], device=x.device, dtype=x.dtype).expand(self.n), None"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=168812)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=168813)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=168814)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=168815)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=168816)",
            "Update(target_node=ASTNode(type=identifier, text=y), value='x')",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'x'), position=0, insert_id=168817)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=168818)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dtype'), position=2, insert_id=168819)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 3560,
        "neg_line": [
            "-return torch.tensor([1.0], device=y.device).expand(self.n), None"
        ],
        "pos_line": [
            "+return torch.tensor([1.0], device=x.device, dtype=x.dtype).expand(self.n), None"
        ],
        "core_change": "-return torch.tensor([1.0], device=y.device).expand(self.n), None +return torch.tensor([1.0], device=x.device, dtype=x.dtype).expand(self.n), None",
        "core_API": "tensor"
    },
    {
        "commit_hash": "09fc038e202c5411c4292d065f4ad04369bd87fc",
        "index": "74bb4af81..8f6b21834 100644",
        "commit_message": "fix linter and add docs\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def _plot_and_save_attention(att_w, filename):",
            "",
            "",
            "def plot_multi_head_attention(data, attn_dict, outdir, suffix=\"png\"):",
            "+    \"\"\"Plot multi head attentions",
            "+",
            "+    :param dict data: utts info from json file",
            "+    :param dict[str, torch.Tensor] attn_dict: multi head attention dict.",
            "+        values should be torch.Tensor (head, input_length, output_length)",
            "+    :param str outdir: dir to save fig",
            "+    :param str suffix: filename suffix including image type (e.g., png)",
            "+    \"\"\"",
            "for name, att_ws in attn_dict.items():",
            "for idx, att_w in enumerate(att_ws):",
            "filename = \"%s/%s.%s.%s\" % ("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('block', None), position=0, insert_id=175130)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=175131)",
            "Insert(target_node=IN(type=expression_statement), node=('string', '\"\"\"Plot multi head attentions\\n\\n    :param dict data: utts info from json file\\n    :param dict[str, torch.Tensor] attn_dict: multi head attention dict.\\n        values should be torch.Tensor (head, input_length, output_length)\\n    :param str outdir: dir to save fig\\n    :param str suffix: filename suffix including image type (e.g., png)\\n    \"\"\"'), position=0, insert_id=175132)",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 7,
        "minus_line": 0,
        "AST_diff_line": 4,
        "number": 3564,
        "neg_line": [],
        "pos_line": [
            "+\"\"\"Plot multi head attentions",
            "+",
            "+:param dict data: utts info from json file",
            "+:param dict[str, torch.Tensor] attn_dict: multi head attention dict.",
            "+values should be torch.Tensor (head, input_length, output_length)",
            "+:param str outdir: dir to save fig",
            "+:param str suffix: filename suffix including image type (e.g., png)",
            "+\"\"\""
        ],
        "core_change": "+\"\"\"Plot multi head attentions + +:param dict data: utts info from json file +:param dict[str, torch.Tensor] attn_dict: multi head attention dict. +values should be torch.Tensor (head, input_length, output_length) +:param str outdir: dir to save fig +:param str suffix: filename suffix including image type (e.g., png) +\"\"\"",
        "core_API": "items"
    },
    {
        "commit_hash": "95b6d985ffb231d9a2047c039c95660913063b9d",
        "index": "dfd33bde..f60fc4a3 100644",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class AucTest(AllenNlpTestCase):",
            "",
            "predictions = torch.randn(8, device=device)",
            "labels = torch.randint(0, 2, (8,), dtype=torch.long, device=device)",
            "-        mask = torch.tensor([1, 1, 1, 1, 0, 0, 0, 0], dtype=torch.uint8, device=device)",
            "+        mask = torch.BoolTensor([True, True, True, True, False, False, False, False], device=device)",
            "",
            "auc(predictions, labels, mask)",
            "computed_auc_value = auc.get_metric(reset=True)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tensor), value='BoolTensor')",
            "Insert(target_node=ASTNode(type=list), node=('true', 'True'), position=1, insert_id=20006)",
            "Insert(target_node=ASTNode(type=list), node=('true', 'True'), position=4, insert_id=20007)",
            "Insert(target_node=ASTNode(type=list), node=('true', 'True'), position=6, insert_id=20008)",
            "Insert(target_node=ASTNode(type=list), node=('true', 'True'), position=9, insert_id=20009)",
            "Insert(target_node=ASTNode(type=list), node=('false', 'False'), position=11, insert_id=20010)",
            "Insert(target_node=ASTNode(type=list), node=('false', 'False'), position=14, insert_id=20011)",
            "Insert(target_node=ASTNode(type=list), node=('false', 'False'), position=16, insert_id=20012)",
            "Insert(target_node=ASTNode(type=list), node=('false', 'False'), position=17, insert_id=20013)",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=integer, text=0))",
            "Delete(target_node=ASTNode(type=integer, text=0))",
            "Delete(target_node=ASTNode(type=integer, text=0))",
            "Delete(target_node=ASTNode(type=integer, text=0))",
            "Delete(target_node=ASTNode(type=identifier, text=dtype))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=uint8))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 25,
        "number": 3565,
        "neg_line": [
            "-mask = torch.tensor([1, 1, 1, 1, 0, 0, 0, 0], dtype=torch.uint8, device=device)"
        ],
        "pos_line": [
            "+mask = torch.BoolTensor([True, True, True, True, False, False, False, False], device=device)"
        ],
        "core_change": "-mask = torch.tensor([1, 1, 1, 1, 0, 0, 0, 0], dtype=torch.uint8, device=device) +mask = torch.BoolTensor([True, True, True, True, False, False, False, False], device=device)",
        "core_API": "randn"
    },
    {
        "commit_hash": "965aa953ac7ea59932a89bf6168aa412694d1f83",
        "index": "456e800a..b0a03a7a 100644",
        "commit_message": "some fixes\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def BatchNorm(x, use_local_stat=None, decay=0.9, epsilon=1e-5):",
            "",
            "n_out = shape[-1]  # channel",
            "assert n_out is not None",
            "-    beta = tf.get_variable('beta', [n_out])",
            "+    beta = tf.get_variable('beta', [n_out],",
            "+            initializer=tf.zeros_initializer)",
            "gamma = tf.get_variable('gamma', [n_out],",
            "-        initializer=tf.ones_initializer)",
            "+            initializer=tf.ones_initializer)",
            "",
            "if len(shape) == 2:",
            "batch_mean, batch_var = tf.nn.moments(x, [0], keep_dims=False)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=2311477)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=2311478)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'initializer'), position=0, insert_id=2311479)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2311480)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=2311481)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2311482)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2311483)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'zeros_initializer'), position=2, insert_id=2311484)"
        ],
        "plus_line": 3,
        "minus_line": 2,
        "AST_diff_line": 8,
        "number": 3567,
        "neg_line": [
            "-beta = tf.get_variable('beta', [n_out])",
            "-initializer=tf.ones_initializer)"
        ],
        "pos_line": [
            "+beta = tf.get_variable('beta', [n_out],",
            "+initializer=tf.zeros_initializer)",
            "+initializer=tf.ones_initializer)"
        ],
        "core_change": "-beta = tf.get_variable('beta', [n_out]) +beta = tf.get_variable('beta', [n_out], +initializer=tf.zeros_initializer) -initializer=tf.ones_initializer) +initializer=tf.ones_initializer)",
        "core_API": "get_variable"
    },
    {
        "commit_hash": "85149cdcd06e57eb2a6b001c9ba1d125383a926f",
        "index": "715cf39d9..18491b023 100644",
        "commit_message": "fixed a bug in the case of multi-gpu\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def recog(args):",
            "with open(args.recog_json, 'rb') as f:",
            "recog_json = json.load(f)['utts']",
            "",
            "+    if not torch_is_old:",
            "+        torch.set_grad_enabled(False)",
            "+",
            "new_json = {}",
            "for name in recog_json.keys():",
            "feat = kaldi_io_py.read_mat(recog_json[name]['input'][0]['feat'])"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=3, insert_id=182768)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=182769)",
            "Insert(target_node=IN(type=if_statement), node=('not_operator', None), position=1, insert_id=182770)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=182771)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=182772)",
            "Insert(target_node=IN(type=not_operator), node=('not', 'not'), position=0, insert_id=182773)",
            "Insert(target_node=IN(type=not_operator), node=('identifier', 'torch_is_old'), position=1, insert_id=182774)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=182775)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=182776)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=182777)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=182778)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=182779)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=182780)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'set_grad_enabled'), position=2, insert_id=182781)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=182782)",
            "Insert(target_node=IN(type=argument_list), node=('false', 'False'), position=1, insert_id=182783)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=182784)"
        ],
        "plus_line": 2,
        "minus_line": 0,
        "AST_diff_line": 17,
        "number": 3568,
        "neg_line": [],
        "pos_line": [
            "+if not torch_is_old:",
            "+torch.set_grad_enabled(False)",
            "+"
        ],
        "core_change": "+if not torch_is_old: +torch.set_grad_enabled(False) +",
        "core_API": "load"
    },
    {
        "commit_hash": "6b05603010b9002e5ad8999fb5d54eabaa0a9fb6",
        "index": "6fd50fb..896aa5a 100644",
        "commit_message": "fix bug\n\n",
        "file": "tflearn.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def densenet_block(incoming, nb_layers, growth, bottleneck=True,",
            "\"\"\"",
            "densenet = incoming",
            "",
            "-    for i in range(nb_layers):",
            "+    with tf.variable_scope(scope, default_name=name, values=[incoming],",
            "+                           reuse=reuse) as scope:",
            "",
            "-        with tf.variable_scope(scope, default_name=name, values=[incoming],",
            "-                               reuse=reuse) as scope:",
            "+        for i in range(nb_layers):",
            "",
            "# Identity",
            "conn = densenet"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=module), node=ASTNode(type=with_statement), position=2)",
            "Insert(target_node=ASTNode(type=with_statement), node=('block', None), position=3, insert_id=2350093)",
            "Insert(target_node=IN(type=block), node=('for_statement', None), position=0, insert_id=2350094)",
            "Insert(target_node=IN(type=for_statement), node=('for', 'for'), position=0, insert_id=2350095)",
            "Insert(target_node=IN(type=for_statement), node=('identifier', 'i'), position=1, insert_id=2350096)",
            "Insert(target_node=IN(type=for_statement), node=('in', 'in'), position=2, insert_id=2350097)",
            "Move(target_node=IN(type=for_statement), node=ASTNode(type=call), position=3)",
            "Insert(target_node=IN(type=for_statement), node=(':', ':'), position=4, insert_id=2350098)",
            "Move(target_node=IN(type=for_statement), node=ASTNode(type=block, text=), position=5)",
            "Delete(target_node=ASTNode(type=for, text=for))",
            "Delete(target_node=ASTNode(type=identifier, text=i))",
            "Delete(target_node=ASTNode(type=in, text=in))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=block))",
            "Delete(target_node=ASTNode(type=for_statement))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 15,
        "number": 3569,
        "neg_line": [
            "-for i in range(nb_layers):",
            "-with tf.variable_scope(scope, default_name=name, values=[incoming],",
            "-reuse=reuse) as scope:"
        ],
        "pos_line": [
            "+with tf.variable_scope(scope, default_name=name, values=[incoming],",
            "+reuse=reuse) as scope:",
            "+for i in range(nb_layers):"
        ],
        "core_change": "-for i in range(nb_layers): +with tf.variable_scope(scope, default_name=name, values=[incoming], +reuse=reuse) as scope: -with tf.variable_scope(scope, default_name=name, values=[incoming], -reuse=reuse) as scope: +for i in range(nb_layers):",
        "core_API": "variable_scope"
    },
    {
        "commit_hash": "2cbe29a7fa8bc63d57f97b03ba726676425e160b",
        "index": "27980e5ac..1684d11c6 100644",
        "commit_message": "[RLlib] Curiosity minor fixes, do-overs, and testing. (#10143)\n\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class LSTMWrapper(RecurrentNetwork, nn.Module):",
            "wrapped_out,",
            "torch.reshape(input_dict[SampleBatch.PREV_ACTIONS].float(),",
            "[-1, self.action_dim]),",
            "-                    torch.reshape(input_dict[SampleBatch.PREV_REWARDS],",
            "+                    torch.reshape(input_dict[SampleBatch.PREV_REWARDS].float(),",
            "[-1, 1]),",
            "],",
            "dim=1)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=1, insert_id=1121709)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1121710)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1121711)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=subscript), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1121712)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'float'), position=2, insert_id=1121713)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1121714)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1121715)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 3570,
        "neg_line": [
            "-torch.reshape(input_dict[SampleBatch.PREV_REWARDS],"
        ],
        "pos_line": [
            "+torch.reshape(input_dict[SampleBatch.PREV_REWARDS].float(),"
        ],
        "core_change": "-torch.reshape(input_dict[SampleBatch.PREV_REWARDS], +torch.reshape(input_dict[SampleBatch.PREV_REWARDS].float(),",
        "core_API": "reshape"
    },
    {
        "commit_hash": "6c22058b10d46efd195fbf8fed09954f0c1cd6a3",
        "index": "28476ec..e190e7b 100644",
        "commit_message": "Added tf.keras support (#513)\n\n* Added support for tf.keras\n\n* Added unit tests\n\n* Refactoring\n\n* Fixed tests\n\n* Hide implementation modules\n\n* Moved _DistributedOptimizer into the impl file and wrapped with function\n\n* Added cooperative multiple inheritance\n\n* Backwards compatability with TensorFlow versions less than 1.4.0\n\n* Removed duplicate headers\n\n",
        "file": "horovod.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "keras",
        "change": [
            "class KerasTests(tf.test.TestCase):",
            "new_opt = new_model.optimizer",
            "os.remove(fname)",
            "",
            "-            self.assertEqual(type(new_opt).__module__, 'horovod.keras')",
            "+            self.assertEqual(type(new_opt).__module__, 'horovod.keras.impl')",
            "self.assertEqual(type(new_opt).__name__, 'TestOptimizer')",
            "self.assertEqual(K.get_value(opt.lr), K.get_value(new_opt.lr))",
            "self.assertEqual(len(opt.get_weights()), len(new_opt.get_weights()))"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text='horovod.keras'), value=\"'horovod.keras.impl'\")"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 3571,
        "neg_line": [
            "-self.assertEqual(type(new_opt).__module__, 'horovod.keras')"
        ],
        "pos_line": [
            "+self.assertEqual(type(new_opt).__module__, 'horovod.keras.impl')"
        ],
        "core_change": "-self.assertEqual(type(new_opt).__module__, 'horovod.keras') +self.assertEqual(type(new_opt).__module__, 'horovod.keras.impl')",
        "core_API": "remove"
    },
    {
        "commit_hash": "40ded605537ed31623a9993a9692f440ec91b307",
        "index": "26280ceeb..d7df32e14 100644",
        "commit_message": "fix: minor fixes and refactoring NER (#539)\n\n* feat: add train from scratch\n\n* feat: add warning to train_from_scratch\n\n* fix: no cudnn rnn fixed mask\n\n* fix: session config added\n\n* fix: warning to log\n\n* fix: pass learning rate to default optimizer\n\n* fix: l2 in conv layer\n\n* fix: ner dataset reader\n\n* feat: add returning loss to the train on batch\n\n* doc: add model descriptions to the ner doc\n\n* chore: move ner metrics to metrics module\n\n* fix: remove train from scratch\n\n* fix: add dataset names\n\n* chore: log.info -> log.warning for warning\n\nCo-Authored-By: mu-arkhipov <arkhipov@yahoo.com>\n\n",
        "file": "DeepPavlov.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def stacked_cnn(units: tf.Tensor,",
            "padding='same',",
            "dilation_rate=dilation_rate,",
            "kernel_initializer=INITIALIZER(),",
            "-                                 kernel_regularizer=tf.nn.l2_loss)",
            "+                                 kernel_regularizer=l2_reg)",
            "if use_batch_norm:",
            "assert training_ph is not None",
            "units = tf.layers.batch_normalization(units, training=training_ph)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tf), value='l2_reg')",
            "Move(target_node=ASTNode(type=default_parameter), node=ASTNode(type=identifier, text=tf), position=2)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=nn))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=l2_loss))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 3572,
        "neg_line": [
            "-kernel_regularizer=tf.nn.l2_loss)"
        ],
        "pos_line": [
            "+kernel_regularizer=l2_reg)"
        ],
        "core_change": "-kernel_regularizer=tf.nn.l2_loss) +kernel_regularizer=l2_reg)",
        "core_API": "batch_normalization"
    },
    {
        "commit_hash": "25d5ca48e08cc3bea1bad8eb7b5f7dc19cc032ac",
        "index": "07fb25610..32ae35b74 100644",
        "commit_message": "Fix scatter LopngTensor\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "cuda",
        "change": [
            "class BertForQuestionAnswering(nn.Module):",
            "def compute_loss(logits, positions):",
            "max_position = positions.max().item()",
            "one_hot = torch.FloatTensor(batch_size, max(max_position, seq_length) +1, device=input_ids.device).zero_()",
            "-                one_hot = one_hot.scatter(1, positions, 1)",
            "+                one_hot = one_hot.scatter(1, positions.cpu(), 1) # Second argument need to be LongTensor and not cuda.LongTensor",
            "one_hot = one_hot[:, :seq_length]",
            "log_probs = nn.functional.log_softmax(logits, dim = -1).view(batch_size, seq_length)",
            "loss = -torch.mean(torch.sum(one_hot*log_probs), dim = -1)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=3, insert_id=1879127)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=7, insert_id=1879128)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1879129)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1879130)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=positions), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1879131)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cpu'), position=2, insert_id=1879132)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1879133)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=), text=)), position=1)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 3573,
        "neg_line": [
            "-one_hot = one_hot.scatter(1, positions, 1)"
        ],
        "pos_line": [
            "+one_hot = one_hot.scatter(1, positions.cpu(), 1) # Second argument need to be LongTensor and not cuda.LongTensor"
        ],
        "core_change": "-one_hot = one_hot.scatter(1, positions, 1) +one_hot = one_hot.scatter(1, positions.cpu(), 1) # Second argument need to be LongTensor and not cuda.LongTensor",
        "core_API": "max"
    },
    {
        "commit_hash": "785f68ff91ff89d29e04433d144d0c776368be90",
        "index": "e01c8c606..1188cce19 100644",
        "commit_message": "fixed bug in unit test\n\n",
        "file": "PySyft.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestChainTensor(TestCase):",
            "x.get()",
            "x.child = x.child.child",
            "",
            "-        # target = sy._PlusIsMinusTensor().on(torch.FloatTensor([1, 1]))",
            "-        target = torch.FloatTensor([1, 1])",
            "+        target = sy._PlusIsMinusTensor().on(torch.FloatTensor([1, 1]))",
            "+        # target = torch.FloatTensor([1, 1])",
            "assert torch.equal(x.grad.data, target)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=855039)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=855040)",
            "Insert(target_node=IN(type=attribute), node=('call', None), position=0, insert_id=855041)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=855042)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'on'), position=2, insert_id=855043)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=855044)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=call), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=855045)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=855046)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=855047)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'sy'), position=0, insert_id=855048)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=855049)",
            "Insert(target_node=IN(type=attribute), node=('identifier', '_PlusIsMinusTensor'), position=2, insert_id=855050)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=855051)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=855052)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 15,
        "number": 3574,
        "neg_line": [
            "-# target = sy._PlusIsMinusTensor().on(torch.FloatTensor([1, 1]))",
            "-target = torch.FloatTensor([1, 1])"
        ],
        "pos_line": [
            "+target = sy._PlusIsMinusTensor().on(torch.FloatTensor([1, 1]))",
            "+# target = torch.FloatTensor([1, 1])"
        ],
        "core_change": "-# target = sy._PlusIsMinusTensor().on(torch.FloatTensor([1, 1])) -target = torch.FloatTensor([1, 1]) +target = sy._PlusIsMinusTensor().on(torch.FloatTensor([1, 1])) +# target = torch.FloatTensor([1, 1])",
        "core_API": "get"
    },
    {
        "commit_hash": "1dfafe003da30679887b523f4a76635d256b969a",
        "index": "8feacd25..15e7426e 100644",
        "commit_message": "fix multiband melgan inference\n\n",
        "file": "TTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class MultibandMelganGenerator(MelganGenerator):",
            "def pqmf_synthesis(self, x):",
            "return self.pqmf_layer.synthesis(x)",
            "",
            "+    @torch.no_grad()",
            "def inference(self, cond_features):",
            "cond_features = cond_features.to(self.layers[1].weight.device)",
            "cond_features = torch.nn.functional.pad(",
            "cond_features,",
            "(self.inference_padding, self.inference_padding),",
            "'replicate')",
            "-        return self.pqmf.synthesis(self.layers(cond_features))",
            "+        return self.pqmf_synthesis(self.layers(cond_features))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('decorated_definition', None), position=3, insert_id=1269815)",
            "Insert(target_node=IN(type=decorated_definition), node=('decorator', None), position=0, insert_id=1269816)",
            "Move(target_node=IN(type=decorated_definition), node=ASTNode(type=function_definition), position=1)",
            "Insert(target_node=IN(type=decorator), node=('@', '@'), position=0, insert_id=1269817)",
            "Insert(target_node=IN(type=decorator), node=('call', None), position=1, insert_id=1269818)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1269819)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1269820)",
            "Update(target_node=ASTNode(type=identifier, text=pqmf), value='pqmf_synthesis')",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1269821)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1269822)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'no_grad'), position=2, insert_id=1269823)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1269824)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1269825)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=synthesis))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 3575,
        "neg_line": [
            "-return self.pqmf.synthesis(self.layers(cond_features))"
        ],
        "pos_line": [
            "+@torch.no_grad()",
            "+return self.pqmf_synthesis(self.layers(cond_features))"
        ],
        "core_change": "+@torch.no_grad() -return self.pqmf.synthesis(self.layers(cond_features)) +return self.pqmf_synthesis(self.layers(cond_features))",
        "core_API": "synthesis"
    },
    {
        "commit_hash": "a217a34ad09559f9362815f4d3effe6de9e832a0",
        "index": "3f25c4b9..7ea0094a 100644",
        "commit_message": "[TEST][DO NOT MERGE] update to pytorch v1.5.0 (#533)\n\n* fix padding issue for pytorch nightly build\n\n* update requirements to pytorch v1.5.0\n\n* fix mypy error in feature laf\n\n* add dtype test type\n\n* fix tests with same dtype arithmetic\n\n* make render gaussian2d input test contiguous\n\n* add PYTORCH_VERISION environment variable to setuptools\n\n* enable grayscale jit tests again\n\n* enable conv_quad grad tests\n\n* test grayscale on float16,float64\n\nCo-authored-by: connorlee77 <connorlee77@gmail.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class PatchDominantGradientOrientation(nn.Module):",
            "self.num_ang_bins = num_angular_bins",
            "self.gradient = SpatialGradient('sobel', 1)",
            "self.eps = eps",
            "-        self.angular_smooth = nn.Conv1d(1, 1, kernel_size=3, padding=2, bias=False, padding_mode=\"circular\")",
            "+        self.angular_smooth = nn.Conv1d(1, 1, kernel_size=3, padding=1, bias=False, padding_mode=\"circular\")",
            "with torch.no_grad():",
            "self.angular_smooth.weight[:] = torch.tensor([[[0.33, 0.34, 0.33]]])",
            "sigma: float = float(self.patch_size) / math.sqrt(2.0)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=integer, text=2), value='1')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 3576,
        "neg_line": [
            "-self.angular_smooth = nn.Conv1d(1, 1, kernel_size=3, padding=2, bias=False, padding_mode=\"circular\")"
        ],
        "pos_line": [
            "+self.angular_smooth = nn.Conv1d(1, 1, kernel_size=3, padding=1, bias=False, padding_mode=\"circular\")"
        ],
        "core_change": "-self.angular_smooth = nn.Conv1d(1, 1, kernel_size=3, padding=2, bias=False, padding_mode=\"circular\") +self.angular_smooth = nn.Conv1d(1, 1, kernel_size=3, padding=1, bias=False, padding_mode=\"circular\")",
        "core_API": "Conv1d"
    },
    {
        "commit_hash": "0e6033c12862db1d1838282421d3dc47f7ad7a86",
        "index": "2c6f73a..bf08ea7 100755",
        "commit_message": "fix type issue\n\n",
        "file": "deep-learning-for-image-processing.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def build_targets(p, targets, model):",
            "# Build targets for compute_loss(), input targets(image_idx,class,x,y,w,h)",
            "nt = targets.shape[0]",
            "tcls, tbox, indices, anch = [], [], [], []",
            "-    gain = torch.ones(6, device=targets.device)  # normalized to gridspace gain",
            "+    gain = torch.ones(6, device=targets.device).long()  # normalized to gridspace gain",
            "",
            "multi_gpu = type(model) in (nn.parallel.DataParallel, nn.parallel.DistributedDataParallel)",
            "for i, j in enumerate(model.yolo_layers):  # j: [89, 101, 113]"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=68596)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=68597)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=68598)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'long'), position=2, insert_id=68599)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=68600)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=68601)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 3577,
        "neg_line": [
            "-gain = torch.ones(6, device=targets.device)  # normalized to gridspace gain"
        ],
        "pos_line": [
            "+gain = torch.ones(6, device=targets.device).long()  # normalized to gridspace gain"
        ],
        "core_change": "-gain = torch.ones(6, device=targets.device)  # normalized to gridspace gain +gain = torch.ones(6, device=targets.device).long()  # normalized to gridspace gain",
        "core_API": "ones"
    },
    {
        "commit_hash": "6ce55e4b011275e43404034832b40648b1483ff6",
        "index": "87281282..3daff29d 100644",
        "commit_message": "Small fixes\n\nSummary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/835\n\nDifferential Revision: D16904038\n\nPulled By: myleott\n\nfbshipit-source-id: 2c9d0b913f8d688297ac80fcabd905bd1397f66a\n\n",
        "file": "fairseq.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class LightconvLayer(nn.Module):",
            "weight = weight.view(1, H, K).expand(T*B, H, K).contiguous().view(T*B*H, K, 1)",
            "",
            "weight = F.dropout(weight, self.weight_dropout, training=self.training)",
            "-            output = torch.bmm(x_unfold, weight) # T*B*H x R x 1",
            "+            output = torch.bmm(x_unfold, weight)  # T*B*H x R x 1",
            "output = output.view(T, B, C)",
            "return output"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 3578,
        "neg_line": [
            "-output = torch.bmm(x_unfold, weight) # T*B*H x R x 1"
        ],
        "pos_line": [
            "+output = torch.bmm(x_unfold, weight)  # T*B*H x R x 1"
        ],
        "core_change": "-output = torch.bmm(x_unfold, weight) # T*B*H x R x 1 +output = torch.bmm(x_unfold, weight)  # T*B*H x R x 1",
        "core_API": "view"
    },
    {
        "commit_hash": "5f81d4de234f579bdc988e8346da14b37a3af160",
        "index": "f533a86..653da40 100644",
        "commit_message": "Move DeiT to own file, vit getting crowded. Working towards fixing #1029, make pooling interface for transformers and mlp closer to convnets. Still working through some details...\n\n",
        "file": "pytorch-image-models.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class CrossViT(nn.Module):",
            "",
            "# NOTE: was before branch token section, move to here to assure all branch token are before layer norm",
            "xs = [norm(xs[i]) for i, norm in enumerate(self.norm)]",
            "-        return [xo[:, 0] for xo in xs]",
            "+        return xs",
            "",
            "def forward(self, x):",
            "xs = self.forward_features(x)",
            "-        ce_logits = [head(xs[i]) for i, head in enumerate(self.head)]",
            "+        ce_logits = [head(xs[i][:, 0]) for i, head in enumerate(self.head)]",
            "if not isinstance(self.head[0], nn.Identity):",
            "ce_logits = torch.mean(torch.stack(ce_logits, dim=0), dim=0)",
            "return ce_logits"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=return_statement), node=ASTNode(type=identifier, text=xs), position=1)",
            "Insert(target_node=ASTNode(type=argument_list), node=('subscript', None), position=1, insert_id=893867)",
            "Move(target_node=IN(type=subscript), node=ASTNode(type=subscript), position=0)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=893868)",
            "Move(target_node=IN(type=subscript), node=ASTNode(type=slice), position=2)",
            "Insert(target_node=IN(type=subscript), node=(',', ','), position=3, insert_id=893869)",
            "Insert(target_node=IN(type=subscript), node=('integer', '0'), position=4, insert_id=893870)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=5, insert_id=893871)",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=identifier, text=xo))",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=integer, text=0))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))",
            "Delete(target_node=ASTNode(type=for, text=for))",
            "Delete(target_node=ASTNode(type=identifier, text=xo))",
            "Delete(target_node=ASTNode(type=in, text=in))",
            "Delete(target_node=ASTNode(type=for_in_clause))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=list_comprehension))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 21,
        "number": 3585,
        "neg_line": [
            "-return [xo[:, 0] for xo in xs]",
            "-ce_logits = [head(xs[i]) for i, head in enumerate(self.head)]"
        ],
        "pos_line": [
            "+return xs",
            "+ce_logits = [head(xs[i][:, 0]) for i, head in enumerate(self.head)]"
        ],
        "core_change": "-return [xo[:, 0] for xo in xs] +return xs -ce_logits = [head(xs[i]) for i, head in enumerate(self.head)] +ce_logits = [head(xs[i][:, 0]) for i, head in enumerate(self.head)]",
        "core_API": "forward_features"
    },
    {
        "commit_hash": "a7a92a88ef62f96e2b3aa3ffab5b14236f3e00a6",
        "index": "372f3dd08..f8d5c0f91 100644",
        "commit_message": "Fix style\n\n",
        "file": "PySyft.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def testtanh():",
            "3.3883e02,",
            "]",
            ")",
            "-",
            "-    #allclose function to compare the expected values and approximations with fixed precision",
            "-    assert torch.allclose(expected, Ptensor.tanh(x),1e-03)",
            "",
            "+    # allclose function to compare the expected values and approximations with fixed precision",
            "+    assert torch.allclose(expected, Ptensor.tanh(x), 1e-03)"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 3587,
        "neg_line": [
            "-",
            "-#allclose function to compare the expected values and approximations with fixed precision",
            "-assert torch.allclose(expected, Ptensor.tanh(x),1e-03)"
        ],
        "pos_line": [
            "+# allclose function to compare the expected values and approximations with fixed precision",
            "+assert torch.allclose(expected, Ptensor.tanh(x), 1e-03)"
        ],
        "core_change": "- -#allclose function to compare the expected values and approximations with fixed precision -assert torch.allclose(expected, Ptensor.tanh(x),1e-03) +# allclose function to compare the expected values and approximations with fixed precision +assert torch.allclose(expected, Ptensor.tanh(x), 1e-03)",
        "core_API": "allclose"
    },
    {
        "commit_hash": "12eb528b5a88d66b81957139ae452acec99a083a",
        "index": "68084f3b7..82bdf8cee 100755",
        "commit_message": "[CI ] Remove `past` in favor of `pat_key_values` (#21443)\n\n* fix past renamed to past_key_value\n\n* update more `past`that were ski^d\n\n* fixup\n\n* remove changes made to rag\n\n* refactor `_reorder_cache` to use `past_key_values`\n\n* fix git `prepare_inputs_for_generation` to pass tests when false is needed in use_cache\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class GPTNeoForCausalLM(GPTNeoPreTrainedModel):",
            "\"\"\"",
            "return tuple(",
            "tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past)",
            "-            for layer_past in past",
            "+            for layer_past in past_key_values",
            ")"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=past), value='past_key_values')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 3589,
        "neg_line": [
            "-for layer_past in past"
        ],
        "pos_line": [
            "+for layer_past in past_key_values"
        ],
        "core_change": "-for layer_past in past +for layer_past in past_key_values",
        "core_API": "index_select"
    },
    {
        "commit_hash": "593dd8cf58fcd2fc0ba48aa81724cd82b904f7cf",
        "index": "6a5cc119..190b3b92 100644",
        "commit_message": "Fix special values overflow/underflow for amp (#3901)\n\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ScalarMix(torch.nn.Module):",
            "variance = (",
            "torch.sum(((tensor_masked - mean) * broadcast_mask) ** 2) / num_elements_not_masked",
            ")",
            "-            return (tensor - mean) / torch.sqrt(variance + 1e-12)",
            "+            return (tensor - mean) / torch.sqrt(variance + util.tiny_value_of_dtype(variance.dtype))",
            "",
            "normed_weights = torch.nn.functional.softmax(",
            "torch.cat([parameter for parameter in self.scalar_parameters]), dim=0"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=binary_operator), node=('call', None), position=2, insert_id=18835)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=18836)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=18837)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'util'), position=0, insert_id=18838)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=18839)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tiny_value_of_dtype'), position=2, insert_id=18840)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=18841)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=18842)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=18843)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'variance'), position=0, insert_id=18844)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=18845)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dtype'), position=2, insert_id=18846)",
            "Delete(target_node=ASTNode(type=float, text=1e-12))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 13,
        "number": 3593,
        "neg_line": [
            "-return (tensor - mean) / torch.sqrt(variance + 1e-12)"
        ],
        "pos_line": [
            "+return (tensor - mean) / torch.sqrt(variance + util.tiny_value_of_dtype(variance.dtype))"
        ],
        "core_change": "-return (tensor - mean) / torch.sqrt(variance + 1e-12) +return (tensor - mean) / torch.sqrt(variance + util.tiny_value_of_dtype(variance.dtype))",
        "core_API": "sum"
    },
    {
        "commit_hash": "15532173d42abace8e2aa122ef51b1421644023f",
        "index": "a056996a..b3eadf68 100644",
        "commit_message": "cuda bugfix\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def coalesce(edge_index, edge_attr=None, num_nodes=None):",
            "_, perm = unique(index)",
            "edge_index = edge_index[:, perm]",
            "else:",
            "-        sparse = getattr(torch.sparse, edge_attr.type().split('.')[-1])",
            "+        t = torch.cuda if edge_attr.is_cuda else torch",
            "+        sparse = getattr(t.sparse, edge_attr.type().split('.')[-1])",
            "n = num_nodes",
            "size = torch.Size([n, n] + list(edge_attr.size())[1:])",
            "adj = sparse(edge_index, edge_attr, size).coalesce()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=1069620)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1069621)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=identifier, text=else), position=0)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=:, text=:), position=1)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=type), position=2)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=3, insert_id=1069622)",
            "Insert(target_node=IN(type=assignment), node=('conditional_expression', None), position=4, insert_id=1069623)",
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=identifier, text=sparse), position=0)",
            "Insert(target_node=ASTNode(type=type), node=('identifier', 't'), position=0, insert_id=1069624)",
            "Insert(target_node=IN(type=conditional_expression), node=('attribute', None), position=0, insert_id=1069625)",
            "Insert(target_node=IN(type=conditional_expression), node=('if', 'if'), position=1, insert_id=1069626)",
            "Insert(target_node=IN(type=conditional_expression), node=('attribute', None), position=2, insert_id=1069627)",
            "Insert(target_node=IN(type=conditional_expression), node=('else', 'else'), position=3, insert_id=1069628)",
            "Insert(target_node=IN(type=conditional_expression), node=('identifier', 'torch'), position=4, insert_id=1069629)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1069630)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1069631)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cuda'), position=2, insert_id=1069632)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'edge_attr'), position=0, insert_id=1069633)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1069634)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'is_cuda'), position=2, insert_id=1069635)",
            "Update(target_node=ASTNode(type=identifier, text=torch), value='t')"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 21,
        "number": 3598,
        "neg_line": [
            "-sparse = getattr(torch.sparse, edge_attr.type().split('.')[-1])"
        ],
        "pos_line": [
            "+t = torch.cuda if edge_attr.is_cuda else torch",
            "+sparse = getattr(t.sparse, edge_attr.type().split('.')[-1])"
        ],
        "core_change": "-sparse = getattr(torch.sparse, edge_attr.type().split('.')[-1]) +t = torch.cuda if edge_attr.is_cuda else torch +sparse = getattr(t.sparse, edge_attr.type().split('.')[-1])",
        "core_API": "type"
    },
    {
        "commit_hash": "547103d4c99415530b11f126c632c24d6246c09f",
        "index": "76bb247..068f81e 100644",
        "commit_message": "Fix casting when computing negative ROI count.\n",
        "file": "Mask_RCNN.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def fpn_classifier_graph(rois, feature_maps,",
            "name=\"mrcnn_class_conv1\")(x)",
            "x = KL.TimeDistributed(BatchNorm(axis=3), name='mrcnn_class_bn1')(x)",
            "x = KL.Activation('relu')(x)",
            "-    # x = KL.Dropout(0.5)(x)",
            "x = KL.TimeDistributed(KL.Conv2D(1024, (1, 1)),",
            "name=\"mrcnn_class_conv2\")(x)",
            "x = KL.TimeDistributed(BatchNorm(axis=3),"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 3599,
        "neg_line": [
            "-# x = KL.Dropout(0.5)(x)"
        ],
        "pos_line": [],
        "core_change": "-# x = KL.Dropout(0.5)(x)",
        "core_API": "TimeDistributed"
    },
    {
        "commit_hash": "7dfb2f886da7efb73e4f526ee46753ac1284ecad",
        "index": "1481eeb2..edc21998 100644",
        "commit_message": "Fix mypy issue with torch.linalg (#1236)\n\n* add a compatibiliy stub for torch solve\n\n* change missing _torch_solve_cast calls\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* bump pytorch version to 1.7.1\n\n* Fix mypy issue with torch.linalg\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "import torch",
            "from packaging import version",
            "",
            "if version.parse(torch.__version__) > version.parse(\"1.7.1\"):",
            "-    from torch.linalg import solve",
            "+    # TODO: remove the type: ignore once Python 3.6 is deprecated.",
            "+    # It turns out that Pytorch has no attribute `torch.linalg` for",
            "+    # Python 3.6 / PyTorch 1.7.0, 1.7.1",
            "+    from torch.linalg import solve  # type: ignore",
            "else:",
            "from torch import solve as _solve"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 4,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 3600,
        "neg_line": [
            "-from torch.linalg import solve"
        ],
        "pos_line": [
            "+# TODO: remove the type: ignore once Python 3.6 is deprecated.",
            "+# It turns out that Pytorch has no attribute `torch.linalg` for",
            "+# Python 3.6 / PyTorch 1.7.0, 1.7.1",
            "+from torch.linalg import solve  # type: ignore"
        ],
        "core_change": "-from torch.linalg import solve +# TODO: remove the type: ignore once Python 3.6 is deprecated. +# It turns out that Pytorch has no attribute `torch.linalg` for +# Python 3.6 / PyTorch 1.7.0, 1.7.1 +from torch.linalg import solve  # type: ignore",
        "core_API": "parse"
    },
    {
        "commit_hash": "86ba09af5eea6fb605c39da57297b6d5d3b2c77d",
        "index": "7c7fb1a..ce45885 100644",
        "commit_message": "Disable tests in TF1 using tf2.enabled().\n\nAlso fix image conversion in mnist_export_v2 to prevent float overflow in dbg mode.\n\nPiperOrigin-RevId: 307367586\n\n",
        "file": "hub.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class ExportTokenEmbeddingTest(tf.test.TestCase):",
            "",
            "if __name__ == \"__main__\":",
            "# This test is only supported in TF 2.0+.",
            "-  if LooseVersion(tf.__version__) >= LooseVersion(\"2.0.0-beta0\"):",
            "+  if tf.executing_eagerly():",
            "logging.info(\"Using TF version: %s\", tf.__version__)",
            "tf.test.main()",
            "else:"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=if_statement), node=ASTNode(type=call), position=1)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=argument_list), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=__version__), value='executing_eagerly')",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=(, text=(), position=0)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=), text=)), position=1)",
            "Delete(target_node=ASTNode(type=identifier, text=LooseVersion))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=>=, text=>=))",
            "Delete(target_node=ASTNode(type=identifier, text=LooseVersion))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=string, text=\"2.0.0-beta0\"))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=comparison_operator))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 15,
        "number": 3603,
        "neg_line": [
            "-if LooseVersion(tf.__version__) >= LooseVersion(\"2.0.0-beta0\"):"
        ],
        "pos_line": [
            "+if tf.executing_eagerly():"
        ],
        "core_change": "-if LooseVersion(tf.__version__) >= LooseVersion(\"2.0.0-beta0\"): +if tf.executing_eagerly():",
        "core_API": "executing_eagerly"
    },
    {
        "commit_hash": "0b0cd2e6688495ffd33beda94a11c5733b53e79f",
        "index": "744345bb..af50403b 100644",
        "commit_message": "Fix unit tests for GERMEVAL dataset rename\n\n",
        "file": "flair.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "def test_load_no_dev_data_explicit(tasks_base_path):",
            "",
            "",
            "def test_multi_corpus(tasks_base_path):",
            "-    corpus_1 = flair.datasets.ColumnCorpus(tasks_base_path / \"germeval_14\", column_format={0: \"text\", 2: \"ner\"})",
            "+    corpus_1 = flair.datasets.ColumnCorpus(tasks_base_path / \"ner_german_germeval\", column_format={0: \"text\", 2: \"ner\"})",
            "",
            "corpus_2 = flair.datasets.ColumnCorpus(tasks_base_path / \"fashion\", column_format={0: \"text\", 2: \"ner\"})",
            "# get two corpora as one"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"germeval_14\"), value='\"ner_german_germeval\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 3604,
        "neg_line": [
            "-corpus_1 = flair.datasets.ColumnCorpus(tasks_base_path / \"germeval_14\", column_format={0: \"text\", 2: \"ner\"})"
        ],
        "pos_line": [
            "+corpus_1 = flair.datasets.ColumnCorpus(tasks_base_path / \"ner_german_germeval\", column_format={0: \"text\", 2: \"ner\"})"
        ],
        "core_change": "-corpus_1 = flair.datasets.ColumnCorpus(tasks_base_path / \"germeval_14\", column_format={0: \"text\", 2: \"ner\"}) +corpus_1 = flair.datasets.ColumnCorpus(tasks_base_path / \"ner_german_germeval\", column_format={0: \"text\", 2: \"ner\"})",
        "core_API": "ColumnCorpus"
    },
    {
        "commit_hash": "c402309470740e17973eeee22a7b90123f85b310",
        "index": "c0a9e6d6e..2d5bf8402 100644",
        "commit_message": "fix CI error\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class FairSeqWav2Vec2Encoder(AbsEncoder):",
            "logging.info(\"Start fine-tuning wav2vec parameters!\")",
            "",
            "with torch.no_grad() if not ft else contextlib.nullcontext():",
            "-            enc_outputs = self.encoders(",
            "-                xs_pad,",
            "-                masks,",
            "-                features_only=True,",
            "-            )",
            "+            enc_outputs = self.encoders(xs_pad, masks, features_only=True)",
            "",
            "xs_pad = enc_outputs[\"x\"]  # (B,T,C),",
            "masks = enc_outputs[\"padding_mask\"]  # (B, T)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 1,
        "minus_line": 5,
        "AST_diff_line": 1,
        "number": 3607,
        "neg_line": [
            "-enc_outputs = self.encoders(",
            "-xs_pad,",
            "-masks,",
            "-features_only=True,",
            "-)"
        ],
        "pos_line": [
            "+enc_outputs = self.encoders(xs_pad, masks, features_only=True)"
        ],
        "core_change": "-enc_outputs = self.encoders( -xs_pad, -masks, -features_only=True, -) +enc_outputs = self.encoders(xs_pad, masks, features_only=True)",
        "core_API": "info"
    },
    {
        "commit_hash": "80b40ce138d7d40ceccfeae49000a8d75e2cef55",
        "index": "d0cbbf3..4f4a76a 100644",
        "commit_message": "Fix incorrect formatting\n\n",
        "file": "autokeras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class BertEncoder(tf.keras.layers.Layer):",
            "position_embeddings = self._position_embedding_layer(word_embeddings)",
            "type_embeddings = self._type_embedding_layer(type_ids)",
            "",
            "-        embeddings = self._add([word_embeddings, position_embeddings, type_embeddings])",
            "+        embeddings = self._add(",
            "+            [word_embeddings, position_embeddings, type_embeddings]",
            "+        )",
            "",
            "embeddings = self._layer_norm(embeddings)",
            "embeddings = self._dropout(embeddings)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 3609,
        "neg_line": [
            "-embeddings = self._add([word_embeddings, position_embeddings, type_embeddings])"
        ],
        "pos_line": [
            "+embeddings = self._add(",
            "+[word_embeddings, position_embeddings, type_embeddings]",
            "+)"
        ],
        "core_change": "-embeddings = self._add([word_embeddings, position_embeddings, type_embeddings]) +embeddings = self._add( +[word_embeddings, position_embeddings, type_embeddings] +)",
        "core_API": "_position_embedding_layer"
    },
    {
        "commit_hash": "584e9cd4cb2812c722b65a79309cc78ee9d5b552",
        "index": "8714238c..ca02f687 100644",
        "commit_message": "Fix hyperparam and optimizer issue in distributed trainer (#431)\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class GraphVarParam(HyperParam):",
            "",
            "def setup_graph(self):",
            "\"\"\" Will setup the assign operator for that variable. \"\"\"",
            "-        all_vars = tf.global_variables()",
            "+        all_vars = tf.all_variables()",
            "for v in all_vars:",
            "if v.name == self.var_name:",
            "self.var = v"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=global_variables), value='all_variables')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 3614,
        "neg_line": [
            "-all_vars = tf.global_variables()"
        ],
        "pos_line": [
            "+all_vars = tf.all_variables()"
        ],
        "core_change": "-all_vars = tf.global_variables() +all_vars = tf.all_variables()",
        "core_API": "global_variables"
    },
    {
        "commit_hash": "ab69f6d5911a4ee48f6657da0367787032a3e560",
        "index": "57007a1b..b28cf28d 100644",
        "commit_message": "minor change to fix errors\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class GroupNorm(Layer):",
            "channels = inputs_shape[-1]",
            "self.int_shape = tf.concat(",
            "[#tf.shape(input=self.inputs)[0:3],",
            "-                inputs_shape[0:3]",
            "-                 tf.convert_to_tensor(value=[self.groups, channels // self.groups])], axis=0",
            "+                inputs_shape[0:3],",
            "+                tf.convert_to_tensor(value=[self.groups, channels // self.groups])], axis=0",
            ")",
            "elif self.data_format == 'channels_first':",
            "channels = shape[1]"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=list), node=ASTNode(type=subscript), position=1)",
            "Insert(target_node=ASTNode(type=list), node=(',', ','), position=2, insert_id=2258906)",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=tf), position=0)",
            "Delete(target_node=ASTNode(type=ERROR))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 4,
        "number": 3615,
        "neg_line": [
            "-inputs_shape[0:3]",
            "-tf.convert_to_tensor(value=[self.groups, channels // self.groups])], axis=0"
        ],
        "pos_line": [
            "+inputs_shape[0:3],",
            "+tf.convert_to_tensor(value=[self.groups, channels // self.groups])], axis=0"
        ],
        "core_change": "-inputs_shape[0:3] -tf.convert_to_tensor(value=[self.groups, channels // self.groups])], axis=0 +inputs_shape[0:3], +tf.convert_to_tensor(value=[self.groups, channels // self.groups])], axis=0",
        "core_API": "concat"
    },
    {
        "commit_hash": "e4bcd6ac00c0721a5fec5154d0d9297241763884",
        "index": "3593010..ee44206 100644",
        "commit_message": "fix https://github.com/Sanster/lama-cleaner/issues/230\n\n",
        "file": "lama-cleaner.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def md5sum(filename):",
            "",
            "",
            "def switch_mps_device(model_name, device):",
            "-    if model_name not in MPS_SUPPORT_MODELS and (",
            "-        device == \"mps\" or device == torch.device(\"mps\")",
            "-    ):",
            "+    if model_name not in MPS_SUPPORT_MODELS and str(device) == \"mps\":",
            "logger.info(f\"{model_name} not support mps, switch to cpu\")",
            "return torch.device(\"cpu\")",
            "return device"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=boolean_operator), node=ASTNode(type=comparison_operator), position=2)",
            "Insert(target_node=ASTNode(type=comparison_operator), node=('==', '=='), position=2, insert_id=481896)",
            "Insert(target_node=ASTNode(type=comparison_operator), node=('string', '\"mps\"'), position=3, insert_id=481897)",
            "Update(target_node=ASTNode(type=identifier, text=torch), value='str')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=torch), position=0)",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'device'), position=1, insert_id=481898)",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=device))",
            "Delete(target_node=ASTNode(type===, text===))",
            "Delete(target_node=ASTNode(type=string, text=\"mps\"))",
            "Delete(target_node=ASTNode(type=comparison_operator))",
            "Delete(target_node=ASTNode(type=or, text=or))",
            "Delete(target_node=ASTNode(type=identifier, text=device))",
            "Delete(target_node=ASTNode(type===, text===))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=device))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=string, text=\"mps\"))",
            "Delete(target_node=ASTNode(type=boolean_operator))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=parenthesized_expression))"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 21,
        "number": 3616,
        "neg_line": [
            "-if model_name not in MPS_SUPPORT_MODELS and (",
            "-device == \"mps\" or device == torch.device(\"mps\")",
            "-):"
        ],
        "pos_line": [
            "+if model_name not in MPS_SUPPORT_MODELS and str(device) == \"mps\":"
        ],
        "core_change": "-if model_name not in MPS_SUPPORT_MODELS and ( -device == \"mps\" or device == torch.device(\"mps\") -): +if model_name not in MPS_SUPPORT_MODELS and str(device) == \"mps\":",
        "core_API": "device"
    },
    {
        "commit_hash": "98cc35b6a8e53e15829ce64fd8da835db0c61da9",
        "index": "276a1335..5d656fe8 100755",
        "commit_message": "Abstract accelerator (step 3) (#2677)\n\n* Integrate accelerator abstraction interface into deepspeed/\n\n* Fix error message in fp16/fused_optimizer\n\n* fix error message in fp16/unfused_optimizer.py\n\n* assign get_accelerator().pin_memory() result to input Tensor name\n\n* no need to check cuda and whether nvtx supported\n\n* move try-except into inner most block\n\n* call Event() and Stream() in get_accelerator() for data type\n\n* Make Stream and Event as properties of abstract interface so they can be used as data type in deepspeed\n\n* Apply op_builder backend api change from #2705 from @jeffra\n\n* fix tests where Builder NAME is used\n\n* keep original ...Builder.NAME interface instead of ...Builder().NAME interface\n\n* fix builder closure for installation\n\n* fix randomltd builder\n\n* add comments to clarify create_op_builder and get_op_builder\n\n* fix compatibility with pip install -e\n\nCo-authored-by: Cheng Li <pistasable@gmail.com>\nCo-authored-by: Olatunji Ruwase <olruwase@microsoft.com>\n",
        "file": "DeepSpeed.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "cuda",
        "change": [
            "class Autotuner:",
            "return False",
            "",
            "def get_gpu_memory_info(self):",
            "-        return torch.cuda.get_device_properties(0).total_memory",
            "+        return get_accelerator().total_memory()",
            "",
            "def get_activation_memory_per_gpu(self):",
            "if self.model_info and \"activation_mem_per_gpu\" in self.model_info:"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=ASTNode(type=return_statement), node=ASTNode(type=call), position=1)",
            "Insert(target_node=ASTNode(type=attribute), node=('call', None), position=0, insert_id=1816186)",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=total_memory), position=3)",
            "Update(target_node=ASTNode(type=identifier, text=torch), value='get_accelerator')",
            "Move(target_node=IN(type=call), node=ASTNode(type=identifier, text=torch), position=0)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1816187)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1816188)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1816189)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=cuda))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=identifier, text=get_device_properties))",
            "Delete(target_node=ASTNode(type=integer, text=0))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 15,
        "number": 3619,
        "neg_line": [
            "-return torch.cuda.get_device_properties(0).total_memory"
        ],
        "pos_line": [
            "+return get_accelerator().total_memory()"
        ],
        "core_change": "-return torch.cuda.get_device_properties(0).total_memory +return get_accelerator().total_memory()",
        "core_API": "get_device_properties"
    },
    {
        "commit_hash": "92f9f1966b6589497e2f12044f01528b955b1c32",
        "index": "d74bdb2b..ee659834 100644",
        "commit_message": "fix target batch\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Set(Data):",
            "target = self.target[s1[i]:s1[i + 1]]",
            "else:",
            "target = self.target[i]",
            "-            target = target.view(1, -1).squeeze(1)",
            "+",
            "+            if torch.is_tensor(target):",
            "+                target = target.view(1, -1).squeeze(1)",
            "",
            "return Data(input, pos, index, weight, target)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=3, insert_id=1079723)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=1079724)",
            "Insert(target_node=IN(type=if_statement), node=('call', None), position=1, insert_id=1079725)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=1079726)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=1079727)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1079728)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1079729)",
            "Move(target_node=IN(type=block), node=ASTNode(type=expression_statement), position=0)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1079730)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1079731)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'is_tensor'), position=2, insert_id=1079732)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1079733)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'target'), position=1, insert_id=1079734)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1079735)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 14,
        "number": 3621,
        "neg_line": [
            "-target = target.view(1, -1).squeeze(1)"
        ],
        "pos_line": [
            "+",
            "+if torch.is_tensor(target):",
            "+target = target.view(1, -1).squeeze(1)"
        ],
        "core_change": "-target = target.view(1, -1).squeeze(1) + +if torch.is_tensor(target): +target = target.view(1, -1).squeeze(1)",
        "core_API": "view"
    },
    {
        "commit_hash": "523d5e03d86c26267ee6bdf17dd20f6ce6bdadd7",
        "index": "bb78aa83..f4bf22a3 100644",
        "commit_message": "Sync for 4.0 release (#961)\n\n* sync for 4.0 release\n\n* fix py2 issue in conversion of torch std op\n",
        "file": "coremltools.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "models",
        "change": [
            "class ImagePreprocessingPass(unittest.TestCase):",
            "x4 = mb.add(x=x1, y=x3)",
            "return mb.relu(x=x4)",
            "",
            "-        proto = converter._convert(prog, inputs=[ImageType(name=\"x\", shape=(10, 20, 30, 3), channel_first=False)], convert_from=\"mil\", convert_to=\"nn_proto\")",
            "-        model = models.MLModel(proto)",
            "-        assert model is not None",
            "-        assert len(model._spec.neuralNetwork.layers) == 3",
            "+        mlmodel = ct.convert(prog,",
            "+            inputs=[ImageType(name=\"x\", shape=(10, 20, 30, 3),",
            "+              channel_first=False)],",
            "+            source=\"mil\", convert_to=\"nn_proto\")",
            "+        assert mlmodel is not None",
            "+        assert len(mlmodel.get_spec().neuralNetwork.layers) == 3"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=proto), value='mlmodel')",
            "Update(target_node=ASTNode(type=identifier, text=model), value='mlmodel')",
            "Update(target_node=ASTNode(type=identifier, text=converter), value='ct')",
            "Update(target_node=ASTNode(type=identifier, text=_convert), value='convert')",
            "Update(target_node=ASTNode(type=identifier, text=convert_from), value='source')",
            "Insert(target_node=ASTNode(type=attribute), node=('call', None), position=0, insert_id=2652293)",
            "Move(target_node=IN(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2652294)",
            "Update(target_node=ASTNode(type=identifier, text=model), value='mlmodel')",
            "Update(target_node=ASTNode(type=identifier, text=_spec), value='get_spec')",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2652295)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=2652296)",
            "Delete(target_node=ASTNode(type=identifier, text=model))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=models))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=MLModel))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=proto))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 6,
        "minus_line": 4,
        "AST_diff_line": 25,
        "number": 3623,
        "neg_line": [
            "-proto = converter._convert(prog, inputs=[ImageType(name=\"x\", shape=(10, 20, 30, 3), channel_first=False)], convert_from=\"mil\", convert_to=\"nn_proto\")",
            "-model = models.MLModel(proto)",
            "-assert model is not None",
            "-assert len(model._spec.neuralNetwork.layers) == 3"
        ],
        "pos_line": [
            "+mlmodel = ct.convert(prog,",
            "+inputs=[ImageType(name=\"x\", shape=(10, 20, 30, 3),",
            "+channel_first=False)],",
            "+source=\"mil\", convert_to=\"nn_proto\")",
            "+assert mlmodel is not None",
            "+assert len(mlmodel.get_spec().neuralNetwork.layers) == 3"
        ],
        "core_change": "-proto = converter._convert(prog, inputs=[ImageType(name=\"x\", shape=(10, 20, 30, 3), channel_first=False)], convert_from=\"mil\", convert_to=\"nn_proto\") -model = models.MLModel(proto) -assert model is not None -assert len(model._spec.neuralNetwork.layers) == 3 +mlmodel = ct.convert(prog, +inputs=[ImageType(name=\"x\", shape=(10, 20, 30, 3), +channel_first=False)], +source=\"mil\", convert_to=\"nn_proto\") +assert mlmodel is not None +assert len(mlmodel.get_spec().neuralNetwork.layers) == 3",
        "core_API": "add"
    },
    {
        "commit_hash": "37e98945728f9961fd33d598a954358d79988c28",
        "index": "a2a59704..5a2b0200 100644",
        "commit_message": "fix flake8 style in tensorpack/\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class CheckGradient(MapGradient):",
            "",
            "def _mapper(self, grad, var):",
            "# this is very slow.... see #3649",
            "-        #op = tf.Assert(tf.reduce_all(tf.is_finite(var)), [var], summarize=100)",
            "+        # op = tf.Assert(tf.reduce_all(tf.is_finite(var)), [var], summarize=100)",
            "grad = tf.check_numerics(grad, 'CheckGradient-' + var.op.name)",
            "return grad"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 3629,
        "neg_line": [
            "-#op = tf.Assert(tf.reduce_all(tf.is_finite(var)), [var], summarize=100)"
        ],
        "pos_line": [
            "+# op = tf.Assert(tf.reduce_all(tf.is_finite(var)), [var], summarize=100)"
        ],
        "core_change": "-#op = tf.Assert(tf.reduce_all(tf.is_finite(var)), [var], summarize=100) +# op = tf.Assert(tf.reduce_all(tf.is_finite(var)), [var], summarize=100)",
        "core_API": "Assert"
    },
    {
        "commit_hash": "681823a9f50e10e61d407436b9cafc1ef2f92c84",
        "index": "9c241ba420..e0656cb33e 100644",
        "commit_message": "Fixed lint formatting and type hints removal\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "functional",
        "change": [
            "def test_categorical_crossentropy(",
            "native_array,",
            "fw,",
            "):",
            "-    y_true = ivy.array(y_true, dtype = ivy.float32)",
            "+    y_true = ivy.array(y_true, dtype=ivy.float32)",
            "dtype, y_pred = dtype_y_pred",
            "",
            "# Perform softmax on prediction if it's not a probability distribution."
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 3631,
        "neg_line": [
            "-y_true = ivy.array(y_true, dtype = ivy.float32)"
        ],
        "pos_line": [
            "+y_true = ivy.array(y_true, dtype=ivy.float32)"
        ],
        "core_change": "-y_true = ivy.array(y_true, dtype = ivy.float32) +y_true = ivy.array(y_true, dtype=ivy.float32)",
        "core_API": "array"
    },
    {
        "commit_hash": "6209d7d6b2c41fccb01e00671261be80ba86029a",
        "index": "66bec3b3..6cac9800 100644",
        "commit_message": "Fix eval_lm (fixes #2083) and a few other small things (#2100)\n\nSummary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/2100\n\nReviewed By: ngoyal2707\n\nDifferential Revision: D21456309\n\nPulled By: myleott\n\nfbshipit-source-id: 291711589fca9f158e0fdbf01194da3e66fbd0aa\n\n",
        "file": "fairseq.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "cuda",
        "change": [
            "def add_dataset_args(parser, train=False, gen=False):",
            "return group",
            "",
            "",
            "-def add_distributed_training_args(parser):",
            "+def add_distributed_training_args(parser, default_world_size=None):",
            "group = parser.add_argument_group(\"Distributed training\")",
            "# fmt: off",
            "+    if default_world_size is None:",
            "+        default_world_size = max(1, torch.cuda.device_count())",
            "group.add_argument('--distributed-world-size', type=int, metavar='N',",
            "-                       default=max(1, torch.cuda.device_count()),",
            "+                       default=default_world_size,",
            "help='total number of GPUs across all nodes (default: all visible GPUs)')",
            "group.add_argument('--distributed-rank', default=0, type=int,",
            "help='rank of the current worker')"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=4, insert_id=1830887)",
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=5, insert_id=1830888)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=1830889)",
            "Insert(target_node=IN(type=if_statement), node=('comparison_operator', None), position=1, insert_id=1830890)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=1830891)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=1830892)",
            "Move(target_node=IN(type=expression_statement), node=ASTNode(type=call), position=0)",
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=2, insert_id=1830893)",
            "Insert(target_node=ASTNode(type=parameters), node=('default_parameter', None), position=3, insert_id=1830894)",
            "Insert(target_node=IN(type=comparison_operator), node=('identifier', 'default_world_size'), position=0, insert_id=1830895)",
            "Insert(target_node=IN(type=comparison_operator), node=('is', 'is'), position=1, insert_id=1830896)",
            "Insert(target_node=IN(type=comparison_operator), node=('none', 'None'), position=2, insert_id=1830897)",
            "Move(target_node=IN(type=block), node=ASTNode(type=expression_statement), position=0)",
            "Insert(target_node=IN(type=default_parameter), node=('identifier', 'default_world_size'), position=0, insert_id=1830898)",
            "Insert(target_node=IN(type=default_parameter), node=('=', '='), position=1, insert_id=1830899)",
            "Insert(target_node=IN(type=default_parameter), node=('none', 'None'), position=2, insert_id=1830900)",
            "Insert(target_node=ASTNode(type=expression_statement), node=('assignment', None), position=0, insert_id=1830901)",
            "Update(target_node=ASTNode(type=identifier, text=default), value='default_world_size')",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=identifier, text=default), position=0)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type==, text==), position=1)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=call), position=2)",
            "Insert(target_node=ASTNode(type=keyword_argument), node=('identifier', 'default'), position=0, insert_id=1830902)",
            "Insert(target_node=ASTNode(type=keyword_argument), node=('=', '='), position=1, insert_id=1830903)",
            "Insert(target_node=ASTNode(type=keyword_argument), node=('identifier', 'default_world_size'), position=2, insert_id=1830904)"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 24,
        "number": 3633,
        "neg_line": [
            "-def add_distributed_training_args(parser):",
            "-default=max(1, torch.cuda.device_count()),"
        ],
        "pos_line": [
            "+def add_distributed_training_args(parser, default_world_size=None):",
            "+if default_world_size is None:",
            "+default_world_size = max(1, torch.cuda.device_count())",
            "+default=default_world_size,"
        ],
        "core_change": "-def add_distributed_training_args(parser): +def add_distributed_training_args(parser, default_world_size=None): +if default_world_size is None: +default_world_size = max(1, torch.cuda.device_count()) -default=max(1, torch.cuda.device_count()), +default=default_world_size,",
        "core_API": "add_argument_group"
    },
    {
        "commit_hash": "d247c6d716589b2e308e566e6b49c03276a43fb2",
        "index": "5ee5a6a..293b095 100644",
        "commit_message": "add comments and fix error in tests\n\n",
        "file": "nebullvm.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "models",
        "change": [
            "def install_openvino(with_optimization: bool = True):",
            "\"\"\"Helper function for installing the OpenVino compiler.",
            "",
            "This function just works on intel machines.",
            "+",
            "+    Args:",
            "+        with_optimization (bool): Flag for installing the full openvino engine",
            "+            or limiting the installation to the tools need for inference",
            "+            models.",
            "\"\"\"",
            "processor = cpuinfo.get_cpu_info()[\"brand_raw\"].lower()",
            "if \"intel\" not in processor:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"\"\"Helper function for installing the OpenVino compiler.\n\nThis function just works on intel machines.\n\"\"\"), value='\"\"\"Helper function for installing the OpenVino compiler.\\n\\nThis function just works on intel machines.\\n\\n    Args:\\n        with_optimization (bool): Flag for installing the full openvino engine\\n            or limiting the installation to the tools need for inference\\n            models.\\n\"\"\"')"
        ],
        "plus_line": 4,
        "minus_line": 0,
        "AST_diff_line": 1,
        "number": 3634,
        "neg_line": [],
        "pos_line": [
            "+",
            "+Args:",
            "+with_optimization (bool): Flag for installing the full openvino engine",
            "+or limiting the installation to the tools need for inference",
            "+models."
        ],
        "core_change": "+ +Args: +with_optimization (bool): Flag for installing the full openvino engine +or limiting the installation to the tools need for inference +models.",
        "core_API": "get_cpu_info"
    },
    {
        "commit_hash": "439d7609dfcbca5bafa13b7b6098c4d0e9ecddf3",
        "index": "0fae6205..a0716571 100644",
        "commit_message": "GH-462: fix cuda errors in classifier and unit tests\n\n",
        "file": "flair.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DocumentLSTMEmbeddings(DocumentEmbeddings):",
            "for add in range(longest_token_sequence_in_batch - len(sentence.tokens)):",
            "word_embeddings.append(",
            "torch.zeros(self.length_of_all_token_embeddings,",
            "-                                dtype=torch.float, device=flair.device).unsqueeze(0)",
            "+                                dtype=torch.float).unsqueeze(0)",
            ")",
            "",
            "-            word_embeddings_tensor = torch.cat(word_embeddings, 0)",
            "+            word_embeddings_tensor = torch.cat(word_embeddings, 0).to(flair.device)",
            "",
            "sentence_states = word_embeddings_tensor"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('call', None), position=2, insert_id=240628)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=240629)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=240630)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=240631)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to'), position=2, insert_id=240632)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=240633)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=attribute), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=240634)",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=device))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 13,
        "number": 3635,
        "neg_line": [
            "-dtype=torch.float, device=flair.device).unsqueeze(0)",
            "-word_embeddings_tensor = torch.cat(word_embeddings, 0)"
        ],
        "pos_line": [
            "+dtype=torch.float).unsqueeze(0)",
            "+word_embeddings_tensor = torch.cat(word_embeddings, 0).to(flair.device)"
        ],
        "core_change": "-dtype=torch.float, device=flair.device).unsqueeze(0) +dtype=torch.float).unsqueeze(0) -word_embeddings_tensor = torch.cat(word_embeddings, 0) +word_embeddings_tensor = torch.cat(word_embeddings, 0).to(flair.device)",
        "core_API": "append"
    },
    {
        "commit_hash": "c317aaa3de289fc0a69e6b1001ffc586bb4d9c9a",
        "index": "b9a734ad..a797487d 100644",
        "commit_message": "Refactor library namespaces [pre-release][0.6-rc1] (#1412)\n\n* flake fixes\n\n* initial flake8 fixeS\n\n* remove top level from kornia.color\n\n* kornia filters\n\n* kornia losses\n\n* kornia features\n\n* geomtry and all ok\n\n* removed jit module\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* apply formatting and few fixes\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add keep block for isort\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* skip init\n\n* fix the docs\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* remove old code\n\n* simplify ci workflow\n\n* fix circular dependency\n\n* few format fixes\n\n* fix code format test\n\n* remove kornia.jit imports\n\n* final fixes\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* ci fixes\n\n* add versioneer\n\n* fix pnp import\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* exclude version files from pre-commit\n\n* exclude files precommit\n\n* update to pytorch 1.10 and add fixes\n\n* Update tests_cpu.yml\n\n* Update setup_dev_env.sh\n\n* Update tests_cpu.yml\n\n* undo versioneer\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix no_grad\n\n* Apply suggestions from code review\n\n* fix skip windows tests\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Update test_integrated.py\n\n* Apply suggestions from code review\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def compute_correspond_epilines(points: torch.Tensor, F_mat: torch.Tensor) -> to",
            "if not (len(F_mat.shape) == 3 and F_mat.shape[-2:] == (3, 3)):",
            "raise AssertionError(F_mat.shape)",
            "",
            "-    points_h: torch.Tensor = kornia.convert_points_to_homogeneous(points)",
            "+    points_h: torch.Tensor = convert_points_to_homogeneous(points)",
            "",
            "# project points and retrieve lines components",
            "a, b, c = torch.chunk(F_mat @ points_h.permute(0, 2, 1), dim=1, chunks=3)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=convert_points_to_homogeneous), position=0)",
            "Delete(target_node=ASTNode(type=identifier, text=kornia))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 3638,
        "neg_line": [
            "-points_h: torch.Tensor = kornia.convert_points_to_homogeneous(points)"
        ],
        "pos_line": [
            "+points_h: torch.Tensor = convert_points_to_homogeneous(points)"
        ],
        "core_change": "-points_h: torch.Tensor = kornia.convert_points_to_homogeneous(points) +points_h: torch.Tensor = convert_points_to_homogeneous(points)",
        "core_API": "convert_points_to_homogeneous"
    },
    {
        "commit_hash": "d3cb28886ac68beba9a6646b422a4d727b056c0c",
        "index": "a93c08345..fc5b5a7b0 100755",
        "commit_message": "Not use -1e4 as attn mask (#17306)\n\n* Use torch.finfo(self.dtype).min\n\n* for GPTNeoX\n\n* for Albert\n\n* For Splinter\n\n* Update src/transformers/models/data2vec/modeling_data2vec_audio.py\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* fix -inf used in Bart-like models\n\n* Fix a few remaining -inf\n\n* more fix\n\n* clean up\n\n* For CLIP\n\n* For FSMT\n\n* clean up\n\n* fix test\n\n* Add dtype argument and use it for LayoutLMv3\n\n* update FlaxLongT5Attention\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class MultiHeadSelfAttention(nn.Module):",
            "q = q / math.sqrt(dim_per_head)  # (bs, n_heads, q_length, dim_per_head)",
            "scores = torch.matmul(q, k.transpose(2, 3))  # (bs, n_heads, q_length, k_length)",
            "mask = (mask == 0).view(mask_reshp).expand_as(scores)  # (bs, n_heads, q_length, k_length)",
            "-        scores = scores.masked_fill(mask, torch.tensor(-float(\"inf\")))  # (bs, n_heads, q_length, k_length)",
            "+        scores = scores.masked_fill(",
            "+            mask, torch.tensor(torch.finfo(scores.dtype).min)",
            "+        )  # (bs, n_heads, q_length, k_length)",
            "",
            "weights = nn.functional.softmax(scores, dim=-1)  # (bs, n_heads, q_length, k_length)",
            "weights = self.dropout(weights)  # (bs, n_heads, q_length, k_length)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('(', '('), position=0, insert_id=1196059)",
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=1, insert_id=1196060)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=2, insert_id=1196061)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1196062)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'min'), position=2, insert_id=1196063)",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1196064)",
            "Update(target_node=ASTNode(type=identifier, text=float), value='torch')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=float), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1196065)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'finfo'), position=2, insert_id=1196066)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=(, text=(), position=0)",
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=1, insert_id=1196067)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=), text=)), position=2)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'scores'), position=0, insert_id=1196068)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1196069)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dtype'), position=2, insert_id=1196070)",
            "Delete(target_node=ASTNode(type=-, text=-))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=string, text=\"inf\"))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=unary_operator))"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 22,
        "number": 3641,
        "neg_line": [
            "-scores = scores.masked_fill(mask, torch.tensor(-float(\"inf\")))  # (bs, n_heads, q_length, k_length)"
        ],
        "pos_line": [
            "+scores = scores.masked_fill(",
            "+mask, torch.tensor(torch.finfo(scores.dtype).min)",
            "+)  # (bs, n_heads, q_length, k_length)"
        ],
        "core_change": "-scores = scores.masked_fill(mask, torch.tensor(-float(\"inf\")))  # (bs, n_heads, q_length, k_length) +scores = scores.masked_fill( +mask, torch.tensor(torch.finfo(scores.dtype).min) +)  # (bs, n_heads, q_length, k_length)",
        "core_API": "sqrt"
    },
    {
        "commit_hash": "e855844e40060383d25746626f1e5927b85ba71a",
        "index": "985b64377..c359280aa 100644",
        "commit_message": "Adapt image datasets (#3362)\n\n* Initial commit\n\n* Add basic decoding\n\n* Replace features.Audio with Audio\n\n* Add Image to package reference\n\n* Use np.array\n\n* Update error msg\n\n* Add mode and channel decoding\n\n* Fix return value\n\n* Finish decoding\n\n* Make CI happy\n\n* Some more fixes\n\n* Minor doc fix\n\n* Remove animated option\n\n* Pin version\n\n* Remove unused imports in setup.py\n\n* Add vision requirements to setup.py\n\n* Add initial tests\n\n* Delete other formats\n\n* Make Image feature hashable\n\n* Add more tests\n\n* Support numpy array in alter data check in TypedSequence\n\n* Fix TypedSequence converion\n\n* Finish tests\n\n* Update Image - add ImageExtensionType and supporting functions\n\n* Update encoding functions\n\n* Add support in TypedSequence for ImageExtensionType\n\n* Add tests\n\n* Remove unused import\n\n* Fix doc and style\n\n* Fix doc indentation\n\n* Improve comment\n\n* Return single image instead of dict\n\n* Return PIL Image and not dict\n\n* Encode dict\n\n* Update tests\n\n* Style\n\n* np.ndarray encoding/decoding\n\n* Minor improvements\n\n* PIL Image support in cast_to_python_objects\n\n* Test cast\n\n* Doc fix\n\n* Extension type fixes\n\n* Style\n\n* Use types_mapper in Dataset.to_pandas\n\n* Add pandas extension array for image type\n\n* Update tests\n\n* image type inference\n\n* Remvoe cast_to_python test after Quentin's change\n\n* Improve tests\n\n* Add storage type\n\n* Improve tests\n\n* Test map that returns np.ndarray\n\n* Rename functions\n\n* Add streaming test\n\n* Use image struct in all situations\n\n* Update src/datasets/features/image.py - encode_example type hint\n\nCo-authored-by: Quentin Lhoest <42851186+lhoestq@users.noreply.github.com>\n\n* Update src/datasets/features/image.py -list_image_compression_formats type hint\n\nCo-authored-by: Quentin Lhoest <42851186+lhoestq@users.noreply.github.com>\n\n* Use Image feature in ImageClassification task\n\n* Adapt cats_vs_dogs\n\n* Adapt beans\n\n* Adapt cifar10\n\n* Adapt cifar100\n\n* Add task templates to cifar10 and cifar100\n\n* Adapt FashionMNIST\n\n* Adapt food101\n\n* Adapt mnist datasets\n\n* Adapt head_qa\n\n* Update example in head_qa readme\n\n* Update head_qa dummy data\n\n* Fix streaming in beans and cats_vs_dogs\n\nCo-authored-by: Quentin Lhoest <lhoest.q@gmail.com>\nCo-authored-by: Quentin Lhoest <42851186+lhoestq@users.noreply.github.com>\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class CatsVsDogs(datasets.GeneratorBasedBuilder):",
            "if b\"JFIF\" in f.peek(10):",
            "yield str(i), {",
            "\"image_file_path\": str(filepath),",
            "+                        \"image\": str(filepath),",
            "\"labels\": filepath.parent.name.lower(),",
            "}",
            "continue"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=dictionary), node=('pair', None), position=3, insert_id=1781336)",
            "Insert(target_node=ASTNode(type=dictionary), node=(',', ','), position=4, insert_id=1781337)",
            "Insert(target_node=IN(type=pair), node=('string', '\"image\"'), position=0, insert_id=1781338)",
            "Insert(target_node=IN(type=pair), node=(':', ':'), position=1, insert_id=1781339)",
            "Insert(target_node=IN(type=pair), node=('call', None), position=2, insert_id=1781340)",
            "Insert(target_node=IN(type=call), node=('identifier', 'str'), position=0, insert_id=1781341)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1781342)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1781343)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'filepath'), position=1, insert_id=1781344)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1781345)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 10,
        "number": 3643,
        "neg_line": [],
        "pos_line": [
            "+\"image\": str(filepath),"
        ],
        "core_change": "+\"image\": str(filepath),",
        "core_API": "peek"
    },
    {
        "commit_hash": "d3cb28886ac68beba9a6646b422a4d727b056c0c",
        "index": "937b8a712..d44bc8036 100644",
        "commit_message": "Not use -1e4 as attn mask (#17306)\n\n* Use torch.finfo(self.dtype).min\n\n* for GPTNeoX\n\n* for Albert\n\n* For Splinter\n\n* Update src/transformers/models/data2vec/modeling_data2vec_audio.py\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* fix -inf used in Bart-like models\n\n* Fix a few remaining -inf\n\n* more fix\n\n* clean up\n\n* For CLIP\n\n* For FSMT\n\n* clean up\n\n* fix test\n\n* Add dtype argument and use it for LayoutLMv3\n\n* update FlaxLongT5Attention\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Attention(nn.Module):",
            "",
            "def fill_with_neg_inf(t):",
            "\"\"\"FP16-compatible function that fills a input_ids with -inf.\"\"\"",
            "-    return t.float().fill_(float(\"-inf\")).type_as(t)",
            "+    return t.float().fill_(torch.finfo(t.dtype).min).type_as(t)",
            "",
            "",
            "# Public API"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=1, insert_id=1196099)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=2, insert_id=1196100)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1196101)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'min'), position=2, insert_id=1196102)",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1196103)",
            "Update(target_node=ASTNode(type=identifier, text=float), value='torch')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=float), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1196104)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'finfo'), position=2, insert_id=1196105)",
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=1, insert_id=1196106)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=), text=)), position=2)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 't'), position=0, insert_id=1196107)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1196108)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dtype'), position=2, insert_id=1196109)",
            "Delete(target_node=ASTNode(type=string, text=\"-inf\"))",
            "Delete(target_node=ASTNode(type=), text=)))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 3644,
        "neg_line": [
            "-return t.float().fill_(float(\"-inf\")).type_as(t)"
        ],
        "pos_line": [
            "+return t.float().fill_(torch.finfo(t.dtype).min).type_as(t)"
        ],
        "core_change": "-return t.float().fill_(float(\"-inf\")).type_as(t) +return t.float().fill_(torch.finfo(t.dtype).min).type_as(t)",
        "core_API": "float"
    },
    {
        "commit_hash": "e4354ef796013d86b0ffdf337bc552482c8367e7",
        "index": "91ee8d0..8b658f1 100644",
        "commit_message": "refactor hdfs_path; fix more deprecation warnings; print label+preds\n\n",
        "file": "TensorFlowOnSpark.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def deploy(config,",
            "",
            "if total_loss is not None:",
            "# Add total_loss to summary.",
            "-      summaries.add(tf.summary.scalar('total_loss', total_loss,",
            "-                                      name='total_loss'))",
            "+      summaries.add(tf.summary.scalar('total_loss', total_loss))",
            "",
            "if summaries:",
            "# Merge all summaries together."
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=name))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=string, text='total_loss'))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 5,
        "number": 3647,
        "neg_line": [
            "-summaries.add(tf.summary.scalar('total_loss', total_loss,",
            "-name='total_loss'))"
        ],
        "pos_line": [
            "+summaries.add(tf.summary.scalar('total_loss', total_loss))"
        ],
        "core_change": "-summaries.add(tf.summary.scalar('total_loss', total_loss, -name='total_loss')) +summaries.add(tf.summary.scalar('total_loss', total_loss))",
        "core_API": "add"
    },
    {
        "commit_hash": "9b5a3ffa6cba5b30182eeab89ca195a772b4614c",
        "index": "37c8651..da803d0 100644",
        "commit_message": "PLY with uint face data (#1104)\n\nSummary: Fix assumption that face indices are signed in the PLY file, as reported in #1104.\n\nReviewed By: nikhilaravi\n\nDifferential Revision: D34892598\n\nfbshipit-source-id: a8b23bfac1357bdc11bbbf752098319142239804\n\n",
        "file": "pytorch3d.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def _load_ply(f, *, path_manager: PathManager) -> _PlyData:",
            "if face.shape[1] < 3:",
            "raise ValueError(\"Faces must have at least 3 vertices.\")",
            "face_arrays = [face[:, [0, i + 1, i + 2]] for i in range(face.shape[1] - 2)]",
            "-        faces = torch.LongTensor(np.vstack(face_arrays))",
            "+        faces = torch.LongTensor(np.vstack(face_arrays).astype(np.int64))",
            "else:",
            "face_list = []",
            "for face_item in face:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=914262)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=914263)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=914264)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'astype'), position=2, insert_id=914265)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=914266)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=914267)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=914268)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'np'), position=0, insert_id=914269)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=914270)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'int64'), position=2, insert_id=914271)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 3649,
        "neg_line": [
            "-faces = torch.LongTensor(np.vstack(face_arrays))"
        ],
        "pos_line": [
            "+faces = torch.LongTensor(np.vstack(face_arrays).astype(np.int64))"
        ],
        "core_change": "-faces = torch.LongTensor(np.vstack(face_arrays)) +faces = torch.LongTensor(np.vstack(face_arrays).astype(np.int64))",
        "core_API": "LongTensor"
    },
    {
        "commit_hash": "40ded605537ed31623a9993a9692f440ec91b307",
        "index": "643b96418..f1330586c 100644",
        "commit_message": "fix: minor fixes and refactoring NER (#539)\n\n* feat: add train from scratch\n\n* feat: add warning to train_from_scratch\n\n* fix: no cudnn rnn fixed mask\n\n* fix: session config added\n\n* fix: warning to log\n\n* fix: pass learning rate to default optimizer\n\n* fix: l2 in conv layer\n\n* fix: ner dataset reader\n\n* feat: add returning loss to the train on batch\n\n* doc: add model descriptions to the ner doc\n\n* chore: move ner metrics to metrics module\n\n* fix: remove train from scratch\n\n* fix: add dataset names\n\n* chore: log.info -> log.warning for warning\n\nCo-Authored-By: mu-arkhipov <arkhipov@yahoo.com>\n\n",
        "file": "DeepPavlov.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFModel(NNModel, metaclass=TfModelMeta):",
            "variables_to_train.extend(tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope_name))",
            "",
            "if optimizer is None:",
            "-                optimizer = tf.train.AdamOptimizer",
            "+                optimizer = tf.train.AdamOptimizer(learning_rate)",
            "",
            "# For batch norm it is necessary to update running averages",
            "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('call', None), position=2, insert_id=1922986)",
            "Move(target_node=IN(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1922987)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1922988)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'learning_rate'), position=1, insert_id=1922989)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1922990)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 3652,
        "neg_line": [
            "-optimizer = tf.train.AdamOptimizer"
        ],
        "pos_line": [
            "+optimizer = tf.train.AdamOptimizer(learning_rate)"
        ],
        "core_change": "-optimizer = tf.train.AdamOptimizer +optimizer = tf.train.AdamOptimizer(learning_rate)",
        "core_API": "extend"
    },
    {
        "commit_hash": "b745d30a3e770bf471db7d6ebae6ed702f7663bc",
        "index": "b0b759c..41528ef 100644",
        "commit_message": "Fix formatting of last commit\n\n",
        "file": "pytorch-image-models.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class PatchEmbed(nn.Module):",
            "",
            "def forward(self, x):",
            "B, C, H, W = x.shape",
            "-        torch._assert(H == self.img_size[0], f\"Input image height ({H}) doesn't match model {self.img_size[0]}.\")",
            "-        torch._assert(W == self.img_size[1], f\"Input image width ({W}) doesn't match model ({self.img_size[1]}.\")",
            "+        torch._assert(H == self.img_size[0], f\"Input image height ({H}) doesn't match model ({self.img_size[0]}).\")",
            "+        torch._assert(W == self.img_size[1], f\"Input image width ({W}) doesn't match model ({self.img_size[1]}).\")",
            "x = self.proj(x)",
            "if self.flatten:",
            "x = x.flatten(2).transpose(1, 2)  # BCHW -> BNC"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=895448)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=895449)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=895450)",
            "Insert(target_node=IN(type=attribute), node=('identifier', '_assert'), position=2, insert_id=895451)",
            "Update(target_node=ASTNode(type=string, text=f\"Input image height ({H}) doesn't match model {self.img_size[0]}.\"), value='f\"Input image height ({H}) doesn\\'t match model ({self.img_size[0]}).\"')",
            "Update(target_node=ASTNode(type=string, text=f\"Input image width ({W}) doesn't match model ({self.img_size[1]}.\"), value='f\"Input image width ({W}) doesn\\'t match model ({self.img_size[1]}).\"')",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=_assert))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 11,
        "number": 3658,
        "neg_line": [
            "-torch._assert(H == self.img_size[0], f\"Input image height ({H}) doesn't match model {self.img_size[0]}.\")",
            "-torch._assert(W == self.img_size[1], f\"Input image width ({W}) doesn't match model ({self.img_size[1]}.\")"
        ],
        "pos_line": [
            "+torch._assert(H == self.img_size[0], f\"Input image height ({H}) doesn't match model ({self.img_size[0]}).\")",
            "+torch._assert(W == self.img_size[1], f\"Input image width ({W}) doesn't match model ({self.img_size[1]}).\")"
        ],
        "core_change": "-torch._assert(H == self.img_size[0], f\"Input image height ({H}) doesn't match model {self.img_size[0]}.\") -torch._assert(W == self.img_size[1], f\"Input image width ({W}) doesn't match model ({self.img_size[1]}.\") +torch._assert(H == self.img_size[0], f\"Input image height ({H}) doesn't match model ({self.img_size[0]}).\") +torch._assert(W == self.img_size[1], f\"Input image width ({W}) doesn't match model ({self.img_size[1]}).\")",
        "core_API": "_assert"
    },
    {
        "commit_hash": "d364fdbb269891d4c81d94b37d130eab34dcc1eb",
        "index": "2e42df0e..a0b6e893 100644",
        "commit_message": "Reland BT enablement on fairseq - fairseq change (#4513)\n\nSummary:\nPull Request resolved: https://github.com/facebookresearch/fairseq/pull/4513\nWith some fixes to torchscript using dual copies.\nReland this diff.\n\nReviewed By: erichan1\n\nDifferential Revision: D37371293\n\nfbshipit-source-id: 4fcfc4083955b6f5fc4ef8600f1b517b6ba69aae\n\n",
        "file": "fairseq.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestJitSequenceGeneratorBase(unittest.TestCase):",
            "JIT_MSG = \"Targeting OSS scriptability for the 1.6 release\"",
            "",
            "",
            "-@unittest.skipIf(torch.__version__ < \"1.6.0\", JIT_MSG)",
            "+@unittest.skipIf(",
            "+    version_check(), \"Targeting OSS scriptability for the 1.13.0.dev20220613 release\"",
            "+)",
            "class TestJitSequenceGenerator(TestJitSequenceGeneratorBase):",
            "def test_export_transformer(self):",
            "model = self.transformer_model"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('(', '('), position=0, insert_id=203506)",
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=1, insert_id=203507)",
            "Insert(target_node=ASTNode(type=argument_list), node=('string', '\"Targeting OSS scriptability for the 1.13.0.dev20220613 release\"'), position=4, insert_id=203508)",
            "Update(target_node=ASTNode(type=identifier, text=torch), value='version_check')",
            "Move(target_node=IN(type=call), node=ASTNode(type=identifier, text=torch), position=0)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=203509)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=(, text=(), position=0)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=203510)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=__version__))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=<, text=<))",
            "Delete(target_node=ASTNode(type=string, text=\"1.6.0\"))",
            "Delete(target_node=ASTNode(type=comparison_operator))",
            "Delete(target_node=ASTNode(type=identifier, text=JIT_MSG))"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 15,
        "number": 3659,
        "neg_line": [
            "-@unittest.skipIf(torch.__version__ < \"1.6.0\", JIT_MSG)"
        ],
        "pos_line": [
            "+@unittest.skipIf(",
            "+version_check(), \"Targeting OSS scriptability for the 1.13.0.dev20220613 release\"",
            "+)"
        ],
        "core_change": "-@unittest.skipIf(torch.__version__ < \"1.6.0\", JIT_MSG) +@unittest.skipIf( +version_check(), \"Targeting OSS scriptability for the 1.13.0.dev20220613 release\" +)",
        "core_API": "skipIf"
    },
    {
        "commit_hash": "324c84e8d9038dc9c707112dda7b29f5e5e36dd6",
        "index": "86255e5ad..f4586830a 100644",
        "commit_message": "[Audio datasets] Adapting all audio datasets (#3081)\n\n* up\n\n* up\n\n* up\n\n* up\n\n* up\n\n* Update src/datasets/utils/resources/readme_structure.yaml\n\nCo-authored-by: Albert Villanova del Moral <8515462+albertvillanova@users.noreply.github.com>\n\n* correct\n\n* correct 2\n\n* Update datasets/covost2/README.md\n\nCo-authored-by: Albert Villanova del Moral <8515462+albertvillanova@users.noreply.github.com>\n\n* Fix typo\n\nCo-authored-by: Albert Villanova del Moral <8515462+albertvillanova@users.noreply.github.com>\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class OpenSlr(datasets.GeneratorBasedBuilder):",
            "# set absolute path for audio file",
            "path = os.path.join(path_to_datas[i], f\"{filename}.wav\")",
            "counter += 1",
            "-                        yield counter, {\"path\": path, \"sentence\": sentence}",
            "+                        yield counter, {\"path\": path, \"audio\": path, \"sentence\": sentence}"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=dictionary), node=('pair', None), position=3, insert_id=1782520)",
            "Insert(target_node=ASTNode(type=dictionary), node=(',', ','), position=4, insert_id=1782521)",
            "Insert(target_node=IN(type=pair), node=('string', '\"audio\"'), position=0, insert_id=1782522)",
            "Insert(target_node=IN(type=pair), node=(':', ':'), position=1, insert_id=1782523)",
            "Insert(target_node=IN(type=pair), node=('identifier', 'path'), position=2, insert_id=1782524)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 3660,
        "neg_line": [
            "-yield counter, {\"path\": path, \"sentence\": sentence}"
        ],
        "pos_line": [
            "+yield counter, {\"path\": path, \"audio\": path, \"sentence\": sentence}"
        ],
        "core_change": "-yield counter, {\"path\": path, \"sentence\": sentence} +yield counter, {\"path\": path, \"audio\": path, \"sentence\": sentence}",
        "core_API": "join"
    },
    {
        "commit_hash": "a828315185a9dc8b21ec8e5dbead9044caf0d3a2",
        "index": "d9983636..e523d32a 100644",
        "commit_message": "[Enhance] Added dtype flag to geometry module (#820)\n\n* Added docs and tests\n\n* Fixed bug for batch prob generator\n\n* Fixed typo\n\n* Fixed mypy\n\n* Fixed mypy\n\n* Added warning\n\n* Updated tests for geometry\n\n* Added dtype for conversions\n\n* Completed tests\n\n* Fixed homography\n\n* Fixed pinhole camera\n\n* Fixed tests\n\n* Fix geometry testing bugs\n\n* Fixed augmentation tests\n\n* code updated\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def find_homography_dlt(",
            "U, S, V = torch.svd(A)",
            "except:",
            "warnings.warn('SVD did not converge', RuntimeWarning)",
            "-        return torch.empty((points1_norm.size(0), 3, 3), device=points1.device)",
            "+        return torch.empty((points1_norm.size(0), 3, 3), device=device, dtype=dtype)",
            "",
            "H = V[..., -1].view(-1, 3, 3)",
            "H = transform2.inverse() @ (H @ transform1)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=432126)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=432127)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=432128)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=identifier, text=device), position=0)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type==, text==), position=1)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=2, insert_id=432129)",
            "Update(target_node=ASTNode(type=identifier, text=points1), value='dtype')",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=identifier, text=points1), position=0)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=432130)",
            "Update(target_node=ASTNode(type=identifier, text=device), value='dtype')",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=identifier, text=device), position=2)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 14,
        "number": 3662,
        "neg_line": [
            "-return torch.empty((points1_norm.size(0), 3, 3), device=points1.device)"
        ],
        "pos_line": [
            "+return torch.empty((points1_norm.size(0), 3, 3), device=device, dtype=dtype)"
        ],
        "core_change": "-return torch.empty((points1_norm.size(0), 3, 3), device=points1.device) +return torch.empty((points1_norm.size(0), 3, 3), device=device, dtype=dtype)",
        "core_API": "svd"
    },
    {
        "commit_hash": "82563da72037ee4889852b2dbb1864be40fc4243",
        "index": "667e603a..9060df0b 100644",
        "commit_message": "test: minor pytest fixes (#3109)\n\nAdds all frameworks to dev-dependencies as well. Skips LightGBM on arm64 because it's not supported.\n\nRemoves most of the pytest flags to make pytest output a little more sane and fix some issues caused by the importlib import method.\n",
        "file": "BentoML.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "models",
        "change": [
            "def test_inputs(framework: str | None) -> list[tuple[ModuleType, FrameworkTestMo",
            ")",
            ")",
            "except ModuleNotFoundError as e:",
            "-            raise ModuleNotFoundError(",
            "-                f\"Failed to find test module for framework {framework_name} (tests.integration.frameworks.models.{framework_name})\"",
            "-            ) from e",
            "+            logger.warning(f\"Failed to find test module for framework {framework_name} (tests.integration.frameworks.models.{framework_name})\")",
            "",
            "return [",
            "(module.framework, _model)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=call), position=13)",
            "Insert(target_node=ASTNode(type=ERROR), node=('ERROR', None), position=14, insert_id=2648127)",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2648128)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=identifier, text=return), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=ModuleNotFoundError), value='logger')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=ModuleNotFoundError), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2648129)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'warning'), position=2, insert_id=2648130)",
            "Delete(target_node=ASTNode(type=identifier, text=raise))",
            "Delete(target_node=ASTNode(type=identifier, text=from))",
            "Delete(target_node=ASTNode(type=identifier, text=e))",
            "Delete(target_node=ASTNode(type=ERROR))"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 12,
        "number": 3665,
        "neg_line": [
            "-raise ModuleNotFoundError(",
            "-f\"Failed to find test module for framework {framework_name} (tests.integration.frameworks.models.{framework_name})\"",
            "-) from e"
        ],
        "pos_line": [
            "+logger.warning(f\"Failed to find test module for framework {framework_name} (tests.integration.frameworks.models.{framework_name})\")"
        ],
        "core_change": "-raise ModuleNotFoundError( -f\"Failed to find test module for framework {framework_name} (tests.integration.frameworks.models.{framework_name})\" -) from e +logger.warning(f\"Failed to find test module for framework {framework_name} (tests.integration.frameworks.models.{framework_name})\")",
        "core_API": "warning"
    },
    {
        "commit_hash": "c3d5ac151eaedb61495e5866f13a9746d3706abc",
        "index": "1230f46..e166722 100644",
        "commit_message": "precommit: yapf (#5494)\n\n* precommit: yapf\n\n* align isort\n\n* fix\n\n# Conflicts:\n#\tutils/plots.py\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Update setup.cfg\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Update setup.cfg\n\n* Update setup.cfg\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Update wandb_utils.py\n\n* Update augmentations.py\n\n* Update setup.cfg\n\n* Update yolo.py\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Update val.py\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* simplify colorstr\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* val run fix\n\n* export.py last comma\n\n* Update export.py\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Update hubconf.py\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* PyTorch Hub tuple fix\n\n* PyTorch Hub tuple fix2\n\n* PyTorch Hub tuple fix3\n\n* Update setup\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n",
        "file": "yolov5.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class MixConv2d(nn.Module):",
            "a[0] = 1",
            "c_ = np.linalg.lstsq(a, b, rcond=None)[0].round()  # solve for equal weight indices, ax = b",
            "",
            "-        self.m = nn.ModuleList(",
            "-            [nn.Conv2d(c1, int(c_), k, s, k // 2, groups=math.gcd(c1, int(c_)), bias=False) for k, c_ in zip(k, c_)])",
            "+        self.m = nn.ModuleList([",
            "+            nn.Conv2d(c1, int(c_), k, s, k // 2, groups=math.gcd(c1, int(c_)), bias=False) for k, c_ in zip(k, c_)])",
            "self.bn = nn.BatchNorm2d(c2)",
            "self.act = nn.SiLU()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 3667,
        "neg_line": [
            "-self.m = nn.ModuleList(",
            "-[nn.Conv2d(c1, int(c_), k, s, k // 2, groups=math.gcd(c1, int(c_)), bias=False) for k, c_ in zip(k, c_)])"
        ],
        "pos_line": [
            "+self.m = nn.ModuleList([",
            "+nn.Conv2d(c1, int(c_), k, s, k // 2, groups=math.gcd(c1, int(c_)), bias=False) for k, c_ in zip(k, c_)])"
        ],
        "core_change": "-self.m = nn.ModuleList( -[nn.Conv2d(c1, int(c_), k, s, k // 2, groups=math.gcd(c1, int(c_)), bias=False) for k, c_ in zip(k, c_)]) +self.m = nn.ModuleList([ +nn.Conv2d(c1, int(c_), k, s, k // 2, groups=math.gcd(c1, int(c_)), bias=False) for k, c_ in zip(k, c_)])",
        "core_API": "lstsq"
    },
    {
        "commit_hash": "c108a786fe7dc055c9494c0956f3cad7872c3975",
        "index": "43ab03ab..857d4903 100644",
        "commit_message": "Fix some error of YOLOX (#5848)\n\n* Add YOLOX config\n\n* update\n\n* fix error\n\n* fix lr error\n\n* fix tiny config error and foreground_mask warning\n\n* fix dp train error\n\n* add comment\n\n* support browse_dataset\n\n* add comment\n\n* fix __repr__\n\n* Switch to synchronizing norm interval.\n\n* delete config\n\n* add MultiImageMixDataset unittest\n\n* add MultiImageMixDataset skip_type_keys unittest\n\n* fix type\n\n* add comment\n",
        "file": "mmdetection.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class YOLOXHead(BaseDenseHead, BBoxTestMixin):",
            "if self.use_l1:",
            "l1_target = self._get_l1_target(l1_target, bbox_target,",
            "priors[pos_inds])",
            "-        foreground_mask = torch.zeros_like(objectness).to(torch.uint8)",
            "+        foreground_mask = torch.zeros_like(objectness).to(torch.bool)",
            "foreground_mask[pos_inds] = 1",
            "return (foreground_mask, cls_target, obj_target, bbox_target,",
            "l1_target, num_pos_per_img)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=uint8), value='bool')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 3668,
        "neg_line": [
            "-foreground_mask = torch.zeros_like(objectness).to(torch.uint8)"
        ],
        "pos_line": [
            "+foreground_mask = torch.zeros_like(objectness).to(torch.bool)"
        ],
        "core_change": "-foreground_mask = torch.zeros_like(objectness).to(torch.uint8) +foreground_mask = torch.zeros_like(objectness).to(torch.bool)",
        "core_API": "_get_l1_target"
    },
    {
        "commit_hash": "b5e2b183af5e40e33a4dc7659e697d137259d56e",
        "index": "391b3c5b3..1095adbde 100644",
        "commit_message": "Doc styler examples (#14953)\n\n* Fix bad examples\n\n* Add black formatting to style_doc\n\n* Use first nonempty line\n\n* Put it at the right place\n\n* Don't add spaces to empty lines\n\n* Better templates\n\n* Deal with triple quotes in docstrings\n\n* Result of style_doc\n\n* Enable mdx treatment and fix code examples in MDXs\n\n* Result of doc styler on doc source files\n\n* Last fixes\n\n* Break copy from\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class XSoftmax(torch.autograd.Function):",
            ">>> from transformers.models.deberta_v2.modeling_deberta_v2 import XSoftmax",
            "",
            ">>> # Make a tensor",
            "-    >>> x = torch.randn([4,20,100])",
            "+    >>> x = torch.randn([4, 20, 100])",
            "",
            ">>> # Create a mask",
            "-    >>> mask = (x>0).int()",
            "+    >>> mask = (x > 0).int()",
            "",
            ">>> # Specify the dimension to apply softmax",
            ">>> dim = -1"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 3673,
        "neg_line": [
            "->>> x = torch.randn([4,20,100])",
            "->>> mask = (x>0).int()"
        ],
        "pos_line": [
            "+>>> x = torch.randn([4, 20, 100])",
            "+>>> mask = (x > 0).int()"
        ],
        "core_change": "->>> x = torch.randn([4,20,100]) +>>> x = torch.randn([4, 20, 100]) ->>> mask = (x>0).int() +>>> mask = (x > 0).int()",
        "core_API": "randn"
    },
    {
        "commit_hash": "d6ef2ee9f37e124a29e8dfde4c1bfa72296f5dcf",
        "index": "97b6e3ce..9adcd1ba 100644",
        "commit_message": "Fix df coercion in MultivariateStudentT (#2228)\n\n* Fix df coercion in MultivariateStudentT\n\n* flake8\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class MultivariateStudentT(TorchDistribution):",
            "def __init__(self, df, loc, scale_tril, validate_args=None):",
            "dim = loc.size(-1)",
            "assert scale_tril.shape[-2:] == (dim, dim)",
            "-        df, = broadcast_all(df)",
            "+        if not isinstance(df, torch.Tensor):",
            "+            df = loc.new_tensor(df)",
            "batch_shape = broadcast_shape(df.shape, loc.shape[:-1], scale_tril.shape[:-2])",
            "event_shape = (dim,)",
            "self.df = df.expand(batch_shape)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=4, insert_id=705989)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=705990)",
            "Insert(target_node=IN(type=if_statement), node=('not_operator', None), position=1, insert_id=705991)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=705992)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=705993)",
            "Insert(target_node=IN(type=not_operator), node=('not', 'not'), position=0, insert_id=705994)",
            "Insert(target_node=IN(type=not_operator), node=('call', None), position=1, insert_id=705995)",
            "Move(target_node=IN(type=block), node=ASTNode(type=expression_statement), position=0)",
            "Insert(target_node=IN(type=call), node=('identifier', 'isinstance'), position=0, insert_id=705996)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=705997)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=705998)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'df'), position=1, insert_id=705999)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=,, text=,), position=2)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=3, insert_id=706000)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=706001)",
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=identifier, text=df), position=0)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=706002)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=706003)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Tensor'), position=2, insert_id=706004)",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=706005)",
            "Update(target_node=ASTNode(type=identifier, text=broadcast_all), value='loc')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=broadcast_all), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=706006)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'new_tensor'), position=2, insert_id=706007)",
            "Delete(target_node=ASTNode(type=pattern_list))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 25,
        "number": 3674,
        "neg_line": [
            "-df, = broadcast_all(df)"
        ],
        "pos_line": [
            "+if not isinstance(df, torch.Tensor):",
            "+df = loc.new_tensor(df)"
        ],
        "core_change": "-df, = broadcast_all(df) +if not isinstance(df, torch.Tensor): +df = loc.new_tensor(df)",
        "core_API": "size"
    },
    {
        "commit_hash": "041e6e9b79ed62a65a349fbb6db15dfa2aa33a17",
        "index": "a7e538edff..a23141ab3b 100644",
        "commit_message": "small fix in nanmean torch backend\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def nanmean(",
            "dtype: Optional[torch.dtype] = None,",
            "out: Optional[torch.Tensor] = None,",
            ") -> torch.Tensor:",
            "-    return torch.nanmean(a, axis=axis, keepdim=keepdims, dtype=dtype, out=out)",
            "+    return torch.nanmean(a, dim=axis, keepdim=keepdims, dtype=dtype, out=out)",
            "",
            "",
            "nanmean_support_native_out = True"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=axis), value='dim')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 3675,
        "neg_line": [
            "-return torch.nanmean(a, axis=axis, keepdim=keepdims, dtype=dtype, out=out)"
        ],
        "pos_line": [
            "+return torch.nanmean(a, dim=axis, keepdim=keepdims, dtype=dtype, out=out)"
        ],
        "core_change": "-return torch.nanmean(a, axis=axis, keepdim=keepdims, dtype=dtype, out=out) +return torch.nanmean(a, dim=axis, keepdim=keepdims, dtype=dtype, out=out)",
        "core_API": "nanmean"
    },
    {
        "commit_hash": "861bf4d4e2e44f9dff27eb6acc1fb576e3e25e7a",
        "index": "34e40c16..d87e6a10 100644",
        "commit_message": "merge code in overfit (#1185)\n\n* merge code in overfit\n\n* fix\n",
        "file": "d2l-en.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "argmax = tf.argmax",
            "tensor = tf.constant",
            "arange = tf.range",
            "astype = tf.cast",
            "+int32 = tf.int32",
            "+float32 = tf.float32",
            "numpy = lambda x, *args, **kwargs: x.numpy(*args, **kwargs)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=4, insert_id=1917621)",
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=5, insert_id=1917622)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1917623)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1917624)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'int32'), position=0, insert_id=1917625)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1917626)",
            "Insert(target_node=IN(type=assignment), node=('attribute', None), position=2, insert_id=1917627)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'float32'), position=0, insert_id=1917628)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1917629)",
            "Insert(target_node=IN(type=assignment), node=('attribute', None), position=2, insert_id=1917630)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=1917631)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1917632)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'int32'), position=2, insert_id=1917633)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=1917634)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1917635)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'float32'), position=2, insert_id=1917636)"
        ],
        "plus_line": 0,
        "minus_line": 0,
        "AST_diff_line": 16,
        "number": 3676,
        "neg_line": [],
        "pos_line": [
            "+int32 = tf.int32",
            "+float32 = tf.float32"
        ],
        "core_change": "+int32 = tf.int32 +float32 = tf.float32",
        "core_API": "numpy"
    },
    {
        "commit_hash": "442a07e7687560276b7490ffdffa92804f676a05",
        "index": "4222d0a..a2d7ed0 100644",
        "commit_message": "fix the mask problem for multi-head attenion\n\n",
        "file": "transformer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def multihead_attention(queries,",
            "outputs = tf.nn.softmax(outputs) # (h*N, T_q, T_k)",
            "",
            "# Query Masking",
            "-        query_masks = tf.sign(tf.abs(tf.reduce_sum(queries, axis=-1))) # (N, T_q)",
            "+        query_masks = tf.sign(tf.reduce_sum(tf.abs(queries), axis=-1)) # (N, T_q)",
            "query_masks = tf.tile(query_masks, [num_heads, 1]) # (h*N, T_q)",
            "query_masks = tf.tile(tf.expand_dims(query_masks, -1), [1, 1, tf.shape(keys)[1]]) # (h*N, T_q, T_k)",
            "outputs *= query_masks # broadcasting. (N, T_q, C)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=1, insert_id=2356689)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=,, text=,), position=2)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=keyword_argument), position=3)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=), text=)), position=4)",
            "Move(target_node=IN(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2356690)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=(, text=(), position=0)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=queries), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2356691)",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=), text=)))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 13,
        "number": 3681,
        "neg_line": [
            "-query_masks = tf.sign(tf.abs(tf.reduce_sum(queries, axis=-1))) # (N, T_q)"
        ],
        "pos_line": [
            "+query_masks = tf.sign(tf.reduce_sum(tf.abs(queries), axis=-1)) # (N, T_q)"
        ],
        "core_change": "-query_masks = tf.sign(tf.abs(tf.reduce_sum(queries, axis=-1))) # (N, T_q) +query_masks = tf.sign(tf.reduce_sum(tf.abs(queries), axis=-1)) # (N, T_q)",
        "core_API": "softmax"
    },
    {
        "commit_hash": "ed63b7ee9e68ba120893aaaaca0d4525a56ed484",
        "index": "4dfb4a75..54d6e601 100644",
        "commit_message": "upgrade to pytorch 0.4.0 (#1126)\n\n* bump pytorch to 0.4 + fix sanitize\n\n* remove check for Variable in block_orthogonal\n\n* rename parameter split_size=\n\n* remove checks for Variable\n\n* fix more tests\n\n* fixes\n\n* get tests to pass\n\n* fix warnings\n\n* get rid of some of the Variables\n\n* more tests passing\n\n* more elimination of variables\n\n* finish removing all Variables\n\n* pylint and such\n\n* a few fixes\n\n* move torch.no_grad into model.forward_on_instances\n\n* more pytorch 0.4 changes\n\n* detach() -> data\n\n* pylint\n\n* fix bad tensor creation\n\n* fix types\n\n* remove print statement\n\n* more 0.4 goodness\n\n* factor out is_tensor\n\n* add no_grad to elmo command\n\n* cleanup\n\n* remove TODO\n\n* address PR feedback\n\n* replace all() with item()\n\n* more cleanup\n\n* further cleanup\n\n* remove Variable\n\n* really fix merge conflict\n\n* fix pylint\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestStackedSelfAttention(AllenNlpTestCase):",
            "",
            "def test_pass_through_encoder_passes_through(self):",
            "encoder = PassThroughEncoder(input_dim=9)",
            "-        tensor = Variable(torch.randn([2, 3, 9]))",
            "+        tensor = torch.randn([2, 3, 9])",
            "output = encoder(tensor)",
            "-        numpy.testing.assert_array_almost_equal(tensor.data.cpu().numpy(),",
            "-                                                output.data.cpu().numpy())",
            "+        numpy.testing.assert_array_almost_equal(tensor.detach().cpu().numpy(),",
            "+                                                output.detach().cpu().numpy())"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=call), position=2)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=37082)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=37083)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=37084)",
            "Insert(target_node=ASTNode(type=attribute), node=('call', None), position=0, insert_id=37085)",
            "Insert(target_node=ASTNode(type=attribute), node=('call', None), position=0, insert_id=37086)",
            "Move(target_node=IN(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=37087)",
            "Move(target_node=IN(type=call), node=ASTNode(type=attribute), position=0)",
            "Move(target_node=IN(type=call), node=ASTNode(type=argument_list), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=data), value='detach')",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=37088)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=37089)",
            "Update(target_node=ASTNode(type=identifier, text=data), value='detach')",
            "Delete(target_node=ASTNode(type=identifier, text=Variable))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 19,
        "number": 3684,
        "neg_line": [
            "-tensor = Variable(torch.randn([2, 3, 9]))",
            "-numpy.testing.assert_array_almost_equal(tensor.data.cpu().numpy(),",
            "-output.data.cpu().numpy())"
        ],
        "pos_line": [
            "+tensor = torch.randn([2, 3, 9])",
            "+numpy.testing.assert_array_almost_equal(tensor.detach().cpu().numpy(),",
            "+output.detach().cpu().numpy())"
        ],
        "core_change": "-tensor = Variable(torch.randn([2, 3, 9])) +tensor = torch.randn([2, 3, 9]) -numpy.testing.assert_array_almost_equal(tensor.data.cpu().numpy(), -output.data.cpu().numpy()) +numpy.testing.assert_array_almost_equal(tensor.detach().cpu().numpy(), +output.detach().cpu().numpy())",
        "core_API": "randn"
    },
    {
        "commit_hash": "e138d6ac2a76c7ca969baa923a89c189478171e4",
        "index": "849424af4..7701a0dc2 100644",
        "commit_message": "update espnet2/bin/asr_mix_inference.py; fix bugs\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ESPnetASRMixModel(AbsESPnetModel):",
            "ignore_label=self.ignore_id,",
            ")",
            ")",
            "-        loss_att = torch.mean(loss_att)",
            "+        loss_att = torch.stack(loss_att, dim=0).mean()",
            "acc_att = np.mean(acc_att)",
            "",
            "# Compute cer/wer using attention-decoder"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=144201)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=144202)",
            "Insert(target_node=IN(type=attribute), node=('call', None), position=0, insert_id=144203)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=144204)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=mean), position=2)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=144205)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=144206)",
            "Move(target_node=IN(type=call), node=ASTNode(type=attribute), position=0)",
            "Move(target_node=IN(type=call), node=ASTNode(type=argument_list), position=1)",
            "Insert(target_node=ASTNode(type=attribute), node=('identifier', 'stack'), position=2, insert_id=144207)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=144208)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=144209)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dim'), position=0, insert_id=144210)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=144211)",
            "Insert(target_node=IN(type=keyword_argument), node=('integer', '0'), position=2, insert_id=144212)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 15,
        "number": 3686,
        "neg_line": [
            "-loss_att = torch.mean(loss_att)"
        ],
        "pos_line": [
            "+loss_att = torch.stack(loss_att, dim=0).mean()"
        ],
        "core_change": "-loss_att = torch.mean(loss_att) +loss_att = torch.stack(loss_att, dim=0).mean()",
        "core_API": "mean"
    },
    {
        "commit_hash": "18587c1c2dc666e858a991591c194261d913b7d7",
        "index": "09746c40..ec76b21f 100755",
        "commit_message": "fix tests\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TestTRPOAgent(unittest.TestCase):",
            "def test_trpo_agent(self):",
            "config = {",
            "'batch_size': 8,",
            "+            \"cg_iterations\": 20,",
            "+            \"cg_damping\": 0.001,",
            "+            \"line_search_steps\": 20,",
            "'max_kl_divergence': 0.01,",
            "'max_episode_length': 4,",
            "'continuous': False,",
            "'state_shape': (2,),",
            "'actions': 2}",
            "+        tf.reset_default_graph()",
            "",
            "config = create_config(config)",
            "network_builder = NeuralNetwork.layered_network(layers=[{'type': 'dense', 'num_outputs': 32}])"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=2245623)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=2245624)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2245625)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2245626)",
            "Insert(target_node=ASTNode(type=dictionary), node=('pair', None), position=3, insert_id=2245627)",
            "Insert(target_node=ASTNode(type=dictionary), node=(',', ','), position=4, insert_id=2245628)",
            "Insert(target_node=ASTNode(type=dictionary), node=('pair', None), position=5, insert_id=2245629)",
            "Insert(target_node=ASTNode(type=dictionary), node=(',', ','), position=6, insert_id=2245630)",
            "Insert(target_node=ASTNode(type=dictionary), node=('pair', None), position=7, insert_id=2245631)",
            "Insert(target_node=ASTNode(type=dictionary), node=(',', ','), position=8, insert_id=2245632)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2245633)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2245634)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'reset_default_graph'), position=2, insert_id=2245635)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2245636)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=2245637)",
            "Insert(target_node=IN(type=pair), node=('string', '\"cg_iterations\"'), position=0, insert_id=2245638)",
            "Insert(target_node=IN(type=pair), node=(':', ':'), position=1, insert_id=2245639)",
            "Insert(target_node=IN(type=pair), node=('integer', '20'), position=2, insert_id=2245640)",
            "Insert(target_node=IN(type=pair), node=('string', '\"cg_damping\"'), position=0, insert_id=2245641)",
            "Insert(target_node=IN(type=pair), node=(':', ':'), position=1, insert_id=2245642)",
            "Insert(target_node=IN(type=pair), node=('float', '0.001'), position=2, insert_id=2245643)",
            "Insert(target_node=IN(type=pair), node=('string', '\"line_search_steps\"'), position=0, insert_id=2245644)",
            "Insert(target_node=IN(type=pair), node=(':', ':'), position=1, insert_id=2245645)",
            "Insert(target_node=IN(type=pair), node=('integer', '20'), position=2, insert_id=2245646)"
        ],
        "plus_line": 4,
        "minus_line": 0,
        "AST_diff_line": 24,
        "number": 3688,
        "neg_line": [],
        "pos_line": [
            "+\"cg_iterations\": 20,",
            "+\"cg_damping\": 0.001,",
            "+\"line_search_steps\": 20,",
            "+tf.reset_default_graph()"
        ],
        "core_change": "+\"cg_iterations\": 20, +\"cg_damping\": 0.001, +\"line_search_steps\": 20, +tf.reset_default_graph()",
        "core_API": "reset_default_graph"
    },
    {
        "commit_hash": "d01dc9e22d5e8625ae6ac49e2e689eebf472b5f8",
        "index": "6ea5d31acd..1575e46c38 100644",
        "commit_message": "[rllib] format with yapf (#2427)\n\n* initial yapf\n\n* manual fix yapf bugs\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def itergroups(items, group_size):",
            "def batched_weighted_sum(weights, vecs, batch_size):",
            "total = 0",
            "num_items_summed = 0",
            "-    for batch_weights, batch_vecs in zip(itergroups(weights, batch_size),",
            "-                                         itergroups(vecs, batch_size)):",
            "+    for batch_weights, batch_vecs in zip(",
            "+            itergroups(weights, batch_size), itergroups(vecs, batch_size)):",
            "assert len(batch_weights) == len(batch_vecs) <= batch_size",
            "-        total += np.dot(np.asarray(batch_weights, dtype=np.float32),",
            "-                        np.asarray(batch_vecs, dtype=np.float32))",
            "+        total += np.dot(",
            "+            np.asarray(batch_weights, dtype=np.float32),",
            "+            np.asarray(batch_vecs, dtype=np.float32))",
            "num_items_summed += len(batch_weights)",
            "return total, num_items_summed"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 5,
        "minus_line": 4,
        "AST_diff_line": 17,
        "number": 3690,
        "neg_line": [
            "-for batch_weights, batch_vecs in zip(itergroups(weights, batch_size),",
            "-itergroups(vecs, batch_size)):",
            "-total += np.dot(np.asarray(batch_weights, dtype=np.float32),",
            "-np.asarray(batch_vecs, dtype=np.float32))"
        ],
        "pos_line": [
            "+for batch_weights, batch_vecs in zip(",
            "+itergroups(weights, batch_size), itergroups(vecs, batch_size)):",
            "+total += np.dot(",
            "+np.asarray(batch_weights, dtype=np.float32),",
            "+np.asarray(batch_vecs, dtype=np.float32))"
        ],
        "core_change": "-for batch_weights, batch_vecs in zip(itergroups(weights, batch_size), -itergroups(vecs, batch_size)): +for batch_weights, batch_vecs in zip( +itergroups(weights, batch_size), itergroups(vecs, batch_size)): -total += np.dot(np.asarray(batch_weights, dtype=np.float32), -np.asarray(batch_vecs, dtype=np.float32)) +total += np.dot( +np.asarray(batch_weights, dtype=np.float32), +np.asarray(batch_vecs, dtype=np.float32))",
        "core_API": "dot"
    },
    {
        "commit_hash": "c5b6ceea4a0a151ab78b9c065584bc416d1ee275",
        "index": "195cac86..65a6044f 100644",
        "commit_message": "tl.layers API Refactoring and various modifications (#667)\n\n* Decorators API Refactored\n\n* extra_requires `all`, `all_cpu` and `all_gpu` added\n\n* Error fix\n\n* YAPF Formating Correction\n\n* Test for private method decorator added\n\n* Test Logging Verbosity Fixed to Debug when runned individually\n\n* YAPF corrections applied\n\n* Changelog Added\n\n* Changelog updated\n\n* PR number changed\n\n* First Refactoring Pass done\n\n* cleaning second pass\n\n* Refactoring 3rd pass\n\n* Refactoring 4th Pass\n\n* Code Error fix\n\n* YAPF Formating Fix\n\n* Arguments now using self\n\n* YAPF error correction\n\n* Bug Fix in Decorator\n\n* act name bug fix\n\n* Error Correction\n\n* YAPF formating fix\n\n* Useless tf.identity removed\n\n* Error Fix\n\n* Changelog Updated\n\n* Error fix in tl.activation\n\n* Documentation error fix\n\n* Lazy Import added\n\n* Import Refactoring with LazyImport when necessary\n\n* Changelog Updated\n\n* Gitter Removed\n\n* Fixed proposed by @zsdonghao\n\n* Documentation updated\n\n* Missing requirements added\n\n* Update to TensorLayer 1.8.6rc1\n\n* Requirements error fix\n\n* Docker Files updated\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Layer_Recurrent_Test(unittest.TestCase):",
            "dropout=None, n_layer=2, return_seq_2d=True, name='Seq2seq'",
            ")",
            "",
            "-        net11 = tl.layers.DenseLayer(net11, n_units=10000, act=tf.identity, name='oo')",
            "+        net11 = tl.layers.DenseLayer(net11, n_units=10000, name='oo')",
            "",
            "# e_loss = tl.cost.cross_entropy_seq_with_mask(logits=net11.outputs, target_seqs=target_seqs, input_mask=target_mask, return_details=False, name='cost')",
            "# y = tf.nn.softmax(net11.outputs)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=act))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=identity))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 3692,
        "neg_line": [
            "-net11 = tl.layers.DenseLayer(net11, n_units=10000, act=tf.identity, name='oo')"
        ],
        "pos_line": [
            "+net11 = tl.layers.DenseLayer(net11, n_units=10000, name='oo')"
        ],
        "core_change": "-net11 = tl.layers.DenseLayer(net11, n_units=10000, act=tf.identity, name='oo') +net11 = tl.layers.DenseLayer(net11, n_units=10000, name='oo')",
        "core_API": "DenseLayer"
    },
    {
        "commit_hash": "c89bdfbe720bc8f41c7dc6db5473a2cb0955f224",
        "index": "eaf353a21..6c49c81fa 100644",
        "commit_message": "Reorganize repo (#8580)\n\n* Put models in subfolders\n\n* Styling\n\n* Fix imports in tests\n\n* More fixes in test imports\n\n* Sneaky hidden imports\n\n* Fix imports in doc files\n\n* More sneaky imports\n\n* Finish fixing tests\n\n* Fix examples\n\n* Fix path for copies\n\n* More fixes for examples\n\n* Fix dummy files\n\n* More fixes for example\n\n* More model import fixes\n\n* Is this why you're unhappy GitHub?\n\n* Fix imports in conver command\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "models",
        "change": [
            "class RagConfig(PretrainedConfig):",
            "decoder_config = kwargs.pop(\"generator\")",
            "decoder_model_type = decoder_config.pop(\"model_type\")",
            "",
            "-        from .configuration_auto import AutoConfig",
            "+        from ..auto.configuration_auto import AutoConfig",
            "",
            "self.question_encoder = AutoConfig.for_model(question_encoder_model_type, **question_encoder_config)",
            "self.generator = AutoConfig.for_model(decoder_model_type, **decoder_config)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=import_prefix), node=('.', '.'), position=1, insert_id=2688878)",
            "Insert(target_node=ASTNode(type=dotted_name), node=('identifier', 'auto'), position=0, insert_id=2688879)",
            "Insert(target_node=ASTNode(type=dotted_name), node=('.', '.'), position=1, insert_id=2688880)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 3,
        "number": 3693,
        "neg_line": [
            "-from .configuration_auto import AutoConfig"
        ],
        "pos_line": [
            "+from ..auto.configuration_auto import AutoConfig"
        ],
        "core_change": "-from .configuration_auto import AutoConfig +from ..auto.configuration_auto import AutoConfig",
        "core_API": "pop"
    },
    {
        "commit_hash": "51b8c5e88ce2681dd5b9b61b78cc81adc11399aa",
        "index": "a0fc29599..4fcd9b667 100644",
        "commit_message": "Fix test scripts and refine some code\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class RNNLM(nn.Module):",
            "emb = self.embed(x)",
            "h[0], c[0] = self.lstm[0](self.dropout[0](emb), (state['h'][0], state['c'][0]))",
            "for n in six.moves.range(1, self.n_layers):",
            "-            h[n], c[n] = self.lstm[n](self.dropout[n](h[n-1]), (state['h'][n], state['c'][n]))",
            "+            h[n], c[n] = self.lstm[n](self.dropout[n](h[n - 1]), (state['h'][n], state['c'][n]))",
            "y = self.lo(self.dropout[-1](h[-1]))",
            "state = {'c': c, 'h': h}",
            "return state, y"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 3695,
        "neg_line": [
            "-h[n], c[n] = self.lstm[n](self.dropout[n](h[n-1]), (state['h'][n], state['c'][n]))"
        ],
        "pos_line": [
            "+h[n], c[n] = self.lstm[n](self.dropout[n](h[n - 1]), (state['h'][n], state['c'][n]))"
        ],
        "core_change": "-h[n], c[n] = self.lstm[n](self.dropout[n](h[n-1]), (state['h'][n], state['c'][n])) +h[n], c[n] = self.lstm[n](self.dropout[n](h[n - 1]), (state['h'][n], state['c'][n]))",
        "core_API": "embed"
    },
    {
        "commit_hash": "34a7ce6d3d3374bff04c42327da70f957ece6216",
        "index": "94ef9b1a..55eb8ac3 100644",
        "commit_message": "fix: tensorflow tests + cleanup (#1885)\n\n\n",
        "file": "BentoML.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tensorflow",
        "change": [
            "TENSOR_CLASS_NAMES = (",
            "\"Tensor\",",
            ")",
            "",
            "-ST = TypeVar(\"ST\")",
            "+ST = t.TypeVar(\"ST\")",
            "",
            "",
            "-def _isinstance_wrapper(obj: ST, sobj: Union[str, type, Sequence]) -> bool:",
            "+def _isinstance_wrapper(obj: ST, sobj: t.Union[str, type, t.Sequence]) -> bool:",
            "\"\"\"",
            "`isinstance` wrapper to check tensor spec"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2397061)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 't'), position=0, insert_id=2397062)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2397063)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=TypeVar), position=2)",
            "Insert(target_node=ASTNode(type=subscript), node=('attribute', None), position=0, insert_id=2397064)",
            "Insert(target_node=ASTNode(type=subscript), node=('attribute', None), position=7, insert_id=2397065)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 't'), position=0, insert_id=2397066)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2397067)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=Union), position=2)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 't'), position=0, insert_id=2397068)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2397069)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=Sequence), position=2)"
        ],
        "plus_line": 0,
        "minus_line": 0,
        "AST_diff_line": 12,
        "number": 3697,
        "neg_line": [
            "-ST = TypeVar(\"ST\")",
            "-def _isinstance_wrapper(obj: ST, sobj: Union[str, type, Sequence]) -> bool:"
        ],
        "pos_line": [
            "+ST = t.TypeVar(\"ST\")",
            "+def _isinstance_wrapper(obj: ST, sobj: t.Union[str, type, t.Sequence]) -> bool:"
        ],
        "core_change": "-ST = TypeVar(\"ST\") +ST = t.TypeVar(\"ST\") -def _isinstance_wrapper(obj: ST, sobj: Union[str, type, Sequence]) -> bool: +def _isinstance_wrapper(obj: ST, sobj: t.Union[str, type, t.Sequence]) -> bool:",
        "core_API": "TypeVar"
    },
    {
        "commit_hash": "c05ace221339102abb1c8b0933b9cbfd51ddd4a3",
        "index": "ba0904469..52987b8c0 100644",
        "commit_message": "Use f-strings in the dataset scripts (#3291)\n\n* Finishes #3257\n\nUsed f-strings to format the .py files in the dataset folder\n\n* Fix style\n\n* Fix hkcancor dataset\n\nCo-authored-by: Mario ako <mario@huggingface.co>\nCo-authored-by: mariosasko <mariosasko777@gmail.com>\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class Newsqa(datasets.GeneratorBasedBuilder):",
            "",
            "if not os.path.exists(path_to_manual_folder):",
            "raise FileNotFoundError(",
            "-                \"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('newsqa', data_dir=...)` that includes files from the Manual download instructions: {}\".format(",
            "-                    path_to_manual_folder, self.manual_download_instructions",
            "-                )",
            "+                f\"{path_to_manual_folder} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('newsqa', data_dir=...)` that includes files from the Manual download instructions: {self.manual_download_instructions}\"",
            ")",
            "",
            "if self.config.name == \"combined-csv\":"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('string', 'f\"{path_to_manual_folder} does not exist. Make sure you insert a manual dir via `datasets.load_dataset(\\'newsqa\\', data_dir=...)` that includes files from the Manual download instructions: {self.manual_download_instructions}\"'), position=1, insert_id=1781631)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=), text=)), position=2)",
            "Delete(target_node=ASTNode(type=string, text=\"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('newsqa', data_dir=...)` that includes files from the Manual download instructions: {}\"))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=format))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=path_to_manual_folder))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=manual_download_instructions))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=), text=)))"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 16,
        "number": 3698,
        "neg_line": [
            "-\"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('newsqa', data_dir=...)` that includes files from the Manual download instructions: {}\".format(",
            "-path_to_manual_folder, self.manual_download_instructions",
            "-)"
        ],
        "pos_line": [
            "+f\"{path_to_manual_folder} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('newsqa', data_dir=...)` that includes files from the Manual download instructions: {self.manual_download_instructions}\""
        ],
        "core_change": "-\"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('newsqa', data_dir=...)` that includes files from the Manual download instructions: {}\".format( -path_to_manual_folder, self.manual_download_instructions -) +f\"{path_to_manual_folder} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('newsqa', data_dir=...)` that includes files from the Manual download instructions: {self.manual_download_instructions}\"",
        "core_API": "exists"
    },
    {
        "commit_hash": "66c8d011e27ccb5e4a2aa2936d74764eadb58139",
        "index": "3ae145f..071476e 100644",
        "commit_message": "Fix loss function in TF2 synthetic benchmark (#1859)\n\nSigned-off-by: Boyuan Deng <contact@boyuandeng.me>\n",
        "file": "horovod.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def benchmark_step(first_batch):",
            "# Horovod: use DistributedGradientTape",
            "with tf.GradientTape() as tape:",
            "probs = model(data, training=True)",
            "-        loss = tf.losses.categorical_crossentropy(target, probs)",
            "+        loss = tf.losses.sparse_categorical_crossentropy(target, probs)",
            "",
            "# Horovod: add Horovod Distributed GradientTape.",
            "tape = hvd.DistributedGradientTape(tape, compression=compression)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=categorical_crossentropy), value='sparse_categorical_crossentropy')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 3704,
        "neg_line": [
            "-loss = tf.losses.categorical_crossentropy(target, probs)"
        ],
        "pos_line": [
            "+loss = tf.losses.sparse_categorical_crossentropy(target, probs)"
        ],
        "core_change": "-loss = tf.losses.categorical_crossentropy(target, probs) +loss = tf.losses.sparse_categorical_crossentropy(target, probs)",
        "core_API": "GradientTape"
    },
    {
        "commit_hash": "fc5417d033a9af9832a579ac464b721f3c446306",
        "index": "0749470..57ba7aa 100644",
        "commit_message": "Improve quantization, move it to wavenet_ops.py, and add test (#63)\n\n* Slightly improve quantization and add test\n* Remove empty lines\n* Adjust to changed API\n* Fix comment\n* Adjust imports in test\n* Move encode/decode to wavenet_ops.py\n* Address comments\n* Fix test\n* Fix pep8 errors\n* Test that all values are produced by mu law\n* Add periods at the end of comments\n",
        "file": "tensorflow-wavenet.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class WaveNet(object):",
            "The variables are all scoped to the given name.",
            "'''",
            "with tf.variable_scope(name):",
            "-            input_batch = self.encode(input_batch)",
            "+            input_batch = mu_law_encode(input_batch,",
            "+                                        self.quantization_channels)",
            "encoded = self._one_hot(input_batch)",
            "raw_output = self._create_network(encoded)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('identifier', 'mu_law_encode'), position=0, insert_id=2208511)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=2208512)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=attribute), position=3)",
            "Update(target_node=ASTNode(type=identifier, text=encode), value='quantization_channels')"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 3705,
        "neg_line": [
            "-input_batch = self.encode(input_batch)"
        ],
        "pos_line": [
            "+input_batch = mu_law_encode(input_batch,",
            "+self.quantization_channels)"
        ],
        "core_change": "-input_batch = self.encode(input_batch) +input_batch = mu_law_encode(input_batch, +self.quantization_channels)",
        "core_API": "variable_scope"
    },
    {
        "commit_hash": "f54bee3cf9b53df87987af6b8ffca6139d63553c",
        "index": "ae29532e9f..150a60c2c0 100644",
        "commit_message": "changes and fixes (#4643)\n\n* changes and fixes\n\n* changes:\n\n* merge conflicts\n\n* lint\n\n* lint fixes\n\n* lint fixes\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def tensordot(",
            "out: Optional[torch.Tensor] = None,",
            ") -> torch.Tensor:",
            "# find the type to promote to",
            "-    dtype = torch.promote_types(x1.dtype, x2.dtype)",
            "+    dtype = ivy.as_native_dtype(ivy.promote_types(x1.dtype, x2.dtype))",
            "# type conversion to one that torch.tensordot can work with",
            "x1, x2 = x1.type(torch.float32), x2.type(torch.float32)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=319473)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=319474)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'ivy'), position=0, insert_id=319475)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=319476)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'as_native_dtype'), position=2, insert_id=319477)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=319478)",
            "Insert(target_node=IN(type=argument_list), node=('call', None), position=1, insert_id=319479)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=319480)",
            "Move(target_node=IN(type=call), node=ASTNode(type=attribute), position=0)",
            "Move(target_node=IN(type=call), node=ASTNode(type=argument_list), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=torch), value='ivy')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 3706,
        "neg_line": [
            "-dtype = torch.promote_types(x1.dtype, x2.dtype)"
        ],
        "pos_line": [
            "+dtype = ivy.as_native_dtype(ivy.promote_types(x1.dtype, x2.dtype))"
        ],
        "core_change": "-dtype = torch.promote_types(x1.dtype, x2.dtype) +dtype = ivy.as_native_dtype(ivy.promote_types(x1.dtype, x2.dtype))",
        "core_API": "promote_types"
    },
    {
        "commit_hash": "97f8a9eb6688fa36dbe548e02eecdc55acc821a7",
        "index": "2ed0279a..258a175f 100644",
        "commit_message": "fixing a bf16 support issue (#1760)\n\n\n",
        "file": "DeepSpeed.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class PipelineEngine(DeepSpeedEngine):",
            "Returns:",
            "A tensor from torch.zeros() allocated on self.device.",
            "\"\"\"",
            "-        if \"dtype\" not in kwargs and self.fp16_enabled():",
            "-            kwargs[\"dtype\"] = torch.half",
            "+        if \"dtype\" not in kwargs:",
            "+            if self.fp16_enabled():",
            "+                kwargs[\"dtype\"] = torch.half",
            "+            if self.bfloat16_enabled():",
            "+                kwargs[\"dtype\"] = torch.bfloat16",
            "",
            "return torch.zeros(shape, device=self.device, **kwargs)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=half), value='bfloat16')"
        ],
        "plus_line": 5,
        "minus_line": 2,
        "AST_diff_line": 1,
        "number": 3708,
        "neg_line": [
            "-if \"dtype\" not in kwargs and self.fp16_enabled():",
            "-kwargs[\"dtype\"] = torch.half"
        ],
        "pos_line": [
            "+if \"dtype\" not in kwargs:",
            "+if self.fp16_enabled():",
            "+kwargs[\"dtype\"] = torch.half",
            "+if self.bfloat16_enabled():",
            "+kwargs[\"dtype\"] = torch.bfloat16"
        ],
        "core_change": "-if \"dtype\" not in kwargs and self.fp16_enabled(): -kwargs[\"dtype\"] = torch.half +if \"dtype\" not in kwargs: +if self.fp16_enabled(): +kwargs[\"dtype\"] = torch.half +if self.bfloat16_enabled(): +kwargs[\"dtype\"] = torch.bfloat16",
        "core_API": "zeros"
    },
    {
        "commit_hash": "fe1da3c70661a5f95e20aeb7307c7d1716f39f70",
        "index": "3a6e89468a..ce9fb32d08 100644",
        "commit_message": "lint fixes\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def sigmoid(x: torch.Tensor, out: Optional[torch.Tensor] = None) -> torch.Tensor",
            "return torch.sigmoid(x, out=out)",
            "",
            "",
            "-def softmax(x: torch.Tensor, axis: Optional[int] = None, out: Optional[torch.Tensor] = None) -> torch.Tensor:",
            "+def softmax(",
            "+    x: torch.Tensor, axis: Optional[int] = None, out: Optional[torch.Tensor] = None",
            "+) -> torch.Tensor:",
            "exp_x = torch.exp(x, out=out)",
            "return exp_x / torch.sum(exp_x, axis, keepdims=True)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('ERROR', None), position=1, insert_id=350672)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=), text=)), position=0)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=->, text=->), position=1)",
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=attribute), position=0)",
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=:, text=:), position=1)",
            "Insert(target_node=ASTNode(type=assignment), node=('type', None), position=2, insert_id=350673)",
            "Move(target_node=IN(type=type), node=ASTNode(type=identifier, text=exp_x), position=0)",
            "Move(target_node=ASTNode(type=expression_list), node=ASTNode(type=ERROR), position=2)",
            "Move(target_node=ASTNode(type=expression_list), node=ASTNode(type=none, text=None), position=3)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 9,
        "number": 3709,
        "neg_line": [
            "-def softmax(x: torch.Tensor, axis: Optional[int] = None, out: Optional[torch.Tensor] = None) -> torch.Tensor:"
        ],
        "pos_line": [
            "+def softmax(",
            "+x: torch.Tensor, axis: Optional[int] = None, out: Optional[torch.Tensor] = None",
            "+) -> torch.Tensor:"
        ],
        "core_change": "-def softmax(x: torch.Tensor, axis: Optional[int] = None, out: Optional[torch.Tensor] = None) -> torch.Tensor: +def softmax( +x: torch.Tensor, axis: Optional[int] = None, out: Optional[torch.Tensor] = None +) -> torch.Tensor:",
        "core_API": "sigmoid"
    },
    {
        "commit_hash": "15ef52bc732d1f907de4de58683f131652c0d68c",
        "index": "960eea34e..a42f3c4d2 100644",
        "commit_message": "Rename LightningLite to Fabric (#16244)\n\n* Rename LightningLite to Fabric\n\n* Fix introspection test\n\n* Fix deprecated Lite tests\n\n* Undo accidental Horovod removal\n\n* Fixes\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "cuda",
        "change": [
            "def main_train(dir_path, max_epochs: int = 20):",
            "stopping = EarlyStopping(monitor=\"val_acc\", mode=\"max\", min_delta=0.005)",
            "trainer = pl.Trainer(",
            "default_root_dir=dir_path,",
            "-        gpus=int(torch.cuda.is_available()),",
            "+        devices=int(torch.cuda.is_available()),",
            "precision=(16 if torch.cuda.is_available() else 32),",
            "callbacks=[stopping],",
            "min_epochs=3,"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=gpus), value='devices')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 3710,
        "neg_line": [
            "-gpus=int(torch.cuda.is_available()),"
        ],
        "pos_line": [
            "+devices=int(torch.cuda.is_available()),"
        ],
        "core_change": "-gpus=int(torch.cuda.is_available()), +devices=int(torch.cuda.is_available()),",
        "core_API": "Trainer"
    },
    {
        "commit_hash": "733c0304c28aa1a16d048a6d79b5cefac07a9eb3",
        "index": "7cbc4aa80..88d815884 100644",
        "commit_message": "fixed default arguments\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class LogMelFbank(AbsFeatsExtract):",
            ")",
            "",
            "def forward(",
            "-        self, input: torch.Tensor, input_lengths: torch.Tensor",
            "+        self, input: torch.Tensor, input_lengths: torch.Tensor = None",
            ") -> Tuple[torch.Tensor, torch.Tensor]:",
            "# 1. Domain-conversion: e.g. Stft: time -> time-freq",
            "input_stft, feats_lens = self.stft(input, input_lengths)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=parameters), node=('typed_default_parameter', None), position=5, insert_id=150847)",
            "Move(target_node=IN(type=typed_default_parameter), node=ASTNode(type=identifier, text=input_lengths), position=0)",
            "Move(target_node=IN(type=typed_default_parameter), node=ASTNode(type=:, text=:), position=1)",
            "Move(target_node=IN(type=typed_default_parameter), node=ASTNode(type=type), position=2)",
            "Insert(target_node=IN(type=typed_default_parameter), node=('=', '='), position=3, insert_id=150848)",
            "Insert(target_node=IN(type=typed_default_parameter), node=('none', 'None'), position=4, insert_id=150849)",
            "Delete(target_node=ASTNode(type=typed_parameter))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 3712,
        "neg_line": [
            "-self, input: torch.Tensor, input_lengths: torch.Tensor"
        ],
        "pos_line": [
            "+self, input: torch.Tensor, input_lengths: torch.Tensor = None"
        ],
        "core_change": "-self, input: torch.Tensor, input_lengths: torch.Tensor +self, input: torch.Tensor, input_lengths: torch.Tensor = None",
        "core_API": "stft"
    },
    {
        "commit_hash": "2c5f2cd9a8c9a6fa38b2d3b9674cbf38760af8d5",
        "index": "107b2c3f..6d9efc66 100644",
        "commit_message": "Fix `NeighborLoader` tests on Windows (#5988)\n\n`pyg-lib` is not yet available on Windows, so `dtype` tests do not pass.\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_homogeneous_neighbor_loader(directed, dtype):",
            "@pytest.mark.parametrize('directed', [True])  # TODO re-enable undirected mode",
            "@pytest.mark.parametrize('dtype', [torch.int64, torch.int32])",
            "def test_heterogeneous_neighbor_loader(directed, dtype):",
            "+    if dtype != torch.int64 and not _WITH_PYG_LIB:",
            "+        return",
            "+",
            "torch.manual_seed(12345)",
            "",
            "data = HeteroData()"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('block', None), position=0, insert_id=950425)",
            "Insert(target_node=IN(type=block), node=('if_statement', None), position=0, insert_id=950426)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=950427)",
            "Insert(target_node=IN(type=if_statement), node=('boolean_operator', None), position=1, insert_id=950428)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=950429)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=950430)",
            "Insert(target_node=IN(type=boolean_operator), node=('comparison_operator', None), position=0, insert_id=950431)",
            "Insert(target_node=IN(type=boolean_operator), node=('and', 'and'), position=1, insert_id=950432)",
            "Insert(target_node=IN(type=boolean_operator), node=('not_operator', None), position=2, insert_id=950433)",
            "Insert(target_node=IN(type=block), node=('return_statement', None), position=0, insert_id=950434)",
            "Insert(target_node=IN(type=comparison_operator), node=('identifier', 'dtype'), position=0, insert_id=950435)",
            "Insert(target_node=IN(type=comparison_operator), node=('!=', '!='), position=1, insert_id=950436)",
            "Insert(target_node=IN(type=comparison_operator), node=('attribute', None), position=2, insert_id=950437)",
            "Insert(target_node=IN(type=not_operator), node=('not', 'not'), position=0, insert_id=950438)",
            "Insert(target_node=IN(type=not_operator), node=('identifier', '_WITH_PYG_LIB'), position=1, insert_id=950439)",
            "Insert(target_node=IN(type=return_statement), node=('return', 'return'), position=0, insert_id=950440)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=950441)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=950442)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'int64'), position=2, insert_id=950443)",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 2,
        "minus_line": 0,
        "AST_diff_line": 20,
        "number": 3713,
        "neg_line": [],
        "pos_line": [
            "+if dtype != torch.int64 and not _WITH_PYG_LIB:",
            "+return",
            "+"
        ],
        "core_change": "+if dtype != torch.int64 and not _WITH_PYG_LIB: +return +",
        "core_API": "parametrize"
    },
    {
        "commit_hash": "dcacbc09ea21a4f8d3a97b38ac81a123929ebb16",
        "index": "63cd77e2..f40d56fc 100644",
        "commit_message": "center transform, py 2.7 fixes\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def coalesce(edge_index, edge_attr=None, num_nodes=None):",
            "else:",
            "sparse = getattr(torch.sparse, edge_attr.type().split('.')[-1])",
            "n = num_nodes",
            "-        size = torch.Size([n, n, *list(edge_attr.size())[1:]])",
            "+        size = torch.Size([n, n] + list(edge_attr.size())[1:])",
            "adj = sparse(edge_index, edge_attr, size).coalesce()",
            "edge_index, edge_attr = adj._indices(), adj._values()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('binary_operator', None), position=1, insert_id=1071899)",
            "Insert(target_node=IN(type=binary_operator), node=('list', None), position=0, insert_id=1071900)",
            "Insert(target_node=IN(type=binary_operator), node=('+', '+'), position=1, insert_id=1071901)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=subscript), position=2)",
            "Move(target_node=IN(type=list), node=ASTNode(type=[, text=[), position=0)",
            "Move(target_node=IN(type=list), node=ASTNode(type=identifier, text=n), position=1)",
            "Move(target_node=IN(type=list), node=ASTNode(type=,, text=,), position=2)",
            "Move(target_node=IN(type=list), node=ASTNode(type=identifier, text=n), position=3)",
            "Insert(target_node=IN(type=list), node=(']', ']'), position=4, insert_id=1071902)",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=*, text=*))",
            "Delete(target_node=ASTNode(type=list_splat))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=list))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 14,
        "number": 3716,
        "neg_line": [
            "-size = torch.Size([n, n, *list(edge_attr.size())[1:]])"
        ],
        "pos_line": [
            "+size = torch.Size([n, n] + list(edge_attr.size())[1:])"
        ],
        "core_change": "-size = torch.Size([n, n, *list(edge_attr.size())[1:]]) +size = torch.Size([n, n] + list(edge_attr.size())[1:])",
        "core_API": "type"
    },
    {
        "commit_hash": "5f4c6e830f603830117877b8990a0e65a2386aa6",
        "index": "b38a49025..e165930a2 100644",
        "commit_message": "Quick fix :) (#606)\n\n* Changing the name\n\n* style + quality\n\n* update doc and logo\n\n* clean up\n\n* circle-CI on the branche for now\n\n* fix daily dialog dataset\n\n* fix urls\n\nCo-authored-by: Quentin Lhoest <lhoest.q@gmail.com>\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class BaseDatasetTest(TestCase):",
            "def reduce_ex(self):",
            "raise pickle.PicklingError()",
            "",
            "-        nlp.arrow_dataset.logger.__reduce_ex__ = reduce_ex",
            "+        datasets.arrow_dataset.logger.__reduce_ex__ = reduce_ex",
            "",
            "def _create_dummy_dataset(self, in_memory: bool, tmp_dir: str, multiple_columns=False):",
            "if multiple_columns:"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=nlp), value='datasets')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 3717,
        "neg_line": [
            "-nlp.arrow_dataset.logger.__reduce_ex__ = reduce_ex"
        ],
        "pos_line": [
            "+datasets.arrow_dataset.logger.__reduce_ex__ = reduce_ex"
        ],
        "core_change": "-nlp.arrow_dataset.logger.__reduce_ex__ = reduce_ex +datasets.arrow_dataset.logger.__reduce_ex__ = reduce_ex",
        "core_API": "PicklingError"
    },
    {
        "commit_hash": "1beddcdfb78a89eb44898882b8ecf5f91e104542",
        "index": "6ed94b25..5e04a94f 100644",
        "commit_message": "a few more doc fixes (#4078)\n\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class BiMpmMatching(nn.Module, FromParams):",
            "",
            "# Returns",
            "",
            "-        A tuple of matching vectors for the two sentences. Each of which is a list of",
            "-        matching vectors of shape (batch, seq_len, num_perspectives or 1)",
            "+        `Tuple[List[torch.Tensor], List[torch.Tensor]]` :",
            "+            A tuple of matching vectors for the two sentences. Each of which is a list of",
            "+            matching vectors of shape (batch, seq_len, num_perspectives or 1)",
            "\"\"\"",
            "assert (not mask_2.requires_grad) and (not mask_1.requires_grad)",
            "assert context_1.size(-1) == context_2.size(-1) == self.hidden_dim"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('ERROR', None), position=0, insert_id=18402)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=class_definition), position=0)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=\", text=\"\"\"), position=1)",
            "Insert(target_node=ASTNode(type=class_definition), node=('ERROR', None), position=3, insert_id=18403)",
            "Insert(target_node=ASTNode(type=class_definition), node=(':', ':'), position=4, insert_id=18404)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=:, text=:), position=0)",
            "Insert(target_node=IN(type=ERROR), node=('string', '`Tuple[List[torch.Tensor], List[torch.Tensor]]`'), position=1, insert_id=18405)",
            "Delete(target_node=ASTNode(type=ERROR))"
        ],
        "plus_line": 3,
        "minus_line": 2,
        "AST_diff_line": 8,
        "number": 3718,
        "neg_line": [
            "-A tuple of matching vectors for the two sentences. Each of which is a list of",
            "-matching vectors of shape (batch, seq_len, num_perspectives or 1)"
        ],
        "pos_line": [
            "+`Tuple[List[torch.Tensor], List[torch.Tensor]]` :",
            "+A tuple of matching vectors for the two sentences. Each of which is a list of",
            "+matching vectors of shape (batch, seq_len, num_perspectives or 1)"
        ],
        "core_change": "-A tuple of matching vectors for the two sentences. Each of which is a list of -matching vectors of shape (batch, seq_len, num_perspectives or 1) +`Tuple[List[torch.Tensor], List[torch.Tensor]]` : +A tuple of matching vectors for the two sentences. Each of which is a list of +matching vectors of shape (batch, seq_len, num_perspectives or 1)",
        "core_API": "size"
    },
    {
        "commit_hash": "695bf1a1f64399cc2eba83932869de57ddccc373",
        "index": "8ea54f0e..6ddd0b6b 100644",
        "commit_message": "bug fix for illegal memory reach\n\n",
        "file": "TTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class GravesAttention(nn.Module):",
            "",
            "def init_states(self, inputs):",
            "if self.J is None or inputs.shape[1] > self.J.shape[-1]:",
            "-            self.J = torch.arange(0, inputs.shape[1]).expand_as(torch.Tensor(inputs.shape[0], self.K, inputs.shape[1])).to(inputs.device)",
            "+            self.J = torch.arange(0, inputs.shape[1]).to(inputs.device).expand([inputs.shape[0], self.K, inputs.shape[1]])",
            "self.attention_weights = torch.zeros(inputs.shape[0], inputs.shape[1]).to(inputs.device)",
            "self.mu_prev = torch.zeros(inputs.shape[0], self.K).to(inputs.device)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=IN(type=root), node=ASTNode(type=call), position=0)",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1271576)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1271577)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'expand'), position=2, insert_id=1271578)",
            "Insert(target_node=ASTNode(type=argument_list), node=('list', None), position=1, insert_id=1271579)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=argument_list), position=1)",
            "Insert(target_node=IN(type=list), node=('[', '['), position=0, insert_id=1271580)",
            "Move(target_node=IN(type=list), node=ASTNode(type=subscript), position=1)",
            "Move(target_node=IN(type=list), node=ASTNode(type=,, text=,), position=2)",
            "Move(target_node=IN(type=list), node=ASTNode(type=attribute), position=3)",
            "Move(target_node=IN(type=list), node=ASTNode(type=,, text=,), position=4)",
            "Move(target_node=IN(type=list), node=ASTNode(type=subscript), position=5)",
            "Insert(target_node=IN(type=list), node=(']', ']'), position=6, insert_id=1271581)",
            "Update(target_node=ASTNode(type=identifier, text=expand_as), value='to')",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=Tensor))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=to))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 25,
        "number": 3720,
        "neg_line": [
            "-self.J = torch.arange(0, inputs.shape[1]).expand_as(torch.Tensor(inputs.shape[0], self.K, inputs.shape[1])).to(inputs.device)"
        ],
        "pos_line": [
            "+self.J = torch.arange(0, inputs.shape[1]).to(inputs.device).expand([inputs.shape[0], self.K, inputs.shape[1]])"
        ],
        "core_change": "-self.J = torch.arange(0, inputs.shape[1]).expand_as(torch.Tensor(inputs.shape[0], self.K, inputs.shape[1])).to(inputs.device) +self.J = torch.arange(0, inputs.shape[1]).to(inputs.device).expand([inputs.shape[0], self.K, inputs.shape[1]])",
        "core_API": "arange"
    },
    {
        "commit_hash": "592ca5055c38cac37aee86bd62fef977f1900f62",
        "index": "c4e01b19..1e9e0bfd 100644",
        "commit_message": "Test fixtures (#4241)\n\n* fix test\n\n* typo\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* skip tests\n\n* run cron only in master repo\n\n* fix test\n\n* update\n\n* Add test\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "from torch_geometric.utils import to_dense_adj",
            "try:",
            "rowptr = torch.tensor([0, 1])",
            "col = torch.tensor([0])",
            "-    torch.ops.torch_sparse.partition(rowptr, col, None, 1)",
            "+    torch.ops.torch_sparse.partition(rowptr, col, None, 1, True)",
            "with_metis = True",
            "except RuntimeError:",
            "with_metis = False"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=8, insert_id=985012)",
            "Insert(target_node=ASTNode(type=argument_list), node=('true', 'True'), position=9, insert_id=985013)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 3722,
        "neg_line": [
            "-torch.ops.torch_sparse.partition(rowptr, col, None, 1)"
        ],
        "pos_line": [
            "+torch.ops.torch_sparse.partition(rowptr, col, None, 1, True)"
        ],
        "core_change": "-torch.ops.torch_sparse.partition(rowptr, col, None, 1) +torch.ops.torch_sparse.partition(rowptr, col, None, 1, True)",
        "core_API": "tensor"
    },
    {
        "commit_hash": "704f9fc6502865d84ded405225e13bfeabe8338e",
        "index": "fa47795f..21a5374d 100644",
        "commit_message": "Fix svdvals usage (#1926)\n\n* big svdvals\n\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def solve_pnp_dlt(",
            "# Checking if world_points_norm (of any element of the batch) has rank = 3. This",
            "# function cannot be used if all world points (of any element of the batch) lie",
            "# on a line or if all world points (of any element of the batch) lie on a plane.",
            "-    s = torch.linalg.svdvals(world_points_norm)",
            "+    s = _torch_linalg_svdvals(world_points_norm)",
            "if torch.any(s[:, -1] < svd_eps):",
            "raise AssertionError(",
            "f\"The last singular value of one/more of the elements of the batch is smaller \""
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=torch), value='_torch_linalg_svdvals')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=torch), position=0)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=linalg))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=svdvals))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 3723,
        "neg_line": [
            "-s = torch.linalg.svdvals(world_points_norm)"
        ],
        "pos_line": [
            "+s = _torch_linalg_svdvals(world_points_norm)"
        ],
        "core_change": "-s = torch.linalg.svdvals(world_points_norm) +s = _torch_linalg_svdvals(world_points_norm)",
        "core_API": "svdvals"
    },
    {
        "commit_hash": "ddd11075bd01c5a68644c2ef1e976f987d0907f6",
        "index": "bfa597f6e..9549ad718 100644",
        "commit_message": "[WIP] ref: deprecated results obj, added support for simpler comms (1/n) (#3681)\n\n* ref: deprecated results obj, added support for simpler comms. Decouples logging from loops\n\n* ref: deprecated results obj, added support for simpler comms. Decouples logging from loops\n\n* ref: deprecated results obj, added support for simpler comms. Decouples logging from loops\n\n* ref: deprecated results obj, added support for simpler comms. Decouples logging from loops\n\n* ref: deprecated results obj, added support for simpler comms. Decouples logging from loops\n\n* ref: deprecated results obj, added support for simpler comms. Decouples logging from loops\n\n* fix global step err\n\n* fix global step err\n\n* fix global step err\n\n* fix global step err\n\n* fix global step err\n\n* fix typing err\n\n* fix str\n\n* fix typing err\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_train_step_epoch_end_scalar(tmpdir):",
            "train_step_out = out.training_step_output_for_epoch_end",
            "assert len(train_step_out) == 1",
            "train_step_out = train_step_out[0][0]",
            "-    assert isinstance(train_step_out, torch.Tensor)",
            "-    assert train_step_out.item() == 171",
            "+    assert isinstance(train_step_out['minimize'], torch.Tensor)",
            "+    assert train_step_out['minimize'].item() == 171",
            "",
            "# make sure the optimizer closure returns the correct things",
            "opt_closure_result = trainer.train_loop.training_step_and_backward("
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('subscript', None), position=1, insert_id=565503)",
            "Move(target_node=IN(type=subscript), node=ASTNode(type=identifier, text=train_step_out), position=0)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=565504)",
            "Insert(target_node=IN(type=subscript), node=('string', \"'minimize'\"), position=2, insert_id=565505)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=565506)",
            "Insert(target_node=ASTNode(type=attribute), node=('subscript', None), position=0, insert_id=565507)",
            "Move(target_node=IN(type=subscript), node=ASTNode(type=identifier, text=train_step_out), position=0)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=565508)",
            "Insert(target_node=IN(type=subscript), node=('string', \"'minimize'\"), position=2, insert_id=565509)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=565510)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 10,
        "number": 3724,
        "neg_line": [
            "-assert isinstance(train_step_out, torch.Tensor)",
            "-assert train_step_out.item() == 171"
        ],
        "pos_line": [
            "+assert isinstance(train_step_out['minimize'], torch.Tensor)",
            "+assert train_step_out['minimize'].item() == 171"
        ],
        "core_change": "-assert isinstance(train_step_out, torch.Tensor) -assert train_step_out.item() == 171 +assert isinstance(train_step_out['minimize'], torch.Tensor) +assert train_step_out['minimize'].item() == 171",
        "core_API": "item"
    },
    {
        "commit_hash": "43043ee4d5c672f7fbc22ac9559e8164731fd053",
        "index": "c5e13bf5e7..7faaf6da9e 100644",
        "commit_message": "[RLlib] Tf2x preparation; part 2 (upgrading `try_import_tf()`). (#9136)\n\n* WIP.\n\n* Fixes.\n\n* LINT.\n\n* WIP.\n\n* WIP.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* WIP.\n\n* Fixes.\n\n* Test\n\n* Fix.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* LINT.\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def build_q_losses(policy, model, _, train_batch):",
            "",
            "",
            "def adam_optimizer(policy, config):",
            "-    return tf.train.AdamOptimizer(",
            "+    return tf1.train.AdamOptimizer(",
            "learning_rate=policy.cur_lr, epsilon=config[\"adam_epsilon\"])"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tf), value='tf1')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 3725,
        "neg_line": [
            "-return tf.train.AdamOptimizer("
        ],
        "pos_line": [
            "+return tf1.train.AdamOptimizer("
        ],
        "core_change": "-return tf.train.AdamOptimizer( +return tf1.train.AdamOptimizer(",
        "core_API": "AdamOptimizer"
    },
    {
        "commit_hash": "7b8cad3ad8449f260e3b282bf0f9b824cc94b58d",
        "index": "a3fa6954..97e45d48 100644",
        "commit_message": "Bug fixed (#475)\n\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def rgb_to_hsv(image: torch.Tensor) -> torch.Tensor:",
            "",
            "h = (h / 6.0) % 1.0",
            "",
            "-    h = 2 * pi * h",
            "+    h = 2 * pi.to(image.device) * h",
            "return torch.stack([h, s, v], dim=-3)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=binary_operator), node=('call', None), position=2, insert_id=443812)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=443813)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=443814)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=pi), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=443815)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to'), position=2, insert_id=443816)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=443817)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=443818)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=443819)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'image'), position=0, insert_id=443820)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=443821)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=443822)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 12,
        "number": 3730,
        "neg_line": [
            "-h = 2 * pi * h"
        ],
        "pos_line": [
            "+h = 2 * pi.to(image.device) * h"
        ],
        "core_change": "-h = 2 * pi * h +h = 2 * pi.to(image.device) * h",
        "core_API": "to"
    },
    {
        "commit_hash": "0070252e186dba391147d72544498eae493dfca1",
        "index": "9cc9460fb..71397f005 100644",
        "commit_message": "Move complex tensor related functions to espnet2/enh/layers/complex_utils.py; Refactor complex support code; fix unit tests\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DNN_Beamformer(torch.nn.Module):",
            "",
            "if isinstance(data, ComplexTensor):",
            "complex_wrapper = FC",
            "-        elif is_torch_1_8_plus and torch.is_complex(data):",
            "+        elif is_torch_1_9_plus and torch.is_complex(data):",
            "complex_wrapper = torch",
            "else:",
            "raise ValueError("
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=is_torch_1_8_plus), value='is_torch_1_9_plus')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 3731,
        "neg_line": [
            "-elif is_torch_1_8_plus and torch.is_complex(data):"
        ],
        "pos_line": [
            "+elif is_torch_1_9_plus and torch.is_complex(data):"
        ],
        "core_change": "-elif is_torch_1_8_plus and torch.is_complex(data): +elif is_torch_1_9_plus and torch.is_complex(data):",
        "core_API": "is_complex"
    },
    {
        "commit_hash": "aeaa4228edaffe255d16950b06ea49cf59c1bd96",
        "index": "fba1805c..e4dceb0e 100644",
        "commit_message": "Fix bugs in tests\n\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Tester(unittest.TestCase):",
            "def test_rotation_matrix_to_angle_axis_gradcheck(self):",
            "# generate input data",
            "batch_size = 2",
            "-        rmat = torch.rand(batch_size, 4, 4)",
            "+        rmat = torch.rand(batch_size, 3, 4)",
            "rmat = utils.tensor_to_gradcheck_var(rmat)  # to var",
            "",
            "# evaluate function gradient"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=integer, text=4), value='3')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 3733,
        "neg_line": [
            "-rmat = torch.rand(batch_size, 4, 4)"
        ],
        "pos_line": [
            "+rmat = torch.rand(batch_size, 3, 4)"
        ],
        "core_change": "-rmat = torch.rand(batch_size, 4, 4) +rmat = torch.rand(batch_size, 3, 4)",
        "core_API": "rand"
    },
    {
        "commit_hash": "9349c6da74165943716a5cd0d637f2c9bfdefda7",
        "index": "0a02394e..b57e416c 100644",
        "commit_message": "fix imgwarp test\n\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def warp_perspective(src, M, dsize, flags='bilinear', border_mode=None,",
            "- Output: :math:`(B, C, H, W)`",
            "",
            ".. note::",
            "-       See a working example `here <../../../examples/warp_perspective.ipynb>`_.",
            "+       See a working example `here <https://github.com/arraiy/torchgeometry/blob/master/examples/warp_perspective.ipynb>`_.",
            "\"\"\"",
            "if not torch.is_tensor(src):",
            "raise TypeError(\"Input src type is not a torch.Tensor. Got {}\""
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=ERROR), node=('ERROR', None), position=13, insert_id=478924)",
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=identifier, text=note), position=14)",
            "Insert(target_node=ASTNode(type=ERROR), node=('ERROR', None), position=15, insert_id=478925)",
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=:, text=:), position=16)",
            "Insert(target_node=ASTNode(type=ERROR), node=('ERROR', None), position=17, insert_id=478926)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=., text=.), position=0)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=., text=.), position=1)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=:, text=:), position=0)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=identifier, text=See), position=0)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=identifier, text=a), position=1)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=identifier, text=working), position=2)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=identifier, text=example), position=3)",
            "Update(target_node=ASTNode(type=string, text=`here <../../../examples/warp_perspective.ipynb>`), value='`here <https://github.com/arraiy/torchgeometry/blob/master/examples/warp_perspective.ipynb>`')",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=string, text=`here <../../../examples/warp_perspective.ipynb>`), position=4)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=identifier, text=_), position=5)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=., text=.), position=6)",
            "Delete(target_node=ASTNode(type=identifier, text=Output))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=identifier, text=math))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=string, text=`(B, C, H, W)`))",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=ERROR))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 25,
        "number": 3735,
        "neg_line": [
            "-Output: :math:`(B, C, H, W)`",
            "-See a working example `here <../../../examples/warp_perspective.ipynb>`_."
        ],
        "pos_line": [
            "+See a working example `here <https://github.com/arraiy/torchgeometry/blob/master/examples/warp_perspective.ipynb>`_."
        ],
        "core_change": "-Output: :math:`(B, C, H, W)` -See a working example `here <../../../examples/warp_perspective.ipynb>`_. +See a working example `here <https://github.com/arraiy/torchgeometry/blob/master/examples/warp_perspective.ipynb>`_.",
        "core_API": "is_tensor"
    },
    {
        "commit_hash": "b197c0c4044f66628c6672fe78581768a54d0e59",
        "index": "a531bc07bf..461296ecd6 100644",
        "commit_message": "[rllib] General RNN support (#2299)\n\n* wip\n\n* cls\n\n* re\n\n* wip\n\n* wip\n\n* a3c working\n\n* torch support\n\n* pg works\n\n* lint\n\n* rm v2\n\n* consumer id\n\n* clean up pg\n\n* clean up more\n\n* fix python 2.7\n\n* tf session management\n\n* docs\n\n* dqn wip\n\n* fix compile\n\n* dqn\n\n* apex runs\n\n* up\n\n* impotrs\n\n* ddpg\n\n* quotes\n\n* fix tests\n\n* fix last r\n\n* fix tests\n\n* lint\n\n* pass checkpoint restore\n\n* kwar\n\n* nits\n\n* policy graph\n\n* fix yapf\n\n* com\n\n* class\n\n* pyt\n\n* vectorization\n\n* update\n\n* test cpe\n\n* unit test\n\n* fix ddpg2\n\n* changes\n\n* wip\n\n* args\n\n* faster test\n\n* common\n\n* fix\n\n* add alg option\n\n* batch mode and policy serving\n\n* multi serving test\n\n* todo\n\n* wip\n\n* serving test\n\n* doc async env\n\n* num envs\n\n* comments\n\n* thread\n\n* remove init hook\n\n* update\n\n* fix ppo\n\n* comments1\n\n* fix\n\n* updates\n\n* add jenkins tests\n\n* fix\n\n* fix pytorch\n\n* fix\n\n* fixes\n\n* fix a3c policy\n\n* fix squeeze\n\n* fix trunc on apex\n\n* fix squeezing for real\n\n* update\n\n* remove horizon test for now\n\n* multiagent wip\n\n* update\n\n* fix race condition\n\n* fix ma\n\n* t\n\n* doc\n\n* st\n\n* wip\n\n* example\n\n* wip\n\n* working\n\n* cartpole\n\n* wip\n\n* batch wip\n\n* fix bug\n\n* make other_batches None default\n\n* working\n\n* debug\n\n* nit\n\n* warn\n\n* comments\n\n* fix ppo\n\n* fix obs filter\n\n* update\n\n* wip\n\n* tf\n\n* update\n\n* fix\n\n* cleanup\n\n* cleanup\n\n* spacing\n\n* model\n\n* fix\n\n* dqn\n\n* fix ddpg\n\n* doc\n\n* keep names\n\n* update\n\n* fix\n\n* com\n\n* docs\n\n* clarify model outputs\n\n* Update torch_policy_graph.py\n\n* fix obs filter\n\n* pass thru worker index\n\n* fix\n\n* rename\n\n* vlad torch comments\n\n* fix log action\n\n* debug name\n\n* fix lstm\n\n* remove unused ddpg net\n\n* remove conv net\n\n* revert lstm\n\n* wip\n\n* wip\n\n* cast\n\n* wip\n\n* works\n\n* fix a3c\n\n* works\n\n* lstm util test\n\n* doc\n\n* clean up\n\n* update\n\n* fix lstm check\n\n* move to end\n\n* fix sphinx\n\n* fix cmd\n\n* remove bad doc\n\n* clarify\n\n* copy\n\n* async sa\n\n* fix\n\n* comments\n\n* fix a3c conf\n\n* tune lstm\n\n* fix reshape\n\n* fix\n\n* back to 16\n\n* tuned a3c update\n\n* update\n\n* tuned\n\n* optional\n\n* fix catalog\n\n* remove prep\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def normc_initializer(std=1.0):",
            "return _initializer",
            "",
            "",
            "+def get_activation_fn(name):",
            "+    return getattr(tf.nn, name)",
            "+",
            "+",
            "def conv2d(x, num_filters, name, filter_size=(3, 3), stride=(1, 1), pad=\"SAME\",",
            "dtype=tf.float32, collections=None):",
            "with tf.variable_scope(name):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('function_definition', None), position=2, insert_id=2154534)",
            "Insert(target_node=IN(type=function_definition), node=('def', 'def'), position=0, insert_id=2154535)",
            "Insert(target_node=IN(type=function_definition), node=('identifier', 'get_activation_fn'), position=1, insert_id=2154536)",
            "Insert(target_node=IN(type=function_definition), node=('parameters', None), position=2, insert_id=2154537)",
            "Insert(target_node=IN(type=function_definition), node=(':', ':'), position=3, insert_id=2154538)",
            "Insert(target_node=IN(type=function_definition), node=('block', None), position=4, insert_id=2154539)",
            "Insert(target_node=IN(type=parameters), node=('(', '('), position=0, insert_id=2154540)",
            "Insert(target_node=IN(type=parameters), node=('identifier', 'name'), position=1, insert_id=2154541)",
            "Insert(target_node=IN(type=parameters), node=(')', ')'), position=2, insert_id=2154542)",
            "Insert(target_node=IN(type=block), node=('return_statement', None), position=0, insert_id=2154543)",
            "Insert(target_node=IN(type=return_statement), node=('return', 'return'), position=0, insert_id=2154544)",
            "Insert(target_node=IN(type=return_statement), node=('call', None), position=1, insert_id=2154545)",
            "Insert(target_node=IN(type=call), node=('identifier', 'getattr'), position=0, insert_id=2154546)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2154547)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2154548)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=2154549)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=2154550)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'name'), position=3, insert_id=2154551)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=2154552)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2154553)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2154554)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'nn'), position=2, insert_id=2154555)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 22,
        "number": 3740,
        "neg_line": [],
        "pos_line": [
            "+def get_activation_fn(name):",
            "+return getattr(tf.nn, name)",
            "+",
            "+"
        ],
        "core_change": "+def get_activation_fn(name): +return getattr(tf.nn, name) + +",
        "core_API": "variable_scope"
    },
    {
        "commit_hash": "1fb4c41ff94e98f7930925337f451e33137c927c",
        "index": "1787277e..ce40842b 100755",
        "commit_message": "fixed summarizer flush\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Model(object):",
            "Returns:",
            "Checkpoint path where the model was saved.",
            "\"\"\"",
            "-        self.monitored_session.run(fetches=self.summarizer_flush)",
            "+        if self.flush_summarizer is not None:",
            "+            self.monitored_session.run(fetches=self.flush_summarizer)",
            "",
            "return self.saver.save(",
            "sess=self.session,"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=ERROR), node=('if', 'if'), position=9, insert_id=2234806)",
            "Insert(target_node=ASTNode(type=type), node=('comparison_operator', None), position=0, insert_id=2234807)",
            "Insert(target_node=IN(type=comparison_operator), node=('attribute', None), position=0, insert_id=2234808)",
            "Insert(target_node=IN(type=comparison_operator), node=('is not', 'is'), position=1, insert_id=2234809)",
            "Insert(target_node=IN(type=comparison_operator), node=('is not', 'not'), position=2, insert_id=2234810)",
            "Insert(target_node=IN(type=comparison_operator), node=('ERROR', None), position=3, insert_id=2234811)",
            "Move(target_node=IN(type=comparison_operator), node=ASTNode(type=call), position=4)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=2234812)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2234813)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'flush_summarizer'), position=2, insert_id=2234814)",
            "Insert(target_node=IN(type=ERROR), node=('none', 'None'), position=0, insert_id=2234815)",
            "Insert(target_node=IN(type=ERROR), node=(':', ':'), position=1, insert_id=2234816)",
            "Update(target_node=ASTNode(type=identifier, text=summarizer_flush), value='flush_summarizer')"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 13,
        "number": 3741,
        "neg_line": [
            "-self.monitored_session.run(fetches=self.summarizer_flush)"
        ],
        "pos_line": [
            "+if self.flush_summarizer is not None:",
            "+self.monitored_session.run(fetches=self.flush_summarizer)"
        ],
        "core_change": "-self.monitored_session.run(fetches=self.summarizer_flush) +if self.flush_summarizer is not None: +self.monitored_session.run(fetches=self.flush_summarizer)",
        "core_API": "run"
    },
    {
        "commit_hash": "8777de3c4b8a84bfce94036bd95ebc2afbfce3e8",
        "index": "ffed2bf0..fb7b8e1e 100755",
        "commit_message": "Fix import order in model, reformatting.\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Evolutionary(Optimizer):",
            "diffs_list.append(diffs)",
            "",
            "with tf.control_dependencies(control_inputs=diffs):",
            "-            diffs = [tf.add_n(inputs=[diffs[n] for diffs in diffs_list]) / self.samples for n in range(len(diffs_list[0]))]",
            "+            diffs = [tf.add_n(inputs=[diffs[n] for diffs in diffs_list]) /",
            "+                     self.samples for n in range(len(diffs_list[0]))]",
            "perturbation_diffs = [diff - pert for diff, pert in zip(diffs, previous_perturbations)]",
            "applied = self.apply_step(variables=variables, diffs=perturbation_diffs)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 3746,
        "neg_line": [
            "-diffs = [tf.add_n(inputs=[diffs[n] for diffs in diffs_list]) / self.samples for n in range(len(diffs_list[0]))]"
        ],
        "pos_line": [
            "+diffs = [tf.add_n(inputs=[diffs[n] for diffs in diffs_list]) /",
            "+self.samples for n in range(len(diffs_list[0]))]"
        ],
        "core_change": "-diffs = [tf.add_n(inputs=[diffs[n] for diffs in diffs_list]) / self.samples for n in range(len(diffs_list[0]))] +diffs = [tf.add_n(inputs=[diffs[n] for diffs in diffs_list]) / +self.samples for n in range(len(diffs_list[0]))]",
        "core_API": "append"
    },
    {
        "commit_hash": "2cff59aa0f72e789b44f06b617516f0eb9bd7e5e",
        "index": "621293e..c2c2795 100644",
        "commit_message": "Fix indentation in nn/batch_norm.py\n\n",
        "file": "learning-to-learn.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class BatchNorm(base.AbstractModule):",
            "trainable=False)",
            "",
            "self._moving_variance = tf.subtract(self._moving_second_moment,",
            "-                                   tf.square(self._moving_mean),",
            "-                                   name=\"moving_variance\")",
            "+                                        tf.square(self._moving_mean),",
            "+                                        name=\"moving_variance\")",
            "",
            "def build_batch_stats():",
            "\"\"\"Builds the batch statistics calculation ops.\"\"\""
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 3747,
        "neg_line": [
            "-tf.square(self._moving_mean),",
            "-name=\"moving_variance\")"
        ],
        "pos_line": [
            "+tf.square(self._moving_mean),",
            "+name=\"moving_variance\")"
        ],
        "core_change": "-tf.square(self._moving_mean), -name=\"moving_variance\") +tf.square(self._moving_mean), +name=\"moving_variance\")",
        "core_API": "subtract"
    },
    {
        "commit_hash": "d5c0b2fc16604d785202ee740ba6cd317a0d1b56",
        "index": "4b9252f3..bf28dd2e 100644",
        "commit_message": "Fixed a problem with  multi_step optimizer and pg_prob_ratio model (related to PPO's optimization procedure)\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class RunningStandardize(Preprocessor):",
            "",
            "def later_run():",
            "# Variance update",
            "-                variance = tf.reduce_sum(input_tensor=((tensor - mean_estimate) * mean), axis=0)",
            "+                variance = tf.reduce_sum(input_tensor=((tensor - mean_estimate) * mean), axis=0)  # reduce_mean?",
            "assignment = tf.assign_add(ref=variance_sum_estimate, value=variance)",
            "with tf.control_dependencies(control_inputs=(assignment,)):",
            "variance_estimate = variance_sum_estimate / (count - 1.0)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 3748,
        "neg_line": [
            "-variance = tf.reduce_sum(input_tensor=((tensor - mean_estimate) * mean), axis=0)"
        ],
        "pos_line": [
            "+variance = tf.reduce_sum(input_tensor=((tensor - mean_estimate) * mean), axis=0)  # reduce_mean?"
        ],
        "core_change": "-variance = tf.reduce_sum(input_tensor=((tensor - mean_estimate) * mean), axis=0) +variance = tf.reduce_sum(input_tensor=((tensor - mean_estimate) * mean), axis=0)  # reduce_mean?",
        "core_API": "reduce_sum"
    },
    {
        "commit_hash": "e1f6e4903a15f6eef27fa568e2ebf7ac3a4fce49",
        "index": "889790c75..c7844ad4b 100644",
        "commit_message": "Fix integration tests for TFWav2Vec2 and TFHubert\n\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFWav2Vec2ModelIntegrationTest(unittest.TestCase):",
            "",
            "input_speech = self._load_datasamples(4)",
            "",
            "-        inputs = processor(input_speech, return_tensors=\"tf\", padding=True, truncation=True)",
            "+        inputs = processor(input_speech, return_tensors=\"tf\", padding=True, sampling_rate=16000)",
            "",
            "input_values = inputs.input_values",
            "attention_mask = inputs.attention_mask",
            "",
            "logits = model(input_values, attention_mask=attention_mask).logits",
            "",
            "-        predicted_ids = tf.argmax(logits, dim=-1)",
            "+        predicted_ids = tf.argmax(logits, axis=-1)",
            "predicted_trans = processor.batch_decode(predicted_ids)",
            "",
            "EXPECTED_TRANSCRIPTIONS = ["
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=truncation), value='sampling_rate')",
            "Insert(target_node=ASTNode(type=keyword_argument), node=('integer', '16000'), position=2, insert_id=2369342)",
            "Update(target_node=ASTNode(type=identifier, text=dim), value='axis')",
            "Delete(target_node=ASTNode(type=true, text=True))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 4,
        "number": 3750,
        "neg_line": [
            "-inputs = processor(input_speech, return_tensors=\"tf\", padding=True, truncation=True)",
            "-predicted_ids = tf.argmax(logits, dim=-1)"
        ],
        "pos_line": [
            "+inputs = processor(input_speech, return_tensors=\"tf\", padding=True, sampling_rate=16000)",
            "+predicted_ids = tf.argmax(logits, axis=-1)"
        ],
        "core_change": "-inputs = processor(input_speech, return_tensors=\"tf\", padding=True, truncation=True) +inputs = processor(input_speech, return_tensors=\"tf\", padding=True, sampling_rate=16000) -predicted_ids = tf.argmax(logits, dim=-1) +predicted_ids = tf.argmax(logits, axis=-1)",
        "core_API": "_load_datasamples"
    },
    {
        "commit_hash": "1ed2ebf60d87ef12bd063c7c58e484e19189c754",
        "index": "634c005c4..8521a9542 100755",
        "commit_message": "[style] consistent nn. and nn.functional (#12124)\n\n* consistent nn. and nn.functional\n\n* fix glitch\n\n* fix glitch #2\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class ReformerPreTrainedModel(PreTrainedModel):",
            "\"\"\"Initialize the weights\"\"\"",
            "if isinstance(module, AxialPositionEmbeddings):",
            "for weight in module.weights:",
            "-                torch.nn.init.normal_(weight, std=self.config.axial_norm_std)",
            "+                nn.init.normal_(weight, std=self.config.axial_norm_std)",
            "elif isinstance(module, nn.Embedding):",
            "module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)",
            "if module.padding_idx is not None:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=IN(type=root), node=ASTNode(type=attribute), position=0)",
            "Move(target_node=ASTNode(type=., text=.), node=ASTNode(type=attribute), position=2)",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 3751,
        "neg_line": [
            "-torch.nn.init.normal_(weight, std=self.config.axial_norm_std)"
        ],
        "pos_line": [
            "+nn.init.normal_(weight, std=self.config.axial_norm_std)"
        ],
        "core_change": "-torch.nn.init.normal_(weight, std=self.config.axial_norm_std) +nn.init.normal_(weight, std=self.config.axial_norm_std)",
        "core_API": "normal_"
    },
    {
        "commit_hash": "ac9304bfb28abac37efb9f56005eb9051eb69ba9",
        "index": "afdd4b91..bf26761f 100644",
        "commit_message": "Improve mnist examples and fix bugs. (#382)\n\n* simplified mnist example\n\n* simplied mnist example\n\n* simplied mnist example\n\n* simplied mnist example\n\n* simplied mnist example\n\n* IMPORTANT : fixed D_TYPE bug / as we splited the layers into many file, the D_TYPE in core.py cant change in other files\n\n* update layer config.\n\n* format code\n\n* decouple set_keep.\n\n* fix hao comments.\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class SpatialTransformer2dAffineLayer(Layer):",
            "# 2.1 W",
            "n_in = int(self.theta_layer.outputs.get_shape()[-1])",
            "shape = (n_in, 6)",
            "-            W = tf.get_variable(name='W', initializer=tf.zeros(shape), dtype=D_TYPE)",
            "+            W = tf.get_variable(name='W', initializer=tf.zeros(shape), dtype=LayersConfig.tf_dtype)",
            "# 2.2 b",
            "identity = tf.constant(np.array([[1., 0, 0], [0, 1., 0]]).astype('float32').flatten())",
            "-            b = tf.get_variable(name='b', initializer=identity, dtype=D_TYPE)",
            "+            b = tf.get_variable(name='b', initializer=identity, dtype=LayersConfig.tf_dtype)",
            "# 2.3 transformation matrix",
            "self.theta = tf.nn.tanh(tf.matmul(self.theta_layer.outputs, W) + b)",
            "# 3. Spatial Transformer Sampling"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2264177)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2264178)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2264179)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'get_variable'), position=2, insert_id=2264180)",
            "Insert(target_node=ASTNode(type=keyword_argument), node=('attribute', None), position=2, insert_id=2264181)",
            "Insert(target_node=ASTNode(type=keyword_argument), node=('attribute', None), position=2, insert_id=2264182)",
            "Update(target_node=ASTNode(type=identifier, text=D_TYPE), value='LayersConfig')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=D_TYPE), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2264183)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf_dtype'), position=2, insert_id=2264184)",
            "Update(target_node=ASTNode(type=identifier, text=D_TYPE), value='LayersConfig')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=D_TYPE), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2264185)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf_dtype'), position=2, insert_id=2264186)",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=get_variable))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 19,
        "number": 3752,
        "neg_line": [
            "-W = tf.get_variable(name='W', initializer=tf.zeros(shape), dtype=D_TYPE)",
            "-b = tf.get_variable(name='b', initializer=identity, dtype=D_TYPE)"
        ],
        "pos_line": [
            "+W = tf.get_variable(name='W', initializer=tf.zeros(shape), dtype=LayersConfig.tf_dtype)",
            "+b = tf.get_variable(name='b', initializer=identity, dtype=LayersConfig.tf_dtype)"
        ],
        "core_change": "-W = tf.get_variable(name='W', initializer=tf.zeros(shape), dtype=D_TYPE) +W = tf.get_variable(name='W', initializer=tf.zeros(shape), dtype=LayersConfig.tf_dtype) -b = tf.get_variable(name='b', initializer=identity, dtype=D_TYPE) +b = tf.get_variable(name='b', initializer=identity, dtype=LayersConfig.tf_dtype)",
        "core_API": "get_shape"
    },
    {
        "commit_hash": "7b8cad3ad8449f260e3b282bf0f9b824cc94b58d",
        "index": "c9a4a615..5417c1b2 100644",
        "commit_message": "Bug fixed (#475)\n\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def rgb_to_hls(image: torch.Tensor) -> torch.Tensor:",
            "hi[imax == 1] = (((b - r) / deltac) + 2)[imax == 1]",
            "hi[imax == 2] = (((r - g) / deltac) + 4)[imax == 2]",
            "",
            "-    h: torch.Tensor = 2. * pi * (60. * hi) / 360.  # hue [0, 2*pi]",
            "+    h: torch.Tensor = 2. * pi.to(image.device) * (60. * hi) / 360.  # hue [0, 2*pi]",
            "",
            "image_hls: torch.Tensor = torch.stack([h, l, s], dim=-3)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=binary_operator), node=('call', None), position=2, insert_id=443773)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=443774)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=443775)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=pi), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=443776)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to'), position=2, insert_id=443777)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=443778)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=443779)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=443780)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'image'), position=0, insert_id=443781)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=443782)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=443783)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 12,
        "number": 3753,
        "neg_line": [
            "-h: torch.Tensor = 2. * pi * (60. * hi) / 360.  # hue [0, 2*pi]"
        ],
        "pos_line": [
            "+h: torch.Tensor = 2. * pi.to(image.device) * (60. * hi) / 360.  # hue [0, 2*pi]"
        ],
        "core_change": "-h: torch.Tensor = 2. * pi * (60. * hi) / 360.  # hue [0, 2*pi] +h: torch.Tensor = 2. * pi.to(image.device) * (60. * hi) / 360.  # hue [0, 2*pi]",
        "core_API": "to"
    },
    {
        "commit_hash": "5c6d5d4ab11ea27767e3fc150b0fd5346e3683b4",
        "index": "d0e5dd0d4..54ce8442a 100644",
        "commit_message": "This PR fixes the currently broken lstm_use_prev_action_reward flag for default lstm models (model.use_lstm=True). (#8970)\n\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Policy(metaclass=ABCMeta):",
            "episodes = [episode]",
            "if state is not None:",
            "state_batch = [",
            "-                s.unsqueeze(0)",
            "-                if torch and isinstance(s, torch.Tensor) else [s]",
            "+                s.unsqueeze(0) if torch and isinstance(s, torch.Tensor) else",
            "+                np.expand_dims(s, 0)",
            "for s in state",
            "]"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=conditional_expression), node=('call', None), position=4, insert_id=1123466)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1123467)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1123468)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'np'), position=0, insert_id=1123469)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1123470)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'expand_dims'), position=2, insert_id=1123471)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1123472)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=s), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=1123473)",
            "Insert(target_node=IN(type=argument_list), node=('integer', '0'), position=3, insert_id=1123474)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=1123475)",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=list))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 14,
        "number": 3756,
        "neg_line": [
            "-s.unsqueeze(0)",
            "-if torch and isinstance(s, torch.Tensor) else [s]"
        ],
        "pos_line": [
            "+s.unsqueeze(0) if torch and isinstance(s, torch.Tensor) else",
            "+np.expand_dims(s, 0)"
        ],
        "core_change": "-s.unsqueeze(0) -if torch and isinstance(s, torch.Tensor) else [s] +s.unsqueeze(0) if torch and isinstance(s, torch.Tensor) else +np.expand_dims(s, 0)",
        "core_API": "unsqueeze"
    },
    {
        "commit_hash": "406a49dfe4a7bced24cd63b5875f3192c72598dd",
        "index": "b2c1797b5..a79dbb8fb 100644",
        "commit_message": "Fix small type hinting error (#7820)\n\n* Fix small type hinting error\n\n* Update tokenization_utils_base.py\n\n* Update src/transformers/tokenization_utils_base.py\n\nCo-authored-by: Lysandre Debut <lysandre@huggingface.co>\n\nCo-authored-by: Lysandre Debut <lysandre@huggingface.co>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class BatchEncoding(UserDict):",
            "return self",
            "",
            "@torch_required",
            "-    def to(self, device: str) -> \"BatchEncoding\":",
            "+    def to(self, device: Union[str, \"torch.device\"]) -> \"BatchEncoding\":",
            "\"\"\"",
            "Send all values to device by calling :obj:`v.to(device)` (PyTorch only)."
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=type), node=('subscript', None), position=0, insert_id=1231909)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'Union'), position=0, insert_id=1231910)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=1231911)",
            "Move(target_node=IN(type=subscript), node=ASTNode(type=identifier, text=str), position=2)",
            "Insert(target_node=IN(type=subscript), node=(',', ','), position=3, insert_id=1231912)",
            "Insert(target_node=IN(type=subscript), node=('string', '\"torch.device\"'), position=4, insert_id=1231913)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=5, insert_id=1231914)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 3757,
        "neg_line": [
            "-def to(self, device: str) -> \"BatchEncoding\":"
        ],
        "pos_line": [
            "+def to(self, device: Union[str, \"torch.device\"]) -> \"BatchEncoding\":"
        ],
        "core_change": "-def to(self, device: str) -> \"BatchEncoding\": +def to(self, device: Union[str, \"torch.device\"]) -> \"BatchEncoding\":",
        "core_API": "to"
    },
    {
        "commit_hash": "6638ce76b1d414b33393f00cdde13b78c3774a43",
        "index": "1165f0a3..5afee285 100644",
        "commit_message": "Fix small mistake `optimizer` to `optimizers`\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "keras",
        "change": [
            "class Model(base_layer.Layer, version_utils.ModelVersionSelector):",
            "Example:",
            "",
            "```python",
            "-    model.compile(optimizer=tf.keras.optimizer.Adam(learning_rate=1e-3),",
            "+    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),",
            "loss=tf.keras.losses.BinaryCrossentropy(),",
            "metrics=[tf.keras.metrics.BinaryAccuracy(),",
            "tf.keras.metrics.FalseNegatives()])"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=optimizer), value='optimizers')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 3759,
        "neg_line": [
            "-model.compile(optimizer=tf.keras.optimizer.Adam(learning_rate=1e-3),"
        ],
        "pos_line": [
            "+model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),"
        ],
        "core_change": "-model.compile(optimizer=tf.keras.optimizer.Adam(learning_rate=1e-3), +model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),",
        "core_API": "compile"
    },
    {
        "commit_hash": "fb8379a51d0963f999d3cdefd00d4bac412f23a7",
        "index": "991087b6c..1de320fab 100644",
        "commit_message": "fix the warning when training (#673)\n\nchange ByteTensor to BoolTensor\n",
        "file": "tutorials.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def outputVar(l, voc):",
            "max_target_len = max([len(indexes) for indexes in indexes_batch])",
            "padList = zeroPadding(indexes_batch)",
            "mask = binaryMatrix(padList)",
            "-    mask = torch.ByteTensor(mask)",
            "+    mask = torch.BoolTensor(mask)",
            "padVar = torch.LongTensor(padList)",
            "return padVar, mask, max_target_len"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=ByteTensor), value='BoolTensor')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 3763,
        "neg_line": [
            "-mask = torch.ByteTensor(mask)"
        ],
        "pos_line": [
            "+mask = torch.BoolTensor(mask)"
        ],
        "core_change": "-mask = torch.ByteTensor(mask) +mask = torch.BoolTensor(mask)",
        "core_API": "ByteTensor"
    },
    {
        "commit_hash": "e3f028f3af9e2fd42bb8cf52ec1bcf720b6fbaf1",
        "index": "cd09a83d9..8e9a7f32c 100644",
        "commit_message": "Add TF whisper (#19378)\n\n* simplify loop\n\n* add featur extractor\n\n* add model\n\n* start conversion\n\n* add dropout\n\n* initial commit of test files\n\n* copnversion for all models\n\n* update processor for correct padding\n\n* update feature extraction\n\n* update integration test logits match\n\n* fmnt: off for the logits\n\n* on the fly mel bank\n\n* small nit\n\n* update test\n\n* update tokenizer\n\n* nit feature extraction\n\n* update\n\n* update tokenizer test\n\n* adds logit processor and update tokenizer to get supress tokens\n\n* style\n\n* clean convert\n\n* revert to original modeling tf utils\n\n* Update\n\n* update\n\n* nit\n\n* clean convert file\n\n* update tests and nits\n\n* quality\n\n* slow generation test\n\n* ffn_dim to allow customization\n\n* update readme\n\n* add to toctreee\n\n* start fixing integration tests\n\n* update tests and code\n\n* fix feature extractor\n\n* fix config tests common\n\n* update code to fix tests\n\n* fix feature exctractor\n\n* nit feature extraction\n\n* update test for new feature extractor\n\n* style\n\n* add absrtact\n\n* large logits wioth custom decoder input ids\n\n* wraap around is otrch available\n\n* fix feature extractor\n\n* correct logits for whisper small.en\n\n* nit\n\n* fix encoder_attentino_mask\n\n* some fixes\n\n* remove unnecessary inputs\n\n* nits\n\n* add normalizer file\n\n* update etst tokenization\n\n* fix attention mask not defined\n\n* fix generate\n\n* remove uncoder attention mask useless\n\n* update test modeling whisper\n\n* update condfig to add second non supress tokens\n\n* nits on feature exrtactor\n\n* nit for test tokenizers\n\n* update etsts\n\n* update tests\n\n* update tokenization test\n\n* fixup\n\n* invalidated hf token. Clean convert openai to whisper\n\n* fix logit tests\n\n* fixup\n\n* Add model to README\n\n* Fix doc tests\n\n* clean merge\n\n* revert toc_tree changes\n\n* remove useless LogitProcessor\n\n* Update whisper .mdx\n\n* update config file doc\n\n* update configuration docstring\n\n* update test tokenization\n\n* update test tokenization\n\n* update tokenization whisper\nAdded copied from where needed\n\n* update feature extraction\n\n* nit test name\n\n* style\n\n* quality\n\n* remove get suppress tokens and update non_speech tokens global variables\n\n* Update src/transformers/models/whisper/feature_extraction_whisper.py\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* clean modeling whisper and test\nRemoved the attention mask arguments that are deprecated\n\n* fix large test\n\n* Add multilingual audio test, and translate test\n\n* style\n\n* fix larg multilingual test\n\n* nits\n\n* add copied from for attention layer\n\n* remove attention masks in doc\n\n* add english normalizer\n\n* Update docs/source/en/model_doc/whisper.mdx\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* update tokenization test\n\n* remove copied from in whisper attention : no bias in k_proj only\n\n* wrap around dependencies in english normalizer\n\n* style\n\n* correct import generation logits\n\n* for now, wrap feature extractor with torch\n\n* remove torch depencies for feature extraction and style\n\n* Update src/transformers/models/whisper/convert_openai_whisper_to_tfms.py\n\nCo-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>\n\n* Update src/transformers/models/whisper/configuration_whisper.py\n\nCo-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/whisper.mdx\n\nCo-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>\n\n* fixup\n\n* nit\n\n* update logitds\n\n* style\n\n* nit\n\n* nits and fix final tests\n\n* add `is_more_itertools_available` to utils\n\n* quality\n\n* add begin supress tokens, supress tokens to generate args and config\n\n* clean supressTokensLogitProcessor in generation logits\n\n* Nit naming\n\n* add supressTokensAtBegin\n\n* udpate tests, supress tokens to None or correct values\n\n* nit and style\n\n* update RAG to fit test and generate_logit\n\n* add copy pasted statment on english normalizer\n\n* add arguments to config_common_kwargs\n\n* Update src/transformers/generation_utils.py\n\nCo-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>\n\n* Update src/transformers/generation_logits_process.py\n\nCo-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>\n\n* revert changes based on reviews\n\n* update doc and nits\n\n* Update src/transformers/models/whisper/configuration_whisper.py\n\nCo-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>\n\n* Apply suggestions from code review\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\nCo-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>\n\n* more nits\n\n* last nits\n\n* update test configuration common\n\n* add BART name in decoder attention mask documentation\n\n* Update src/transformers/models/whisper/modeling_whisper.py\n\nCo-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>\n\n* style\n\n* nit\n\n* nit\n\n* add english.json file to git\n\n* nits on documentation\n\n* nit\n\n* nits\n\n* last styling\n\n* add main toctree file\n\n* remove sentence piece dependency\n\n* clean init file\n\n* fix tokenizer that has no dependencies on sentencepiece\n\n* update whisper init file, nit\n\n* remove english.json file\n\n* add get decoder prompt id\n\n* All weights loading\n\n* Remove hanging pdb\n\n* Fixup and tidy up\n\n* Use same copied from as PT model\n\n* Remove whitespace changes\n\n* Remove torch references\n\n* Tie embeddings\n\n* Remove logits processor input to generate\n\n* Update logit values\n\n* revert changes and add forced logit processor\n\n* nit\n\n* clean normalizer\n\n* remove protected\n\n* Add logit processors and update generation code & tests\n\n* Some tidy up\n\n* Update docstring\n\n* update\n\n* update based on review\n\n* Update src/transformers/models/whisper/configuration_whisper.py\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n\n* Update src/transformers/models/whisper/configuration_whisper.py\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n\n* Update to reflect changes on the PT model branch\n\n* Tidy up\n\n* Remove extra whitespace\n\n* Fix test - make input ids small enough we can append\n\n* Include upstream changes on main\n\n* PR comments - add batch tests, remove comments & defaults\n\n* Fix model output imports\n\n* Update src/transformers/models/whisper/modeling_tf_whisper.py\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>\n\n* Update src/transformers/generation_tf_logits_process.py\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>\n\n* Update src/transformers/models/whisper/modeling_tf_whisper.py\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>\n\n* Update src/transformers/models/whisper/modeling_tf_whisper.py\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>\n\n* Update tests/models/whisper/test_modeling_tf_whisper.py\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>\n\n* Update src/transformers/models/whisper/modeling_tf_whisper.py\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>\n\n* Update src/transformers/models/whisper/modeling_tf_whisper.py\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>\n\n* Update docstring example\n\n* Update src/transformers/models/whisper/modeling_tf_whisper.py\n\nCo-authored-by: Matt <Rocketknight1@users.noreply.github.com>\n\n* Remove changes to adjust_logits_during_generation function\n\n* Update src/transformers/models/whisper/modeling_tf_whisper.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* Tidy up imports that don't require TF\n\n* Update tests - skip and no more skip\n\n* Update tests/generation/test_generation_tf_logits_process.py\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>\n\n* Update src/transformers/models/whisper/modeling_tf_whisper.py\n\n* Update src/transformers/models/whisper/modeling_tf_whisper.py\n\nCo-authored-by: Matt <Rocketknight1@users.noreply.github.com>\n\n* Add training flags\n\n* Add (skipped) XLA generation tests\n\n* Add embedding correctness test\n\n* Add constant ids for generation tests\n\n* Make logits finding a bit tidier\n\n* Remove unused args\n\n* xla generation enabled\n\n* Don't skip XLA tests anymore\n\n* Fix tests - add position ids to expected signature and update rag generation\n\n* Undo method reorder\n\n* Remove added whitespace\n\n* Remove copy-paste gradient checkopint ref\n\n* Remove\n\n* Trigger CI - (issue with refs when pulling)\n\nCo-authored-by: Arthur Zucker <arthur.zucker@gmail.com>\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\nCo-authored-by: NielsRogge <niels.rogge1@gmail.com>\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\nCo-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>\nCo-authored-by: Matt <Rocketknight1@users.noreply.github.com>\nCo-authored-by: Joao Gante <joao@huggingface.co>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFRagTokenForGeneration(TFRagPreTrainedModel, TFCausalLanguageModelingLoss",
            "eos_token_id=eos_token_id,",
            "forced_bos_token_id=None,",
            "forced_eos_token_id=None,",
            "+                input_ids_seq_length=tf.shape(decoder_input_ids)[-1],",
            ")",
            "model_kwargs[\"attention_mask\"] = context_attention_mask"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=9, insert_id=2359513)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=10, insert_id=2359514)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=11, insert_id=2359515)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'input_ids_seq_length'), position=0, insert_id=2359516)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2359517)",
            "Insert(target_node=IN(type=keyword_argument), node=('subscript', None), position=2, insert_id=2359518)",
            "Insert(target_node=IN(type=subscript), node=('call', None), position=0, insert_id=2359519)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=2359520)",
            "Insert(target_node=IN(type=subscript), node=('unary_operator', '-1'), position=2, insert_id=2359521)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=2359522)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2359523)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2359524)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2359525)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2359526)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'shape'), position=2, insert_id=2359527)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2359528)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'decoder_input_ids'), position=1, insert_id=2359529)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=), text=)), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 18,
        "number": 3764,
        "neg_line": [],
        "pos_line": [
            "+input_ids_seq_length=tf.shape(decoder_input_ids)[-1],"
        ],
        "core_change": "+input_ids_seq_length=tf.shape(decoder_input_ids)[-1],",
        "core_API": "shape"
    },
    {
        "commit_hash": "73a532651a2a6a6ade9d29bb51f85d10d829ee2e",
        "index": "32e020d7b..27324f59d 100644",
        "commit_message": "New TF GLUE example (#12028)\n\n* Pushing partially-complete new GLUE example\n\n* First draft of the new TF GLUE example! Needs a little more testing to be sure but it's almost ready.\n\n* Fix to the fit() call\n\n* Bugfixes, making sure TPU and multi-GPU support is ready\n\n* Remove logger line that depends on Pytorch\n\n* Style pass\n\n* Deleting old TF GLUE example\n\n* Include label2id and id2label in the saved model config\n\n* Don't clobber the existing model.config.label2id\n\n* Style fixes\n\n* Update examples/tensorflow/text-classification/run_glue.py\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def main():",
            "model.compile(optimizer=optimizer, loss=loss_fn, metrics=metrics)",
            "# endregion",
            "",
            "-        # region Convert data to TF format",
            "+        # region Convert data to a tf.data.Dataset",
            "",
            "-        # Convert data to a tf.keras.utils.Sequence object for training if we're not using a TPU",
            "-        # For TPU, convert to a tf.data.Dataset",
            "tf_data = dict()",
            "max_samples = {",
            "\"train\": data_args.max_train_samples,"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 17,
        "number": 3767,
        "neg_line": [
            "-# region Convert data to TF format",
            "-# Convert data to a tf.keras.utils.Sequence object for training if we're not using a TPU",
            "-# For TPU, convert to a tf.data.Dataset"
        ],
        "pos_line": [
            "+# region Convert data to a tf.data.Dataset"
        ],
        "core_change": "-# region Convert data to TF format +# region Convert data to a tf.data.Dataset -# Convert data to a tf.keras.utils.Sequence object for training if we're not using a TPU -# For TPU, convert to a tf.data.Dataset",
        "core_API": "compile"
    },
    {
        "commit_hash": "4ed7a01460a9f99e55bb0ced1ec82cd8ea936055",
        "index": "08e09edd..e574c46a 100644",
        "commit_message": "Fixing RNN compute_dtype in v1.\n\nIn v1, since there isn't a global policy, the layer compute_dtype will be \"_inferred\" from input, and the inferred dtype are actually populate on the cell.\n\nPiperOrigin-RevId: 394779149\n\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class RNN(Layer):",
            "else:",
            "initial_state = self.states",
            "initial_state = tf.nest.map_structure(",
            "-          lambda v: tf.cast(v, self.compute_dtype), initial_state",
            "+          # When the layer has a inferred dtype, use the dtype from the cell.",
            "+          lambda v: tf.cast(v, self.compute_dtype or self.cell.compute_dtype),",
            "+          initial_state",
            ")",
            "elif initial_state is None:",
            "initial_state = self.get_initial_state(inputs)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('boolean_operator', None), position=3, insert_id=2079909)",
            "Move(target_node=IN(type=boolean_operator), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=boolean_operator), node=('or', 'or'), position=1, insert_id=2079910)",
            "Insert(target_node=IN(type=boolean_operator), node=('attribute', None), position=2, insert_id=2079911)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2079912)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2079913)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'compute_dtype'), position=2, insert_id=2079914)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=2079915)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2079916)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cell'), position=2, insert_id=2079917)"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 3768,
        "neg_line": [
            "-lambda v: tf.cast(v, self.compute_dtype), initial_state"
        ],
        "pos_line": [
            "+# When the layer has a inferred dtype, use the dtype from the cell.",
            "+lambda v: tf.cast(v, self.compute_dtype or self.cell.compute_dtype),",
            "+initial_state"
        ],
        "core_change": "-lambda v: tf.cast(v, self.compute_dtype), initial_state +# When the layer has a inferred dtype, use the dtype from the cell. +lambda v: tf.cast(v, self.compute_dtype or self.cell.compute_dtype), +initial_state",
        "core_API": "map_structure"
    },
    {
        "commit_hash": "ae914973df79cb45d8d42e7a8b36f0bb1c810b44",
        "index": "331177b4..a0bea039 100644",
        "commit_message": "Fix tests on CUDA (#2098)\n\n\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestDexiNed:",
            "dtype=dtype,",
            ")",
            "",
            "-        out = model(img)",
            "-        assert_close(out[-1][:, :1], expect, atol=1e-4, rtol=1e-4)",
            "+        out = model(img)[-1]",
            "+        assert_close(out, expect, atol=3e-4, rtol=3e-4)",
            "",
            "def test_jit(self, device, dtype):",
            "op = kornia.filters.DexiNed(pretrained=False).to(device, dtype)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('subscript', None), position=2, insert_id=394298)",
            "Move(target_node=IN(type=subscript), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=394299)",
            "Insert(target_node=IN(type=subscript), node=('unary_operator', '-1'), position=2, insert_id=394300)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=394301)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=out), position=1)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=,, text=,), position=2)",
            "Update(target_node=ASTNode(type=float, text=1e-4), value='3e-4')",
            "Update(target_node=ASTNode(type=float, text=1e-4), value='3e-4')",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=unary_operator, text=-1))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=slice))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=slice))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 22,
        "number": 3771,
        "neg_line": [
            "-out = model(img)",
            "-assert_close(out[-1][:, :1], expect, atol=1e-4, rtol=1e-4)"
        ],
        "pos_line": [
            "+out = model(img)[-1]",
            "+assert_close(out, expect, atol=3e-4, rtol=3e-4)"
        ],
        "core_change": "-out = model(img) -assert_close(out[-1][:, :1], expect, atol=1e-4, rtol=1e-4) +out = model(img)[-1] +assert_close(out, expect, atol=3e-4, rtol=3e-4)",
        "core_API": "DexiNed"
    },
    {
        "commit_hash": "32484500befa825b3def2cdc15cb12eb2251f681",
        "index": "0adbd8d..bab5737 100644",
        "commit_message": "tutorial fixes from #336. Wheels with cuda10.1.\n\nSummary:\nAdd a document to explain how to run the tutorials.\nFix API of TexturesVertex in fit_textured_mesh.\nPrepare cuda 10.1 wheels (not 10.2) for linux to be available on pypi - this matches what colab has.\nChange the tutorials to use these new wheels.\n\nReviewed By: gkioxari\n\nDifferential Revision: D23324479\n\nfbshipit-source-id: 60e92a3f46a2d878f811b7703638f8d1dae143d9\n\n",
        "file": "pytorch3d.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "\"outputs\": [],",
            "\"source\": [",
            "\"!pip install torch torchvision\\n\",",
            "-    \"!pip install 'git+https://github.com/facebookresearch/pytorch3d.git'\"",
            "+    \"import sys\\n\",",
            "+    \"if torch.__version__=='1.6.0+cu101' and sys.platform.startswith('linux'):\\n\",",
            "+    \"    !pip install pytorch3d\\n\",",
            "+    \"else:\\n\",",
            "+    \"    !pip install 'git+https://github.com/facebookresearch/pytorch3d.git@stable'\"",
            "]",
            "},",
            "{"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"!pip install 'git+https://github.com/facebookresearch/pytorch3d.git'\"), value='\"import sys\\\\n\"')",
            "Insert(target_node=ASTNode(type=subscript), node=(',', ','), position=6, insert_id=922903)",
            "Insert(target_node=ASTNode(type=subscript), node=('string', '\"if torch.__version__==\\'1.6.0+cu101\\' and sys.platform.startswith(\\'linux\\'):\\\\n\"'), position=7, insert_id=922904)",
            "Insert(target_node=ASTNode(type=subscript), node=(',', ','), position=8, insert_id=922905)",
            "Insert(target_node=ASTNode(type=subscript), node=('string', '\"    !pip install pytorch3d\\\\n\"'), position=9, insert_id=922906)",
            "Insert(target_node=ASTNode(type=subscript), node=(',', ','), position=10, insert_id=922907)",
            "Insert(target_node=ASTNode(type=subscript), node=('string', '\"else:\\\\n\"'), position=11, insert_id=922908)",
            "Insert(target_node=ASTNode(type=subscript), node=(',', ','), position=12, insert_id=922909)",
            "Insert(target_node=ASTNode(type=subscript), node=('string', '\"    !pip install \\'git+https://github.com/facebookresearch/pytorch3d.git@stable\\'\"'), position=13, insert_id=922910)"
        ],
        "plus_line": 5,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 3774,
        "neg_line": [
            "-\"!pip install 'git+https://github.com/facebookresearch/pytorch3d.git'\""
        ],
        "pos_line": [
            "+\"import sys\\n\",",
            "+\"if torch.__version__=='1.6.0+cu101' and sys.platform.startswith('linux'):\\n\",",
            "+\"    !pip install pytorch3d\\n\",",
            "+\"else:\\n\",",
            "+\"    !pip install 'git+https://github.com/facebookresearch/pytorch3d.git@stable'\""
        ],
        "core_change": "-\"!pip install 'git+https://github.com/facebookresearch/pytorch3d.git'\" +\"import sys\\n\", +\"if torch.__version__=='1.6.0+cu101' and sys.platform.startswith('linux'):\\n\", +\"    !pip install pytorch3d\\n\", +\"else:\\n\", +\"    !pip install 'git+https://github.com/facebookresearch/pytorch3d.git@stable'\"",
        "core_API": "startswith"
    },
    {
        "commit_hash": "424548254dbf6186e0413386171592fac1c03f08",
        "index": "557e314b..bb739614 100644",
        "commit_message": "Dog and fix features (#591)\n\n* match OpenCV DoG except two things: single iteration quad interp and no edge filtering\n\n* Added DoG to the docs\n\n\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def laf_is_inside_image(laf: torch.Tensor, images: torch.Tensor) -> torch.Tensor",
            "raise_error_if_laf_is_not_valid(laf)",
            "n, ch, h, w = images.size()",
            "pts: torch.Tensor = laf_to_boundary_points(laf, 12)",
            "-    good_lafs_mask: torch.Tensor = (pts[..., 0] >= 0) * (pts[..., 0] <= w) * (pts[..., 1] >= 0) * (pts[..., 1] <= h)",
            "+    good_lafs_mask: torch.Tensor = (pts[..., 0] >= border) *\\",
            "+        (pts[..., 0] <= w - border) *\\",
            "+        (pts[..., 1] >= border) *\\",
            "+        (pts[..., 1] <= h - border)",
            "good_lafs_mask = good_lafs_mask.min(dim=2)[0]",
            "return good_lafs_mask"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=comparison_operator), node=('binary_operator', None), position=2, insert_id=439684)",
            "Insert(target_node=ASTNode(type=comparison_operator), node=('identifier', 'border'), position=2, insert_id=439685)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=identifier, text=h), position=0)",
            "Insert(target_node=IN(type=binary_operator), node=('-', '-'), position=1, insert_id=439686)",
            "Insert(target_node=IN(type=binary_operator), node=('identifier', 'border'), position=2, insert_id=439687)",
            "Insert(target_node=ASTNode(type=comparison_operator), node=('identifier', 'border'), position=2, insert_id=439688)",
            "Insert(target_node=ASTNode(type=comparison_operator), node=('binary_operator', None), position=2, insert_id=439689)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=identifier, text=w), position=0)",
            "Insert(target_node=IN(type=binary_operator), node=('-', '-'), position=1, insert_id=439690)",
            "Insert(target_node=IN(type=binary_operator), node=('identifier', 'border'), position=2, insert_id=439691)",
            "Delete(target_node=ASTNode(type=integer, text=0))",
            "Delete(target_node=ASTNode(type=integer, text=0))"
        ],
        "plus_line": 4,
        "minus_line": 1,
        "AST_diff_line": 12,
        "number": 3775,
        "neg_line": [
            "-good_lafs_mask: torch.Tensor = (pts[..., 0] >= 0) * (pts[..., 0] <= w) * (pts[..., 1] >= 0) * (pts[..., 1] <= h)"
        ],
        "pos_line": [
            "+good_lafs_mask: torch.Tensor = (pts[..., 0] >= border) *\\",
            "+(pts[..., 0] <= w - border) *\\",
            "+(pts[..., 1] >= border) *\\",
            "+(pts[..., 1] <= h - border)"
        ],
        "core_change": "-good_lafs_mask: torch.Tensor = (pts[..., 0] >= 0) * (pts[..., 0] <= w) * (pts[..., 1] >= 0) * (pts[..., 1] <= h) +good_lafs_mask: torch.Tensor = (pts[..., 0] >= border) *\\ +(pts[..., 0] <= w - border) *\\ +(pts[..., 1] >= border) *\\ +(pts[..., 1] <= h - border)",
        "core_API": "size"
    },
    {
        "commit_hash": "f38aaa5be05104ddfc3452249eeb521f139ded99",
        "index": "d34a5865..301de236 100755",
        "commit_message": "improved global tensor handling, various other fixes\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class NaturalGradient(Optimizer):",
            "return [tf.zeros_like(tensor=delta) for delta in deltas]",
            "",
            "# Natural gradient step only works if constant > 0",
            "-        return tf.cond(pred=(constant > 0.0), true_fn=natural_gradient_step, false_fn=zero_step)",
            "+        return self.cond(pred=(constant > 0.0), true_fn=natural_gradient_step, false_fn=zero_step)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tf), value='self')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 3779,
        "neg_line": [
            "-return tf.cond(pred=(constant > 0.0), true_fn=natural_gradient_step, false_fn=zero_step)"
        ],
        "pos_line": [
            "+return self.cond(pred=(constant > 0.0), true_fn=natural_gradient_step, false_fn=zero_step)"
        ],
        "core_change": "-return tf.cond(pred=(constant > 0.0), true_fn=natural_gradient_step, false_fn=zero_step) +return self.cond(pred=(constant > 0.0), true_fn=natural_gradient_step, false_fn=zero_step)",
        "core_API": "zeros_like"
    },
    {
        "commit_hash": "87eb2a8df326148df6d704044952e49b598668e5",
        "index": "a4118d15..f9fb7b9a 100644",
        "commit_message": "Fix GPU device placement when calling tensor.cuda() (#624)\n\n* Correctly set cuda device when constructing cuda tensors\n\n* Fix .enumerate_support() methods\n\n* Revert to older invocation of torch.arange(-, -)\n\n* Support pytorch 0.2 invocation of .expand()\n\n* Fix type of Bernoulli sample\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class OneHotCategorical(Distribution):",
            "sample. The last dimension is used for the one-hot encoding.",
            ":rtype: torch.autograd.Variable.",
            "\"\"\"",
            "-        return Variable(torch.stack([t.expand_as(self.ps) for t in torch_eye(*self.event_shape())]))",
            "+        result = torch.stack([t.expand_as(self.ps) for t in torch_eye(*self.event_shape())])",
            "+        if self.ps.is_cuda:",
            "+            result = result.cuda(self.ps.get_device())",
            "+        return Variable(result)",
            "",
            "def analytic_mean(self):",
            "\"\"\""
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"\"\"\n        return Variable(torch.stack([t.expand_as(self.ps) for t in torch_eye(*self.event_shape())]))\n\ndef analytic_mean(self):\n\"\"\"), value='\"\"\"\\n        result = torch.stack([t.expand_as(self.ps) for t in torch_eye(*self.event_shape())])\\n        if self.ps.is_cuda:\\n            result = result.cuda(self.ps.get_device())\\n        return Variable(result)\\n\\ndef analytic_mean(self):\\n\"\"\"')"
        ],
        "plus_line": 4,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 3780,
        "neg_line": [
            "-return Variable(torch.stack([t.expand_as(self.ps) for t in torch_eye(*self.event_shape())]))"
        ],
        "pos_line": [
            "+result = torch.stack([t.expand_as(self.ps) for t in torch_eye(*self.event_shape())])",
            "+if self.ps.is_cuda:",
            "+result = result.cuda(self.ps.get_device())",
            "+return Variable(result)"
        ],
        "core_change": "-return Variable(torch.stack([t.expand_as(self.ps) for t in torch_eye(*self.event_shape())])) +result = torch.stack([t.expand_as(self.ps) for t in torch_eye(*self.event_shape())]) +if self.ps.is_cuda: +result = result.cuda(self.ps.get_device()) +return Variable(result)",
        "core_API": "stack"
    },
    {
        "commit_hash": "fdbe99c1769d29a8c697e8850440a57b3e1b52f4",
        "index": "1da4614eb..509bf5452 100644",
        "commit_message": "fix bugs for data_loading_tutorial and dcgan_faces_tutorial (#1092)\n\n\n",
        "file": "tutorials.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "for epoch in range(num_epochs):",
            "# Format batch",
            "real_cpu = data[0].to(device)",
            "b_size = real_cpu.size(0)",
            "-        label = torch.full((b_size,), real_label, device=device)",
            "+        label = torch.full((b_size,), real_label, dtype=torch.float, device=device)",
            "# Forward pass real batch through D",
            "output = netD(real_cpu).view(-1)",
            "# Calculate loss on all-real batch"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=1276701)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=6, insert_id=1276702)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=1276703)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1276704)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=1276705)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1276706)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1276707)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'float'), position=2, insert_id=1276708)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 3781,
        "neg_line": [
            "-label = torch.full((b_size,), real_label, device=device)"
        ],
        "pos_line": [
            "+label = torch.full((b_size,), real_label, dtype=torch.float, device=device)"
        ],
        "core_change": "-label = torch.full((b_size,), real_label, device=device) +label = torch.full((b_size,), real_label, dtype=torch.float, device=device)",
        "core_API": "size"
    },
    {
        "commit_hash": "461ae86812f9d75762bbdae2ac5776f9a5d702ea",
        "index": "9e5dc33eb..7d9957e3d 100644",
        "commit_message": "Fix tf boolean mask in graph mode (#6741)\n\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFCausalLanguageModelingLoss:",
            ")",
            "# make sure only labels that are not equal to -100",
            "# are taken into account as loss",
            "-        active_loss = tf.reshape(labels, (-1,)) != -100",
            "+        active_loss = tf.not_equal(tf.reshape(labels, (-1,)), -100)",
            "reduced_logits = tf.boolean_mask(tf.reshape(logits, (-1, shape_list(logits)[2])), active_loss)",
            "labels = tf.boolean_mask(tf.reshape(labels, (-1,)), active_loss)",
            "return loss_fn(labels, reduced_logits)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('call', None), position=2, insert_id=2380501)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2380502)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2380503)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2380504)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2380505)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'not_equal'), position=2, insert_id=2380506)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2380507)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=call), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=2380508)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=unary_operator, text=-100), position=3)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=2380509)",
            "Delete(target_node=ASTNode(type=!=, text=!=))",
            "Delete(target_node=ASTNode(type=comparison_operator))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 13,
        "number": 3785,
        "neg_line": [
            "-active_loss = tf.reshape(labels, (-1,)) != -100"
        ],
        "pos_line": [
            "+active_loss = tf.not_equal(tf.reshape(labels, (-1,)), -100)"
        ],
        "core_change": "-active_loss = tf.reshape(labels, (-1,)) != -100 +active_loss = tf.not_equal(tf.reshape(labels, (-1,)), -100)",
        "core_API": "reshape"
    },
    {
        "commit_hash": "de4159a318f9a4f2f474733306c00ac326e643a7",
        "index": "ec7458a7e..e9cf9adb7 100644",
        "commit_message": "More TF int dtype fixes (#20384)\n\n* Add a test to ensure int dummy inputs are int64\n\n* Move the test into the existing int64 test and update a lot of existing dummies\n\n* Fix remaining dummies\n\n* Fix remaining dummies\n\n* Test for int64 serving sigs as well\n\n* Update core tests to use tf.int64\n\n* Add better messages to the assertions\n\n* Update all serving sigs to int64\n\n* More sneaky hiding tf.int32s\n\n* Add an optional int32 signature in save_pretrained\n\n* make fixup\n\n* Add Amy's suggestions\n\n* Switch all serving sigs back to tf.int32\n\n* Switch all dummies to tf.int32\n\n* Adjust tests to check for tf.int32 instead of tf.int64\n\n* Fix base dummy_inputs dtype\n\n* Start casting to tf.int32 in input_processing\n\n* Change dtype for unpack_inputs test\n\n* Add proper tf.int32 test\n\n* Make the alternate serving signature int64\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFHubertPreTrainedModel(TFPreTrainedModel):",
            "input_signature=[",
            "{",
            "\"input_values\": tf.TensorSpec((None, None), tf.float32, name=\"input_values\"),",
            "-                \"attention_mask\": tf.TensorSpec((None, None), tf.int64, name=\"attention_mask\"),",
            "-                \"token_type_ids\": tf.TensorSpec((None, None), tf.int64, name=\"token_type_ids\"),",
            "+                \"attention_mask\": tf.TensorSpec((None, None), tf.int32, name=\"attention_mask\"),",
            "+                \"token_type_ids\": tf.TensorSpec((None, None), tf.int32, name=\"token_type_ids\"),",
            "}",
            "]",
            ")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2358024)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2358025)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2358026)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'TensorSpec'), position=2, insert_id=2358027)",
            "Insert(target_node=ASTNode(type=argument_list), node=('tuple', None), position=1, insert_id=2358028)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=tuple), position=1)",
            "Insert(target_node=IN(type=tuple), node=('(', '('), position=0, insert_id=2358029)",
            "Insert(target_node=IN(type=tuple), node=('none', 'None'), position=1, insert_id=2358030)",
            "Insert(target_node=IN(type=tuple), node=(',', ','), position=2, insert_id=2358031)",
            "Insert(target_node=IN(type=tuple), node=('none', 'None'), position=3, insert_id=2358032)",
            "Insert(target_node=IN(type=tuple), node=(')', ')'), position=4, insert_id=2358033)",
            "Update(target_node=ASTNode(type=identifier, text=int64), value='int32')",
            "Update(target_node=ASTNode(type=identifier, text=int64), value='int32')",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=TensorSpec))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=none, text=None))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=none, text=None))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=tuple))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 24,
        "number": 3786,
        "neg_line": [
            "-\"attention_mask\": tf.TensorSpec((None, None), tf.int64, name=\"attention_mask\"),",
            "-\"token_type_ids\": tf.TensorSpec((None, None), tf.int64, name=\"token_type_ids\"),"
        ],
        "pos_line": [
            "+\"attention_mask\": tf.TensorSpec((None, None), tf.int32, name=\"attention_mask\"),",
            "+\"token_type_ids\": tf.TensorSpec((None, None), tf.int32, name=\"token_type_ids\"),"
        ],
        "core_change": "-\"attention_mask\": tf.TensorSpec((None, None), tf.int64, name=\"attention_mask\"), -\"token_type_ids\": tf.TensorSpec((None, None), tf.int64, name=\"token_type_ids\"), +\"attention_mask\": tf.TensorSpec((None, None), tf.int32, name=\"attention_mask\"), +\"token_type_ids\": tf.TensorSpec((None, None), tf.int32, name=\"token_type_ids\"),",
        "core_API": "TensorSpec"
    },
    {
        "commit_hash": "cbac2c5c0f7606aca8ccf08fbd418ffe3adfe427",
        "index": "9bc390dd..040276eb 100644",
        "commit_message": "[Compression] fix typehints (#4800)\n\n\n",
        "file": "nni.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DistMetricsCalculator(MetricsCalculator):",
            "if len(across_dim) == 0:",
            "dist_sum = torch.abs(reorder_tensor - other).sum()",
            "else:",
            "-                    dist_sum = torch.norm((reorder_tensor - other), p=self.p, dim=across_dim).sum()",
            "+                    dist_sum = torch.norm((reorder_tensor - other), p=self.p, dim=across_dim).sum()  # type: ignore",
            "# NOTE: this place need refactor when support layer level pruning.",
            "tmp_metric = metric",
            "for i in idx[:-1]:"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 3787,
        "neg_line": [
            "-dist_sum = torch.norm((reorder_tensor - other), p=self.p, dim=across_dim).sum()"
        ],
        "pos_line": [
            "+dist_sum = torch.norm((reorder_tensor - other), p=self.p, dim=across_dim).sum()  # type: ignore"
        ],
        "core_change": "-dist_sum = torch.norm((reorder_tensor - other), p=self.p, dim=across_dim).sum() +dist_sum = torch.norm((reorder_tensor - other), p=self.p, dim=across_dim).sum()  # type: ignore",
        "core_API": "abs"
    },
    {
        "commit_hash": "af3f0e27d7888e83a595bcd0e76c1bf9273c16b7",
        "index": "400a532..9c8b36d 100644",
        "commit_message": "Fix return_seq=True for dynamic RNN. (#229)\n\n\n",
        "file": "tflearn.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def bidirectional_rnn(incoming, rnncell_fw, rnncell_bw, return_seq=False,",
            "tf.add_to_collection(tf.GraphKeys.ACTIVATIONS, outputs[-1])",
            "",
            "if dynamic:",
            "-        outputs = tf.transpose(tf.pack(outputs), [1, 0, 2])",
            "-        o = advanced_indexing_op(outputs, sequence_length)",
            "+        if return_seq:",
            "+            o = outputs",
            "+        else:",
            "+            outputs = tf.transpose(tf.pack(outputs), [1, 0, 2])",
            "+            o = advanced_indexing_op(outputs, sequence_length)",
            "else:",
            "o = outputs if return_seq else outputs[-1]"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=ERROR), node=('default_parameter', None), position=4, insert_id=2353106)",
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=identifier, text=dynamic), position=11)",
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=:, text=:), position=12)",
            "Insert(target_node=ASTNode(type=ERROR), node=('if', 'if'), position=13, insert_id=2353107)",
            "Insert(target_node=ASTNode(type=ERROR), node=('identifier', 'return_seq'), position=14, insert_id=2353108)",
            "Insert(target_node=ASTNode(type=ERROR), node=(':', ':'), position=15, insert_id=2353109)",
            "Insert(target_node=ASTNode(type=ERROR), node=('identifier', 'o'), position=16, insert_id=2353110)",
            "Insert(target_node=ASTNode(type=ERROR), node=('=', '='), position=17, insert_id=2353111)",
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=identifier, text=outputs), position=18)",
            "Insert(target_node=ASTNode(type=ERROR), node=('else', 'else'), position=19, insert_id=2353112)",
            "Insert(target_node=ASTNode(type=ERROR), node=(':', ':'), position=20, insert_id=2353113)",
            "Insert(target_node=IN(type=default_parameter), node=('identifier', 'outputs'), position=0, insert_id=2353114)",
            "Move(target_node=IN(type=default_parameter), node=ASTNode(type==, text==), position=1)",
            "Move(target_node=IN(type=default_parameter), node=ASTNode(type=conditional_expression), position=2)",
            "Delete(target_node=ASTNode(type=type))",
            "Delete(target_node=ASTNode(type=typed_default_parameter))"
        ],
        "plus_line": 5,
        "minus_line": 2,
        "AST_diff_line": 16,
        "number": 3788,
        "neg_line": [
            "-outputs = tf.transpose(tf.pack(outputs), [1, 0, 2])",
            "-o = advanced_indexing_op(outputs, sequence_length)"
        ],
        "pos_line": [
            "+if return_seq:",
            "+o = outputs",
            "+else:",
            "+outputs = tf.transpose(tf.pack(outputs), [1, 0, 2])",
            "+o = advanced_indexing_op(outputs, sequence_length)"
        ],
        "core_change": "-outputs = tf.transpose(tf.pack(outputs), [1, 0, 2]) -o = advanced_indexing_op(outputs, sequence_length) +if return_seq: +o = outputs +else: +outputs = tf.transpose(tf.pack(outputs), [1, 0, 2]) +o = advanced_indexing_op(outputs, sequence_length)",
        "core_API": "add_to_collection"
    },
    {
        "commit_hash": "f38aaa5be05104ddfc3452249eeb521f139ded99",
        "index": "f4239345..5f7d1de5 100755",
        "commit_message": "improved global tensor handling, various other fixes\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class EpsilonDecay(Exploration):",
            "",
            "pred = tf.logical_or(x=(timestep < self.start_timestep),",
            "y=(timestep > self.start_timestep + int(self.timesteps)))",
            "-        return tf.fill(dims=shape, value=tf.cond(pred=pred, true_fn=true_fn, false_fn=false_fn))",
            "+        return tf.fill(dims=shape, value=self.cond(pred=pred, true_fn=true_fn, false_fn=false_fn))"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tf), value='self')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 3789,
        "neg_line": [
            "-return tf.fill(dims=shape, value=tf.cond(pred=pred, true_fn=true_fn, false_fn=false_fn))"
        ],
        "pos_line": [
            "+return tf.fill(dims=shape, value=self.cond(pred=pred, true_fn=true_fn, false_fn=false_fn))"
        ],
        "core_change": "-return tf.fill(dims=shape, value=tf.cond(pred=pred, true_fn=true_fn, false_fn=false_fn)) +return tf.fill(dims=shape, value=self.cond(pred=pred, true_fn=true_fn, false_fn=false_fn))",
        "core_API": "logical_or"
    },
    {
        "commit_hash": "32e5293feacd55cd694ddea3baebb02a5bebbaed",
        "index": "8e109513..b2a042ca 100644",
        "commit_message": "Fix and improve DPG agent, more policy/value-function changes\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class SignatureDict(NestedDict):",
            "elif name in kwargs:",
            "arg = kwargs[name]",
            "if isinstance(spec, self.value_type):",
            "-                    assert isinstance(arg, (tf.IndexedSlices, tf.Tensor, tf.Variable)), (name, spec, arg)",
            "+                    assert isinstance(arg, (tf.IndexedSlices, tf.Tensor, tf.Variable))",
            "if isinstance(arg, tf.IndexedSlices):",
            "# spec = tf.IndexedSlicesSpec(",
            "#     shape=spec.shape, dtype=spec.dtype, indices_dtype=arg.indices.dtype"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=name))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=spec))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=arg))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=tuple))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 3795,
        "neg_line": [
            "-assert isinstance(arg, (tf.IndexedSlices, tf.Tensor, tf.Variable)), (name, spec, arg)"
        ],
        "pos_line": [
            "+assert isinstance(arg, (tf.IndexedSlices, tf.Tensor, tf.Variable))"
        ],
        "core_change": "-assert isinstance(arg, (tf.IndexedSlices, tf.Tensor, tf.Variable)), (name, spec, arg) +assert isinstance(arg, (tf.IndexedSlices, tf.Tensor, tf.Variable))",
        "core_API": "IndexedSlicesSpec"
    },
    {
        "commit_hash": "1009792f24b115210228232a0c9b8426251db77f",
        "index": "1d4f64f0..d3d42f78 100644",
        "commit_message": "Fix init weights\n\n",
        "file": "d2l-en.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "def init_seq2seq_weights(module):",
            "",
            "Defined in :numref:`sec_seq2seq`\"\"\"",
            "if type(module) == nn.Linear:",
            "-         nn.init.xavier_uniform_(layer.weight)",
            "+         nn.init.xavier_uniform_(module.weight)",
            "if type(module) == nn.GRU:",
            "-        for param in layer._flat_weights_names:",
            "+        for param in module._flat_weights_names:",
            "if \"weight\" in param:",
            "nn.init.xavier_uniform_(module._parameters[param])"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 3796,
        "neg_line": [
            "-nn.init.xavier_uniform_(layer.weight)",
            "-for param in layer._flat_weights_names:"
        ],
        "pos_line": [
            "+nn.init.xavier_uniform_(module.weight)",
            "+for param in module._flat_weights_names:"
        ],
        "core_change": "-nn.init.xavier_uniform_(layer.weight) +nn.init.xavier_uniform_(module.weight) -for param in layer._flat_weights_names: +for param in module._flat_weights_names:",
        "core_API": "xavier_uniform_"
    },
    {
        "commit_hash": "44d8f890858f0c215eabb557c0eeefb6def9c599",
        "index": "e48e658c..c14ec314 100644",
        "commit_message": "fix heatconv\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class HEATConv(MessagePassing):",
            "alpha = softmax(alpha, index, ptr, size_i)",
            "alpha = F.dropout(alpha, p=self.dropout, training=self.training)",
            "",
            "-        out = self.lin(torch.cat([x_i, edge_attr], dim=-1)).unsqueeze(-2)",
            "-        out = out * alpha.unsqueeze(-1)",
            "-        return out",
            "+        out = self.lin(torch.cat([x_j, edge_attr], dim=-1)).unsqueeze(-2)",
            "+        return out * alpha.unsqueeze(-1)",
            "",
            "def __repr__(self) -> str:",
            "return (f'{self.__class__.__name__}({self.in_channels}, '"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('return_statement', None), position=4, insert_id=994043)",
            "Insert(target_node=IN(type=return_statement), node=('return', 'return'), position=0, insert_id=994044)",
            "Move(target_node=IN(type=return_statement), node=ASTNode(type=binary_operator), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=x_i), value='x_j')",
            "Delete(target_node=ASTNode(type=identifier, text=out))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=return, text=return))",
            "Delete(target_node=ASTNode(type=identifier, text=out))",
            "Delete(target_node=ASTNode(type=return_statement))"
        ],
        "plus_line": 2,
        "minus_line": 3,
        "AST_diff_line": 11,
        "number": 3800,
        "neg_line": [
            "-out = self.lin(torch.cat([x_i, edge_attr], dim=-1)).unsqueeze(-2)",
            "-out = out * alpha.unsqueeze(-1)",
            "-return out"
        ],
        "pos_line": [
            "+out = self.lin(torch.cat([x_j, edge_attr], dim=-1)).unsqueeze(-2)",
            "+return out * alpha.unsqueeze(-1)"
        ],
        "core_change": "-out = self.lin(torch.cat([x_i, edge_attr], dim=-1)).unsqueeze(-2) -out = out * alpha.unsqueeze(-1) -return out +out = self.lin(torch.cat([x_j, edge_attr], dim=-1)).unsqueeze(-2) +return out * alpha.unsqueeze(-1)",
        "core_API": "dropout"
    },
    {
        "commit_hash": "3dda5ac9fcf390d8b83eb855249360711707c11c",
        "index": "63833e2f..97040b1c 100644",
        "commit_message": "for discussion: incorporate black code formatter (#3308)\n\n* setup files\n\n* run black\n\n* undo\n\n* update CONTRIBUTING.md\n\n* fix quotes in test_other_modules\n\n* make flake8 happy\n\n* set black to 100 characters per line\n\n* move type: ignore to where mypy wants them\n\n* more flake8\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class EntropyTest(AllenNlpTestCase):",
            "def test_masked_case(self):",
            "metric = Entropy()",
            "# This would have non-zero entropy without the mask.",
            "-        logits = torch.Tensor([[1, 1, 1, 1],",
            "-                               [10000, -10000, -10000, -1000]])",
            "+        logits = torch.Tensor([[1, 1, 1, 1], [10000, -10000, -10000, -1000]])",
            "mask = torch.Tensor([0, 1])",
            "metric(logits, mask)",
            "assert metric.get_metric() == 0.0"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 3803,
        "neg_line": [
            "-logits = torch.Tensor([[1, 1, 1, 1],",
            "-[10000, -10000, -10000, -1000]])"
        ],
        "pos_line": [
            "+logits = torch.Tensor([[1, 1, 1, 1], [10000, -10000, -10000, -1000]])"
        ],
        "core_change": "-logits = torch.Tensor([[1, 1, 1, 1], -[10000, -10000, -10000, -1000]]) +logits = torch.Tensor([[1, 1, 1, 1], [10000, -10000, -10000, -1000]])",
        "core_API": "Tensor"
    },
    {
        "commit_hash": "33b7532e69aaed83f31e8ad5da0a36c28e140da2",
        "index": "2a8e862c2..d254c115f 100644",
        "commit_message": "Fix longformer attention mask type casting when using apex (#4574)\n\n* Fix longformer attention mask casting when using apex\n\n* remove extra type casting\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class LongformerSelfAttention(nn.Module):",
            "]",
            "attn[extra_attention_mask_nonzeros[::-1]] = nonzero_selected_attn.view(",
            "len(selection_padding_mask_nonzeros[0]), -1",
            "-            ).type_as(hidden_states)",
            "+            )",
            "",
            "context_layer = attn.transpose(0, 1)",
            "if self.output_attentions:"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=type_as))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=hidden_states))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 3804,
        "neg_line": [
            "-).type_as(hidden_states)"
        ],
        "pos_line": [
            "+)"
        ],
        "core_change": "-).type_as(hidden_states) +)",
        "core_API": "view"
    },
    {
        "commit_hash": "176b2e4e766c94e64f945b9c1323e612d8887d83",
        "index": "ed2d7446..85412514 100644",
        "commit_message": "Fix warning for empty tensor without type\n\nSummary:\nFairseq create an empty tensor without type.\nIt will create warning for torchscript model.\nWarning: Creating a tensor from an empty intlist will create a tensor of default floating point type  (currently Float) in python but a tensor of type int in torchscript.\n\nThis diff adds definition of the type.\n\nReviewed By: myleott\n\nDifferential Revision: D29081170\n\nfbshipit-source-id: 5c32aae65c9998b245eac43bfedc820bea509338\n\n",
        "file": "fairseq.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class NGramRepeatBlock(nn.Module):",
            "]",
            "for bbsz_idx in range(bsz * beam_size):",
            "lprobs[bbsz_idx][",
            "-                torch.tensor(banned_tokens[bbsz_idx]).long()",
            "+                torch.tensor(banned_tokens[bbsz_idx], dtype=torch.int64)",
            "] = torch.tensor(-math.inf).to(lprobs)",
            "return lprobs"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=IN(type=root), node=ASTNode(type=call), position=0)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=206027)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=206028)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=206029)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=206030)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=206031)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=206032)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=long), value='int64')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=long), position=2)",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 14,
        "number": 3805,
        "neg_line": [
            "-torch.tensor(banned_tokens[bbsz_idx]).long()"
        ],
        "pos_line": [
            "+torch.tensor(banned_tokens[bbsz_idx], dtype=torch.int64)"
        ],
        "core_change": "-torch.tensor(banned_tokens[bbsz_idx]).long() +torch.tensor(banned_tokens[bbsz_idx], dtype=torch.int64)",
        "core_API": "tensor"
    },
    {
        "commit_hash": "8f72c929868a0be6ff132614b0a161c3ad63d778",
        "index": "6777359..8df7809 100644",
        "commit_message": "Following griegler's suggestion, fixed a bug in NLayerDiscriminator that always applied sigmoid to the output\n\n",
        "file": "pytorch-CycleGAN-and-pix2pix.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class NLayerDiscriminator(nn.Module):",
            "]",
            "",
            "sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=1)]",
            "-        sequence += [nn.Sigmoid()]",
            "+",
            "+        if use_sigmoid:",
            "+            sequence += [nn.Sigmoid()]",
            "",
            "self.model = nn.Sequential(*sequence)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=3, insert_id=1473254)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=1473255)",
            "Insert(target_node=IN(type=if_statement), node=('identifier', 'use_sigmoid'), position=1, insert_id=1473256)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=1473257)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=1473258)",
            "Move(target_node=IN(type=block), node=ASTNode(type=expression_statement), position=0)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 3809,
        "neg_line": [
            "-sequence += [nn.Sigmoid()]"
        ],
        "pos_line": [
            "+",
            "+if use_sigmoid:",
            "+sequence += [nn.Sigmoid()]"
        ],
        "core_change": "-sequence += [nn.Sigmoid()] + +if use_sigmoid: +sequence += [nn.Sigmoid()]",
        "core_API": "Conv2d"
    },
    {
        "commit_hash": "cf90394a091648eb29a74f5c35e0f11b7aeaf99d",
        "index": "d76014e88e..4e9e4143af 100644",
        "commit_message": "[rllib] Fix TF2 import of EagerVariableStore (#5625)\n\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tensorflow",
        "change": [
            "class DistributionalQModel(TFModelV2):",
            "return state_score",
            "",
            "if tf.executing_eagerly():",
            "+            from tensorflow.python.ops import variable_scope",
            "# Have to use a variable store to reuse variables in eager mode",
            "-            import tensorflow.contrib as tfc",
            "-            store = tfc.eager.EagerVariableStore()",
            "+            store = variable_scope.EagerVariableStore()",
            "",
            "# Save the scope objects, since in eager we will execute this",
            "# path repeatedly and there is no guarantee it will always be run"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=block), node=('import_from_statement', None), position=0, insert_id=2454159)",
            "Insert(target_node=IN(type=import_from_statement), node=('from', 'from'), position=0, insert_id=2454160)",
            "Move(target_node=IN(type=import_from_statement), node=ASTNode(type=dotted_name), position=1)",
            "Insert(target_node=IN(type=import_from_statement), node=('import', 'import'), position=2, insert_id=2454161)",
            "Insert(target_node=IN(type=import_from_statement), node=('dotted_name', None), position=3, insert_id=2454162)",
            "Update(target_node=ASTNode(type=identifier, text=contrib), value='python')",
            "Insert(target_node=ASTNode(type=dotted_name), node=('.', '.'), position=3, insert_id=2454163)",
            "Insert(target_node=ASTNode(type=dotted_name), node=('identifier', 'ops'), position=4, insert_id=2454164)",
            "Update(target_node=ASTNode(type=identifier, text=tfc), value='variable_scope')",
            "Move(target_node=IN(type=dotted_name), node=ASTNode(type=identifier, text=tfc), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=tfc), value='variable_scope')",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=tfc), position=0)",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Delete(target_node=ASTNode(type=import, text=import))",
            "Delete(target_node=ASTNode(type=as, text=as))",
            "Delete(target_node=ASTNode(type=aliased_import))",
            "Delete(target_node=ASTNode(type=import_statement))",
            "Delete(target_node=ASTNode(type=identifier, text=eager))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 20,
        "number": 3811,
        "neg_line": [
            "-import tensorflow.contrib as tfc",
            "-store = tfc.eager.EagerVariableStore()"
        ],
        "pos_line": [
            "+from tensorflow.python.ops import variable_scope",
            "+store = variable_scope.EagerVariableStore()"
        ],
        "core_change": "+from tensorflow.python.ops import variable_scope -import tensorflow.contrib as tfc -store = tfc.eager.EagerVariableStore() +store = variable_scope.EagerVariableStore()",
        "core_API": "executing_eagerly"
    },
    {
        "commit_hash": "3c7c1022262ecbede6043ffc46e10b20db5112c5",
        "index": "bb303b6..16d65a5 100644",
        "commit_message": "Fix vector normalization bug in semantic_similarity_with_tf_hub_unversal_encoder colab\n\nPiperOrigin-RevId: 191657545\n\n",
        "file": "hub.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "\"\\n\",",
            "\"# For evaluation we use exactly normalized rather than\\n\",",
            "\"# approximately normalized.\\n\",",
            "-        \"sts_encode1 = tf.nn.l2_normalize(embed(sts_input1))\\n\",",
            "-        \"sts_encode2 = tf.nn.l2_normalize(embed(sts_input2))\\n\",",
            "-        \"sim_scores = tf.reduce_sum(tf.multiply(sts_encode1, sts_encode2), axis=1)\"",
            "+        \"sts_encode1 = tf.nn.l2_normalize(embed(sts_input1), axis=1)\\n\",",
            "+        \"sts_encode2 = tf.nn.l2_normalize(embed(sts_input2), axis=1)\\n\",",
            "+        \"sim_scores = -tf.acos(tf.reduce_sum(tf.multiply(sts_encode1, sts_encode2), axis=1))\"",
            "]",
            "},",
            "{"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"sts_encode1 = tf.nn.l2_normalize(embed(sts_input1))\\n\"), value='\"sts_encode1 = tf.nn.l2_normalize(embed(sts_input1), axis=1)\\\\n\"')",
            "Update(target_node=ASTNode(type=string, text=\"sts_encode2 = tf.nn.l2_normalize(embed(sts_input2))\\n\"), value='\"sts_encode2 = tf.nn.l2_normalize(embed(sts_input2), axis=1)\\\\n\"')",
            "Update(target_node=ASTNode(type=string, text=\"sim_scores = tf.reduce_sum(tf.multiply(sts_encode1, sts_encode2), axis=1)\"), value='\"sim_scores = -tf.acos(tf.reduce_sum(tf.multiply(sts_encode1, sts_encode2), axis=1))\"')"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 3,
        "number": 3813,
        "neg_line": [
            "-\"sts_encode1 = tf.nn.l2_normalize(embed(sts_input1))\\n\",",
            "-\"sts_encode2 = tf.nn.l2_normalize(embed(sts_input2))\\n\",",
            "-\"sim_scores = tf.reduce_sum(tf.multiply(sts_encode1, sts_encode2), axis=1)\""
        ],
        "pos_line": [
            "+\"sts_encode1 = tf.nn.l2_normalize(embed(sts_input1), axis=1)\\n\",",
            "+\"sts_encode2 = tf.nn.l2_normalize(embed(sts_input2), axis=1)\\n\",",
            "+\"sim_scores = -tf.acos(tf.reduce_sum(tf.multiply(sts_encode1, sts_encode2), axis=1))\""
        ],
        "core_change": "-\"sts_encode1 = tf.nn.l2_normalize(embed(sts_input1))\\n\", -\"sts_encode2 = tf.nn.l2_normalize(embed(sts_input2))\\n\", -\"sim_scores = tf.reduce_sum(tf.multiply(sts_encode1, sts_encode2), axis=1)\" +\"sts_encode1 = tf.nn.l2_normalize(embed(sts_input1), axis=1)\\n\", +\"sts_encode2 = tf.nn.l2_normalize(embed(sts_input2), axis=1)\\n\", +\"sim_scores = -tf.acos(tf.reduce_sum(tf.multiply(sts_encode1, sts_encode2), axis=1))\"",
        "core_API": "l2_normalize"
    },
    {
        "commit_hash": "3e46a483a233c01e4ac5e2a7650c88e17feb90a7",
        "index": "880aac66a4..0ddfcf56d9 100644",
        "commit_message": "lintfixbot: Auto-commit fixed lint errors in codebase\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def nanmedian(",
            "overwrite_input: Optional[bool] = False,",
            "out: Optional[torch.tensor] = None,",
            ") -> torch.tensor:",
            "-    return torch.nanmedian(input, axis=axis, keepdims=keepdims, overwrite_input=overwrite_input, out=out)",
            "+    return torch.nanmedian(",
            "+        input, axis=axis, keepdims=keepdims, overwrite_input=overwrite_input, out=out",
            "+    )"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 3817,
        "neg_line": [
            "-return torch.nanmedian(input, axis=axis, keepdims=keepdims, overwrite_input=overwrite_input, out=out)"
        ],
        "pos_line": [
            "+return torch.nanmedian(",
            "+input, axis=axis, keepdims=keepdims, overwrite_input=overwrite_input, out=out",
            "+)"
        ],
        "core_change": "-return torch.nanmedian(input, axis=axis, keepdims=keepdims, overwrite_input=overwrite_input, out=out) +return torch.nanmedian( +input, axis=axis, keepdims=keepdims, overwrite_input=overwrite_input, out=out +)",
        "core_API": "nanmedian"
    },
    {
        "commit_hash": "2a6fcc6cab0a0371698bc9fa375c67e633817c86",
        "index": "bb98c51739..7bd835921a 100644",
        "commit_message": "formatting fixes for Array API submodule in Torch backend.\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def cos(x: torch.Tensor)\\",
            "return torch.cos(x)",
            "",
            "",
            "-def logical_not(x: torch.Tensor) -> torch.Tensor:",
            "+def logical_not(x: torch.Tensor)\\",
            "+        -> torch.Tensor:",
            "return torch.logical_not(x.type(torch.bool))"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 17,
        "number": 3819,
        "neg_line": [
            "-def logical_not(x: torch.Tensor) -> torch.Tensor:"
        ],
        "pos_line": [
            "+def logical_not(x: torch.Tensor)\\",
            "+-> torch.Tensor:"
        ],
        "core_change": "-def logical_not(x: torch.Tensor) -> torch.Tensor: +def logical_not(x: torch.Tensor)\\ +-> torch.Tensor:",
        "core_API": "cos"
    },
    {
        "commit_hash": "0af8eb6aa9c1b40e6805fd3dd84b65d101cb55f8",
        "index": "82beef5c..89828029 100644",
        "commit_message": "[Feat] Jit warp perspective (#574)\n\n* Torchscriptable warp_perspective()\n\n* Simplify device and type casting\n\n* fix tests\n\n* Remove redundant parameter and update docs of warp_grid\n\n* Update docs\n\n* fix typing errors\n\n* Fixed homography warper tests\n\n* Added jit warp perspective tests\n\n* Added jit warp perspective tests\n\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def transform_points(trans_01: torch.Tensor,",
            ">>> trans_01 = torch.eye(4).view(1, 4, 4)  # Bx4x4",
            ">>> points_0 = kornia.transform_points(trans_01, points_1)  # BxNx3",
            "\"\"\"",
            "-    if not torch.is_tensor(trans_01) or not torch.is_tensor(points_1):",
            "-        raise TypeError(\"Input type is not a torch.Tensor\")",
            "+    check_is_tensor(trans_01)",
            "+    check_is_tensor(points_1)",
            "if not trans_01.device == points_1.device:",
            "raise TypeError(\"Tensor must be in the same device\")",
            "if not trans_01.shape[0] == points_1.shape[0] and trans_01.shape[0] != 1:"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 3820,
        "neg_line": [
            "-if not torch.is_tensor(trans_01) or not torch.is_tensor(points_1):",
            "-raise TypeError(\"Input type is not a torch.Tensor\")"
        ],
        "pos_line": [
            "+check_is_tensor(trans_01)",
            "+check_is_tensor(points_1)"
        ],
        "core_change": "-if not torch.is_tensor(trans_01) or not torch.is_tensor(points_1): -raise TypeError(\"Input type is not a torch.Tensor\") +check_is_tensor(trans_01) +check_is_tensor(points_1)",
        "core_API": "eye"
    },
    {
        "commit_hash": "6b423ba8bde19c831b7e8c7eb1d6d67f886d71ad",
        "index": "1d2185da..aa1139de 100644",
        "commit_message": "Heterogeneous Graph Support + GraphGym (#3068)\n\n* added HGT DBLP example\n\n* typo\n\n* Merge PyG master (#52)\n\n* Adding the Facebok Page-Page dataset\n\n* type hints\n\n* documentation CI\n\n* py 3.8\n\n* fix links\n\n* fix links\n\n* fail on warning\n\n* fail on warning\n\n* fix doc\n\nCo-authored-by: benedekrozemberczki <benedek.rozemberczki@gmail.com>\n\n* revert\n\n* Fix Documentation Rendering (#51)\n\n* fix doc rendering\n\n* fix linting\n\n* retrigger checks\n\n* remove pytorch 1.7.0 legacy code (#50)\n\n* Fix `copy.deepcopy` within lazy `nn.dense.Linear` (#44)\n\n* fix deepcopy within lazy Linear\n\n* fix merge\n\n* assert exception\n\n* example to doc\n\n* resolve conflict\n\n* resolve conflict\n\n* Add Figure and Equation to `to_hetero` docstring (#60)\n\n* add tex\n\n* add svg + docstring\n\n* typo\n\n* added equation\n\n* Message Passing Hooks (#53)\n\n* add hooks\n\n* docstring\n\n* add docstring\n\n* allow modification of inputs/output\n\n* add test for modifying output\n\n* add additional asserts for modifying output test\n\n* Rename `HeteroData.get_edges` and `HeteroData.get_nodes` (#58)\n\n* rename to_edges and to_nodes\n\n* typo\n\n* `HeteroConv` (#64)\n\n* clean heteroconv\n\n* init\n\n* init\n\n* clean up\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* fix documentation\n\n* bipartite function\n\n* fix test CI\n\n* remove pillow version\n\n* clean up for merge\n\n* Merge PyG master (#69)\n\n* renaming: PointConv to PointNetConv\n\n* Fix a broken link in datasets/gdelt.py (#2800)\n\n* fix test\n\n* re-add batching of strings\n\n* add quick start table\n\n* gnn cheatsheet\n\n* remove pillow version\n\nCo-authored-by: Dongkwan Kim <todoaskit@gmail.com>\n\n* re-merge\n\n* add lazy column to GNN cheatsheet (#70)\n\n* `to_hetero_with_bases(model)` (#63)\n\n* update\n\n* fix linting\n\n* basisconv\n\n* add ValueError\n\n* to_hetero_with_bases impl done\n\n* add test\n\n* add comments\n\n* add comments\n\n* docstring\n\n* typo\n\n* update figure\n\n* svg\n\n* typo\n\n* add test\n\n* update\n\n* add rgcn equality test\n\n* typos\n\n* update\n\n* typos\n\n* update figures\n\n* generate new svgs\n\n* fix assignment\n\n* rename\n\n* delete sorted edge types\n\n* rename\n\n* add legend\n\n* fix typo\n\n* Test: Check equal outputs of `to_hetero` and `RGCNConv` (#59)\n\n* check equal output\n\n* add sparsetensor test\n\n* check equal output\n\n* add sparsetensor test\n\n* rename\n\n* linting\n\n* add missing import\n\n* `HeteroData` support for `T.NormalizeFeatures` (#56)\n\n* normalize features\n\n* allow normalization of any feature\n\n* in-place div\n\n* normalize features\n\n* allow normalization of any feature\n\n* in-place div\n\n* fix test\n\n* no need to re-assign\n\n* `HeteroData` support for `T.AddSelfLoops` (#54)\n\n* hetero support for AddSelfLoops\n\n* check for edge_index attribute\n\n* f-string\n\n* retrigger checks\n\n* revert bipartite changes\n\n* hetero support for AddSelfLoops\n\n* check for edge_index attribute\n\n* f-string\n\n* retrigger checks\n\n* revert bipartite changes\n\n* merge master\n\n* merge master\n\n* `HeteroData` support for `T.ToSparseTensor` (#55)\n\n* hetero support for ToSparseTensor\n\n* add test\n\n* customize the attribute of SparseTensor.value\n\n* rework sort_edge_index\n\n* hetero support for ToSparseTensor\n\n* add test\n\n* customize the attribute of SparseTensor.value\n\n* rework sort_edge_index\n\n* linting\n\n* `HeteroData` support for `T.ToUndirected` (#57)\n\n* to_undirected\n\n* revert bipartite changes\n\n* coalesce + undirected enhancement\n\n* merge master\n\n* revert bipartite changes\n\n* coalesce + undirected enhancement\n\n* merge master\n\n* clean up\n\n* new default relation type\n\n* fix tests\n\n* resolve merge conflicts\n\n* resolve merge conflicts 2\n\n* resolve merge conflicts 3\n\n* Merge PyG master (#74)\n\n* renaming: PointConv to PointNetConv\n\n* Fix a broken link in datasets/gdelt.py (#2800)\n\n* fix test\n\n* re-add batching of strings\n\n* add quick start table\n\n* gnn cheatsheet\n\n* remove pillow version\n\n* clean up doc for to_dense_batch\n\n* clean up\n\n* add legend to cheatsheet\n\n* Improve terminology (#2837)\n\nI think the previous version of the document uses the term 'symmetric' incorrectly. A symmetric matrix is a square matrix that is is equal to its transpose (https://en.wikipedia.org/wiki/Symmetric_matrix). However, the text is only talking about the shape of the matrix, not its content. Hence, 'square (matrix)' would be the correct term to use.\n\n* Add batch_size input to to_dense_batch (#2838)\n\n* Add batch_size input to to_dense_batch\n\n* to_dense_batch fix typo in batch_size param use\n\n* add typehints\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* typo\n\n* Added return_attention_weights to TransformerConv. (#2807)\n\n* added return_weights functionality to tranformer\n\n* added return attn weights tests\n\n* flake8\n\n* added typehints\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* MD17 (#2843)\n\n* Added MD17 dataset\n\n* Updated Documentation\n\n* Added link to sGDML website in doc\n\n* fixed typos in doc and made train variable description clearer\n\n* clean up\n\n* fix linting\n\n* fix doc warning\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* update doc\n\n* remove forward doc\n\n* add static graph support info to cheatsheet\n\n* fix num_nodes in case edge_index is empty\n\n* fix math formula\n\n* faster GDC import\n\n* lazy import\n\n* lazy import for datasets\n\n* lazy import for nn\n\n* Sequential jittable + traceable\n\n* typo\n\n* typo\n\n* update doc\n\nCo-authored-by: Dongkwan Kim <todoaskit@gmail.com>\nCo-authored-by: Markus <markus.zopf@outlook.com>\nCo-authored-by: Jimmie <jimmiebtlr@gmail.com>\nCo-authored-by: Jinu Sunil <jinu.sunil@gmail.com>\nCo-authored-by: Moritz R Schfer <moritz.schaefer@protonmail.com>\n\n* re-add\n\n* GraphGym cleaned version (#82)\n\n* GraphGym cleaned version\n\n* remove deepsnap dependency\n\n* fix lint errors, part 1\n\n* fix all lint errors\n\n* fix all lint errors\n\n* fix all lint errors\n\n* apply yapf\n\n* Update .gitignore\n\n* Integrate GraphGym into PyG (#85)\n\n* GraphGym cleaned version\n\n* remove deepsnap dependency\n\n* fix lint errors, part 1\n\n* fix all lint errors\n\n* fix all lint errors\n\n* fix all lint errors\n\n* apply yapf\n\n* Integrate graphgym into pyg, keep user API in project root\n\n* fix merge conflict\n\n* fix lint errors\n\n* Make optional dependencies\n\n* merge LICENSE from GraphGym\n\n* add import\n\n* clean up LICENSE\n\n* fix import\n\n* resolve merge conflicts\n\n* resolve merge conflicts 2\n\n* Merge PyG master (#87)\n\n* renaming: PointConv to PointNetConv\n\n* Fix a broken link in datasets/gdelt.py (#2800)\n\n* fix test\n\n* re-add batching of strings\n\n* add quick start table\n\n* gnn cheatsheet\n\n* remove pillow version\n\n* clean up doc for to_dense_batch\n\n* clean up\n\n* add legend to cheatsheet\n\n* Improve terminology (#2837)\n\nI think the previous version of the document uses the term 'symmetric' incorrectly. A symmetric matrix is a square matrix that is is equal to its transpose (https://en.wikipedia.org/wiki/Symmetric_matrix). However, the text is only talking about the shape of the matrix, not its content. Hence, 'square (matrix)' would be the correct term to use.\n\n* Add batch_size input to to_dense_batch (#2838)\n\n* Add batch_size input to to_dense_batch\n\n* to_dense_batch fix typo in batch_size param use\n\n* add typehints\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* typo\n\n* Added return_attention_weights to TransformerConv. (#2807)\n\n* added return_weights functionality to tranformer\n\n* added return attn weights tests\n\n* flake8\n\n* added typehints\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* MD17 (#2843)\n\n* Added MD17 dataset\n\n* Updated Documentation\n\n* Added link to sGDML website in doc\n\n* fixed typos in doc and made train variable description clearer\n\n* clean up\n\n* fix linting\n\n* fix doc warning\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* update doc\n\n* remove forward doc\n\n* add static graph support info to cheatsheet\n\n* fix num_nodes in case edge_index is empty\n\n* fix math formula\n\n* faster GDC import\n\n* lazy import\n\n* lazy import for datasets\n\n* lazy import for nn\n\n* Sequential jittable + traceable\n\n* typo\n\n* typo\n\n* update doc\n\n* Simple models (#2869)\n\n* Inclusion of new backbone models\n\n* Eliminating head from asap.py\n\n* small correction\n\n* Create test_gcn.py\n\n* Update __init__.py\n\n* Update test_gcn.py\n\n* Left only the convolutional simple models\n\n* Tests included\n\n* update\n\n* clean up\n\n* clean up v2\n\n* fix activation\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* Example for MemPooling. (#2729)\n\n* example for mem pooling\n\n* backprop on kl loss is done at the end of an epoch. Keys in memory layers are trained only on kl loss.\n\n* added learning rate decay. Using PROTIENS_full\n\n* flake8\n\n* reduced lr. increased weight decay\n\n* changed download location\n\n* added comments\n\n* clean up\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* typos\n\n* fix removeisolatednodes transform in case 'data.num_nodes' is present\n\n* fix XConv with dilation > 1\n\n* fix XConv with dilation > 1\n\n* rgcn link prediction  (#2734)\n\n* implemented LinkPrediction dataset for loading FB15k237\n\n* implemented evaluation for relational link prediction\n\n* implemented R-GCNConf link prediction example\n\n* fixed bug: wrong initial objects in negative_sampling\n\n* changed file downloader urllib.request.urlretrieve  to pytorch.data.download_url; renamed LinkPrediction class to RelationalLinkPredictionDataset\n\n* update dataset\n\n* update example script\n\n* rename\n\nCo-authored-by: Moritz <moritzblum>\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* fix gnnexplainer draw kwargs\n\n* remove python-louvain dependency\n\n* allow customization of output in MP jit mode\n\n* fix test for py3.6\n\n* changed normalisation to same norm from instance norm to be robust to small var (#2917)\n\n* add CITATION.cff\n\n* format\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* add basetransform ABC (#2924)\n\n* clean up BaseTransform\n\n* clean up GATConv and add comments\n\n* add max_num_neighbors as an additional argument\n\n* fix jit GATConv on PyTorch 1.8.0\n\n* fix doc\n\n* fix gnn explainer with existing self-loops\n\n* Rgcn link pred fix (#2946)\n\n* added regularization, removed typo in test\n\n* clean up\n\nCo-authored-by: Moritz <moritzblum>\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* typo\n\n* Correct gini coefficient mathcal formula (#2932)\n\n* typo\n\n* typo\n\n* Update from_networkx (#2923)\n\n* Update from_networkx\n\n* Update test\n\n* Update convert.py\n\n* Minor corrections\n\n* Update test_convert.py\n\n* Corrections\n\n* Update test_convert.py\n\n* Case where there are no edges\n\n* Correcting how edge_attr are concatenated\n\n* clean up + new test\n\n* remove unused code\n\n* add union type\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* fix deterministic ordering in from_networkx\n\n* recursive-include *.jinja files\n\nCo-authored-by: Dongkwan Kim <todoaskit@gmail.com>\nCo-authored-by: Markus <markus.zopf@outlook.com>\nCo-authored-by: Jimmie <jimmiebtlr@gmail.com>\nCo-authored-by: Jinu Sunil <jinu.sunil@gmail.com>\nCo-authored-by: Moritz R Schfer <moritz.schaefer@protonmail.com>\nCo-authored-by: PabloAMC <pmorenocf@alumnos.unex.es>\nCo-authored-by: Moritz Blum <31183934+moritzblum@users.noreply.github.com>\nCo-authored-by: fbragman <fbragman@users.noreply.github.com>\nCo-authored-by: Christopher Lee <2824685+CCInc@users.noreply.github.com>\nCo-authored-by: Tim Daubenschtz <tim@daubenschuetz.de>\n\n* resolve merge conflicts 3\n\n* resolve merge conflicts 4\n\n* Implementation of the `HGTLoader` + `ogbn-mag` example (#73)\n\n* first try\n\n* update\n\n* HGT Loader\n\n* typo\n\n* first try\n\n* update\n\n* HGT Loader\n\n* typo\n\n* bugfixes\n\n* lazy GATConv\n\n* bugfix\n\n* bugfix\n\n* full working pipeline\n\n* update\n\n* rename\n\n* docstring\n\n* typos\n\n* update\n\n* typo\n\n* typo\n\n* typo\n\n* added comments\n\n* add test\n\n* add tests\n\n* fix example\n\n* rename\n\n* linting\n\n* Random split functionalities (#72)\n\n* link split\n\n* create split\n\n* example tests\n\n* link split tests\n\n* fix linting\n\n* update docstring\n\n* undirected option, refactor and docs\n\n* add num nodes as argument to neg sampling\n\n* clean up + remove single object\n\n* update example\n\n* typo\n\n* fix compose\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* add basetransform\n\n* typo\n\n* typo\n\n* fix test\n\n* Improve `torch_geometric.data` Documentation (#98)\n\n* update data doc\n\n* typo\n\n* typo\n\n* note\n\n* typo\n\n* add docstring\n\n* only show inherited members for data and hetero_data\n\n* documentation update for batch and dataset\n\n* update doc\n\n* update\n\n* fix\n\n* record_stream\n\n* update\n\n* typo\n\n* add/fix data functionality\n\n* linting\n\n* typo\n\n* `_parent` memory leak fix (#103)\n\n* memory leak fix\n\n* Clean up\n\n* clean up\n\n* bugfix tests\n\n* typos\n\n* fix test\n\n* fix test\n\n* rename reverse\n\n* (Heterogeneous) `NeighborLoader` (#92)\n\n* initial commit\n\n* typo\n\n* neighbor loader functionality + tests\n\n* docstring\n\n* fix docstring\n\n* skip tests\n\n* fix share_memory_\n\n* typo\n\n* typo\n\n* update example\n\n* typo\n\n* share_strategy\n\n* fix cuda calls\n\n* better print\n\n* fix size\n\n* fix print\n\n* final commit\n\n* fix\n\n* some todos\n\n* preprocessed features\n\n* fix to_undirected\n\n* more documentation\n\n* update doc\n\n* fix doc\n\n* fix doc\n\n* Add benchmark code and the example with existing graph classification examples (#93)\n\n* add benchmarking utilities\n\n* update graph classification benchmark\n\n* improve code style\n\n* add pytorch-memlab for benchmark code\n\n* skip some tests when cuda is not available\n\n* add type hint when appropriate\n\n* add seed_everything to improve code\n\n* code refactoring\n\n* code refactoring\n\n* code refactoring\n\n* code improvement\n\n* remove unnecessary dataloader import\n\n* change benchmark interface with decorator\n\n* documentation improvement\n\n* linting\n\n* linting part 2\n\n* linting part 3\n\n* seed_everything\n\n* create utils file\n\n* update\n\n* use utils functions\n\n* fix test\n\n* update the profiler to the latest torch (1.8.1+)\n\n* refactor profiler and add more documentation\n\n* refactor profiler and add more documentation\n\n* resolve lint errors\n\n* resolve lint errors\n\n* update\n\n* clean up test and profile\n\n* fix linting\n\n* add to doc\n\n* fix doc\n\n* typo\n\n* update benchmark\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* Move `HGTLoader` to `torch_geometric.loader` + clean up (#104)\n\n* move files\n\n* use utils functions\n\n* fix example\n\n* update\n\n* fix tests\n\n* fix seed\n\n* fix linear test\n\n* rename\n\n* Support GraphGym custom modules outside PyG package (#102)\n\n* GraphGym cleaned version\n\n* remove deepsnap dependency\n\n* fix lint errors, part 1\n\n* fix all lint errors\n\n* fix all lint errors\n\n* fix all lint errors\n\n* apply yapf\n\n* Integrate graphgym into pyg, keep user API in project root\n\n* fix merge conflict\n\n* fix lint errors\n\n* Make optional dependencies\n\n* merge LICENSE from GraphGym\n\n* Enable adding GraphGym customized modules outside PyG package\n\n* lint\n\n* Rename `AddTrainValTestMask` to `RandomNodeSplit` (#108)\n\n* initial commit\n\n* rename example\n\n* remove AddTrainValTestMask\n\n* fix linting\n\n* create optimizer config and scheduler config separately (#113)\n\n* create optimizer config and scheduler config separately\n\n* fix format\n\n* import explicitly\n\nCo-authored-by: Dong Wang <dongwang@yannis-air.lan>\n\n* Heterogeneous Graph Tutorial (#83)\n\n* add HG tutorial roadmap\n\n* started working on hg tutorial\n\n* hg_tutorial, some text and .tex figure\n\n* added svg\n\n* hg tutorial content\n\n* fix CI\n\n* text and structure\n\n* finished first draft\n\n* fixed one code example\n\n* fixing conventions\n\n* fixing links\n\n* update svg\n\n* some smaller improvements of tutorial\n\n* improvements on tutorial\n\n* hg-tutorial: fixed compiling issue, added detailed content\n\n* added absolute links\n\n* fixed warnings\n\n* streamlined dataset section\n\n* update svg\n\n* update tutorial\n\n* update 2\n\nCo-authored-by: Jan Eric Lenssen <janeric.lenssen@tu-dortmund.de>\n\n* typo\n\n* Move data loaders to `torch_geometric.loader` (#110)\n\n* move graphsaint\n\n* deprecations\n\n* move clusterloader\n\n* deprecations\n\n* type hints\n\n* move shadow\n\n* typo\n\n* typo\n\n* move datalistloader\n\n* dense data loader\n\n* random node sampler\n\n* fix doc\n\n* Lazy GNN operators (#89)\n\n* lazy cheb conv\n\n* lazy GraphConv\n\n* lazy GATv2Conv\n\n* lazy TAGConv\n\n* lazy FAConv\n\n* lazy FeaStConv\n\n* lazy NNConv\n\n* typo\n\n* fix tests\n\n* lazy SuperGATConv\n\n* lazy SuperGATConv fix\n\n* lazy SplineConv\n\n* fix lazy check\n\n* lazy GravNetConv\n\n* arma conv lazy\n\n* dense linear in gmmconv\n\n* typo\n\n* add test\n\n* lazy GMMConv\n\n* doc\n\n* rename (#116)\n\n* Revisit `MetaPath2Vec` (#114)\n\n* revisit metapath2vec\n\n* update\n\n* typo\n\n* update\n\n* fix doc\n\n* update\n\n* check for attributes rather than key\n\n* Clean up `torch_geometric.profile` further (#111)\n\n* remove print_layer_stats\n\n* typos\n\n* update\n\n* readme highlights and quick tour (#99)\n\n* readme highlights and quick tour\n\n* arch\n\n* arch image\n\n* arch overview\n\n* list categories\n\n* categorization\n\n* category description\n\n* Update README.md\n\nfrom Matthias\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* improved highlights\n\n* Update README.md\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* Update README.md\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* Update README.md\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* Update README.md\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* minor\n\n* update readme\n\n* update\n\n* update\n\n* update\n\n* update\n\n* fix url\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* move ops\n\n* toc\n\n* typo\n\n* typo\n\n* add svgs\n\n* update figure\n\n* fix links\n\n* fix size\n\n* fix size\n\n* typo\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* fix broken links\n\n* fix links\n\n* Heterogeneous Graph Sampler Tutorial (#117)\n\n* initial commit\n\n* address comments\n\n* remove todo\n\n* typo\n\n* Conversion between heterogenous and homogeneous graph objects (#115)\n\n* temp checkpoint (wip, will remove)\n\n* (wip) typed graph conversion\n\n* (wip) typed graph conversion\n\n* (wip) typed graph conversion\n\n* update\n\n* typo\n\n* delete examples\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* fix test\n\n* update doc\n\n* deprecate NeighborSampler (#119)\n\n* Move `torch_geometric.data.DataLoader` to `torch_geometric.loader.DataLoader` (#120)\n\n* move dataloader\n\n* rename\n\n* typos\n\n* typos\n\n* fix __cat_dim__\n\n* updategp\n\n* Deprecate `train_test_split_edges` + Modifications to `RandomLinkSplit` (#121)\n\n* deprecate train_test_split_edges\n\n* to device transform\n\n* fix example\n\n* add split_labels argument\n\n* fix autoencoder example\n\n* typos\n\n* add docstring\n\n* ARGVA\n\n* seal\n\n* adress comments\n\n* Create example to load `*.csv` and transfer to `HeteroData` (#76)\n\n* create example to load csv file and transfer to heter-data\n\n* add ipython notebook version load csv with documentation\n\n* address comment\n\n* first version of csv loading doc\n\n* first version of csv loading doc\n\n* suggestion docs/source/notes/loading_csv.rst\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* suggestion docs/source/notes/loading_csv.rst\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* suggestion docs/source/notes/loading_csv.rst\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* suggestion docs/source/notes/loading_csv.rst\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* suggestions csv tutorial\n\n* example script load csv + extract fix\n\n* fixed edge index stacking dimension in example and jupyter nb\n\n* linting\n\n* linting2\n\n* rename\n\n* update\n\n* update\n\n* update\n\n* typo\n\n* typo\n\n* update\n\n* rename\n\n* update tutorial\n\n* typo\n\n* address comments\n\nCo-authored-by: Dong Wang <dongwang@yannis-air.lan>\nCo-authored-by: Jan Eric Lenssen <janeric.lenssen@tu-dortmund.de>\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* typo\n\n* fix\n\n* typo\n\n* update\n\n* fix\n\n* fix\n\nCo-authored-by: benedekrozemberczki <benedek.rozemberczki@gmail.com>\nCo-authored-by: Rex Ying <rexying@stanford.edu>\nCo-authored-by: Dongkwan Kim <todoaskit@gmail.com>\nCo-authored-by: Markus <markus.zopf@outlook.com>\nCo-authored-by: Jimmie <jimmiebtlr@gmail.com>\nCo-authored-by: Jinu Sunil <jinu.sunil@gmail.com>\nCo-authored-by: Moritz R Schfer <moritz.schaefer@protonmail.com>\nCo-authored-by: Jiaxuan <youjiaxuan@gmail.com>\nCo-authored-by: PabloAMC <pmorenocf@alumnos.unex.es>\nCo-authored-by: Moritz Blum <31183934+moritzblum@users.noreply.github.com>\nCo-authored-by: fbragman <fbragman@users.noreply.github.com>\nCo-authored-by: Christopher Lee <2824685+CCInc@users.noreply.github.com>\nCo-authored-by: Tim Daubenschtz <tim@daubenschuetz.de>\nCo-authored-by: Yue Zhao <yzhao062@gmail.com>\nCo-authored-by: Dong Wang <dongw89@gmail.com>\nCo-authored-by: Dong Wang <dongwang@yannis-air.lan>\nCo-authored-by: Jan Eric Lenssen <janeric.lenssen@tu-dortmund.de>\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_metapath2vec():",
            "z = model('author', torch.arange(2))",
            "assert z.size() == (2, 16)",
            "",
            "-    pos_rw, neg_rw = model.sample(torch.arange(3))",
            "+    pos_rw, neg_rw = model._sample(torch.arange(3))",
            "",
            "loss = model.loss(pos_rw, neg_rw)",
            "assert 0 <= loss.item()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=sample), value='_sample')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 3821,
        "neg_line": [
            "-pos_rw, neg_rw = model.sample(torch.arange(3))"
        ],
        "pos_line": [
            "+pos_rw, neg_rw = model._sample(torch.arange(3))"
        ],
        "core_change": "-pos_rw, neg_rw = model.sample(torch.arange(3)) +pos_rw, neg_rw = model._sample(torch.arange(3))",
        "core_API": "arange"
    },
    {
        "commit_hash": "84fd7e977ac5ad91ff7aa5ac8f959c52373eded0",
        "index": "38922ed..bbe3b3c 100644",
        "commit_message": "Fix attention shape\n\n",
        "file": "seq2seq.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class AttentionDecoder(RNNDecoder):",
            "logits=self.vocab_size,",
            "predicted_ids=tf.TensorShape([]),",
            "cell_output=self.cell.output_size,",
            "-        attention_scores=tf.concat(",
            "-            [[0], tf.shape(self.attention_values)[1:-1]], 0),",
            "+        attention_scores=tf.shape(self.attention_values)[1:-1],",
            "attention_context=self.attention_values.get_shape()[-1])",
            "",
            "@property"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=expression_list), node=ASTNode(type=subscript), position=0)",
            "Move(target_node=ASTNode(type=expression_list), node=ASTNode(type=,, text=,), position=1)",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=concat))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=integer, text=0))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=list))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=list))",
            "Delete(target_node=ASTNode(type=integer, text=0))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 20,
        "number": 3825,
        "neg_line": [
            "-attention_scores=tf.concat(",
            "-[[0], tf.shape(self.attention_values)[1:-1]], 0),"
        ],
        "pos_line": [
            "+attention_scores=tf.shape(self.attention_values)[1:-1],"
        ],
        "core_change": "-attention_scores=tf.concat( -[[0], tf.shape(self.attention_values)[1:-1]], 0), +attention_scores=tf.shape(self.attention_values)[1:-1],",
        "core_API": "TensorShape"
    },
    {
        "commit_hash": "101ed2cb5cc15344f41dc2f062b5284d8327dece",
        "index": "6979f7f1..d2be398c 100644",
        "commit_message": "fix coalesce on CUDA\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def coalesce(",
            "return edge_index",
            "",
            "dim_size = edge_index.size(1)",
            "-    idx = torch.arange(0, nnz).sub_(mask.logical_not_().cumsum(dim=0))",
            "+    idx = torch.arange(0, nnz, device=edge_index.device)",
            "+    idx.sub_(mask.logical_not_().cumsum(dim=0))",
            "",
            "if isinstance(edge_attr, Tensor):",
            "edge_attr = scatter(edge_attr, idx, 0, None, dim_size, reduce)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('ERROR', None), position=1, insert_id=994031)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'idx'), position=0, insert_id=994032)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=994033)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=994034)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=994035)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=994036)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=994037)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'edge_index'), position=0, insert_id=994038)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=994039)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=994040)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 3828,
        "neg_line": [
            "-idx = torch.arange(0, nnz).sub_(mask.logical_not_().cumsum(dim=0))"
        ],
        "pos_line": [
            "+idx = torch.arange(0, nnz, device=edge_index.device)",
            "+idx.sub_(mask.logical_not_().cumsum(dim=0))"
        ],
        "core_change": "-idx = torch.arange(0, nnz).sub_(mask.logical_not_().cumsum(dim=0)) +idx = torch.arange(0, nnz, device=edge_index.device) +idx.sub_(mask.logical_not_().cumsum(dim=0))",
        "core_API": "size"
    },
    {
        "commit_hash": "fec3112cbacf19cd211109b1bc0b33a502595aca",
        "index": "05f29650..b2b0eeda 100644",
        "commit_message": "Update to PyTorch 1.9.0 (#2887)\n\n* Update to PyTorch 1.9.0\n\n* Update torch.cholesky, torch.symeig\n\n* Fix torch.tensordot, torch.qr, funsor dependency\n\n* Fix torch import, AutoGuideList usage\n\n* Ignore bug in PyTorch jit\n\n* Ignore tracer warnings\n\n* Replace torch.solve with torch.linalg.solve\n\n* Xfail vectorized markov funsor tests\n\n* Update funsor version\n\n* Switch MNIST mirrors\n\n* Pin pillow version\n\n* Bump torchvision version\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SparseGPRegression(GPModel):",
            "W_Dinv = W / D",
            "K = W_Dinv.matmul(W.t()).contiguous()",
            "K.view(-1)[::M + 1] += 1  # add identity matrix to K",
            "-        L = K.cholesky()",
            "+        L = torch.linalg.cholesky(K)",
            "",
            "# get y_residual and convert it into 2D tensor for packing",
            "y_residual = self.y - self.mean_function(self.X)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=676826)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=676827)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=K), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=676828)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'linalg'), position=2, insert_id=676829)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 3830,
        "neg_line": [
            "-L = K.cholesky()"
        ],
        "pos_line": [
            "+L = torch.linalg.cholesky(K)"
        ],
        "core_change": "-L = K.cholesky() +L = torch.linalg.cholesky(K)",
        "core_API": "matmul"
    },
    {
        "commit_hash": "e8a0d8592fb77c9abe93456f7a1b040da52da41a",
        "index": "eecf73f..c0fe58e 100644",
        "commit_message": " Fix  #109, don't call _build() if you want save into pb.\n\n",
        "file": "TensorFlowTTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFFastSpeech2(TFFastSpeech):",
            "duration_outputs = self.duration_predictor(",
            "[last_encoder_hidden_states, speaker_ids, attention_mask]",
            ")  # [batch_size, length]",
            "-        duration_outputs = tf.math.exp(duration_outputs) - 1.0",
            "+        duration_outputs = tf.nn.relu(tf.math.exp(duration_outputs) - 1.0)",
            "duration_outputs = tf.cast(",
            "tf.math.round(duration_outputs * speed_ratios), tf.int32",
            ")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('call', None), position=2, insert_id=2215716)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2215717)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2215718)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2215719)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2215720)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'relu'), position=2, insert_id=2215721)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2215722)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=binary_operator), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2215723)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2215724)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2215725)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'nn'), position=2, insert_id=2215726)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 12,
        "number": 3831,
        "neg_line": [
            "-duration_outputs = tf.math.exp(duration_outputs) - 1.0"
        ],
        "pos_line": [
            "+duration_outputs = tf.nn.relu(tf.math.exp(duration_outputs) - 1.0)"
        ],
        "core_change": "-duration_outputs = tf.math.exp(duration_outputs) - 1.0 +duration_outputs = tf.nn.relu(tf.math.exp(duration_outputs) - 1.0)",
        "core_API": "duration_predictor"
    },
    {
        "commit_hash": "914fa54fa027fdd037dbe90382e48071c89a2e6d",
        "index": "5f362531..ee6e7a9a 100644",
        "commit_message": "Fix save spec bug with the call function kwargs.\n\nPiperOrigin-RevId: 422951302\n\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Layer(tf.Module, version_utils.LayerVersionSelector):",
            "flat_specs = [tf_utils.get_tensor_spec(x) for x in flat_kwarg]",
            "if any(s is None for s in flat_specs):",
            "continue",
            "-      kwargs[key] = args_spec.append(",
            "-          tf.nest.pack_sequence_as(kwarg, flat_specs))",
            "+      kwargs_spec[key] = tf.nest.pack_sequence_as(kwarg, flat_specs)",
            "",
            "self._saved_model_inputs_spec = inputs_spec",
            "self._saved_model_arg_spec = ([inputs_spec] + args_spec, kwargs_spec)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=call), position=2)",
            "Update(target_node=ASTNode(type=identifier, text=kwargs), value='kwargs_spec')",
            "Delete(target_node=ASTNode(type=identifier, text=args_spec))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=append))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 10,
        "number": 3832,
        "neg_line": [
            "-kwargs[key] = args_spec.append(",
            "-tf.nest.pack_sequence_as(kwarg, flat_specs))"
        ],
        "pos_line": [
            "+kwargs_spec[key] = tf.nest.pack_sequence_as(kwarg, flat_specs)"
        ],
        "core_change": "-kwargs[key] = args_spec.append( -tf.nest.pack_sequence_as(kwarg, flat_specs)) +kwargs_spec[key] = tf.nest.pack_sequence_as(kwarg, flat_specs)",
        "core_API": "get_tensor_spec"
    },
    {
        "commit_hash": "b0b7fba19cef0ad4cd5ef55c690ddb0430bd59f3",
        "index": "9b5cca2..56c8dd6 100644",
        "commit_message": "fix dropout configuration\n\n",
        "file": "text-classification-cnn-rnn.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TextCNN(object):",
            "# dropoutrelu",
            "fc = tf.layers.dense(gmp, self.config.hidden_dim, name='fc1')",
            "fc = tf.contrib.layers.dropout(fc,",
            "-                self.config.dropout_keep_prob)",
            "+                self.keep_prob)",
            "fc = tf.nn.relu(fc)",
            "",
            "# "
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=config), value='keep_prob')",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=dropout_keep_prob))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 3,
        "number": 3833,
        "neg_line": [
            "-self.config.dropout_keep_prob)"
        ],
        "pos_line": [
            "+self.keep_prob)"
        ],
        "core_change": "-self.config.dropout_keep_prob) +self.keep_prob)",
        "core_API": "dense"
    },
    {
        "commit_hash": "9b2705ab6ca32799a28ea007c8760c24707dc765",
        "index": "e68d4a34..098f137b 100644",
        "commit_message": "fix D413\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tensorflow",
        "change": [
            "def pixel_wise_softmax(x, name='pixel_wise_softmax'):",
            "References",
            "- `tf.reverse <https://www.tensorflow.org/versions/master/api_docs/python/array_ops.html#reverse>`_",
            "+",
            "\"\"\"",
            "with tf.name_scope(name):",
            "return tf.nn.softmax(x)"
        ],
        "hunk_index": 4,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=string, text=`tf.reverse <https://www.tensorflow.org/versions/master/api_docs/python/array_ops.html#reverse>`))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=identifier, text=_))",
            "Delete(target_node=ASTNode(type=ERROR))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 3837,
        "neg_line": [
            "-`tf.reverse <https://www.tensorflow.org/versions/master/api_docs/python/array_ops.html#reverse>`_"
        ],
        "pos_line": [
            "+"
        ],
        "core_change": "-`tf.reverse <https://www.tensorflow.org/versions/master/api_docs/python/array_ops.html#reverse>`_ +",
        "core_API": "name_scope"
    },
    {
        "commit_hash": "9b2b32ae80085a5945c7d5f74e93ead457c9e754",
        "index": "ab1d3bee..79e88222 100644",
        "commit_message": "fix lint error\n\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class InverseDepthSmoothnessLoss(nn.Module):",
            "assert len(img.shape) == 4, img.shape",
            "return img[:, :, :-1, :] - img[:, :, 1:, :]",
            "",
            "-    def forward(self, idepth: torch.Tensor, image: torch.Tensor) -> torch.Tensor:",
            "+    def forward(",
            "+            self,",
            "+            idepth: torch.Tensor,",
            "+            image: torch.Tensor) -> torch.Tensor:",
            "if not torch.is_tensor(idepth):",
            "raise TypeError(\"Input idepth type is not a torch.Tensor. Got {}\"",
            ".format(type(idepth)))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 4,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 3842,
        "neg_line": [
            "-def forward(self, idepth: torch.Tensor, image: torch.Tensor) -> torch.Tensor:"
        ],
        "pos_line": [
            "+def forward(",
            "+self,",
            "+idepth: torch.Tensor,",
            "+image: torch.Tensor) -> torch.Tensor:"
        ],
        "core_change": "-def forward(self, idepth: torch.Tensor, image: torch.Tensor) -> torch.Tensor: +def forward( +self, +idepth: torch.Tensor, +image: torch.Tensor) -> torch.Tensor:",
        "core_API": "is_tensor"
    },
    {
        "commit_hash": "de4159a318f9a4f2f474733306c00ac326e643a7",
        "index": "c85989913..242d8d398 100644",
        "commit_message": "More TF int dtype fixes (#20384)\n\n* Add a test to ensure int dummy inputs are int64\n\n* Move the test into the existing int64 test and update a lot of existing dummies\n\n* Fix remaining dummies\n\n* Fix remaining dummies\n\n* Test for int64 serving sigs as well\n\n* Update core tests to use tf.int64\n\n* Add better messages to the assertions\n\n* Update all serving sigs to int64\n\n* More sneaky hiding tf.int32s\n\n* Add an optional int32 signature in save_pretrained\n\n* make fixup\n\n* Add Amy's suggestions\n\n* Switch all serving sigs back to tf.int32\n\n* Switch all dummies to tf.int32\n\n* Adjust tests to check for tf.int32 instead of tf.int64\n\n* Fix base dummy_inputs dtype\n\n* Start casting to tf.int32 in input_processing\n\n* Change dtype for unpack_inputs test\n\n* Add proper tf.int32 test\n\n* Make the alternate serving signature int64\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFLayoutLMv3PreTrainedModel(TFPreTrainedModel):",
            "@tf.function(",
            "input_signature=[",
            "{",
            "-                \"input_ids\": tf.TensorSpec((None, None), tf.int64, name=\"input_ids\"),",
            "-                \"bbox\": tf.TensorSpec((None, None, 4), tf.int64, name=\"bbox\"),",
            "+                \"input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"input_ids\"),",
            "+                \"bbox\": tf.TensorSpec((None, None, 4), tf.int32, name=\"bbox\"),",
            "\"pixel_values\": tf.TensorSpec((None, None, None, None), tf.float32, name=\"pixel_values\"),",
            "-                \"attention_mask\": tf.TensorSpec((None, None), tf.int64, name=\"attention_mask\"),",
            "+                \"attention_mask\": tf.TensorSpec((None, None), tf.int32, name=\"attention_mask\"),",
            "}",
            "]",
            ")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2358036)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2358037)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2358038)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'TensorSpec'), position=2, insert_id=2358039)",
            "Insert(target_node=ASTNode(type=argument_list), node=('tuple', None), position=1, insert_id=2358040)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=tuple), position=1)",
            "Insert(target_node=IN(type=tuple), node=('(', '('), position=0, insert_id=2358041)",
            "Insert(target_node=IN(type=tuple), node=('none', 'None'), position=1, insert_id=2358042)",
            "Insert(target_node=IN(type=tuple), node=(',', ','), position=2, insert_id=2358043)",
            "Insert(target_node=IN(type=tuple), node=('none', 'None'), position=3, insert_id=2358044)",
            "Insert(target_node=IN(type=tuple), node=(')', ')'), position=4, insert_id=2358045)",
            "Update(target_node=ASTNode(type=identifier, text=int64), value='int32')",
            "Update(target_node=ASTNode(type=identifier, text=int64), value='int32')",
            "Update(target_node=ASTNode(type=identifier, text=int64), value='int32')",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=TensorSpec))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=none, text=None))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=none, text=None))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=tuple))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 25,
        "number": 3843,
        "neg_line": [
            "-\"input_ids\": tf.TensorSpec((None, None), tf.int64, name=\"input_ids\"),",
            "-\"bbox\": tf.TensorSpec((None, None, 4), tf.int64, name=\"bbox\"),",
            "-\"attention_mask\": tf.TensorSpec((None, None), tf.int64, name=\"attention_mask\"),"
        ],
        "pos_line": [
            "+\"input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"input_ids\"),",
            "+\"bbox\": tf.TensorSpec((None, None, 4), tf.int32, name=\"bbox\"),",
            "+\"attention_mask\": tf.TensorSpec((None, None), tf.int32, name=\"attention_mask\"),"
        ],
        "core_change": "-\"input_ids\": tf.TensorSpec((None, None), tf.int64, name=\"input_ids\"), -\"bbox\": tf.TensorSpec((None, None, 4), tf.int64, name=\"bbox\"), +\"input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"input_ids\"), +\"bbox\": tf.TensorSpec((None, None, 4), tf.int32, name=\"bbox\"), -\"attention_mask\": tf.TensorSpec((None, None), tf.int64, name=\"attention_mask\"), +\"attention_mask\": tf.TensorSpec((None, None), tf.int32, name=\"attention_mask\"),",
        "core_API": "function"
    },
    {
        "commit_hash": "4728863f9de59e64c9302ea91cd9bb341a93e8f9",
        "index": "534b0fa..1fdc064 100644",
        "commit_message": "Fix inference on cpu device (#241)\n\n\n",
        "file": "MockingBird.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Tacotron(nn.Module):",
            "with open(path, \"a\") as f:",
            "print(msg, file=f)",
            "",
            "-    def load(self, path, optimizer=None):",
            "+    def load(self, path, device, optimizer=None):",
            "# Use device of model params as location for loaded state",
            "-        checkpoint = torch.load(str(path))",
            "+        checkpoint = torch.load(str(path), map_location=device)",
            "self.load_state_dict(checkpoint[\"model_state\"], strict=False)",
            "",
            "if \"optimizer_state\" in checkpoint and optimizer is not None:"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=parameters), node=('identifier', 'device'), position=5, insert_id=649638)",
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=6, insert_id=649639)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=649640)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=649641)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'map_location'), position=0, insert_id=649642)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=649643)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=2, insert_id=649644)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 7,
        "number": 3846,
        "neg_line": [
            "-def load(self, path, optimizer=None):",
            "-checkpoint = torch.load(str(path))"
        ],
        "pos_line": [
            "+def load(self, path, device, optimizer=None):",
            "+checkpoint = torch.load(str(path), map_location=device)"
        ],
        "core_change": "-def load(self, path, optimizer=None): +def load(self, path, device, optimizer=None): -checkpoint = torch.load(str(path)) +checkpoint = torch.load(str(path), map_location=device)",
        "core_API": "load"
    },
    {
        "commit_hash": "11069c87845ea9a14e6fe807094313a67f9946dc",
        "index": "caeb9e882..529d64f79 100644",
        "commit_message": "Fix ddp tests + .test() (#2512)\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* fix deprecation warnings\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* Update pytorch_lightning/trainer/trainer.py\n\nCo-authored-by: Jeremy Jordan <13970565+jeremyjordan@users.noreply.github.com>\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\nCo-authored-by: Jirka <jirka@pytorchlightning.ai>\nCo-authored-by: Jeremy Jordan <13970565+jeremyjordan@users.noreply.github.com>\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DummyDataset(Dataset):",
            "return 12",
            "",
            "def __getitem__(self, idx):",
            "-        return np.array([0.5, 1.0, 2.0])",
            "+        return torch.tensor([0.5, 1.0, 2.0])"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=np), value='torch')",
            "Update(target_node=ASTNode(type=identifier, text=array), value='tensor')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 3847,
        "neg_line": [
            "-return np.array([0.5, 1.0, 2.0])"
        ],
        "pos_line": [
            "+return torch.tensor([0.5, 1.0, 2.0])"
        ],
        "core_change": "-return np.array([0.5, 1.0, 2.0]) +return torch.tensor([0.5, 1.0, 2.0])",
        "core_API": "array"
    },
    {
        "commit_hash": "8d8339c56b2f408ebdff6ba121d03b8104ee2870",
        "index": "5a37c84450..2e33313225 100644",
        "commit_message": "Fixed failing test for elementwise floormod (#4187)\n\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "floor_divide.support_native_out = True",
            "",
            "",
            "def floormod(",
            "-    x: torch.Tensor, y: torch.Tensor, *, out: Optional[torch.Tensor] = None",
            "+    x: torch.Tensor, y: torch.Tensor, /, *, out: Optional[torch.Tensor] = None",
            ") -> torch.Tensor:",
            "+    x, y = ivy.promote_types_of_inputs(x, y)",
            "return x % y"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=function_definition), node=('block', None), position=6, insert_id=330184)",
            "Insert(target_node=ASTNode(type=parameters), node=('positional_separator', None), position=5, insert_id=330185)",
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=6, insert_id=330186)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=330187)",
            "Insert(target_node=IN(type=positional_separator), node=('/', '/'), position=0, insert_id=330188)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=330189)",
            "Insert(target_node=IN(type=assignment), node=('pattern_list', None), position=0, insert_id=330190)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=330191)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=330192)",
            "Insert(target_node=IN(type=pattern_list), node=('identifier', 'x'), position=0, insert_id=330193)",
            "Insert(target_node=IN(type=pattern_list), node=(',', ','), position=1, insert_id=330194)",
            "Insert(target_node=IN(type=pattern_list), node=('identifier', 'y'), position=2, insert_id=330195)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=330196)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=330197)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'ivy'), position=0, insert_id=330198)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=330199)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'promote_types_of_inputs'), position=2, insert_id=330200)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=330201)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'x'), position=1, insert_id=330202)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=330203)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'y'), position=3, insert_id=330204)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=330205)",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 23,
        "number": 3849,
        "neg_line": [
            "-x: torch.Tensor, y: torch.Tensor, *, out: Optional[torch.Tensor] = None"
        ],
        "pos_line": [
            "+x: torch.Tensor, y: torch.Tensor, /, *, out: Optional[torch.Tensor] = None",
            "+x, y = ivy.promote_types_of_inputs(x, y)"
        ],
        "core_change": "-x: torch.Tensor, y: torch.Tensor, *, out: Optional[torch.Tensor] = None +x: torch.Tensor, y: torch.Tensor, /, *, out: Optional[torch.Tensor] = None +x, y = ivy.promote_types_of_inputs(x, y)",
        "core_API": "promote_types_of_inputs"
    },
    {
        "commit_hash": "95b6d985ffb231d9a2047c039c95660913063b9d",
        "index": "f82e91fc..bc7631e9 100644",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestAutoRegressiveSeqDecoder(AllenNlpTestCase):",
            ").eval()",
            "",
            "encoded_state = torch.randn(batch_size, time_steps, decoder_inout_dim)",
            "-        source_mask = torch.ones(batch_size, time_steps).long()",
            "+        source_mask = torch.ones(batch_size, time_steps).bool()",
            "target_tokens = {\"tokens\": {\"tokens\": torch.ones(batch_size, time_steps).long()}}",
            "-        source_mask[0, 1:] = 0",
            "+        source_mask[0, 1:] = False",
            "encoder_out = {\"source_mask\": source_mask, \"encoder_outputs\": encoded_state}",
            "",
            "auto_regressive_seq_decoder.forward(encoder_out, target_tokens)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('false', 'False'), position=2, insert_id=19768)",
            "Update(target_node=ASTNode(type=identifier, text=long), value='bool')",
            "Delete(target_node=ASTNode(type=integer, text=0))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 3,
        "number": 3856,
        "neg_line": [
            "-source_mask = torch.ones(batch_size, time_steps).long()",
            "-source_mask[0, 1:] = 0"
        ],
        "pos_line": [
            "+source_mask = torch.ones(batch_size, time_steps).bool()",
            "+source_mask[0, 1:] = False"
        ],
        "core_change": "-source_mask = torch.ones(batch_size, time_steps).long() +source_mask = torch.ones(batch_size, time_steps).bool() -source_mask[0, 1:] = 0 +source_mask[0, 1:] = False",
        "core_API": "randn"
    },
    {
        "commit_hash": "5751edd199f20c17a366c0eed4932539b3697c18",
        "index": "64096d0..40e90ff 100644",
        "commit_message": "Fix distributed/rpc/rnn unused import, typos, formatting (#875)\n\n\n",
        "file": "examples.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class RNNModel(nn.Module):",
            "# pass input to the remote embedding table and fetch emb tensor back",
            "emb = _remote_method(EmbeddingTable.forward, self.emb_table_rref, input)",
            "output, hidden = self.rnn(emb, hidden)",
            "-        # pass output to the rremote decoder and get the decoded output back",
            "+        # pass output to the remote decoder and get the decoded output back",
            "decoded = _remote_method(Decoder.forward, self.decoder_rref, output)",
            "return decoded, hidden"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 3861,
        "neg_line": [
            "-# pass output to the rremote decoder and get the decoded output back"
        ],
        "pos_line": [
            "+# pass output to the remote decoder and get the decoded output back"
        ],
        "core_change": "-# pass output to the rremote decoder and get the decoded output back +# pass output to the remote decoder and get the decoded output back",
        "core_API": "rnn"
    },
    {
        "commit_hash": "57d34592ec9cbeb5416a5528c40f202b34487c4f",
        "index": "0a5054c7..2136fff8 100644",
        "commit_message": "Add loss evaluator (#678)\n\n* Fix license in setup.py\n\n* Add code for loss evaluator\n\n* Configs support loss evaluator\n\n* Fix a little bug\n\n* Fix flake8\n\n* return revised bbox to reg\n\n* return revised bbox to reg\n\n* revision according to comments\n\n* fix flake8\n\n",
        "file": "mmdetection.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class FCNMaskHead(nn.Module):",
            "def loss(self, mask_pred, mask_targets, labels):",
            "loss = dict()",
            "if self.class_agnostic:",
            "-            loss_mask = mask_cross_entropy(mask_pred, mask_targets,",
            "-                                           torch.zeros_like(labels))",
            "+            loss_mask = self.loss_mask(mask_pred, mask_targets,",
            "+                                       torch.zeros_like(labels))",
            "else:",
            "-            loss_mask = mask_cross_entropy(mask_pred, mask_targets, labels)",
            "+            loss_mask = self.loss_mask(mask_pred, mask_targets, labels)",
            "loss['loss_mask'] = loss_mask",
            "return loss"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=644796)",
            "Update(target_node=ASTNode(type=identifier, text=mask_cross_entropy), value='self')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=mask_cross_entropy), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=644797)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'loss_mask'), position=2, insert_id=644798)",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=644799)",
            "Update(target_node=ASTNode(type=identifier, text=mask_cross_entropy), value='self')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=mask_cross_entropy), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=644800)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'loss_mask'), position=2, insert_id=644801)"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 10,
        "number": 3863,
        "neg_line": [
            "-loss_mask = mask_cross_entropy(mask_pred, mask_targets,",
            "-torch.zeros_like(labels))",
            "-loss_mask = mask_cross_entropy(mask_pred, mask_targets, labels)"
        ],
        "pos_line": [
            "+loss_mask = self.loss_mask(mask_pred, mask_targets,",
            "+torch.zeros_like(labels))",
            "+loss_mask = self.loss_mask(mask_pred, mask_targets, labels)"
        ],
        "core_change": "-loss_mask = mask_cross_entropy(mask_pred, mask_targets, -torch.zeros_like(labels)) +loss_mask = self.loss_mask(mask_pred, mask_targets, +torch.zeros_like(labels)) -loss_mask = mask_cross_entropy(mask_pred, mask_targets, labels) +loss_mask = self.loss_mask(mask_pred, mask_targets, labels)",
        "core_API": "zeros_like"
    },
    {
        "commit_hash": "8b704eb4191624ff46446021baa9d41e41f9baf4",
        "index": "d1669b73b..97c91b848 100644",
        "commit_message": "Small fix for Cuda Torch DQN. (#10177)\n\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def build_q_losses(policy, model, _, train_batch):",
            "q_tp1_best_one_hot_selection = F.one_hot(q_tp1_best_using_online_net,",
            "policy.action_space.n)",
            "q_tp1_best = torch.sum(",
            "-            torch.where(q_tp1 > -float(\"inf\"), q_tp1, torch.tensor(0.0)) *",
            "+            torch.where(q_tp1 > -float(\"inf\"), q_tp1,",
            "+                        torch.tensor(0.0, device=policy.device)) *",
            "q_tp1_best_one_hot_selection, 1)",
            "q_probs_tp1_best = torch.sum(",
            "q_probs_tp1 * torch.unsqueeze(q_tp1_best_one_hot_selection, -1), 1)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1121683)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1121684)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=1121685)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1121686)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=1121687)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'policy'), position=0, insert_id=1121688)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1121689)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=1121690)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 3864,
        "neg_line": [
            "-torch.where(q_tp1 > -float(\"inf\"), q_tp1, torch.tensor(0.0)) *"
        ],
        "pos_line": [
            "+torch.where(q_tp1 > -float(\"inf\"), q_tp1,",
            "+torch.tensor(0.0, device=policy.device)) *"
        ],
        "core_change": "-torch.where(q_tp1 > -float(\"inf\"), q_tp1, torch.tensor(0.0)) * +torch.where(q_tp1 > -float(\"inf\"), q_tp1, +torch.tensor(0.0, device=policy.device)) *",
        "core_API": "one_hot"
    },
    {
        "commit_hash": "82c4e7763ae52f14552c46f7090c6857284aaefd",
        "index": "265c9db7..dfd61d1b 100644",
        "commit_message": "[refactor] Training of densehead (#6315)\n\n* Refactor one-stage get_bboxes logic (#5317)\n\n* revert batch to single\n\n* update anchor_head\n\n* replace preds with bboxes\n\n* add point_bbox_coder\n\n* FCOS add get_selected_priori\n\n* unified anchor-free and anchor-based get_bbox_single\n\n* update code\n\n* update reppoints and sabl\n\n* add sparse priors\n\n* add mlvlpointsgenerator\n\n* revert __init__ of core\n\n* refactor reppoints\n\n* delete label channal\n\n* add docstr\n\n* fix typo\n\n* fix args\n\n* fix typo\n\n* fix doc\n\n* fix stride_h\n\n* add offset\n\n* Unified bbox coder\n\n* add offset\n\n* remove point_bbox_coder.py\n\n* fix docstr\n\n* new interface of single_proir\n\n* fix device\n\n* add unitest\n\n* add cuda unitest\n\n* add more cuda unintest\n\n* fix reppoints\n\n* fix device\n\n* update all prior\n\n* update vfnet\n\n* add unintest for ssd and yolo and rename prior_idxs\n\n* add docstr for MlvlPointGenerator\n\n* update reppoints and rpnhead\n\n* add space\n\n* add num_base_priors\n\n* update some model\n\n* update docstr\n\n* fixAugFPN test and lint.\n\n* Fix autoassign\n\n* add docs\n\n* Unified fcos decoding\n\n* update docstr\n\n* fix train error\n\n* Fix Vfnet\n\n* Fix some\n\n* update centernet\n\n* revert\n\n* add warnings\n\n* fix unittest error\n\n* delete duplicated\n\n* fix comment\n\n* fix docs\n\n* fix type\n\nCo-authored-by: zhangshilong <2392587229zsl@gmail.com>\n\n* support onnx export for fcos\n\n* support onnx export for fcos fsaf retina and ssd\n\n* resolve comments\n\n* resolve comments\n\n* add with nms\n\n* support cornernet\n\n* resolve comments\n\n* add default with nms\n\n* fix trt arrange should be int\n\n* refactor anchor head anchor free head\n\n* add dtype to single_level_grid_priors\n\n* atss fcos autoassign\n\n* fovea\n\n* fsaf free anchor\n\n* suport more\n\n* suport more\n\n* support all\n\n* resolve conversation\n\n* fix point generator\n\n* fix device\n\n* change to distancecoder\n\n* resolve conversation\n\n* fix grid prior\n\n* fix typos in autoassgin\n\n* fix typos\n\n* fix doc\n\nCo-authored-by: Haian Huang() <1286304229@qq.com>\n",
        "file": "mmdetection.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class RPNHead(AnchorHead):",
            "self.rpn_conv = nn.Conv2d(",
            "self.in_channels, self.feat_channels, 3, padding=1)",
            "self.rpn_cls = nn.Conv2d(self.feat_channels,",
            "-                                 self.num_anchors * self.cls_out_channels, 1)",
            "-        self.rpn_reg = nn.Conv2d(self.feat_channels, self.num_anchors * 4, 1)",
            "+                                 self.num_base_priors * self.cls_out_channels,",
            "+                                 1)",
            "+        self.rpn_reg = nn.Conv2d(self.feat_channels, self.num_base_priors * 4,",
            "+                                 1)",
            "",
            "def forward_single(self, x):",
            "\"\"\"Forward feature map of a single scale level.\"\"\""
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1423922)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'nn'), position=0, insert_id=1423923)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1423924)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Conv2d'), position=2, insert_id=1423925)",
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=1, insert_id=1423926)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=attribute), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=1423927)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1423928)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'feat_channels'), position=2, insert_id=1423929)",
            "Update(target_node=ASTNode(type=identifier, text=num_anchors), value='num_base_priors')",
            "Update(target_node=ASTNode(type=identifier, text=num_anchors), value='num_base_priors')",
            "Delete(target_node=ASTNode(type=identifier, text=nn))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=Conv2d))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=feat_channels))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 4,
        "minus_line": 2,
        "AST_diff_line": 20,
        "number": 3865,
        "neg_line": [
            "-self.num_anchors * self.cls_out_channels, 1)",
            "-self.rpn_reg = nn.Conv2d(self.feat_channels, self.num_anchors * 4, 1)"
        ],
        "pos_line": [
            "+self.num_base_priors * self.cls_out_channels,",
            "+1)",
            "+self.rpn_reg = nn.Conv2d(self.feat_channels, self.num_base_priors * 4,",
            "+1)"
        ],
        "core_change": "-self.num_anchors * self.cls_out_channels, 1) -self.rpn_reg = nn.Conv2d(self.feat_channels, self.num_anchors * 4, 1) +self.num_base_priors * self.cls_out_channels, +1) +self.rpn_reg = nn.Conv2d(self.feat_channels, self.num_base_priors * 4, +1)",
        "core_API": "Conv2d"
    },
    {
        "commit_hash": "db3e02b789e788523d3ad98449ca5b29fd894439",
        "index": "d89c65054e..c61a60c01d 100644",
        "commit_message": "lintfixbot: Auto-commit fixed lint errors in codebase\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "msort_support_native_out = True",
            "",
            "# lexsort",
            "def lexsort(",
            "-    keys: torch.Tensor,",
            "-    /,",
            "-    *,",
            "-    axis: int = -1,",
            "-    out: Optional[torch.Tensor] = None",
            "+    keys: torch.Tensor, /, *, axis: int = -1, out: Optional[torch.Tensor] = None",
            ") -> torch.Tensor:",
            "shape = keys.size()",
            "if len(shape) == 1:",
            "_, result = torch.sort(keys, dim=axis, stable=True)",
            "return result",
            "if shape[0] == 0:",
            "-        raise TypeError('need sequence of keys with len > 0 in lexsort')",
            "+        raise TypeError(\"need sequence of keys with len > 0 in lexsort\")",
            "if len(shape) == 2 and shape[1] == 1:",
            "return torch.tensor([0])",
            "_, result = torch.sort(keys[0], dim=axis, stable=True)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text='need sequence of keys with len > 0 in lexsort'), value='\"need sequence of keys with len > 0 in lexsort\"')"
        ],
        "plus_line": 2,
        "minus_line": 6,
        "AST_diff_line": 1,
        "number": 3867,
        "neg_line": [
            "-keys: torch.Tensor,",
            "-/,",
            "-*,",
            "-axis: int = -1,",
            "-out: Optional[torch.Tensor] = None",
            "-raise TypeError('need sequence of keys with len > 0 in lexsort')"
        ],
        "pos_line": [
            "+keys: torch.Tensor, /, *, axis: int = -1, out: Optional[torch.Tensor] = None",
            "+raise TypeError(\"need sequence of keys with len > 0 in lexsort\")"
        ],
        "core_change": "-keys: torch.Tensor, -/, -*, -axis: int = -1, -out: Optional[torch.Tensor] = None +keys: torch.Tensor, /, *, axis: int = -1, out: Optional[torch.Tensor] = None -raise TypeError('need sequence of keys with len > 0 in lexsort') +raise TypeError(\"need sequence of keys with len > 0 in lexsort\")",
        "core_API": "size"
    },
    {
        "commit_hash": "4292fe0532b9b5f68af8a48d8b844585a3b59f86",
        "index": "0f61b94c5..a21242c3b 100644",
        "commit_message": "Fix metric state reset (#5273)\n\n* Fix metric state reset\n\n* Fix test\n\n* Improve formatting\n\nCo-authored-by: Ananya Harsh Jha <ananya@pytorchlightning.ai>\n(cherry picked from commit 4913cbb987a0516f8b33c016134b19c0588d107a)\n\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Metric(nn.Module, ABC):",
            "\"\"\"",
            "for attr, default in self._defaults.items():",
            "current_val = getattr(self, attr)",
            "-            if isinstance(current_val, torch.Tensor):",
            "+            if isinstance(default, torch.Tensor):",
            "setattr(self, attr, deepcopy(default).to(current_val.device))",
            "else:",
            "setattr(self, attr, deepcopy(default))"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=current_val), value='default')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 3868,
        "neg_line": [
            "-if isinstance(current_val, torch.Tensor):"
        ],
        "pos_line": [
            "+if isinstance(default, torch.Tensor):"
        ],
        "core_change": "-if isinstance(current_val, torch.Tensor): +if isinstance(default, torch.Tensor):",
        "core_API": "items"
    },
    {
        "commit_hash": "854976324f47a26ca803166084cfc94c07f9cd54",
        "index": "579905a..1f6a694 100644",
        "commit_message": "Bug fix\n\n",
        "file": "apex.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DistributedFusedAdam(torch.optim.Optimizer):",
            "if torch.distributed.get_rank() in ranks:",
            "self._rs_pg.append(grp)",
            "if self._compute_L2_grad_norm and torch.distributed.get_rank() in ranks:",
            "-                #self._l2_grad_norm_pg = torch.distributed.new_group(ranks=ranks)",
            "-                self._l2_grad_norm_pg = self._rs_pg[-1]",
            "+                self._l2_grad_norm_pg = torch.distributed.new_group(ranks=ranks)",
            "self._rs_st = [torch.cuda.Stream() for _ in range(self._num_rs_pg)]",
            "if self._num_ag_pg == 0:",
            "self._ag_pg = self._rs_pg"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('call', None), position=2, insert_id=52372)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=52373)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=52374)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=52375)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'new_group'), position=2, insert_id=52376)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=52377)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=1, insert_id=52378)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=52379)",
            "Update(target_node=ASTNode(type=identifier, text=self), value='torch')",
            "Update(target_node=ASTNode(type=identifier, text=_rs_pg), value='distributed')",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'ranks'), position=0, insert_id=52380)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=52381)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'ranks'), position=2, insert_id=52382)",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=unary_operator, text=-1))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 18,
        "number": 3869,
        "neg_line": [
            "-#self._l2_grad_norm_pg = torch.distributed.new_group(ranks=ranks)",
            "-self._l2_grad_norm_pg = self._rs_pg[-1]"
        ],
        "pos_line": [
            "+self._l2_grad_norm_pg = torch.distributed.new_group(ranks=ranks)"
        ],
        "core_change": "-#self._l2_grad_norm_pg = torch.distributed.new_group(ranks=ranks) -self._l2_grad_norm_pg = self._rs_pg[-1] +self._l2_grad_norm_pg = torch.distributed.new_group(ranks=ranks)",
        "core_API": "get_rank"
    },
    {
        "commit_hash": "8410cf6ac0227b0972b1e012130dec307ace05d3",
        "index": "88d1a7a..062dac9 100644",
        "commit_message": "Fix tests in networks_test.py\n\ntf.zeros_initializer is now a function that needs to be called to get\nthe initializer.\n\n",
        "file": "learning-to-learn.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class KernelDeepLSTMTest(tf.test.TestCase):",
            "@parameterized.expand([",
            "[\"zeros\"],",
            "[{\"w\": \"zeros\", \"b\": \"zeros\", \"bad\": \"bad\"}],",
            "-      [{\"w\": tf.zeros_initializer, \"b\": np.array([0])}],",
            "-      [{\"linear\": {\"w\": tf.zeros_initializer, \"b\": \"zeros\"}}]",
            "+      [{\"w\": tf.zeros_initializer(), \"b\": np.array([0])}],",
            "+      [{\"linear\": {\"w\": tf.zeros_initializer(), \"b\": \"zeros\"}}]",
            "])",
            "def testResults(self, initializer):",
            "\"\"\"Tests zero updates when last layer is initialized to zero.\"\"\""
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=pair), node=('call', None), position=2, insert_id=2120314)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2120315)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2120316)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2120317)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2120318)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'zeros_initializer'), position=2, insert_id=2120319)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2120320)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=2120321)",
            "Insert(target_node=ASTNode(type=pair), node=('call', None), position=2, insert_id=2120322)",
            "Move(target_node=IN(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2120323)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2120324)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=2120325)",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=zeros_initializer))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 3870,
        "neg_line": [
            "-[{\"w\": tf.zeros_initializer, \"b\": np.array([0])}],",
            "-[{\"linear\": {\"w\": tf.zeros_initializer, \"b\": \"zeros\"}}]"
        ],
        "pos_line": [
            "+[{\"w\": tf.zeros_initializer(), \"b\": np.array([0])}],",
            "+[{\"linear\": {\"w\": tf.zeros_initializer(), \"b\": \"zeros\"}}]"
        ],
        "core_change": "-[{\"w\": tf.zeros_initializer, \"b\": np.array([0])}], -[{\"linear\": {\"w\": tf.zeros_initializer, \"b\": \"zeros\"}}] +[{\"w\": tf.zeros_initializer(), \"b\": np.array([0])}], +[{\"linear\": {\"w\": tf.zeros_initializer(), \"b\": \"zeros\"}}]",
        "core_API": "expand"
    },
    {
        "commit_hash": "2f4175d312816197a981222b13e803952adb235a",
        "index": "8ef39a9d..53a81dc0 100644",
        "commit_message": "fix\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def negative_sampling(edge_index, num_nodes=None, num_neg_samples=None,",
            "",
            "if force_undirected:",
            "# (-sqrt((2 * N + 1)^2 - 8 * perm) + 2 * N + 1) / 2",
            "-        row = torch.floor((-torch.sqrt((2 * num_nodes + 1)**2 - 8 * perm) +",
            "+        row = torch.floor((-torch.sqrt((2. * num_nodes + 1.)**2 - 8. * perm) +",
            "2 * num_nodes + 1) / 2)",
            "col = perm - row * (2 * num_nodes - row - 1) // 2",
            "neg_edge_index = torch.stack([row, col], dim=0).long()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=binary_operator), node=('float', '8.'), position=0, insert_id=1035869)",
            "Insert(target_node=ASTNode(type=binary_operator), node=('float', '1.'), position=2, insert_id=1035870)",
            "Insert(target_node=ASTNode(type=binary_operator), node=('float', '2.'), position=0, insert_id=1035871)",
            "Delete(target_node=ASTNode(type=integer, text=2))",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=integer, text=8))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 3874,
        "neg_line": [
            "-row = torch.floor((-torch.sqrt((2 * num_nodes + 1)**2 - 8 * perm) +"
        ],
        "pos_line": [
            "+row = torch.floor((-torch.sqrt((2. * num_nodes + 1.)**2 - 8. * perm) +"
        ],
        "core_change": "-row = torch.floor((-torch.sqrt((2 * num_nodes + 1)**2 - 8 * perm) + +row = torch.floor((-torch.sqrt((2. * num_nodes + 1.)**2 - 8. * perm) +",
        "core_API": "floor"
    },
    {
        "commit_hash": "5d7388d43bb59b005e9e6fbdbceb9ee691d54484",
        "index": "8ee727ebc..3e5852ac5 100644",
        "commit_message": "Fix when _stable_1d_sort to work when n >= N (#6177)\n\n* Fix when _stable_1d_sort to work when n >= N\n\n* Apply suggestions\n\nCo-authored-by: Carlos Mocholi <carlossmocholi@gmail.com>\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def _stable_1d_sort(x: torch, N: int = 2049):",
            "n = x.numel()",
            "if N - n > 0:",
            "x_max = x.max()",
            "-        x_pad = torch.cat([x, (x_max + 1) * torch.ones(2049 - n, dtype=x.dtype, device=x.device)], 0)",
            "-    x_sort = x_pad.sort()",
            "-    return x_sort.values[:n], x_sort.indices[:n]",
            "+        x = torch.cat([x, (x_max + 1) * torch.ones(N - n, dtype=x.dtype, device=x.device)], 0)",
            "+    x_sort = x.sort()",
            "+    i = min(N, n)",
            "+    return x_sort.values[:i], x_sort.indices[:i]"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=6, insert_id=544655)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=544656)",
            "Update(target_node=ASTNode(type=identifier, text=x_pad), value='x')",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'i'), position=0, insert_id=544657)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=544658)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=544659)",
            "Insert(target_node=IN(type=call), node=('identifier', 'min'), position=0, insert_id=544660)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=544661)",
            "Update(target_node=ASTNode(type=identifier, text=x_pad), value='x')",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=544662)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'N'), position=1, insert_id=544663)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=544664)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'n'), position=3, insert_id=544665)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=544666)",
            "Update(target_node=ASTNode(type=identifier, text=n), value='i')",
            "Update(target_node=ASTNode(type=identifier, text=n), value='i')",
            "Insert(target_node=ASTNode(type=binary_operator), node=('identifier', 'N'), position=0, insert_id=544667)",
            "Delete(target_node=ASTNode(type=integer, text=2049))"
        ],
        "plus_line": 4,
        "minus_line": 3,
        "AST_diff_line": 18,
        "number": 3877,
        "neg_line": [
            "-x_pad = torch.cat([x, (x_max + 1) * torch.ones(2049 - n, dtype=x.dtype, device=x.device)], 0)",
            "-x_sort = x_pad.sort()",
            "-return x_sort.values[:n], x_sort.indices[:n]"
        ],
        "pos_line": [
            "+x = torch.cat([x, (x_max + 1) * torch.ones(N - n, dtype=x.dtype, device=x.device)], 0)",
            "+x_sort = x.sort()",
            "+i = min(N, n)",
            "+return x_sort.values[:i], x_sort.indices[:i]"
        ],
        "core_change": "-x_pad = torch.cat([x, (x_max + 1) * torch.ones(2049 - n, dtype=x.dtype, device=x.device)], 0) -x_sort = x_pad.sort() -return x_sort.values[:n], x_sort.indices[:n] +x = torch.cat([x, (x_max + 1) * torch.ones(N - n, dtype=x.dtype, device=x.device)], 0) +x_sort = x.sort() +i = min(N, n) +return x_sort.values[:i], x_sort.indices[:i]",
        "core_API": "numel"
    },
    {
        "commit_hash": "c67d1a0259cbb3aef31952b4f37d4fee0e36f134",
        "index": "5bdf1f792..2f72469f1 100644",
        "commit_message": "Tf model outputs (#6247)\n\n* TF outputs and test on BERT\n\n* Albert to DistilBert\n\n* All remaining TF models except T5\n\n* Documentation\n\n* One file forgotten\n\n* TF outputs and test on BERT\n\n* Albert to DistilBert\n\n* All remaining TF models except T5\n\n* Documentation\n\n* One file forgotten\n\n* Add new models and fix issues\n\n* Quality improvements\n\n* Add T5\n\n* A bit of cleanup\n\n* Fix for slow tests\n\n* Style\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def _prepare_output_docstrings(output_type, config_class):",
            "",
            "# Add the return introduction",
            "full_output_type = f\"{output_type.__module__}.{output_type.__name__}\"",
            "-    intro = RETURN_INTRODUCTION.format(full_output_type=full_output_type, config_class=config_class)",
            "+    intro = TF_RETURN_INTRODUCTION if output_type.__name__.startswith(\"TF\") else PT_RETURN_INTRODUCTION",
            "+    intro = intro.format(full_output_type=full_output_type, config_class=config_class)",
            "return intro + docstrings"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=2, insert_id=2380562)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=2380563)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'intro'), position=0, insert_id=2380564)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=2380565)",
            "Insert(target_node=IN(type=assignment), node=('conditional_expression', None), position=2, insert_id=2380566)",
            "Insert(target_node=IN(type=conditional_expression), node=('identifier', 'TF_RETURN_INTRODUCTION'), position=0, insert_id=2380567)",
            "Insert(target_node=IN(type=conditional_expression), node=('if', 'if'), position=1, insert_id=2380568)",
            "Insert(target_node=IN(type=conditional_expression), node=('call', None), position=2, insert_id=2380569)",
            "Insert(target_node=IN(type=conditional_expression), node=('else', 'else'), position=3, insert_id=2380570)",
            "Insert(target_node=IN(type=conditional_expression), node=('identifier', 'PT_RETURN_INTRODUCTION'), position=4, insert_id=2380571)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2380572)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2380573)",
            "Update(target_node=ASTNode(type=identifier, text=RETURN_INTRODUCTION), value='intro')",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2380574)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2380575)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'startswith'), position=2, insert_id=2380576)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2380577)",
            "Insert(target_node=IN(type=argument_list), node=('string', '\"TF\"'), position=1, insert_id=2380578)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2380579)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'output_type'), position=0, insert_id=2380580)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2380581)",
            "Insert(target_node=IN(type=attribute), node=('identifier', '__name__'), position=2, insert_id=2380582)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 22,
        "number": 3880,
        "neg_line": [
            "-intro = RETURN_INTRODUCTION.format(full_output_type=full_output_type, config_class=config_class)"
        ],
        "pos_line": [
            "+intro = TF_RETURN_INTRODUCTION if output_type.__name__.startswith(\"TF\") else PT_RETURN_INTRODUCTION",
            "+intro = intro.format(full_output_type=full_output_type, config_class=config_class)"
        ],
        "core_change": "-intro = RETURN_INTRODUCTION.format(full_output_type=full_output_type, config_class=config_class) +intro = TF_RETURN_INTRODUCTION if output_type.__name__.startswith(\"TF\") else PT_RETURN_INTRODUCTION +intro = intro.format(full_output_type=full_output_type, config_class=config_class)",
        "core_API": "format"
    },
    {
        "commit_hash": "48263082f505689363de03b53ef0d6de5e352489",
        "index": "b9e905362..6d1c6c6c5 100644",
        "commit_message": "fix ci check(2): missed over-indent\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def train(args):",
            "rho=0.95, eps=args.eps,",
            "weight_decay=args.weight_decay)",
            "elif args.opt == 'adam':",
            "-            optimizer = torch.optim.Adam(params,",
            "-                                         weight_decay=args.weight_decay)",
            "+        optimizer = torch.optim.Adam(params,",
            "+                                     weight_decay=args.weight_decay)",
            "else:",
            "raise NotImplementedError(\"unknown optimizer: \" + args.opt)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 3881,
        "neg_line": [
            "-optimizer = torch.optim.Adam(params,",
            "-weight_decay=args.weight_decay)"
        ],
        "pos_line": [
            "+optimizer = torch.optim.Adam(params,",
            "+weight_decay=args.weight_decay)"
        ],
        "core_change": "-optimizer = torch.optim.Adam(params, -weight_decay=args.weight_decay) +optimizer = torch.optim.Adam(params, +weight_decay=args.weight_decay)",
        "core_API": "Adam"
    },
    {
        "commit_hash": "737b1f467e3610691f6f7ca4debb94a9d6219ad1",
        "index": "7c0946d9..4c552b61 100644",
        "commit_message": "Full GraphGym + PyG integration (#3076)\n\n* Full GraphGym + PyG integration\n\n* fix torch.allclose\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_copy_linear(lazy):",
            "assert torch.allclose(copied_lin.weight, lin.weight)",
            "assert id(copied_lin.bias) != id(lin.bias)",
            "assert copied_lin.bias.data_ptr() != lin.bias.data_ptr()",
            "-    assert torch.allclose(copied_lin.bias, lin.bias)",
            "+    assert torch.allclose(copied_lin.bias, lin.bias, atol=1e-6)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=1000044)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=1000045)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'atol'), position=0, insert_id=1000046)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1000047)",
            "Insert(target_node=IN(type=keyword_argument), node=('float', '1e-6'), position=2, insert_id=1000048)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 3886,
        "neg_line": [
            "-assert torch.allclose(copied_lin.bias, lin.bias)"
        ],
        "pos_line": [
            "+assert torch.allclose(copied_lin.bias, lin.bias, atol=1e-6)"
        ],
        "core_change": "-assert torch.allclose(copied_lin.bias, lin.bias) +assert torch.allclose(copied_lin.bias, lin.bias, atol=1e-6)",
        "core_API": "allclose"
    },
    {
        "commit_hash": "feb04554250d1833c6070499ae77004de7d44303",
        "index": "002fc0477..a41921333 100644",
        "commit_message": "Fix CI errors for torch 1.9.1\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_neural_beamformer_bf_output(",
            "assert others[\"mask_spk{}\".format(n)].shape[-2] == ch",
            "assert specs[n - 1].shape == others[\"mask_spk{}\".format(n)][..., 0, :].shape",
            "assert specs[n - 1].shape == input_spectrum[..., 0, :].shape",
            "-        if is_torch_1_9_plus and torch.is_complex(specs[n - 1]):",
            "+        if is_torch_complex_tensor(specs[n - 1]):",
            "assert specs[n - 1].dtype == torch.complex64",
            "else:",
            "assert specs[n - 1].dtype == torch.float"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=ASTNode(type=if_statement), node=ASTNode(type=call), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=torch), value='is_torch_complex_tensor')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=torch), position=0)",
            "Delete(target_node=ASTNode(type=identifier, text=is_torch_1_9_plus))",
            "Delete(target_node=ASTNode(type=and, text=and))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=is_complex))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=boolean_operator))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 3887,
        "neg_line": [
            "-if is_torch_1_9_plus and torch.is_complex(specs[n - 1]):"
        ],
        "pos_line": [
            "+if is_torch_complex_tensor(specs[n - 1]):"
        ],
        "core_change": "-if is_torch_1_9_plus and torch.is_complex(specs[n - 1]): +if is_torch_complex_tensor(specs[n - 1]):",
        "core_API": "is_complex"
    },
    {
        "commit_hash": "d49d0efbaf6611dc0e7c06a4807a145258051bcf",
        "index": "4329d38dd..917911c0f 100644",
        "commit_message": "[RLlib] Bug fix: when on GPU, sample_batch.to_device() only converts the device and does not convert float64 to float32. (#25460)\n\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SampleBatch(dict):",
            "assert torch is not None",
            "for k, v in self.items():",
            "if isinstance(v, np.ndarray) and v.dtype != object:",
            "-                    self[k] = torch.from_numpy(v).to(device)",
            "+                    self[k] = convert_to_torch_tensor(v, device)",
            "else:",
            "raise NotImplementedError",
            "return self"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=torch), value='convert_to_torch_tensor')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=torch), position=0)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=v), position=1)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1111191)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=from_numpy))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=to))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 14,
        "number": 3889,
        "neg_line": [
            "-self[k] = torch.from_numpy(v).to(device)"
        ],
        "pos_line": [
            "+self[k] = convert_to_torch_tensor(v, device)"
        ],
        "core_change": "-self[k] = torch.from_numpy(v).to(device) +self[k] = convert_to_torch_tensor(v, device)",
        "core_API": "items"
    },
    {
        "commit_hash": "813eba85b266fe46b0ac02a62fce8b25e3eeabac",
        "index": "b2b3bc7..58113f0 100644",
        "commit_message": "Empty val batch CUDA device fix (#7539)\n\nVerified fix for https://github.com/ultralytics/yolov5/pull/7525#issuecomment-1106081123\n",
        "file": "yolov5.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def run(",
            "",
            "if npr == 0:",
            "if nl:",
            "-                    stats.append((correct, *torch.zeros((3, 0))))",
            "+                    stats.append((correct, *torch.zeros((3, 0), device=device)))",
            "continue",
            "",
            "# Predictions"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1294747)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1294748)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=1294749)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1294750)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=2, insert_id=1294751)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 3890,
        "neg_line": [
            "-stats.append((correct, *torch.zeros((3, 0))))"
        ],
        "pos_line": [
            "+stats.append((correct, *torch.zeros((3, 0), device=device)))"
        ],
        "core_change": "-stats.append((correct, *torch.zeros((3, 0)))) +stats.append((correct, *torch.zeros((3, 0), device=device)))",
        "core_API": "append"
    },
    {
        "commit_hash": "8e861e75ed9afee06a046c24539fad5535e5c578",
        "index": "08a8e5ed..9e909397 100644",
        "commit_message": "pytorch 1.0 (#2165)\n\n* fixes\n\n* allenlp\n\n* fix bug\n\n* reduce=False -> reduction='none'\n\n* fix _sparse_mask\n\n* remove print statement\n\n* fix more tests\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "def make_model(num_layers: int = 6,",
            "# Initialize parameters with Glorot / fan_avg.",
            "for p in model.parameters():",
            "if p.dim() > 1:",
            "-            torch.nn.init.xavier_uniform(p)",
            "+            torch.nn.init.xavier_uniform_(p)",
            "return model"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=xavier_uniform), value='xavier_uniform_')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 3891,
        "neg_line": [
            "-torch.nn.init.xavier_uniform(p)"
        ],
        "pos_line": [
            "+torch.nn.init.xavier_uniform_(p)"
        ],
        "core_change": "-torch.nn.init.xavier_uniform(p) +torch.nn.init.xavier_uniform_(p)",
        "core_API": "parameters"
    },
    {
        "commit_hash": "306db252a7c4d2ff3f66b30aae3b37a72737ee3b",
        "index": "53c2792c4..1598d8236 100644",
        "commit_message": "fixes and clean-up\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class VGG2L(torch.nn.Module):",
            "if self.output is not None:",
            "sequence = self.output(sequence)",
            "",
            "-        if mask is not None:",
            "-            return sequence, self.create_new_mask(mask)",
            "-",
            "-        return sequence, mask",
            "+        return sequence, self.create_new_mask(mask)",
            "",
            "def create_new_conformer_mask(self, mask: torch.Tensor) -> torch.Tensor:",
            "\"\"\"Create a new conformer mask for output sequences."
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=ASTNode(type=module), node=ASTNode(type=return_statement), position=3)",
            "Delete(target_node=ASTNode(type=if, text=if))",
            "Delete(target_node=ASTNode(type=identifier, text=mask))",
            "Delete(target_node=ASTNode(type=is not, text=is))",
            "Delete(target_node=ASTNode(type=is not, text=not))",
            "Delete(target_node=ASTNode(type=none, text=None))",
            "Delete(target_node=ASTNode(type=comparison_operator))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=block))",
            "Delete(target_node=ASTNode(type=if_statement))",
            "Delete(target_node=ASTNode(type=return, text=return))",
            "Delete(target_node=ASTNode(type=identifier, text=sequence))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=mask))",
            "Delete(target_node=ASTNode(type=expression_list))",
            "Delete(target_node=ASTNode(type=return_statement))"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 16,
        "number": 3892,
        "neg_line": [
            "-if mask is not None:",
            "-return sequence, self.create_new_mask(mask)",
            "-",
            "-return sequence, mask"
        ],
        "pos_line": [
            "+return sequence, self.create_new_mask(mask)"
        ],
        "core_change": "-if mask is not None: -return sequence, self.create_new_mask(mask) - -return sequence, mask +return sequence, self.create_new_mask(mask)",
        "core_API": "output"
    },
    {
        "commit_hash": "43043ee4d5c672f7fbc22ac9559e8164731fd053",
        "index": "947142f1db..cb00f88c09 100644",
        "commit_message": "[RLlib] Tf2x preparation; part 2 (upgrading `try_import_tf()`). (#9136)\n\n* WIP.\n\n* Fixes.\n\n* LINT.\n\n* WIP.\n\n* WIP.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* WIP.\n\n* Fixes.\n\n* Test\n\n* Fix.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* LINT.\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def setup_mixins(policy, obs_space, action_space, config):",
            "ValueNetworkMixin.__init__(policy)",
            "# Set up a tf-var for the moving avg (do this here to make it work with",
            "# eager mode).",
            "-    policy._ma_adv_norm = tf.get_variable(",
            "+    policy._ma_adv_norm = tf1.get_variable(",
            "name=\"moving_average_of_advantage_norm\",",
            "dtype=tf.float32,",
            "initializer=100.0,"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tf), value='tf1')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 3901,
        "neg_line": [
            "-policy._ma_adv_norm = tf.get_variable("
        ],
        "pos_line": [
            "+policy._ma_adv_norm = tf1.get_variable("
        ],
        "core_change": "-policy._ma_adv_norm = tf.get_variable( +policy._ma_adv_norm = tf1.get_variable(",
        "core_API": "__init__"
    },
    {
        "commit_hash": "e7c32ae49810cf66c624126f6a63e6e01f8814c0",
        "index": "0e8abeca..6a7a946e 100644",
        "commit_message": "fix cascade r-cnn in TF1. tf.zeros([tensor]) is supported only in TF2 (fix #1525)\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class CascadeRCNNHead(object):",
            "max_iou_per_box = tf.reduce_max(iou, axis=1)  # N",
            "best_iou_ind = tf.cond(tf.shape(iou)[1] > 0,",
            "lambda: tf.argmax(iou, axis=1),   # #proposal, each in 0~m-1",
            "-                                       lambda: tf.zeros([tf.shape(iou)[0]], dtype=tf.int64))",
            "+                                       lambda: tf.zeros(tf.shape(iou)[0], dtype=tf.int64))",
            "labels_per_box = tf.gather(self.gt_labels, best_iou_ind)",
            "fg_mask = max_iou_per_box >= iou_threshold",
            "fg_inds_wrt_gt = tf.boolean_mask(best_iou_ind, fg_mask)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=subscript), position=1)",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=list))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 3902,
        "neg_line": [
            "-lambda: tf.zeros([tf.shape(iou)[0]], dtype=tf.int64))"
        ],
        "pos_line": [
            "+lambda: tf.zeros(tf.shape(iou)[0], dtype=tf.int64))"
        ],
        "core_change": "-lambda: tf.zeros([tf.shape(iou)[0]], dtype=tf.int64)) +lambda: tf.zeros(tf.shape(iou)[0], dtype=tf.int64))",
        "core_API": "reduce_max"
    },
    {
        "commit_hash": "de4159a318f9a4f2f474733306c00ac326e643a7",
        "index": "e1e23773a..c3da50d82 100644",
        "commit_message": "More TF int dtype fixes (#20384)\n\n* Add a test to ensure int dummy inputs are int64\n\n* Move the test into the existing int64 test and update a lot of existing dummies\n\n* Fix remaining dummies\n\n* Fix remaining dummies\n\n* Test for int64 serving sigs as well\n\n* Update core tests to use tf.int64\n\n* Add better messages to the assertions\n\n* Update all serving sigs to int64\n\n* More sneaky hiding tf.int32s\n\n* Add an optional int32 signature in save_pretrained\n\n* make fixup\n\n* Add Amy's suggestions\n\n* Switch all serving sigs back to tf.int32\n\n* Switch all dummies to tf.int32\n\n* Adjust tests to check for tf.int32 instead of tf.int64\n\n* Fix base dummy_inputs dtype\n\n* Start casting to tf.int32 in input_processing\n\n* Change dtype for unpack_inputs test\n\n* Add proper tf.int32 test\n\n* Make the alternate serving signature int64\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFTransfoXLPreTrainedModel(TFPreTrainedModel):",
            "@tf.function(",
            "input_signature=[",
            "{",
            "-                \"input_ids\": tf.TensorSpec((None, None), tf.int64, name=\"input_ids\"),",
            "+                \"input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"input_ids\"),",
            "}",
            "]",
            ")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=int64), value='int32')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 3905,
        "neg_line": [
            "-\"input_ids\": tf.TensorSpec((None, None), tf.int64, name=\"input_ids\"),"
        ],
        "pos_line": [
            "+\"input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"input_ids\"),"
        ],
        "core_change": "-\"input_ids\": tf.TensorSpec((None, None), tf.int64, name=\"input_ids\"), +\"input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"input_ids\"),",
        "core_API": "function"
    },
    {
        "commit_hash": "4b7c0fae00084b72dffe37fdd0ea7d2e9b60d103",
        "index": "60c0f5f84..94e6cf376 100644",
        "commit_message": "Fix amp autocast  (#6080)\n\n* precision fixes\n\n* add amp test model\n\n* fix test\n\n* revert\n\n* move assert to training step\n\n* fix test\n\n* fix test\n\n* remove unrelated changes\n\n* add changelog\n\n* remove unused import\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "cuda",
        "change": [
            "class NativeMixedPrecisionPlugin(MixedPrecisionPlugin):",
            "@contextmanager",
            "def train_step_context(self) -> Generator[autocast, None, None]:",
            "\"\"\"Enable autocast context\"\"\"",
            "-        yield torch.cuda.amp.autocast()",
            "+        with torch.cuda.amp.autocast():",
            "+            yield"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('with_statement', None), position=3, insert_id=1844487)",
            "Insert(target_node=IN(type=with_statement), node=('with', 'with'), position=0, insert_id=1844488)",
            "Insert(target_node=IN(type=with_statement), node=('with_clause', None), position=1, insert_id=1844489)",
            "Insert(target_node=IN(type=with_statement), node=(':', ':'), position=2, insert_id=1844490)",
            "Insert(target_node=IN(type=with_statement), node=('block', None), position=3, insert_id=1844491)",
            "Insert(target_node=IN(type=with_clause), node=('with_item', None), position=0, insert_id=1844492)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=1844493)",
            "Move(target_node=IN(type=with_item), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=expression_statement), node=('yield', 'yield'), position=0, insert_id=1844494)",
            "Delete(target_node=ASTNode(type=yield, text=yield))",
            "Delete(target_node=ASTNode(type=yield))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 12,
        "number": 3907,
        "neg_line": [
            "-yield torch.cuda.amp.autocast()"
        ],
        "pos_line": [
            "+with torch.cuda.amp.autocast():",
            "+yield"
        ],
        "core_change": "-yield torch.cuda.amp.autocast() +with torch.cuda.amp.autocast(): +yield",
        "core_API": "autocast"
    },
    {
        "commit_hash": "f72f17d0e324cfa813592f35e71ad51588dc57fe",
        "index": "298a4f39..23aa5574 100644",
        "commit_message": "quickfix batch shape\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class VPGModel(PGModel):",
            "self.log_probabilities = self.dist.log_prob(self.policy.get_policy_variables(), self.actions)",
            "",
            "# Concise: Get log likelihood of actions, weigh by advantages, compute gradient on that",
            "-            self.loss = -tf.reduce_mean(self.log_probabilities * self.advantage, name=\"loss_op\")",
            "+            self.loss = -tf.reduce_mean(self.log_probabilities * self.advantage, name=\"loss_op\", axis=1)",
            "",
            "self.optimize_op = self.optimizer.minimize(self.loss)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=2246011)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=2246012)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'axis'), position=0, insert_id=2246013)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2246014)",
            "Insert(target_node=IN(type=keyword_argument), node=('integer', '1'), position=2, insert_id=2246015)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 3908,
        "neg_line": [
            "-self.loss = -tf.reduce_mean(self.log_probabilities * self.advantage, name=\"loss_op\")"
        ],
        "pos_line": [
            "+self.loss = -tf.reduce_mean(self.log_probabilities * self.advantage, name=\"loss_op\", axis=1)"
        ],
        "core_change": "-self.loss = -tf.reduce_mean(self.log_probabilities * self.advantage, name=\"loss_op\") +self.loss = -tf.reduce_mean(self.log_probabilities * self.advantage, name=\"loss_op\", axis=1)",
        "core_API": "log_prob"
    },
    {
        "commit_hash": "2d54116baa1284057d68bde52daf49ee19c5f899",
        "index": "1e1f350ae..ef8c1c289 100644",
        "commit_message": "annotat unused vars (#5017)\n\n* annotate all unused vars\n\n* rank_zero_warn\n\n* Apply suggestions from code review\n\nCo-authored-by: Carlos Mochol <carlossmocholi@gmail.com>\n\n* f1 fixed\n\nCo-authored-by: Carlos Mochol <carlossmocholi@gmail.com>\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def bleu_score(",
            "assert len(translate_corpus) == len(reference_corpus)",
            "numerator = torch.zeros(n_gram)",
            "denominator = torch.zeros(n_gram)",
            "-    precision_scores = torch.zeros(n_gram)",
            "c = 0.0",
            "r = 0.0"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=IN(type=root), node=ASTNode(type=attribute), position=0)",
            "Delete(target_node=ASTNode(type=identifier, text=precision_scores))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=zeros))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 3913,
        "neg_line": [
            "-precision_scores = torch.zeros(n_gram)"
        ],
        "pos_line": [],
        "core_change": "-precision_scores = torch.zeros(n_gram)",
        "core_API": "zeros"
    },
    {
        "commit_hash": "22ccc43670dac93eb7fe81520a84cf3979d05693",
        "index": "5295866769..de92cae264 100644",
        "commit_message": "[RLlib] DQN torch version. (#7597)\n\n* Fix.\n\n* Rollback.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* Fix.\n\n* Fix.\n\n* Fix.\n\n* Fix.\n\n* Fix.\n\n* WIP.\n\n* WIP.\n\n* Fix.\n\n* Test case fixes.\n\n* Test case fixes and LINT.\n\n* Test case fixes and LINT.\n\n* Rollback.\n\n* WIP.\n\n* WIP.\n\n* Test case fixes.\n\n* Fix.\n\n* Fix.\n\n* Fix.\n\n* Add regression test for DQN w/ param noise.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* Comment\n\n* Regression test case.\n\n* WIP.\n\n* WIP.\n\n* LINT.\n\n* LINT.\n\n* WIP.\n\n* Fix.\n\n* Fix.\n\n* Fix.\n\n* LINT.\n\n* Fix (SAC does currently not support eager).\n\n* Fix.\n\n* WIP.\n\n* LINT.\n\n* Update rllib/evaluation/sampler.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* Update rllib/evaluation/sampler.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* Update rllib/utils/exploration/exploration.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* Update rllib/utils/exploration/exploration.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* WIP.\n\n* WIP.\n\n* Fix.\n\n* LINT.\n\n* LINT.\n\n* Fix and LINT.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* Fix.\n\n* LINT.\n\n* Fix.\n\n* Fix and LINT.\n\n* Update rllib/utils/exploration/exploration.py\n\n* Update rllib/policy/dynamic_tf_policy.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* Update rllib/policy/dynamic_tf_policy.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* Update rllib/policy/dynamic_tf_policy.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* Fixes.\n\n* WIP.\n\n* LINT.\n\n* Fixes and LINT.\n\n* LINT and fixes.\n\n* LINT.\n\n* Move action_dist back into torch extra_action_out_fn and LINT.\n\n* Working SimpleQ learning cartpole on both torch AND tf.\n\n* Working Rainbow learning cartpole on tf.\n\n* Working Rainbow learning cartpole on tf.\n\n* WIP.\n\n* LINT.\n\n* LINT.\n\n* Update docs and add torch to APEX test.\n\n* LINT.\n\n* Fix.\n\n* LINT.\n\n* Fix.\n\n* Fix.\n\n* Fix and docstrings.\n\n* Fix broken RLlib tests in master.\n\n* Split BAZEL learning tests into cartpole and pendulum (reached the 60min barrier).\n\n* Fix error_outputs option in BAZEL for RLlib regression tests.\n\n* Fix.\n\n* Tune param-noise tests.\n\n* LINT.\n\n* Fix.\n\n* Fix.\n\n* test\n\n* test\n\n* test\n\n* Fix.\n\n* Fix.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* LINT.\n\n* WIP.\n\nCo-authored-by: Eric Liang <ekhliang@gmail.com>\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class RolloutWorker(EvaluatorInterface, ParallelIteratorWorker):",
            "policy_config = policy_config or {}",
            "if (tf and policy_config.get(\"eager\")",
            "and not policy_config.get(\"no_eager_on_workers\")):",
            "-            tf.enable_eager_execution()",
            "+            # This check is necessary for certain all-framework tests that",
            "+            # use tf's eager_mode() context generator.",
            "+            if not tf.executing_eagerly():",
            "+                tf.enable_eager_execution()",
            "",
            "if log_level:",
            "logging.getLogger(\"ray.rllib\").setLevel(log_level)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=block), node=('if_statement', None), position=0, insert_id=2146524)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=2146525)",
            "Insert(target_node=IN(type=if_statement), node=('not_operator', None), position=1, insert_id=2146526)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=2146527)",
            "Move(target_node=IN(type=if_statement), node=ASTNode(type=block), position=3)",
            "Insert(target_node=IN(type=not_operator), node=('not', 'not'), position=0, insert_id=2146528)",
            "Insert(target_node=IN(type=not_operator), node=('call', None), position=1, insert_id=2146529)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2146530)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2146531)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2146532)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2146533)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'executing_eagerly'), position=2, insert_id=2146534)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2146535)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=2146536)"
        ],
        "plus_line": 4,
        "minus_line": 1,
        "AST_diff_line": 14,
        "number": 3915,
        "neg_line": [
            "-tf.enable_eager_execution()"
        ],
        "pos_line": [
            "+# This check is necessary for certain all-framework tests that",
            "+# use tf's eager_mode() context generator.",
            "+if not tf.executing_eagerly():",
            "+tf.enable_eager_execution()"
        ],
        "core_change": "-tf.enable_eager_execution() +# This check is necessary for certain all-framework tests that +# use tf's eager_mode() context generator. +if not tf.executing_eagerly(): +tf.enable_eager_execution()",
        "core_API": "get"
    },
    {
        "commit_hash": "b9ee04beefae68d2431a6162f8b622debd249954",
        "index": "21a7ac9..b1139d9 100644",
        "commit_message": "fix typo\n\n",
        "file": "Reinforcement-learning-with-tensorflow.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class DeepQNetwork:",
            "t_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='target_net')",
            "e_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='eval_net')",
            "",
            "-        with tf.variable_scope('soft_replacement'):",
            "+        with tf.variable_scope('hard_replacement'):",
            "self.target_replace_op = [tf.assign(t, e) for t, e in zip(t_params, e_params)]",
            "",
            "self.sess = tf.Session()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text='soft_replacement'), value=\"'hard_replacement'\")"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 3918,
        "neg_line": [
            "-with tf.variable_scope('soft_replacement'):"
        ],
        "pos_line": [
            "+with tf.variable_scope('hard_replacement'):"
        ],
        "core_change": "-with tf.variable_scope('soft_replacement'): +with tf.variable_scope('hard_replacement'):",
        "core_API": "get_collection"
    },
    {
        "commit_hash": "cc11be06d7efd1e9a2a1cd5a35e7f790592e5067",
        "index": "3268be46..10f4fb0a 100644",
        "commit_message": "fixing \"No space allowed before...\" compile errors\n\nfixing \"No space allowed before...\" compile errors\n",
        "file": "TTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Synthesizer(object):",
            "wav = self.pwgan.inference(vocoder_input, hop_size=self.ap.hop_length)",
            "elif self.wavernn:",
            "vocoder_input = None",
            "-                if self.tts_config.model == \"Tacotron\" :",
            "-                    vocoder_input = torch.FloatTensor(self.ap.out_linear_to_mel(linear_spec = postnet_output.T).T).T.unsqueeze(0)",
            "+                if self.tts_config.model == \"Tacotron\":",
            "+                    vocoder_input = torch.FloatTensor(self.ap.out_linear_to_mel(linear_spec=postnet_output.T).T).T.unsqueeze(0)",
            "else:",
            "vocoder_input = torch.FloatTensor(postnet_output.T).unsqueeze(0)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 3925,
        "neg_line": [
            "-if self.tts_config.model == \"Tacotron\" :",
            "-vocoder_input = torch.FloatTensor(self.ap.out_linear_to_mel(linear_spec = postnet_output.T).T).T.unsqueeze(0)"
        ],
        "pos_line": [
            "+if self.tts_config.model == \"Tacotron\":",
            "+vocoder_input = torch.FloatTensor(self.ap.out_linear_to_mel(linear_spec=postnet_output.T).T).T.unsqueeze(0)"
        ],
        "core_change": "-if self.tts_config.model == \"Tacotron\" : -vocoder_input = torch.FloatTensor(self.ap.out_linear_to_mel(linear_spec = postnet_output.T).T).T.unsqueeze(0) +if self.tts_config.model == \"Tacotron\": +vocoder_input = torch.FloatTensor(self.ap.out_linear_to_mel(linear_spec=postnet_output.T).T).T.unsqueeze(0)",
        "core_API": "inference"
    },
    {
        "commit_hash": "8a9f0e68d0ae90c3b531ebf42ac308090165810d",
        "index": "447c6d14..f341776c 100644",
        "commit_message": "Fix CUDA tests on dev (#1484)\n\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class NUTS(HMC):",
            "else:",
            "new_tree_prob = new_tree.weight / tree_weight",
            "rand = pyro.sample(\"rand_t={}_treedepth={}\".format(self._t, tree_depth),",
            "-                                   dist.Uniform(torch.zeros(1), torch.ones(1)))",
            "+                                   dist.Uniform(new_tree_prob.new_tensor(0.),",
            "+                                                new_tree_prob.new_tensor(1.)))",
            "if rand < new_tree_prob:",
            "accepted = True",
            "z = new_tree.z_proposal"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('(', '('), position=0, insert_id=732642)",
            "Update(target_node=ASTNode(type=identifier, text=torch), value='new_tree_prob')",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=732643)",
            "Update(target_node=ASTNode(type=identifier, text=zeros), value='new_tensor')",
            "Insert(target_node=ASTNode(type=argument_list), node=('float', '0.'), position=1, insert_id=732644)",
            "Update(target_node=ASTNode(type=identifier, text=torch), value='new_tree_prob')",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=ones), value='new_tensor')",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=(, text=(), position=0)",
            "Insert(target_node=ASTNode(type=argument_list), node=('float', '1.'), position=1, insert_id=732645)",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=integer, text=1))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 14,
        "number": 3927,
        "neg_line": [
            "-dist.Uniform(torch.zeros(1), torch.ones(1)))"
        ],
        "pos_line": [
            "+dist.Uniform(new_tree_prob.new_tensor(0.),",
            "+new_tree_prob.new_tensor(1.)))"
        ],
        "core_change": "-dist.Uniform(torch.zeros(1), torch.ones(1))) +dist.Uniform(new_tree_prob.new_tensor(0.), +new_tree_prob.new_tensor(1.)))",
        "core_API": "sample"
    },
    {
        "commit_hash": "b754525e99ca62424c484fe529b6142f6bab939e",
        "index": "4e49730..d9961f5 100644",
        "commit_message": "Check `'onnxruntime-gpu' if torch.has_cuda` (#5087)\n\n* Check `'onnxruntime-gpu' if torch.has_cuda`\n\n* fix indent\n",
        "file": "yolov5.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def run(weights=ROOT / 'yolov5s.pt',  # model.pt path(s)",
            "# check_requirements(('opencv-python>=4.5.4',))",
            "net = cv2.dnn.readNetFromONNX(w)",
            "else:",
            "-            check_requirements(('onnx', 'onnxruntime'))",
            "+            check_requirements(('onnx', 'onnxruntime-gpu' if torch.has_cuda else 'onnxruntime'))",
            "import onnxruntime",
            "session = onnxruntime.InferenceSession(w, None)",
            "else:  # TensorFlow models"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=tuple), node=('conditional_expression', None), position=3, insert_id=1296490)",
            "Insert(target_node=IN(type=conditional_expression), node=('string', \"'onnxruntime-gpu'\"), position=0, insert_id=1296491)",
            "Insert(target_node=IN(type=conditional_expression), node=('if', 'if'), position=1, insert_id=1296492)",
            "Insert(target_node=IN(type=conditional_expression), node=('attribute', None), position=2, insert_id=1296493)",
            "Insert(target_node=IN(type=conditional_expression), node=('else', 'else'), position=3, insert_id=1296494)",
            "Move(target_node=IN(type=conditional_expression), node=ASTNode(type=string, text='onnxruntime'), position=4)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1296495)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1296496)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'has_cuda'), position=2, insert_id=1296497)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 3930,
        "neg_line": [
            "-check_requirements(('onnx', 'onnxruntime'))"
        ],
        "pos_line": [
            "+check_requirements(('onnx', 'onnxruntime-gpu' if torch.has_cuda else 'onnxruntime'))"
        ],
        "core_change": "-check_requirements(('onnx', 'onnxruntime')) +check_requirements(('onnx', 'onnxruntime-gpu' if torch.has_cuda else 'onnxruntime'))",
        "core_API": "readNetFromONNX"
    },
    {
        "commit_hash": "0c166dcfb8b9e9d79d013b7967a59dfb1ac4b804",
        "index": "475c06ae..990b1a27 100644",
        "commit_message": "define and apply YAPF formatting  (#1039)\n\n* define YAPF\n\n* yapf kornia -rip\n\n* yapf examples -rip\n\n* yapf test -rip\n\n* Update CONTRIBUTING.rst\n\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\n\n* update config\n\n* fix 4lint\n\n* mypy\n\n* mypy\n\n* mypy\n\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestDrawRectangle:",
            "points_list.append([])",
            "for n in range(N):",
            "points_list[b].append([])",
            "-                points_list[b][n].append(int(torch.randint(0, w - 1, (1,))))",
            "-                points_list[b][n].append(int(torch.randint(0, h - 1, (1,))))",
            "-                points_list[b][n].append(int(torch.randint(points_list[b][n][-2] + 1, w, (1,))))",
            "-                points_list[b][n].append(int(torch.randint(points_list[b][n][-2] + 1, h, (1,))))",
            "+                points_list[b][n].append(int(torch.randint(0, w - 1, (1, ))))",
            "+                points_list[b][n].append(int(torch.randint(0, h - 1, (1, ))))",
            "+                points_list[b][n].append(int(torch.randint(points_list[b][n][-2] + 1, w, (1, ))))",
            "+                points_list[b][n].append(int(torch.randint(points_list[b][n][-2] + 1, h, (1, ))))",
            "",
            "points = torch.tensor(points_list).to(device)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 4,
        "minus_line": 4,
        "AST_diff_line": 17,
        "number": 3932,
        "neg_line": [
            "-points_list[b][n].append(int(torch.randint(0, w - 1, (1,))))",
            "-points_list[b][n].append(int(torch.randint(0, h - 1, (1,))))",
            "-points_list[b][n].append(int(torch.randint(points_list[b][n][-2] + 1, w, (1,))))",
            "-points_list[b][n].append(int(torch.randint(points_list[b][n][-2] + 1, h, (1,))))"
        ],
        "pos_line": [
            "+points_list[b][n].append(int(torch.randint(0, w - 1, (1, ))))",
            "+points_list[b][n].append(int(torch.randint(0, h - 1, (1, ))))",
            "+points_list[b][n].append(int(torch.randint(points_list[b][n][-2] + 1, w, (1, ))))",
            "+points_list[b][n].append(int(torch.randint(points_list[b][n][-2] + 1, h, (1, ))))"
        ],
        "core_change": "-points_list[b][n].append(int(torch.randint(0, w - 1, (1,)))) -points_list[b][n].append(int(torch.randint(0, h - 1, (1,)))) -points_list[b][n].append(int(torch.randint(points_list[b][n][-2] + 1, w, (1,)))) -points_list[b][n].append(int(torch.randint(points_list[b][n][-2] + 1, h, (1,)))) +points_list[b][n].append(int(torch.randint(0, w - 1, (1, )))) +points_list[b][n].append(int(torch.randint(0, h - 1, (1, )))) +points_list[b][n].append(int(torch.randint(points_list[b][n][-2] + 1, w, (1, )))) +points_list[b][n].append(int(torch.randint(points_list[b][n][-2] + 1, h, (1, ))))",
        "core_API": "append"
    },
    {
        "commit_hash": "88925314200812bf1c7fc8eb2f6ff7eea8eadee9",
        "index": "6e701372..a6c96678 100644",
        "commit_message": "[Bugbash] fix bug in compression (#4259)\n\n\n",
        "file": "nni.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "if __name__ == '__main__':",
            "evaluator(model)",
            "",
            "pruner._unwrap_model()",
            "-    ModelSpeedup(model, dummy_input=torch.rand(10, 3, 32, 32).to(device), masks_file='simple_masks.pth').speedup_model()",
            "+    ModelSpeedup(model, dummy_input=torch.rand(10, 3, 32, 32).to(device), masks_file=masks).speedup_model()",
            "",
            "print('\\nThe accuracy after speed up:')",
            "evaluator(model)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=keyword_argument), node=('identifier', 'masks'), position=2, insert_id=660274)",
            "Delete(target_node=ASTNode(type=string, text='simple_masks.pth'))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 3934,
        "neg_line": [
            "-ModelSpeedup(model, dummy_input=torch.rand(10, 3, 32, 32).to(device), masks_file='simple_masks.pth').speedup_model()"
        ],
        "pos_line": [
            "+ModelSpeedup(model, dummy_input=torch.rand(10, 3, 32, 32).to(device), masks_file=masks).speedup_model()"
        ],
        "core_change": "-ModelSpeedup(model, dummy_input=torch.rand(10, 3, 32, 32).to(device), masks_file='simple_masks.pth').speedup_model() +ModelSpeedup(model, dummy_input=torch.rand(10, 3, 32, 32).to(device), masks_file=masks).speedup_model()",
        "core_API": "_unwrap_model"
    },
    {
        "commit_hash": "a9653400d3fac5b316429f641ae61846ae024cc7",
        "index": "e99485458..5076fa641 100755",
        "commit_message": "Fix valid ratio for Deformable Detr (#20958)\n\n* fix: valid ratio has right value\n\n* chore: remove unnecessary line\n\nCo-authored-by: Jeongyeon Nam <jy.nam@navercorp.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DeformableDetrModel(DeformableDetrPreTrainedModel):",
            "spatial_shapes = torch.as_tensor(spatial_shapes, dtype=torch.long, device=source_flatten.device)",
            "level_start_index = torch.cat((spatial_shapes.new_zeros((1,)), spatial_shapes.prod(1).cumsum(0)[:-1]))",
            "valid_ratios = torch.stack([self.get_valid_ratio(m) for m in masks], 1)",
            "-",
            "-        # revert valid_ratios",
            "-        valid_ratios = ~valid_ratios.bool()",
            "valid_ratios = valid_ratios.float()",
            "",
            "# Fourth, sent source_flatten + mask_flatten + lvl_pos_embed_flatten (backbone + proj layer output) through encoder"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=valid_ratios))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=~, text=~))",
            "Delete(target_node=ASTNode(type=identifier, text=valid_ratios))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=bool))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=unary_operator))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 0,
        "minus_line": 2,
        "AST_diff_line": 14,
        "number": 3938,
        "neg_line": [
            "-",
            "-# revert valid_ratios",
            "-valid_ratios = ~valid_ratios.bool()"
        ],
        "pos_line": [],
        "core_change": "- -# revert valid_ratios -valid_ratios = ~valid_ratios.bool()",
        "core_API": "as_tensor"
    },
    {
        "commit_hash": "d0c05209ef7c8c93dc9cf70188ccc207437cba34",
        "index": "f99bf15042..724761641d 100644",
        "commit_message": "lint fixes\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "trunc.unsupported_dtypes = (\"float16\",)",
            "",
            "",
            "def abs(",
            "-    x: Union[float, torch.Tensor],",
            "-    *,",
            "-    out: Optional[torch.Tensor] = None",
            "+    x: Union[float, torch.Tensor], *, out: Optional[torch.Tensor] = None",
            ") -> torch.Tensor:",
            "x = _cast_for_unary_op(x)",
            "return torch.abs(x, out=out)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 17,
        "number": 3942,
        "neg_line": [
            "-x: Union[float, torch.Tensor],",
            "-*,",
            "-out: Optional[torch.Tensor] = None"
        ],
        "pos_line": [
            "+x: Union[float, torch.Tensor], *, out: Optional[torch.Tensor] = None"
        ],
        "core_change": "-x: Union[float, torch.Tensor], -*, -out: Optional[torch.Tensor] = None +x: Union[float, torch.Tensor], *, out: Optional[torch.Tensor] = None",
        "core_API": "abs"
    },
    {
        "commit_hash": "b3a916c9afe40484ebb3c8d3da82bf6c89c27eb2",
        "index": "23771d2e..b437e83f 100755",
        "commit_message": "Fixing exploration tensors. Needs templating and get variables\nfor compatibility with train scaffold.\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Model(object):",
            "",
            "elif action_spec['type'] == 'float':",
            "if 'min_value' in action_spec:",
            "-                    exploration = tf.clip_by_value(",
            "+                    exploration_value = tf.clip_by_value(",
            "t=exploration_value,",
            "clip_value_min=action_spec['min_value'],",
            "clip_value_max=action_spec['max_value']",
            ")",
            "",
            "-                action += tf.reshape(exploration, tf.shape(action))",
            "+                action += tf.reshape(exploration_value, tf.shape(action))",
            "",
            "return action"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=exploration), value='exploration_value')",
            "Update(target_node=ASTNode(type=identifier, text=exploration), value='exploration_value')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 3947,
        "neg_line": [
            "-exploration = tf.clip_by_value(",
            "-action += tf.reshape(exploration, tf.shape(action))"
        ],
        "pos_line": [
            "+exploration_value = tf.clip_by_value(",
            "+action += tf.reshape(exploration_value, tf.shape(action))"
        ],
        "core_change": "-exploration = tf.clip_by_value( +exploration_value = tf.clip_by_value( -action += tf.reshape(exploration, tf.shape(action)) +action += tf.reshape(exploration_value, tf.shape(action))",
        "core_API": "clip_by_value"
    },
    {
        "commit_hash": "86cff21cf60b86859934f6e6d867c95cceb8d6ac",
        "index": "50bcf1cc8..32ce3f856 100644",
        "commit_message": "Fix some TF GPT-J CI testings (#16454)\n\n* Fix for test_mixed_precision\n\n* Fix test_saved_model_creation by using shape_list instead of shape\n\n* skit test_model_from_pretrained on GPU for now to avoid GPU OOM\n\n* skip test_gptj_sample_max_time for now\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFGPTJModelLanguageGenerationTest(unittest.TestCase):",
            ")  # token_type_ids should change output",
            "",
            "@slow",
            "+    @unittest.skip(reason=\"TF generate currently has no time-based stopping criteria\")",
            "def test_gptj_sample_max_time(self):",
            "tokenizer = AutoTokenizer.from_pretrained(\"anton-l/gpt-j-tiny-random\")",
            "model = TFGPTJForCausalLM.from_pretrained(\"anton-l/gpt-j-tiny-random\", from_pt=True)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=decorated_definition), node=('decorator', None), position=1, insert_id=2365587)",
            "Insert(target_node=IN(type=decorator), node=('@', '@'), position=0, insert_id=2365588)",
            "Insert(target_node=IN(type=decorator), node=('call', None), position=1, insert_id=2365589)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2365590)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2365591)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'unittest'), position=0, insert_id=2365592)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2365593)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'skip'), position=2, insert_id=2365594)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2365595)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=1, insert_id=2365596)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2365597)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'reason'), position=0, insert_id=2365598)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2365599)",
            "Insert(target_node=IN(type=keyword_argument), node=('string', '\"TF generate currently has no time-based stopping criteria\"'), position=2, insert_id=2365600)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 14,
        "number": 3948,
        "neg_line": [],
        "pos_line": [
            "+@unittest.skip(reason=\"TF generate currently has no time-based stopping criteria\")"
        ],
        "core_change": "+@unittest.skip(reason=\"TF generate currently has no time-based stopping criteria\")",
        "core_API": "skip"
    },
    {
        "commit_hash": "245698302b6b300cc7ee2179759ee16f8e921641",
        "index": "0e20c4b2..bc8bf2b2 100644",
        "commit_message": "Add option to keep Viterbi scores when predicting (#1314)\n\n* fix torch version\n\n* fix tensorboard version\n\n* add keep_scores option\n\n* remove extra space\n\n* change to always return scores\n\n* rename to avoid type conflict\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ConditionalRandomField(torch.nn.Module):",
            "tag_sequence[sequence_length + 1, end_tag] = 0.",
            "",
            "# We pass the tags and the transitions to ``viterbi_decode``.",
            "-            viterbi_path, _ = util.viterbi_decode(tag_sequence[:(sequence_length + 2)], transitions)",
            "+            viterbi_path, viterbi_score = util.viterbi_decode(tag_sequence[:(sequence_length + 2)], transitions)",
            "# Get rid of START and END sentinels and append.",
            "-            all_tags.append(viterbi_path[1:-1])",
            "+            viterbi_path = viterbi_path[1:-1]",
            "+            best_paths.append((viterbi_path, viterbi_score.item()))",
            "",
            "-        return all_tags",
            "+        return best_paths"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=4, insert_id=36603)",
            "Insert(target_node=ASTNode(type=expression_statement), node=('assignment', None), position=0, insert_id=36604)",
            "Move(target_node=IN(type=expression_statement), node=ASTNode(type=call), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=all_tags), value='best_paths')",
            "Update(target_node=ASTNode(type=identifier, text=all_tags), value='viterbi_path')",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=identifier, text=all_tags), position=0)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=36605)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=subscript), position=2)",
            "Update(target_node=ASTNode(type=identifier, text=_), value='viterbi_score')",
            "Insert(target_node=ASTNode(type=attribute), node=('identifier', 'best_paths'), position=0, insert_id=36606)",
            "Insert(target_node=ASTNode(type=argument_list), node=('tuple', None), position=1, insert_id=36607)",
            "Insert(target_node=IN(type=tuple), node=('(', '('), position=0, insert_id=36608)",
            "Insert(target_node=IN(type=tuple), node=('identifier', 'viterbi_path'), position=1, insert_id=36609)",
            "Insert(target_node=IN(type=tuple), node=(',', ','), position=2, insert_id=36610)",
            "Insert(target_node=IN(type=tuple), node=('call', None), position=3, insert_id=36611)",
            "Insert(target_node=IN(type=tuple), node=(')', ')'), position=4, insert_id=36612)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=36613)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=36614)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'viterbi_score'), position=0, insert_id=36615)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=36616)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'item'), position=2, insert_id=36617)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=36618)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=36619)"
        ],
        "plus_line": 4,
        "minus_line": 3,
        "AST_diff_line": 23,
        "number": 3949,
        "neg_line": [
            "-viterbi_path, _ = util.viterbi_decode(tag_sequence[:(sequence_length + 2)], transitions)",
            "-all_tags.append(viterbi_path[1:-1])",
            "-return all_tags"
        ],
        "pos_line": [
            "+viterbi_path, viterbi_score = util.viterbi_decode(tag_sequence[:(sequence_length + 2)], transitions)",
            "+viterbi_path = viterbi_path[1:-1]",
            "+best_paths.append((viterbi_path, viterbi_score.item()))",
            "+return best_paths"
        ],
        "core_change": "-viterbi_path, _ = util.viterbi_decode(tag_sequence[:(sequence_length + 2)], transitions) +viterbi_path, viterbi_score = util.viterbi_decode(tag_sequence[:(sequence_length + 2)], transitions) -all_tags.append(viterbi_path[1:-1]) +viterbi_path = viterbi_path[1:-1] +best_paths.append((viterbi_path, viterbi_score.item())) -return all_tags +return best_paths",
        "core_API": "viterbi_decode"
    },
    {
        "commit_hash": "35fe3213ef5b1c566c4d73a9d90b0c2aace04f88",
        "index": "6045bd82..8bb216cf 100644",
        "commit_message": "RANSAC improvements (#1435)\n\n- Added LU solver for RANSAC-homography, it speeds-up the minimal-solver part x10\n- Now find_fundamental weights is optional, as it was for homography\n- Fix test precision and made one xFail test to be compulsory\n- Fixed tests for fundamental matrix, as one have to have >= 8 points (also added to docstring)\n- updates deprecated torch.svd to torch.linalg.svd\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def zca_mean(",
            "else:",
            "cov = cov / float(N)",
            "",
            "-    U, S, _ = _torch_svd_cast(cov)",
            "+    U, S, _ = torch.linalg.svd(cov)",
            "",
            "S = S.reshape(-1, 1)",
            "S_inv_root: torch.Tensor = torch.rsqrt(S + eps)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=398965)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=398966)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=398967)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'svd'), position=2, insert_id=398968)",
            "Update(target_node=ASTNode(type=identifier, text=_torch_svd_cast), value='torch')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=_torch_svd_cast), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=398969)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'linalg'), position=2, insert_id=398970)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 3951,
        "neg_line": [
            "-U, S, _ = _torch_svd_cast(cov)"
        ],
        "pos_line": [
            "+U, S, _ = torch.linalg.svd(cov)"
        ],
        "core_change": "-U, S, _ = _torch_svd_cast(cov) +U, S, _ = torch.linalg.svd(cov)",
        "core_API": "svd"
    },
    {
        "commit_hash": "a3a1c19a4baf0a8c2a6f18cff39130b00c1d4b0d",
        "index": "b44f03d68..ac3eca7c9 100644",
        "commit_message": "Fixed style\n\n",
        "file": "PySyft.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def testLog():",
            "",
            "Ptensor = PolynomialTensor()",
            "",
            "-    x = torch.randn(50000)",
            "-",
            "+    x = torch.randn(50000)",
            "+",
            "print(x)",
            "",
            "m = torch.nn.tanh()",
            "ten = m(x)",
            "",
            "assert (EvalError(ten, Ptensor.log(x))) == True",
            "-",
            "-testExp()",
            "",
            "+",
            "+testExp()"
        ],
        "hunk_index": 4,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 3952,
        "neg_line": [
            "-x = torch.randn(50000)",
            "-",
            "-",
            "-testExp()"
        ],
        "pos_line": [
            "+x = torch.randn(50000)",
            "+",
            "+",
            "+testExp()"
        ],
        "core_change": "-x = torch.randn(50000) - +x = torch.randn(50000) + - -testExp() + +testExp()",
        "core_API": "randn"
    },
    {
        "commit_hash": "f223bf1e246bca5c063d77194b2e55e8fb6af0ed",
        "index": "fc62e3c9..fab8e7ce 100644",
        "commit_message": "fixed categorical bug\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Bernoulli(Distribution):",
            "Reparameterized Bernoulli sampler.",
            "\"\"\"",
            "_ps = self._sanitize_input(ps)",
            "-        return torch.bernoulli(_ps)",
            "+        return torch.bernoulli(_ps).type_as(_ps)",
            "",
            "def log_pdf(self, x, ps=None, batch_size=1, *args, **kwargs):",
            "\"\"\""
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"\"\"\n_ps = self._sanitize_input(ps)\n        return torch.bernoulli(_ps)\n\ndef log_pdf(self, x, ps=None, batch_size=1, *args, **kwargs):\n\"\"\"), value='\"\"\"\\n_ps = self._sanitize_input(ps)\\n        return torch.bernoulli(_ps).type_as(_ps)\\n\\ndef log_pdf(self, x, ps=None, batch_size=1, *args, **kwargs):\\n\"\"\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 3953,
        "neg_line": [
            "-return torch.bernoulli(_ps)"
        ],
        "pos_line": [
            "+return torch.bernoulli(_ps).type_as(_ps)"
        ],
        "core_change": "-return torch.bernoulli(_ps) +return torch.bernoulli(_ps).type_as(_ps)",
        "core_API": "_sanitize_input"
    },
    {
        "commit_hash": "0c166dcfb8b9e9d79d013b7967a59dfb1ac4b804",
        "index": "96045492..19a75efe 100644",
        "commit_message": "define and apply YAPF formatting  (#1039)\n\n* define YAPF\n\n* yapf kornia -rip\n\n* yapf examples -rip\n\n* yapf test -rip\n\n* Update CONTRIBUTING.rst\n\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\n\n* update config\n\n* fix 4lint\n\n* mypy\n\n* mypy\n\n* mypy\n\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def normalize_min_max(x: torch.Tensor, min_val: float = 0., max_val: float = 1.,",
            "x_min: torch.Tensor = x.view(B, C, -1).min(-1)[0].view(B, C, 1)",
            "x_max: torch.Tensor = x.view(B, C, -1).max(-1)[0].view(B, C, 1)",
            "",
            "-    x_out: torch.Tensor = (",
            "-        (max_val - min_val) * (x.view(B, C, -1) - x_min) / (x_max - x_min + eps) + min_val",
            "-    )",
            "+    x_out: torch.Tensor = ((max_val - min_val) * (x.view(B, C, -1) - x_min) / (x_max - x_min + eps) + min_val)",
            "return x_out.view(shape)"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 17,
        "number": 3955,
        "neg_line": [
            "-x_out: torch.Tensor = (",
            "-(max_val - min_val) * (x.view(B, C, -1) - x_min) / (x_max - x_min + eps) + min_val",
            "-)"
        ],
        "pos_line": [
            "+x_out: torch.Tensor = ((max_val - min_val) * (x.view(B, C, -1) - x_min) / (x_max - x_min + eps) + min_val)"
        ],
        "core_change": "-x_out: torch.Tensor = ( -(max_val - min_val) * (x.view(B, C, -1) - x_min) / (x_max - x_min + eps) + min_val -) +x_out: torch.Tensor = ((max_val - min_val) * (x.view(B, C, -1) - x_min) / (x_max - x_min + eps) + min_val)",
        "core_API": "view"
    },
    {
        "commit_hash": "48a3208c39d9d4e5fcd5dd2942a9e8751cfe2674",
        "index": "978fd7a1..0fb3019c 100644",
        "commit_message": "Fixed nightly errors (#886)\n\n\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestRandomPerspective:",
            "assert out_perspective[0].shape == x_data.shape",
            "assert out_perspective[1].shape == (1, 3, 3)",
            "assert_allclose(out_perspective[0], x_data)",
            "-        assert_allclose(out_perspective[1], torch.eye(3, device=device))",
            "+        assert_allclose(out_perspective[1], torch.eye(3, device=device)[None])",
            "",
            "def test_transform_module_should_return_expected_transform(self, device):",
            "torch.manual_seed(0)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('subscript', None), position=3, insert_id=429618)",
            "Move(target_node=IN(type=subscript), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=429619)",
            "Insert(target_node=IN(type=subscript), node=('none', 'None'), position=2, insert_id=429620)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=429621)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 3960,
        "neg_line": [
            "-assert_allclose(out_perspective[1], torch.eye(3, device=device))"
        ],
        "pos_line": [
            "+assert_allclose(out_perspective[1], torch.eye(3, device=device)[None])"
        ],
        "core_change": "-assert_allclose(out_perspective[1], torch.eye(3, device=device)) +assert_allclose(out_perspective[1], torch.eye(3, device=device)[None])",
        "core_API": "eye"
    },
    {
        "commit_hash": "8915849ed89e033019e1a648fafc89221d335590",
        "index": "13f19f97..df940ef7 100644",
        "commit_message": "bugfix: persist collection order\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def average_grads(all_grads):",
            "for grad_and_vars in zip(*all_grads):",
            "# Ngpu * 2",
            "v = grad_and_vars[0][1]",
            "-            all_grads = [g for (g, _) in grad_and_vars]",
            "+            grads = [g for (g, _) in grad_and_vars]",
            "",
            "with tf.device(v.device):       # colocate summed grad with var",
            "grad = tf.multiply(",
            "-                    tf.add_n(all_grads), 1.0 / nr_tower)",
            "+                    tf.add_n(grads), 1.0 / nr_tower)",
            "ret.append((grad, v))",
            "return ret"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=all_grads), value='grads')",
            "Update(target_node=ASTNode(type=identifier, text=all_grads), value='grads')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 3963,
        "neg_line": [
            "-all_grads = [g for (g, _) in grad_and_vars]",
            "-tf.add_n(all_grads), 1.0 / nr_tower)"
        ],
        "pos_line": [
            "+grads = [g for (g, _) in grad_and_vars]",
            "+tf.add_n(grads), 1.0 / nr_tower)"
        ],
        "core_change": "-all_grads = [g for (g, _) in grad_and_vars] +grads = [g for (g, _) in grad_and_vars] -tf.add_n(all_grads), 1.0 / nr_tower) +tf.add_n(grads), 1.0 / nr_tower)",
        "core_API": "device"
    },
    {
        "commit_hash": "8e5e4892af72062a7c3f7fd890caefdcf19bab28",
        "index": "7239988..71618ce 100644",
        "commit_message": "fix seed generator\n\n",
        "file": "lama-cleaner.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SD(DiffusionInpaintModel):",
            "callback=self.callback,",
            "height=img_h,",
            "width=img_w,",
            "+            generator=torch.manual_seed(config.sd_seed)",
            ").images[0]",
            "",
            "output = (output * 255).round().astype(\"uint8\")"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('assignment', None), position=2, insert_id=481860)",
            "Insert(target_node=IN(type=assignment), node=('pattern_list', None), position=0, insert_id=481861)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=481862)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=481863)",
            "Move(target_node=IN(type=pattern_list), node=ASTNode(type=identifier, text=img_w), position=0)",
            "Move(target_node=IN(type=pattern_list), node=ASTNode(type=,, text=,), position=1)",
            "Insert(target_node=IN(type=pattern_list), node=('identifier', 'generator'), position=2, insert_id=481864)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=481865)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=481866)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=481867)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=481868)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'manual_seed'), position=2, insert_id=481869)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=481870)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=481871)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=481872)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'config'), position=0, insert_id=481873)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=481874)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'sd_seed'), position=2, insert_id=481875)",
            "Delete(target_node=ASTNode(type=expression_list))"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 19,
        "number": 3966,
        "neg_line": [],
        "pos_line": [
            "+generator=torch.manual_seed(config.sd_seed)"
        ],
        "core_change": "+generator=torch.manual_seed(config.sd_seed)",
        "core_API": "manual_seed"
    },
    {
        "commit_hash": "274d850d340c0004ee63627c88c433d0e55f5876",
        "index": "cda4c1fc2..e4bbc2d14 100644",
        "commit_message": "Fix #4098\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Attention(nn.Module):",
            "w = w / (v.size(-1) ** 0.5)",
            "nd, ns = w.size(-2), w.size(-1)",
            "mask = self.bias[:, :, ns - nd : ns, :ns]",
            "-        w = torch.where(mask, w, self.masked_bias)",
            "+        w = torch.where(mask, w, self.masked_bias.to(w.dtype))",
            "",
            "if attention_mask is not None:",
            "# Apply the attention mask"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=5, insert_id=1239255)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=6, insert_id=1239256)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1239257)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1239258)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1239259)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to'), position=2, insert_id=1239260)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1239261)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=1239262)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=), text=)), position=2)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'w'), position=0, insert_id=1239263)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1239264)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dtype'), position=2, insert_id=1239265)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 13,
        "number": 3968,
        "neg_line": [
            "-w = torch.where(mask, w, self.masked_bias)"
        ],
        "pos_line": [
            "+w = torch.where(mask, w, self.masked_bias.to(w.dtype))"
        ],
        "core_change": "-w = torch.where(mask, w, self.masked_bias) +w = torch.where(mask, w, self.masked_bias.to(w.dtype))",
        "core_API": "size"
    },
    {
        "commit_hash": "d1c16a9387b8f7de51c7124214b6bde2dd8870ca",
        "index": "eb1de7959..dfbcd20a8 100644",
        "commit_message": "fix: Highway network layer fixed\n\n",
        "file": "DeepPavlov.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def highway_convolutional_network(input_units,",
            "kernel_initializer=xavier_initializer())",
            "if use_batch_norm:",
            "units = tf.layers.batch_normalization(units, training=training_ph)",
            "-        sigmoid_gate = tf.layers.dense(input_units, activation=tf.sigmoid, kernel_initializer=xavier_initializer())",
            "+        sigmoid_gate = tf.layers.dense(input_units, 1, activation=tf.sigmoid, kernel_initializer=xavier_initializer())",
            "input_units = sigmoid_gate * input_units + (1 - sigmoid_gate) * units",
            "input_units = tf.nn.relu(input_units)",
            "return input_units"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('integer', '1'), position=3, insert_id=1924634)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=1924635)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 3969,
        "neg_line": [
            "-sigmoid_gate = tf.layers.dense(input_units, activation=tf.sigmoid, kernel_initializer=xavier_initializer())"
        ],
        "pos_line": [
            "+sigmoid_gate = tf.layers.dense(input_units, 1, activation=tf.sigmoid, kernel_initializer=xavier_initializer())"
        ],
        "core_change": "-sigmoid_gate = tf.layers.dense(input_units, activation=tf.sigmoid, kernel_initializer=xavier_initializer()) +sigmoid_gate = tf.layers.dense(input_units, 1, activation=tf.sigmoid, kernel_initializer=xavier_initializer())",
        "core_API": "batch_normalization"
    },
    {
        "commit_hash": "76c77f8447358be6bbd5eec41389171546394e2e",
        "index": "f0803ae20..aa3c0ac64 100644",
        "commit_message": "replace remote_torch.Tensor([R]) with R.reshape(1) to fix proxy error\n\n",
        "file": "PySyft.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "\"        \\n\",",
            "\"        # calculate critic (value) loss using L1 smooth loss\\n\",",
            "\"        value_losses.append(remote_torch.nn.functional.smooth_l1_loss(value,\\n\",",
            "-    \"                                                                remote_torch.Tensor([R])))\\n\",",
            "+    \"                                                                R.reshape(1)))\\n\",",
            "\"    # reset gradients    \\n\",",
            "\"    optimizer.zero_grad()\\n\",",
            "\"    \\n\","
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"                                                                remote_torch.Tensor([R])))\\n\"), value='\"                                                                R.reshape(1)))\\\\n\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 3971,
        "neg_line": [
            "-\"                                                                remote_torch.Tensor([R])))\\n\","
        ],
        "pos_line": [
            "+\"                                                                R.reshape(1)))\\n\","
        ],
        "core_change": "-\"                                                                remote_torch.Tensor([R])))\\n\", +\"                                                                R.reshape(1)))\\n\",",
        "core_API": "append"
    },
    {
        "commit_hash": "0c166dcfb8b9e9d79d013b7967a59dfb1ac4b804",
        "index": "3327d065..ae7fa7ef 100644",
        "commit_message": "define and apply YAPF formatting  (#1039)\n\n* define YAPF\n\n* yapf kornia -rip\n\n* yapf examples -rip\n\n* yapf test -rip\n\n* Update CONTRIBUTING.rst\n\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\n\n* update config\n\n* fix 4lint\n\n* mypy\n\n* mypy\n\n* mypy\n\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def elastic_transform2d(image: torch.Tensor,",
            "# Warp image based on displacement matrix",
            "b, c, h, w = image.shape",
            "grid = kornia.utils.create_meshgrid(h, w, device=image.device).to(image.dtype)",
            "-    warped = F.grid_sample(",
            "-        image, (grid + disp).clamp(-1, 1), align_corners=align_corners, mode=mode)",
            "+    warped = F.grid_sample(image, (grid + disp).clamp(-1, 1), align_corners=align_corners, mode=mode)",
            "",
            "return warped"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 3974,
        "neg_line": [
            "-warped = F.grid_sample(",
            "-image, (grid + disp).clamp(-1, 1), align_corners=align_corners, mode=mode)"
        ],
        "pos_line": [
            "+warped = F.grid_sample(image, (grid + disp).clamp(-1, 1), align_corners=align_corners, mode=mode)"
        ],
        "core_change": "-warped = F.grid_sample( -image, (grid + disp).clamp(-1, 1), align_corners=align_corners, mode=mode) +warped = F.grid_sample(image, (grid + disp).clamp(-1, 1), align_corners=align_corners, mode=mode)",
        "core_API": "create_meshgrid"
    },
    {
        "commit_hash": "aaebe83d7e4ce25165cb069835365b0049d41f18",
        "index": "2144a18cb7..27f6c77bde 100644",
        "commit_message": "Formatting fixes to meshgrid on all backends (#7579)\n\nCo-authored-by: @simonetgordon <simonegordon12@icloud.com>\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def linspace(",
            "",
            "@with_unsupported_dtypes({\"2.9.1 and below\": (\"bool\",)}, backend_version)",
            "def meshgrid(",
            "-    *arrays: Union[tf.Tensor, tf.Variable], sparse: bool = False, indexing: str = \"xy\"",
            "+    *arrays: Union[tf.Tensor, tf.Variable],",
            "+    sparse: bool = False,",
            "+    indexing: str = \"xy\",",
            ") -> List[Union[tf.Tensor, tf.Variable]]:",
            "if not sparse:",
            "return tf.meshgrid(*arrays, indexing=indexing)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=6, insert_id=1967490)"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 3975,
        "neg_line": [
            "-*arrays: Union[tf.Tensor, tf.Variable], sparse: bool = False, indexing: str = \"xy\""
        ],
        "pos_line": [
            "+*arrays: Union[tf.Tensor, tf.Variable],",
            "+sparse: bool = False,",
            "+indexing: str = \"xy\","
        ],
        "core_change": "-*arrays: Union[tf.Tensor, tf.Variable], sparse: bool = False, indexing: str = \"xy\" +*arrays: Union[tf.Tensor, tf.Variable], +sparse: bool = False, +indexing: str = \"xy\",",
        "core_API": "meshgrid"
    },
    {
        "commit_hash": "6ab589583c86f265a8a226620cc1e87c1c2266f1",
        "index": "75bcb7f..e441b8e 100644",
        "commit_message": "Add colorstr() (#1887)\n\n* Add colorful()\n\n* update\n\n* newline fix\n\n* add git description\n\n* --always\n\n* update loss scaling\n\n* update loss scaling 2\n\n* rename to colorstr()\n",
        "file": "yolov5.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def select_device(device='', batch_size=None):",
            "p = torch.cuda.get_device_properties(i)",
            "s += f\"{'' if i == 0 else space}CUDA:{d} ({p.name}, {p.total_memory / 1024 ** 2}MB)\\n\"  # bytes to MB",
            "else:",
            "-        s += 'CPU'",
            "+        s += 'CPU\\n'",
            "",
            "-    logger.info(f'{s}\\n')  # skip a line",
            "+    logger.info(s)  # skip a line",
            "return torch.device('cuda:0' if cuda else 'cpu')"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text='CPU'), value=\"'CPU\\\\n'\")",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 's'), position=1, insert_id=1299586)",
            "Delete(target_node=ASTNode(type=string, text=f'{s}\\n'))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 3,
        "number": 3976,
        "neg_line": [
            "-s += 'CPU'",
            "-logger.info(f'{s}\\n')  # skip a line"
        ],
        "pos_line": [
            "+s += 'CPU\\n'",
            "+logger.info(s)  # skip a line"
        ],
        "core_change": "-s += 'CPU' +s += 'CPU\\n' -logger.info(f'{s}\\n')  # skip a line +logger.info(s)  # skip a line",
        "core_API": "get_device_properties"
    },
    {
        "commit_hash": "95b6d985ffb231d9a2047c039c95660913063b9d",
        "index": "f2eed3a7..61b85563 100644",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestCnnHighwayEncoder(AllenNlpTestCase):",
            "encoder = TimeDistributed(encoder)",
            "",
            "embedding = torch.from_numpy(np.random.randn(5, 6, 50, 4)).float()",
            "-        mask = torch.ones(5, 6, 50).long()",
            "+        mask = torch.ones(5, 6, 50).bool()",
            "token_embedding = encoder(embedding, mask)",
            "",
            "assert list(token_embedding.size()) == [5, 6, 16]"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=long), value='bool')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 3978,
        "neg_line": [
            "-mask = torch.ones(5, 6, 50).long()"
        ],
        "pos_line": [
            "+mask = torch.ones(5, 6, 50).bool()"
        ],
        "core_change": "-mask = torch.ones(5, 6, 50).long() +mask = torch.ones(5, 6, 50).bool()",
        "core_API": "from_numpy"
    },
    {
        "commit_hash": "d89453018099062ad0a15d6df2db9a3c65fd3a1a",
        "index": "1f56117..41b6197 100644",
        "commit_message": "Fix(core): fix memory leak issue and switch to subprocess backend (#216)\n\n* fix RAM leak error\n\n* fix another memory leak\n\n* update boxes.py\n\n* make flake8 happy\n\n",
        "file": "YOLOX.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def main():",
            "log_level=trt.Logger.INFO,",
            "max_workspace_size=(1 << 32),",
            ")",
            "-    torch.save(model_trt.state_dict(), os.path.join(file_name, 'model_trt.pth'))",
            "+    torch.save(model_trt.state_dict(), os.path.join(file_name, \"model_trt.pth\"))",
            "logger.info(\"Converted TensorRT model done.\")",
            "-    engine_file = os.path.join(file_name, 'model_trt.engine')",
            "-    engine_file_demo = os.path.join('demo', 'TensorRT', 'cpp', 'model_trt.engine')",
            "-    with open(engine_file, 'wb') as f:",
            "+    engine_file = os.path.join(file_name, \"model_trt.engine\")",
            "+    engine_file_demo = os.path.join(\"demo\", \"TensorRT\", \"cpp\", \"model_trt.engine\")",
            "+    with open(engine_file, \"wb\") as f:",
            "f.write(model_trt.engine.serialize())",
            "",
            "shutil.copyfile(engine_file, engine_file_demo)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1307210)",
            "Update(target_node=ASTNode(type=string, text='model_trt.engine'), value='\"model_trt.engine\"')",
            "Update(target_node=ASTNode(type=string, text='demo'), value='\"demo\"')",
            "Update(target_node=ASTNode(type=string, text='TensorRT'), value='\"TensorRT\"')",
            "Update(target_node=ASTNode(type=string, text='cpp'), value='\"cpp\"')",
            "Update(target_node=ASTNode(type=string, text='model_trt.engine'), value='\"model_trt.engine\"')",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1307211)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1307212)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'join'), position=2, insert_id=1307213)",
            "Update(target_node=ASTNode(type=string, text='model_trt.pth'), value='\"model_trt.pth\"')",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'os'), position=0, insert_id=1307214)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1307215)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'path'), position=2, insert_id=1307216)",
            "Update(target_node=ASTNode(type=string, text='wb'), value='\"wb\"')",
            "Delete(target_node=ASTNode(type=identifier, text=os))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=path))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=join))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 4,
        "minus_line": 4,
        "AST_diff_line": 22,
        "number": 3979,
        "neg_line": [
            "-torch.save(model_trt.state_dict(), os.path.join(file_name, 'model_trt.pth'))",
            "-engine_file = os.path.join(file_name, 'model_trt.engine')",
            "-engine_file_demo = os.path.join('demo', 'TensorRT', 'cpp', 'model_trt.engine')",
            "-with open(engine_file, 'wb') as f:"
        ],
        "pos_line": [
            "+torch.save(model_trt.state_dict(), os.path.join(file_name, \"model_trt.pth\"))",
            "+engine_file = os.path.join(file_name, \"model_trt.engine\")",
            "+engine_file_demo = os.path.join(\"demo\", \"TensorRT\", \"cpp\", \"model_trt.engine\")",
            "+with open(engine_file, \"wb\") as f:"
        ],
        "core_change": "-torch.save(model_trt.state_dict(), os.path.join(file_name, 'model_trt.pth')) +torch.save(model_trt.state_dict(), os.path.join(file_name, \"model_trt.pth\")) -engine_file = os.path.join(file_name, 'model_trt.engine') -engine_file_demo = os.path.join('demo', 'TensorRT', 'cpp', 'model_trt.engine') -with open(engine_file, 'wb') as f: +engine_file = os.path.join(file_name, \"model_trt.engine\") +engine_file_demo = os.path.join(\"demo\", \"TensorRT\", \"cpp\", \"model_trt.engine\") +with open(engine_file, \"wb\") as f:",
        "core_API": "save"
    },
    {
        "commit_hash": "4b9d169989cad88775e958beaa5c2836aaff7a4e",
        "index": "83342a45..af2585a5 100644",
        "commit_message": "Fixes\n\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "if K.backend() == 'cntk':",
            "supports_sparse = False",
            "elif K.backend() == 'theano' and not KTH.th_sparse_module:",
            "supports_sparse = False",
            "-else:",
            "-    supports_sparse = True",
            "+elif K.backend() == 'tensorflow':",
            "+    # Must wait for tf.keras to support sparse ops.",
            "+    supports_sparse = False",
            "",
            "",
            "def check_dtype(var, dtype):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('ERROR', None), position=4, insert_id=2110626)",
            "Insert(target_node=IN(type=ERROR), node=('comparison_operator', None), position=0, insert_id=2110627)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type==, text==), position=1)",
            "Insert(target_node=IN(type=ERROR), node=('false', 'False'), position=2, insert_id=2110628)",
            "Insert(target_node=IN(type=comparison_operator), node=('call', None), position=0, insert_id=2110629)",
            "Insert(target_node=IN(type=comparison_operator), node=('==', '=='), position=1, insert_id=2110630)",
            "Insert(target_node=IN(type=comparison_operator), node=('ERROR', None), position=2, insert_id=2110631)",
            "Move(target_node=IN(type=comparison_operator), node=ASTNode(type=identifier, text=supports_sparse), position=3)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2110632)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2110633)",
            "Insert(target_node=IN(type=ERROR), node=('string', \"'tensorflow'\"), position=0, insert_id=2110634)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=:, text=:), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=else), value='elif')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=else), position=0)",
            "Insert(target_node=IN(type=attribute), node=('ERROR', None), position=1, insert_id=2110635)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=2, insert_id=2110636)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'backend'), position=3, insert_id=2110637)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2110638)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=2110639)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'K'), position=0, insert_id=2110640)",
            "Delete(target_node=ASTNode(type=type))",
            "Delete(target_node=ASTNode(type=true, text=True))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 24,
        "number": 3981,
        "neg_line": [
            "-else:",
            "-supports_sparse = True"
        ],
        "pos_line": [
            "+elif K.backend() == 'tensorflow':",
            "+# Must wait for tf.keras to support sparse ops.",
            "+supports_sparse = False"
        ],
        "core_change": "-else: -supports_sparse = True +elif K.backend() == 'tensorflow': +# Must wait for tf.keras to support sparse ops. +supports_sparse = False",
        "core_API": "backend"
    },
    {
        "commit_hash": "cde8accb0ebc4f6b82c1dc71c806039386fdbe25",
        "index": "497a34b16f..b0fa0534da 100644",
        "commit_message": "small fix to tensorflow variable function.\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "from ivy.container import Container",
            "",
            "",
            "def variable(x):",
            "-    with ivy.dev(x, as_native=True):",
            "+    with tf.device(ivy.dev(x, as_native=True)):",
            "return tf.Variable(x, trainable=True)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2008822)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=2008823)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2008824)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2008825)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=2008826)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2008827)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=call), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2008828)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 3982,
        "neg_line": [
            "-with ivy.dev(x, as_native=True):"
        ],
        "pos_line": [
            "+with tf.device(ivy.dev(x, as_native=True)):"
        ],
        "core_change": "-with ivy.dev(x, as_native=True): +with tf.device(ivy.dev(x, as_native=True)):",
        "core_API": "dev"
    },
    {
        "commit_hash": "5f4c6e830f603830117877b8990a0e65a2386aa6",
        "index": "c7414cc12..12b95b928 100644",
        "commit_message": "Quick fix :) (#606)\n\n* Changing the name\n\n* style + quality\n\n* update doc and logo\n\n* clean up\n\n* circle-CI on the branche for now\n\n* fix daily dialog dataset\n\n* fix urls\n\nCo-authored-by: Quentin Lhoest <lhoest.q@gmail.com>\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class MetricInfo:",
            "for key, value in self.features.items():",
            "if not isinstance(value, Value):",
            "raise ValueError(",
            "-                        f\"When using 'numpy' format, all features should be a `nlp.Value` feature. \"",
            "+                        f\"When using 'numpy' format, all features should be a `datasets.Value` feature. \"",
            "f\"Here {key} is an instance of {value.__class__.__name__}\"",
            ")"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=f\"When using 'numpy' format, all features should be a `nlp.Value` feature. \"), value='f\"When using \\'numpy\\' format, all features should be a `datasets.Value` feature. \"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 3983,
        "neg_line": [
            "-f\"When using 'numpy' format, all features should be a `nlp.Value` feature. \""
        ],
        "pos_line": [
            "+f\"When using 'numpy' format, all features should be a `datasets.Value` feature. \""
        ],
        "core_change": "-f\"When using 'numpy' format, all features should be a `nlp.Value` feature. \" +f\"When using 'numpy' format, all features should be a `datasets.Value` feature. \"",
        "core_API": "items"
    },
    {
        "commit_hash": "8c7bf5fa6af7cc9244ae1dc5dbf1b88605719f15",
        "index": "7cef8fd1e..cb463326a 100644",
        "commit_message": "Fix training/eval mode\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class E2E(STInterface, torch.nn.Module):",
            "att_ws = self.dec.calculate_all_attentions(",
            "hpad, hlens, ys_pad, lang_ids=tgt_lang_ids",
            ")",
            "-",
            "+        self.train()",
            "return att_ws",
            "",
            "def subsample_frames(self, x):"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=2, insert_id=153207)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=153208)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=153209)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=153210)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=153211)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=153212)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'train'), position=2, insert_id=153213)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=153214)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=153215)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 9,
        "number": 3986,
        "neg_line": [
            "-"
        ],
        "pos_line": [
            "+self.train()"
        ],
        "core_change": "- +self.train()",
        "core_API": "calculate_all_attentions"
    },
    {
        "commit_hash": "5d271cf4b182355da9fc83cd2d3d4d7574472f67",
        "index": "48ca0c79..274b4e18 100644",
        "commit_message": "Fix tests which will break once MirroredStrategy switch to collective ops\n\nPiperOrigin-RevId: 515152548\n\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "from keras.testing_infra import test_utils",
            "def create_mirrored_strategy():",
            "# The test creates two virtual CPUs, and we use both of them to test with",
            "# multiple devices.",
            "+    # pylint: disable=protected-access",
            "+    tf.distribute.MirroredStrategy._collective_key_base += 1",
            "return tf.distribute.MirroredStrategy([\"cpu:0\", \"cpu:1\"])"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('block', None), position=0, insert_id=2040811)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=2040812)",
            "Insert(target_node=IN(type=expression_statement), node=('augmented_assignment', None), position=0, insert_id=2040813)",
            "Insert(target_node=IN(type=augmented_assignment), node=('attribute', None), position=0, insert_id=2040814)",
            "Insert(target_node=IN(type=augmented_assignment), node=('+=', '+='), position=1, insert_id=2040815)",
            "Insert(target_node=IN(type=augmented_assignment), node=('integer', '1'), position=2, insert_id=2040816)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2040817)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2040818)",
            "Insert(target_node=IN(type=attribute), node=('identifier', '_collective_key_base'), position=2, insert_id=2040819)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2040820)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2040821)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'MirroredStrategy'), position=2, insert_id=2040822)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2040823)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2040824)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'distribute'), position=2, insert_id=2040825)",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 2,
        "minus_line": 0,
        "AST_diff_line": 16,
        "number": 3987,
        "neg_line": [],
        "pos_line": [
            "+# pylint: disable=protected-access",
            "+tf.distribute.MirroredStrategy._collective_key_base += 1"
        ],
        "core_change": "+# pylint: disable=protected-access +tf.distribute.MirroredStrategy._collective_key_base += 1",
        "core_API": "MirroredStrategy"
    },
    {
        "commit_hash": "6ad93dd7b54c7cfff02b586f60c025f1d672ed1e",
        "index": "e9c31e96a..611808596 100644",
        "commit_message": "fix import issue\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_frontend_backward_multi_channel(train, use_wpe, use_beamformer):",
            "frontend.train()",
            "else:",
            "frontend.eval()",
            "-    torch.random.manual_seed(14)",
            "+    set_all_random_seed(14)",
            "x = torch.randn(2, 1000, 2, requires_grad=True)",
            "x_lengths = torch.LongTensor([1000, 980])",
            "y, y_lengths = frontend(x, x_lengths)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=torch), value='set_all_random_seed')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=torch), position=0)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=random))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=manual_seed))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 3988,
        "neg_line": [
            "-torch.random.manual_seed(14)"
        ],
        "pos_line": [
            "+set_all_random_seed(14)"
        ],
        "core_change": "-torch.random.manual_seed(14) +set_all_random_seed(14)",
        "core_API": "train"
    },
    {
        "commit_hash": "ddfde0f138a7effd99090d4f4239fbba840cb079",
        "index": "a2e714a..355a3a6 100644",
        "commit_message": "Fix: Use .size to get integer dimension\n\n",
        "file": "gpt-neo.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def attn(x, scope, n_state, *, past, params, block_offset=0, train=False):",
            "return mtf.transformer.attention.visibility_mask_to_attention_bias(vis, dtype)",
            "",
            "with tf.variable_scope(scope):",
            "-        dim_qkv = mtf.Dimension(\"qkv\", n_state * 3)",
            "+        dim_qkv = mtf.Dimension(\"qkv\", n_state.size * 3)",
            "c = conv1d(x, 'c_attn', dim_qkv, params=params)",
            "",
            "conv_output_channels = c.shape[2]  # should be equal to dim_qkv"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=binary_operator), node=('attribute', None), position=0, insert_id=1942968)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=n_state), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1942969)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'size'), position=2, insert_id=1942970)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 3989,
        "neg_line": [
            "-dim_qkv = mtf.Dimension(\"qkv\", n_state * 3)"
        ],
        "pos_line": [
            "+dim_qkv = mtf.Dimension(\"qkv\", n_state.size * 3)"
        ],
        "core_change": "-dim_qkv = mtf.Dimension(\"qkv\", n_state * 3) +dim_qkv = mtf.Dimension(\"qkv\", n_state.size * 3)",
        "core_API": "visibility_mask_to_attention_bias"
    },
    {
        "commit_hash": "7ba37885c67844d2eb18a63dcf3b7ac7f66ce89f",
        "index": "b2dacd40bc..d479d4a51f 100644",
        "commit_message": "Cast rewards as tf.float32 to fix error in DQN in tf2 (#28384)\n\n* Cast rewards as tf.float32 to fix error in DQN in tf2\n\nSigned-off-by: mgerstgrasser <matthias@gerstgrasser.net>\n\n* Add test case for DQN with integer rewards\n\nSigned-off-by: mgerstgrasser <matthias@gerstgrasser.net>\n\nSigned-off-by: mgerstgrasser <matthias@gerstgrasser.net>\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def build_q_losses(policy: Policy, model, _, train_batch: SampleBatch) -> Tensor",
            "q_tp1_best,",
            "q_dist_tp1_best,",
            "train_batch[PRIO_WEIGHTS],",
            "-        train_batch[SampleBatch.REWARDS],",
            "+        tf.cast(train_batch[SampleBatch.REWARDS], tf.float32),",
            "tf.cast(train_batch[SampleBatch.DONES], tf.float32),",
            "config[\"gamma\"],",
            "config[\"n_step\"],"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=type), node=('call', None), position=0, insert_id=2133889)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2133890)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2133891)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2133892)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2133893)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cast'), position=2, insert_id=2133894)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2133895)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=subscript), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=2133896)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=3, insert_id=2133897)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=2133898)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2133899)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2133900)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'float32'), position=2, insert_id=2133901)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 14,
        "number": 3991,
        "neg_line": [
            "-train_batch[SampleBatch.REWARDS],"
        ],
        "pos_line": [
            "+tf.cast(train_batch[SampleBatch.REWARDS], tf.float32),"
        ],
        "core_change": "-train_batch[SampleBatch.REWARDS], +tf.cast(train_batch[SampleBatch.REWARDS], tf.float32),",
        "core_API": "cast"
    },
    {
        "commit_hash": "e153e3179f54819e06c07df21bbf49e260dec5f2",
        "index": "5edc941cd3..892ecf0290 100644",
        "commit_message": "[RLlib] Exploration API: Policy changes needed for forward pass noisifications. (#7798)\n\n* Rollback.\n\n* WIP.\n\n* WIP.\n\n* LINT.\n\n* WIP.\n\n* Fix.\n\n* Fix.\n\n* Fix.\n\n* LINT.\n\n* Fix (SAC does currently not support eager).\n\n* Fix.\n\n* WIP.\n\n* LINT.\n\n* Update rllib/evaluation/sampler.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* Update rllib/evaluation/sampler.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* Update rllib/utils/exploration/exploration.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* Update rllib/utils/exploration/exploration.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* WIP.\n\n* WIP.\n\n* Fix.\n\n* LINT.\n\n* LINT.\n\n* Fix and LINT.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* Fix.\n\n* LINT.\n\n* Fix.\n\n* Fix and LINT.\n\n* Update rllib/utils/exploration/exploration.py\n\n* Update rllib/policy/dynamic_tf_policy.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* Update rllib/policy/dynamic_tf_policy.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* Update rllib/policy/dynamic_tf_policy.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* Fixes.\n\n* LINT.\n\n* WIP.\n\nCo-authored-by: Eric Liang <ekhliang@gmail.com>\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Deterministic(TFActionDistribution):",
            "return self.inputs",
            "",
            "@override(TFActionDistribution)",
            "-    def sampled_action_logp(self):",
            "-        return 0.0",
            "+    def logp(self, x):",
            "+        return tf.zeros_like(self.inputs)",
            "",
            "@override(TFActionDistribution)",
            "def _build_sample_op(self):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=sampled_action_logp), value='logp')",
            "Insert(target_node=ASTNode(type=parameters), node=('(', '('), position=0, insert_id=2146680)",
            "Insert(target_node=ASTNode(type=parameters), node=('identifier', 'self'), position=1, insert_id=2146681)",
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=2, insert_id=2146682)",
            "Insert(target_node=ASTNode(type=parameters), node=('identifier', 'x'), position=3, insert_id=2146683)",
            "Insert(target_node=ASTNode(type=parameters), node=(')', ')'), position=4, insert_id=2146684)",
            "Insert(target_node=ASTNode(type=return_statement), node=('call', None), position=1, insert_id=2146685)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2146686)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2146687)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2146688)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2146689)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'zeros_like'), position=2, insert_id=2146690)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=(, text=(), position=0)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=2146691)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=), text=)), position=2)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=self), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2146692)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'inputs'), position=2, insert_id=2146693)",
            "Delete(target_node=ASTNode(type=float, text=0.0))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 19,
        "number": 3992,
        "neg_line": [
            "-def sampled_action_logp(self):",
            "-return 0.0"
        ],
        "pos_line": [
            "+def logp(self, x):",
            "+return tf.zeros_like(self.inputs)"
        ],
        "core_change": "-def sampled_action_logp(self): -return 0.0 +def logp(self, x): +return tf.zeros_like(self.inputs)",
        "core_API": "zeros_like"
    },
    {
        "commit_hash": "da2f1325014a158be36d62bdd20293ef97e574f4",
        "index": "bf272055..41df71c8 100644",
        "commit_message": "enable black in the precommit (#1777)\n\n* enable black in the precommit\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add some fixes\n\n* update libface detection url\n\n* added url from kornia checkpoint\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def find_fundamental(points1: torch.Tensor, points2: torch.Tensor, weights: torc",
            "",
            "# reconstruct and force the matrix to have rank2",
            "U, S, V = torch.svd(F_mat)",
            "-    rank_mask = torch.tensor([1.0, 1.0, 0.0],",
            "-                             device=F_mat.device,",
            "-                             dtype=F_mat.dtype)",
            "+    rank_mask = torch.tensor([1.0, 1.0, 0.0], device=F_mat.device, dtype=F_mat.dtype)",
            "",
            "F_projected = U @ (torch.diag_embed(S * rank_mask) @ V.transpose(-2, -1))",
            "F_est = transform2.transpose(-2, -1) @ (F_projected @ transform1)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 17,
        "number": 3993,
        "neg_line": [
            "-rank_mask = torch.tensor([1.0, 1.0, 0.0],",
            "-device=F_mat.device,",
            "-dtype=F_mat.dtype)"
        ],
        "pos_line": [
            "+rank_mask = torch.tensor([1.0, 1.0, 0.0], device=F_mat.device, dtype=F_mat.dtype)"
        ],
        "core_change": "-rank_mask = torch.tensor([1.0, 1.0, 0.0], -device=F_mat.device, -dtype=F_mat.dtype) +rank_mask = torch.tensor([1.0, 1.0, 0.0], device=F_mat.device, dtype=F_mat.dtype)",
        "core_API": "svd"
    },
    {
        "commit_hash": "fec3112cbacf19cd211109b1bc0b33a502595aca",
        "index": "e326aec6..46c46028 100644",
        "commit_message": "Update to PyTorch 1.9.0 (#2887)\n\n* Update to PyTorch 1.9.0\n\n* Update torch.cholesky, torch.symeig\n\n* Fix torch.tensordot, torch.qr, funsor dependency\n\n* Fix torch import, AutoGuideList usage\n\n* Ignore bug in PyTorch jit\n\n* Ignore tracer warnings\n\n* Replace torch.solve with torch.linalg.solve\n\n* Xfail vectorized markov funsor tests\n\n* Update funsor version\n\n* Switch MNIST mirrors\n\n* Pin pillow version\n\n* Bump torchvision version\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_cholesky_transform(batch_shape, dim, transform):",
            "assert_close(log_det, torch.slogdet(jacobian)[1])",
            "",
            "assert log_det.shape == batch_shape",
            "-    assert_close(y, x_mat.cholesky())",
            "+    assert_close(y, torch.linalg.cholesky(x_mat))",
            "assert_close(transform.inv(y), x_mat)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=677153)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=677154)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=x_mat), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=677155)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'linalg'), position=2, insert_id=677156)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 4000,
        "neg_line": [
            "-assert_close(y, x_mat.cholesky())"
        ],
        "pos_line": [
            "+assert_close(y, torch.linalg.cholesky(x_mat))"
        ],
        "core_change": "-assert_close(y, x_mat.cholesky()) +assert_close(y, torch.linalg.cholesky(x_mat))",
        "core_API": "slogdet"
    },
    {
        "commit_hash": "7a9a08c5d3ca4699fb439f691c40e1320b37507a",
        "index": "082e0c617..a0054d179 100644",
        "commit_message": "Drop torch 1.6 testing (#10390)\n\n* Drop torch 1.6 support\n\n* Drop 1.6 support\n\n* Update CHANGELOG\n\n* Fixes\n\n* Split change\n\n* Undo change\n\n* 1.7 -> 1.7.1\n\nhttps://github.com/pytorch/pytorch/issues/47354\n\n* Force trigger nightly\n\n* Update .github/workflows/events-nightly.yml\n\nCo-authored-by: Aki Nitta <nitta@akihironitta.com>\n\n* Revert 1.7.1 change - try wildcard\n\n* Update adjust versions and test it\n\n* Undo test changes\n\n* Revert \"Undo test changes\"\n\nThis reverts commit 3a6acadd115e86f02d83a788f1978372ab6764f3.\n\n* Update CHANGELOG.md\n\nCo-authored-by: Aki Nitta <nitta@akihironitta.com>\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class LightningDistributed:",
            "if self.rank != 0:",
            "obj = [None] * len(obj)",
            "",
            "-        broadcast_object_list(obj, 0, group=group or _group.WORLD)",
            "+        torch.distributed.broadcast_object_list(obj, 0, group=group or _group.WORLD)",
            "",
            "return obj[0]"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=521100)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=521101)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=521102)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=broadcast_object_list), position=2)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=521103)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=521104)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'distributed'), position=2, insert_id=521105)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 4004,
        "neg_line": [
            "-broadcast_object_list(obj, 0, group=group or _group.WORLD)"
        ],
        "pos_line": [
            "+torch.distributed.broadcast_object_list(obj, 0, group=group or _group.WORLD)"
        ],
        "core_change": "-broadcast_object_list(obj, 0, group=group or _group.WORLD) +torch.distributed.broadcast_object_list(obj, 0, group=group or _group.WORLD)",
        "core_API": "broadcast_object_list"
    },
    {
        "commit_hash": "5d55ee34f53e5fed07363de4fd5429240f9f4401",
        "index": "b19b5cf7..dbb93dc4 100644",
        "commit_message": "GH-407: fix training on cuda without CRF\n\n",
        "file": "flair.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SequenceTagger(flair.nn.Model):",
            "for sentence_feats, sentence_tags, sentence_length in zip(features, tags, lengths):",
            "sentence_feats = sentence_feats[:sentence_length]",
            "",
            "-                # print(sentence_tags)",
            "-                # tag_tensor = torch.LongTensor(sentence_tags)",
            "-                # tag_tensor = tag_tensor.to(flair.device)",
            "-",
            "score += torch.nn.functional.cross_entropy(sentence_feats, sentence_tags)",
            "",
            "return score"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 0,
        "minus_line": 3,
        "AST_diff_line": 17,
        "number": 4006,
        "neg_line": [
            "-# print(sentence_tags)",
            "-# tag_tensor = torch.LongTensor(sentence_tags)",
            "-# tag_tensor = tag_tensor.to(flair.device)",
            "-"
        ],
        "pos_line": [],
        "core_change": "-# print(sentence_tags) -# tag_tensor = torch.LongTensor(sentence_tags) -# tag_tensor = tag_tensor.to(flair.device) -",
        "core_API": "LongTensor"
    },
    {
        "commit_hash": "2777264ee8d8b0241c5691120175286cbede76bf",
        "index": "815488d7..d676b87b 100644",
        "commit_message": "`enable_model_cpu_offload` (#2285)\n\n* enable_model_offload PoC\n\nIt's surprisingly more involved than expected, see comments in the PR.\n\n* Rename final_offload_hook\n\n* Invoke the vae forward hook manually.\n\n* Completely remove decoder.\n\n* Style\n\n* apply_forward_hook decorator\n\n* Rename method.\n\n* Style\n\n* Copy enable_model_cpu_offload\n\n* Fix copies.\n\n* Remove comment.\n\n* Fix copies\n\n* Missing import\n\n* Fix doc-builder style.\n\n* Merge main and fix again.\n\n* Add docs\n\n* Fix docs.\n\n* Add a couple of tests.\n\n* style\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class VersatileDiffusionImageVariationPipeline(DiffusionPipeline):",
            "`pipeline.enable_sequential_cpu_offload()` the execution device can only be inferred from Accelerate's module",
            "hooks.",
            "\"\"\"",
            "-        if self.device != torch.device(\"meta\") or not hasattr(self.image_unet, \"_hf_hook\"):",
            "+        if not hasattr(self.image_unet, \"_hf_hook\"):",
            "return self.device",
            "for module in self.image_unet.modules():",
            "if ("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 4008,
        "neg_line": [
            "-if self.device != torch.device(\"meta\") or not hasattr(self.image_unet, \"_hf_hook\"):"
        ],
        "pos_line": [
            "+if not hasattr(self.image_unet, \"_hf_hook\"):"
        ],
        "core_change": "-if self.device != torch.device(\"meta\") or not hasattr(self.image_unet, \"_hf_hook\"): +if not hasattr(self.image_unet, \"_hf_hook\"):",
        "core_API": "enable_sequential_cpu_offload"
    },
    {
        "commit_hash": "337cd46a6bbcfc65774969685843c184a93318c9",
        "index": "12341e8a2c..2501b7b859 100644",
        "commit_message": "lint fixes\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def remainder(",
            "return tf.math.floormod(x1, x2)",
            "",
            "",
            "-def round(",
            "-    x: Union[tf.Tensor, tf.Variable]",
            "-) -> Union[tf.Tensor, tf.Variable]:",
            "+def round(x: Union[tf.Tensor, tf.Variable]) -> Union[tf.Tensor, tf.Variable]:",
            "if \"int\" in str(x.dtype):",
            "return x",
            "else:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=parameters), node=('typed_parameter', None), position=2, insert_id=2006949)",
            "Move(target_node=ASTNode(type=parameters), node=ASTNode(type=), text=)), position=3)",
            "Insert(target_node=ASTNode(type=ERROR), node=('(', '('), position=9, insert_id=2006950)",
            "Move(target_node=IN(type=typed_parameter), node=ASTNode(type=identifier, text=x), position=0)",
            "Move(target_node=IN(type=typed_parameter), node=ASTNode(type=:, text=:), position=1)",
            "Insert(target_node=IN(type=typed_parameter), node=('type', None), position=2, insert_id=2006951)",
            "Move(target_node=IN(type=type), node=ASTNode(type=subscript), position=0)",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=tuple_pattern))",
            "Delete(target_node=ASTNode(type=), text=))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 4011,
        "neg_line": [
            "-def round(",
            "-x: Union[tf.Tensor, tf.Variable]",
            "-) -> Union[tf.Tensor, tf.Variable]:"
        ],
        "pos_line": [
            "+def round(x: Union[tf.Tensor, tf.Variable]) -> Union[tf.Tensor, tf.Variable]:"
        ],
        "core_change": "-def round( -x: Union[tf.Tensor, tf.Variable] -) -> Union[tf.Tensor, tf.Variable]: +def round(x: Union[tf.Tensor, tf.Variable]) -> Union[tf.Tensor, tf.Variable]:",
        "core_API": "floormod"
    },
    {
        "commit_hash": "0a85f1afff3be95b1708191eaa4de2ce66feac63",
        "index": "0e60d914..fbe76d11 100644",
        "commit_message": "fix error messages not appearing\n\n",
        "file": "stable-diffusion-webui.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "else:",
            "sd_model = sd_model.to(device)",
            "",
            "model_hijack = StableDiffusionModelHijack()",
            "-model_hijack.hijack(sd_model)",
            "+#model_hijack.hijack(sd_model)",
            "",
            "with open(os.path.join(script_path, \"style.css\"), \"r\", encoding=\"utf8\") as file:",
            "css = file.read()"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=model_hijack))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=hijack))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=sd_model))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 0,
        "minus_line": 0,
        "AST_diff_line": 10,
        "number": 4016,
        "neg_line": [
            "-model_hijack.hijack(sd_model)"
        ],
        "pos_line": [
            "+#model_hijack.hijack(sd_model)"
        ],
        "core_change": "-model_hijack.hijack(sd_model) +#model_hijack.hijack(sd_model)",
        "core_API": "to"
    },
    {
        "commit_hash": "a5cba88be438e89a6698cf002019533095e3993b",
        "index": "b8b4ec9977..133734f603 100644",
        "commit_message": "fix kw_only missmatch for diff function with out arg missing for jax, torch and tensorflow as opposed to np backend and ivy functional\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def diff(",
            "axis: Optional[int] = -1,",
            "prepend: Optional[Union[torch.Tensor, int, float, list, tuple]] = None,",
            "append: Optional[Union[torch.Tensor, int, float, list, tuple]] = None,",
            "+    out: Optional[torch.Tensor] = None,",
            ") -> torch.Tensor:",
            "x = x if type(x) == torch.Tensor else torch.Tensor(x)",
            "prepend = ("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=parameters), node=('typed_default_parameter', None), position=7, insert_id=271799)",
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=8, insert_id=271800)",
            "Insert(target_node=IN(type=typed_default_parameter), node=('identifier', 'out'), position=0, insert_id=271801)",
            "Insert(target_node=IN(type=typed_default_parameter), node=(':', ':'), position=1, insert_id=271802)",
            "Insert(target_node=IN(type=typed_default_parameter), node=('type', None), position=2, insert_id=271803)",
            "Insert(target_node=IN(type=typed_default_parameter), node=('=', '='), position=3, insert_id=271804)",
            "Insert(target_node=IN(type=typed_default_parameter), node=('none', 'None'), position=4, insert_id=271805)",
            "Insert(target_node=IN(type=type), node=('subscript', None), position=0, insert_id=271806)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'Optional'), position=0, insert_id=271807)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=271808)",
            "Insert(target_node=IN(type=subscript), node=('attribute', None), position=2, insert_id=271809)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=271810)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=271811)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=271812)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Tensor'), position=2, insert_id=271813)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 15,
        "number": 4017,
        "neg_line": [],
        "pos_line": [
            "+out: Optional[torch.Tensor] = None,"
        ],
        "core_change": "+out: Optional[torch.Tensor] = None,",
        "core_API": "Tensor"
    },
    {
        "commit_hash": "d7a3582e0d915fce9ecf12a8cf07da52aa61f096",
        "index": "5c897a43..037799a8 100644",
        "commit_message": "fixed broken tests due to tensor broadcasting\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Normal(Distribution):",
            "def _sanitize_input(self, mu, sigma):",
            "if mu is not None:",
            "# stateless distribution",
            "-            mu = torch.unsqueeze(mu, 1)",
            "return mu, sigma",
            "elif self.mu is not None:",
            "# stateful distribution"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('block', ''), position=0, insert_id=768524)",
            "Delete(target_node=ASTNode(type=identifier, text=mu))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=unsqueeze))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=mu))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=block))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 4018,
        "neg_line": [
            "-mu = torch.unsqueeze(mu, 1)"
        ],
        "pos_line": [],
        "core_change": "-mu = torch.unsqueeze(mu, 1)",
        "core_API": "unsqueeze"
    },
    {
        "commit_hash": "324d80ebda0794f7b316feaefdf1a97551f3589a",
        "index": "ce118ce4..413701d8 100644",
        "commit_message": "fix typing\n\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def random_hflip(input, p=0.5):",
            "probs: torch.Tensor = torch.empty(input.shape[0], device=device).uniform_(0, 1)",
            "else:",
            "input = input.unsqueeze(0)",
            "-        probs: torch.Tensor = torch.empty(1, device=device).uniform_(0, 1)",
            "+        probs = torch.empty(1, device=device).uniform_(0, 1)",
            "",
            "to_flip: torch.Tensor = probs < p",
            "",
            "if input[to_flip].nelement() != 0:",
            "",
            "-        flipped: torch.Tensor = flips.hflip(input[to_flip])",
            "+        flipped: torch.Tensor = hflip(input[to_flip])",
            "trans_mat: torch.Tensor = torch.zeros((3, 3))",
            "trans_mat[0][0] = -1",
            "trans_mat[1][1] = 1"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=type), position=2)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=hflip), position=0)",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=Tensor))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=type))",
            "Delete(target_node=ASTNode(type=identifier, text=flips))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 11,
        "number": 4019,
        "neg_line": [
            "-probs: torch.Tensor = torch.empty(1, device=device).uniform_(0, 1)",
            "-flipped: torch.Tensor = flips.hflip(input[to_flip])"
        ],
        "pos_line": [
            "+probs = torch.empty(1, device=device).uniform_(0, 1)",
            "+flipped: torch.Tensor = hflip(input[to_flip])"
        ],
        "core_change": "-probs: torch.Tensor = torch.empty(1, device=device).uniform_(0, 1) +probs = torch.empty(1, device=device).uniform_(0, 1) -flipped: torch.Tensor = flips.hflip(input[to_flip]) +flipped: torch.Tensor = hflip(input[to_flip])",
        "core_API": "empty"
    },
    {
        "commit_hash": "b4f7b9461e7f8f3b2f315f588dbb75f44c208794",
        "index": "ca01d371..ecd5334f 100644",
        "commit_message": "use dicts for namespaces within text fields (#25)\n\n* switch text and list fields to use dicts\n\n* unify namespace attribute in token indexers\n\n* switch dataset, iterators and tagger to new dict based textfields\n\n* fix unrecognised arg in char indexer\n\n* improve docs\n\n* make dataset iteration clearer\n\n* change to as_arrays, remove list options from data/\n\n* remove non-defaults from dataset test\n\n* add generic types to fields\n\n* move TextField internals to be dicts\n\n* fix mypy by removing DataArray type annotation\n\n* use new dict for dataset loop\n\n* Revert \"use new dict for dataset loop\"\n\nThis reverts commit 6447f7a4607ec5725e92e53689e469f73794e0cf.\n\n* address matts comments\n\n* fix typo\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SimpleTaggerTest(AllenNlpTestCase):",
            "training_arrays = self.dataset.as_arrays()",
            "",
            "# TODO(Mark): clean this up once the Trainer is finalised.",
            "-        sequence = training_arrays[\"tokens\"][0]",
            "+        sequence = training_arrays[\"tokens\"][\"tokens\"]",
            "tags = training_arrays[\"tags\"]",
            "-        training_arrays = {\"tokens\": Variable(torch.from_numpy(sequence)),  # pylint: disable=no-member",
            "+        training_arrays = {\"tokens\": {\"tokens\": Variable(torch.from_numpy(sequence))},  # pylint: disable=no-member",
            "\"tags\": Variable(torch.from_numpy(tags))}  # pylint: disable=no-member",
            "_ = self.model.forward(**training_arrays)",
            "",
            "def test_tag_returns_distributions_per_token(self):",
            "-        text = TextField([\"This\", \"is\", \"a\", \"sentence\"], token_indexers=[SingleIdTokenIndexer()])",
            "+        text = TextField([\"This\", \"is\", \"a\", \"sentence\"], token_indexers={\"tokens\": SingleIdTokenIndexer()})",
            "output = self.model.tag(text)",
            "possible_tags = self.vocab.get_index_to_token_vocabulary(\"tags\").values()",
            "for tag in output[\"tags\"]:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=subscript), node=('string', '\"tokens\"'), position=2, insert_id=48280)",
            "Insert(target_node=ASTNode(type=dictionary), node=('pair', None), position=1, insert_id=48281)",
            "Insert(target_node=IN(type=pair), node=('string', '\"tokens\"'), position=0, insert_id=48282)",
            "Insert(target_node=IN(type=pair), node=(':', ':'), position=1, insert_id=48283)",
            "Insert(target_node=IN(type=pair), node=('dictionary', None), position=2, insert_id=48284)",
            "Insert(target_node=IN(type=dictionary), node=('{', '{'), position=0, insert_id=48285)",
            "Move(target_node=IN(type=dictionary), node=ASTNode(type=pair), position=1)",
            "Insert(target_node=IN(type=dictionary), node=('}', '}'), position=2, insert_id=48286)",
            "Insert(target_node=ASTNode(type=keyword_argument), node=('dictionary', None), position=2, insert_id=48287)",
            "Insert(target_node=IN(type=dictionary), node=('{', '{'), position=0, insert_id=48288)",
            "Insert(target_node=IN(type=dictionary), node=('pair', None), position=1, insert_id=48289)",
            "Insert(target_node=IN(type=dictionary), node=('}', '}'), position=2, insert_id=48290)",
            "Insert(target_node=IN(type=pair), node=('string', '\"tokens\"'), position=0, insert_id=48291)",
            "Insert(target_node=IN(type=pair), node=(':', ':'), position=1, insert_id=48292)",
            "Move(target_node=IN(type=pair), node=ASTNode(type=call), position=2)",
            "Delete(target_node=ASTNode(type=integer, text=0))",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=list))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 19,
        "number": 4025,
        "neg_line": [
            "-sequence = training_arrays[\"tokens\"][0]",
            "-training_arrays = {\"tokens\": Variable(torch.from_numpy(sequence)),  # pylint: disable=no-member",
            "-text = TextField([\"This\", \"is\", \"a\", \"sentence\"], token_indexers=[SingleIdTokenIndexer()])"
        ],
        "pos_line": [
            "+sequence = training_arrays[\"tokens\"][\"tokens\"]",
            "+training_arrays = {\"tokens\": {\"tokens\": Variable(torch.from_numpy(sequence))},  # pylint: disable=no-member",
            "+text = TextField([\"This\", \"is\", \"a\", \"sentence\"], token_indexers={\"tokens\": SingleIdTokenIndexer()})"
        ],
        "core_change": "-sequence = training_arrays[\"tokens\"][0] +sequence = training_arrays[\"tokens\"][\"tokens\"] -training_arrays = {\"tokens\": Variable(torch.from_numpy(sequence)),  # pylint: disable=no-member +training_arrays = {\"tokens\": {\"tokens\": Variable(torch.from_numpy(sequence))},  # pylint: disable=no-member -text = TextField([\"This\", \"is\", \"a\", \"sentence\"], token_indexers=[SingleIdTokenIndexer()]) +text = TextField([\"This\", \"is\", \"a\", \"sentence\"], token_indexers={\"tokens\": SingleIdTokenIndexer()})",
        "core_API": "as_arrays"
    },
    {
        "commit_hash": "2912239fe608d8e9927bdb7d5580cef3d0c9d0e1",
        "index": "02a9c16cf..afffd3768 100644",
        "commit_message": "Add useful errors when model is not configured correctly (#1199)\n\n* add check_model_configuration method\n\n* trying to fix errors\n\n* trying to fix tests\n\n* added test_epoch_end to lightning template\n\n* fix tests\n\n* fix new test after rebase\n\n* fix spelling\n\n* added more checks\n\n* updated formating\n\n* added tests\n\n* fixed CHANGELOG\n\n* Apply suggestions from code review\n\n* move test to new module\n\n* change check on configure_optimizers\n\nCo-authored-by: Nicki Skafte <nugginea@gmail.com>\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "dataloader",
        "change": [
            "class LightTestStepMultipleDataloadersMixin:",
            "class LightTestFitSingleTestDataloadersMixin:",
            "\"\"\"Test fit single test dataloaders mixin.\"\"\"",
            "",
            "+    def test_dataloader(self):",
            "+        return self._dataloader(train=False)",
            "+",
            "def test_step(self, batch, batch_idx, *args, **kwargs):",
            "\"\"\"",
            "Lightning calls this inside the validation loop"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('function_definition', None), position=3, insert_id=1771140)",
            "Insert(target_node=IN(type=function_definition), node=('def', 'def'), position=0, insert_id=1771141)",
            "Insert(target_node=IN(type=function_definition), node=('identifier', 'test_dataloader'), position=1, insert_id=1771142)",
            "Insert(target_node=IN(type=function_definition), node=('parameters', None), position=2, insert_id=1771143)",
            "Insert(target_node=IN(type=function_definition), node=(':', ':'), position=3, insert_id=1771144)",
            "Insert(target_node=IN(type=function_definition), node=('block', None), position=4, insert_id=1771145)",
            "Insert(target_node=IN(type=parameters), node=('(', '('), position=0, insert_id=1771146)",
            "Insert(target_node=IN(type=parameters), node=('identifier', 'self'), position=1, insert_id=1771147)",
            "Insert(target_node=IN(type=parameters), node=(')', ')'), position=2, insert_id=1771148)",
            "Insert(target_node=IN(type=block), node=('return_statement', None), position=0, insert_id=1771149)",
            "Insert(target_node=IN(type=return_statement), node=('return', 'return'), position=0, insert_id=1771150)",
            "Insert(target_node=IN(type=return_statement), node=('call', None), position=1, insert_id=1771151)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1771152)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1771153)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=1771154)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1771155)",
            "Insert(target_node=IN(type=attribute), node=('identifier', '_dataloader'), position=2, insert_id=1771156)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1771157)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=1, insert_id=1771158)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1771159)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'train'), position=0, insert_id=1771160)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1771161)",
            "Insert(target_node=IN(type=keyword_argument), node=('false', 'False'), position=2, insert_id=1771162)"
        ],
        "plus_line": 2,
        "minus_line": 0,
        "AST_diff_line": 23,
        "number": 4029,
        "neg_line": [],
        "pos_line": [
            "+def test_dataloader(self):",
            "+return self._dataloader(train=False)",
            "+"
        ],
        "core_change": "+def test_dataloader(self): +return self._dataloader(train=False) +",
        "core_API": "_dataloader"
    },
    {
        "commit_hash": "5adb0a7bf7fee1732f307dfda9013bf2cf7b0ff9",
        "index": "de9c9269..094ca0fb 100644",
        "commit_message": "use torch.matmul instead of einsum in attnetion. (#445)\n\n* use torch.matmul instead of einsum\n\n* fix softmax\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class CrossAttention(nn.Module):",
            "for i in range(hidden_states.shape[0] // slice_size):",
            "start_idx = i * slice_size",
            "end_idx = (i + 1) * slice_size",
            "-            attn_slice = (",
            "-                torch.einsum(\"b i d, b j d -> b i j\", query[start_idx:end_idx], key[start_idx:end_idx]) * self.scale",
            "-            )",
            "+            attn_slice = torch.matmul(query[start_idx:end_idx], key[start_idx:end_idx].transpose(1, 2)) * self.scale",
            "attn_slice = attn_slice.softmax(dim=-1)",
            "-            attn_slice = torch.einsum(\"b i j, b j d -> b i d\", attn_slice, value[start_idx:end_idx])",
            "+            attn_slice = torch.matmul(attn_slice, value[start_idx:end_idx])",
            "",
            "hidden_states[start_idx:end_idx] = attn_slice"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=binary_operator), position=2)",
            "Update(target_node=ASTNode(type=identifier, text=einsum), value='matmul')",
            "Update(target_node=ASTNode(type=identifier, text=einsum), value='matmul')",
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=4, insert_id=104902)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=5, insert_id=104903)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=104904)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=104905)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=subscript), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=104906)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'transpose'), position=2, insert_id=104907)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=104908)",
            "Insert(target_node=IN(type=argument_list), node=('integer', '1'), position=1, insert_id=104909)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=104910)",
            "Insert(target_node=IN(type=argument_list), node=('integer', '2'), position=3, insert_id=104911)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=), text=)), position=4)",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=string, text=\"b i d, b j d -> b i j\"))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=parenthesized_expression))",
            "Delete(target_node=ASTNode(type=string, text=\"b i j, b j d -> b i d\"))",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 2,
        "minus_line": 4,
        "AST_diff_line": 22,
        "number": 4031,
        "neg_line": [
            "-attn_slice = (",
            "-torch.einsum(\"b i d, b j d -> b i j\", query[start_idx:end_idx], key[start_idx:end_idx]) * self.scale",
            "-)",
            "-attn_slice = torch.einsum(\"b i j, b j d -> b i d\", attn_slice, value[start_idx:end_idx])"
        ],
        "pos_line": [
            "+attn_slice = torch.matmul(query[start_idx:end_idx], key[start_idx:end_idx].transpose(1, 2)) * self.scale",
            "+attn_slice = torch.matmul(attn_slice, value[start_idx:end_idx])"
        ],
        "core_change": "-attn_slice = ( -torch.einsum(\"b i d, b j d -> b i j\", query[start_idx:end_idx], key[start_idx:end_idx]) * self.scale -) +attn_slice = torch.matmul(query[start_idx:end_idx], key[start_idx:end_idx].transpose(1, 2)) * self.scale -attn_slice = torch.einsum(\"b i j, b j d -> b i d\", attn_slice, value[start_idx:end_idx]) +attn_slice = torch.matmul(attn_slice, value[start_idx:end_idx])",
        "core_API": "einsum"
    },
    {
        "commit_hash": "d9f1874e3489edc915228dcfdae64d5294034464",
        "index": "3482adef04..e83d121d4c 100644",
        "commit_message": "[RLlib] Minor fixes (torch GPU bugs + some cleanup). (#11609)\n\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def build_vtrace_loss(policy, model, dist_class, train_batch):",
            "if isinstance(policy.action_space, gym.spaces.Discrete):",
            "is_multidiscrete = False",
            "output_hidden_shape = [policy.action_space.n]",
            "-    elif isinstance(policy.action_space,",
            "-                    gym.spaces.multi_discrete.MultiDiscrete):",
            "+    elif isinstance(policy.action_space, gym.spaces.MultiDiscrete):",
            "is_multidiscrete = True",
            "output_hidden_shape = policy.action_space.nvec.astype(np.int32)",
            "else:"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=IN(type=root), node=ASTNode(type=attribute), position=0)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=multi_discrete))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 3,
        "number": 4033,
        "neg_line": [
            "-elif isinstance(policy.action_space,",
            "-gym.spaces.multi_discrete.MultiDiscrete):"
        ],
        "pos_line": [
            "+elif isinstance(policy.action_space, gym.spaces.MultiDiscrete):"
        ],
        "core_change": "-elif isinstance(policy.action_space, -gym.spaces.multi_discrete.MultiDiscrete): +elif isinstance(policy.action_space, gym.spaces.MultiDiscrete):",
        "core_API": "astype"
    },
    {
        "commit_hash": "39b3ccba71853970ceb721ed4a8dd26e059c7fbb",
        "index": "3a0a815f7..803130641 100644",
        "commit_message": "test is fixed now\n\n",
        "file": "PySyft.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestTorchVariable(TestCase):",
            "x = Var(torch.FloatTensor([1, 2, -3, 4, 5])).send(remote)",
            "assert torch.equal(x.ceil().get(), Var(torch.FloatTensor([1, 2, -3, 4, 5])))",
            "assert torch.equal(x.ceil_().get(), Var(torch.FloatTensor([1, 2, -3, 4, 5])))",
            "-        assert torch.equal(x.cpu().get(), Var(torch.FloatTensor([1, 2, -3, 4, 5])))",
            "\\ No newline at end of file",
            "+        assert torch.equal(x.cpu().get(), Var(torch.FloatTensor([1, 2, -3, 4, 5])))"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assert_statement), node=('ERROR', None), position=1, insert_id=860974)",
            "Insert(target_node=ASTNode(type=assert_statement), node=('identifier', 'file'), position=2, insert_id=860975)",
            "Move(target_node=ASTNode(type=assert_statement), node=ASTNode(type=call), position=1)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=ERROR), node=('ERROR', ' '), position=1, insert_id=860976)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'No'), position=2, insert_id=860977)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'newline'), position=3, insert_id=860978)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'at'), position=4, insert_id=860979)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'end'), position=5, insert_id=860980)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'of'), position=6, insert_id=860981)",
            "Delete(target_node=ASTNode(type=ERROR, text= ))",
            "Delete(target_node=ASTNode(type=identifier, text=No))",
            "Delete(target_node=ASTNode(type=identifier, text=newline))",
            "Delete(target_node=ASTNode(type=identifier, text=at))",
            "Delete(target_node=ASTNode(type=identifier, text=end))",
            "Delete(target_node=ASTNode(type=identifier, text=of))",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=identifier, text=file))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 18,
        "number": 4034,
        "neg_line": [
            "-assert torch.equal(x.cpu().get(), Var(torch.FloatTensor([1, 2, -3, 4, 5])))"
        ],
        "pos_line": [
            "+assert torch.equal(x.cpu().get(), Var(torch.FloatTensor([1, 2, -3, 4, 5])))"
        ],
        "core_change": "-assert torch.equal(x.cpu().get(), Var(torch.FloatTensor([1, 2, -3, 4, 5]))) +assert torch.equal(x.cpu().get(), Var(torch.FloatTensor([1, 2, -3, 4, 5])))",
        "core_API": "FloatTensor"
    },
    {
        "commit_hash": "6d5b199c791e608a99fa3e313624e2c4c7bba736",
        "index": "0c0594d7..1c328998 100644",
        "commit_message": "fix many pylint issues.\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class BatchNormLayer(Layer):",
            "params_shape = x_shape[-1:]",
            "",
            "from tensorflow.python.training import moving_averages",
            "-        from tensorflow.python.ops import control_flow_ops",
            "",
            "-        with tf.variable_scope(name) as vs:",
            "+        with tf.variable_scope(name):",
            "axis = list(range(len(x_shape) - 1))",
            "",
            "# 1. beta, gamma"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=with_item), node=ASTNode(type=call), position=0)",
            "Delete(target_node=ASTNode(type=from, text=from))",
            "Delete(target_node=ASTNode(type=identifier, text=tensorflow))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=python))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=ops))",
            "Delete(target_node=ASTNode(type=dotted_name))",
            "Delete(target_node=ASTNode(type=import, text=import))",
            "Delete(target_node=ASTNode(type=identifier, text=control_flow_ops))",
            "Delete(target_node=ASTNode(type=dotted_name))",
            "Delete(target_node=ASTNode(type=import_from_statement))",
            "Delete(target_node=ASTNode(type=as, text=as))",
            "Delete(target_node=ASTNode(type=identifier, text=vs))",
            "Delete(target_node=ASTNode(type=as_pattern_target))",
            "Delete(target_node=ASTNode(type=as_pattern))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 16,
        "number": 4035,
        "neg_line": [
            "-from tensorflow.python.ops import control_flow_ops",
            "-with tf.variable_scope(name) as vs:"
        ],
        "pos_line": [
            "+with tf.variable_scope(name):"
        ],
        "core_change": "-from tensorflow.python.ops import control_flow_ops -with tf.variable_scope(name) as vs: +with tf.variable_scope(name):",
        "core_API": "variable_scope"
    },
    {
        "commit_hash": "88b6a19862d34b6895bc71cc0681af666a49b072",
        "index": "42bb85acb..cd2db5367 100644",
        "commit_message": "fix label mapping (#3180)\n\n* fix label mapping\n\n* add pretty_name\n\n* update infos\n\nCo-authored-by: Quentin Lhoest <lhoest.q@gmail.com>\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class HLGD(datasets.GeneratorBasedBuilder):",
            "\"date_b\": datasets.Value(\"string\"),",
            "\"url_a\": datasets.Value(\"string\"),",
            "\"url_b\": datasets.Value(\"string\"),",
            "-                \"label\": datasets.features.ClassLabel(names=[\"different_event\", \"same_event\"]),",
            "+                \"label\": datasets.features.ClassLabel(names=[\"same_event\", \"different_event\"]),",
            "}",
            ")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=string, text=\"different_event\"), node=ASTNode(type=list), position=3)",
            "Move(target_node=ASTNode(type=,, text=,), node=ASTNode(type=list), position=4)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 4037,
        "neg_line": [
            "-\"label\": datasets.features.ClassLabel(names=[\"different_event\", \"same_event\"]),"
        ],
        "pos_line": [
            "+\"label\": datasets.features.ClassLabel(names=[\"same_event\", \"different_event\"]),"
        ],
        "core_change": "-\"label\": datasets.features.ClassLabel(names=[\"different_event\", \"same_event\"]), +\"label\": datasets.features.ClassLabel(names=[\"same_event\", \"different_event\"]),",
        "core_API": "Value"
    },
    {
        "commit_hash": "6902c48a5f0637c8155c1c4bc10ad35930f3e772",
        "index": "3b001c7..853ee1d 100644",
        "commit_message": "Fix ResNet based models to work w/ norm layers w/o affine params. Reformat long arg lists into vertical form.\n\n",
        "file": "pytorch-image-models.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class ResNestBottleneck(nn.Module):",
            "self.downsample = downsample",
            "",
            "def zero_init_last(self):",
            "-        nn.init.zeros_(self.bn3.weight)",
            "+        if getattr(self.bn3, 'weight', None) is not None:",
            "+            nn.init.zeros_(self.bn3.weight)",
            "",
            "def forward(self, x):",
            "shortcut = x"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=block), node=('if_statement', None), position=0, insert_id=1476613)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=1476614)",
            "Insert(target_node=IN(type=if_statement), node=('comparison_operator', None), position=1, insert_id=1476615)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=1476616)",
            "Move(target_node=IN(type=if_statement), node=ASTNode(type=block), position=3)",
            "Insert(target_node=IN(type=comparison_operator), node=('call', None), position=0, insert_id=1476617)",
            "Insert(target_node=IN(type=comparison_operator), node=('is not', 'is'), position=1, insert_id=1476618)",
            "Insert(target_node=IN(type=comparison_operator), node=('is not', 'not'), position=2, insert_id=1476619)",
            "Insert(target_node=IN(type=comparison_operator), node=('none', 'None'), position=3, insert_id=1476620)",
            "Insert(target_node=IN(type=call), node=('identifier', 'getattr'), position=0, insert_id=1476621)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1476622)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1476623)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=1476624)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=1476625)",
            "Insert(target_node=IN(type=argument_list), node=('string', \"'weight'\"), position=3, insert_id=1476626)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=4, insert_id=1476627)",
            "Insert(target_node=IN(type=argument_list), node=('none', 'None'), position=5, insert_id=1476628)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=6, insert_id=1476629)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=1476630)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1476631)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'bn3'), position=2, insert_id=1476632)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 21,
        "number": 4038,
        "neg_line": [
            "-nn.init.zeros_(self.bn3.weight)"
        ],
        "pos_line": [
            "+if getattr(self.bn3, 'weight', None) is not None:",
            "+nn.init.zeros_(self.bn3.weight)"
        ],
        "core_change": "-nn.init.zeros_(self.bn3.weight) +if getattr(self.bn3, 'weight', None) is not None: +nn.init.zeros_(self.bn3.weight)",
        "core_API": "zeros_"
    },
    {
        "commit_hash": "60a1bdcdacc4ec3d2ca6dad16e61b8ad8022285e",
        "index": "3008787cd..0783c1bcc 100644",
        "commit_message": "fix some errors for distributed lm_finetuning\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def main():",
            "global_step += 1",
            "",
            "# Save a trained model",
            "-        logger.info(\"** ** * Saving fine - tuned model ** ** * \")",
            "model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self",
            "output_model_file = os.path.join(args.output_dir, WEIGHTS_NAME)",
            "output_config_file = os.path.join(args.output_dir, CONFIG_NAME)",
            "-        if args.do_train:",
            "+        if args.do_train and torch.distributed.get_rank() == 0:",
            "+            logger.info(\"** ** * Saving fine - tuned model ** ** * \")",
            "torch.save(model_to_save.state_dict(), output_model_file)",
            "model_to_save.config.to_json_file(output_config_file)",
            "tokenizer.save_vocabulary(args.output_dir)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=if_statement), node=('boolean_operator', None), position=1, insert_id=1248152)",
            "Insert(target_node=ASTNode(type=if_statement), node=('block', None), position=4, insert_id=1248153)",
            "Move(target_node=IN(type=boolean_operator), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=boolean_operator), node=('and', 'and'), position=1, insert_id=1248154)",
            "Insert(target_node=IN(type=boolean_operator), node=('comparison_operator', None), position=2, insert_id=1248155)",
            "Move(target_node=IN(type=block), node=ASTNode(type=expression_statement), position=0)",
            "Insert(target_node=IN(type=comparison_operator), node=('call', None), position=0, insert_id=1248156)",
            "Insert(target_node=IN(type=comparison_operator), node=('==', '=='), position=1, insert_id=1248157)",
            "Insert(target_node=IN(type=comparison_operator), node=('integer', '0'), position=2, insert_id=1248158)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1248159)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1248160)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1248161)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1248162)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'get_rank'), position=2, insert_id=1248163)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1248164)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1248165)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1248166)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1248167)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'distributed'), position=2, insert_id=1248168)",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 20,
        "number": 4040,
        "neg_line": [
            "-logger.info(\"** ** * Saving fine - tuned model ** ** * \")",
            "-if args.do_train:"
        ],
        "pos_line": [
            "+if args.do_train and torch.distributed.get_rank() == 0:",
            "+logger.info(\"** ** * Saving fine - tuned model ** ** * \")"
        ],
        "core_change": "-logger.info(\"** ** * Saving fine - tuned model ** ** * \") -if args.do_train: +if args.do_train and torch.distributed.get_rank() == 0: +logger.info(\"** ** * Saving fine - tuned model ** ** * \")",
        "core_API": "info"
    },
    {
        "commit_hash": "01b238d0de58ec1cf21aa8f651a412480d677636",
        "index": "bd9688cc..10ee253f 100644",
        "commit_message": "fix typo\n\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class LatentDiffusion(DiffusionPipeline):",
            "# 3. optionally sample variance",
            "variance = 0",
            "if eta > 0:",
            "-                noise = torch.randn(image.shape, generator=generator)to(image.device)",
            "+                noise = torch.randn(image.shape, generator=generator).to(image.device)",
            "variance = self.noise_scheduler.get_variance(t, num_inference_steps).sqrt() * eta * noise",
            "",
            "# 4. set current image to prev_image: x_t -> x_t-1"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=113263)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=113264)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=to), position=2)",
            "Delete(target_node=ASTNode(type=ERROR))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 4042,
        "neg_line": [
            "-noise = torch.randn(image.shape, generator=generator)to(image.device)"
        ],
        "pos_line": [
            "+noise = torch.randn(image.shape, generator=generator).to(image.device)"
        ],
        "core_change": "-noise = torch.randn(image.shape, generator=generator)to(image.device) +noise = torch.randn(image.shape, generator=generator).to(image.device)",
        "core_API": "randn"
    },
    {
        "commit_hash": "e49a2d52380660e00de8549e309af94e523e2718",
        "index": "d3e295f9..fd15b3b4 100644",
        "commit_message": "[Fix] gpu tests for crop3d and flip (#727)\n\n* fix gpu tests for crop3d and flip\n\n* add 0.4.1 pytorch dependency\n\n* add missing docs for warp3d functions\n\n* disable get_perspective_transform3d test for a while\n\n* Fixed 3D crop GPU test\n\n* Fixed partial projwarp\n\n* Recomputed the tests for 3D rotation\n\n* Fixed one more test\n\n* Fixed lint\n\n* Update gradcheck for float64\n\n* Fixed lint\n\nCo-authored-by: shijianjian <sj8716643@126.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def vflip(input: torch.Tensor) -> torch.Tensor:",
            "\"\"\"",
            "",
            "h = input.shape[-2]",
            "-    return input[..., torch.arange(h - 1, -1, -1), :]",
            "+    return input[..., torch.arange(h - 1, -1, -1, device=input.device), :]"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=6, insert_id=435085)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=7, insert_id=435086)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=435087)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=435088)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=435089)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'input'), position=0, insert_id=435090)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=435091)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=435092)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 4046,
        "neg_line": [
            "-return input[..., torch.arange(h - 1, -1, -1), :]"
        ],
        "pos_line": [
            "+return input[..., torch.arange(h - 1, -1, -1, device=input.device), :]"
        ],
        "core_change": "-return input[..., torch.arange(h - 1, -1, -1), :] +return input[..., torch.arange(h - 1, -1, -1, device=input.device), :]",
        "core_API": "arange"
    },
    {
        "commit_hash": "746eb3da346ccaff6345361b1af3754312525b6a",
        "index": "dc992c4..9d53cb0 100644",
        "commit_message": "Fixed indentation\n",
        "file": "facenet.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def main(argv=None):",
            "facenet.plot_roc(fpr, tpr, 'NN4')",
            "",
            "if __name__ == '__main__':",
            "-  tf.app.run()",
            "+    tf.app.run()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 4048,
        "neg_line": [
            "-tf.app.run()"
        ],
        "pos_line": [
            "+tf.app.run()"
        ],
        "core_change": "-tf.app.run() +tf.app.run()",
        "core_API": "plot_roc"
    },
    {
        "commit_hash": "ddebbdc53544c64a1957cf0ec04a34578e9b57ba",
        "index": "d2d38909..972293a5 100644",
        "commit_message": "Make most metrics work on GPU (#3851)\n\n* Make most metrics work on GPU\n\n* Make metric tests work on both GPU and CPU\n\n* Add a test for the test utility\n\n* mypy\n\n* Update allennlp/common/testing/test_case.py\n\nCo-Authored-By: Mark Neumann <markn@allenai.org>\n\n* Fix a PR comment\n\n* flake8\n\nCo-authored-by: Mark Neumann <markn@allenai.org>\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class BLEU(Metric):",
            "",
            "None",
            "\"\"\"",
            "-        predictions, gold_targets = self.unwrap_to_tensors(predictions, gold_targets)",
            "+        predictions, gold_targets = self.detach_tensors(predictions, gold_targets)",
            "for ngram_size, _ in enumerate(self._ngram_weights, start=1):",
            "precision_matches, precision_totals = self._get_modified_precision_counts(",
            "predictions, gold_targets, ngram_size"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=unwrap_to_tensors), value='detach_tensors')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 4054,
        "neg_line": [
            "-predictions, gold_targets = self.unwrap_to_tensors(predictions, gold_targets)"
        ],
        "pos_line": [
            "+predictions, gold_targets = self.detach_tensors(predictions, gold_targets)"
        ],
        "core_change": "-predictions, gold_targets = self.unwrap_to_tensors(predictions, gold_targets) +predictions, gold_targets = self.detach_tensors(predictions, gold_targets)",
        "core_API": "unwrap_to_tensors"
    },
    {
        "commit_hash": "6f79d264422245d88c7a34032c1a8254a0c65752",
        "index": "ec246aaff..b61398163 100644",
        "commit_message": "Update quality tooling for formatting (#21480)\n\n* Result of black 23.1\n\n* Update target to Python 3.7\n\n* Switch flake8 to ruff\n\n* Configure isort\n\n* Configure isort\n\n* Apply isort with line limit\n\n* Put the right black version\n\n* adapt black in check copies\n\n* Fix copies\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFGenerationMixin:",
            "generated, finished_sequences, cur_len, model_kwargs, next_step_cached_variables",
            "):",
            "maximum_iterations = max_length - cur_len",
            "-            generated, _, cur_len, _, _, = tf.while_loop(",
            "+            (",
            "+                generated,",
            "+                _,",
            "+                cur_len,",
            "+                _,",
            "+                _,",
            "+            ) = tf.while_loop(",
            "contrastive_search_cond_fn,",
            "contrastive_search_body_fn,",
            "(generated, finished_sequences, cur_len, model_kwargs, next_step_cached_variables),"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=ERROR), node=('tuple_pattern', None), position=0, insert_id=2357202)",
            "Insert(target_node=IN(type=tuple_pattern), node=('(', '('), position=0, insert_id=2357203)",
            "Move(target_node=IN(type=tuple_pattern), node=ASTNode(type=identifier, text=generated), position=1)",
            "Move(target_node=IN(type=tuple_pattern), node=ASTNode(type=,, text=,), position=2)",
            "Move(target_node=IN(type=tuple_pattern), node=ASTNode(type=identifier, text=_), position=3)",
            "Move(target_node=IN(type=tuple_pattern), node=ASTNode(type=,, text=,), position=4)",
            "Move(target_node=IN(type=tuple_pattern), node=ASTNode(type=identifier, text=cur_len), position=5)",
            "Move(target_node=IN(type=tuple_pattern), node=ASTNode(type=,, text=,), position=6)",
            "Move(target_node=IN(type=tuple_pattern), node=ASTNode(type=identifier, text=_), position=7)",
            "Move(target_node=IN(type=tuple_pattern), node=ASTNode(type=,, text=,), position=8)",
            "Move(target_node=IN(type=tuple_pattern), node=ASTNode(type=identifier, text=_), position=9)",
            "Move(target_node=IN(type=tuple_pattern), node=ASTNode(type=,, text=,), position=10)",
            "Insert(target_node=IN(type=tuple_pattern), node=(')', ')'), position=11, insert_id=2357204)",
            "Delete(target_node=ASTNode(type=pattern_list))"
        ],
        "plus_line": 7,
        "minus_line": 1,
        "AST_diff_line": 14,
        "number": 4056,
        "neg_line": [
            "-generated, _, cur_len, _, _, = tf.while_loop("
        ],
        "pos_line": [
            "+(",
            "+generated,",
            "+_,",
            "+cur_len,",
            "+_,",
            "+_,",
            "+) = tf.while_loop("
        ],
        "core_change": "-generated, _, cur_len, _, _, = tf.while_loop( +( +generated, +_, +cur_len, +_, +_, +) = tf.while_loop(",
        "core_API": "while_loop"
    },
    {
        "commit_hash": "27d733030600cac722658219dee91e05374ec288",
        "index": "efd61c05..98dde0f4 100644",
        "commit_message": "fix 'logits' naming in A3C (fix #197)\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class GANModelDesc(ModelDesc):",
            "d_pos_acc = tf.reduce_mean(tf.cast(score_real > 0.5, tf.float32), name='accuracy_real')",
            "d_neg_acc = tf.reduce_mean(tf.cast(score_fake < 0.5, tf.float32), name='accuracy_fake')",
            "",
            "-                self.d_accuracy = tf.add(.5 * d_pos_acc, .5 * d_neg_acc, name='accuracy')",
            "+                d_accuracy = tf.add(.5 * d_pos_acc, .5 * d_neg_acc, name='accuracy')",
            "self.d_loss = tf.add(.5 * d_loss_pos, .5 * d_loss_neg, name='loss')",
            "",
            "with tf.name_scope(\"gen\"):",
            "self.g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(",
            "logits=logits_fake, labels=tf.ones_like(logits_fake)), name='loss')",
            "-                self.g_accuracy = tf.reduce_mean(tf.cast(score_fake > 0.5, tf.float32), name='accuracy')",
            "+                g_accuracy = tf.reduce_mean(tf.cast(score_fake > 0.5, tf.float32), name='accuracy')",
            "",
            "-            add_moving_summary(self.g_loss, self.d_loss, self.d_accuracy, self.g_accuracy)",
            "+            add_moving_summary(self.g_loss, self.d_loss, d_accuracy, g_accuracy)",
            "",
            "",
            "class GANTrainer(FeedfreeTrainerBase):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=identifier, text=d_accuracy), position=0)",
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=identifier, text=g_accuracy), position=0)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=d_accuracy), position=5)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=g_accuracy), position=8)",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 16,
        "number": 4058,
        "neg_line": [
            "-self.d_accuracy = tf.add(.5 * d_pos_acc, .5 * d_neg_acc, name='accuracy')",
            "-self.g_accuracy = tf.reduce_mean(tf.cast(score_fake > 0.5, tf.float32), name='accuracy')",
            "-add_moving_summary(self.g_loss, self.d_loss, self.d_accuracy, self.g_accuracy)"
        ],
        "pos_line": [
            "+d_accuracy = tf.add(.5 * d_pos_acc, .5 * d_neg_acc, name='accuracy')",
            "+g_accuracy = tf.reduce_mean(tf.cast(score_fake > 0.5, tf.float32), name='accuracy')",
            "+add_moving_summary(self.g_loss, self.d_loss, d_accuracy, g_accuracy)"
        ],
        "core_change": "-self.d_accuracy = tf.add(.5 * d_pos_acc, .5 * d_neg_acc, name='accuracy') +d_accuracy = tf.add(.5 * d_pos_acc, .5 * d_neg_acc, name='accuracy') -self.g_accuracy = tf.reduce_mean(tf.cast(score_fake > 0.5, tf.float32), name='accuracy') +g_accuracy = tf.reduce_mean(tf.cast(score_fake > 0.5, tf.float32), name='accuracy') -add_moving_summary(self.g_loss, self.d_loss, self.d_accuracy, self.g_accuracy) +add_moving_summary(self.g_loss, self.d_loss, d_accuracy, g_accuracy)",
        "core_API": "reduce_mean"
    },
    {
        "commit_hash": "cde8c3f1e00c1b62b1a1a778d7546e151391621c",
        "index": "05d4e21f..1945c56f 100644",
        "commit_message": "fix build errors\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tensorflow",
        "change": [
            "class ReshapeLayer(Layer):",
            "",
            "",
            "class TransposeLayer(Layer):",
            "-    \"\"\"",
            "-    The :class:`TransposeLayer` class transposes the dimension of a teneor, see `tf.transpose() <https://www.tensorflow.org/api_docs/python/tf/transpose>`__ .",
            "+    \"\"\"A layer that transposes the dimension of a tensor.",
            "+",
            "+    See `tf.transpose() <https://www.tensorflow.org/api_docs/python/tf/transpose>`__ .",
            "",
            "Parameters"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=The), value='A')",
            "Insert(target_node=ASTNode(type=ERROR), node=('identifier', 'layer'), position=2, insert_id=2455809)",
            "Update(target_node=ASTNode(type=identifier, text=class), value='that')",
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=identifier, text=class), position=3)",
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=identifier, text=transposes), position=4)",
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=identifier, text=the), position=5)",
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=identifier, text=dimension), position=6)",
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=identifier, text=of), position=7)",
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=identifier, text=a), position=8)",
            "Insert(target_node=ASTNode(type=ERROR), node=('attribute', None), position=9, insert_id=2455810)",
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=string, text=`tf.transpose() <https://www.tensorflow.org/api_docs/python/tf/transpose>`), position=10)",
            "Update(target_node=ASTNode(type=identifier, text=teneor), value='tensor')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=teneor), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2455811)",
            "Update(target_node=ASTNode(type=identifier, text=see), value='See')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=see), position=2)",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=string, text=`TransposeLayer`))",
            "Delete(target_node=ASTNode(type=identifier, text=class))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=ERROR))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 22,
        "number": 4060,
        "neg_line": [
            "-\"\"\"",
            "-The :class:`TransposeLayer` class transposes the dimension of a teneor, see `tf.transpose() <https://www.tensorflow.org/api_docs/python/tf/transpose>`__ ."
        ],
        "pos_line": [
            "+\"\"\"A layer that transposes the dimension of a tensor.",
            "+",
            "+See `tf.transpose() <https://www.tensorflow.org/api_docs/python/tf/transpose>`__ ."
        ],
        "core_change": "-\"\"\" -The :class:`TransposeLayer` class transposes the dimension of a teneor, see `tf.transpose() <https://www.tensorflow.org/api_docs/python/tf/transpose>`__ . +\"\"\"A layer that transposes the dimension of a tensor. + +See `tf.transpose() <https://www.tensorflow.org/api_docs/python/tf/transpose>`__ .",
        "core_API": "transpose"
    },
    {
        "commit_hash": "5b89ef2c6e5895b168f8f150ddce345dcee6be91",
        "index": "fded8f87..a5db64e9 100644",
        "commit_message": "fix speaker-embeddings dimension during inference\n",
        "file": "TTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Tacotron2(TacotronAbstract):",
            "if self.num_speakers > 1:",
            "if not self.embeddings_per_sample:",
            "speaker_embeddings = self.speaker_embedding(speaker_ids)[:, None]",
            "+                speaker_embeddings = torch.unsqueeze(speaker_embeddings, 0).transpose(1, 2)",
            "encoder_outputs = self._concat_speaker_embedding(encoder_outputs, speaker_embeddings)",
            "",
            "mel_outputs, alignments, stop_tokens = self.decoder.inference_truncated(encoder_outputs)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=4, insert_id=1260659)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1260660)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'speaker_embeddings'), position=0, insert_id=1260661)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1260662)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=1260663)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1260664)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1260665)",
            "Insert(target_node=IN(type=attribute), node=('call', None), position=0, insert_id=1260666)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1260667)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'transpose'), position=2, insert_id=1260668)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1260669)",
            "Insert(target_node=IN(type=argument_list), node=('integer', '1'), position=1, insert_id=1260670)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=1260671)",
            "Insert(target_node=IN(type=argument_list), node=('integer', '2'), position=3, insert_id=1260672)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=1260673)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1260674)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1260675)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1260676)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1260677)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'unsqueeze'), position=2, insert_id=1260678)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1260679)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'speaker_embeddings'), position=1, insert_id=1260680)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=1260681)",
            "Insert(target_node=IN(type=argument_list), node=('integer', '0'), position=3, insert_id=1260682)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=1260683)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 25,
        "number": 4064,
        "neg_line": [],
        "pos_line": [
            "+speaker_embeddings = torch.unsqueeze(speaker_embeddings, 0).transpose(1, 2)"
        ],
        "core_change": "+speaker_embeddings = torch.unsqueeze(speaker_embeddings, 0).transpose(1, 2)",
        "core_API": "speaker_embedding"
    },
    {
        "commit_hash": "deea1861ab87b64394d461f8739814937fed69bc",
        "index": "1dc2945f9..abffbcafd 100644",
        "commit_message": "[rllib] Try fixing torch GPU and masking errors (#10168)\n\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TorchParametricActionsModel(DQNTorchModel):",
            "# Mask out invalid actions (use -inf to tag invalid).",
            "# These are then recognized by the EpsilonGreedy exploration component",
            "# as invalid actions that are not to be chosen.",
            "-        inf_mask = torch.clamp(",
            "-            torch.log(action_mask), -float(\"inf\"), float(\"inf\"))",
            "+        inf_mask = torch.clamp(torch.log(action_mask), FLOAT_MIN, FLOAT_MAX)",
            "+",
            "return action_logits + inf_mask, state",
            "",
            "def value_function(self):"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'FLOAT_MIN'), position=3, insert_id=1121677)",
            "Update(target_node=ASTNode(type=identifier, text=float), value='FLOAT_MAX')",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=float), position=6)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=), text=)), position=7)",
            "Delete(target_node=ASTNode(type=-, text=-))",
            "Delete(target_node=ASTNode(type=identifier, text=float))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=string, text=\"inf\"))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=unary_operator))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=string, text=\"inf\"))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=), text=)))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 4066,
        "neg_line": [
            "-inf_mask = torch.clamp(",
            "-torch.log(action_mask), -float(\"inf\"), float(\"inf\"))"
        ],
        "pos_line": [
            "+inf_mask = torch.clamp(torch.log(action_mask), FLOAT_MIN, FLOAT_MAX)",
            "+"
        ],
        "core_change": "-inf_mask = torch.clamp( -torch.log(action_mask), -float(\"inf\"), float(\"inf\")) +inf_mask = torch.clamp(torch.log(action_mask), FLOAT_MIN, FLOAT_MAX) +",
        "core_API": "clamp"
    },
    {
        "commit_hash": "ebd0bc17929b36b851fc825bbfd5fbd41f6d8111",
        "index": "e97c8872..70c07bcb 100644",
        "commit_message": "Update to PyTorch 1.11.0 (#3045)\n\n* Run black\n\n* Fix ProvenanceTensor\n\n* Replace torch.triangular_solve -> torch.linalg.solve_triangular\n\n* More fixes to torch.linalg.solve_triangular\n\n* Bump torch version\n\n* Bump Python version 3.6 -> 3.7\n\n* Fix some tests\n\n* Decrease tolerance on stable tests\n\n* Remove obsolete xfail\n\n* Resolve #3032\n\n* Fix solve_triangular in ops.gaussian, revert test weakening\n\n* Fix catching of singular matrices in hmc\n\n* xfail funsor tests\n\n* Work around pandas 1.3 bug\n\n* Allow mypy to install missing stubs\n\n* Clarify triangular_solve test\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def triu_inverse(x):",
            "B_Dinv = B / x.bottom_diag.unsqueeze(-2)",
            "",
            "identity = torch.eye(head_size, dtype=A.dtype, device=A.device)",
            "-    top_left = torch.triangular_solve(identity, A, upper=True)[0]",
            "+    top_left = torch.linalg.solve_triangular(A, identity, upper=True)",
            "top_right = -top_left.matmul(B_Dinv)  # complexity: head_size^2 x N",
            "top = torch.cat([top_left, top_right], -1)",
            "bottom_diag = x.bottom_diag.reciprocal()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=call), position=2)",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=672081)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=672082)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'solve_triangular'), position=2, insert_id=672083)",
            "Update(target_node=ASTNode(type=identifier, text=identity), value='A')",
            "Update(target_node=ASTNode(type=identifier, text=A), value='identity')",
            "Update(target_node=ASTNode(type=identifier, text=triangular_solve), value='linalg')",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=integer, text=0))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 12,
        "number": 4067,
        "neg_line": [
            "-top_left = torch.triangular_solve(identity, A, upper=True)[0]"
        ],
        "pos_line": [
            "+top_left = torch.linalg.solve_triangular(A, identity, upper=True)"
        ],
        "core_change": "-top_left = torch.triangular_solve(identity, A, upper=True)[0] +top_left = torch.linalg.solve_triangular(A, identity, upper=True)",
        "core_API": "unsqueeze"
    },
    {
        "commit_hash": "408baf66e2e56b08f08e7034ca6e383ff396d29c",
        "index": "127adad..5df7365 100644",
        "commit_message": "weight_decay fix\n\n",
        "file": "yolov3.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def train():",
            "",
            "# Compute loss",
            "loss, loss_items = compute_loss(pred, targets, model)",
            "+            if torch.isnan(loss):",
            "+                print('WARNING: nan loss detected, skipping batch ', loss_items)",
            "+                continue",
            "",
            "# Scale loss by nominal batch_size of 64",
            "loss *= batch_size / 64"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=2, insert_id=1284226)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=1284227)",
            "Insert(target_node=IN(type=if_statement), node=('call', None), position=1, insert_id=1284228)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=1284229)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=1284230)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1284231)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1284232)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=1284233)",
            "Insert(target_node=IN(type=block), node=('continue_statement', None), position=1, insert_id=1284234)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1284235)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1284236)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'isnan'), position=2, insert_id=1284237)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1284238)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'loss'), position=1, insert_id=1284239)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1284240)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=1284241)",
            "Insert(target_node=IN(type=continue_statement), node=('continue', 'continue'), position=0, insert_id=1284242)",
            "Insert(target_node=IN(type=call), node=('identifier', 'print'), position=0, insert_id=1284243)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1284244)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1284245)",
            "Insert(target_node=IN(type=argument_list), node=('string', \"'WARNING: nan loss detected, skipping batch '\"), position=1, insert_id=1284246)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=1284247)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'loss_items'), position=3, insert_id=1284248)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=1284249)"
        ],
        "plus_line": 3,
        "minus_line": 0,
        "AST_diff_line": 24,
        "number": 4068,
        "neg_line": [],
        "pos_line": [
            "+if torch.isnan(loss):",
            "+print('WARNING: nan loss detected, skipping batch ', loss_items)",
            "+continue"
        ],
        "core_change": "+if torch.isnan(loss): +print('WARNING: nan loss detected, skipping batch ', loss_items) +continue",
        "core_API": "isnan"
    },
    {
        "commit_hash": "327b57d1bf776d149b703ce3aed8d50db3562ba9",
        "index": "a7403de54e..2e6f4c0dc4 100644",
        "commit_message": "Fixed failing test for manipulation concat due to bfloat16 casting (#4598)\n\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def concat(",
            "xs = list(xs)",
            "highest_dtype = xs[0].dtype",
            "for i in xs:",
            "-        highest_dtype = tf.experimental.numpy.promote_types(highest_dtype, i.dtype)",
            "+        highest_dtype = ivy.as_native_dtype(ivy.promote_types(highest_dtype, i.dtype))",
            "",
            "for i in range(len(xs)):",
            "if is_axis_none:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=1993127)",
            "Update(target_node=ASTNode(type=identifier, text=tf), value='ivy')",
            "Update(target_node=ASTNode(type=identifier, text=experimental), value='as_native_dtype')",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1993128)",
            "Insert(target_node=IN(type=argument_list), node=('call', None), position=1, insert_id=1993129)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1993130)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1993131)",
            "Move(target_node=IN(type=call), node=ASTNode(type=argument_list), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=numpy), value='ivy')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=numpy), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=promote_types), position=2)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 16,
        "number": 4070,
        "neg_line": [
            "-highest_dtype = tf.experimental.numpy.promote_types(highest_dtype, i.dtype)"
        ],
        "pos_line": [
            "+highest_dtype = ivy.as_native_dtype(ivy.promote_types(highest_dtype, i.dtype))"
        ],
        "core_change": "-highest_dtype = tf.experimental.numpy.promote_types(highest_dtype, i.dtype) +highest_dtype = ivy.as_native_dtype(ivy.promote_types(highest_dtype, i.dtype))",
        "core_API": "promote_types"
    },
    {
        "commit_hash": "4fd8977eafd658056c92ffa6294027e77cdc46c0",
        "index": "54f1386ea7..963d4d816d 100644",
        "commit_message": "[RLlib] Minor cleanup in preparation to tf2.x support. (#9130)\n\n* WIP.\n\n* Fixes.\n\n* LINT.\n\n* Fixes.\n\n* Fixes and LINT.\n\n* WIP.\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class PPOLoss:",
            "self.mean_policy_loss = reduce_mean_valid(-surrogate_loss)",
            "",
            "if use_gae:",
            "-            vf_loss1 = tf.square(value_fn - value_targets)",
            "+            vf_loss1 = tf.math.square(value_fn - value_targets)",
            "vf_clipped = vf_preds + tf.clip_by_value(",
            "value_fn - vf_preds, -vf_clip_param, vf_clip_param)",
            "-            vf_loss2 = tf.square(vf_clipped - value_targets)",
            "+            vf_loss2 = tf.math.square(vf_clipped - value_targets)",
            "vf_loss = tf.maximum(vf_loss1, vf_loss2)",
            "self.mean_vf_loss = reduce_mean_valid(vf_loss)",
            "loss = reduce_mean_valid("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=2145924)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=2145925)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=tf), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'math'), position=2, insert_id=2145926)",
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=2145927)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=2145928)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=tf), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'math'), position=2, insert_id=2145929)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 10,
        "number": 4071,
        "neg_line": [
            "-vf_loss1 = tf.square(value_fn - value_targets)",
            "-vf_loss2 = tf.square(vf_clipped - value_targets)"
        ],
        "pos_line": [
            "+vf_loss1 = tf.math.square(value_fn - value_targets)",
            "+vf_loss2 = tf.math.square(vf_clipped - value_targets)"
        ],
        "core_change": "-vf_loss1 = tf.square(value_fn - value_targets) +vf_loss1 = tf.math.square(value_fn - value_targets) -vf_loss2 = tf.square(vf_clipped - value_targets) +vf_loss2 = tf.math.square(vf_clipped - value_targets)",
        "core_API": "square"
    },
    {
        "commit_hash": "d81cb26d92fc8126af7375a171bc1f755feef54a",
        "index": "9dff9b39..3c59ba63 100755",
        "commit_message": "Fix setup.py for cpu-only environment installation (#538)\n\n* Add guard to not using `torch.version.cuda` above no-CUDA environment.\n* Fix several typos on setup.py.\n\nSigned-off-by: Seunghwan Hong <seunghwan@scatterlab.co.kr>\n\nCo-authored-by: Jeff Rasley <jerasley@microsoft.com>\n",
        "file": "DeepSpeed.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "cuda",
        "change": [
            "else:",
            "version_str += f'+{git_hash}'",
            "",
            "torch_version = \".\".join([TORCH_MAJOR, TORCH_MINOR])",
            "-cuda_version = \".\".join(torch.version.cuda.split('.')[:2])",
            "+# Set cuda_version to 0.0 if cpu-only",
            "+cuda_version = \"0.0\"",
            "+if torch.version.cuda is not None:",
            "+    cuda_version = \".\".join(torch.version.cuda.split('.')[:2])",
            "torch_info = {\"version\": torch_version, \"cuda_version\": cuda_version}",
            "",
            "print(f\"version={version_str}, git_hash={git_hash}, git_branch={git_branch}\")"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=2, insert_id=1819183)",
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=3, insert_id=1819184)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1819185)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=1819186)",
            "Insert(target_node=IN(type=if_statement), node=('comparison_operator', None), position=1, insert_id=1819187)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=1819188)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=1819189)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'cuda_version'), position=0, insert_id=1819190)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1819191)",
            "Insert(target_node=IN(type=assignment), node=('string', '\"0.0\"'), position=2, insert_id=1819192)",
            "Insert(target_node=IN(type=comparison_operator), node=('attribute', None), position=0, insert_id=1819193)",
            "Insert(target_node=IN(type=comparison_operator), node=('is not', 'is'), position=1, insert_id=1819194)",
            "Insert(target_node=IN(type=comparison_operator), node=('is not', 'not'), position=2, insert_id=1819195)",
            "Insert(target_node=IN(type=comparison_operator), node=('none', 'None'), position=3, insert_id=1819196)",
            "Move(target_node=IN(type=block), node=ASTNode(type=expression_statement), position=0)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1819197)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1819198)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cuda'), position=2, insert_id=1819199)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1819200)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1819201)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'version'), position=2, insert_id=1819202)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 21,
        "number": 4072,
        "neg_line": [
            "-cuda_version = \".\".join(torch.version.cuda.split('.')[:2])"
        ],
        "pos_line": [
            "+# Set cuda_version to 0.0 if cpu-only",
            "+cuda_version = \"0.0\"",
            "+if torch.version.cuda is not None:",
            "+cuda_version = \".\".join(torch.version.cuda.split('.')[:2])"
        ],
        "core_change": "-cuda_version = \".\".join(torch.version.cuda.split('.')[:2]) +# Set cuda_version to 0.0 if cpu-only +cuda_version = \"0.0\" +if torch.version.cuda is not None: +cuda_version = \".\".join(torch.version.cuda.split('.')[:2])",
        "core_API": "split"
    },
    {
        "commit_hash": "7aa4461209250bba4cfafed79ee0c44e13c7e1e3",
        "index": "b7b3b85d..4d666662 100644",
        "commit_message": "Fix compatibility of Retiarii CGO (#4621)\n\n* fix compatibility of CGO\n* remove old test files for CGO\n* revert to fit pytorch-lightning 1.5.x\n* update PyTorch-lightning version in doc\n* fix nni.trace issue\n",
        "file": "nni.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "def _new_trainer():",
            "train_dataset = serialize(MNIST, root='data/mnist', train=True, download=True, transform=transform)",
            "test_dataset = serialize(MNIST, root='data/mnist', train=False, download=True, transform=transform)",
            "",
            "-    multi_module = MultiModelSupervisedLearningModule(nn.CrossEntropyLoss, {'acc': pl._AccuracyWithLogits})",
            "+    multi_module = _MultiModelSupervisedLearningModule(nn.CrossEntropyLoss, {'acc': pl._AccuracyWithLogits})",
            "",
            "lightning = pl.Lightning(multi_module, cgo_trainer.Trainer(use_cgo=True,",
            "max_epochs=1,"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=MultiModelSupervisedLearningModule), value='_MultiModelSupervisedLearningModule')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 4074,
        "neg_line": [
            "-multi_module = MultiModelSupervisedLearningModule(nn.CrossEntropyLoss, {'acc': pl._AccuracyWithLogits})"
        ],
        "pos_line": [
            "+multi_module = _MultiModelSupervisedLearningModule(nn.CrossEntropyLoss, {'acc': pl._AccuracyWithLogits})"
        ],
        "core_change": "-multi_module = MultiModelSupervisedLearningModule(nn.CrossEntropyLoss, {'acc': pl._AccuracyWithLogits}) +multi_module = _MultiModelSupervisedLearningModule(nn.CrossEntropyLoss, {'acc': pl._AccuracyWithLogits})",
        "core_API": "Lightning"
    },
    {
        "commit_hash": "fd3d70b167c2dcc92787d2d8b52d302f1da1ad6c",
        "index": "a615a3a90..61a69a8ff 100644",
        "commit_message": "Fixed code based on flake8\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Decoder(torch.nn.Module):",
            "if self.labeldist is not None:",
            "if self.vlabeldist is None:",
            "self.vlabeldist = to_cuda(self, Variable(torch.from_numpy(self.labeldist)))",
            "-            loss_reg = - torch.sum((functional.log_softmax(y_all, dim=1) * self.vlabeldist).view(-1), dim=0) / len(ys_in)",
            "+            loss_reg = - torch.sum((functional.log_softmax(y_all, dim=1) *",
            "+                                    self.vlabeldist).view(-1), dim=0) / len(ys_in)",
            "self.loss = (1. - self.lsm_weight) * self.loss + self.lsm_weight * loss_reg",
            "",
            "return self.loss, acc, att_weight_all"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 4076,
        "neg_line": [
            "-loss_reg = - torch.sum((functional.log_softmax(y_all, dim=1) * self.vlabeldist).view(-1), dim=0) / len(ys_in)"
        ],
        "pos_line": [
            "+loss_reg = - torch.sum((functional.log_softmax(y_all, dim=1) *",
            "+self.vlabeldist).view(-1), dim=0) / len(ys_in)"
        ],
        "core_change": "-loss_reg = - torch.sum((functional.log_softmax(y_all, dim=1) * self.vlabeldist).view(-1), dim=0) / len(ys_in) +loss_reg = - torch.sum((functional.log_softmax(y_all, dim=1) * +self.vlabeldist).view(-1), dim=0) / len(ys_in)",
        "core_API": "from_numpy"
    },
    {
        "commit_hash": "6ba2231d7227825dfb76e9df161824d04234e69f",
        "index": "953fa446..d02b7702 100644",
        "commit_message": "Reproducibility 3/3 (#1924)\n\n* make tests deterministic\n\n* run slow tests\n\n* prepare for testing\n\n* finish\n\n* refactor\n\n* add print statements\n\n* finish more\n\n* correct some test failures\n\n* more fixes\n\n* set up to correct tests\n\n* more corrections\n\n* up\n\n* fix more\n\n* more prints\n\n* add\n\n* up\n\n* up\n\n* up\n\n* uP\n\n* uP\n\n* more fixes\n\n* uP\n\n* up\n\n* up\n\n* up\n\n* up\n\n* fix more\n\n* up\n\n* up\n\n* clean tests\n\n* up\n\n* up\n\n* up\n\n* more fixes\n\n* Apply suggestions from code review\n\nCo-authored-by: Suraj Patil <surajp815@gmail.com>\n\n* make\n\n* correct\n\n* finish\n\n* finish\n\nCo-authored-by: Suraj Patil <surajp815@gmail.com>\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class PaintByExamplePipelineIntegrationTests(unittest.TestCase):",
            "image_slice = image[0, -3:, -3:, -1]",
            "",
            "assert image.shape == (1, 512, 512, 3)",
            "-        expected_slice = np.array(",
            "-            [0.47455794, 0.47086594, 0.47683704, 0.51024145, 0.5064255, 0.5123164, 0.532502, 0.5328063, 0.5428694]",
            "-        )",
            "+        expected_slice = np.array([0.4834, 0.4811, 0.4874, 0.5122, 0.5081, 0.5144, 0.5291, 0.5290, 0.5374])",
            "+",
            "assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-2"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=float, text=0.47455794), value='0.4834')",
            "Update(target_node=ASTNode(type=float, text=0.47086594), value='0.4811')",
            "Update(target_node=ASTNode(type=float, text=0.47683704), value='0.4874')",
            "Update(target_node=ASTNode(type=float, text=0.51024145), value='0.5122')",
            "Update(target_node=ASTNode(type=float, text=0.5064255), value='0.5081')",
            "Update(target_node=ASTNode(type=float, text=0.5123164), value='0.5144')",
            "Update(target_node=ASTNode(type=float, text=0.532502), value='0.5291')",
            "Update(target_node=ASTNode(type=float, text=0.5328063), value='0.5290')",
            "Update(target_node=ASTNode(type=float, text=0.5428694), value='0.5374')"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 9,
        "number": 4078,
        "neg_line": [
            "-expected_slice = np.array(",
            "-[0.47455794, 0.47086594, 0.47683704, 0.51024145, 0.5064255, 0.5123164, 0.532502, 0.5328063, 0.5428694]",
            "-)"
        ],
        "pos_line": [
            "+expected_slice = np.array([0.4834, 0.4811, 0.4874, 0.5122, 0.5081, 0.5144, 0.5291, 0.5290, 0.5374])",
            "+"
        ],
        "core_change": "-expected_slice = np.array( -[0.47455794, 0.47086594, 0.47683704, 0.51024145, 0.5064255, 0.5123164, 0.532502, 0.5328063, 0.5428694] -) +expected_slice = np.array([0.4834, 0.4811, 0.4874, 0.5122, 0.5081, 0.5144, 0.5291, 0.5290, 0.5374]) +",
        "core_API": "array"
    },
    {
        "commit_hash": "5d62c95cd94df4b2a303309d90cec75424c05b3d",
        "index": "2dbbdb9..acf76ed 100644",
        "commit_message": "fix docs of customized (#1508)\n\n\n",
        "file": "autokeras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "number of neurons is tunable.",
            "class SingleDenseLayerBlock(ak.Block):",
            "def build(self, hp, inputs=None):",
            "# Get the input_node from inputs.",
            "-        input_node = tf.python.util.nest.flatten(inputs)[0]",
            "+        input_node = tf.nest.flatten(inputs)[0]",
            "layer = tf.keras.layers.Dense(",
            "hp.Int(\"num_units\", min_value=32, max_value=512, step=32)",
            ")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=python))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=util))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 4081,
        "neg_line": [
            "-input_node = tf.python.util.nest.flatten(inputs)[0]"
        ],
        "pos_line": [
            "+input_node = tf.nest.flatten(inputs)[0]"
        ],
        "core_change": "-input_node = tf.python.util.nest.flatten(inputs)[0] +input_node = tf.nest.flatten(inputs)[0]",
        "core_API": "flatten"
    },
    {
        "commit_hash": "34cb277dc5316d8c41cbc7e2020ccf9be5c7dd84",
        "index": "006ade3..851d679 100644",
        "commit_message": "Fix val.py 'no labels found bug' (#8806)\n\nResolves https://github.com/ultralytics/yolov5/issues/8791\n\nBug first introduced in #8782\n",
        "file": "yolov5.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def run(",
            "tp, fp, p, r, f1, ap, ap_class = ap_per_class(*stats, plot=plots, save_dir=save_dir, names=names)",
            "ap50, ap = ap[:, 0], ap.mean(1)  # AP@0.5, AP@0.5:0.95",
            "mp, mr, map50, map = p.mean(), r.mean(), ap50.mean(), ap.mean()",
            "-        nt = np.bincount(stats[3].astype(int), minlength=nc)  # number of targets per class",
            "-    else:",
            "-        nt = torch.zeros(1)",
            "+    nt = np.bincount(stats[3].astype(int), minlength=nc)  # number of targets per class",
            "",
            "# Print results",
            "pf = '%20s' + '%11i' * 2 + '%11.3g' * 4  # print format"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=IN(type=root), node=ASTNode(type=call), position=0)",
            "Delete(target_node=ASTNode(type=else, text=else))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=identifier, text=nt))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=zeros))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 14,
        "number": 4082,
        "neg_line": [
            "-nt = np.bincount(stats[3].astype(int), minlength=nc)  # number of targets per class",
            "-else:",
            "-nt = torch.zeros(1)"
        ],
        "pos_line": [
            "+nt = np.bincount(stats[3].astype(int), minlength=nc)  # number of targets per class"
        ],
        "core_change": "-nt = np.bincount(stats[3].astype(int), minlength=nc)  # number of targets per class -else: -nt = torch.zeros(1) +nt = np.bincount(stats[3].astype(int), minlength=nc)  # number of targets per class",
        "core_API": "mean"
    },
    {
        "commit_hash": "455c6390938a5c737fa63e78396cedae41e4e87e",
        "index": "eaedec26b..06ba2aa43 100644",
        "commit_message": "CDN urls (#4030)\n\n* [file_utils] use_cdn + documentation\n\n* Move to cdn. urls for weights\n\n* [urls] Hotfix for bert-base-japanese\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "models",
        "change": [
            "from .tokenization_utils import BatchEncoding",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "-TF_OPENAI_GPT_PRETRAINED_MODEL_ARCHIVE_MAP = {",
            "-    \"openai-gpt\": \"https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-tf_model.h5\"",
            "-}",
            "+TF_OPENAI_GPT_PRETRAINED_MODEL_ARCHIVE_MAP = {\"openai-gpt\": \"https://cdn.huggingface.co/openai-gpt-tf_model.h5\"}",
            "",
            "",
            "def gelu(x):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-tf_model.h5\"), value='\"https://cdn.huggingface.co/openai-gpt-tf_model.h5\"')"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 4083,
        "neg_line": [
            "-TF_OPENAI_GPT_PRETRAINED_MODEL_ARCHIVE_MAP = {",
            "-\"openai-gpt\": \"https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-tf_model.h5\"",
            "-}"
        ],
        "pos_line": [
            "+TF_OPENAI_GPT_PRETRAINED_MODEL_ARCHIVE_MAP = {\"openai-gpt\": \"https://cdn.huggingface.co/openai-gpt-tf_model.h5\"}"
        ],
        "core_change": "-TF_OPENAI_GPT_PRETRAINED_MODEL_ARCHIVE_MAP = { -\"openai-gpt\": \"https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-tf_model.h5\" -} +TF_OPENAI_GPT_PRETRAINED_MODEL_ARCHIVE_MAP = {\"openai-gpt\": \"https://cdn.huggingface.co/openai-gpt-tf_model.h5\"}",
        "core_API": "getLogger"
    },
    {
        "commit_hash": "c22ed57a40340adefa08b5d06ac0a0253a3cb3ac",
        "index": "7b76b68b..2b46ca61 100644",
        "commit_message": "Spacy token indexer (#3040)\n\n* add a tokenizer to ud\n\n* add spacy indexer\n\n* allow token_indexers to specify their own type\n\n* dumb hack to allow a whitespace spacy tokenizer...\n\n* pass through token embedder\n\n* add ndarray to TokenType, tests for pass through embedder\n\n* add doc\n\n* remove todo, test\n\n* fix docs\n\n* why is this test flaky\n\n* fix the correct test\n\n* add as_padded_tensor method\n\n* better place for depreciation stuff\n\n* add warning for calling inherited get_padding_token\n\n* ignore type for backward compatability\n\n* mattg comments\n\n* pylint\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestElmoTokenRepresentation(ElmoTestCase):",
            "for k in range(10):",
            "char_indices = indices[\"elmo\"][(k * 50):((k + 1) * 50)]",
            "sentences.append(",
            "-                    indexer.pad_token_sequence(",
            "+                    indexer.as_padded_tensor(",
            "{'key': char_indices}, desired_num_tokens={'key': 50}, padding_lengths={}",
            ")['key']",
            ")",
            "-        batch = torch.from_numpy(numpy.array(sentences))",
            "+        batch = torch.stack(sentences)",
            "",
            "elmo_token_embedder = _ElmoCharacterEncoder(self.options_file, self.weight_file)",
            "elmo_token_embedder_output = elmo_token_embedder(batch)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=argument_list), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=from_numpy), value='stack')",
            "Update(target_node=ASTNode(type=identifier, text=pad_token_sequence), value='as_padded_tensor')",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=numpy))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=array))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 11,
        "number": 4084,
        "neg_line": [
            "-indexer.pad_token_sequence(",
            "-batch = torch.from_numpy(numpy.array(sentences))"
        ],
        "pos_line": [
            "+indexer.as_padded_tensor(",
            "+batch = torch.stack(sentences)"
        ],
        "core_change": "-indexer.pad_token_sequence( +indexer.as_padded_tensor( -batch = torch.from_numpy(numpy.array(sentences)) +batch = torch.stack(sentences)",
        "core_API": "append"
    },
    {
        "commit_hash": "8d38fd8ccfd0234599d6853a880aaca4d89e130e",
        "index": "1f7c6f17..01beecd4 100644",
        "commit_message": "Reimplement \"FCOS: Fully Convolutional One-Stage Object Detection\" (#586)\n\n* add fcos\n\n* use P5 instead of C5\n\n* add relu before extra convs in FPN\n\n* add singleclass_nms, use caffe2 lr\n\n* fix log interval\n\n* use caffe2init and relu in extra layers\n\n* fix scale layer, use p5 instead of c5\n\n* fix focs target\n\n* refactor code\n\n* delete useless file\n\n* clean\n\n* refactor code\n\n* change num_classes to cls_out_channels\n\n* fix bug of in get_bboxes\n\n* fix bug in test\n\n* add r101 2x cfg\n\n* ms use value mode, add x101-64x4d cfg\n\n* add more comment and rename some variable\n\n* rename centers to points, modify doc string of distance2bbox\n\n* add fcos detector, replace frozen with requires_grad\n\n* add README.md\n\n* add r101-1x performance, rename cfg, add detector FCOS\n\n* update fcos r50 2x performance, remove fpn caffe2 initialize\n\n* fix flake8 error\n\n* rename cfg\n\n* fix grammar error of some comments\n\n* minor fix comment\n\n* change work_dir to be consistent with config name\n\n* add FCOS support in README\n\n",
        "file": "mmdetection.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def multiclass_nms(multi_bboxes, multi_scores, score_thr, nms_cfg, max_num=-1):",
            "else:",
            "_bboxes = multi_bboxes[cls_inds, i * 4:(i + 1) * 4]",
            "_scores = multi_scores[cls_inds, i]",
            "+        if score_factors is not None:",
            "+            _scores *= score_factors[cls_inds]",
            "cls_dets = torch.cat([_bboxes, _scores[:, None]], dim=1)",
            "cls_dets, _ = nms_op(cls_dets, **nms_cfg_)",
            "-        cls_labels = multi_bboxes.new_full(",
            "-            (cls_dets.shape[0], ), i - 1, dtype=torch.long)",
            "+        cls_labels = multi_bboxes.new_full((cls_dets.shape[0], ),",
            "+                                           i - 1,",
            "+                                           dtype=torch.long)",
            "bboxes.append(cls_dets)",
            "labels.append(cls_labels)",
            "if bboxes:"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=3, insert_id=645091)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=645092)",
            "Insert(target_node=IN(type=if_statement), node=('comparison_operator', None), position=1, insert_id=645093)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=645094)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=645095)",
            "Insert(target_node=IN(type=comparison_operator), node=('identifier', 'score_factors'), position=0, insert_id=645096)",
            "Insert(target_node=IN(type=comparison_operator), node=('is not', 'is'), position=1, insert_id=645097)",
            "Insert(target_node=IN(type=comparison_operator), node=('is not', 'not'), position=2, insert_id=645098)",
            "Insert(target_node=IN(type=comparison_operator), node=('none', 'None'), position=3, insert_id=645099)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=645100)",
            "Insert(target_node=IN(type=expression_statement), node=('augmented_assignment', None), position=0, insert_id=645101)",
            "Insert(target_node=IN(type=augmented_assignment), node=('identifier', '_scores'), position=0, insert_id=645102)",
            "Insert(target_node=IN(type=augmented_assignment), node=('*=', '*='), position=1, insert_id=645103)",
            "Insert(target_node=IN(type=augmented_assignment), node=('subscript', None), position=2, insert_id=645104)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'score_factors'), position=0, insert_id=645105)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=645106)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'cls_inds'), position=2, insert_id=645107)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=645108)"
        ],
        "plus_line": 5,
        "minus_line": 2,
        "AST_diff_line": 18,
        "number": 4086,
        "neg_line": [
            "-cls_labels = multi_bboxes.new_full(",
            "-(cls_dets.shape[0], ), i - 1, dtype=torch.long)"
        ],
        "pos_line": [
            "+if score_factors is not None:",
            "+_scores *= score_factors[cls_inds]",
            "+cls_labels = multi_bboxes.new_full((cls_dets.shape[0], ),",
            "+i - 1,",
            "+dtype=torch.long)"
        ],
        "core_change": "+if score_factors is not None: +_scores *= score_factors[cls_inds] -cls_labels = multi_bboxes.new_full( -(cls_dets.shape[0], ), i - 1, dtype=torch.long) +cls_labels = multi_bboxes.new_full((cls_dets.shape[0], ), +i - 1, +dtype=torch.long)",
        "core_API": "cat"
    },
    {
        "commit_hash": "2aef2f0bbcd3b192af18718684615019a7777a9b",
        "index": "777e62459..baf153140 100644",
        "commit_message": "[common attributes] Fix previous commit for transfo-xl\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class CommonTestCases:",
            "model = model_class(config)",
            "self.assertIsInstance(",
            "model.get_input_embeddings(),",
            "-                    torch.nn.Embedding",
            "+                    (torch.nn.Embedding, AdaptiveEmbedding)",
            ")",
            "model.set_input_embeddings(torch.nn.Embedding(10, 10))",
            "x = model.get_output_embeddings()"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('tuple', None), position=3, insert_id=1244820)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=4, insert_id=1244821)",
            "Insert(target_node=IN(type=tuple), node=('(', '('), position=0, insert_id=1244822)",
            "Move(target_node=IN(type=tuple), node=ASTNode(type=attribute), position=1)",
            "Insert(target_node=IN(type=tuple), node=(',', ','), position=2, insert_id=1244823)",
            "Insert(target_node=IN(type=tuple), node=('identifier', 'AdaptiveEmbedding'), position=3, insert_id=1244824)",
            "Move(target_node=IN(type=tuple), node=ASTNode(type=), text=)), position=4)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 4087,
        "neg_line": [
            "-torch.nn.Embedding"
        ],
        "pos_line": [
            "+(torch.nn.Embedding, AdaptiveEmbedding)"
        ],
        "core_change": "-torch.nn.Embedding +(torch.nn.Embedding, AdaptiveEmbedding)",
        "core_API": "assertIsInstance"
    },
    {
        "commit_hash": "3c6d73bc5cca8295a2f7b17eb868f45327a89d08",
        "index": "0e32d0437..60cf7ae26 100755",
        "commit_message": "Fix BERT/MobileBERT classifier dropout\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class BertForMultipleChoice(BertPreTrainedModel):",
            "super().__init__(config)",
            "",
            "self.bert = BertModel(config)",
            "-        self.dropout = nn.Dropout(config.classifier_dropout_prob)",
            "+        self.dropout = nn.Dropout(config.classifier_dropout)",
            "self.classifier = nn.Linear(config.hidden_size, 1)",
            "",
            "self.init_weights()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=classifier_dropout_prob), value='classifier_dropout')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 4091,
        "neg_line": [
            "-self.dropout = nn.Dropout(config.classifier_dropout_prob)"
        ],
        "pos_line": [
            "+self.dropout = nn.Dropout(config.classifier_dropout)"
        ],
        "core_change": "-self.dropout = nn.Dropout(config.classifier_dropout_prob) +self.dropout = nn.Dropout(config.classifier_dropout)",
        "core_API": "Dropout"
    },
    {
        "commit_hash": "b3c13b58d2e16ab772349f4851bf97c00397a465",
        "index": "a214d699..8e9c6d9d 100644",
        "commit_message": "Fix enumeration notebook tiny problem (#2621)\n\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "\"@config_enumerate\\n\",",
            "\"def model(data, num_components=3):\\n\",",
            "\"    print('Running model with {} data points'.format(len(data)))\\n\",",
            "-    \"    p = pyro.sample(\\\"p\\\", dist.Dirichlet(0.5 * torch.ones(3)))\\n\",",
            "+    \"    p = pyro.sample(\\\"p\\\", dist.Dirichlet(0.5 * torch.ones(num_components)))\\n\",",
            "\"    scale = pyro.sample(\\\"scale\\\", dist.LogNormal(0, num_components))\\n\",",
            "\"    with pyro.plate(\\\"components\\\", num_components):\\n\",",
            "\"        loc = pyro.sample(\\\"loc\\\", dist.Normal(0, 10))\\n\","
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"    p = pyro.sample(\\\"p\\\", dist.Dirichlet(0.5 * torch.ones(3)))\\n\"), value='\"    p = pyro.sample(\\\\\"p\\\\\", dist.Dirichlet(0.5 * torch.ones(num_components)))\\\\n\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 4092,
        "neg_line": [
            "-\"    p = pyro.sample(\\\"p\\\", dist.Dirichlet(0.5 * torch.ones(3)))\\n\","
        ],
        "pos_line": [
            "+\"    p = pyro.sample(\\\"p\\\", dist.Dirichlet(0.5 * torch.ones(num_components)))\\n\","
        ],
        "core_change": "-\"    p = pyro.sample(\\\"p\\\", dist.Dirichlet(0.5 * torch.ones(3)))\\n\", +\"    p = pyro.sample(\\\"p\\\", dist.Dirichlet(0.5 * torch.ones(num_components)))\\n\",",
        "core_API": "sample"
    },
    {
        "commit_hash": "f72f17d0e324cfa813592f35e71ad51588dc57fe",
        "index": "e9e2a875..144c78cd 100644",
        "commit_message": "quickfix batch shape\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class PGModel(Model):",
            "advantage = episode['returns'] - baseline",
            "",
            "if self.normalize_advantage:",
            "-            return zero_mean_unit_variance(advantage)",
            "+            return np.squeeze(zero_mean_unit_variance(advantage))",
            "else:",
            "-            return advantage",
            "+            return np.squeeze(advantage)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=return_statement), node=('call', None), position=1, insert_id=2245941)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2245942)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2245943)",
            "Insert(target_node=ASTNode(type=return_statement), node=('call', None), position=1, insert_id=2245944)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'np'), position=0, insert_id=2245945)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2245946)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'squeeze'), position=2, insert_id=2245947)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2245948)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=call), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2245949)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2245950)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2245951)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'np'), position=0, insert_id=2245952)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2245953)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'squeeze'), position=2, insert_id=2245954)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2245955)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=advantage), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2245956)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 18,
        "number": 4094,
        "neg_line": [
            "-return zero_mean_unit_variance(advantage)",
            "-return advantage"
        ],
        "pos_line": [
            "+return np.squeeze(zero_mean_unit_variance(advantage))",
            "+return np.squeeze(advantage)"
        ],
        "core_change": "-return zero_mean_unit_variance(advantage) +return np.squeeze(zero_mean_unit_variance(advantage)) -return advantage +return np.squeeze(advantage)",
        "core_API": "squeeze"
    },
    {
        "commit_hash": "47d663f0fa97919cce60a64ffa977d9aed6cff45",
        "index": "f5036c71..59876fe3 100644",
        "commit_message": "Add docstrings of core modules and methods (#3120)\n\n* Add docstrings of core modules and methods\n\n* Update docs and fix comments\n\n* Complete docstrings\n\n* Resolve comments\n\n* reformat docstrings\n\n* resolve comments\n",
        "file": "mmdetection.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def bbox2roi(bbox_list):",
            "",
            "",
            "def roi2bbox(rois):",
            "+    \"\"\"Convert rois to bounding box format",
            "+",
            "+    Args:",
            "+        rois (torch.Tensor): RoIs with the shape (n, 5) where the first",
            "+            column indicates batch id of each RoI.",
            "+",
            "+    Returns:",
            "+        list[torch.Tensor]: Converted boxes of corresponding rois.",
            "+    \"\"\"",
            "bbox_list = []",
            "img_ids = torch.unique(rois[:, 0].cpu(), sorted=True)",
            "for img_id in img_ids:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('block', None), position=0, insert_id=636692)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=636693)",
            "Insert(target_node=IN(type=expression_statement), node=('string', '\"\"\"Convert rois to bounding box format\\n\\n    Args:\\n        rois (torch.Tensor): RoIs with the shape (n, 5) where the first\\n            column indicates batch id of each RoI.\\n\\n    Returns:\\n        list[torch.Tensor]: Converted boxes of corresponding rois.\\n    \"\"\"'), position=0, insert_id=636694)",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 7,
        "minus_line": 0,
        "AST_diff_line": 4,
        "number": 4095,
        "neg_line": [],
        "pos_line": [
            "+\"\"\"Convert rois to bounding box format",
            "+",
            "+Args:",
            "+rois (torch.Tensor): RoIs with the shape (n, 5) where the first",
            "+column indicates batch id of each RoI.",
            "+",
            "+Returns:",
            "+list[torch.Tensor]: Converted boxes of corresponding rois.",
            "+\"\"\""
        ],
        "core_change": "+\"\"\"Convert rois to bounding box format + +Args: +rois (torch.Tensor): RoIs with the shape (n, 5) where the first +column indicates batch id of each RoI. + +Returns: +list[torch.Tensor]: Converted boxes of corresponding rois. +\"\"\"",
        "core_API": "unique"
    },
    {
        "commit_hash": "d9f1874e3489edc915228dcfdae64d5294034464",
        "index": "ff15783d1..93410a086 100644",
        "commit_message": "[RLlib] Minor fixes (torch GPU bugs + some cleanup). (#11609)\n\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DQNTorchModel(TorchModelV2, nn.Module):",
            "if self.num_atoms > 1:",
            "# Distributional Q-learning uses a discrete support z",
            "# to represent the action value distribution",
            "-            z = torch.range(0.0, self.num_atoms - 1, dtype=torch.float32)",
            "+            z = torch.range(",
            "+                0.0, self.num_atoms - 1,",
            "+                dtype=torch.float32).to(action_scores.device)",
            "z = self.v_min + \\",
            "z * (self.v_max - self.v_min) / float(self.num_atoms - 1)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1120813)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=1120814)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1120815)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to'), position=2, insert_id=1120816)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1120817)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=1120818)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1120819)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'action_scores'), position=0, insert_id=1120820)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1120821)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=1120822)"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 4100,
        "neg_line": [
            "-z = torch.range(0.0, self.num_atoms - 1, dtype=torch.float32)"
        ],
        "pos_line": [
            "+z = torch.range(",
            "+0.0, self.num_atoms - 1,",
            "+dtype=torch.float32).to(action_scores.device)"
        ],
        "core_change": "-z = torch.range(0.0, self.num_atoms - 1, dtype=torch.float32) +z = torch.range( +0.0, self.num_atoms - 1, +dtype=torch.float32).to(action_scores.device)",
        "core_API": "range"
    },
    {
        "commit_hash": "6aaa4c58ec530c59ad4eb64e32182669f3db056c",
        "index": "f9ada9a0..1e62c4c4 100644",
        "commit_message": "fix flake8\n\n",
        "file": "flair.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class EntityLinker(flair.nn.DefaultClassifier[Sentence]):",
            "if len(embedding_list) > 0:",
            "embedded_entity_pairs = torch.cat(embedding_list, 0)",
            "",
            "-            return embedded_entity_pairs,",
            "+            return (embedded_entity_pairs,)",
            "else:",
            "-            return torch.zeros(0, self.word_embeddings.embedding_length, device=flair.device),",
            "+            return (torch.zeros(0, self.word_embeddings.embedding_length, device=flair.device),)",
            "",
            "def _get_state_dict(self):",
            "model_state = {"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=module), node=ASTNode(type=expression_statement), position=4)",
            "Insert(target_node=ASTNode(type=return_statement), node=('tuple', None), position=1, insert_id=232897)",
            "Insert(target_node=IN(type=tuple), node=('(', '('), position=0, insert_id=232898)",
            "Move(target_node=IN(type=tuple), node=ASTNode(type=identifier, text=embedded_entity_pairs), position=1)",
            "Move(target_node=IN(type=tuple), node=ASTNode(type=,, text=,), position=2)",
            "Insert(target_node=IN(type=tuple), node=(')', ')'), position=3, insert_id=232899)",
            "Insert(target_node=ASTNode(type=type), node=('call', None), position=0, insert_id=232900)",
            "Insert(target_node=IN(type=call), node=('identifier', 'return'), position=0, insert_id=232901)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=232902)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=232903)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=call), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=232904)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=3, insert_id=232905)",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=torch), position=0)",
            "Delete(target_node=ASTNode(type=expression_list))",
            "Delete(target_node=ASTNode(type=identifier, text=return))",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=ERROR))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 19,
        "number": 4103,
        "neg_line": [
            "-return embedded_entity_pairs,",
            "-return torch.zeros(0, self.word_embeddings.embedding_length, device=flair.device),"
        ],
        "pos_line": [
            "+return (embedded_entity_pairs,)",
            "+return (torch.zeros(0, self.word_embeddings.embedding_length, device=flair.device),)"
        ],
        "core_change": "-return embedded_entity_pairs, +return (embedded_entity_pairs,) -return torch.zeros(0, self.word_embeddings.embedding_length, device=flair.device), +return (torch.zeros(0, self.word_embeddings.embedding_length, device=flair.device),)",
        "core_API": "cat"
    },
    {
        "commit_hash": "603026f0325339c6412e5c045b5149f351bd1778",
        "index": "282b1800..654ea040 100644",
        "commit_message": "support for TF 1.13, improved while loops, fixed lstm problem\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class InternalLstm(InternalLayer, TransformationBase):",
            ")",
            "",
            "def tf_apply(self, x, state):",
            "-        state = tf.contrib.rnn.LSTMStateTuple(c=state[:, 0, :], h=state[:, 1, :])",
            "+        state = tf.nn.rnn_cell.LSTMStateTuple(c=state[:, 0, :], h=state[:, 1, :])",
            "",
            "x, state = self.cell(inputs=x, state=state)",
            "state = tf.stack(values=(state.c, state.h), axis=1)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=rnn), value='rnn_cell')",
            "Update(target_node=ASTNode(type=identifier, text=contrib), value='nn')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 4107,
        "neg_line": [
            "-state = tf.contrib.rnn.LSTMStateTuple(c=state[:, 0, :], h=state[:, 1, :])"
        ],
        "pos_line": [
            "+state = tf.nn.rnn_cell.LSTMStateTuple(c=state[:, 0, :], h=state[:, 1, :])"
        ],
        "core_change": "-state = tf.contrib.rnn.LSTMStateTuple(c=state[:, 0, :], h=state[:, 1, :]) +state = tf.nn.rnn_cell.LSTMStateTuple(c=state[:, 0, :], h=state[:, 1, :])",
        "core_API": "LSTMStateTuple"
    },
    {
        "commit_hash": "f1a32203aa806a49f69ff0d439b6b9af80b86230",
        "index": "aa5f63b7..c1f67e55 100644",
        "commit_message": "[Tests] Fix UnCLIP cpu offload tests (#1769)\n\n\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class UnCLIPPipelineIntegrationTests(unittest.TestCase):",
            ")",
            "",
            "mem_bytes = torch.cuda.max_memory_allocated()",
            "-        # make sure that less than 1.5 GB is allocated",
            "-        assert mem_bytes < 1.5 * 10**9",
            "+        # make sure that less than 7 GB is allocated",
            "+        assert mem_bytes < 7 * 10**9"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=binary_operator), node=('integer', '7'), position=0, insert_id=92777)",
            "Delete(target_node=ASTNode(type=float, text=1.5))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 4110,
        "neg_line": [
            "-# make sure that less than 1.5 GB is allocated",
            "-assert mem_bytes < 1.5 * 10**9"
        ],
        "pos_line": [
            "+# make sure that less than 7 GB is allocated",
            "+assert mem_bytes < 7 * 10**9"
        ],
        "core_change": "-# make sure that less than 1.5 GB is allocated -assert mem_bytes < 1.5 * 10**9 +# make sure that less than 7 GB is allocated +assert mem_bytes < 7 * 10**9",
        "core_API": "max_memory_allocated"
    },
    {
        "commit_hash": "c51dc4f92755c67a83f3fc8a0bd6b3e64df199e4",
        "index": "cc3e7bd2c..388d9b3d5 100755",
        "commit_message": "[torch] remove deprecated uint8 in favor of bool (#21384)\n\n* uint8 -> bool\n\n* fix copies\n\n* style\n\n* update test modeling commen when checking attention buffers\n\n* style\n\n* use logical not on random mask instead of subtraction with 1\n\n* remove torch uint8\n\n* quality\n\n* remove modified modeling utils\n\n* Update based on review\n\nCo-authored-by: sgugger <sylvain.gugger@gmail.com>\n\n---------\n\nCo-authored-by: sgugger <sylvain.gugger@gmail.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class GPTNeoXJapaneseAttention(nn.Module):",
            "batch_size, num_attention_heads, query_length, attn_head_size = query.size()",
            "key_length = key.size(-2)",
            "",
            "-        causal_mask = self._create_casual_mask(key_length, query_length)",
            "+        causal_mask = self._create_causal_mask(key_length, query_length)",
            "",
            "query = query.view(batch_size * num_attention_heads, query_length, attn_head_size)",
            "key = key.view(batch_size * num_attention_heads, key_length, attn_head_size)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=_create_casual_mask), value='_create_causal_mask')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 4112,
        "neg_line": [
            "-causal_mask = self._create_casual_mask(key_length, query_length)"
        ],
        "pos_line": [
            "+causal_mask = self._create_causal_mask(key_length, query_length)"
        ],
        "core_change": "-causal_mask = self._create_casual_mask(key_length, query_length) +causal_mask = self._create_causal_mask(key_length, query_length)",
        "core_API": "size"
    },
    {
        "commit_hash": "d83ee8fe45e3c0d776d4a865aca21d7c2ac324c4",
        "index": "ee5ff411..c2b5b7c2 100644",
        "commit_message": "Adding neural HMM TTS Model (#2272)\n\n* Adding neural HMM TTS\n\n* Adding tests\n\n* Adding neural hmm on readme\n\n* renaming training recipe\n\n* Removing overflow\\s decoder parameters from the config\n\n* Update the Trainer requirement version for a compatible one (#2276)\n\n* Bump up to v0.10.2\n\n* Adding neural HMM TTS\n\n* Adding tests\n\n* Adding neural hmm on readme\n\n* renaming training recipe\n\n* Removing overflow\\s decoder parameters from the config\n\n* fixing documentation\n\nCo-authored-by: Edresson Casanova <edresson1@gmail.com>\nCo-authored-by: Eren Glge <erogol@hotmail.com>\n",
        "file": "TTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Overflow(BaseTTS):",
            "self.register_buffer(\"mean\", torch.tensor(0))",
            "self.register_buffer(\"std\", torch.tensor(1))",
            "",
            "-        # self.mean = nn.Parameter(torch.zeros(1), requires_grad=False)",
            "-        # self.std = nn.Parameter(torch.ones(1), requires_grad=False)",
            "-",
            "def update_mean_std(self, statistics_dict: Dict):",
            "self.mean.data = torch.tensor(statistics_dict[\"mean\"])",
            "self.std.data = torch.tensor(statistics_dict[\"std\"])"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 0,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 4113,
        "neg_line": [
            "-# self.mean = nn.Parameter(torch.zeros(1), requires_grad=False)",
            "-# self.std = nn.Parameter(torch.ones(1), requires_grad=False)",
            "-"
        ],
        "pos_line": [],
        "core_change": "-# self.mean = nn.Parameter(torch.zeros(1), requires_grad=False) -# self.std = nn.Parameter(torch.ones(1), requires_grad=False) -",
        "core_API": "register_buffer"
    },
    {
        "commit_hash": "3764368aa14a4236a72bc053bdc69142b7df2038",
        "index": "801f8a21c..f52c3902c 100644",
        "commit_message": "Fix kernel dimensions for LeNet model code example (#2192)\n\n* Change kernel to 5x5 in  1st Conv2d layer in model init\n\nSigned-off-by: Kiersten Stokes <kierstenstokes@gmail.com>\n\n* Change kernel to 5x5 in 2nd Conv2d layer in model init\n\n* Fix dimensions of 1st Linear layer to match new expected size\n---------\nSigned-off-by: Kiersten Stokes <kierstenstokes@gmail.com>\nCo-authored-by: Suraj Subramanian <5676233+suraj813@users.noreply.github.com>\nCo-authored-by: Svetlana Karslioglu <svekars@fb.com>\n",
        "file": "tutorials.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class LeNet(nn.Module):",
            "",
            "def __init__(self):",
            "super(LeNet, self).__init__()",
            "-        # 1 input image channel (black & white), 6 output channels, 3x3 square convolution",
            "+        # 1 input image channel (black & white), 6 output channels, 5x5 square convolution",
            "# kernel",
            "-        self.conv1 = nn.Conv2d(1, 6, 3)",
            "-        self.conv2 = nn.Conv2d(6, 16, 3)",
            "+        self.conv1 = nn.Conv2d(1, 6, 5)",
            "+        self.conv2 = nn.Conv2d(6, 16, 5)",
            "# an affine operation: y = Wx + b",
            "-        self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension",
            "+        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5*5 from image dimension",
            "self.fc2 = nn.Linear(120, 84)",
            "self.fc3 = nn.Linear(84, 10)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1559723)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'nn'), position=0, insert_id=1559724)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1559725)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Conv2d'), position=2, insert_id=1559726)",
            "Update(target_node=ASTNode(type=integer, text=3), value='5')",
            "Update(target_node=ASTNode(type=integer, text=3), value='5')",
            "Update(target_node=ASTNode(type=integer, text=6), value='5')",
            "Update(target_node=ASTNode(type=integer, text=6), value='5')",
            "Delete(target_node=ASTNode(type=identifier, text=nn))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=Conv2d))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 4,
        "minus_line": 4,
        "AST_diff_line": 13,
        "number": 4114,
        "neg_line": [
            "-# 1 input image channel (black & white), 6 output channels, 3x3 square convolution",
            "-self.conv1 = nn.Conv2d(1, 6, 3)",
            "-self.conv2 = nn.Conv2d(6, 16, 3)",
            "-self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension"
        ],
        "pos_line": [
            "+# 1 input image channel (black & white), 6 output channels, 5x5 square convolution",
            "+self.conv1 = nn.Conv2d(1, 6, 5)",
            "+self.conv2 = nn.Conv2d(6, 16, 5)",
            "+self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5*5 from image dimension"
        ],
        "core_change": "-# 1 input image channel (black & white), 6 output channels, 3x3 square convolution +# 1 input image channel (black & white), 6 output channels, 5x5 square convolution -self.conv1 = nn.Conv2d(1, 6, 3) -self.conv2 = nn.Conv2d(6, 16, 3) +self.conv1 = nn.Conv2d(1, 6, 5) +self.conv2 = nn.Conv2d(6, 16, 5) -self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension +self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5*5 from image dimension",
        "core_API": "Conv2d"
    },
    {
        "commit_hash": "6578af4d8b3ad821fb7963fbcbb55d8906cfd61d",
        "index": "e95516611b..884d7ab808 100644",
        "commit_message": "lintfixbot: Auto-commit fixed lint errors in codebase\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def quantile(",
            "",
            "temp = a.reshape((-1,) + tuple(desired_shape))",
            "",
            "-        return torch.quantile(temp, q, dim=0, keepdim=keepdims, interpolation=interpolation)",
            "+        return torch.quantile(",
            "+            temp, q, dim=0, keepdim=keepdims, interpolation=interpolation",
            "+        )",
            "",
            "return torch.quantile(a, q, dim=axis, keepdim=keepdims, interpolation=interpolation)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 4117,
        "neg_line": [
            "-return torch.quantile(temp, q, dim=0, keepdim=keepdims, interpolation=interpolation)"
        ],
        "pos_line": [
            "+return torch.quantile(",
            "+temp, q, dim=0, keepdim=keepdims, interpolation=interpolation",
            "+)"
        ],
        "core_change": "-return torch.quantile(temp, q, dim=0, keepdim=keepdims, interpolation=interpolation) +return torch.quantile( +temp, q, dim=0, keepdim=keepdims, interpolation=interpolation +)",
        "core_API": "reshape"
    },
    {
        "commit_hash": "b6bfe88d9ae11ba51eb8b4ba720826cf9274969c",
        "index": "f57913c29..60b766d7d 100644",
        "commit_message": "Deprecate prepare_module (#3166)\n\n* Refactor prepare_module\n\n* Add deprecation warning in prepare_module\n\n* Remove prepare_module in inspect\n\n* Remove prepare_module in patching\n\n* Remove prepare_module in dummy_data\n\n* Remove prepare_module in run_beam\n\n* Remove prepare_module in test_dataset_common\n\n* Fix hash in run_beam\n\n* Remove prepare_module from test_load\n\n* Remove prepare_module from test_metric_common\n\n* Remove prepare_module from test_hf_gcp\n\n* Use deprecated function instead\n\n* Add deprecation to docstring\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class LocalMetricTest(parameterized.TestCase):",
            "@slow",
            "def test_load_real_metric(self, metric_name):",
            "doctest.ELLIPSIS_MARKER = \"[...]\"",
            "-        metric_module = importlib.import_module(datasets.load.prepare_module(os.path.join(\"metrics\", metric_name))[0])",
            "+        metric_module = importlib.import_module(",
            "+            datasets.load.metric_module_factory(os.path.join(\"metrics\", metric_name)).module_path",
            "+        )",
            "# run doctest",
            "with self.use_local_metrics():",
            "results = doctest.testmod(metric_module, verbose=True, raise_on_error=True)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=1, insert_id=1782160)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1782161)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'module_path'), position=2, insert_id=1782162)",
            "Update(target_node=ASTNode(type=identifier, text=prepare_module), value='metric_module_factory')",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=integer, text=0))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 4119,
        "neg_line": [
            "-metric_module = importlib.import_module(datasets.load.prepare_module(os.path.join(\"metrics\", metric_name))[0])"
        ],
        "pos_line": [
            "+metric_module = importlib.import_module(",
            "+datasets.load.metric_module_factory(os.path.join(\"metrics\", metric_name)).module_path",
            "+)"
        ],
        "core_change": "-metric_module = importlib.import_module(datasets.load.prepare_module(os.path.join(\"metrics\", metric_name))[0]) +metric_module = importlib.import_module( +datasets.load.metric_module_factory(os.path.join(\"metrics\", metric_name)).module_path +)",
        "core_API": "import_module"
    },
    {
        "commit_hash": "0f7514002d261688c60681f2acf6bde05ebca32b",
        "index": "99932627d..2a076ea21 100644",
        "commit_message": "fix mulenc asr\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class CustomConverterMulEnc(object):",
            "self.dtype = dtype",
            "self.num_encs = len(subsamping_factors)",
            "",
            "-    def __call__(self, batch, device):",
            "+    def __call__(self, batch, device=torch.device('cpu')):",
            "\"\"\"Transform a batch and send it to a device.",
            "",
            "Args:"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=parameters), node=('default_parameter', None), position=5, insert_id=161786)",
            "Insert(target_node=ASTNode(type=parameters), node=(')', ')'), position=6, insert_id=161787)",
            "Move(target_node=IN(type=default_parameter), node=ASTNode(type=identifier, text=device), position=0)",
            "Insert(target_node=IN(type=default_parameter), node=('=', '='), position=1, insert_id=161788)",
            "Insert(target_node=IN(type=default_parameter), node=('call', None), position=2, insert_id=161789)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=161790)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=161791)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=161792)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=161793)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=161794)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=161795)",
            "Insert(target_node=IN(type=argument_list), node=('string', \"'cpu'\"), position=1, insert_id=161796)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=), text=)), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 13,
        "number": 4125,
        "neg_line": [
            "-def __call__(self, batch, device):"
        ],
        "pos_line": [
            "+def __call__(self, batch, device=torch.device('cpu')):"
        ],
        "core_change": "-def __call__(self, batch, device): +def __call__(self, batch, device=torch.device('cpu')):",
        "core_API": "device"
    },
    {
        "commit_hash": "4d8bf5635d7341a8a66527acec6f2a462af06286",
        "index": "118c4b8d7..7f4ec4188 100644",
        "commit_message": "[hotfix] Lint formatting for new Tune optimizer ZOOpt (#8040)\n\n* formatting\n\n* removedill\n\n* lint\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "MOCK_MODULES = [",
            "\"tensorflow.contrib.rnn\", \"tensorflow.contrib.slim\", \"tensorflow.core\",",
            "\"tensorflow.core.util\", \"tensorflow.python\", \"tensorflow.python.client\",",
            "\"tensorflow.python.util\", \"torch\", \"torch.distributed\", \"torch.nn\",",
            "-    \"torch.nn.parallel\", \"torch.utils.data\", \"torch.utils.data.distributed\"",
            "+    \"torch.nn.parallel\", \"torch.utils.data\", \"torch.utils.data.distributed\",",
            "+    \"zoopt\"",
            "]",
            "for mod_name in MOCK_MODULES:",
            "sys.modules[mod_name] = mock.Mock()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=,, text=,), node=ASTNode(type=list), position=21)",
            "Insert(target_node=ASTNode(type=list), node=(',', ','), position=2, insert_id=1124720)",
            "Insert(target_node=ASTNode(type=list), node=(',', ','), position=27, insert_id=1124721)",
            "Insert(target_node=ASTNode(type=list), node=('string', '\"zoopt\"'), position=28, insert_id=1124722)",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 4128,
        "neg_line": [
            "-\"torch.nn.parallel\", \"torch.utils.data\", \"torch.utils.data.distributed\""
        ],
        "pos_line": [
            "+\"torch.nn.parallel\", \"torch.utils.data\", \"torch.utils.data.distributed\",",
            "+\"zoopt\""
        ],
        "core_change": "-\"torch.nn.parallel\", \"torch.utils.data\", \"torch.utils.data.distributed\" +\"torch.nn.parallel\", \"torch.utils.data\", \"torch.utils.data.distributed\", +\"zoopt\"",
        "core_API": "Mock"
    },
    {
        "commit_hash": "c126a239bcea9c68453cf86045a5177afbe2be6c",
        "index": "dfdb66606..32ce09aaa 100644",
        "commit_message": "Fix tflongformer int dtype (#18907)\n\n* Use int64 throughout TFLongFormer\n\n* make style\n\n* Do some more fixed casting in TFLongFormer\n\n* Fix some wonky \"is None\" conditionals\n\n* Cast all the dtypes, salt the earth\n\n* Fix copies to TFLED as well and do some casting there\n\n* dtype fix in TFLongformer test\n\n* Make fixup\n\n* Expand tolerances on the LED tests too (I think this is a TF32 thing)\n\n* Expand test tolerances for LED a tiny bit (probably a Tensorfloat thing again)\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFLEDModelIntegrationTest(unittest.TestCase):",
            "expected_slice = tf.convert_to_tensor(",
            "[[33.6507, 6.4572, 16.8089], [5.8739, -2.4238, 11.2902], [-3.2139, -4.3149, 4.2783]],",
            ")",
            "-        tf.debugging.assert_near(output[:, :3, :3], expected_slice, atol=TOLERANCE)",
            "+        tf.debugging.assert_near(output[:, :3, :3], expected_slice, atol=1e-3, rtol=1e-3)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=6, insert_id=2361739)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=7, insert_id=2361740)",
            "Insert(target_node=ASTNode(type=keyword_argument), node=('float', '1e-3'), position=2, insert_id=2361741)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'rtol'), position=0, insert_id=2361742)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2361743)",
            "Insert(target_node=IN(type=keyword_argument), node=('float', '1e-3'), position=2, insert_id=2361744)",
            "Delete(target_node=ASTNode(type=identifier, text=TOLERANCE))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 4132,
        "neg_line": [
            "-tf.debugging.assert_near(output[:, :3, :3], expected_slice, atol=TOLERANCE)"
        ],
        "pos_line": [
            "+tf.debugging.assert_near(output[:, :3, :3], expected_slice, atol=1e-3, rtol=1e-3)"
        ],
        "core_change": "-tf.debugging.assert_near(output[:, :3, :3], expected_slice, atol=TOLERANCE) +tf.debugging.assert_near(output[:, :3, :3], expected_slice, atol=1e-3, rtol=1e-3)",
        "core_API": "convert_to_tensor"
    },
    {
        "commit_hash": "936eb4c3ad32453c3edae4782bb30bc0744d40b8",
        "index": "c8bc8aa3b..7e5e7757e 100644",
        "commit_message": "FIX small bugs in `run_classifier_pytorch.py`\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def main():",
            "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.train_batch_size)",
            "",
            "model.train()",
            "-        for epoch in args.num_train_epochs:",
            "+        for epoch in range(args.num_train_epochs):",
            "for input_ids, input_mask, segment_ids, label_ids in train_dataloader:",
            "input_ids = input_ids.to(device)",
            "input_mask = input_mask.float().to(device)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=for_statement), node=('call', None), position=3, insert_id=1252197)",
            "Insert(target_node=IN(type=call), node=('identifier', 'range'), position=0, insert_id=1252198)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1252199)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1252200)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=attribute), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1252201)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 4134,
        "neg_line": [
            "-for epoch in args.num_train_epochs:"
        ],
        "pos_line": [
            "+for epoch in range(args.num_train_epochs):"
        ],
        "core_change": "-for epoch in args.num_train_epochs: +for epoch in range(args.num_train_epochs):",
        "core_API": "train"
    },
    {
        "commit_hash": "de67c1546d065c86d5bd5cecf022925bb01d0ec7",
        "index": "adfc89de..d2e3d5ee 100644",
        "commit_message": "Add Cuda tests for pyro.distributions; fix errors (#297)\n\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Dirichlet(Distribution):",
            "alpha = self._sanitize_input(alpha)",
            "if alpha.dim() not in (1, 2):",
            "raise ValueError('Expected alpha.dim() in (1,2), actual: {}'.format(alpha.dim()))",
            "-        alpha_np = alpha.data.numpy()",
            "+        alpha_np = alpha.data.cpu().numpy()",
            "if alpha.dim() == 1:",
            "x_np = spr.dirichlet.rvs(alpha_np)[0]",
            "else:",
            "x_np = np.empty_like(alpha_np)",
            "for i in range(alpha_np.shape[0]):",
            "x_np[i, :] = spr.dirichlet.rvs(alpha_np[i, :])[0]",
            "-        x = Variable(torch.Tensor(x_np))",
            "+        x = Variable(type(alpha.data)(x_np))",
            "return x",
            "",
            "# TODO Remove the batch_size argument."
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=763109)",
            "Insert(target_node=ASTNode(type=attribute), node=('call', None), position=0, insert_id=763110)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=763111)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=763112)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=763113)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=763114)",
            "Move(target_node=IN(type=call), node=ASTNode(type=argument_list), position=1)",
            "Insert(target_node=ASTNode(type=call), node=('call', None), position=0, insert_id=763115)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=763116)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cpu'), position=2, insert_id=763117)",
            "Insert(target_node=IN(type=call), node=('identifier', 'type'), position=0, insert_id=763118)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=763119)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'alpha'), position=0, insert_id=763120)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=763121)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'data'), position=2, insert_id=763122)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=763123)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=attribute), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=763124)",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=Tensor))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 23,
        "number": 4135,
        "neg_line": [
            "-alpha_np = alpha.data.numpy()",
            "-x = Variable(torch.Tensor(x_np))"
        ],
        "pos_line": [
            "+alpha_np = alpha.data.cpu().numpy()",
            "+x = Variable(type(alpha.data)(x_np))"
        ],
        "core_change": "-alpha_np = alpha.data.numpy() +alpha_np = alpha.data.cpu().numpy() -x = Variable(torch.Tensor(x_np)) +x = Variable(type(alpha.data)(x_np))",
        "core_API": "_sanitize_input"
    },
    {
        "commit_hash": "802941994dda827e278b07e5689059516fa7bed8",
        "index": "f2bc94ff72..28fe422b9b 100644",
        "commit_message": "[rllib] Use RLlib preprocessors in DQN (fixes PongDeterministic-v4) (#1124)\n\n* fix pong\n\n* rename\n\n* update\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class DQNGraph(object):",
            "with tf.variable_scope(\"q_func\", reuse=True):",
            "q_tp1_using_online_net = _build_q_network(",
            "self.obs_tp1, num_actions, config)",
            "-            q_tp1_best_using_online_net = tf.arg_max(q_tp1_using_online_net, 1)",
            "+            q_tp1_best_using_online_net = tf.argmax(q_tp1_using_online_net, 1)",
            "q_tp1_best = tf.reduce_sum(",
            "self.q_tp1 * tf.one_hot(",
            "q_tp1_best_using_online_net, num_actions), 1)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=arg_max), value='argmax')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 4136,
        "neg_line": [
            "-q_tp1_best_using_online_net = tf.arg_max(q_tp1_using_online_net, 1)"
        ],
        "pos_line": [
            "+q_tp1_best_using_online_net = tf.argmax(q_tp1_using_online_net, 1)"
        ],
        "core_change": "-q_tp1_best_using_online_net = tf.arg_max(q_tp1_using_online_net, 1) +q_tp1_best_using_online_net = tf.argmax(q_tp1_using_online_net, 1)",
        "core_API": "variable_scope"
    },
    {
        "commit_hash": "cec92098641d3f4c395cd51d84ba93b691d1cdf3",
        "index": "d10de4d8..cceaa6ea 100644",
        "commit_message": "Several micro optimizations (#4833)\n\n* benchmark transfers\n\n* create tensors directl on device when possible\n\n* fix\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ROUGE(Metric):",
            "sequence_count = len(predictions)",
            "if is_distributed():",
            "device = predictions.device",
            "-            _sequence_count = torch.tensor(sequence_count).to(device)",
            "+            _sequence_count = torch.tensor(sequence_count, device=device)",
            "dist.all_reduce(_sequence_count, op=dist.ReduceOp.SUM)",
            "sequence_count = _sequence_count.item()",
            "self._total_sequence_count += sequence_count"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=5503)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=(, text=(), position=0)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=sequence_count), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=5504)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=3, insert_id=5505)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=), text=)), position=4)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=5506)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=5507)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=identifier, text=device), position=2)",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=to))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 18,
        "number": 4137,
        "neg_line": [
            "-_sequence_count = torch.tensor(sequence_count).to(device)"
        ],
        "pos_line": [
            "+_sequence_count = torch.tensor(sequence_count, device=device)"
        ],
        "core_change": "-_sequence_count = torch.tensor(sequence_count).to(device) +_sequence_count = torch.tensor(sequence_count, device=device)",
        "core_API": "tensor"
    },
    {
        "commit_hash": "da71df1afcff0d119299393fabcfa2a808c5c9dc",
        "index": "69fb0d0e0..d5854220d 100644",
        "commit_message": "fix integration test levit (#17555)\n\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class LevitModelIntegrationTest(unittest.TestCase):",
            "expected_shape = torch.Size((1, 1000))",
            "self.assertEqual(outputs.logits.shape, expected_shape)",
            "",
            "-        expected_slice = torch.tensor([0.0096, -1.0084, -1.4318]).to(torch_device)",
            "+        expected_slice = torch.tensor([1.0448, -0.3745, -1.8317]).to(torch_device)",
            "",
            "self.assertTrue(torch.allclose(outputs.logits[0, :3], expected_slice, atol=1e-4))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=float, text=0.0096), value='1.0448')",
            "Update(target_node=ASTNode(type=float, text=1.0084), value='0.3745')",
            "Update(target_node=ASTNode(type=float, text=1.4318), value='1.8317')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 3,
        "number": 4138,
        "neg_line": [
            "-expected_slice = torch.tensor([0.0096, -1.0084, -1.4318]).to(torch_device)"
        ],
        "pos_line": [
            "+expected_slice = torch.tensor([1.0448, -0.3745, -1.8317]).to(torch_device)"
        ],
        "core_change": "-expected_slice = torch.tensor([0.0096, -1.0084, -1.4318]).to(torch_device) +expected_slice = torch.tensor([1.0448, -0.3745, -1.8317]).to(torch_device)",
        "core_API": "Size"
    },
    {
        "commit_hash": "ac99217e92c43066af7ec96554054d75532565d7",
        "index": "b2c11bb44..ef00cd869 100644",
        "commit_message": "Fix the CI (#4903)\n\n* Fix CI\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ModelTesterMixin:",
            "model.to(torch_device)",
            "model.eval()",
            "with torch.no_grad():",
            "-                outputs = model(**inputs_dict)",
            "+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))",
            "attentions = outputs[-1]",
            "self.assertEqual(model.config.output_hidden_states, False)",
            "self.assertEqual(len(attentions), self.model_tester.num_hidden_layers)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=dictionary_splat), node=('call', None), position=1, insert_id=1237481)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1237482)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1237483)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=1237484)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1237485)",
            "Insert(target_node=IN(type=attribute), node=('identifier', '_prepare_for_class'), position=2, insert_id=1237486)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1237487)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=inputs_dict), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=1237488)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'model_class'), position=3, insert_id=1237489)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=1237490)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 4140,
        "neg_line": [
            "-outputs = model(**inputs_dict)"
        ],
        "pos_line": [
            "+outputs = model(**self._prepare_for_class(inputs_dict, model_class))"
        ],
        "core_change": "-outputs = model(**inputs_dict) +outputs = model(**self._prepare_for_class(inputs_dict, model_class))",
        "core_API": "to"
    },
    {
        "commit_hash": "0dfd6b624081fc4e1c72fc74ae0cd2de199c334c",
        "index": "8b7cadb0..2a0f20d0 100644",
        "commit_message": "Add linting with black (#2678)\n\nSummary:\n# Before submitting\n\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\n- [ ] Did you make sure to update the docs?\n- [ ] Did you write any new necessary tests?\n\n## What does this PR do?\nFixes # (issue).\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding \n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2678\n\nReviewed By: Mortimerp9\n\nDifferential Revision: D32653381\n\nPulled By: dianaml0\n\nfbshipit-source-id: 2810d14867cd7d64f4d340740e2b590b82de47fe\n\n",
        "file": "fairseq.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "optim",
        "change": [
            "def setup_model_loss_criterion(cfg, args, rank, is_cuda):",
            "loss_fn = loss_fn.cuda()",
            "",
            "optimizer = optim.sgd.SGD(args, model.parameters())",
            "-    optimizer = optim.FairseqBMUF(",
            "-        cfg=cfg.bmuf,",
            "-        optimizer=optimizer",
            "-    )",
            "+    optimizer = optim.FairseqBMUF(cfg=cfg.bmuf, optimizer=optimizer)",
            "",
            "return model, loss_fn, optimizer"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 4,
        "AST_diff_line": 17,
        "number": 4141,
        "neg_line": [
            "-optimizer = optim.FairseqBMUF(",
            "-cfg=cfg.bmuf,",
            "-optimizer=optimizer",
            "-)"
        ],
        "pos_line": [
            "+optimizer = optim.FairseqBMUF(cfg=cfg.bmuf, optimizer=optimizer)"
        ],
        "core_change": "-optimizer = optim.FairseqBMUF( -cfg=cfg.bmuf, -optimizer=optimizer -) +optimizer = optim.FairseqBMUF(cfg=cfg.bmuf, optimizer=optimizer)",
        "core_API": "cuda"
    },
    {
        "commit_hash": "6b423ba8bde19c831b7e8c7eb1d6d67f886d71ad",
        "index": "e8e7137c..d8a73eab 100644",
        "commit_message": "Heterogeneous Graph Support + GraphGym (#3068)\n\n* added HGT DBLP example\n\n* typo\n\n* Merge PyG master (#52)\n\n* Adding the Facebok Page-Page dataset\n\n* type hints\n\n* documentation CI\n\n* py 3.8\n\n* fix links\n\n* fix links\n\n* fail on warning\n\n* fail on warning\n\n* fix doc\n\nCo-authored-by: benedekrozemberczki <benedek.rozemberczki@gmail.com>\n\n* revert\n\n* Fix Documentation Rendering (#51)\n\n* fix doc rendering\n\n* fix linting\n\n* retrigger checks\n\n* remove pytorch 1.7.0 legacy code (#50)\n\n* Fix `copy.deepcopy` within lazy `nn.dense.Linear` (#44)\n\n* fix deepcopy within lazy Linear\n\n* fix merge\n\n* assert exception\n\n* example to doc\n\n* resolve conflict\n\n* resolve conflict\n\n* Add Figure and Equation to `to_hetero` docstring (#60)\n\n* add tex\n\n* add svg + docstring\n\n* typo\n\n* added equation\n\n* Message Passing Hooks (#53)\n\n* add hooks\n\n* docstring\n\n* add docstring\n\n* allow modification of inputs/output\n\n* add test for modifying output\n\n* add additional asserts for modifying output test\n\n* Rename `HeteroData.get_edges` and `HeteroData.get_nodes` (#58)\n\n* rename to_edges and to_nodes\n\n* typo\n\n* `HeteroConv` (#64)\n\n* clean heteroconv\n\n* init\n\n* init\n\n* clean up\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* fix documentation\n\n* bipartite function\n\n* fix test CI\n\n* remove pillow version\n\n* clean up for merge\n\n* Merge PyG master (#69)\n\n* renaming: PointConv to PointNetConv\n\n* Fix a broken link in datasets/gdelt.py (#2800)\n\n* fix test\n\n* re-add batching of strings\n\n* add quick start table\n\n* gnn cheatsheet\n\n* remove pillow version\n\nCo-authored-by: Dongkwan Kim <todoaskit@gmail.com>\n\n* re-merge\n\n* add lazy column to GNN cheatsheet (#70)\n\n* `to_hetero_with_bases(model)` (#63)\n\n* update\n\n* fix linting\n\n* basisconv\n\n* add ValueError\n\n* to_hetero_with_bases impl done\n\n* add test\n\n* add comments\n\n* add comments\n\n* docstring\n\n* typo\n\n* update figure\n\n* svg\n\n* typo\n\n* add test\n\n* update\n\n* add rgcn equality test\n\n* typos\n\n* update\n\n* typos\n\n* update figures\n\n* generate new svgs\n\n* fix assignment\n\n* rename\n\n* delete sorted edge types\n\n* rename\n\n* add legend\n\n* fix typo\n\n* Test: Check equal outputs of `to_hetero` and `RGCNConv` (#59)\n\n* check equal output\n\n* add sparsetensor test\n\n* check equal output\n\n* add sparsetensor test\n\n* rename\n\n* linting\n\n* add missing import\n\n* `HeteroData` support for `T.NormalizeFeatures` (#56)\n\n* normalize features\n\n* allow normalization of any feature\n\n* in-place div\n\n* normalize features\n\n* allow normalization of any feature\n\n* in-place div\n\n* fix test\n\n* no need to re-assign\n\n* `HeteroData` support for `T.AddSelfLoops` (#54)\n\n* hetero support for AddSelfLoops\n\n* check for edge_index attribute\n\n* f-string\n\n* retrigger checks\n\n* revert bipartite changes\n\n* hetero support for AddSelfLoops\n\n* check for edge_index attribute\n\n* f-string\n\n* retrigger checks\n\n* revert bipartite changes\n\n* merge master\n\n* merge master\n\n* `HeteroData` support for `T.ToSparseTensor` (#55)\n\n* hetero support for ToSparseTensor\n\n* add test\n\n* customize the attribute of SparseTensor.value\n\n* rework sort_edge_index\n\n* hetero support for ToSparseTensor\n\n* add test\n\n* customize the attribute of SparseTensor.value\n\n* rework sort_edge_index\n\n* linting\n\n* `HeteroData` support for `T.ToUndirected` (#57)\n\n* to_undirected\n\n* revert bipartite changes\n\n* coalesce + undirected enhancement\n\n* merge master\n\n* revert bipartite changes\n\n* coalesce + undirected enhancement\n\n* merge master\n\n* clean up\n\n* new default relation type\n\n* fix tests\n\n* resolve merge conflicts\n\n* resolve merge conflicts 2\n\n* resolve merge conflicts 3\n\n* Merge PyG master (#74)\n\n* renaming: PointConv to PointNetConv\n\n* Fix a broken link in datasets/gdelt.py (#2800)\n\n* fix test\n\n* re-add batching of strings\n\n* add quick start table\n\n* gnn cheatsheet\n\n* remove pillow version\n\n* clean up doc for to_dense_batch\n\n* clean up\n\n* add legend to cheatsheet\n\n* Improve terminology (#2837)\n\nI think the previous version of the document uses the term 'symmetric' incorrectly. A symmetric matrix is a square matrix that is is equal to its transpose (https://en.wikipedia.org/wiki/Symmetric_matrix). However, the text is only talking about the shape of the matrix, not its content. Hence, 'square (matrix)' would be the correct term to use.\n\n* Add batch_size input to to_dense_batch (#2838)\n\n* Add batch_size input to to_dense_batch\n\n* to_dense_batch fix typo in batch_size param use\n\n* add typehints\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* typo\n\n* Added return_attention_weights to TransformerConv. (#2807)\n\n* added return_weights functionality to tranformer\n\n* added return attn weights tests\n\n* flake8\n\n* added typehints\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* MD17 (#2843)\n\n* Added MD17 dataset\n\n* Updated Documentation\n\n* Added link to sGDML website in doc\n\n* fixed typos in doc and made train variable description clearer\n\n* clean up\n\n* fix linting\n\n* fix doc warning\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* update doc\n\n* remove forward doc\n\n* add static graph support info to cheatsheet\n\n* fix num_nodes in case edge_index is empty\n\n* fix math formula\n\n* faster GDC import\n\n* lazy import\n\n* lazy import for datasets\n\n* lazy import for nn\n\n* Sequential jittable + traceable\n\n* typo\n\n* typo\n\n* update doc\n\nCo-authored-by: Dongkwan Kim <todoaskit@gmail.com>\nCo-authored-by: Markus <markus.zopf@outlook.com>\nCo-authored-by: Jimmie <jimmiebtlr@gmail.com>\nCo-authored-by: Jinu Sunil <jinu.sunil@gmail.com>\nCo-authored-by: Moritz R Schfer <moritz.schaefer@protonmail.com>\n\n* re-add\n\n* GraphGym cleaned version (#82)\n\n* GraphGym cleaned version\n\n* remove deepsnap dependency\n\n* fix lint errors, part 1\n\n* fix all lint errors\n\n* fix all lint errors\n\n* fix all lint errors\n\n* apply yapf\n\n* Update .gitignore\n\n* Integrate GraphGym into PyG (#85)\n\n* GraphGym cleaned version\n\n* remove deepsnap dependency\n\n* fix lint errors, part 1\n\n* fix all lint errors\n\n* fix all lint errors\n\n* fix all lint errors\n\n* apply yapf\n\n* Integrate graphgym into pyg, keep user API in project root\n\n* fix merge conflict\n\n* fix lint errors\n\n* Make optional dependencies\n\n* merge LICENSE from GraphGym\n\n* add import\n\n* clean up LICENSE\n\n* fix import\n\n* resolve merge conflicts\n\n* resolve merge conflicts 2\n\n* Merge PyG master (#87)\n\n* renaming: PointConv to PointNetConv\n\n* Fix a broken link in datasets/gdelt.py (#2800)\n\n* fix test\n\n* re-add batching of strings\n\n* add quick start table\n\n* gnn cheatsheet\n\n* remove pillow version\n\n* clean up doc for to_dense_batch\n\n* clean up\n\n* add legend to cheatsheet\n\n* Improve terminology (#2837)\n\nI think the previous version of the document uses the term 'symmetric' incorrectly. A symmetric matrix is a square matrix that is is equal to its transpose (https://en.wikipedia.org/wiki/Symmetric_matrix). However, the text is only talking about the shape of the matrix, not its content. Hence, 'square (matrix)' would be the correct term to use.\n\n* Add batch_size input to to_dense_batch (#2838)\n\n* Add batch_size input to to_dense_batch\n\n* to_dense_batch fix typo in batch_size param use\n\n* add typehints\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* typo\n\n* Added return_attention_weights to TransformerConv. (#2807)\n\n* added return_weights functionality to tranformer\n\n* added return attn weights tests\n\n* flake8\n\n* added typehints\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* MD17 (#2843)\n\n* Added MD17 dataset\n\n* Updated Documentation\n\n* Added link to sGDML website in doc\n\n* fixed typos in doc and made train variable description clearer\n\n* clean up\n\n* fix linting\n\n* fix doc warning\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* update doc\n\n* remove forward doc\n\n* add static graph support info to cheatsheet\n\n* fix num_nodes in case edge_index is empty\n\n* fix math formula\n\n* faster GDC import\n\n* lazy import\n\n* lazy import for datasets\n\n* lazy import for nn\n\n* Sequential jittable + traceable\n\n* typo\n\n* typo\n\n* update doc\n\n* Simple models (#2869)\n\n* Inclusion of new backbone models\n\n* Eliminating head from asap.py\n\n* small correction\n\n* Create test_gcn.py\n\n* Update __init__.py\n\n* Update test_gcn.py\n\n* Left only the convolutional simple models\n\n* Tests included\n\n* update\n\n* clean up\n\n* clean up v2\n\n* fix activation\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* Example for MemPooling. (#2729)\n\n* example for mem pooling\n\n* backprop on kl loss is done at the end of an epoch. Keys in memory layers are trained only on kl loss.\n\n* added learning rate decay. Using PROTIENS_full\n\n* flake8\n\n* reduced lr. increased weight decay\n\n* changed download location\n\n* added comments\n\n* clean up\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* typos\n\n* fix removeisolatednodes transform in case 'data.num_nodes' is present\n\n* fix XConv with dilation > 1\n\n* fix XConv with dilation > 1\n\n* rgcn link prediction  (#2734)\n\n* implemented LinkPrediction dataset for loading FB15k237\n\n* implemented evaluation for relational link prediction\n\n* implemented R-GCNConf link prediction example\n\n* fixed bug: wrong initial objects in negative_sampling\n\n* changed file downloader urllib.request.urlretrieve  to pytorch.data.download_url; renamed LinkPrediction class to RelationalLinkPredictionDataset\n\n* update dataset\n\n* update example script\n\n* rename\n\nCo-authored-by: Moritz <moritzblum>\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* fix gnnexplainer draw kwargs\n\n* remove python-louvain dependency\n\n* allow customization of output in MP jit mode\n\n* fix test for py3.6\n\n* changed normalisation to same norm from instance norm to be robust to small var (#2917)\n\n* add CITATION.cff\n\n* format\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* add basetransform ABC (#2924)\n\n* clean up BaseTransform\n\n* clean up GATConv and add comments\n\n* add max_num_neighbors as an additional argument\n\n* fix jit GATConv on PyTorch 1.8.0\n\n* fix doc\n\n* fix gnn explainer with existing self-loops\n\n* Rgcn link pred fix (#2946)\n\n* added regularization, removed typo in test\n\n* clean up\n\nCo-authored-by: Moritz <moritzblum>\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* typo\n\n* Correct gini coefficient mathcal formula (#2932)\n\n* typo\n\n* typo\n\n* Update from_networkx (#2923)\n\n* Update from_networkx\n\n* Update test\n\n* Update convert.py\n\n* Minor corrections\n\n* Update test_convert.py\n\n* Corrections\n\n* Update test_convert.py\n\n* Case where there are no edges\n\n* Correcting how edge_attr are concatenated\n\n* clean up + new test\n\n* remove unused code\n\n* add union type\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* fix deterministic ordering in from_networkx\n\n* recursive-include *.jinja files\n\nCo-authored-by: Dongkwan Kim <todoaskit@gmail.com>\nCo-authored-by: Markus <markus.zopf@outlook.com>\nCo-authored-by: Jimmie <jimmiebtlr@gmail.com>\nCo-authored-by: Jinu Sunil <jinu.sunil@gmail.com>\nCo-authored-by: Moritz R Schfer <moritz.schaefer@protonmail.com>\nCo-authored-by: PabloAMC <pmorenocf@alumnos.unex.es>\nCo-authored-by: Moritz Blum <31183934+moritzblum@users.noreply.github.com>\nCo-authored-by: fbragman <fbragman@users.noreply.github.com>\nCo-authored-by: Christopher Lee <2824685+CCInc@users.noreply.github.com>\nCo-authored-by: Tim Daubenschtz <tim@daubenschuetz.de>\n\n* resolve merge conflicts 3\n\n* resolve merge conflicts 4\n\n* Implementation of the `HGTLoader` + `ogbn-mag` example (#73)\n\n* first try\n\n* update\n\n* HGT Loader\n\n* typo\n\n* first try\n\n* update\n\n* HGT Loader\n\n* typo\n\n* bugfixes\n\n* lazy GATConv\n\n* bugfix\n\n* bugfix\n\n* full working pipeline\n\n* update\n\n* rename\n\n* docstring\n\n* typos\n\n* update\n\n* typo\n\n* typo\n\n* typo\n\n* added comments\n\n* add test\n\n* add tests\n\n* fix example\n\n* rename\n\n* linting\n\n* Random split functionalities (#72)\n\n* link split\n\n* create split\n\n* example tests\n\n* link split tests\n\n* fix linting\n\n* update docstring\n\n* undirected option, refactor and docs\n\n* add num nodes as argument to neg sampling\n\n* clean up + remove single object\n\n* update example\n\n* typo\n\n* fix compose\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* add basetransform\n\n* typo\n\n* typo\n\n* fix test\n\n* Improve `torch_geometric.data` Documentation (#98)\n\n* update data doc\n\n* typo\n\n* typo\n\n* note\n\n* typo\n\n* add docstring\n\n* only show inherited members for data and hetero_data\n\n* documentation update for batch and dataset\n\n* update doc\n\n* update\n\n* fix\n\n* record_stream\n\n* update\n\n* typo\n\n* add/fix data functionality\n\n* linting\n\n* typo\n\n* `_parent` memory leak fix (#103)\n\n* memory leak fix\n\n* Clean up\n\n* clean up\n\n* bugfix tests\n\n* typos\n\n* fix test\n\n* fix test\n\n* rename reverse\n\n* (Heterogeneous) `NeighborLoader` (#92)\n\n* initial commit\n\n* typo\n\n* neighbor loader functionality + tests\n\n* docstring\n\n* fix docstring\n\n* skip tests\n\n* fix share_memory_\n\n* typo\n\n* typo\n\n* update example\n\n* typo\n\n* share_strategy\n\n* fix cuda calls\n\n* better print\n\n* fix size\n\n* fix print\n\n* final commit\n\n* fix\n\n* some todos\n\n* preprocessed features\n\n* fix to_undirected\n\n* more documentation\n\n* update doc\n\n* fix doc\n\n* fix doc\n\n* Add benchmark code and the example with existing graph classification examples (#93)\n\n* add benchmarking utilities\n\n* update graph classification benchmark\n\n* improve code style\n\n* add pytorch-memlab for benchmark code\n\n* skip some tests when cuda is not available\n\n* add type hint when appropriate\n\n* add seed_everything to improve code\n\n* code refactoring\n\n* code refactoring\n\n* code refactoring\n\n* code improvement\n\n* remove unnecessary dataloader import\n\n* change benchmark interface with decorator\n\n* documentation improvement\n\n* linting\n\n* linting part 2\n\n* linting part 3\n\n* seed_everything\n\n* create utils file\n\n* update\n\n* use utils functions\n\n* fix test\n\n* update the profiler to the latest torch (1.8.1+)\n\n* refactor profiler and add more documentation\n\n* refactor profiler and add more documentation\n\n* resolve lint errors\n\n* resolve lint errors\n\n* update\n\n* clean up test and profile\n\n* fix linting\n\n* add to doc\n\n* fix doc\n\n* typo\n\n* update benchmark\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* Move `HGTLoader` to `torch_geometric.loader` + clean up (#104)\n\n* move files\n\n* use utils functions\n\n* fix example\n\n* update\n\n* fix tests\n\n* fix seed\n\n* fix linear test\n\n* rename\n\n* Support GraphGym custom modules outside PyG package (#102)\n\n* GraphGym cleaned version\n\n* remove deepsnap dependency\n\n* fix lint errors, part 1\n\n* fix all lint errors\n\n* fix all lint errors\n\n* fix all lint errors\n\n* apply yapf\n\n* Integrate graphgym into pyg, keep user API in project root\n\n* fix merge conflict\n\n* fix lint errors\n\n* Make optional dependencies\n\n* merge LICENSE from GraphGym\n\n* Enable adding GraphGym customized modules outside PyG package\n\n* lint\n\n* Rename `AddTrainValTestMask` to `RandomNodeSplit` (#108)\n\n* initial commit\n\n* rename example\n\n* remove AddTrainValTestMask\n\n* fix linting\n\n* create optimizer config and scheduler config separately (#113)\n\n* create optimizer config and scheduler config separately\n\n* fix format\n\n* import explicitly\n\nCo-authored-by: Dong Wang <dongwang@yannis-air.lan>\n\n* Heterogeneous Graph Tutorial (#83)\n\n* add HG tutorial roadmap\n\n* started working on hg tutorial\n\n* hg_tutorial, some text and .tex figure\n\n* added svg\n\n* hg tutorial content\n\n* fix CI\n\n* text and structure\n\n* finished first draft\n\n* fixed one code example\n\n* fixing conventions\n\n* fixing links\n\n* update svg\n\n* some smaller improvements of tutorial\n\n* improvements on tutorial\n\n* hg-tutorial: fixed compiling issue, added detailed content\n\n* added absolute links\n\n* fixed warnings\n\n* streamlined dataset section\n\n* update svg\n\n* update tutorial\n\n* update 2\n\nCo-authored-by: Jan Eric Lenssen <janeric.lenssen@tu-dortmund.de>\n\n* typo\n\n* Move data loaders to `torch_geometric.loader` (#110)\n\n* move graphsaint\n\n* deprecations\n\n* move clusterloader\n\n* deprecations\n\n* type hints\n\n* move shadow\n\n* typo\n\n* typo\n\n* move datalistloader\n\n* dense data loader\n\n* random node sampler\n\n* fix doc\n\n* Lazy GNN operators (#89)\n\n* lazy cheb conv\n\n* lazy GraphConv\n\n* lazy GATv2Conv\n\n* lazy TAGConv\n\n* lazy FAConv\n\n* lazy FeaStConv\n\n* lazy NNConv\n\n* typo\n\n* fix tests\n\n* lazy SuperGATConv\n\n* lazy SuperGATConv fix\n\n* lazy SplineConv\n\n* fix lazy check\n\n* lazy GravNetConv\n\n* arma conv lazy\n\n* dense linear in gmmconv\n\n* typo\n\n* add test\n\n* lazy GMMConv\n\n* doc\n\n* rename (#116)\n\n* Revisit `MetaPath2Vec` (#114)\n\n* revisit metapath2vec\n\n* update\n\n* typo\n\n* update\n\n* fix doc\n\n* update\n\n* check for attributes rather than key\n\n* Clean up `torch_geometric.profile` further (#111)\n\n* remove print_layer_stats\n\n* typos\n\n* update\n\n* readme highlights and quick tour (#99)\n\n* readme highlights and quick tour\n\n* arch\n\n* arch image\n\n* arch overview\n\n* list categories\n\n* categorization\n\n* category description\n\n* Update README.md\n\nfrom Matthias\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* improved highlights\n\n* Update README.md\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* Update README.md\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* Update README.md\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* Update README.md\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* minor\n\n* update readme\n\n* update\n\n* update\n\n* update\n\n* update\n\n* fix url\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* move ops\n\n* toc\n\n* typo\n\n* typo\n\n* add svgs\n\n* update figure\n\n* fix links\n\n* fix size\n\n* fix size\n\n* typo\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* fix broken links\n\n* fix links\n\n* Heterogeneous Graph Sampler Tutorial (#117)\n\n* initial commit\n\n* address comments\n\n* remove todo\n\n* typo\n\n* Conversion between heterogenous and homogeneous graph objects (#115)\n\n* temp checkpoint (wip, will remove)\n\n* (wip) typed graph conversion\n\n* (wip) typed graph conversion\n\n* (wip) typed graph conversion\n\n* update\n\n* typo\n\n* delete examples\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* fix test\n\n* update doc\n\n* deprecate NeighborSampler (#119)\n\n* Move `torch_geometric.data.DataLoader` to `torch_geometric.loader.DataLoader` (#120)\n\n* move dataloader\n\n* rename\n\n* typos\n\n* typos\n\n* fix __cat_dim__\n\n* updategp\n\n* Deprecate `train_test_split_edges` + Modifications to `RandomLinkSplit` (#121)\n\n* deprecate train_test_split_edges\n\n* to device transform\n\n* fix example\n\n* add split_labels argument\n\n* fix autoencoder example\n\n* typos\n\n* add docstring\n\n* ARGVA\n\n* seal\n\n* adress comments\n\n* Create example to load `*.csv` and transfer to `HeteroData` (#76)\n\n* create example to load csv file and transfer to heter-data\n\n* add ipython notebook version load csv with documentation\n\n* address comment\n\n* first version of csv loading doc\n\n* first version of csv loading doc\n\n* suggestion docs/source/notes/loading_csv.rst\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* suggestion docs/source/notes/loading_csv.rst\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* suggestion docs/source/notes/loading_csv.rst\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* suggestion docs/source/notes/loading_csv.rst\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* suggestions csv tutorial\n\n* example script load csv + extract fix\n\n* fixed edge index stacking dimension in example and jupyter nb\n\n* linting\n\n* linting2\n\n* rename\n\n* update\n\n* update\n\n* update\n\n* typo\n\n* typo\n\n* update\n\n* rename\n\n* update tutorial\n\n* typo\n\n* address comments\n\nCo-authored-by: Dong Wang <dongwang@yannis-air.lan>\nCo-authored-by: Jan Eric Lenssen <janeric.lenssen@tu-dortmund.de>\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* typo\n\n* fix\n\n* typo\n\n* update\n\n* fix\n\n* fix\n\nCo-authored-by: benedekrozemberczki <benedek.rozemberczki@gmail.com>\nCo-authored-by: Rex Ying <rexying@stanford.edu>\nCo-authored-by: Dongkwan Kim <todoaskit@gmail.com>\nCo-authored-by: Markus <markus.zopf@outlook.com>\nCo-authored-by: Jimmie <jimmiebtlr@gmail.com>\nCo-authored-by: Jinu Sunil <jinu.sunil@gmail.com>\nCo-authored-by: Moritz R Schfer <moritz.schaefer@protonmail.com>\nCo-authored-by: Jiaxuan <youjiaxuan@gmail.com>\nCo-authored-by: PabloAMC <pmorenocf@alumnos.unex.es>\nCo-authored-by: Moritz Blum <31183934+moritzblum@users.noreply.github.com>\nCo-authored-by: fbragman <fbragman@users.noreply.github.com>\nCo-authored-by: Christopher Lee <2824685+CCInc@users.noreply.github.com>\nCo-authored-by: Tim Daubenschtz <tim@daubenschuetz.de>\nCo-authored-by: Yue Zhao <yzhao062@gmail.com>\nCo-authored-by: Dong Wang <dongw89@gmail.com>\nCo-authored-by: Dong Wang <dongwang@yannis-air.lan>\nCo-authored-by: Jan Eric Lenssen <janeric.lenssen@tu-dortmund.de>\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Net(torch.nn.Module):",
            "x = global_max_pool(x, batch)",
            "x = self.lin(x).view(-1)",
            "",
            "-        attn_loss = F.kl_div(",
            "-            torch.log(score + 1e-14), data.attn[perm], reduction='none')",
            "+        attn_loss = F.kl_div(torch.log(score + 1e-14), data.attn[perm],",
            "+                             reduction='none')",
            "attn_loss = scatter_mean(attn_loss, batch)",
            "",
            "return x, attn_loss, ratio"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 4142,
        "neg_line": [
            "-attn_loss = F.kl_div(",
            "-torch.log(score + 1e-14), data.attn[perm], reduction='none')"
        ],
        "pos_line": [
            "+attn_loss = F.kl_div(torch.log(score + 1e-14), data.attn[perm],",
            "+reduction='none')"
        ],
        "core_change": "-attn_loss = F.kl_div( -torch.log(score + 1e-14), data.attn[perm], reduction='none') +attn_loss = F.kl_div(torch.log(score + 1e-14), data.attn[perm], +reduction='none')",
        "core_API": "lin"
    },
    {
        "commit_hash": "d4c0895cb98f5aa6f112a6f28533a49ad9d6312a",
        "index": "797aa263..6a2e432c 100644",
        "commit_message": "Bump torch, torchvision, python versions (#2663)\n\n* Bump torch, torchvision, python versions\n\n* Fix .nonzero() usage\n\n* Add missing licenses\n\n* Fix more .nonzero() errors\n\n* Fix errors\n\n* Fix pyplot.hist invocation\n\n* Work around torch 1.6 incompatibility with pyplot.hist\n\n* Fix more pyplot.hist() errors\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Model(ForecastingModel):",
            "",
            "# Sample global parameters.",
            "noise_scale = pyro.sample(\"noise_scale\",",
            "-                                  dist.LogNormal(torch.full((dim,), -3), 1).to_event(1))",
            "+                                  dist.LogNormal(torch.full((dim,), -3.), 1.).to_event(1))",
            "assert noise_scale.shape[-1:] == (dim,)",
            "trans_timescale = pyro.sample(\"trans_timescale\",",
            "dist.LogNormal(torch.zeros(dim), 1).to_event(1))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('float', '1.'), position=3, insert_id=690894)",
            "Insert(target_node=ASTNode(type=argument_list), node=('unary_operator', None), position=3, insert_id=690895)",
            "Insert(target_node=IN(type=unary_operator), node=('-', '-'), position=0, insert_id=690896)",
            "Insert(target_node=IN(type=unary_operator), node=('float', '3.'), position=1, insert_id=690897)",
            "Delete(target_node=ASTNode(type=unary_operator, text=-3))",
            "Delete(target_node=ASTNode(type=integer, text=1))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 4143,
        "neg_line": [
            "-dist.LogNormal(torch.full((dim,), -3), 1).to_event(1))"
        ],
        "pos_line": [
            "+dist.LogNormal(torch.full((dim,), -3.), 1.).to_event(1))"
        ],
        "core_change": "-dist.LogNormal(torch.full((dim,), -3), 1).to_event(1)) +dist.LogNormal(torch.full((dim,), -3.), 1.).to_event(1))",
        "core_API": "sample"
    },
    {
        "commit_hash": "2f0fd60186ef3ae8c6691841be315609f196fe42",
        "index": "146fb60..7f5a0e4 100644",
        "commit_message": "fix pts scale, save ply\n\nSummary:\nFix:\n* Scaling of point clouds for scalars\n* save_ply compatible cat\n\nReviewed By: nikhilaravi\n\nDifferential Revision: D22298609\n\nfbshipit-source-id: abe94a5b64baf325587202d20adfc36912cc1478\n\n",
        "file": "pytorch3d.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Pointclouds(object):",
            "self.",
            "\"\"\"",
            "if not torch.is_tensor(scale):",
            "-            scale = torch.full(len(self), scale)",
            "+            scale = torch.full((len(self),), scale, device=self.device)",
            "new_points_list = []",
            "points_list = self.points_list()",
            "for i, old_points in enumerate(points_list):"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('tuple', None), position=1, insert_id=925290)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=925291)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=5, insert_id=925292)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=6, insert_id=925293)",
            "Insert(target_node=IN(type=tuple), node=('(', '('), position=0, insert_id=925294)",
            "Move(target_node=IN(type=tuple), node=ASTNode(type=call), position=1)",
            "Move(target_node=IN(type=tuple), node=ASTNode(type=,, text=,), position=2)",
            "Insert(target_node=IN(type=tuple), node=(')', ')'), position=3, insert_id=925295)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=925296)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=925297)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=925298)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=925299)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=925300)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=925301)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 14,
        "number": 4147,
        "neg_line": [
            "-scale = torch.full(len(self), scale)"
        ],
        "pos_line": [
            "+scale = torch.full((len(self),), scale, device=self.device)"
        ],
        "core_change": "-scale = torch.full(len(self), scale) +scale = torch.full((len(self),), scale, device=self.device)",
        "core_API": "is_tensor"
    },
    {
        "commit_hash": "5f4c6e830f603830117877b8990a0e65a2386aa6",
        "index": "c68926174..508f90cbf 100644",
        "commit_message": "Quick fix :) (#606)\n\n* Changing the name\n\n* style + quality\n\n* update doc and logo\n\n* clean up\n\n* circle-CI on the branche for now\n\n* fix daily dialog dataset\n\n* fix urls\n\nCo-authored-by: Quentin Lhoest <lhoest.q@gmail.com>\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class TranslationVariableLanguages:",
            "",
            "# At construction time:",
            "",
            "-        nlp.features.Translation(languages=['en', 'fr', 'de'])",
            "+        datasets.features.Translation(languages=['en', 'fr', 'de'])",
            "",
            "# During data generation:"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=nlp), value='datasets')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 4148,
        "neg_line": [
            "-nlp.features.Translation(languages=['en', 'fr', 'de'])"
        ],
        "pos_line": [
            "+datasets.features.Translation(languages=['en', 'fr', 'de'])"
        ],
        "core_change": "-nlp.features.Translation(languages=['en', 'fr', 'de']) +datasets.features.Translation(languages=['en', 'fr', 'de'])",
        "core_API": "Translation"
    },
    {
        "commit_hash": "f2229ae984d84a26e17e71cf0af99f28f83dd060",
        "index": "0fbb88a9..10e2ffba 100644",
        "commit_message": "py2.7 fix\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def stack(sequence, horizontal=True, vertical=True):",
            "# Concat all indices and values to one new large sparse matrix.",
            "indices = torch.cat(indices, dim=1)",
            "values = torch.cat([mat._values() for mat in sequence])",
            "-    size = torch.Size([y_sum, x_sum, *(sequence[0].size()[2:])])",
            "+    size = torch.Size([y_sum, x_sum, *list(sequence[0].size()[2:])])",
            "slices = torch.LongTensor(slices)",
            "",
            "return torch.sparse.FloatTensor(indices, values, size), slices"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=list_splat), node=('call', None), position=1, insert_id=1092705)",
            "Insert(target_node=IN(type=call), node=('identifier', 'list'), position=0, insert_id=1092706)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1092707)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=(, text=(), position=0)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=subscript), position=1)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=), text=)), position=2)",
            "Delete(target_node=ASTNode(type=parenthesized_expression))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 4153,
        "neg_line": [
            "-size = torch.Size([y_sum, x_sum, *(sequence[0].size()[2:])])"
        ],
        "pos_line": [
            "+size = torch.Size([y_sum, x_sum, *list(sequence[0].size()[2:])])"
        ],
        "core_change": "-size = torch.Size([y_sum, x_sum, *(sequence[0].size()[2:])]) +size = torch.Size([y_sum, x_sum, *list(sequence[0].size()[2:])])",
        "core_API": "cat"
    },
    {
        "commit_hash": "5866646cc8347e45c17ca1204f3e770712365f99",
        "index": "2e92ccb..a288312 100644",
        "commit_message": "Fix float zeros format (#5491)\n\n* Fix float zeros format\n\n* 255 to integer\n",
        "file": "yolov5.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Sum(nn.Module):",
            "self.weight = weight  # apply weights boolean",
            "self.iter = range(n - 1)  # iter object",
            "if weight:",
            "-            self.w = nn.Parameter(-torch.arange(1., n) / 2, requires_grad=True)  # layer weights",
            "+            self.w = nn.Parameter(-torch.arange(1.0, n) / 2, requires_grad=True)  # layer weights",
            "",
            "def forward(self, x):",
            "y = x[0]  # no weight"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=float, text=1.), value='1.0')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 4156,
        "neg_line": [
            "-self.w = nn.Parameter(-torch.arange(1., n) / 2, requires_grad=True)  # layer weights"
        ],
        "pos_line": [
            "+self.w = nn.Parameter(-torch.arange(1.0, n) / 2, requires_grad=True)  # layer weights"
        ],
        "core_change": "-self.w = nn.Parameter(-torch.arange(1., n) / 2, requires_grad=True)  # layer weights +self.w = nn.Parameter(-torch.arange(1.0, n) / 2, requires_grad=True)  # layer weights",
        "core_API": "Parameter"
    },
    {
        "commit_hash": "5b931e64e262c3b44125fdb2534fb4a940cd6e79",
        "index": "5ea6c447..619499e6 100644",
        "commit_message": "Fix serialization error due to EagerTensor constant\n\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def EfficientNet(",
            "# original implementation.",
            "# See https://github.com/tensorflow/tensorflow/issues/49930 for more",
            "# details",
            "-        x = layers.Rescaling(1.0 / tf.math.sqrt(IMAGENET_STDDEV_RGB))(x)",
            "+        x = layers.Rescaling(",
            "+            [1.0 / math.sqrt(stddev) for stddev in IMAGENET_STDDEV_RGB]",
            "+        )(x)",
            "",
            "x = layers.ZeroPadding2D(",
            "padding=imagenet_utils.correct_pad(x, 3), name=\"stem_conv_pad\""
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('(', '('), position=0, insert_id=2042683)",
            "Insert(target_node=ASTNode(type=argument_list), node=('list_comprehension', None), position=1, insert_id=2042684)",
            "Insert(target_node=IN(type=list_comprehension), node=('[', '['), position=0, insert_id=2042685)",
            "Move(target_node=IN(type=list_comprehension), node=ASTNode(type=binary_operator), position=1)",
            "Insert(target_node=IN(type=list_comprehension), node=('for_in_clause', None), position=2, insert_id=2042686)",
            "Insert(target_node=IN(type=list_comprehension), node=(']', ']'), position=3, insert_id=2042687)",
            "Insert(target_node=IN(type=for_in_clause), node=('for', 'for'), position=0, insert_id=2042688)",
            "Insert(target_node=IN(type=for_in_clause), node=('identifier', 'stddev'), position=1, insert_id=2042689)",
            "Insert(target_node=IN(type=for_in_clause), node=('in', 'in'), position=2, insert_id=2042690)",
            "Move(target_node=IN(type=for_in_clause), node=ASTNode(type=identifier, text=IMAGENET_STDDEV_RGB), position=3)",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=math), position=0)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=(, text=(), position=0)",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'stddev'), position=1, insert_id=2042691)",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 4159,
        "neg_line": [
            "-x = layers.Rescaling(1.0 / tf.math.sqrt(IMAGENET_STDDEV_RGB))(x)"
        ],
        "pos_line": [
            "+x = layers.Rescaling(",
            "+[1.0 / math.sqrt(stddev) for stddev in IMAGENET_STDDEV_RGB]",
            "+)(x)"
        ],
        "core_change": "-x = layers.Rescaling(1.0 / tf.math.sqrt(IMAGENET_STDDEV_RGB))(x) +x = layers.Rescaling( +[1.0 / math.sqrt(stddev) for stddev in IMAGENET_STDDEV_RGB] +)(x)",
        "core_API": "Rescaling"
    },
    {
        "commit_hash": "4f71c63771718b8d2865f4b6fdec39232815eb3b",
        "index": "1234b772..68d2f253 100644",
        "commit_message": "Fix padding_idx logical error in Adaptive Input (#1629)\n\nSummary:\n# Before submitting\n\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\n- [ ] Did you make sure to update the docs?\n- [ ] Did you write any new necessary tests?\n\n## What does this PR do?\n\nI think if we keep pass **padding index of vocabulary** as `padding_idx` to adaptive embedding layers,\nthere will be no chance to train some words.\n\ne.g. If `cut_off` is (20000,60000) and vocab is larger than 60000,\nwe can't learn[**20,000+padding_idx**]th word and [**60,000+padding_idx**]th word.\nBecause those words' ids will be **padding_idx** by subtraction logic and eventually get zero tensors.\n\nSo, I changed `self.padding_idx` to `None` after assign vocab's `padding_idx`\n**for the first time at head embedding representation**.\nPull Request resolved: https://github.com/pytorch/fairseq/pull/1629\n\nDifferential Revision: D19557340\n\nPulled By: myleott\n\nfbshipit-source-id: e0c3b38862374d422a46dc62c248b2ecfbf08fd2\n\n",
        "file": "fairseq.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class AdaptiveInput(nn.Module):",
            "size = self.cutoff[i] - prev",
            "dim = int(initial_dim // (factor ** i))",
            "seq = nn.Sequential(",
            "-                nn.Embedding(size, dim, padding_idx),",
            "+                nn.Embedding(size, dim, self.padding_idx),",
            "nn.Linear(dim, output_dim, bias=False)",
            ")",
            "self.embeddings.append(seq)",
            "+            self.padding_idx = None",
            "+        self.padding_idx = padding_idx",
            "",
            "def init_weights(m):",
            "if isinstance(m, nn.Embedding):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=5, insert_id=1356853)",
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=6, insert_id=1356854)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1356855)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1356856)",
            "Insert(target_node=IN(type=assignment), node=('attribute', None), position=0, insert_id=1356857)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1356858)",
            "Insert(target_node=IN(type=assignment), node=('none', 'None'), position=2, insert_id=1356859)",
            "Insert(target_node=IN(type=assignment), node=('attribute', None), position=0, insert_id=1356860)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1356861)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'padding_idx'), position=2, insert_id=1356862)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=1356863)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1356864)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'padding_idx'), position=2, insert_id=1356865)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=1356866)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1356867)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'padding_idx'), position=2, insert_id=1356868)",
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=5, insert_id=1356869)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=1356870)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1356871)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=padding_idx), position=2)"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 20,
        "number": 4160,
        "neg_line": [
            "-nn.Embedding(size, dim, padding_idx),"
        ],
        "pos_line": [
            "+nn.Embedding(size, dim, self.padding_idx),",
            "+self.padding_idx = None",
            "+self.padding_idx = padding_idx"
        ],
        "core_change": "-nn.Embedding(size, dim, padding_idx), +nn.Embedding(size, dim, self.padding_idx), +self.padding_idx = None +self.padding_idx = padding_idx",
        "core_API": "Sequential"
    },
    {
        "commit_hash": "d4b3e56d6443aff5148419854f9d4cd45d2db915",
        "index": "ce8db379b..29eddb7f7 100644",
        "commit_message": "[Hotfix] Fix Swin model outputs (#15414)\n\n* Fix Swin model outputs\n\n* Rename pooler\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SwinModelIntegrationTest(unittest.TestCase):",
            "expected_shape = torch.Size((1, 1000))",
            "self.assertEqual(outputs.logits.shape, expected_shape)",
            "",
            "-        expected_slice = torch.tensor([-0.2952, -0.4777, 0.2025]).to(torch_device)",
            "+        expected_slice = torch.tensor([-0.0948, -0.6454, -0.0921]).to(torch_device)",
            "",
            "self.assertTrue(torch.allclose(outputs.logits[0, :3], expected_slice, atol=1e-4))"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=list), node=('unary_operator', None), position=5, insert_id=1207551)",
            "Update(target_node=ASTNode(type=float, text=0.2952), value='0.0948')",
            "Update(target_node=ASTNode(type=float, text=0.4777), value='0.6454')",
            "Insert(target_node=IN(type=unary_operator), node=('-', '-'), position=0, insert_id=1207552)",
            "Insert(target_node=IN(type=unary_operator), node=('float', '0.0921'), position=1, insert_id=1207553)",
            "Delete(target_node=ASTNode(type=float, text=0.2025))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 4164,
        "neg_line": [
            "-expected_slice = torch.tensor([-0.2952, -0.4777, 0.2025]).to(torch_device)"
        ],
        "pos_line": [
            "+expected_slice = torch.tensor([-0.0948, -0.6454, -0.0921]).to(torch_device)"
        ],
        "core_change": "-expected_slice = torch.tensor([-0.2952, -0.4777, 0.2025]).to(torch_device) +expected_slice = torch.tensor([-0.0948, -0.6454, -0.0921]).to(torch_device)",
        "core_API": "Size"
    },
    {
        "commit_hash": "0655e4a2bd89ccc20f4f1157f65b3e5a61f140e1",
        "index": "f8b5c6ec..7ec39af9 100644",
        "commit_message": "fixed lstm, added gru, other fixes\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Model(Module):",
            "#     raise TensorForceError(\"Invalid model directory/file.\")",
            "",
            "self.saver.restore(sess=self.session, save_path=file)",
            "-        self.session.run(fetches=self.list_buffer_index_reset_op)",
            "+        self.session.run(fetches=self.reset_buffer_indices)",
            "",
            "def get_components(self):",
            "\"\"\""
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=list_buffer_index_reset_op), value='reset_buffer_indices')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 4167,
        "neg_line": [
            "-self.session.run(fetches=self.list_buffer_index_reset_op)"
        ],
        "pos_line": [
            "+self.session.run(fetches=self.reset_buffer_indices)"
        ],
        "core_change": "-self.session.run(fetches=self.list_buffer_index_reset_op) +self.session.run(fetches=self.reset_buffer_indices)",
        "core_API": "restore"
    },
    {
        "commit_hash": "3df6c31c612ab0261fe3a5f8dd39b7957f5b2a56",
        "index": "6ff3cbb..6ac0827 100644",
        "commit_message": "fix norm issues in cvt\n\n",
        "file": "vit-pytorch.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class CvT(nn.Module):",
            "",
            "layers.append(nn.Sequential(",
            "nn.Conv2d(dim, config['emb_dim'], kernel_size = config['emb_kernel'], padding = (config['emb_kernel'] // 2), stride = config['emb_stride']),",
            "+                LayerNorm(config['emb_dim']),",
            "Transformer(dim = config['emb_dim'], proj_kernel = config['proj_kernel'], kv_proj_stride = config['kv_proj_stride'], depth = config['depth'], heads = config['heads'], mlp_mult = config['mlp_mult'], dropout = dropout)",
            "))"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=3, insert_id=1278006)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=1278007)",
            "Insert(target_node=IN(type=call), node=('identifier', 'LayerNorm'), position=0, insert_id=1278008)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1278009)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1278010)",
            "Insert(target_node=IN(type=argument_list), node=('subscript', None), position=1, insert_id=1278011)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1278012)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'config'), position=0, insert_id=1278013)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=1278014)",
            "Insert(target_node=IN(type=subscript), node=('string', \"'emb_dim'\"), position=2, insert_id=1278015)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=1278016)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 11,
        "number": 4169,
        "neg_line": [],
        "pos_line": [
            "+LayerNorm(config['emb_dim']),"
        ],
        "core_change": "+LayerNorm(config['emb_dim']),",
        "core_API": "append"
    },
    {
        "commit_hash": "c05ace221339102abb1c8b0933b9cbfd51ddd4a3",
        "index": "32abb5db4..8a5b124a5 100644",
        "commit_message": "Use f-strings in the dataset scripts (#3291)\n\n* Finishes #3257\n\nUsed f-strings to format the .py files in the dataset folder\n\n* Fix style\n\n* Fix hkcancor dataset\n\nCo-authored-by: Mario ako <mario@huggingface.co>\nCo-authored-by: mariosasko <mariosasko777@gmail.com>\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class JigsawToxicityPred(datasets.GeneratorBasedBuilder):",
            "",
            "if not os.path.exists(data_dir):",
            "raise FileNotFoundError(",
            "-                \"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('jigsaw_toxicity_pred', data_dir=...)`. Manual download instructions: {}\".format(",
            "-                    data_dir, self.manual_download_instructions",
            "-                )",
            "+                f\"{data_dir} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('jigsaw_toxicity_pred', data_dir=...)`. Manual download instructions: {self.manual_download_instructions}\"",
            ")",
            "",
            "return ["
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('string', 'f\"{data_dir} does not exist. Make sure you insert a manual dir via `datasets.load_dataset(\\'jigsaw_toxicity_pred\\', data_dir=...)`. Manual download instructions: {self.manual_download_instructions}\"'), position=1, insert_id=1781601)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=), text=)), position=2)",
            "Delete(target_node=ASTNode(type=string, text=\"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('jigsaw_toxicity_pred', data_dir=...)`. Manual download instructions: {}\"))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=format))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=data_dir))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=manual_download_instructions))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=), text=)))"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 16,
        "number": 4179,
        "neg_line": [
            "-\"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('jigsaw_toxicity_pred', data_dir=...)`. Manual download instructions: {}\".format(",
            "-data_dir, self.manual_download_instructions",
            "-)"
        ],
        "pos_line": [
            "+f\"{data_dir} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('jigsaw_toxicity_pred', data_dir=...)`. Manual download instructions: {self.manual_download_instructions}\""
        ],
        "core_change": "-\"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('jigsaw_toxicity_pred', data_dir=...)`. Manual download instructions: {}\".format( -data_dir, self.manual_download_instructions -) +f\"{data_dir} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('jigsaw_toxicity_pred', data_dir=...)`. Manual download instructions: {self.manual_download_instructions}\"",
        "core_API": "exists"
    },
    {
        "commit_hash": "ebd0bc17929b36b851fc825bbfd5fbd41f6d8111",
        "index": "5a17eeb1..c078e0ef 100644",
        "commit_message": "Update to PyTorch 1.11.0 (#3045)\n\n* Run black\n\n* Fix ProvenanceTensor\n\n* Replace torch.triangular_solve -> torch.linalg.solve_triangular\n\n* More fixes to torch.linalg.solve_triangular\n\n* Bump torch version\n\n* Bump Python version 3.6 -> 3.7\n\n* Fix some tests\n\n* Decrease tolerance on stable tests\n\n* Remove obsolete xfail\n\n* Resolve #3032\n\n* Fix solve_triangular in ops.gaussian, revert test weakening\n\n* Fix catching of singular matrices in hmc\n\n* xfail funsor tests\n\n* Work around pandas 1.3 bug\n\n* Allow mypy to install missing stubs\n\n* Clarify triangular_solve test\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class LowerCholeskyAffine(Transform):",
            "",
            "Inverts y => x.",
            "\"\"\"",
            "-        return torch.triangular_solve(",
            "-            (y - self.loc).unsqueeze(-1), self.scale_tril, upper=False, transpose=False",
            "-        )[0].squeeze(-1)",
            "+        return torch.linalg.solve_triangular(",
            "+            self.scale_tril, (y - self.loc).unsqueeze(-1), upper=False",
            "+        ).squeeze(-1)",
            "",
            "def log_abs_det_jacobian(self, x, y):",
            "\"\"\""
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"\"\"\n        return torch.triangular_solve(\n            (y - self.loc).unsqueeze(-1), self.scale_tril, upper=False, transpose=False\n        )[0].squeeze(-1)\n\ndef log_abs_det_jacobian(self, x, y):\n\"\"\"), value='\"\"\"\\n        return torch.linalg.solve_triangular(\\n            self.scale_tril, (y - self.loc).unsqueeze(-1), upper=False\\n        ).squeeze(-1)\\n\\ndef log_abs_det_jacobian(self, x, y):\\n\"\"\"')"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 1,
        "number": 4180,
        "neg_line": [
            "-return torch.triangular_solve(",
            "-(y - self.loc).unsqueeze(-1), self.scale_tril, upper=False, transpose=False",
            "-)[0].squeeze(-1)"
        ],
        "pos_line": [
            "+return torch.linalg.solve_triangular(",
            "+self.scale_tril, (y - self.loc).unsqueeze(-1), upper=False",
            "+).squeeze(-1)"
        ],
        "core_change": "-return torch.triangular_solve( -(y - self.loc).unsqueeze(-1), self.scale_tril, upper=False, transpose=False -)[0].squeeze(-1) +return torch.linalg.solve_triangular( +self.scale_tril, (y - self.loc).unsqueeze(-1), upper=False +).squeeze(-1)",
        "core_API": "triangular_solve"
    },
    {
        "commit_hash": "3bffd2e8e5d726d581e0a66746b25c64d49e231d",
        "index": "efa835107..c0586b03b 100644",
        "commit_message": "more fixes\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def main():",
            "",
            "",
            "# Save the trained model and the tokenizer",
            "-    if args.do_train and args.local_rank == -1 or torch.distributed.get_rank() == 0:",
            "+    if args.do_train and (args.local_rank == -1 or torch.distributed.get_rank() == 0):",
            "# Create output directory if needed",
            "if not os.path.exists(args.output_dir) and args.local_rank in [-1, 0]:",
            "os.makedirs(args.output_dir)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=boolean_operator), node=('parenthesized_expression', None), position=2, insert_id=1247721)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('(', '('), position=0, insert_id=1247722)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('boolean_operator', None), position=1, insert_id=1247723)",
            "Insert(target_node=IN(type=parenthesized_expression), node=(')', ')'), position=2, insert_id=1247724)",
            "Move(target_node=IN(type=boolean_operator), node=ASTNode(type=comparison_operator), position=0)",
            "Move(target_node=IN(type=boolean_operator), node=ASTNode(type=or, text=or), position=1)",
            "Move(target_node=IN(type=boolean_operator), node=ASTNode(type=comparison_operator), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 4185,
        "neg_line": [
            "-if args.do_train and args.local_rank == -1 or torch.distributed.get_rank() == 0:"
        ],
        "pos_line": [
            "+if args.do_train and (args.local_rank == -1 or torch.distributed.get_rank() == 0):"
        ],
        "core_change": "-if args.do_train and args.local_rank == -1 or torch.distributed.get_rank() == 0: +if args.do_train and (args.local_rank == -1 or torch.distributed.get_rank() == 0):",
        "core_API": "get_rank"
    },
    {
        "commit_hash": "ee45c6c2da78ae944d6bb0043d6414122590299e",
        "index": "5b52c061..faac2c85 100644",
        "commit_message": "[Fix] MotionBlur bug fix and doctest update (#782)\n\n* Fixed #779\n\n* Added tests for _extract_device_dtype\n\n* Fixed broken tests\n\n* Added tests against functional\n\n* Fixed doctests\n\n* bug fix\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def homography_i_H_ref(pinhole_i, pinhole_ref):",
            "- Output: :math:`(N, 4, 4)`",
            "",
            "Example:",
            "-        >>> pinhole_i = torch.rand(1, 12)    # Nx12",
            "-        >>> pinhole_ref = torch.rand(1, 12)  # Nx12",
            "-        >>> homography_i_H_ref(pinhole_i, pinhole_ref)  # Nx4x4",
            "+        pinhole_i = torch.rand(1, 12)    # Nx12",
            "+        pinhole_ref = torch.rand(1, 12)  # Nx12",
            "+        homography_i_H_ref(pinhole_i, pinhole_ref)  # Nx4x4",
            "\"\"\"",
            "+    # TODO: Add doctest once having `rtvec_to_pose`.",
            "assert len(",
            "pinhole_i.shape) == 2 and pinhole_i.shape[1] == 12, pinhole.shape",
            "assert pinhole_i.shape == pinhole_ref.shape, pinhole_ref.shape"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=function_definition), node=('block', ''), position=4, insert_id=433288)",
            "Delete(target_node=ASTNode(type=identifier, text=Output))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=identifier, text=math))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=string, text=`(N, 4, 4)`))",
            "Delete(target_node=ASTNode(type=type))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=block))",
            "Delete(target_node=ASTNode(type=>>, text=>>))",
            "Delete(target_node=ASTNode(type=>, text=>))",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=>>, text=>>))",
            "Delete(target_node=ASTNode(type=>, text=>))",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=>>, text=>>))",
            "Delete(target_node=ASTNode(type=>, text=>))",
            "Delete(target_node=ASTNode(type=ERROR))"
        ],
        "plus_line": 4,
        "minus_line": 4,
        "AST_diff_line": 21,
        "number": 4186,
        "neg_line": [
            "-Output: :math:`(N, 4, 4)`",
            "->>> pinhole_i = torch.rand(1, 12)    # Nx12",
            "->>> pinhole_ref = torch.rand(1, 12)  # Nx12",
            "->>> homography_i_H_ref(pinhole_i, pinhole_ref)  # Nx4x4"
        ],
        "pos_line": [
            "+pinhole_i = torch.rand(1, 12)    # Nx12",
            "+pinhole_ref = torch.rand(1, 12)  # Nx12",
            "+homography_i_H_ref(pinhole_i, pinhole_ref)  # Nx4x4",
            "+# TODO: Add doctest once having `rtvec_to_pose`."
        ],
        "core_change": "-Output: :math:`(N, 4, 4)` ->>> pinhole_i = torch.rand(1, 12)    # Nx12 ->>> pinhole_ref = torch.rand(1, 12)  # Nx12 ->>> homography_i_H_ref(pinhole_i, pinhole_ref)  # Nx4x4 +pinhole_i = torch.rand(1, 12)    # Nx12 +pinhole_ref = torch.rand(1, 12)  # Nx12 +homography_i_H_ref(pinhole_i, pinhole_ref)  # Nx4x4 +# TODO: Add doctest once having `rtvec_to_pose`.",
        "core_API": "rand"
    },
    {
        "commit_hash": "89c2091bf4b4c676bf4ff36e508db08be6eabe54",
        "index": "f327c11..350645e 100644",
        "commit_message": "Improve snt.Embed performance in distributed training.\n\nAdds a fix to avoid excess computation on parameter servers.\n\nPiperOrigin-RevId: 179912285\n\n",
        "file": "sonnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Embed(base.AbstractModule):",
            "regularizer=self._regularizers.get(self.EMBEDDINGS, None),",
            "trainable=self._trainable)",
            "",
            "+    # On the backwards pass, we want to convert the gradient from",
            "+    # indexed-slices to a regular tensor before sending it back to the",
            "+    # parameter server. This avoids excess computation on the parameter server.",
            "+",
            "+    embeddings = util.convert_gradient_to_tensor(self._embeddings)",
            "+",
            "# Lookup embeddings",
            "-    return tf.nn.embedding_lookup(",
            "-        self._embeddings, ids, name=\"embedding_lookup\")",
            "+    return tf.nn.embedding_lookup(embeddings, ids, name=\"embedding_lookup\")",
            "",
            "@property",
            "def vocab_size(self):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=2, insert_id=2195719)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=2195720)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=identifier, text=trainable), position=0)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type==, text==), position=1)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=ERROR), position=2)",
            "Insert(target_node=IN(type=assignment), node=('assignment', None), position=3, insert_id=2195721)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'embeddings'), position=0, insert_id=2195722)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=2195723)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=2195724)",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'embeddings'), position=1, insert_id=2195725)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2195726)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2195727)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'util'), position=0, insert_id=2195728)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2195729)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'convert_gradient_to_tensor'), position=2, insert_id=2195730)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2195731)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=attribute), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2195732)"
        ],
        "plus_line": 5,
        "minus_line": 2,
        "AST_diff_line": 18,
        "number": 4189,
        "neg_line": [
            "-return tf.nn.embedding_lookup(",
            "-self._embeddings, ids, name=\"embedding_lookup\")"
        ],
        "pos_line": [
            "+# On the backwards pass, we want to convert the gradient from",
            "+# indexed-slices to a regular tensor before sending it back to the",
            "+# parameter server. This avoids excess computation on the parameter server.",
            "+",
            "+embeddings = util.convert_gradient_to_tensor(self._embeddings)",
            "+",
            "+return tf.nn.embedding_lookup(embeddings, ids, name=\"embedding_lookup\")"
        ],
        "core_change": "+# On the backwards pass, we want to convert the gradient from +# indexed-slices to a regular tensor before sending it back to the +# parameter server. This avoids excess computation on the parameter server. + +embeddings = util.convert_gradient_to_tensor(self._embeddings) + -return tf.nn.embedding_lookup( -self._embeddings, ids, name=\"embedding_lookup\") +return tf.nn.embedding_lookup(embeddings, ids, name=\"embedding_lookup\")",
        "core_API": "get"
    },
    {
        "commit_hash": "d875ba0dfd4a7f59060a5eadd95072fdb37d41b5",
        "index": "6ef6cfa43..85ae71c24 100644",
        "commit_message": "wandb.init(): When the user passes an \"id\" attribute but no \"name\", use the id value for name, but make WANDB_NAME take precedence.\n\nFix a TensorFlow test that was depending on run history being a shared global.\n\n",
        "file": "wandb.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tensorflow",
        "change": [
            "def test_hook(history):",
            "tf.summary.scalar('c1', c1)",
            "summary_op = tf.summary.merge_all()",
            "",
            "-        hook = wandb_tensorflow.WandbHook(summary_op)",
            "+        hook = wandb_tensorflow.WandbHook(summary_op, history=history)",
            "with tf.train.MonitoredTrainingSession(hooks=[hook]) as sess:",
            "summary, acc = sess.run([summary_op, c1])",
            "",
            "assert wandb_tensorflow.tf_summary_to_dict(summary) == {'c1': 42.0}",
            "+    print(history.rows)",
            "+    # TODO(adrian): there is still some kind of bug here where the history",
            "+    # is being shared with another test that manages to add rows before this one.",
            "assert history.rows[0]['c1'] == 42.0"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=7, insert_id=2461688)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=2461689)",
            "Insert(target_node=IN(type=call), node=('identifier', 'print'), position=0, insert_id=2461690)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2461691)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2461692)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=2461693)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2461694)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=2461695)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=2461696)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'history'), position=0, insert_id=2461697)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2461698)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'rows'), position=2, insert_id=2461699)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'history'), position=0, insert_id=2461700)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2461701)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'history'), position=2, insert_id=2461702)"
        ],
        "plus_line": 4,
        "minus_line": 1,
        "AST_diff_line": 15,
        "number": 4192,
        "neg_line": [
            "-hook = wandb_tensorflow.WandbHook(summary_op)"
        ],
        "pos_line": [
            "+hook = wandb_tensorflow.WandbHook(summary_op, history=history)",
            "+print(history.rows)",
            "+# TODO(adrian): there is still some kind of bug here where the history",
            "+# is being shared with another test that manages to add rows before this one."
        ],
        "core_change": "-hook = wandb_tensorflow.WandbHook(summary_op) +hook = wandb_tensorflow.WandbHook(summary_op, history=history) +print(history.rows) +# TODO(adrian): there is still some kind of bug here where the history +# is being shared with another test that manages to add rows before this one.",
        "core_API": "scalar"
    },
    {
        "commit_hash": "4dd784c32f76fb8285f205b94e2a6ebde731a1cd",
        "index": "bed053ae4..4cf0d21ec 100644",
        "commit_message": "Fix momentum and epsilon values (#19454)\n\nThe momentum value for PyTorch and TensorFlow batch normalization layers is not equivalent. The TensorFlow value should be (1 - pytorch_momentum) in order to ensure the correct updates are applied to the running mean and running variance calculations. We wouldn't observe a difference loading a pretrained model and performing inference, but evaluation outputs would change after some training steps.\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "layers",
        "change": [
            "class TFResNetShortCut(tf.keras.layers.Layer):",
            "out_channels, kernel_size=1, strides=stride, use_bias=False, name=\"convolution\"",
            ")",
            "# Use same default momentum and epsilon as PyTorch equivalent",
            "-        self.normalization = tf.keras.layers.BatchNormalization(epsilon=1e-5, momentum=0.1, name=\"normalization\")",
            "+        self.normalization = tf.keras.layers.BatchNormalization(epsilon=1e-5, momentum=0.9, name=\"normalization\")",
            "",
            "def call(self, x: tf.Tensor, training: bool = False) -> tf.Tensor:",
            "hidden_state = x"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=float, text=0.1), value='0.9')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 4196,
        "neg_line": [
            "-self.normalization = tf.keras.layers.BatchNormalization(epsilon=1e-5, momentum=0.1, name=\"normalization\")"
        ],
        "pos_line": [
            "+self.normalization = tf.keras.layers.BatchNormalization(epsilon=1e-5, momentum=0.9, name=\"normalization\")"
        ],
        "core_change": "-self.normalization = tf.keras.layers.BatchNormalization(epsilon=1e-5, momentum=0.1, name=\"normalization\") +self.normalization = tf.keras.layers.BatchNormalization(epsilon=1e-5, momentum=0.9, name=\"normalization\")",
        "core_API": "BatchNormalization"
    },
    {
        "commit_hash": "e6a107c14eec6dde40bc3c73c4e2b54dae3996df",
        "index": "7a9f645a7..314eb1260 100644",
        "commit_message": "Generalize TorchHook to FrameworkHook (#2561)\n\n* create FrameworkHook and move generic hooking functionality over\n\n* PEP-ify import statements all around\n\n* PR comment - fix docstring\n\n",
        "file": "PySyft.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "if dependency_check.torch_available:",
            "import torch",
            "",
            "framework_tensors.append(torch.Tensor)",
            "+    framework_tensors.append(torch.nn.Parameter)",
            "framework_shapes.append(torch.Size)",
            "",
            "framework_tensors = tuple(framework_tensors)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=1457920)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=1457921)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1457922)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1457923)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'framework_tensors'), position=0, insert_id=1457924)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1457925)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'append'), position=2, insert_id=1457926)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1457927)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=1457928)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1457929)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1457930)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1457931)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Parameter'), position=2, insert_id=1457932)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1457933)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1457934)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'nn'), position=2, insert_id=1457935)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 16,
        "number": 4197,
        "neg_line": [],
        "pos_line": [
            "+framework_tensors.append(torch.nn.Parameter)"
        ],
        "core_change": "+framework_tensors.append(torch.nn.Parameter)",
        "core_API": "append"
    },
    {
        "commit_hash": "1ed2ebf60d87ef12bd063c7c58e484e19189c754",
        "index": "a7faef942..86a8d0158 100644",
        "commit_message": "[style] consistent nn. and nn.functional (#12124)\n\n* consistent nn. and nn.functional\n\n* fix glitch\n\n* fix glitch #2\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class OpusState:",
            "load_layers_(model.model.decoder.layers, state_dict, BART_CONVERTER, is_decoder=True)",
            "",
            "# handle tensors not associated with layers",
            "-        wemb_tensor = torch.nn.Parameter(torch.FloatTensor(self.wemb))",
            "-        bias_tensor = torch.nn.Parameter(torch.FloatTensor(self.final_bias))",
            "+        wemb_tensor = nn.Parameter(torch.FloatTensor(self.wemb))",
            "+        bias_tensor = nn.Parameter(torch.FloatTensor(self.final_bias))",
            "model.model.shared.weight = wemb_tensor",
            "model.model.encoder.embed_tokens = model.model.decoder.embed_tokens = model.model.shared"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=nn), position=0)",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=nn), position=0)",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 8,
        "number": 4205,
        "neg_line": [
            "-wemb_tensor = torch.nn.Parameter(torch.FloatTensor(self.wemb))",
            "-bias_tensor = torch.nn.Parameter(torch.FloatTensor(self.final_bias))"
        ],
        "pos_line": [
            "+wemb_tensor = nn.Parameter(torch.FloatTensor(self.wemb))",
            "+bias_tensor = nn.Parameter(torch.FloatTensor(self.final_bias))"
        ],
        "core_change": "-wemb_tensor = torch.nn.Parameter(torch.FloatTensor(self.wemb)) -bias_tensor = torch.nn.Parameter(torch.FloatTensor(self.final_bias)) +wemb_tensor = nn.Parameter(torch.FloatTensor(self.wemb)) +bias_tensor = nn.Parameter(torch.FloatTensor(self.final_bias))",
        "core_API": "Parameter"
    },
    {
        "commit_hash": "01e5b1e06c683dd0eb2dc8c0d90c9749b90b7324",
        "index": "c9a1234a10..b3e180522a 100644",
        "commit_message": "fixed lint errors.\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def trace(",
            "return ret",
            "",
            "",
            "-def det(",
            "-        x: torch.Tensor,",
            "-        out: Optional[torch.Tensor] = None",
            "-) -> torch.Tensor:",
            "+def det(x: torch.Tensor, out: Optional[torch.Tensor] = None) -> torch.Tensor:",
            "return torch.linalg.det(x, out=out)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 0,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 4208,
        "neg_line": [
            "-def det(",
            "-x: torch.Tensor,",
            "-out: Optional[torch.Tensor] = None",
            "-) -> torch.Tensor:"
        ],
        "pos_line": [
            "+def det(x: torch.Tensor, out: Optional[torch.Tensor] = None) -> torch.Tensor:"
        ],
        "core_change": "-def det( -x: torch.Tensor, -out: Optional[torch.Tensor] = None -) -> torch.Tensor: +def det(x: torch.Tensor, out: Optional[torch.Tensor] = None) -> torch.Tensor:",
        "core_API": "det"
    },
    {
        "commit_hash": "7e34478a90513a680611a89025b4b592e6289dcc",
        "index": "f5be228..dbfbc22 100644",
        "commit_message": "fix a bug for save weights when multi-GPU training\n\n",
        "file": "deep-learning-for-image-processing.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def main(args):",
            "tb_writer.add_scalar(tags[1], acc, epoch)",
            "tb_writer.add_scalar(tags[2], optimizer.param_groups[0][\"lr\"], epoch)",
            "",
            "-            torch.save(model.state_dict(), \"./weights/model-{}.pth\".format(epoch))",
            "+            torch.save(model.module.state_dict(), \"./weights/model-{}.pth\".format(epoch))",
            "",
            "# ",
            "if rank == 0:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=71425)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=71426)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=model), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'module'), position=2, insert_id=71427)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 4210,
        "neg_line": [
            "-torch.save(model.state_dict(), \"./weights/model-{}.pth\".format(epoch))"
        ],
        "pos_line": [
            "+torch.save(model.module.state_dict(), \"./weights/model-{}.pth\".format(epoch))"
        ],
        "core_change": "-torch.save(model.state_dict(), \"./weights/model-{}.pth\".format(epoch)) +torch.save(model.module.state_dict(), \"./weights/model-{}.pth\".format(epoch))",
        "core_API": "add_scalar"
    },
    {
        "commit_hash": "43043ee4d5c672f7fbc22ac9559e8164731fd053",
        "index": "26abd766a4..75b17215e0 100644",
        "commit_message": "[RLlib] Tf2x preparation; part 2 (upgrading `try_import_tf()`). (#9136)\n\n* WIP.\n\n* Fixes.\n\n* LINT.\n\n* WIP.\n\n* WIP.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* WIP.\n\n* Fixes.\n\n* Test\n\n* Fix.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* LINT.\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class EpsilonGreedy(Exploration):",
            "),",
            "false_fn=lambda: exploit_action)",
            "",
            "-        assign_op = tf.assign(self.last_timestep, timestep)",
            "-        with tf.control_dependencies([assign_op]):",
            "+        assign_op = tf1.assign(self.last_timestep, timestep)",
            "+        with tf1.control_dependencies([assign_op]):",
            "return action, tf.zeros_like(action, dtype=tf.float32)",
            "",
            "def _get_torch_exploration_action(self, q_values, explore, timestep):"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tf), value='tf1')",
            "Update(target_node=ASTNode(type=identifier, text=tf), value='tf1')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 4212,
        "neg_line": [
            "-assign_op = tf.assign(self.last_timestep, timestep)",
            "-with tf.control_dependencies([assign_op]):"
        ],
        "pos_line": [
            "+assign_op = tf1.assign(self.last_timestep, timestep)",
            "+with tf1.control_dependencies([assign_op]):"
        ],
        "core_change": "-assign_op = tf.assign(self.last_timestep, timestep) -with tf.control_dependencies([assign_op]): +assign_op = tf1.assign(self.last_timestep, timestep) +with tf1.control_dependencies([assign_op]):",
        "core_API": "assign"
    },
    {
        "commit_hash": "335035c91393cbc7ae4488953df2d17b0e6f2735",
        "index": "c23682c2d..c4428041d 100644",
        "commit_message": "[MOD] fix typo, add tests for ldconv\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class LightweightConvolution2D(nn.Module):",
            "# convolution along frequency axis",
            "weight_f = F.softmax(self.weight_f, dim=-1)",
            "weight_f = F.dropout(weight_f, self.dropout_rate, training=self.training)",
            "-        weight_new = torch.zeros(B * T, 1, self.kernel_size, device=x.device).copy_(weight_f)",
            "+        weight_new = torch.zeros(B * T, 1, self.kernel_size, device=x.device, dtype=x.dtype).copy_(weight_f)",
            "xf = F.conv1d(x.view(1, B * T, C), weight_new, padding=self.padding_size, groups=B * T).view(B, T, C)",
            "",
            "# lightconv"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=8, insert_id=158006)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=9, insert_id=158007)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=158008)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=158009)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=158010)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'x'), position=0, insert_id=158011)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=158012)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dtype'), position=2, insert_id=158013)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 4214,
        "neg_line": [
            "-weight_new = torch.zeros(B * T, 1, self.kernel_size, device=x.device).copy_(weight_f)"
        ],
        "pos_line": [
            "+weight_new = torch.zeros(B * T, 1, self.kernel_size, device=x.device, dtype=x.dtype).copy_(weight_f)"
        ],
        "core_change": "-weight_new = torch.zeros(B * T, 1, self.kernel_size, device=x.device).copy_(weight_f) +weight_new = torch.zeros(B * T, 1, self.kernel_size, device=x.device, dtype=x.dtype).copy_(weight_f)",
        "core_API": "softmax"
    },
    {
        "commit_hash": "bb8bef02c002eccbb6369292ac54490875bebbc4",
        "index": "7169a3e3..e5893422 100644",
        "commit_message": "Fixed automl APIs to work with remote filesystems (#2650)\n\n\n",
        "file": "ludwig.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ECD(BaseModel):",
            "\"\"\"Loads the model from the given path.\"\"\"",
            "weights_save_path = os.path.join(save_path, MODEL_WEIGHTS_FILE_NAME)",
            "device = torch.device(get_torch_device())",
            "-        self.load_state_dict(torch.load(weights_save_path, map_location=device))",
            "+        with open_file(weights_save_path, \"rb\") as f:",
            "+            self.load_state_dict(torch.load(f, map_location=device))",
            "",
            "def get_args(self):",
            "\"\"\"Returns init arguments for constructing this model.\"\"\""
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('with_statement', None), position=4, insert_id=597954)",
            "Insert(target_node=IN(type=with_statement), node=('with', 'with'), position=0, insert_id=597955)",
            "Insert(target_node=IN(type=with_statement), node=('with_clause', None), position=1, insert_id=597956)",
            "Insert(target_node=IN(type=with_statement), node=(':', ':'), position=2, insert_id=597957)",
            "Insert(target_node=IN(type=with_statement), node=('block', None), position=3, insert_id=597958)",
            "Insert(target_node=IN(type=with_clause), node=('with_item', None), position=0, insert_id=597959)",
            "Move(target_node=IN(type=block), node=ASTNode(type=expression_statement), position=0)",
            "Insert(target_node=IN(type=with_item), node=('as_pattern', None), position=0, insert_id=597960)",
            "Insert(target_node=IN(type=as_pattern), node=('call', None), position=0, insert_id=597961)",
            "Insert(target_node=IN(type=as_pattern), node=('as', 'as'), position=1, insert_id=597962)",
            "Insert(target_node=IN(type=as_pattern), node=('as_pattern_target', None), position=2, insert_id=597963)",
            "Insert(target_node=IN(type=call), node=('identifier', 'open_file'), position=0, insert_id=597964)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=597965)",
            "Insert(target_node=IN(type=as_pattern_target), node=('identifier', 'f'), position=0, insert_id=597966)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=597967)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'weights_save_path'), position=1, insert_id=597968)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=597969)",
            "Insert(target_node=IN(type=argument_list), node=('string', '\"rb\"'), position=3, insert_id=597970)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=597971)",
            "Update(target_node=ASTNode(type=identifier, text=weights_save_path), value='f')"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 20,
        "number": 4216,
        "neg_line": [
            "-self.load_state_dict(torch.load(weights_save_path, map_location=device))"
        ],
        "pos_line": [
            "+with open_file(weights_save_path, \"rb\") as f:",
            "+self.load_state_dict(torch.load(f, map_location=device))"
        ],
        "core_change": "-self.load_state_dict(torch.load(weights_save_path, map_location=device)) +with open_file(weights_save_path, \"rb\") as f: +self.load_state_dict(torch.load(f, map_location=device))",
        "core_API": "join"
    },
    {
        "commit_hash": "3b91f96fc94e9e201a2c637b2f654cbdc6a21ee1",
        "index": "9da9f45d4..642b070ab 100755",
        "commit_message": "Fix torch meshgrid warnings (#20475)\n\n* fix torch meshgrid warnings\n\n* support lower torch versions\n\n* don't edit examples\n\n* dont edit examples\n\n* fix ci\n\n* fix style\n\n* rebase cleanup\n\n* fix ci again\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ViltEmbeddings(nn.Module):",
            "x = x.flatten(2).transpose(1, 2)",
            "# Set `device` here, otherwise `patch_index` will always be on `CPU` and will fail near the end for torch>=1.13",
            "patch_index = torch.stack(",
            "-            torch.meshgrid(torch.arange(x_mask.shape[-2]), torch.arange(x_mask.shape[-1]), indexing=\"ij\"), dim=-1",
            "+            meshgrid(torch.arange(x_mask.shape[-2]), torch.arange(x_mask.shape[-1]), indexing=\"ij\"), dim=-1",
            ").to(device=x_mask.device)",
            "patch_index = patch_index[None, None, :, :, :]",
            "patch_index = patch_index.expand(x_mask.shape[0], x_mask.shape[1], -1, -1, -1)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=meshgrid), position=0)",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 4221,
        "neg_line": [
            "-torch.meshgrid(torch.arange(x_mask.shape[-2]), torch.arange(x_mask.shape[-1]), indexing=\"ij\"), dim=-1"
        ],
        "pos_line": [
            "+meshgrid(torch.arange(x_mask.shape[-2]), torch.arange(x_mask.shape[-1]), indexing=\"ij\"), dim=-1"
        ],
        "core_change": "-torch.meshgrid(torch.arange(x_mask.shape[-2]), torch.arange(x_mask.shape[-1]), indexing=\"ij\"), dim=-1 +meshgrid(torch.arange(x_mask.shape[-2]), torch.arange(x_mask.shape[-1]), indexing=\"ij\"), dim=-1",
        "core_API": "flatten"
    },
    {
        "commit_hash": "1ed2ebf60d87ef12bd063c7c58e484e19189c754",
        "index": "994e9ff9c..35c3600ea 100755",
        "commit_message": "[style] consistent nn. and nn.functional (#12124)\n\n* consistent nn. and nn.functional\n\n* fix glitch\n\n* fix glitch #2\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class VisualBertForQuestionAnswering(VisualBertPreTrainedModel):",
            "",
            "loss = None",
            "if labels is not None:",
            "-            loss_fct = torch.nn.KLDivLoss(reduction=\"batchmean\")",
            "-            log_softmax = torch.nn.LogSoftmax(dim=-1)",
            "+            loss_fct = nn.KLDivLoss(reduction=\"batchmean\")",
            "+            log_softmax = nn.LogSoftmax(dim=-1)",
            "reshaped_logits = log_softmax(reshaped_logits)",
            "loss = loss_fct(reshaped_logits, labels.contiguous())",
            "if not return_dict:"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=nn), position=0)",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=nn), position=0)",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 8,
        "number": 4223,
        "neg_line": [
            "-loss_fct = torch.nn.KLDivLoss(reduction=\"batchmean\")",
            "-log_softmax = torch.nn.LogSoftmax(dim=-1)"
        ],
        "pos_line": [
            "+loss_fct = nn.KLDivLoss(reduction=\"batchmean\")",
            "+log_softmax = nn.LogSoftmax(dim=-1)"
        ],
        "core_change": "-loss_fct = torch.nn.KLDivLoss(reduction=\"batchmean\") -log_softmax = torch.nn.LogSoftmax(dim=-1) +loss_fct = nn.KLDivLoss(reduction=\"batchmean\") +log_softmax = nn.LogSoftmax(dim=-1)",
        "core_API": "KLDivLoss"
    },
    {
        "commit_hash": "6828b3331682f6addeb1c664c4f044f56421254a",
        "index": "c6adb35..87e6e93 100644",
        "commit_message": "Fix tf-nightly-gpu break (#1124)\n\nSigned-off-by: Alex Sergeev <alsrgv@users.noreply.github.com>\n",
        "file": "horovod.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "from common import mpi_env_rank_and_size",
            "config = tf.ConfigProto()",
            "config.gpu_options.allow_growth = True",
            "",
            "+if _has_eager:",
            "+    # Specifies the config to use with eager execution. Does not preclude",
            "+    # tests from running in the graph mode.",
            "+    tf.enable_eager_execution(config=config)",
            "+",
            "# MLSL supports only byte, float and double data types",
            "mlsl_supported_types = set([tf.float32, tf.float64])"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=3, insert_id=1946872)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=1946873)",
            "Insert(target_node=IN(type=if_statement), node=('identifier', '_has_eager'), position=1, insert_id=1946874)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=1946875)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=1946876)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=1946877)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=1946878)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1946879)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1946880)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=1946881)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1946882)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'enable_eager_execution'), position=2, insert_id=1946883)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1946884)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=1, insert_id=1946885)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1946886)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'config'), position=0, insert_id=1946887)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1946888)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'config'), position=2, insert_id=1946889)"
        ],
        "plus_line": 3,
        "minus_line": 0,
        "AST_diff_line": 18,
        "number": 4225,
        "neg_line": [],
        "pos_line": [
            "+if _has_eager:",
            "+# Specifies the config to use with eager execution. Does not preclude",
            "+# tests from running in the graph mode.",
            "+tf.enable_eager_execution(config=config)",
            "+"
        ],
        "core_change": "+if _has_eager: +# Specifies the config to use with eager execution. Does not preclude +# tests from running in the graph mode. +tf.enable_eager_execution(config=config) +",
        "core_API": "ConfigProto"
    },
    {
        "commit_hash": "5e61368b48ad159cc2e7ff29e1cd72fa5f3104f0",
        "index": "c95dcef18c..cf74bb3c9d 100644",
        "commit_message": "small fix to test_torch_tan, with num_positional_args now determined correctly.\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "functional",
        "change": [
            "import ivy.functional.backends.torch as ivy_torch",
            "set(ivy_torch.valid_float_dtypes)))),",
            "as_variable=st.booleans(),",
            "with_out=st.booleans(),",
            "-    num_positional_args=helpers.num_positional_args(fn_name=\"tan\"),",
            "+    num_positional_args=helpers.num_positional_args(",
            "+        fn_name=\"functional.frontends.torch.tan\"),",
            "native_array=st.booleans(),",
            ")",
            "def test_torch_tan("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"tan\"), value='\"functional.frontends.torch.tan\"')"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 4227,
        "neg_line": [
            "-num_positional_args=helpers.num_positional_args(fn_name=\"tan\"),"
        ],
        "pos_line": [
            "+num_positional_args=helpers.num_positional_args(",
            "+fn_name=\"functional.frontends.torch.tan\"),"
        ],
        "core_change": "-num_positional_args=helpers.num_positional_args(fn_name=\"tan\"), +num_positional_args=helpers.num_positional_args( +fn_name=\"functional.frontends.torch.tan\"),",
        "core_API": "booleans"
    },
    {
        "commit_hash": "390e121fb5233aa553d7a456c40ad5c95a72b7f1",
        "index": "cd3635887..a68ee3b83 100644",
        "commit_message": "[Examples] TPU-based training of a language model using TensorFlow (#21657)\n\n* add: tokenizer training script for TF TPU LM training.\n\n* add: script for preparing the TFRecord shards.\n\n* add: sequence of execution to readme.\n\n* remove limit from the tfrecord shard name.\n\n* Add initial train_model.py\n\n* Add basic training arguments and model init\n\n* Get up to the point of writing the data collator\n\n* Pushing progress so far!\n\n* Complete first draft of model training code\n\n* feat: grouping of texts efficiently.\n\nCo-authored-by: Matt <rocketknight1@gmail.com>\n\n* Add proper masking collator and get training loop working\n\n* fix: things.\n\n* Read sample counts from filenames\n\n* Read sample counts from filenames\n\n* Draft README\n\n* Improve TPU warning\n\n* Use distribute instead of distribute.experimental\n\n* Apply suggestions from code review\n\nCo-authored-by: Matt <Rocketknight1@users.noreply.github.com>\n\n* Modularize loading and add MLM probability as arg\n\n* minor refactoring to better use the cli args.\n\n* readme fillup.\n\n* include tpu and inference sections in the readme.\n\n* table of contents.\n\n* parallelize maps.\n\n* polish readme.\n\n* change script name to run_mlm.py\n\n* address PR feedback (round I).\n\n---------\n\nCo-authored-by: Matt <rocketknight1@gmail.com>\nCo-authored-by: Matt <Rocketknight1@users.noreply.github.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class DataCollatorForLanguageModeling(DataCollatorMixin):",
            "inputs = tf.where(indices_replaced, mask_token_id, inputs)",
            "",
            "# 10% of the time, we replace masked input tokens with random word",
            "-        indices_random = self.tf_bernoulli(input_shape, 0.5) & masked_indices & ~indices_replaced",
            "-        random_words = tf.random.uniform(input_shape, maxval=vocab_size, dtype=tf.int64)",
            "+        indices_random = self.tf_bernoulli(input_shape, 0.1) & masked_indices & ~indices_replaced",
            "+        random_words = tf.random.uniform(input_shape, maxval=vocab_size, dtype=inputs.dtype)",
            "+",
            "inputs = tf.where(indices_random, random_words, inputs)",
            "",
            "# The rest of the time (10% of the time) we keep the masked input tokens unchanged"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=float, text=0.5), value='0.1')",
            "Update(target_node=ASTNode(type=identifier, text=tf), value='inputs')",
            "Update(target_node=ASTNode(type=identifier, text=int64), value='dtype')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 3,
        "number": 4229,
        "neg_line": [
            "-indices_random = self.tf_bernoulli(input_shape, 0.5) & masked_indices & ~indices_replaced",
            "-random_words = tf.random.uniform(input_shape, maxval=vocab_size, dtype=tf.int64)"
        ],
        "pos_line": [
            "+indices_random = self.tf_bernoulli(input_shape, 0.1) & masked_indices & ~indices_replaced",
            "+random_words = tf.random.uniform(input_shape, maxval=vocab_size, dtype=inputs.dtype)",
            "+"
        ],
        "core_change": "-indices_random = self.tf_bernoulli(input_shape, 0.5) & masked_indices & ~indices_replaced -random_words = tf.random.uniform(input_shape, maxval=vocab_size, dtype=tf.int64) +indices_random = self.tf_bernoulli(input_shape, 0.1) & masked_indices & ~indices_replaced +random_words = tf.random.uniform(input_shape, maxval=vocab_size, dtype=inputs.dtype) +",
        "core_API": "where"
    },
    {
        "commit_hash": "a71ff4d7995718ac7b51c7efdbb53f7943e2295e",
        "index": "b55532e8..38b8e554 100644",
        "commit_message": "move summary_op from trainer to callbacks. fix #125\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Trainer(object):",
            "logger.info(\"Epoch {} (global_step {}) finished, time:{:.2f} sec.\".format(",
            "self.epoch_num, self.global_step, time.time() - start_time))",
            "",
            "-                    # trigger epoch outside the timing region.",
            "-                    self.trigger_epoch()",
            "+                    self.trigger_epoch()  # trigger epoch outside the timing region.",
            "except StopTraining:",
            "logger.info(\"Training was stopped.\")",
            "except KeyboardInterrupt:"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 4235,
        "neg_line": [
            "-# trigger epoch outside the timing region.",
            "-self.trigger_epoch()"
        ],
        "pos_line": [
            "+self.trigger_epoch()  # trigger epoch outside the timing region."
        ],
        "core_change": "-# trigger epoch outside the timing region. -self.trigger_epoch() +self.trigger_epoch()  # trigger epoch outside the timing region.",
        "core_API": "info"
    },
    {
        "commit_hash": "afe5d42d8d1d80af911ed980c2936bfe887078f6",
        "index": "8a48b616a..40533ede4 100644",
        "commit_message": "Black preview (#17217)\n\n* Black preview\n\n* Fixup too!\n\n* Fix check copies\n\n* Use the same version as the CI\n\n* Bump black\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "layers",
        "change": [
            "def load_tf2_weights_in_bert(model, tf_checkpoint_path, config):",
            "pointer.data = torch.from_numpy(array)",
            "else:",
            "raise ValueError(",
            "-                f\"Shape mismatch in layer {full_name}: Model expects shape {pointer.shape} but layer contains shape: {array.shape}\"",
            "+                f\"Shape mismatch in layer {full_name}: Model expects shape {pointer.shape} but layer contains shape:\"",
            "+                f\" {array.shape}\"",
            ")",
            "logger.info(f\"Successfully set variable {full_name} to PyTorch layer {trace}\")",
            "return model"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('concatenated_string', None), position=1, insert_id=2641302)",
            "Update(target_node=ASTNode(type=string, text=f\"Shape mismatch in layer {full_name}: Model expects shape {pointer.shape} but layer contains shape: {array.shape}\"), value='f\"Shape mismatch in layer {full_name}: Model expects shape {pointer.shape} but layer contains shape:\"')",
            "Move(target_node=IN(type=concatenated_string), node=ASTNode(type=string, text=f\"Shape mismatch in layer {full_name}: Model expects shape {pointer.shape} but layer contains shape: {array.shape}\"), position=0)",
            "Insert(target_node=IN(type=concatenated_string), node=('string', 'f\" {array.shape}\"'), position=1, insert_id=2641303)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 4236,
        "neg_line": [
            "-f\"Shape mismatch in layer {full_name}: Model expects shape {pointer.shape} but layer contains shape: {array.shape}\""
        ],
        "pos_line": [
            "+f\"Shape mismatch in layer {full_name}: Model expects shape {pointer.shape} but layer contains shape:\"",
            "+f\" {array.shape}\""
        ],
        "core_change": "-f\"Shape mismatch in layer {full_name}: Model expects shape {pointer.shape} but layer contains shape: {array.shape}\" +f\"Shape mismatch in layer {full_name}: Model expects shape {pointer.shape} but layer contains shape:\" +f\" {array.shape}\"",
        "core_API": "from_numpy"
    },
    {
        "commit_hash": "90e98e56c46bc466d4ad7712bab93566afe5d1d0",
        "index": "cae44cad..8e6c979c 100644",
        "commit_message": "Fix doc formatting and improve doc styling (#4072)\n\n* switch to pydoc-markdown with custom processor\n\n* add some extra css\n\n* fixes\n\n* fixes\n\n* minor tweaks\n\n* fixes\n\n* add breadcrumbs\n\n* fixes\n\n* fix arg formatting\n\n* fix\n\n* fixes\n\n* more fixes\n\n* fix\n\n* fix cross-refs within module\n\n* fix dev requirements\n\n* pin pydoc-markdown to latest commit\n\n* small refactor and docstring fixes\n\n* more small fixes\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class PretrainedTransformerMismatchedEmbedder(TokenEmbedder):",
            "segment_concat_mask: Optional[torch.BoolTensor]",
            "See `PretrainedTransformerEmbedder`.",
            "",
            "-        # Returns:",
            "+        # Returns",
            "",
            "-        Shape: [batch_size, num_orig_tokens, embedding_size].",
            "+        `torch.Tensor`",
            "+            Shape: [batch_size, num_orig_tokens, embedding_size].",
            "\"\"\"",
            "# Shape: [batch_size, num_wordpieces, embedding_size].",
            "embeddings = self._matched_embedder("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('ERROR', None), position=2, insert_id=18579)",
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=18580)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=identifier, text=See), position=0)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=string, text=`PretrainedTransformerEmbedder`), position=1)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=., text=.), position=2)",
            "Insert(target_node=IN(type=expression_statement), node=('string', '`torch.Tensor`'), position=0, insert_id=18581)",
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=identifier, text=Shape), position=0)",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 3,
        "minus_line": 2,
        "AST_diff_line": 8,
        "number": 4238,
        "neg_line": [
            "-# Returns:",
            "-Shape: [batch_size, num_orig_tokens, embedding_size]."
        ],
        "pos_line": [
            "+# Returns",
            "+`torch.Tensor`",
            "+Shape: [batch_size, num_orig_tokens, embedding_size]."
        ],
        "core_change": "-# Returns: +# Returns -Shape: [batch_size, num_orig_tokens, embedding_size]. +`torch.Tensor` +Shape: [batch_size, num_orig_tokens, embedding_size].",
        "core_API": "_matched_embedder"
    },
    {
        "commit_hash": "052cdf129dc7dd66d86b4523bc72cb9a48a39c29",
        "index": "05788554..086b0c79 100644",
        "commit_message": "fixed a typo\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def knn(x, y, k, batch_x=None, batch_y=None, cosine=False, num_workers=1):",
            "x = torch.Tensor([[-1, -1], [-1, 1], [1, -1], [1, 1]])",
            "batch_x = torch.tensor([0, 0, 0, 0])",
            "y = torch.Tensor([[-1, 0], [1, 0]])",
            "-        batch_x = torch.tensor([0, 0])",
            "+        batch_y = torch.tensor([0, 0])",
            "assign_index = knn(x, y, 2, batch_x, batch_y)",
            "\"\"\"",
            "if torch_cluster is None:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=batch_x), value='batch_y')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 4241,
        "neg_line": [
            "-batch_x = torch.tensor([0, 0])"
        ],
        "pos_line": [
            "+batch_y = torch.tensor([0, 0])"
        ],
        "core_change": "-batch_x = torch.tensor([0, 0]) +batch_y = torch.tensor([0, 0])",
        "core_API": "Tensor"
    },
    {
        "commit_hash": "acb3808dd81f7ed8aacc0df9f07ad9d748c307f1",
        "index": "bb6c04d22a..147464bb7a 100644",
        "commit_message": "lint fix\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def vecdot(",
            "*,",
            "out: Optional[torch.Tensor] = None,",
            ") -> torch.Tensor:",
            "-    dtype = torch.promote_types(x1.dtype, x2.dtype)",
            "x1, x2 = x1.to(torch.float32), x2.to(torch.float32)",
            "return torch.tensordot(x1, x2, dims=([axis], [axis]), out=out)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('block', ''), position=0, insert_id=335858)",
            "Delete(target_node=ASTNode(type=identifier, text=dtype))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=promote_types))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=x1))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=dtype))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=x2))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=dtype))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=block))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 23,
        "number": 4243,
        "neg_line": [
            "-dtype = torch.promote_types(x1.dtype, x2.dtype)"
        ],
        "pos_line": [],
        "core_change": "-dtype = torch.promote_types(x1.dtype, x2.dtype)",
        "core_API": "promote_types"
    },
    {
        "commit_hash": "cefc7f7c320ba41efa3be7eafa59c4ddb7ad8270",
        "index": "75846b7ec..e290d77b3 100644",
        "commit_message": "Feature/log computational graph (#3003)\n\n* add methods\n\n* log in trainer\n\n* add tests\n\n* changelog\n\n* fix tests\n\n* fix tests\n\n* fix tests\n\n* fix tests\n\n* fix tests\n\n* fix tests\n\n* fix tests\n\n* text\n\n* added argument\n\n* update tests\n\n* fix styling\n\n* improve testing\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_tbptt_cpu_model_result_auto_reduce(tmpdir):",
            ")",
            "",
            "model = BpttTestModel(**hparams)",
            "+    model.example_input_array = torch.randn(5, truncated_bptt_steps)",
            "",
            "# fit model",
            "trainer = Trainer("
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=571567)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=571568)",
            "Insert(target_node=IN(type=assignment), node=('attribute', None), position=0, insert_id=571569)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=571570)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=571571)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'model'), position=0, insert_id=571572)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=571573)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'example_input_array'), position=2, insert_id=571574)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=571575)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=571576)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=571577)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=571578)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'randn'), position=2, insert_id=571579)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=571580)",
            "Insert(target_node=IN(type=argument_list), node=('integer', '5'), position=1, insert_id=571581)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=571582)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'truncated_bptt_steps'), position=3, insert_id=571583)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=571584)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 18,
        "number": 4245,
        "neg_line": [],
        "pos_line": [
            "+model.example_input_array = torch.randn(5, truncated_bptt_steps)"
        ],
        "core_change": "+model.example_input_array = torch.randn(5, truncated_bptt_steps)",
        "core_API": "randn"
    },
    {
        "commit_hash": "1e84df4f495ddcf8d7ce18dc22d75eb65c4309af",
        "index": "cbe0010c95..47476aabbf 100644",
        "commit_message": "fix tf random multinomial test (#7098)\n\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "functional",
        "change": [
            "def test_multinomial(",
            "):",
            "prob_dtype, batch_size, population_size, num_samples, replace, probs = everything",
            "# tensorflow does not support multinomial without replacement",
            "-    if backend_fw == \"tensorflow\":",
            "-        assume(replace is True)",
            "+    if backend_fw == ivy.functional.backends.tensorflow:",
            "+        assume(replace)",
            "",
            "def call():",
            "return helpers.test_function("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=comparison_operator), node=('attribute', None), position=2, insert_id=1642931)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1642932)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1642933)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tensorflow'), position=2, insert_id=1642934)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1642935)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1642936)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'backends'), position=2, insert_id=1642937)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'ivy'), position=0, insert_id=1642938)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1642939)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'functional'), position=2, insert_id=1642940)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=replace), position=1)",
            "Delete(target_node=ASTNode(type=string, text=\"tensorflow\"))",
            "Delete(target_node=ASTNode(type=is, text=is))",
            "Delete(target_node=ASTNode(type=true, text=True))",
            "Delete(target_node=ASTNode(type=comparison_operator))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 15,
        "number": 4247,
        "neg_line": [
            "-if backend_fw == \"tensorflow\":",
            "-assume(replace is True)"
        ],
        "pos_line": [
            "+if backend_fw == ivy.functional.backends.tensorflow:",
            "+assume(replace)"
        ],
        "core_change": "-if backend_fw == \"tensorflow\": -assume(replace is True) +if backend_fw == ivy.functional.backends.tensorflow: +assume(replace)",
        "core_API": "test_function"
    },
    {
        "commit_hash": "6cf0abaed1c2911e5bf23e48e4908929b43d85ae",
        "index": "e9a11d0aa..8eef944fb 100644",
        "commit_message": "Align tqdm control/cache control with Transformers (#3897)\n\n* Align tqdm with transformers\n\n* Add tqdm utilites to docs\n\n* Introduce enable_caching / disable_caching\n\n* Fix test naming\n\n* Update src/datasets/fingerprint.py\n\nCo-authored-by: Quentin Lhoest <42851186+lhoestq@users.noreply.github.com>\n\n* Style and rename _active to _tqdm_active\n\nCo-authored-by: Quentin Lhoest <42851186+lhoestq@users.noreply.github.com>\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "def set_test_cache_config(tmp_path_factory, monkeypatch):",
            "",
            "@pytest.fixture(autouse=True, scope=\"session\")",
            "def disable_tqdm_output():",
            "-    datasets.set_progress_bar_enabled(False)",
            "+    datasets.disable_progress_bar()",
            "",
            "",
            "@pytest.fixture(autouse=True)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=set_progress_bar_enabled), value='disable_progress_bar')",
            "Delete(target_node=ASTNode(type=false, text=False))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 4248,
        "neg_line": [
            "-datasets.set_progress_bar_enabled(False)"
        ],
        "pos_line": [
            "+datasets.disable_progress_bar()"
        ],
        "core_change": "-datasets.set_progress_bar_enabled(False) +datasets.disable_progress_bar()",
        "core_API": "fixture"
    },
    {
        "commit_hash": "693f857dee7daa969a7896e156a8a77dd1f61a6f",
        "index": "239b9c81..ad01429c 100644",
        "commit_message": "Fix some of the tests so that they pass for Bazel.\n\nPiperOrigin-RevId: 268469295\n\n",
        "file": "tf-quant-finance.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class MonotoneConvexTest(tf.test.TestCase):",
            "test_time = tf.constant([1.1, 2.7], dtype=dtype)",
            "interpolated, _ = monotone_convex.interpolate(test_time, interval_values,",
            "interval_times)",
            "-    gradient_1y = self.evaluate(tf.gradients(interpolated[0], knot_1y)[0])",
            "-    gradient_zero = self.evaluate(tf.gradients(interpolated[1], knot_1y)[0])",
            "+    gradient_1y = self.evaluate(tf.convert_to_tensor(",
            "+        tf.gradients(interpolated[0], knot_1y)[0]))",
            "+    gradient_zero = self.evaluate(tf.convert_to_tensor(",
            "+        tf.gradients(interpolated[1], knot_1y)[0]))",
            "+",
            "self.assertAlmostEqual(gradient_1y[0], 0.42)",
            "self.assertAlmostEqual(gradient_zero[0], 0.0)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('call', None), position=2, insert_id=2348706)",
            "Insert(target_node=ASTNode(type=assignment), node=('call', None), position=2, insert_id=2348707)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2348708)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2348709)",
            "Move(target_node=IN(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2348710)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=2348711)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2348712)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'evaluate'), position=2, insert_id=2348713)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2348714)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=call), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2348715)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2348716)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=call), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2348717)",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2348718)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2348719)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2348720)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'convert_to_tensor'), position=2, insert_id=2348721)",
            "Update(target_node=ASTNode(type=identifier, text=self), value='tf')",
            "Update(target_node=ASTNode(type=identifier, text=evaluate), value='convert_to_tensor')"
        ],
        "plus_line": 4,
        "minus_line": 2,
        "AST_diff_line": 21,
        "number": 4249,
        "neg_line": [
            "-gradient_1y = self.evaluate(tf.gradients(interpolated[0], knot_1y)[0])",
            "-gradient_zero = self.evaluate(tf.gradients(interpolated[1], knot_1y)[0])"
        ],
        "pos_line": [
            "+gradient_1y = self.evaluate(tf.convert_to_tensor(",
            "+tf.gradients(interpolated[0], knot_1y)[0]))",
            "+gradient_zero = self.evaluate(tf.convert_to_tensor(",
            "+tf.gradients(interpolated[1], knot_1y)[0]))",
            "+"
        ],
        "core_change": "-gradient_1y = self.evaluate(tf.gradients(interpolated[0], knot_1y)[0]) -gradient_zero = self.evaluate(tf.gradients(interpolated[1], knot_1y)[0]) +gradient_1y = self.evaluate(tf.convert_to_tensor( +tf.gradients(interpolated[0], knot_1y)[0])) +gradient_zero = self.evaluate(tf.convert_to_tensor( +tf.gradients(interpolated[1], knot_1y)[0])) +",
        "core_API": "constant"
    },
    {
        "commit_hash": "fdc907675050cb9e3aa887bf89d6c92519178349",
        "index": "15b2905c..2044a6a8 100755",
        "commit_message": "fix use of deprecated TF functions.\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Model(ModelDesc):",
            "# weight decay on all W of fc layers",
            "wd_w = tf.train.exponential_decay(0.00004, get_global_step_var(),",
            "80000, 0.7, True)",
            "-        wd_cost = tf.mul(wd_w, regularize_cost('.*/W', tf.nn.l2_loss), name='l2_regularize_loss')",
            "+        wd_cost = tf.multiply(wd_w, regularize_cost('.*/W', tf.nn.l2_loss), name='l2_regularize_loss')",
            "",
            "self.cost = tf.add_n([0.4 * loss1, loss2, wd_cost], name='cost')",
            "add_moving_summary(loss1, loss2, wd_cost, self.cost)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=mul), value='multiply')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 4250,
        "neg_line": [
            "-wd_cost = tf.mul(wd_w, regularize_cost('.*/W', tf.nn.l2_loss), name='l2_regularize_loss')"
        ],
        "pos_line": [
            "+wd_cost = tf.multiply(wd_w, regularize_cost('.*/W', tf.nn.l2_loss), name='l2_regularize_loss')"
        ],
        "core_change": "-wd_cost = tf.mul(wd_w, regularize_cost('.*/W', tf.nn.l2_loss), name='l2_regularize_loss') +wd_cost = tf.multiply(wd_w, regularize_cost('.*/W', tf.nn.l2_loss), name='l2_regularize_loss')",
        "core_API": "exponential_decay"
    },
    {
        "commit_hash": "5979e5d0907ea1e8cc9a7b5f4c177d179829c132",
        "index": "8ebf595..928a9e9 100755",
        "commit_message": "Fix multi-class prediction\n\n",
        "file": "Pytorch-UNet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def mask_to_image(mask: np.ndarray, mask_values):",
            "if isinstance(mask_values[0], list):",
            "out = np.zeros((mask.shape[-2], mask.shape[-1], len(mask_values[0])), dtype=np.uint8)",
            "elif mask_values == [0, 1]:",
            "-        out = np.zeros((mask.shape[-2], mask.shape[-1]), dtype=np.bool)",
            "+        out = np.zeros((mask.shape[-2], mask.shape[-1]), dtype=bool)",
            "else:",
            "out = np.zeros((mask.shape[-2], mask.shape[-1]), dtype=np.uint8)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=keyword_argument), node=ASTNode(type=identifier, text=bool), position=2)",
            "Delete(target_node=ASTNode(type=identifier, text=np))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 4256,
        "neg_line": [
            "-out = np.zeros((mask.shape[-2], mask.shape[-1]), dtype=np.bool)"
        ],
        "pos_line": [
            "+out = np.zeros((mask.shape[-2], mask.shape[-1]), dtype=bool)"
        ],
        "core_change": "-out = np.zeros((mask.shape[-2], mask.shape[-1]), dtype=np.bool) +out = np.zeros((mask.shape[-2], mask.shape[-1]), dtype=bool)",
        "core_API": "zeros"
    },
    {
        "commit_hash": "b0e64d3417ec0d454708ffd77e1f7e2e1a82aa8f",
        "index": "a2d46e8df..2a9bc87f0 100644",
        "commit_message": "fix review comments\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def train(args):",
            "rnn = RNNLM(args.n_vocab, args.layer, args.unit, args.type, args.dropout_rate)",
            "model = ClassifierWithState(rnn)",
            "if args.ngpu > 0:",
            "-        model = torch.nn.DataParallel(model).cuda()",
            "+        model = torch.nn.DataParallel(model, device_ids=list(range(args.ngpu))).cuda()",
            "setattr(model, \"reporter\", model.module.reporter)",
            "gpu_id = 0",
            "else:"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=170603)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=170604)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=4, insert_id=170605)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device_ids'), position=0, insert_id=170606)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=170607)",
            "Insert(target_node=IN(type=keyword_argument), node=('call', None), position=2, insert_id=170608)",
            "Insert(target_node=IN(type=call), node=('identifier', 'list'), position=0, insert_id=170609)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=170610)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=170611)",
            "Insert(target_node=IN(type=argument_list), node=('call', None), position=1, insert_id=170612)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=170613)",
            "Insert(target_node=IN(type=call), node=('identifier', 'range'), position=0, insert_id=170614)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=170615)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=170616)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=170617)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=), text=)), position=2)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'args'), position=0, insert_id=170618)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=170619)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'ngpu'), position=2, insert_id=170620)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 19,
        "number": 4258,
        "neg_line": [
            "-model = torch.nn.DataParallel(model).cuda()"
        ],
        "pos_line": [
            "+model = torch.nn.DataParallel(model, device_ids=list(range(args.ngpu))).cuda()"
        ],
        "core_change": "-model = torch.nn.DataParallel(model).cuda() +model = torch.nn.DataParallel(model, device_ids=list(range(args.ngpu))).cuda()",
        "core_API": "DataParallel"
    },
    {
        "commit_hash": "954de8d0edab99b048647b1b2da7e751ccb93a44",
        "index": "058c9344..80b55c07 100644",
        "commit_message": "[PyTorch][Bug Fix] RNN Scratch train_ch8 step once in a batch (#1347)\n\n\n",
        "file": "d2l-en.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def train_ch8(model, train_iter, vocab, lr, num_epochs, device,",
            "animator = d2l.Animator(xlabel='epoch', ylabel='perplexity',",
            "legend=['train'], xlim=[1, num_epochs])",
            "if isinstance(model, nn.Module):",
            "-        trainer = torch.optim.SGD(model.parameters(), lr)",
            "-        updater = lambda batch_size: trainer.step()",
            "+        updater = torch.optim.SGD(model.parameters(), lr)",
            "else:",
            "updater = lambda batch_size: d2l.sgd(model.params, lr, batch_size)",
            "predict = lambda prefix: predict_ch8(prefix, 50, model, vocab, device)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=trainer), value='updater')",
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=else, text=else), position=5)",
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=:, text=:), position=6)",
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=identifier, text=updater), position=7)",
            "Delete(target_node=ASTNode(type=identifier, text=updater))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=lambda, text=lambda))",
            "Delete(target_node=ASTNode(type=identifier, text=batch_size))",
            "Delete(target_node=ASTNode(type=lambda_parameters))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=identifier, text=trainer))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=step))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=lambda))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 20,
        "number": 4267,
        "neg_line": [
            "-trainer = torch.optim.SGD(model.parameters(), lr)",
            "-updater = lambda batch_size: trainer.step()"
        ],
        "pos_line": [
            "+updater = torch.optim.SGD(model.parameters(), lr)"
        ],
        "core_change": "-trainer = torch.optim.SGD(model.parameters(), lr) -updater = lambda batch_size: trainer.step() +updater = torch.optim.SGD(model.parameters(), lr)",
        "core_API": "Animator"
    },
    {
        "commit_hash": "6b3af99b4c3f4e0bd508ed17f37f3001fd10733f",
        "index": "763672dd5..670405321 100644",
        "commit_message": "fix \"some elements of the input tensor and the written-to tensor refer to a single memory location\" error in ddp_pipeline tutorial (#2002)\n\nCo-authored-by: Svetlana Karslioglu <svekars@fb.com>\n",
        "file": "tutorials.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class PositionalEncoding(nn.Module):",
            "pe[:, 0::2] = torch.sin(position * div_term)",
            "pe[:, 1::2] = torch.cos(position * div_term)",
            "pe = pe.unsqueeze(0).transpose(0, 1)",
            "-        self.register_buffer('pe', pe)",
            "+        self.pe = nn.Parameter(pe, requires_grad=False)",
            "",
            "def forward(self, x):",
            "x = x + self.pe[:x.size(0), :]"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=expression_statement), node=('assignment', None), position=0, insert_id=1559729)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1559730)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=call), position=2)",
            "Update(target_node=ASTNode(type=identifier, text=register_buffer), value='pe')",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1559731)",
            "Move(target_node=ASTNode(type=,, text=,), node=ASTNode(type=argument_list), position=3)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'nn'), position=0, insert_id=1559732)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1559733)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Parameter'), position=2, insert_id=1559734)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1559735)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'requires_grad'), position=0, insert_id=1559736)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1559737)",
            "Insert(target_node=IN(type=keyword_argument), node=('false', 'False'), position=2, insert_id=1559738)",
            "Delete(target_node=ASTNode(type=string, text='pe'))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 15,
        "number": 4269,
        "neg_line": [
            "-self.register_buffer('pe', pe)"
        ],
        "pos_line": [
            "+self.pe = nn.Parameter(pe, requires_grad=False)"
        ],
        "core_change": "-self.register_buffer('pe', pe) +self.pe = nn.Parameter(pe, requires_grad=False)",
        "core_API": "sin"
    },
    {
        "commit_hash": "e173956c4dfc65137f2b8ff41460fa55f672e17a",
        "index": "0ac27aac..e4eda46f 100644",
        "commit_message": "Support more batch distributions in HaarReparam (#2731)\n\n* Support more batch distributions in HaarReparam\n\n* Reorder comment\n\n* Avoid draconian warning\n\n* Fix strictness check\n\n* Work around torch.view_as_complex() error\n\n* Add support for dist.Uniform\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def idct(x, dim=-1):",
            "X = torch.stack([x[..., :M], xi], dim=-1)",
            "coef_real = torch.cos(torch.linspace(0, 0.5 * math.pi, N + 1, dtype=x.dtype, device=x.device))",
            "coef = torch.stack([coef_real[:M], coef_real[-M:].flip(-1)], dim=-1)",
            "-    Y = torch.view_as_complex(coef) * torch.view_as_complex(X)",
            "+    Y = as_complex(coef) * as_complex(X)",
            "# Step 2",
            "y = irfft(Y, n=N)",
            "# Step 3"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=torch), value='as_complex')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=torch), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=torch), value='as_complex')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=torch), position=0)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=view_as_complex))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=view_as_complex))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 4273,
        "neg_line": [
            "-Y = torch.view_as_complex(coef) * torch.view_as_complex(X)"
        ],
        "pos_line": [
            "+Y = as_complex(coef) * as_complex(X)"
        ],
        "core_change": "-Y = torch.view_as_complex(coef) * torch.view_as_complex(X) +Y = as_complex(coef) * as_complex(X)",
        "core_API": "stack"
    },
    {
        "commit_hash": "fd1cf2f90b8af8dc11c1e373cbf98aa83ceb4df7",
        "index": "b1b8e8ce..1c4da039 100644",
        "commit_message": "Fix some pydocstyle lints (#784)\n\n* RTD with Mock Imports\n\n* cleanup docs (fix D208)\n\n* fix D300\n\n* fix D402\n\n* fix D412\n\n* fix D204\n\n* fix D403\n\n* fix D207\n\n* fix D301\n\n* fix D200\n\n* fix format\n\n* disable D302\n\n* fix D210\n\n* fix D202\n\n* fix doc test and make it happen earlier\n\n* Issue with Deprecated Decorator Fixed\n\n* Changelog updated + YAPF cleaning\n\n* Update .readthedocs.yml\n\n* disable D202, since it has conflict with yapf\n\n* TF 1.6.0 Errors fixed\n\n* YAPF Cleaning\n\n* Doc Mock Import for ROI Pooling\n\n* TL init if protected\n\n* Error fix - Test Documentation\n\n* fix documentation test\n\n* update changelog\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def retrieve_seq_length_op3(data, pad_val=0):  # HangSheng: return tensor for se",
            "",
            "",
            "def target_mask_op(data, pad_val=0):  # HangSheng: return tensor for mask,if input is tf.string",
            "-    \"\"\"Return tensor for mask, if input is ``tf.string``.",
            "-",
            "-    \"\"\"",
            "+    \"\"\"Return tensor for mask, if input is ``tf.string``.\"\"\"",
            "data_shape_size = data.get_shape().ndims",
            "if data_shape_size == 3:",
            "return tf.cast(tf.reduce_any(tf.not_equal(data, pad_val), axis=2), dtype=tf.int32)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"\"\"Return tensor for mask, if input is ``tf.string``.\n\n    \"\"\"), value='\"\"\"Return tensor for mask, if input is ``tf.string``.\"\"\"')"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 1,
        "number": 4274,
        "neg_line": [
            "-\"\"\"Return tensor for mask, if input is ``tf.string``.",
            "-",
            "-\"\"\""
        ],
        "pos_line": [
            "+\"\"\"Return tensor for mask, if input is ``tf.string``.\"\"\""
        ],
        "core_change": "-\"\"\"Return tensor for mask, if input is ``tf.string``. - -\"\"\" +\"\"\"Return tensor for mask, if input is ``tf.string``.\"\"\"",
        "core_API": "get_shape"
    },
    {
        "commit_hash": "4a107083dae59377aeee39ce94d20e340bba7782",
        "index": "d3adfa6ab..5c13beb4a 100644",
        "commit_message": "fix float16 for torch.ctc by casting float32\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class CTC(torch.nn.Module):",
            "# expected shape of seqLength x batchSize x alphabet_size",
            "dtype = ys_hat.dtype",
            "ys_hat = ys_hat.transpose(0, 1)",
            "-        if self.ctc_type == \"warpctc\":",
            "+        if self.ctc_type == \"warpctc\" or dtype == torch.float16:",
            "# warpctc only supports float32",
            "ys_hat = ys_hat.to(dtype=torch.float32)",
            "else:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=if_statement), node=('boolean_operator', None), position=1, insert_id=158029)",
            "Move(target_node=IN(type=boolean_operator), node=ASTNode(type=comparison_operator), position=0)",
            "Insert(target_node=IN(type=boolean_operator), node=('or', 'or'), position=1, insert_id=158030)",
            "Insert(target_node=IN(type=boolean_operator), node=('comparison_operator', None), position=2, insert_id=158031)",
            "Insert(target_node=IN(type=comparison_operator), node=('identifier', 'dtype'), position=0, insert_id=158032)",
            "Insert(target_node=IN(type=comparison_operator), node=('==', '=='), position=1, insert_id=158033)",
            "Insert(target_node=IN(type=comparison_operator), node=('attribute', None), position=2, insert_id=158034)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=158035)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=158036)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'float16'), position=2, insert_id=158037)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 4276,
        "neg_line": [
            "-if self.ctc_type == \"warpctc\":"
        ],
        "pos_line": [
            "+if self.ctc_type == \"warpctc\" or dtype == torch.float16:"
        ],
        "core_change": "-if self.ctc_type == \"warpctc\": +if self.ctc_type == \"warpctc\" or dtype == torch.float16:",
        "core_API": "transpose"
    },
    {
        "commit_hash": "8ba58675175e91d306f55380833458acfcb38cdd",
        "index": "bb2e28ad..d37d1d10 100644",
        "commit_message": "Added coverage to WikiTables ERM parser (#1181)\n\n* wikitables coverage parser\n\n* minor comment updates and bug fixes\n\n* updated table question kg test and coverage computation\n\n* removed unused imports\n\n* copy decoder step's output projection weights properly\n\n* fixed new variable creation\n\n* addressed pr comments\n\n* mypy issues\n\n* addressed remaining PR comments\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ExpectedRiskMinimization(DecoderTrainer[Callable[[StateType], torch.Tensor",
            "state.score,",
            "state.action_history):",
            "if self._normalize_by_length:",
            "-                    path_length = nn_util.new_variable_with_data(model_score,",
            "-                                                                 torch.Tensor([len(history)]))",
            "+                    path_length = Variable(model_score.data.new([len(history)]))",
            "model_score = model_score / path_length",
            "batch_scores[batch_index].append(model_score)",
            "return batch_scores"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=path_length), position=0)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=ERROR), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=nn_util), value='Variable')",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=37598)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=37599)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'new'), position=2, insert_id=37600)",
            "Update(target_node=ASTNode(type=identifier, text=torch), value='model_score')",
            "Update(target_node=ASTNode(type=identifier, text=Tensor), value='data')",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=new_variable_with_data))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=identifier, text=model_score))",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 14,
        "number": 4282,
        "neg_line": [
            "-path_length = nn_util.new_variable_with_data(model_score,",
            "-torch.Tensor([len(history)]))"
        ],
        "pos_line": [
            "+path_length = Variable(model_score.data.new([len(history)]))"
        ],
        "core_change": "-path_length = nn_util.new_variable_with_data(model_score, -torch.Tensor([len(history)])) +path_length = Variable(model_score.data.new([len(history)]))",
        "core_API": "new_variable_with_data"
    },
    {
        "commit_hash": "8e76796fd0535d8fbc9934e7f6604a1abcfb119e",
        "index": "a8bf265e26..65da4e1460 100644",
        "commit_message": "ci: Redo `format.sh --all` script & backfill lint fixes (#9956)\n\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def build_eager_tf_policy(name,",
            "dummy_batch[\"seq_lens\"] = np.array([1], dtype=np.int32)",
            "",
            "# Convert everything to tensors.",
            "-            dummy_batch = tf.nest.map_structure(",
            "-                tf1.convert_to_tensor, dummy_batch)",
            "+            dummy_batch = tf.nest.map_structure(tf1.convert_to_tensor,",
            "+                                                dummy_batch)",
            "",
            "# for IMPALA which expects a certain sample batch size.",
            "def tile_to(tensor, n):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 4284,
        "neg_line": [
            "-dummy_batch = tf.nest.map_structure(",
            "-tf1.convert_to_tensor, dummy_batch)"
        ],
        "pos_line": [
            "+dummy_batch = tf.nest.map_structure(tf1.convert_to_tensor,",
            "+dummy_batch)"
        ],
        "core_change": "-dummy_batch = tf.nest.map_structure( -tf1.convert_to_tensor, dummy_batch) +dummy_batch = tf.nest.map_structure(tf1.convert_to_tensor, +dummy_batch)",
        "core_API": "array"
    },
    {
        "commit_hash": "b51c9844a2ac0f25929caffedcbe6800d4299486",
        "index": "a53914d84..2800809fa 100644",
        "commit_message": "Fix send/get in chain Tensors and partial fix of torch func\n- All functionalities claimed have unit tests\n- torch func with args=FloatTensor and output=FloatTensor work remotely\n- So, output=IntTensors or output=float fails so far\n\n",
        "file": "PySyft.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class _TorchObject(object):",
            "def create_pointer(self, register=False):",
            "return self.child.create_pointer(parent=self, register=register)",
            "",
            "-    def footprint(self):",
            "-        return self.child.footprint()",
            "-",
            "def get(self):",
            "new_child_obj = self.child.get(parent=self)",
            "return self"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=def, text=def))",
            "Delete(target_node=ASTNode(type=identifier, text=footprint))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=parameters))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=return, text=return))",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=child))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=footprint))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=return_statement))",
            "Delete(target_node=ASTNode(type=block))",
            "Delete(target_node=ASTNode(type=function_definition))"
        ],
        "plus_line": 0,
        "minus_line": 2,
        "AST_diff_line": 22,
        "number": 4286,
        "neg_line": [
            "-def footprint(self):",
            "-return self.child.footprint()",
            "-"
        ],
        "pos_line": [],
        "core_change": "-def footprint(self): -return self.child.footprint() -",
        "core_API": "create_pointer"
    },
    {
        "commit_hash": "ae52995c503195215d9903b85651ce9c5a0fbcfd",
        "index": "5a9a406c..366501ac 100644",
        "commit_message": "Fix unit tests\n\n",
        "file": "flair.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "def test_load_no_dev_data_explicit(tasks_base_path):",
            "",
            "def test_multi_corpus(tasks_base_path):",
            "",
            "-    corpus_1 = flair.datasets.NER_GERMAN_GERMEVAL(tasks_base_path)",
            "+    corpus_1 = flair.datasets.ColumnCorpus(tasks_base_path  / \"germeval_14\", column_format={0: \"text\", 2: \"ner\"})",
            "",
            "corpus_2 = flair.datasets.ColumnCorpus(tasks_base_path / \"fashion\", column_format={0: \"text\", 2: \"ner\"})",
            "# get two corpora as one"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=NER_GERMAN_GERMEVAL), value='ColumnCorpus')",
            "Insert(target_node=ASTNode(type=argument_list), node=('binary_operator', None), position=1, insert_id=1792833)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1792834)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1792835)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=identifier, text=tasks_base_path), position=0)",
            "Insert(target_node=IN(type=binary_operator), node=('/', '/'), position=1, insert_id=1792836)",
            "Insert(target_node=IN(type=binary_operator), node=('string', '\"germeval_14\"'), position=2, insert_id=1792837)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'column_format'), position=0, insert_id=1792838)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1792839)",
            "Insert(target_node=IN(type=keyword_argument), node=('dictionary', None), position=2, insert_id=1792840)",
            "Insert(target_node=IN(type=dictionary), node=('{', '{'), position=0, insert_id=1792841)",
            "Insert(target_node=IN(type=dictionary), node=('pair', None), position=1, insert_id=1792842)",
            "Insert(target_node=IN(type=dictionary), node=(',', ','), position=2, insert_id=1792843)",
            "Insert(target_node=IN(type=dictionary), node=('pair', None), position=3, insert_id=1792844)",
            "Insert(target_node=IN(type=dictionary), node=('}', '}'), position=4, insert_id=1792845)",
            "Insert(target_node=IN(type=pair), node=('integer', '0'), position=0, insert_id=1792846)",
            "Insert(target_node=IN(type=pair), node=(':', ':'), position=1, insert_id=1792847)",
            "Insert(target_node=IN(type=pair), node=('string', '\"text\"'), position=2, insert_id=1792848)",
            "Insert(target_node=IN(type=pair), node=('integer', '2'), position=0, insert_id=1792849)",
            "Insert(target_node=IN(type=pair), node=(':', ':'), position=1, insert_id=1792850)",
            "Insert(target_node=IN(type=pair), node=('string', '\"ner\"'), position=2, insert_id=1792851)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 21,
        "number": 4290,
        "neg_line": [
            "-corpus_1 = flair.datasets.NER_GERMAN_GERMEVAL(tasks_base_path)"
        ],
        "pos_line": [
            "+corpus_1 = flair.datasets.ColumnCorpus(tasks_base_path  / \"germeval_14\", column_format={0: \"text\", 2: \"ner\"})"
        ],
        "core_change": "-corpus_1 = flair.datasets.NER_GERMAN_GERMEVAL(tasks_base_path) +corpus_1 = flair.datasets.ColumnCorpus(tasks_base_path  / \"germeval_14\", column_format={0: \"text\", 2: \"ner\"})",
        "core_API": "NER_GERMAN_GERMEVAL"
    },
    {
        "commit_hash": "c90465df7a0cdf07a538262ad15dbb32552e4a66",
        "index": "acb0dabb88..03d1e6be83 100644",
        "commit_message": "fix lint and docstring failures\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def full(",
            "fill_value: Union[int, float],",
            "*,",
            "dtype: tf.DType = None,",
            "-    device: str",
            "+    device: str,",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "with tf.device(as_native_dev(default_device(device))):",
            "return tf.fill("
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=8, insert_id=2007144)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 4291,
        "neg_line": [
            "-device: str"
        ],
        "pos_line": [
            "+device: str,"
        ],
        "core_change": "-device: str +device: str,",
        "core_API": "device"
    },
    {
        "commit_hash": "2708bfa1278a9b7db6fd99a158f9baeb109bba9b",
        "index": "637ec12b8..b0c9f4674 100644",
        "commit_message": "Rename compute_loss in TF models (#15207)\n\n* Rename compute_loss to hf_compute_loss to avoid conflicts with the new Keras method\n\n* make style\n\n* Adding deprecation warning to `compute_loss`\n\n* Fix sneaky reference to compute_loss\n\n* Replace logger.warning with warnings.warn\n\n* Clarifying warning and deprecation timeline\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFGPT2ForSequenceClassification(TFGPT2PreTrainedModel, TFSequenceClassific",
            "if not tf.is_tensor(sequence_lengths):",
            "in_logits = logits[0 : logits_shape[0], sequence_lengths]",
            "",
            "-            loss = self.compute_loss(tf.reshape(inputs[\"labels\"], [-1]), tf.reshape(in_logits, [-1, self.num_labels]))",
            "+            loss = self.hf_compute_loss(",
            "+                tf.reshape(inputs[\"labels\"], [-1]), tf.reshape(in_logits, [-1, self.num_labels])",
            "+            )",
            "pooled_logits = in_logits if in_logits is not None else logits",
            "",
            "if not inputs[\"return_dict\"]:"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=compute_loss), value='hf_compute_loss')"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 4292,
        "neg_line": [
            "-loss = self.compute_loss(tf.reshape(inputs[\"labels\"], [-1]), tf.reshape(in_logits, [-1, self.num_labels]))"
        ],
        "pos_line": [
            "+loss = self.hf_compute_loss(",
            "+tf.reshape(inputs[\"labels\"], [-1]), tf.reshape(in_logits, [-1, self.num_labels])",
            "+)"
        ],
        "core_change": "-loss = self.compute_loss(tf.reshape(inputs[\"labels\"], [-1]), tf.reshape(in_logits, [-1, self.num_labels])) +loss = self.hf_compute_loss( +tf.reshape(inputs[\"labels\"], [-1]), tf.reshape(in_logits, [-1, self.num_labels]) +)",
        "core_API": "is_tensor"
    },
    {
        "commit_hash": "3e87c9fa4d0c0f181734e0a053e8cd41b0849717",
        "index": "0fcc7c6..6a06a5d 100644",
        "commit_message": "fixes #484 (#495)\n\n\n",
        "file": "tensorboardX.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SummaryWriter(object):",
            "make_tsv(metadata, save_path, metadata_header=metadata_header)",
            "if label_img is not None:",
            "assert mat.shape[0] == label_img.shape[0], '#images should equal with #data points'",
            "+            assert label_img.shape[2] == label_img.shape[3], 'Image should be square, see tensorflow/tensorboard#670'",
            "make_sprite(label_img, save_path)",
            "assert mat.ndim == 2, 'mat should be 2D, where mat.size(0) is the number of data points'",
            "make_mat(mat, save_path)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('assert_statement', None), position=4, insert_id=1153096)",
            "Insert(target_node=IN(type=assert_statement), node=('assert', 'assert'), position=0, insert_id=1153097)",
            "Insert(target_node=IN(type=assert_statement), node=('comparison_operator', None), position=1, insert_id=1153098)",
            "Insert(target_node=IN(type=assert_statement), node=(',', ','), position=2, insert_id=1153099)",
            "Insert(target_node=IN(type=assert_statement), node=('string', \"'Image should be square, see tensorflow/tensorboard#670'\"), position=3, insert_id=1153100)",
            "Insert(target_node=IN(type=comparison_operator), node=('subscript', None), position=0, insert_id=1153101)",
            "Insert(target_node=IN(type=comparison_operator), node=('==', '=='), position=1, insert_id=1153102)",
            "Insert(target_node=IN(type=comparison_operator), node=('subscript', None), position=2, insert_id=1153103)",
            "Insert(target_node=IN(type=subscript), node=('attribute', None), position=0, insert_id=1153104)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=1153105)",
            "Insert(target_node=IN(type=subscript), node=('integer', '2'), position=2, insert_id=1153106)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=1153107)",
            "Insert(target_node=IN(type=subscript), node=('attribute', None), position=0, insert_id=1153108)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=1153109)",
            "Insert(target_node=IN(type=subscript), node=('integer', '3'), position=2, insert_id=1153110)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=1153111)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'label_img'), position=0, insert_id=1153112)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1153113)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'shape'), position=2, insert_id=1153114)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'label_img'), position=0, insert_id=1153115)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1153116)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'shape'), position=2, insert_id=1153117)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 22,
        "number": 4296,
        "neg_line": [],
        "pos_line": [
            "+assert label_img.shape[2] == label_img.shape[3], 'Image should be square, see tensorflow/tensorboard#670'"
        ],
        "core_change": "+assert label_img.shape[2] == label_img.shape[3], 'Image should be square, see tensorflow/tensorboard#670'",
        "core_API": "size"
    },
    {
        "commit_hash": "a41f9e83173c72317aa8909e8e7634cc2f33773f",
        "index": "b28e11ba..38deffcb 100755",
        "commit_message": "update and fix grad bug\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Model(ModelDesc):",
            "l = tf.nn.dropout(l, keep_prob)",
            "",
            "# fc will have activation summary by default. disable this for the output layer",
            "-        logits = FullyConnected('fc1', l, out_dim=10,",
            "-                             summary_activation=False, nl=tf.identity)",
            "+        logits = FullyConnected('fc1', l, out_dim=10, nl=tf.identity)",
            "prob = tf.nn.softmax(logits, name='prob')",
            "",
            "y = one_hot(label, 10)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=summary_activation))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=false, text=False))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 5,
        "number": 4297,
        "neg_line": [
            "-logits = FullyConnected('fc1', l, out_dim=10,",
            "-summary_activation=False, nl=tf.identity)"
        ],
        "pos_line": [
            "+logits = FullyConnected('fc1', l, out_dim=10, nl=tf.identity)"
        ],
        "core_change": "-logits = FullyConnected('fc1', l, out_dim=10, -summary_activation=False, nl=tf.identity) +logits = FullyConnected('fc1', l, out_dim=10, nl=tf.identity)",
        "core_API": "dropout"
    },
    {
        "commit_hash": "71a98c2a5fd014867188b8d81a05129cbc64268d",
        "index": "230e40e6..ca89a3fa 100644",
        "commit_message": "stricter typing for Optional[T] types, improve handling of Lazy params (#4743)\n\n* stricter typing for Optional[T] types\n\n* fix linting error\n\n* fix checkpointer test\n\n* fix add_field method\n\n* fix '_extract_token_and_type_ids' method\n\n* fix typing on Lazy\n\n* improve Lazy API\n\n* add notes about Lazy to CHANGELOG\n\n* fix CHANGELOG\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ElmoLstm(_EncoderBase):",
            "# the final states for all the layers.",
            "final_states.append(",
            "(",
            "-                    torch.cat([forward_state[0], backward_state[0]], -1),",
            "-                    torch.cat([forward_state[1], backward_state[1]], -1),",
            "+                    torch.cat([forward_state[0], backward_state[0]], -1),  # type: ignore",
            "+                    torch.cat([forward_state[1], backward_state[1]], -1),  # type: ignore",
            ")",
            ")"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 4300,
        "neg_line": [
            "-torch.cat([forward_state[0], backward_state[0]], -1),",
            "-torch.cat([forward_state[1], backward_state[1]], -1),"
        ],
        "pos_line": [
            "+torch.cat([forward_state[0], backward_state[0]], -1),  # type: ignore",
            "+torch.cat([forward_state[1], backward_state[1]], -1),  # type: ignore"
        ],
        "core_change": "-torch.cat([forward_state[0], backward_state[0]], -1), -torch.cat([forward_state[1], backward_state[1]], -1), +torch.cat([forward_state[0], backward_state[0]], -1),  # type: ignore +torch.cat([forward_state[1], backward_state[1]], -1),  # type: ignore",
        "core_API": "append"
    },
    {
        "commit_hash": "3d3f8b924c255c9026b5c134ff9bd265c3aba8d9",
        "index": "a312bb2ae..7bb9a05e4 100644",
        "commit_message": "fixed test\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_forward(use_token_averaged_energy, reduction_factor):",
            "es, elens = layer(xs, torch.LongTensor([384, 128]))",
            "assert es.shape[1] == max(elens)",
            "else:",
            "-        ds = torch.LongTensor([[3, 3, 1], [3, 0, 0]])",
            "+        ds = torch.LongTensor([[3, 3, 1], [3, 0, 0]]) // reduction_factor",
            "dlens = torch.LongTensor([3, 1])",
            "es, _ = layer(",
            "xs, torch.LongTensor([384, 128]), durations=ds, durations_lengths=dlens"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('binary_operator', None), position=4, insert_id=144380)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=binary_operator), node=('//', '//'), position=1, insert_id=144381)",
            "Insert(target_node=IN(type=binary_operator), node=('identifier', 'reduction_factor'), position=2, insert_id=144382)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 4301,
        "neg_line": [
            "-ds = torch.LongTensor([[3, 3, 1], [3, 0, 0]])"
        ],
        "pos_line": [
            "+ds = torch.LongTensor([[3, 3, 1], [3, 0, 0]]) // reduction_factor"
        ],
        "core_change": "-ds = torch.LongTensor([[3, 3, 1], [3, 0, 0]]) +ds = torch.LongTensor([[3, 3, 1], [3, 0, 0]]) // reduction_factor",
        "core_API": "LongTensor"
    },
    {
        "commit_hash": "fb4236a19f9b85800655695ad5cd237557183c78",
        "index": "cd1494b..bc387f4 100644",
        "commit_message": "Fixed PyTorch MNIST dataset (#2707)\n\nSigned-off-by: Travis Addair <tgaddair@gmail.com>\n",
        "file": "horovod.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "def train_fn():",
            "transforms.Normalize((0.1307, ), (0.3081, ))])",
            "",
            "test_dataset = datasets.MNIST(",
            "-        'data-%d' % hvd.rank(), train=False, transform=transformations)",
            "+        data_dir, train=False, transform=transformations)",
            "# Horovod: use DistributedSampler to partition the test data.",
            "test_sampler = torch.utils.data.distributed.DistributedSampler(",
            "test_dataset, num_replicas=hvd.size(), rank=hvd.rank())"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'data_dir'), position=1, insert_id=1794923)",
            "Delete(target_node=ASTNode(type=string, text='data-%d'))",
            "Delete(target_node=ASTNode(type=%, text=%))",
            "Delete(target_node=ASTNode(type=identifier, text=hvd))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=rank))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=binary_operator))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 12,
        "number": 4302,
        "neg_line": [
            "-'data-%d' % hvd.rank(), train=False, transform=transformations)"
        ],
        "pos_line": [
            "+data_dir, train=False, transform=transformations)"
        ],
        "core_change": "-'data-%d' % hvd.rank(), train=False, transform=transformations) +data_dir, train=False, transform=transformations)",
        "core_API": "Normalize"
    },
    {
        "commit_hash": "f3937bc8f3667772c9f1428b66f0c44b6087b04d",
        "index": "7d72ddf7..0ab92eff 100644",
        "commit_message": "[Refactor] Remove set_seed (#289)\n\n* [Refactor] Remove set_seed and class attributes\n\n* apply anton's suggestiosn\n\n* fix\n\n* Apply suggestions from code review\n\nCo-authored-by: Pedro Cuenca <pedro@huggingface.co>\n\n* up\n\n* update\n\n* make style\n\n* Apply suggestions from code review\n\nCo-authored-by: Anton Lozhkov <anton@huggingface.co>\n\n* make fix-copies\n\n* make style\n\n* make style and new copies\n\nCo-authored-by: Pedro Cuenca <pedro@huggingface.co>\nCo-authored-by: Anton Lozhkov <anton@huggingface.co>\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ScoreSdeVePipeline(DiffusionPipeline):",
            "# correction step",
            "for _ in range(self.scheduler.correct_steps):",
            "model_output = self.unet(sample, sigma_t)[\"sample\"]",
            "-                sample = self.scheduler.step_correct(model_output, sample)[\"prev_sample\"]",
            "+                sample = self.scheduler.step_correct(model_output, sample, generator=generator)[\"prev_sample\"]",
            "",
            "# prediction step",
            "model_output = model(sample, sigma_t)[\"sample\"]",
            "-            output = self.scheduler.step_pred(model_output, t, sample)",
            "+            output = self.scheduler.step_pred(model_output, t, sample, generator=generator)",
            "",
            "sample, sample_mean = output[\"prev_sample\"], output[\"prev_sample_mean\"]"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=6, insert_id=106189)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=7, insert_id=106190)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=106191)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=106192)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'generator'), position=0, insert_id=106193)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=106194)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'generator'), position=2, insert_id=106195)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'generator'), position=0, insert_id=106196)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=106197)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'generator'), position=2, insert_id=106198)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 10,
        "number": 4304,
        "neg_line": [
            "-sample = self.scheduler.step_correct(model_output, sample)[\"prev_sample\"]",
            "-output = self.scheduler.step_pred(model_output, t, sample)"
        ],
        "pos_line": [
            "+sample = self.scheduler.step_correct(model_output, sample, generator=generator)[\"prev_sample\"]",
            "+output = self.scheduler.step_pred(model_output, t, sample, generator=generator)"
        ],
        "core_change": "-sample = self.scheduler.step_correct(model_output, sample)[\"prev_sample\"] +sample = self.scheduler.step_correct(model_output, sample, generator=generator)[\"prev_sample\"] -output = self.scheduler.step_pred(model_output, t, sample) +output = self.scheduler.step_pred(model_output, t, sample, generator=generator)",
        "core_API": "unet"
    },
    {
        "commit_hash": "7fb6b414864c4a1976346f3d593665628d2111ab",
        "index": "7692ee5f..4890994c 100644",
        "commit_message": "fix linter and mypy error for laplacian\n\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Laplacian(nn.Module):",
            "computed = (kernel_size - 1) // 2",
            "return computed",
            "",
            "-    def forward(self, x: torch.Tensor):",
            "+    def forward(self, x: torch.Tensor):  # type: ignore",
            "if not torch.is_tensor(x):",
            "raise TypeError(\"Input x type is not a torch.Tensor. Got {}\"",
            ".format(type(x)))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 4307,
        "neg_line": [
            "-def forward(self, x: torch.Tensor):"
        ],
        "pos_line": [
            "+def forward(self, x: torch.Tensor):  # type: ignore"
        ],
        "core_change": "-def forward(self, x: torch.Tensor): +def forward(self, x: torch.Tensor):  # type: ignore",
        "core_API": "is_tensor"
    },
    {
        "commit_hash": "9ad776bf1500a56fa60cd961516dc14ca4926f8c",
        "index": "84905878..ae9f4bf7 100644",
        "commit_message": "Adds quick fix for pretrained models not loading by modifying state_dict keys on load. (#2911)\n\n* Adds quick fix for pretrained models not loading.  Fix modifies state_dict keys before loading.\n\n* Shortens keys in test equally.\n",
        "file": "ludwig.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ECD(BaseModel):",
            "weights_save_path = os.path.join(save_path, MODEL_WEIGHTS_FILE_NAME)",
            "device = torch.device(get_torch_device())",
            "with open_file(weights_save_path, \"rb\") as f:",
            "-            self.load_state_dict(torch.load(f, map_location=device))",
            "+            state_dict = torch.load(f, map_location=device)",
            "+            self.load_state_dict(update_state_dict(state_dict))",
            "",
            "def get_args(self):",
            "\"\"\"Returns init arguments for constructing this model.\"\"\""
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=block), node=('expression_statement', None), position=1, insert_id=596794)",
            "Insert(target_node=ASTNode(type=expression_statement), node=('assignment', None), position=0, insert_id=596795)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=596796)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'state_dict'), position=0, insert_id=596797)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=596798)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=call), position=2)",
            "Move(target_node=IN(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=596799)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=596800)",
            "Insert(target_node=IN(type=argument_list), node=('call', None), position=1, insert_id=596801)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=596802)",
            "Insert(target_node=IN(type=call), node=('identifier', 'update_state_dict'), position=0, insert_id=596803)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=596804)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=596805)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'state_dict'), position=1, insert_id=596806)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=596807)",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 20,
        "number": 4308,
        "neg_line": [
            "-self.load_state_dict(torch.load(f, map_location=device))"
        ],
        "pos_line": [
            "+state_dict = torch.load(f, map_location=device)",
            "+self.load_state_dict(update_state_dict(state_dict))"
        ],
        "core_change": "-self.load_state_dict(torch.load(f, map_location=device)) +state_dict = torch.load(f, map_location=device) +self.load_state_dict(update_state_dict(state_dict))",
        "core_API": "join"
    },
    {
        "commit_hash": "50a9828679d075772a0875a5b2488fb9febb1082",
        "index": "775803a..0750be6 100644",
        "commit_message": "DDP `torch.jit.trace()` `--sync-bn` fix (#4615)\n\n* Remove assert\n\n* debug0\n\n* trace=not opt.sync\n\n* sync to sync_bn fix\n\n* Cleanup\n",
        "file": "yolov5.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Loggers():",
            "if self.wandb:",
            "self.wandb.log({\"Labels\": [wandb.Image(str(x), caption=x.name) for x in paths]})",
            "",
            "-    def on_train_batch_end(self, ni, model, imgs, targets, paths, plots):",
            "+    def on_train_batch_end(self, ni, model, imgs, targets, paths, plots, sync_bn):",
            "# Callback runs on train batch end",
            "if plots:",
            "if ni == 0:",
            "-                with warnings.catch_warnings():",
            "-                    warnings.simplefilter('ignore')  # suppress jit trace warning",
            "-                    self.tb.add_graph(torch.jit.trace(de_parallel(model), imgs[0:1], strict=False), [])",
            "+                if not sync_bn:  # tb.add_graph() --sync known issue https://github.com/ultralytics/yolov5/issues/3754",
            "+                    with warnings.catch_warnings():",
            "+                        warnings.simplefilter('ignore')  # suppress jit trace warning",
            "+                        self.tb.add_graph(torch.jit.trace(de_parallel(model), imgs[0:1], strict=False), [])",
            "if ni < 3:",
            "f = self.save_dir / f'train_batch{ni}.jpg'  # filename",
            "Thread(target=plot_images, args=(imgs, targets, paths, f), daemon=True).start()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=if_statement), node=('block', None), position=3, insert_id=1296930)",
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=14, insert_id=1296931)",
            "Insert(target_node=ASTNode(type=parameters), node=('identifier', 'sync_bn'), position=15, insert_id=1296932)",
            "Insert(target_node=IN(type=block), node=('if_statement', None), position=0, insert_id=1296933)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=1296934)",
            "Insert(target_node=IN(type=if_statement), node=('not_operator', None), position=1, insert_id=1296935)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=1296936)",
            "Move(target_node=IN(type=if_statement), node=ASTNode(type=block), position=3)",
            "Insert(target_node=IN(type=not_operator), node=('not', 'not'), position=0, insert_id=1296937)",
            "Insert(target_node=IN(type=not_operator), node=('identifier', 'sync_bn'), position=1, insert_id=1296938)"
        ],
        "plus_line": 5,
        "minus_line": 4,
        "AST_diff_line": 10,
        "number": 4310,
        "neg_line": [
            "-def on_train_batch_end(self, ni, model, imgs, targets, paths, plots):",
            "-with warnings.catch_warnings():",
            "-warnings.simplefilter('ignore')  # suppress jit trace warning",
            "-self.tb.add_graph(torch.jit.trace(de_parallel(model), imgs[0:1], strict=False), [])"
        ],
        "pos_line": [
            "+def on_train_batch_end(self, ni, model, imgs, targets, paths, plots, sync_bn):",
            "+if not sync_bn:  # tb.add_graph() --sync known issue https://github.com/ultralytics/yolov5/issues/3754",
            "+with warnings.catch_warnings():",
            "+warnings.simplefilter('ignore')  # suppress jit trace warning",
            "+self.tb.add_graph(torch.jit.trace(de_parallel(model), imgs[0:1], strict=False), [])"
        ],
        "core_change": "-def on_train_batch_end(self, ni, model, imgs, targets, paths, plots): +def on_train_batch_end(self, ni, model, imgs, targets, paths, plots, sync_bn): -with warnings.catch_warnings(): -warnings.simplefilter('ignore')  # suppress jit trace warning -self.tb.add_graph(torch.jit.trace(de_parallel(model), imgs[0:1], strict=False), []) +if not sync_bn:  # tb.add_graph() --sync known issue https://github.com/ultralytics/yolov5/issues/3754 +with warnings.catch_warnings(): +warnings.simplefilter('ignore')  # suppress jit trace warning +self.tb.add_graph(torch.jit.trace(de_parallel(model), imgs[0:1], strict=False), [])",
        "core_API": "log"
    },
    {
        "commit_hash": "b4bdadfe15f0bdd14e4902c5efd3e4c87c5d2ad9",
        "index": "54d451b..91c930b 100644",
        "commit_message": "fix model: relu/normalize after every conv1d\n\n",
        "file": "deep-voice-conversion.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Model:",
            "enc = tf.layers.max_pooling1d(enc, 2, 1, padding=\"same\")  # (N, T, K * E / 2)",
            "",
            "### Conv1D projections",
            "-            enc = conv1d(enc, hp.hidden_units // 2, 3, scope=\"conv1d_1\")  # (N, T, E/2)",
            "-            enc = normalize(enc, type=hp.norm_type, is_training=self.is_training, activation_fn=tf.nn.relu)",
            "-            enc = conv1d(enc, hp.hidden_units // 2, 3, scope=\"conv1d_2\")  # (N, T, E/2)",
            "+            enc = conv1d(enc, hp.hidden_units // 2, 3, scope=\"conv1d_1\", activation_fn=tf.nn.relu)  # (N, T, E/2)",
            "+            enc = normalize(enc, type=hp.norm_type, is_training=self.is_training)",
            "+            enc = conv1d(enc, hp.hidden_units // 2, 3, scope=\"conv1d_2\", activation_fn=None)  # (N, T, E/2)",
            "enc += prenet_out  # (N, T, E/2) # residual connections",
            "",
            "### Highway Nets"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('binary_operator', None), position=3, insert_id=1920941)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=9, insert_id=1920942)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=keyword_argument), position=10)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=binary_operator), position=3)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=9, insert_id=1920943)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=10, insert_id=1920944)",
            "Insert(target_node=IN(type=binary_operator), node=('attribute', None), position=0, insert_id=1920945)",
            "Insert(target_node=IN(type=binary_operator), node=('//', '//'), position=1, insert_id=1920946)",
            "Insert(target_node=IN(type=binary_operator), node=('integer', '2'), position=2, insert_id=1920947)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'activation_fn'), position=0, insert_id=1920948)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1920949)",
            "Insert(target_node=IN(type=keyword_argument), node=('none', 'None'), position=2, insert_id=1920950)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'hp'), position=0, insert_id=1920951)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1920952)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'hidden_units'), position=2, insert_id=1920953)",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=hp))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=hidden_units))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=//, text=//))",
            "Delete(target_node=ASTNode(type=integer, text=2))",
            "Delete(target_node=ASTNode(type=binary_operator))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 23,
        "number": 4311,
        "neg_line": [
            "-enc = conv1d(enc, hp.hidden_units // 2, 3, scope=\"conv1d_1\")  # (N, T, E/2)",
            "-enc = normalize(enc, type=hp.norm_type, is_training=self.is_training, activation_fn=tf.nn.relu)",
            "-enc = conv1d(enc, hp.hidden_units // 2, 3, scope=\"conv1d_2\")  # (N, T, E/2)"
        ],
        "pos_line": [
            "+enc = conv1d(enc, hp.hidden_units // 2, 3, scope=\"conv1d_1\", activation_fn=tf.nn.relu)  # (N, T, E/2)",
            "+enc = normalize(enc, type=hp.norm_type, is_training=self.is_training)",
            "+enc = conv1d(enc, hp.hidden_units // 2, 3, scope=\"conv1d_2\", activation_fn=None)  # (N, T, E/2)"
        ],
        "core_change": "-enc = conv1d(enc, hp.hidden_units // 2, 3, scope=\"conv1d_1\")  # (N, T, E/2) -enc = normalize(enc, type=hp.norm_type, is_training=self.is_training, activation_fn=tf.nn.relu) -enc = conv1d(enc, hp.hidden_units // 2, 3, scope=\"conv1d_2\")  # (N, T, E/2) +enc = conv1d(enc, hp.hidden_units // 2, 3, scope=\"conv1d_1\", activation_fn=tf.nn.relu)  # (N, T, E/2) +enc = normalize(enc, type=hp.norm_type, is_training=self.is_training) +enc = conv1d(enc, hp.hidden_units // 2, 3, scope=\"conv1d_2\", activation_fn=None)  # (N, T, E/2)",
        "core_API": "max_pooling1d"
    },
    {
        "commit_hash": "553dccfaa8211a4f7f2ba7dad8f693f2434ad1fd",
        "index": "3aba330e..58f991a5 100644",
        "commit_message": "bugfix\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def eval_on_ILSVRC12(model, model_file, dataflow):",
            "",
            "def image_preprocess(image, bgr=True):",
            "if image.dtype.base_dtype != tf.float32:",
            "-        image = tf.case(image, tf.float32)",
            "+        image = tf.cast(image, tf.float32)",
            "image = image * (1.0 / 255)",
            "",
            "mean = [0.485, 0.456, 0.406]    # rgb"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=case), value='cast')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 4312,
        "neg_line": [
            "-image = tf.case(image, tf.float32)"
        ],
        "pos_line": [
            "+image = tf.cast(image, tf.float32)"
        ],
        "core_change": "-image = tf.case(image, tf.float32) +image = tf.cast(image, tf.float32)",
        "core_API": "case"
    },
    {
        "commit_hash": "53ed4b0c7beed94f5caa176facb9a7ad151981ba",
        "index": "6f699b85..f6b9775e 100644",
        "commit_message": "Add test-cuda make target, and fix cuda bugs (#338)\n\n* Add test-cuda make target; fix cuda bugs\n\n* Fix more cuda bugs; distributions now passes\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Multinomial(Distribution):",
            "n = int(n.data.cpu()[0][0])",
            "else:",
            "n = int(n.data.cpu()[0])",
            "-        return Variable(torch.multinomial(ps.data, n, replacement=True))",
            "+        return Variable(torch_multinomial(ps.data, n, replacement=True))",
            "",
            "def batch_log_pdf(self, x, ps=None, n=None):",
            "ps, n = self._sanitize_input(ps, n)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=torch), value='torch_multinomial')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=torch), position=0)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=multinomial))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 4313,
        "neg_line": [
            "-return Variable(torch.multinomial(ps.data, n, replacement=True))"
        ],
        "pos_line": [
            "+return Variable(torch_multinomial(ps.data, n, replacement=True))"
        ],
        "core_change": "-return Variable(torch.multinomial(ps.data, n, replacement=True)) +return Variable(torch_multinomial(ps.data, n, replacement=True))",
        "core_API": "cpu"
    },
    {
        "commit_hash": "029e137bc2024f730d5ddc90c8a0e80aa9ff2bfd",
        "index": "ef77e34..468fdf8 100755",
        "commit_message": "bug fix\n\n",
        "file": "yolov3.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def build_targets(p, targets, model):",
            "tcls, tbox, indices, av = [], [], [], []",
            "reject, use_all_anchors = True, True",
            "gain = torch.ones(6, device=targets.device)  # normalized to gridspace gain",
            "+",
            "+    # m = list(model.modules())[-1]",
            "+    # for i in range(m.nl):",
            "+    #     anchor_vec = m.anchor_vec[i]",
            "multi_gpu = type(model) in (nn.parallel.DataParallel, nn.parallel.DistributedDataParallel)",
            "for i, j in enumerate(model.yolo_layers):",
            "# get number of grid points and anchor vec for this yolo layer",
            "anchor_vec = model.module.module_list[j].anchor_vec if multi_gpu else model.module_list[j].anchor_vec",
            "",
            "# iou of targets-anchors",
            "-        gain[2:] = torch.tensor(p[i].shape)[[2, 3, 2, 3]]  # xyxy gain",
            "+        gain[2:] = torch.tensor(p[i].shape)[[3, 2, 3, 2]]  # xyxy gain",
            "t, a = targets * gain, []",
            "gwh = t[:, 4:6]",
            "if nt:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=integer, text=3), node=ASTNode(type=list), position=1)",
            "Insert(target_node=ASTNode(type=list), node=(',', ','), position=4, insert_id=1280096)",
            "Insert(target_node=ASTNode(type=list), node=('integer', '3'), position=4, insert_id=1280097)",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=integer, text=3))"
        ],
        "plus_line": 4,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 4314,
        "neg_line": [
            "-gain[2:] = torch.tensor(p[i].shape)[[2, 3, 2, 3]]  # xyxy gain"
        ],
        "pos_line": [
            "+",
            "+# m = list(model.modules())[-1]",
            "+# for i in range(m.nl):",
            "+#     anchor_vec = m.anchor_vec[i]",
            "+gain[2:] = torch.tensor(p[i].shape)[[3, 2, 3, 2]]  # xyxy gain"
        ],
        "core_change": "+ +# m = list(model.modules())[-1] +# for i in range(m.nl): +#     anchor_vec = m.anchor_vec[i] -gain[2:] = torch.tensor(p[i].shape)[[2, 3, 2, 3]]  # xyxy gain +gain[2:] = torch.tensor(p[i].shape)[[3, 2, 3, 2]]  # xyxy gain",
        "core_API": "ones"
    },
    {
        "commit_hash": "02c9c545579ec2054cb38b8a5193f26f11540fd9",
        "index": "98c86adc..7caa784e 100644",
        "commit_message": "Fix default dtype in HJM, HullWhite, and Heston model, as well as in PiecewiseConstant class.\n\nPiperOrigin-RevId: 387639335\n\n",
        "file": "tf-quant-finance.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class GaussianHJM(quasi_gaussian_hjm.QuasiGaussianHJM):",
            "",
            "self._exact_discretization_setup(dim)",
            "super(quasi_gaussian_hjm.QuasiGaussianHJM,",
            "-            self).__init__(dim, _drift_fn, _vol_fn, dtype, self._name)",
            "+            self).__init__(dim, _drift_fn, _vol_fn, self._dtype, self._name)",
            "",
            "def sample_paths(self,",
            "times,"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=7, insert_id=2336765)",
            "Update(target_node=ASTNode(type=identifier, text=dtype), value='self')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=dtype), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2336766)",
            "Insert(target_node=IN(type=attribute), node=('identifier', '_dtype'), position=2, insert_id=2336767)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 4316,
        "neg_line": [
            "-self).__init__(dim, _drift_fn, _vol_fn, dtype, self._name)"
        ],
        "pos_line": [
            "+self).__init__(dim, _drift_fn, _vol_fn, self._dtype, self._name)"
        ],
        "core_change": "-self).__init__(dim, _drift_fn, _vol_fn, dtype, self._name) +self).__init__(dim, _drift_fn, _vol_fn, self._dtype, self._name)",
        "core_API": "_exact_discretization_setup"
    },
    {
        "commit_hash": "ef2a6088ff5bc515be27e24f5f6c8b16b96d3bb8",
        "index": "5a1cf95cf..a9910294d 100644",
        "commit_message": "Drop support for PyTorch 1.10 (#16492)\n\n* Drop support for PyTorch 1.10\n\n* CHANGELOG\n\n* READMEs\n\n* mypy\n\n* ls\n\n* New poplar version\n\n* Fixed tests\n\n* links\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* skip azure badges\n\n* Table\n\n* Matching dockerfiles\n\n* Drop unnecessary channels and packages\n\n* Push nightly\n\n* Undo unrelated changes\n\n* Revert \"Push nightly\"\n\nThis reverts commit 9618f737c4dc65331fef4bb11fe46a61513d220a.\n\n---------\n\nCo-authored-by: Jirka <jirka.borovec@seznam.cz>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class AcceleratorConnector:",
            "",
            "def _init_deterministic(self, deterministic: Optional[Union[bool, _LITERAL_WARN]]) -> None:",
            "self.deterministic = deterministic or False  # default to False if not set",
            "-        if _TORCH_GREATER_EQUAL_1_11 and deterministic == \"warn\":",
            "+        if deterministic == \"warn\":",
            "torch.use_deterministic_algorithms(True, warn_only=True)",
            "-        else:",
            "-            torch.use_deterministic_algorithms(self.deterministic)",
            "if self.deterministic:",
            "# https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility",
            "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\""
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=if_statement), node=ASTNode(type=comparison_operator), position=1)",
            "Delete(target_node=ASTNode(type=identifier, text=_TORCH_GREATER_EQUAL_1_11))",
            "Delete(target_node=ASTNode(type=and, text=and))",
            "Delete(target_node=ASTNode(type=boolean_operator))",
            "Delete(target_node=ASTNode(type=identifier, text=else))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=use_deterministic_algorithms))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=deterministic))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=type))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 21,
        "number": 4317,
        "neg_line": [
            "-if _TORCH_GREATER_EQUAL_1_11 and deterministic == \"warn\":",
            "-else:",
            "-torch.use_deterministic_algorithms(self.deterministic)"
        ],
        "pos_line": [
            "+if deterministic == \"warn\":"
        ],
        "core_change": "-if _TORCH_GREATER_EQUAL_1_11 and deterministic == \"warn\": +if deterministic == \"warn\": -else: -torch.use_deterministic_algorithms(self.deterministic)",
        "core_API": "use_deterministic_algorithms"
    },
    {
        "commit_hash": "948395b15a3f8eb022129cfe31be3d87d310032b",
        "index": "640a9ca07d..39b1efd675 100644",
        "commit_message": "lint fixes\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def linear_resample(x, num_samples, axis=-1):",
            "num_x_dims = len(x_shape)",
            "axis = axis % num_x_dims",
            "num_vals = x.shape[axis]",
            "-    if x.dtype not in ['float16','float32','float64']:",
            "-        x=tf.cast(x,tf.float32)",
            "+    if x.dtype not in [\"float16\", \"float32\", \"float64\"]:",
            "+        x = tf.cast(x, tf.float32)",
            "xp = tf.range(num_vals, dtype=tf.float32)",
            "x_coords = tf.range(num_samples, dtype=tf.float32) * (",
            "-                (num_vals - 1) / (num_samples - 1)",
            "+            (num_vals - 1) / (num_samples - 1)",
            ")",
            "else:",
            "xp = tf.range(num_vals, dtype=x.dtype)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text='float16'), value='\"float16\"')",
            "Update(target_node=ASTNode(type=string, text='float32'), value='\"float32\"')",
            "Update(target_node=ASTNode(type=string, text='float64'), value='\"float64\"')"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 3,
        "number": 4318,
        "neg_line": [
            "-if x.dtype not in ['float16','float32','float64']:",
            "-x=tf.cast(x,tf.float32)",
            "-(num_vals - 1) / (num_samples - 1)"
        ],
        "pos_line": [
            "+if x.dtype not in [\"float16\", \"float32\", \"float64\"]:",
            "+x = tf.cast(x, tf.float32)",
            "+(num_vals - 1) / (num_samples - 1)"
        ],
        "core_change": "-if x.dtype not in ['float16','float32','float64']: -x=tf.cast(x,tf.float32) +if x.dtype not in [\"float16\", \"float32\", \"float64\"]: +x = tf.cast(x, tf.float32) -(num_vals - 1) / (num_samples - 1) +(num_vals - 1) / (num_samples - 1)",
        "core_API": "cast"
    },
    {
        "commit_hash": "8bf6d28c10493adb4f8d60042aa78bcc66d5e214",
        "index": "33401c3c0..0719700c0 100644",
        "commit_message": "made _load_pretrained_model_low_mem static + bug fix (#16548)\n\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin, PushToHubMix",
            "resolved_archive_file = [resolved_archive_file]",
            "",
            "for archive_file in resolved_archive_file:",
            "-            state_dict = torch.load(resolved_archive_file, map_location=\"cpu\")",
            "+            state_dict = torch.load(archive_file, map_location=\"cpu\")",
            "",
            "# materialize state_dict entries one by one on CPU",
            "for k in loaded_state_dict_keys:"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=resolved_archive_file), value='archive_file')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 4320,
        "neg_line": [
            "-state_dict = torch.load(resolved_archive_file, map_location=\"cpu\")"
        ],
        "pos_line": [
            "+state_dict = torch.load(archive_file, map_location=\"cpu\")"
        ],
        "core_change": "-state_dict = torch.load(resolved_archive_file, map_location=\"cpu\") +state_dict = torch.load(archive_file, map_location=\"cpu\")",
        "core_API": "load"
    },
    {
        "commit_hash": "66a5a6fda8fdb71b35d76d98d8908eb2cc586e00",
        "index": "14741d862..9f30335f8 100644",
        "commit_message": "fix to ensure that returned tensors after the tokenization is Long (#7039)\n\n* fix to ensure that returned tensors after the tokenization is Long\n\n* fix to ensure that returned tensors after the tokenization is Long\n\nCo-authored-by: Ashwin Geet Dsa <adsa@grvingt-6.nancy.grid5000.fr>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DataCollatorForLanguageModeling:",
            ") -> torch.Tensor:",
            "# In order to accept both lists of lists and lists of Tensors",
            "if isinstance(examples[0], (list, tuple)):",
            "-            examples = [torch.Tensor(e) for e in examples]",
            "+            examples = [torch.tensor(e, dtype=torch.long) for e in examples]",
            "length_of_first = examples[0].size(0)",
            "are_tensors_same_length = all(x.size(0) == length_of_first for x in examples)",
            "if are_tensors_same_length:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=Tensor), value='tensor')",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1232699)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1232700)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=1232701)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1232702)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=1232703)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1232704)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1232705)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'long'), position=2, insert_id=1232706)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 4323,
        "neg_line": [
            "-examples = [torch.Tensor(e) for e in examples]"
        ],
        "pos_line": [
            "+examples = [torch.tensor(e, dtype=torch.long) for e in examples]"
        ],
        "core_change": "-examples = [torch.Tensor(e) for e in examples] +examples = [torch.tensor(e, dtype=torch.long) for e in examples]",
        "core_API": "Tensor"
    },
    {
        "commit_hash": "a4d5494d46baf89df50b6cc1914b0f479eb4283f",
        "index": "2fc57882c..965346310 100644",
        "commit_message": "[release] fix long running horovod tune tests. (#29505)\n\n\n\nCo-authored-by: Amog Kamsetty <amogkam@users.noreply.github.com>\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def train_loop_per_worker(config):",
            "import horovod.torch as hvd",
            "",
            "hvd.init()",
            "-    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")",
            "+    device = ray.train.torch.get_device()",
            "mode = config[\"mode\"]",
            "net = Net(mode).to(device)",
            "optimizer = torch.optim.SGD("
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1106849)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=argument_list), position=1)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1106850)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=cuda), value='get_device')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=cuda), position=2)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1106851)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=torch), position=2)",
            "Update(target_node=ASTNode(type=identifier, text=torch), value='ray')",
            "Update(target_node=ASTNode(type=identifier, text=device), value='train')",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=string, text=\"cuda\"))",
            "Delete(target_node=ASTNode(type=if, text=if))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=is_available))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=else, text=else))",
            "Delete(target_node=ASTNode(type=string, text=\"cpu\"))",
            "Delete(target_node=ASTNode(type=conditional_expression))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 24,
        "number": 4326,
        "neg_line": [
            "-device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
        ],
        "pos_line": [
            "+device = ray.train.torch.get_device()"
        ],
        "core_change": "-device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") +device = ray.train.torch.get_device()",
        "core_API": "init"
    },
    {
        "commit_hash": "63e68e24915981a815c35a1ac15b4ba80310c0ae",
        "index": "902cc7ab..5f53d2c7 100644",
        "commit_message": "Fix comment.\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class ExpandDims(Preprocessor):",
            "return shape[:position] + (1,) + shape[position:]",
            "",
            "def tf_process(self, tensor):",
            "-        # Flatten tensor",
            "-        return tf.expand_dims(tensor, self.axis)",
            "+        # Expand tensor.",
            "+        return tf.expand_dims(input=tensor, axis=self.axis)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=1, insert_id=2235440)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=4, insert_id=2235441)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'input'), position=0, insert_id=2235442)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2235443)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=identifier, text=tensor), position=2)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'axis'), position=0, insert_id=2235444)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2235445)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=attribute), position=2)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 8,
        "number": 4327,
        "neg_line": [
            "-# Flatten tensor",
            "-return tf.expand_dims(tensor, self.axis)"
        ],
        "pos_line": [
            "+# Expand tensor.",
            "+return tf.expand_dims(input=tensor, axis=self.axis)"
        ],
        "core_change": "-# Flatten tensor -return tf.expand_dims(tensor, self.axis) +# Expand tensor. +return tf.expand_dims(input=tensor, axis=self.axis)",
        "core_API": "expand_dims"
    },
    {
        "commit_hash": "80e566342235622cf2dea655600d3a5d894f60f7",
        "index": "e44683a6c4..03685c37d8 100644",
        "commit_message": "small fix to unravel_index of torch\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def unravel_index(",
            "*,",
            "out: Optional[torch.Tensor] = None,",
            ") -> torch.Tensor:",
            "-    temp = indices.to(\"int64\")",
            "+    temp = indices.detach()",
            "output = []",
            "for dim in reversed(shape):",
            "output.append(temp % dim)",
            "temp = temp // dim",
            "-    return torch.tensor(reversed(output))",
            "+    return torch.tensor(reversed(output), dtype=torch.int64)",
            "",
            "",
            "unravel_index.support_native_out = False"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=264462)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=264463)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=264464)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=264465)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=264466)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=264467)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=264468)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'int64'), position=2, insert_id=264469)",
            "Update(target_node=ASTNode(type=identifier, text=to), value='detach')",
            "Delete(target_node=ASTNode(type=string, text=\"int64\"))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 10,
        "number": 4329,
        "neg_line": [
            "-temp = indices.to(\"int64\")",
            "-return torch.tensor(reversed(output))"
        ],
        "pos_line": [
            "+temp = indices.detach()",
            "+return torch.tensor(reversed(output), dtype=torch.int64)"
        ],
        "core_change": "-temp = indices.to(\"int64\") +temp = indices.detach() -return torch.tensor(reversed(output)) +return torch.tensor(reversed(output), dtype=torch.int64)",
        "core_API": "to"
    },
    {
        "commit_hash": "e1ee46fb5d40d4dcbdee1c731299cae0d4964fd0",
        "index": "31a3ca2e3..1097cc83b 100644",
        "commit_message": "Fix indent\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class E2E(ASRInterface, torch.nn.Module):",
            "lpz = None",
            "",
            "word_eds, word_ref_lens, char_eds, char_ref_lens = [], [], [], []",
            "-            nbest_hyps = self.dec.recognize_beam_batch(hs_pad, torch.tensor(hlens), lpz,",
            "-                                                       self.recog_args, self.char_list,",
            "-                                                       self.rnnlm,",
            "-                                                       tgt_lang_ids=tgt_lang_ids.squeeze(1).tolist() if self.replace_sos else None)",
            "+            nbest_hyps = self.dec.recognize_beam_batch(",
            "+                hs_pad, torch.tensor(hlens), lpz,",
            "+                self.recog_args, self.char_list,",
            "+                self.rnnlm,",
            "+                tgt_lang_ids=tgt_lang_ids.squeeze(1).tolist() if self.replace_sos else None)",
            "# remove <sos> and <eos>",
            "y_hats = [nbest_hyp[0]['yseq'][1:-1] for nbest_hyp in nbest_hyps]",
            "for i, y_hat in enumerate(y_hats):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 5,
        "minus_line": 4,
        "AST_diff_line": 17,
        "number": 4330,
        "neg_line": [
            "-nbest_hyps = self.dec.recognize_beam_batch(hs_pad, torch.tensor(hlens), lpz,",
            "-self.recog_args, self.char_list,",
            "-self.rnnlm,",
            "-tgt_lang_ids=tgt_lang_ids.squeeze(1).tolist() if self.replace_sos else None)"
        ],
        "pos_line": [
            "+nbest_hyps = self.dec.recognize_beam_batch(",
            "+hs_pad, torch.tensor(hlens), lpz,",
            "+self.recog_args, self.char_list,",
            "+self.rnnlm,",
            "+tgt_lang_ids=tgt_lang_ids.squeeze(1).tolist() if self.replace_sos else None)"
        ],
        "core_change": "-nbest_hyps = self.dec.recognize_beam_batch(hs_pad, torch.tensor(hlens), lpz, -self.recog_args, self.char_list, -self.rnnlm, -tgt_lang_ids=tgt_lang_ids.squeeze(1).tolist() if self.replace_sos else None) +nbest_hyps = self.dec.recognize_beam_batch( +hs_pad, torch.tensor(hlens), lpz, +self.recog_args, self.char_list, +self.rnnlm, +tgt_lang_ids=tgt_lang_ids.squeeze(1).tolist() if self.replace_sos else None)",
        "core_API": "recognize_beam_batch"
    },
    {
        "commit_hash": "7732d0fe7a759c9844215920e9f1c5540eafb1a6",
        "index": "3d8ec1bc5..27d4cf178 100644",
        "commit_message": "Upgrade black to version ~=22.0 (#15565)\n\n* Upgrade black to version ~=22.0\n\n* Check copies\n\n* Fix code\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def _calculate_expected_result(",
            "# PyTorch does not currently support Huber loss with custom delta so we define it ourself",
            "def huber_loss(input, target, delta: float = 1.0):",
            "errors = torch.abs(input - target)  # shape (batch_size,)",
            "-    return torch.where(errors < delta, 0.5 * errors ** 2, errors * delta - (0.5 * delta ** 2))",
            "+    return torch.where(errors < delta, 0.5 * errors**2, errors * delta - (0.5 * delta**2))",
            "",
            "",
            "def _calculate_regression_loss("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 4331,
        "neg_line": [
            "-return torch.where(errors < delta, 0.5 * errors ** 2, errors * delta - (0.5 * delta ** 2))"
        ],
        "pos_line": [
            "+return torch.where(errors < delta, 0.5 * errors**2, errors * delta - (0.5 * delta**2))"
        ],
        "core_change": "-return torch.where(errors < delta, 0.5 * errors ** 2, errors * delta - (0.5 * delta ** 2)) +return torch.where(errors < delta, 0.5 * errors**2, errors * delta - (0.5 * delta**2))",
        "core_API": "abs"
    },
    {
        "commit_hash": "de36e0426d4f01216653979af4beb0481a1b2735",
        "index": "8ed6ca9c..cc3ea89b 100644",
        "commit_message": "update deprecated tensorflow casting (#12367)\n\n* update deprecated tensorflow casting\n\n* Fix PEP8\n\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def ctc_decode(y_pred, input_length, greedy=True, beam_width=100,",
            "the log probability of each decoded sequence.",
            "\"\"\"",
            "y_pred = tf.log(tf.transpose(y_pred, perm=[1, 0, 2]) + epsilon())",
            "-    input_length = tf.to_int32(input_length)",
            "+    input_length = tf.cast(input_length, tf.int32)",
            "",
            "if greedy:",
            "(decoded, log_prob) = ctc.ctc_greedy_decoder("
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=to_int32), value='cast')",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=2110775)",
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=3, insert_id=2110776)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2110777)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2110778)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'int32'), position=2, insert_id=2110779)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 4332,
        "neg_line": [
            "-input_length = tf.to_int32(input_length)"
        ],
        "pos_line": [
            "+input_length = tf.cast(input_length, tf.int32)"
        ],
        "core_change": "-input_length = tf.to_int32(input_length) +input_length = tf.cast(input_length, tf.int32)",
        "core_API": "log"
    },
    {
        "commit_hash": "327245c2bddd1038b2436f7ab17f8fe66e53a664",
        "index": "de3a9c8..05e1023 100644",
        "commit_message": "fix indentation\n\n",
        "file": "darkflow.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class local(BaseOp):",
            "tf.nn.conv2d(tij, kij,",
            "padding = 'VALID',",
            "strides = [1] * 4))",
            "-                out += [tf.concat(row_i, 2)]",
            "+            out += [tf.concat(row_i, 2)]",
            "",
            "-            self.out = tf.concat(out, 1)",
            "+        self.out = tf.concat(out, 1)",
            "",
            "def speak(self):",
            "l = self.lay"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 4334,
        "neg_line": [
            "-out += [tf.concat(row_i, 2)]",
            "-self.out = tf.concat(out, 1)"
        ],
        "pos_line": [
            "+out += [tf.concat(row_i, 2)]",
            "+self.out = tf.concat(out, 1)"
        ],
        "core_change": "-out += [tf.concat(row_i, 2)] +out += [tf.concat(row_i, 2)] -self.out = tf.concat(out, 1) +self.out = tf.concat(out, 1)",
        "core_API": "conv2d"
    },
    {
        "commit_hash": "2563fb873eaaa54f6539012d1c7e9fad12fb474b",
        "index": "5b372338..c18a5e98 100644",
        "commit_message": "Fixed postnet for GST.\n\n",
        "file": "TTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class TacotronGST(nn.Module):",
            "forward_attn, trans_agent, forward_attn_mask,",
            "location_attn, separate_stopnet)",
            "self.postnet = PostCBHG(mel_dim)",
            "-        self.last_linear = nn.Sequential(",
            "-            nn.Linear(self.postnet.cbhg.gru_features * 2, linear_dim),",
            "-            nn.Sigmoid())",
            "+        self.last_linear = nn.Linear(self.postnet.cbhg.gru_features * 2, linear_dim)",
            "+",
            "",
            "def forward(self, characters, text_lengths, mel_specs, speaker_ids=None):",
            "B = characters.size(0)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=nn))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=Sequential))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=nn))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=Sigmoid))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 16,
        "number": 4335,
        "neg_line": [
            "-self.last_linear = nn.Sequential(",
            "-nn.Linear(self.postnet.cbhg.gru_features * 2, linear_dim),",
            "-nn.Sigmoid())"
        ],
        "pos_line": [
            "+self.last_linear = nn.Linear(self.postnet.cbhg.gru_features * 2, linear_dim)",
            "+"
        ],
        "core_change": "-self.last_linear = nn.Sequential( -nn.Linear(self.postnet.cbhg.gru_features * 2, linear_dim), -nn.Sigmoid()) +self.last_linear = nn.Linear(self.postnet.cbhg.gru_features * 2, linear_dim) +",
        "core_API": "Sequential"
    },
    {
        "commit_hash": "1ed2ebf60d87ef12bd063c7c58e484e19189c754",
        "index": "32902fa8e..2e2e3f3a6 100755",
        "commit_message": "[style] consistent nn. and nn.functional (#12124)\n\n* consistent nn. and nn.functional\n\n* fix glitch\n\n* fix glitch #2\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "def set_model_weights_in_torch(weights, torch_model, hidden_size):",
            "assert (",
            "position_embeddings.weights[emb_idx].shape == emb_weights.shape",
            "), f\"{position_embeddings[emb_idx]} emb does not match\"",
            "-            position_embeddings.weights[emb_idx] = torch.nn.Parameter(torch.tensor(emb_weights))",
            "+            position_embeddings.weights[emb_idx] = nn.Parameter(torch.tensor(emb_weights))",
            "",
            "trax_layer_weights = weights[5]",
            "assert len(torch_model_reformer.encoder.layers) * 4 == len("
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 4337,
        "neg_line": [
            "-position_embeddings.weights[emb_idx] = torch.nn.Parameter(torch.tensor(emb_weights))"
        ],
        "pos_line": [
            "+position_embeddings.weights[emb_idx] = nn.Parameter(torch.tensor(emb_weights))"
        ],
        "core_change": "-position_embeddings.weights[emb_idx] = torch.nn.Parameter(torch.tensor(emb_weights)) +position_embeddings.weights[emb_idx] = nn.Parameter(torch.tensor(emb_weights))",
        "core_API": "Parameter"
    },
    {
        "commit_hash": "5b2af5f7cd64199297946be0c1e69e756305bca5",
        "index": "75541e8d..36e443c5 100644",
        "commit_message": "`HeteroData` support for `Batch.from_datalist`  (#3080)\n\n* new dataloader\n\n* fix test and linting\n\n* update\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def split(data, batch):",
            "if data.x is not None:",
            "slices['x'] = node_slice",
            "else:",
            "-        data.num_nodes = torch.bincount(batch).tolist()",
            "-        slices['num_nodes'] = torch.arange(len(data.num_nodes) + 1)",
            "+        # Imitate `collate` functionality:",
            "+        data._num_nodes = torch.bincount(batch).tolist()",
            "+        data.num_nodes = batch.numel()",
            "if data.edge_attr is not None:",
            "slices['edge_attr'] = edge_slice",
            "if data.y is not None:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=ASTNode(type=type), node=('attribute', None), position=0, insert_id=1000038)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=argument_list), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'data'), position=0, insert_id=1000039)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1000040)",
            "Insert(target_node=IN(type=attribute), node=('identifier', '_num_nodes'), position=2, insert_id=1000041)",
            "Update(target_node=ASTNode(type=identifier, text=torch), value='batch')",
            "Update(target_node=ASTNode(type=identifier, text=arange), value='numel')",
            "Delete(target_node=ASTNode(type=identifier, text=slices))",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=string, text='num_nodes'))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=len))",
            "Delete(target_node=ASTNode(type=identifier, text=data))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=num_nodes))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=+, text=+))",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=binary_operator))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 3,
        "minus_line": 2,
        "AST_diff_line": 25,
        "number": 4340,
        "neg_line": [
            "-data.num_nodes = torch.bincount(batch).tolist()",
            "-slices['num_nodes'] = torch.arange(len(data.num_nodes) + 1)"
        ],
        "pos_line": [
            "+# Imitate `collate` functionality:",
            "+data._num_nodes = torch.bincount(batch).tolist()",
            "+data.num_nodes = batch.numel()"
        ],
        "core_change": "-data.num_nodes = torch.bincount(batch).tolist() -slices['num_nodes'] = torch.arange(len(data.num_nodes) + 1) +# Imitate `collate` functionality: +data._num_nodes = torch.bincount(batch).tolist() +data.num_nodes = batch.numel()",
        "core_API": "bincount"
    },
    {
        "commit_hash": "6d6c8a678932c01854e456924e21e273d7bd607d",
        "index": "896aa5a..6fd50fb 100644",
        "commit_message": "densenet fixes\n\n",
        "file": "tflearn.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def densenet_block(incoming, nb_layers, growth, bottleneck=True,",
            "\"\"\"",
            "densenet = incoming",
            "",
            "-    with tf.variable_scope(scope, default_name=name, values=[incoming],",
            "-                           reuse=reuse) as scope:",
            "+    for i in range(nb_layers):",
            "",
            "-        for i in range(nb_layers):",
            "+        with tf.variable_scope(scope, default_name=name, values=[incoming],",
            "+                               reuse=reuse) as scope:",
            "",
            "# Identity",
            "conn = densenet"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('for_statement', None), position=2, insert_id=2350101)",
            "Insert(target_node=IN(type=for_statement), node=('for', 'for'), position=0, insert_id=2350102)",
            "Insert(target_node=IN(type=for_statement), node=('identifier', 'i'), position=1, insert_id=2350103)",
            "Insert(target_node=IN(type=for_statement), node=('in', 'in'), position=2, insert_id=2350104)",
            "Move(target_node=IN(type=for_statement), node=ASTNode(type=call), position=3)",
            "Insert(target_node=IN(type=for_statement), node=(':', ':'), position=4, insert_id=2350105)",
            "Insert(target_node=IN(type=for_statement), node=('block', None), position=5, insert_id=2350106)",
            "Move(target_node=IN(type=block), node=ASTNode(type=with_statement), position=0)",
            "Move(target_node=ASTNode(type=with_statement), node=ASTNode(type=block, text=), position=3)",
            "Delete(target_node=ASTNode(type=for, text=for))",
            "Delete(target_node=ASTNode(type=identifier, text=i))",
            "Delete(target_node=ASTNode(type=in, text=in))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=for_statement))",
            "Delete(target_node=ASTNode(type=block))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 15,
        "number": 4344,
        "neg_line": [
            "-with tf.variable_scope(scope, default_name=name, values=[incoming],",
            "-reuse=reuse) as scope:",
            "-for i in range(nb_layers):"
        ],
        "pos_line": [
            "+for i in range(nb_layers):",
            "+with tf.variable_scope(scope, default_name=name, values=[incoming],",
            "+reuse=reuse) as scope:"
        ],
        "core_change": "-with tf.variable_scope(scope, default_name=name, values=[incoming], -reuse=reuse) as scope: +for i in range(nb_layers): -for i in range(nb_layers): +with tf.variable_scope(scope, default_name=name, values=[incoming], +reuse=reuse) as scope:",
        "core_API": "variable_scope"
    },
    {
        "commit_hash": "828ca68605cad6d62b20fdbf6903fc13ff7d66c2",
        "index": "21b61a8848..c9a0e2df9e 100644",
        "commit_message": "lint fixes (#5332)\n\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def sinc(",
            "def vorbis_window(",
            "window_length: Union[tf.Tensor, tf.Variable],",
            "*,",
            "-    dtype:Optional[tf.DType] = tf.dtypes.float32,",
            "-    out: Optional[Union[tf.Tensor, tf.Variable]] = None",
            "+    dtype: Optional[tf.DType] = tf.dtypes.float32,",
            "+    out: Optional[Union[tf.Tensor, tf.Variable]] = None,",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "-    return tf.signal.vorbis_window(",
            "-    window_length,",
            "-    dtype=dtype,",
            "-    name=None",
            "-    )",
            "\\ No newline at end of file",
            "+    return tf.signal.vorbis_window(window_length, dtype=dtype, name=None)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=module), node=ASTNode(type=return_statement), position=1)",
            "Move(target_node=ASTNode(type=function_definition), node=ASTNode(type=ERROR), position=6)",
            "Insert(target_node=ASTNode(type=function_definition), node=('block', None), position=7, insert_id=1989397)",
            "Move(target_node=ASTNode(type=return_statement), node=ASTNode(type=call), position=1)",
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=9, insert_id=1989398)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=1989399)",
            "Move(target_node=IN(type=expression_statement), node=ASTNode(type=identifier, text=file), position=0)",
            "Delete(target_node=ASTNode(type=block))"
        ],
        "plus_line": 3,
        "minus_line": 7,
        "AST_diff_line": 8,
        "number": 4345,
        "neg_line": [
            "-dtype:Optional[tf.DType] = tf.dtypes.float32,",
            "-out: Optional[Union[tf.Tensor, tf.Variable]] = None",
            "-return tf.signal.vorbis_window(",
            "-window_length,",
            "-dtype=dtype,",
            "-name=None",
            "-)"
        ],
        "pos_line": [
            "+dtype: Optional[tf.DType] = tf.dtypes.float32,",
            "+out: Optional[Union[tf.Tensor, tf.Variable]] = None,",
            "+return tf.signal.vorbis_window(window_length, dtype=dtype, name=None)"
        ],
        "core_change": "-dtype:Optional[tf.DType] = tf.dtypes.float32, -out: Optional[Union[tf.Tensor, tf.Variable]] = None +dtype: Optional[tf.DType] = tf.dtypes.float32, +out: Optional[Union[tf.Tensor, tf.Variable]] = None, -return tf.signal.vorbis_window( -window_length, -dtype=dtype, -name=None -) +return tf.signal.vorbis_window(window_length, dtype=dtype, name=None)",
        "core_API": "vorbis_window"
    },
    {
        "commit_hash": "314bdc7c14452862e4fa498e84a07c9a950b8980",
        "index": "132bc9e62..7c85c7290 100644",
        "commit_message": "fix typo in test\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class GPT2ModelLanguageGenerationTest(unittest.TestCase):",
            "@slow",
            "def test_lm_generate_distilgpt2(self):",
            "model = GPT2LMHeadModel.from_pretrained(\"distilgpt2\")",
            "-        input_ids = torch.tensor([[463, 1893]], dtype=torch.long, device=torch_device)  # The president",
            "+        input_ids = torch.tensor([[464, 1893]], dtype=torch.long, device=torch_device)  # The president",
            "expected_output_ids = [",
            "464,",
            "1893,"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=integer, text=463), value='464')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 4346,
        "neg_line": [
            "-input_ids = torch.tensor([[463, 1893]], dtype=torch.long, device=torch_device)  # The president"
        ],
        "pos_line": [
            "+input_ids = torch.tensor([[464, 1893]], dtype=torch.long, device=torch_device)  # The president"
        ],
        "core_change": "-input_ids = torch.tensor([[463, 1893]], dtype=torch.long, device=torch_device)  # The president +input_ids = torch.tensor([[464, 1893]], dtype=torch.long, device=torch_device)  # The president",
        "core_API": "from_pretrained"
    },
    {
        "commit_hash": "f21d53cfda625c8ffacf314db187d3ecd25255a8",
        "index": "af9b72ff1..51d18db00 100644",
        "commit_message": "Fixed Slice proto to allow for optional entries\n\n- Added tests for using slice on Tensors and Pandas DataFrame\n- Rebuilt protos\n- Added a new pytest category called duet for integration tests\n\n",
        "file": "PySyft.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "_RETURNTYPES = _descriptor.Descriptor(",
            "syntax=\"proto3\",",
            "extension_ranges=[],",
            "oneofs=[],",
            "-    serialized_start=122,",
            "-    serialized_end=197,",
            "+    serialized_start=83,",
            "+    serialized_end=158,",
            ")",
            "",
            "_RETURNTYPES.fields_by_name["
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=integer, text=122), value='83')",
            "Update(target_node=ASTNode(type=integer, text=197), value='158')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 4349,
        "neg_line": [
            "-serialized_start=122,",
            "-serialized_end=197,"
        ],
        "pos_line": [
            "+serialized_start=83,",
            "+serialized_end=158,"
        ],
        "core_change": "-serialized_start=122, -serialized_end=197, +serialized_start=83, +serialized_end=158,",
        "core_API": "Descriptor"
    },
    {
        "commit_hash": "151e4ab4e786b9b4b702205b5077ea2dfe67b4dd",
        "index": "1873040a8..589a065a1 100644",
        "commit_message": "Fix CTRL past\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class CTRLModel(CTRLPreTrainedModel):",
            "inputs_embeds = self.w(input_ids)",
            "# inputs_embeds = embedded.unsqueeze(0) if len(input_ids.shape)<2 else embedded",
            "seq_len = input_ids.shape[-1]",
            "-        mask = torch.triu(torch.ones(seq_len, seq_len), 1).to(inputs_embeds.device)",
            "+        mask = torch.triu(torch.ones(seq_len + past_length, seq_len + past_length), 1).to(inputs_embeds.device)",
            "",
            "inputs_embeds *= np.sqrt(self.d_model_size)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('binary_operator', None), position=1, insert_id=1245205)",
            "Insert(target_node=ASTNode(type=argument_list), node=('binary_operator', None), position=4, insert_id=1245206)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=identifier, text=seq_len), position=0)",
            "Insert(target_node=IN(type=binary_operator), node=('+', '+'), position=1, insert_id=1245207)",
            "Insert(target_node=IN(type=binary_operator), node=('identifier', 'past_length'), position=2, insert_id=1245208)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=identifier, text=seq_len), position=0)",
            "Insert(target_node=IN(type=binary_operator), node=('+', '+'), position=1, insert_id=1245209)",
            "Insert(target_node=IN(type=binary_operator), node=('identifier', 'past_length'), position=2, insert_id=1245210)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 4350,
        "neg_line": [
            "-mask = torch.triu(torch.ones(seq_len, seq_len), 1).to(inputs_embeds.device)"
        ],
        "pos_line": [
            "+mask = torch.triu(torch.ones(seq_len + past_length, seq_len + past_length), 1).to(inputs_embeds.device)"
        ],
        "core_change": "-mask = torch.triu(torch.ones(seq_len, seq_len), 1).to(inputs_embeds.device) +mask = torch.triu(torch.ones(seq_len + past_length, seq_len + past_length), 1).to(inputs_embeds.device)",
        "core_API": "w"
    },
    {
        "commit_hash": "6287c929c162a417cd5d355c9113e9908710858d",
        "index": "1cf1c242a..1731b244d 100755",
        "commit_message": "[lm examples] fix overflow in perplexity calc (#11855)\n\n* fix overflow in perplexity calc\n\n* use inf\n\n* fix\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def main():",
            "",
            "losses = torch.cat(losses)",
            "losses = losses[: len(eval_dataset)]",
            "-        perplexity = math.exp(torch.mean(losses))",
            "+        try:",
            "+            perplexity = math.exp(torch.mean(losses))",
            "+        except OverflowError:",
            "+            perplexity = float(\"inf\")",
            "",
            "logger.info(f\"epoch {epoch}: perplexity: {perplexity}\")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('try_statement', None), position=3, insert_id=1215355)",
            "Insert(target_node=IN(type=try_statement), node=('try', 'try'), position=0, insert_id=1215356)",
            "Insert(target_node=IN(type=try_statement), node=(':', ':'), position=1, insert_id=1215357)",
            "Insert(target_node=IN(type=try_statement), node=('block', None), position=2, insert_id=1215358)",
            "Insert(target_node=IN(type=try_statement), node=('except_clause', None), position=3, insert_id=1215359)",
            "Move(target_node=IN(type=block), node=ASTNode(type=expression_statement), position=0)",
            "Insert(target_node=IN(type=except_clause), node=('except', 'except'), position=0, insert_id=1215360)",
            "Insert(target_node=IN(type=except_clause), node=('identifier', 'OverflowError'), position=1, insert_id=1215361)",
            "Insert(target_node=IN(type=except_clause), node=(':', ':'), position=2, insert_id=1215362)",
            "Insert(target_node=IN(type=except_clause), node=('block', None), position=3, insert_id=1215363)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=1215364)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1215365)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'perplexity'), position=0, insert_id=1215366)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1215367)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=1215368)",
            "Insert(target_node=IN(type=call), node=('identifier', 'float'), position=0, insert_id=1215369)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1215370)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1215371)",
            "Insert(target_node=IN(type=argument_list), node=('string', '\"inf\"'), position=1, insert_id=1215372)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1215373)"
        ],
        "plus_line": 4,
        "minus_line": 1,
        "AST_diff_line": 20,
        "number": 4351,
        "neg_line": [
            "-perplexity = math.exp(torch.mean(losses))"
        ],
        "pos_line": [
            "+try:",
            "+perplexity = math.exp(torch.mean(losses))",
            "+except OverflowError:",
            "+perplexity = float(\"inf\")"
        ],
        "core_change": "-perplexity = math.exp(torch.mean(losses)) +try: +perplexity = math.exp(torch.mean(losses)) +except OverflowError: +perplexity = float(\"inf\")",
        "core_API": "cat"
    },
    {
        "commit_hash": "f6c76230938f8c5cd09ef85913a414c37862b206",
        "index": "13a023ed..be6a3275 100644",
        "commit_message": "Add Pre-commit bot fixes (#3722)\n\n* flake8\n\n* flake8\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_deep_graph_infomax():",
            "loss = model.loss(pos_z, neg_z, summary)",
            "assert 0 <= loss.item()",
            "",
            "-    acc = model.test(",
            "-        torch.ones(20, 16), torch.randint(10, (20, )), torch.ones(20, 16),",
            "-        torch.randint(10, (20, )))",
            "+    acc = model.test(torch.ones(20, 16), torch.randint(10, (20, )),",
            "+                     torch.ones(20, 16), torch.randint(10, (20, )))",
            "assert 0 <= acc and acc <= 1"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 3,
        "AST_diff_line": 17,
        "number": 4353,
        "neg_line": [
            "-acc = model.test(",
            "-torch.ones(20, 16), torch.randint(10, (20, )), torch.ones(20, 16),",
            "-torch.randint(10, (20, )))"
        ],
        "pos_line": [
            "+acc = model.test(torch.ones(20, 16), torch.randint(10, (20, )),",
            "+torch.ones(20, 16), torch.randint(10, (20, )))"
        ],
        "core_change": "-acc = model.test( -torch.ones(20, 16), torch.randint(10, (20, )), torch.ones(20, 16), -torch.randint(10, (20, ))) +acc = model.test(torch.ones(20, 16), torch.randint(10, (20, )), +torch.ones(20, 16), torch.randint(10, (20, )))",
        "core_API": "loss"
    },
    {
        "commit_hash": "fdc907675050cb9e3aa887bf89d6c92519178349",
        "index": "ddcd939d..3f1f6429 100755",
        "commit_message": "fix use of deprecated TF functions.\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Model(ModelDesc):",
            ".BatchNorm('lastbn')",
            ".apply(nonlin)",
            ".GlobalAvgPooling('gap')",
            "-                      .tf.mul(49)  # this is due to a bug in our model design",
            "+                      .tf.multiply(49)  # this is due to a bug in our model design",
            ".FullyConnected('fct', 1000)())",
            "prob = tf.nn.softmax(logits, name='output')",
            "wrong = prediction_incorrect(logits, label, 1, name='wrong-top1')"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=mul), value='multiply')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 4356,
        "neg_line": [
            "-.tf.mul(49)  # this is due to a bug in our model design"
        ],
        "pos_line": [
            "+.tf.multiply(49)  # this is due to a bug in our model design"
        ],
        "core_change": "-.tf.mul(49)  # this is due to a bug in our model design +.tf.multiply(49)  # this is due to a bug in our model design",
        "core_API": "mul"
    },
    {
        "commit_hash": "ee950b503eeed5aca3747a4bcf2a40f624b743a0",
        "index": "3a388230..09dd96e6 100755",
        "commit_message": "fixed trpo\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def shape(x, unknown=-1):",
            "",
            "",
            "def no_operation():",
            "+    # return tf.constant(value=False, dtype=tf_dtype(dtype='bool'))",
            "return identity_operation(x=tf.constant(value=False, dtype=tf_dtype(dtype='bool')))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 17,
        "number": 4358,
        "neg_line": [],
        "pos_line": [
            "+# return tf.constant(value=False, dtype=tf_dtype(dtype='bool'))"
        ],
        "core_change": "+# return tf.constant(value=False, dtype=tf_dtype(dtype='bool'))",
        "core_API": "constant"
    },
    {
        "commit_hash": "fc2d4a7736384930044fd4ad2684f47cf6602700",
        "index": "4768de47..4059564d 100644",
        "commit_message": "Fixed tests and docs (#654)\n\n* Fixed tests\n\n* Fixed doc error and warnings\n\n* Fixed lint\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Affine(nn.Module):",
            "self.translation = translation",
            "",
            "if scale_factor is None:",
            "-            scale_factor = torch.ones(batch_size)",
            "+            scale_factor = torch.ones(batch_size, 2)",
            "self.scale_factor = scale_factor",
            "",
            "self.shear = shear"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=437235)",
            "Insert(target_node=ASTNode(type=argument_list), node=('integer', '2'), position=3, insert_id=437236)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 4360,
        "neg_line": [
            "-scale_factor = torch.ones(batch_size)"
        ],
        "pos_line": [
            "+scale_factor = torch.ones(batch_size, 2)"
        ],
        "core_change": "-scale_factor = torch.ones(batch_size) +scale_factor = torch.ones(batch_size, 2)",
        "core_API": "ones"
    },
    {
        "commit_hash": "4c64d0958cd6f484f73f7434750177dc1ec7c596",
        "index": "aa1971f7..3dd0946f 100644",
        "commit_message": "Fixed confidence_penalty for newer versions of pytorch (#3156)\n\n\n",
        "file": "ludwig.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def rmspe_loss(targets: torch.Tensor, predictions: torch.Tensor) -> torch.Tensor",
            "def mean_confidence_penalty(probabilities: torch.Tensor, num_classes: int) -> torch.Tensor:",
            "max_entropy = torch.log(torch.tensor(num_classes))",
            "# clipping needed for avoiding log(0) = -inf",
            "-    entropy_per_class = torch.maximum(-probabilities * torch.log(torch.clamp(probabilities, 1e-10, 1)), 0)",
            "+    entropy_per_class, _ = torch.max(-probabilities * torch.log(torch.clamp(probabilities, 1e-10, 1)), dim=0)",
            "entropy = torch.sum(entropy_per_class, -1)",
            "penalty = (max_entropy - entropy) / max_entropy",
            "return torch.mean(penalty)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('pattern_list', None), position=0, insert_id=595198)",
            "Move(target_node=IN(type=pattern_list), node=ASTNode(type=identifier, text=entropy_per_class), position=0)",
            "Insert(target_node=IN(type=pattern_list), node=(',', ','), position=1, insert_id=595199)",
            "Insert(target_node=IN(type=pattern_list), node=('identifier', '_'), position=2, insert_id=595200)",
            "Update(target_node=ASTNode(type=identifier, text=maximum), value='max')",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=595201)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dim'), position=0, insert_id=595202)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=595203)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=integer, text=0), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 4363,
        "neg_line": [
            "-entropy_per_class = torch.maximum(-probabilities * torch.log(torch.clamp(probabilities, 1e-10, 1)), 0)"
        ],
        "pos_line": [
            "+entropy_per_class, _ = torch.max(-probabilities * torch.log(torch.clamp(probabilities, 1e-10, 1)), dim=0)"
        ],
        "core_change": "-entropy_per_class = torch.maximum(-probabilities * torch.log(torch.clamp(probabilities, 1e-10, 1)), 0) +entropy_per_class, _ = torch.max(-probabilities * torch.log(torch.clamp(probabilities, 1e-10, 1)), dim=0)",
        "core_API": "log"
    },
    {
        "commit_hash": "dc1b762f61900130501cf0336b1acaa9d75aa643",
        "index": "8497194f..8865ee2e 100644",
        "commit_message": "More clarification fixes for update_freq documentation\n\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class DistributedTrainingTest(tf.test.TestCase):",
            "strategy.cluster_resolver",
            "and strategy.cluster_resolver.task_type == \"worker\"",
            "):",
            "-            # Workaround for an issue with",
            "-            # `tf.distribute.MultiWorkerMirroredStrategy`",
            "+            # The below assertion is run by both chief and workers when using",
            "+            # `tf.distribute.MultiWorkerMirroredStrategy`, but only the chief",
            "+            # will log events.",
            "events_expected = []",
            "",
            "self.assertEqual(events_got, events_expected)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 3,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 4367,
        "neg_line": [
            "-# Workaround for an issue with",
            "-# `tf.distribute.MultiWorkerMirroredStrategy`"
        ],
        "pos_line": [
            "+# The below assertion is run by both chief and workers when using",
            "+# `tf.distribute.MultiWorkerMirroredStrategy`, but only the chief",
            "+# will log events."
        ],
        "core_change": "-# Workaround for an issue with -# `tf.distribute.MultiWorkerMirroredStrategy` +# The below assertion is run by both chief and workers when using +# `tf.distribute.MultiWorkerMirroredStrategy`, but only the chief +# will log events.",
        "core_API": "assertEqual"
    },
    {
        "commit_hash": "6824f52cbfb6303aad67caf93c41cf0faa57f908",
        "index": "f08d9fe3d..c4628d864 100644",
        "commit_message": "Rename config and environment variable for in memory max size (#2454)\n\n* Rename config and env variable IN_MEMORY_MAX_SIZE\n\n* Rename also in documentation\n\n* Fix style\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "def test_is_small_dataset(",
            "dataset_size, config_max_in_memory_dataset_size, env_max_in_memory_dataset_size, monkeypatch",
            "):",
            "if config_max_in_memory_dataset_size != \"default\":",
            "-        monkeypatch.setattr(",
            "-            datasets.config, \"HF_MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES\", config_max_in_memory_dataset_size",
            "-        )",
            "+        monkeypatch.setattr(datasets.config, \"IN_MEMORY_MAX_SIZE\", config_max_in_memory_dataset_size)",
            "",
            "-    max_in_memory_dataset_size = datasets.config.HF_MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES",
            "+    max_in_memory_dataset_size = datasets.config.IN_MEMORY_MAX_SIZE",
            "if config_max_in_memory_dataset_size == \"default\":",
            "if env_max_in_memory_dataset_size:",
            "assert max_in_memory_dataset_size == env_max_in_memory_dataset_size"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=HF_MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES), value='IN_MEMORY_MAX_SIZE')",
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=1, insert_id=1785051)",
            "Update(target_node=ASTNode(type=string, text=\"HF_MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES\"), value='\"IN_MEMORY_MAX_SIZE\"')",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'datasets'), position=0, insert_id=1785052)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1785053)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'config'), position=2, insert_id=1785054)",
            "Delete(target_node=ASTNode(type=identifier, text=datasets))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=config))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 4,
        "AST_diff_line": 11,
        "number": 4370,
        "neg_line": [
            "-monkeypatch.setattr(",
            "-datasets.config, \"HF_MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES\", config_max_in_memory_dataset_size",
            "-)",
            "-max_in_memory_dataset_size = datasets.config.HF_MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES"
        ],
        "pos_line": [
            "+monkeypatch.setattr(datasets.config, \"IN_MEMORY_MAX_SIZE\", config_max_in_memory_dataset_size)",
            "+max_in_memory_dataset_size = datasets.config.IN_MEMORY_MAX_SIZE"
        ],
        "core_change": "-monkeypatch.setattr( -datasets.config, \"HF_MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES\", config_max_in_memory_dataset_size -) +monkeypatch.setattr(datasets.config, \"IN_MEMORY_MAX_SIZE\", config_max_in_memory_dataset_size) -max_in_memory_dataset_size = datasets.config.HF_MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES +max_in_memory_dataset_size = datasets.config.IN_MEMORY_MAX_SIZE",
        "core_API": "setattr"
    },
    {
        "commit_hash": "13bd6718eba69d34797f1df8f3d2ad972c959dab",
        "index": "184fb3405..dc56d539c 100644",
        "commit_message": "fix CI/PEP\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def check_batch_state(state, max_len, pad_token):",
            "",
            "",
            "def custom_torch_load(model_path, model, training=True):",
            "-    \"\"\"Custom torch loading for transducer-models modules and parameters.",
            "+    \"\"\"Load transducer model modules and parameters with training-only ones removed.",
            "",
            "Args:",
            "-        model_path (str): Model path.",
            "-        model (torch.nn.Module): The model with pretrained modules.",
            "+        model_path (str): Model path",
            "+        model (torch.nn.Module): The model with pretrained modules",
            "",
            "\"\"\"",
            "if \"snapshot\" in os.path.basename(model_path):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"\"\"Custom torch loading for transducer-models modules and parameters.\n\nArgs:\n        model_path (str): Model path.\n        model (torch.nn.Module): The model with pretrained modules.\n\n\"\"\"), value='\"\"\"Load transducer model modules and parameters with training-only ones removed.\\n\\nArgs:\\n        model_path (str): Model path\\n        model (torch.nn.Module): The model with pretrained modules\\n\\n\"\"\"')"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 1,
        "number": 4372,
        "neg_line": [
            "-\"\"\"Custom torch loading for transducer-models modules and parameters.",
            "-model_path (str): Model path.",
            "-model (torch.nn.Module): The model with pretrained modules."
        ],
        "pos_line": [
            "+\"\"\"Load transducer model modules and parameters with training-only ones removed.",
            "+model_path (str): Model path",
            "+model (torch.nn.Module): The model with pretrained modules"
        ],
        "core_change": "-\"\"\"Custom torch loading for transducer-models modules and parameters. +\"\"\"Load transducer model modules and parameters with training-only ones removed. -model_path (str): Model path. -model (torch.nn.Module): The model with pretrained modules. +model_path (str): Model path +model (torch.nn.Module): The model with pretrained modules",
        "core_API": "basename"
    },
    {
        "commit_hash": "8d8677e03b5ab7cd65d965d187438a79cdfb350b",
        "index": "f5c9472..d783532 100644",
        "commit_message": "Fix #139. Broken SKResNets after BlurPool addition, as a plus, SKResNets support AA now too.\n\n",
        "file": "pytorch-image-models.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class ConvBnAct(nn.Module):",
            "x = self.drop_block(x)",
            "if self.act is not None:",
            "x = self.act(x)",
            "+        if self.aa is not None:",
            "+            x = self.aa(x)",
            "return x"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=4, insert_id=1481771)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=1481772)",
            "Insert(target_node=IN(type=if_statement), node=('comparison_operator', None), position=1, insert_id=1481773)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=1481774)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=1481775)",
            "Insert(target_node=IN(type=comparison_operator), node=('attribute', None), position=0, insert_id=1481776)",
            "Insert(target_node=IN(type=comparison_operator), node=('is not', 'is'), position=1, insert_id=1481777)",
            "Insert(target_node=IN(type=comparison_operator), node=('is not', 'not'), position=2, insert_id=1481778)",
            "Insert(target_node=IN(type=comparison_operator), node=('none', 'None'), position=3, insert_id=1481779)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=1481780)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=1481781)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1481782)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'aa'), position=2, insert_id=1481783)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1481784)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'x'), position=0, insert_id=1481785)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1481786)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=1481787)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1481788)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1481789)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=1481790)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1481791)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'aa'), position=2, insert_id=1481792)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1481793)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'x'), position=1, insert_id=1481794)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1481795)"
        ],
        "plus_line": 2,
        "minus_line": 0,
        "AST_diff_line": 25,
        "number": 4374,
        "neg_line": [],
        "pos_line": [
            "+if self.aa is not None:",
            "+x = self.aa(x)"
        ],
        "core_change": "+if self.aa is not None: +x = self.aa(x)",
        "core_API": "drop_block"
    },
    {
        "commit_hash": "3dda5ac9fcf390d8b83eb855249360711707c11c",
        "index": "c5d02dba..3de36967 100644",
        "commit_message": "for discussion: incorporate black code formatter (#3308)\n\n* setup files\n\n* run black\n\n* undo\n\n* update CONTRIBUTING.md\n\n* fix quotes in test_other_modules\n\n* make flake8 happy\n\n* set black to 100 characters per line\n\n* move type: ignore to where mypy wants them\n\n* more flake8\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class PearsonCorrelation(Metric):",
            "labels_variance = self._labels_variance.get_metric(reset=reset)",
            "if reset:",
            "self.reset()",
            "-        denominator = (math.sqrt(predictions_variance) * math.sqrt(labels_variance))",
            "+        denominator = math.sqrt(predictions_variance) * math.sqrt(labels_variance)",
            "if np.around(denominator, decimals=5) == 0:",
            "pearson_r = 0",
            "else:"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=binary_operator), position=2)",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=parenthesized_expression))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 4376,
        "neg_line": [
            "-denominator = (math.sqrt(predictions_variance) * math.sqrt(labels_variance))"
        ],
        "pos_line": [
            "+denominator = math.sqrt(predictions_variance) * math.sqrt(labels_variance)"
        ],
        "core_change": "-denominator = (math.sqrt(predictions_variance) * math.sqrt(labels_variance)) +denominator = math.sqrt(predictions_variance) * math.sqrt(labels_variance)",
        "core_API": "get_metric"
    },
    {
        "commit_hash": "b338596346469676c59daa7b74d98a08d3e5f046",
        "index": "7d493c57c..2b24fd2ae 100644",
        "commit_message": "Fixing image segmentation with inference mode. (#14204)\n\n* Fixing image segmentation for inference mode.\n\n* Update src/transformers/pipelines/base.py\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ImageSegmentationPipeline(Pipeline):",
            "",
            "return super().__call__(*args, **kwargs)",
            "",
            "+    def get_inference_context(self):",
            "+        return torch.no_grad",
            "+",
            "def preprocess(self, image):",
            "image = self.load_image(image)",
            "target_size = torch.IntTensor([[image.height, image.width]])"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('function_definition', None), position=2, insert_id=1209901)",
            "Insert(target_node=IN(type=function_definition), node=('def', 'def'), position=0, insert_id=1209902)",
            "Insert(target_node=IN(type=function_definition), node=('identifier', 'get_inference_context'), position=1, insert_id=1209903)",
            "Insert(target_node=IN(type=function_definition), node=('parameters', None), position=2, insert_id=1209904)",
            "Insert(target_node=IN(type=function_definition), node=(':', ':'), position=3, insert_id=1209905)",
            "Insert(target_node=IN(type=function_definition), node=('block', None), position=4, insert_id=1209906)",
            "Insert(target_node=IN(type=parameters), node=('(', '('), position=0, insert_id=1209907)",
            "Insert(target_node=IN(type=parameters), node=('identifier', 'self'), position=1, insert_id=1209908)",
            "Insert(target_node=IN(type=parameters), node=(')', ')'), position=2, insert_id=1209909)",
            "Insert(target_node=IN(type=block), node=('return_statement', None), position=0, insert_id=1209910)",
            "Insert(target_node=IN(type=return_statement), node=('return', 'return'), position=0, insert_id=1209911)",
            "Insert(target_node=IN(type=return_statement), node=('attribute', None), position=1, insert_id=1209912)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1209913)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1209914)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'no_grad'), position=2, insert_id=1209915)"
        ],
        "plus_line": 2,
        "minus_line": 0,
        "AST_diff_line": 15,
        "number": 4377,
        "neg_line": [],
        "pos_line": [
            "+def get_inference_context(self):",
            "+return torch.no_grad",
            "+"
        ],
        "core_change": "+def get_inference_context(self): +return torch.no_grad +",
        "core_API": "load_image"
    },
    {
        "commit_hash": "d8fb7fdbe4e546db9c6236478819ef3e6f831914",
        "index": "f6549784..ff6c2431 100644",
        "commit_message": "[Docs] Improve docs for geometry transform (#913)\n\n* fixes linter in clahe transpose\n\n* add tolerance in add_weighted jit test\n\n* update exclude in github matrix\n\n* fix docs in camera matrix shape for unproject_points\n\n* fix transpose in clahe\n\n* reorganize geometry transform docs\n\n* improve geometry documentation links\n\n* improve image transforms docs\n\n* fix mypy in affine3d\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def _compute_equalized_tiles(interp_tiles: torch.Tensor, luts: torch.Tensor) ->",
            "tiles_equalized: torch.Tensor = torch.zeros_like(interp_tiles, dtype=torch.long)",
            "",
            "# compute the interpolation weights (shapes are 2 x TH x TW because they must be applied to 2 interp tiles)",
            "-    ih = torch.arange(2 * th - 1, -1, -1, device=interp_tiles.device).div(2. * th - 1)[None].T.expand(2 * th, tw)",
            "+    ih = torch.arange(2 * th - 1, -1, -1, device=interp_tiles.device).div(",
            "+        2. * th - 1)[None].transpose(-2, -1).expand(2 * th, tw)",
            "ih = ih.unfold(0, th, th).unfold(1, tw, tw)  # 2 x 1 x TH x TW",
            "iw = torch.arange(2 * tw - 1, -1, -1, device=interp_tiles.device).div(2. * tw - 1).expand(th, 2 * tw)",
            "iw = iw.unfold(0, th, th).unfold(1, tw, tw)  # 1 x 2 x TH x TW"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('call', None), position=0, insert_id=429497)",
            "Move(target_node=IN(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=429498)",
            "Update(target_node=ASTNode(type=identifier, text=T), value='transpose')",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=429499)",
            "Insert(target_node=IN(type=argument_list), node=('unary_operator', '-2'), position=1, insert_id=429500)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=429501)",
            "Insert(target_node=IN(type=argument_list), node=('unary_operator', '-1'), position=3, insert_id=429502)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=429503)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 4378,
        "neg_line": [
            "-ih = torch.arange(2 * th - 1, -1, -1, device=interp_tiles.device).div(2. * th - 1)[None].T.expand(2 * th, tw)"
        ],
        "pos_line": [
            "+ih = torch.arange(2 * th - 1, -1, -1, device=interp_tiles.device).div(",
            "+2. * th - 1)[None].transpose(-2, -1).expand(2 * th, tw)"
        ],
        "core_change": "-ih = torch.arange(2 * th - 1, -1, -1, device=interp_tiles.device).div(2. * th - 1)[None].T.expand(2 * th, tw) +ih = torch.arange(2 * th - 1, -1, -1, device=interp_tiles.device).div( +2. * th - 1)[None].transpose(-2, -1).expand(2 * th, tw)",
        "core_API": "zeros_like"
    },
    {
        "commit_hash": "47ec97ebd975efc396ff7957712511acdf5b2058",
        "index": "651b13bb..f803592e 100755",
        "commit_message": "fix assert message (#1040)\n\n* fix assert\n\nThe current assert \"Model must initialized in fp16 mode for ZeRO Stage 3.\" needs TLC - I rewrote it completely to match its cousen assert, so now we have 2 consistent matching asserts:\n\n- f\"fp16 is enabled but one or several model parameters have dtype that is not fp16\"\n- f\"fp16 is not enabled but one or several model parameters have dtype of fp16\"\n\n* remove f\n",
        "file": "DeepSpeed.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DeepSpeedEngine(Module):",
            "if self.zero_optimization_partition_weights() and any(",
            "[hasattr(param,",
            "'ds_id') for param in self.module.parameters()]):",
            "-                assert all([param.dtype == torch.half for param in self.module.parameters()]), f\"Model must initialized in fp16 mode for ZeRO Stage 3.\"",
            "+                assert all([param.dtype == torch.half for param in self.module.parameters()]), \"fp16 is enabled but one or several model parameters have dtype that is not fp16\"",
            "self.module.half()",
            "else:",
            "-            assert all([param.dtype == torch.float for param in self.module.parameters()]), f\"fp16 is not enabled but one or several model parameters have dtype of fp16\"",
            "+            assert all([param.dtype == torch.float for param in self.module.parameters()]), \"fp16 is not enabled but one or several model parameters have dtype of fp16\"",
            "",
            "if not self.dont_change_device:",
            "self.module.to(self.device)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=f\"Model must initialized in fp16 mode for ZeRO Stage 3.\"), value='\"fp16 is enabled but one or several model parameters have dtype that is not fp16\"')",
            "Update(target_node=ASTNode(type=string, text=f\"fp16 is not enabled but one or several model parameters have dtype of fp16\"), value='\"fp16 is not enabled but one or several model parameters have dtype of fp16\"')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 4381,
        "neg_line": [
            "-assert all([param.dtype == torch.half for param in self.module.parameters()]), f\"Model must initialized in fp16 mode for ZeRO Stage 3.\"",
            "-assert all([param.dtype == torch.float for param in self.module.parameters()]), f\"fp16 is not enabled but one or several model parameters have dtype of fp16\""
        ],
        "pos_line": [
            "+assert all([param.dtype == torch.half for param in self.module.parameters()]), \"fp16 is enabled but one or several model parameters have dtype that is not fp16\"",
            "+assert all([param.dtype == torch.float for param in self.module.parameters()]), \"fp16 is not enabled but one or several model parameters have dtype of fp16\""
        ],
        "core_change": "-assert all([param.dtype == torch.half for param in self.module.parameters()]), f\"Model must initialized in fp16 mode for ZeRO Stage 3.\" +assert all([param.dtype == torch.half for param in self.module.parameters()]), \"fp16 is enabled but one or several model parameters have dtype that is not fp16\" -assert all([param.dtype == torch.float for param in self.module.parameters()]), f\"fp16 is not enabled but one or several model parameters have dtype of fp16\" +assert all([param.dtype == torch.float for param in self.module.parameters()]), \"fp16 is not enabled but one or several model parameters have dtype of fp16\"",
        "core_API": "zero_optimization_partition_weights"
    },
    {
        "commit_hash": "2ab8b841deb8bb85c5c1acfe02b4d5be47520b69",
        "index": "b62d64563..67900bd77 100644",
        "commit_message": "fix pytorch RNNLM, and chime5 recipt to remove long/short utt\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Decoder(torch.nn.Module):",
            "local_best_scores, joint_best_ids = torch.topk(local_scores, beam, dim=1)",
            "local_best_ids = local_best_ids[:, joint_best_ids[0]]",
            "else:",
            "-                    local_best_scores, local_best_ids = torch.topk(local_att_scores, beam, dim=1)",
            "+                    local_best_scores, local_best_ids = torch.topk(local_scores, beam, dim=1)",
            "",
            "for j in six.moves.range(beam):",
            "new_hyp = {}"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=local_att_scores), value='local_scores')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 4387,
        "neg_line": [
            "-local_best_scores, local_best_ids = torch.topk(local_att_scores, beam, dim=1)"
        ],
        "pos_line": [
            "+local_best_scores, local_best_ids = torch.topk(local_scores, beam, dim=1)"
        ],
        "core_change": "-local_best_scores, local_best_ids = torch.topk(local_att_scores, beam, dim=1) +local_best_scores, local_best_ids = torch.topk(local_scores, beam, dim=1)",
        "core_API": "topk"
    },
    {
        "commit_hash": "14c9e9cde997e83a68f4852423d416f95f3cf513",
        "index": "3e8376a4..3fbd3141 100644",
        "commit_message": "Loss bug fix - target_flat vs target\n\n",
        "file": "TTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "functional",
        "change": [
            "class L1LossMasked(nn.Module):",
            "# target_flat: (batch * max_len, dim)",
            "target_flat = target.view(-1, target.shape[-1])",
            "# losses_flat: (batch * max_len, dim)",
            "-        losses_flat = functional.l1_loss(input, target, size_average=False,",
            "+        losses_flat = functional.l1_loss(input, target_flat, size_average=False,",
            "reduce=False)",
            "# losses: (batch, max_len, dim)",
            "losses = losses_flat.view(*target.size())"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=target), value='target_flat')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 4391,
        "neg_line": [
            "-losses_flat = functional.l1_loss(input, target, size_average=False,"
        ],
        "pos_line": [
            "+losses_flat = functional.l1_loss(input, target_flat, size_average=False,"
        ],
        "core_change": "-losses_flat = functional.l1_loss(input, target, size_average=False, +losses_flat = functional.l1_loss(input, target_flat, size_average=False,",
        "core_API": "view"
    },
    {
        "commit_hash": "2d54116baa1284057d68bde52daf49ee19c5f899",
        "index": "e1ba601b5..ffada8840 100644",
        "commit_message": "annotat unused vars (#5017)\n\n* annotate all unused vars\n\n* rank_zero_warn\n\n* Apply suggestions from code review\n\nCo-authored-by: Carlos Mochol <carlossmocholi@gmail.com>\n\n* f1 fixed\n\nCo-authored-by: Carlos Mochol <carlossmocholi@gmail.com>\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def stat_scores_multiple_classes(",
            "",
            "tps = torch.zeros((num_classes + 1,), device=pred.device)",
            "fps = torch.zeros((num_classes + 1,), device=pred.device)",
            "-        tns = torch.zeros((num_classes + 1,), device=pred.device)",
            "fns = torch.zeros((num_classes + 1,), device=pred.device)",
            "sups = torch.zeros((num_classes + 1,), device=pred.device)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=tns))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=zeros))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 4393,
        "neg_line": [
            "-tns = torch.zeros((num_classes + 1,), device=pred.device)"
        ],
        "pos_line": [],
        "core_change": "-tns = torch.zeros((num_classes + 1,), device=pred.device)",
        "core_API": "zeros"
    },
    {
        "commit_hash": "99a7d749961fc6e2bb529d97a9e505ba1816bcca",
        "index": "df96afee..a18292dd 100755",
        "commit_message": "misc small changes and fix #688\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "if __name__ == '__main__':",
            "parser.add_argument(dest='output', help='output model file, can be npz or TF checkpoint')",
            "args = parser.parse_args()",
            "",
            "-    tf.train.import_meta_graph(args.meta)",
            "+    tf.train.import_meta_graph(args.meta, clear_devices=True)",
            "",
            "# loading...",
            "init = get_model_loader(args.input)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=2288732)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=2288733)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'clear_devices'), position=0, insert_id=2288734)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2288735)",
            "Insert(target_node=IN(type=keyword_argument), node=('true', 'True'), position=2, insert_id=2288736)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 4396,
        "neg_line": [
            "-tf.train.import_meta_graph(args.meta)"
        ],
        "pos_line": [
            "+tf.train.import_meta_graph(args.meta, clear_devices=True)"
        ],
        "core_change": "-tf.train.import_meta_graph(args.meta) +tf.train.import_meta_graph(args.meta, clear_devices=True)",
        "core_API": "add_argument"
    },
    {
        "commit_hash": "f86dd55145eaa6ea81903f43efef7485f250ce8b",
        "index": "d2be9894a..ef056f88a 100644",
        "commit_message": "fixes tpu data loader bug (#957)\n\n* fixes tpu data loader bug\n\n* fixes tpu data loader bug\n\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "dataloader",
        "change": [
            "class TrainerTrainLoopMixin(ABC):",
            "if self.reload_dataloaders_every_epoch:",
            "self.reset_train_dataloader(self.get_model())",
            "",
            "+        # track local dataloader so TPU can wrap each epoch",
            "+        train_dataloader = self.train_dataloader",
            "+",
            "# on TPU we have to wrap it under the ParallelLoader",
            "if self.use_tpu:",
            "device = xm.xla_device()",
            "-            self.train_dataloader = xla_pl.ParallelLoader(self.train_dataloader, [device])",
            "-            self.train_dataloader = self.train_dataloader.per_device_loader(device)",
            "+            train_dataloader = xla_pl.ParallelLoader(train_dataloader, [device])",
            "+            train_dataloader = train_dataloader.per_device_loader(device)",
            "",
            "# run epoch",
            "for batch_idx, batch in self.profiler.profile_iterable(",
            "-            enumerate(self.train_dataloader), \"get_train_batch\"",
            "+            enumerate(train_dataloader), \"get_train_batch\"",
            "):",
            "# stop epoch if we limited the number of training batches",
            "if batch_idx >= self.num_training_batches:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=1771595)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1771596)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'train_dataloader'), position=0, insert_id=1771597)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1771598)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=attribute), position=2)",
            "Insert(target_node=ASTNode(type=assignment), node=('identifier', 'train_dataloader'), position=0, insert_id=1771599)",
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=identifier, text=train_dataloader), position=0)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=train_dataloader), position=1)",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=train_dataloader), position=0)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=train_dataloader), position=1)",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 5,
        "minus_line": 3,
        "AST_diff_line": 22,
        "number": 4397,
        "neg_line": [
            "-self.train_dataloader = xla_pl.ParallelLoader(self.train_dataloader, [device])",
            "-self.train_dataloader = self.train_dataloader.per_device_loader(device)",
            "-enumerate(self.train_dataloader), \"get_train_batch\""
        ],
        "pos_line": [
            "+# track local dataloader so TPU can wrap each epoch",
            "+train_dataloader = self.train_dataloader",
            "+",
            "+train_dataloader = xla_pl.ParallelLoader(train_dataloader, [device])",
            "+train_dataloader = train_dataloader.per_device_loader(device)",
            "+enumerate(train_dataloader), \"get_train_batch\""
        ],
        "core_change": "+# track local dataloader so TPU can wrap each epoch +train_dataloader = self.train_dataloader + -self.train_dataloader = xla_pl.ParallelLoader(self.train_dataloader, [device]) -self.train_dataloader = self.train_dataloader.per_device_loader(device) +train_dataloader = xla_pl.ParallelLoader(train_dataloader, [device]) +train_dataloader = train_dataloader.per_device_loader(device) -enumerate(self.train_dataloader), \"get_train_batch\" +enumerate(train_dataloader), \"get_train_batch\"",
        "core_API": "reset_train_dataloader"
    },
    {
        "commit_hash": "de4159a318f9a4f2f474733306c00ac326e643a7",
        "index": "5ccd6c198..551e7a513 100644",
        "commit_message": "More TF int dtype fixes (#20384)\n\n* Add a test to ensure int dummy inputs are int64\n\n* Move the test into the existing int64 test and update a lot of existing dummies\n\n* Fix remaining dummies\n\n* Fix remaining dummies\n\n* Test for int64 serving sigs as well\n\n* Update core tests to use tf.int64\n\n* Add better messages to the assertions\n\n* Update all serving sigs to int64\n\n* More sneaky hiding tf.int32s\n\n* Add an optional int32 signature in save_pretrained\n\n* make fixup\n\n* Add Amy's suggestions\n\n* Switch all serving sigs back to tf.int32\n\n* Switch all dummies to tf.int32\n\n* Adjust tests to check for tf.int32 instead of tf.int64\n\n* Fix base dummy_inputs dtype\n\n* Start casting to tf.int32 in input_processing\n\n* Change dtype for unpack_inputs test\n\n* Add proper tf.int32 test\n\n* Make the alternate serving signature int64\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFGroupViTModel(TFGroupViTPreTrainedModel):",
            "@tf.function(",
            "input_signature=[",
            "{",
            "-                \"input_ids\": tf.TensorSpec((None, None), tf.int64, name=\"input_ids\"),",
            "+                \"input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"input_ids\"),",
            "\"pixel_values\": tf.TensorSpec((None, None, None, None), tf.float64, name=\"pixel_values\"),",
            "-                \"attention_mask\": tf.TensorSpec((None, None), tf.int64, name=\"attention_mask\"),",
            "+                \"attention_mask\": tf.TensorSpec((None, None), tf.int32, name=\"attention_mask\"),",
            "}",
            "]",
            ")"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2358000)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2358001)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2358002)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'TensorSpec'), position=2, insert_id=2358003)",
            "Insert(target_node=ASTNode(type=argument_list), node=('tuple', None), position=1, insert_id=2358004)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=tuple), position=1)",
            "Insert(target_node=IN(type=tuple), node=('(', '('), position=0, insert_id=2358005)",
            "Insert(target_node=IN(type=tuple), node=('none', 'None'), position=1, insert_id=2358006)",
            "Insert(target_node=IN(type=tuple), node=(',', ','), position=2, insert_id=2358007)",
            "Insert(target_node=IN(type=tuple), node=('none', 'None'), position=3, insert_id=2358008)",
            "Insert(target_node=IN(type=tuple), node=(')', ')'), position=4, insert_id=2358009)",
            "Update(target_node=ASTNode(type=identifier, text=int64), value='int32')",
            "Update(target_node=ASTNode(type=identifier, text=int64), value='int32')",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=TensorSpec))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=none, text=None))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=none, text=None))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=tuple))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 24,
        "number": 4398,
        "neg_line": [
            "-\"input_ids\": tf.TensorSpec((None, None), tf.int64, name=\"input_ids\"),",
            "-\"attention_mask\": tf.TensorSpec((None, None), tf.int64, name=\"attention_mask\"),"
        ],
        "pos_line": [
            "+\"input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"input_ids\"),",
            "+\"attention_mask\": tf.TensorSpec((None, None), tf.int32, name=\"attention_mask\"),"
        ],
        "core_change": "-\"input_ids\": tf.TensorSpec((None, None), tf.int64, name=\"input_ids\"), +\"input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"input_ids\"), -\"attention_mask\": tf.TensorSpec((None, None), tf.int64, name=\"attention_mask\"), +\"attention_mask\": tf.TensorSpec((None, None), tf.int32, name=\"attention_mask\"),",
        "core_API": "function"
    },
    {
        "commit_hash": "1a3584fcb13dcf7a4272465ba71d1c72e5b31abc",
        "index": "cbddaecdb..5a480b113 100644",
        "commit_message": "black fix\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class StochasticDurationPredictor(torch.nn.Module):",
            "z, logdet = flow(z, x_mask, g=x, inverse=inverse)",
            "logdet_tot = logdet_tot + logdet",
            "nll = (",
            "-                torch.sum(0.5 * (math.log(2 * math.pi) + (z ** 2)) * x_mask, [1, 2])",
            "+                torch.sum(0.5 * (math.log(2 * math.pi) + (z**2)) * x_mask, [1, 2])",
            "- logdet_tot",
            ")",
            "return nll + logq  # (B,)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=logdet_tot))",
            "Delete(target_node=ASTNode(type=ERROR))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 4400,
        "neg_line": [
            "-torch.sum(0.5 * (math.log(2 * math.pi) + (z ** 2)) * x_mask, [1, 2])",
            "-logdet_tot"
        ],
        "pos_line": [
            "+torch.sum(0.5 * (math.log(2 * math.pi) + (z**2)) * x_mask, [1, 2])"
        ],
        "core_change": "-torch.sum(0.5 * (math.log(2 * math.pi) + (z ** 2)) * x_mask, [1, 2]) +torch.sum(0.5 * (math.log(2 * math.pi) + (z**2)) * x_mask, [1, 2]) -logdet_tot",
        "core_API": "sum"
    },
    {
        "commit_hash": "d215aa258e77f43b6c36950bc9a484d5d849eb4f",
        "index": "b6659e8c..0dc42ec6 100644",
        "commit_message": "fix torch.nonzero usage in pytorch 1.5 (#2602)\n\n\n",
        "file": "mmdetection.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class RPNHead(AnchorHead):",
            "if cfg.min_bbox_size > 0:",
            "w = proposals[:, 2] - proposals[:, 0]",
            "h = proposals[:, 3] - proposals[:, 1]",
            "-            valid_inds = torch.nonzero((w >= cfg.min_bbox_size)",
            "-                                       & (h >= cfg.min_bbox_size)).squeeze()",
            "+            valid_inds = torch.nonzero(",
            "+                (w >= cfg.min_bbox_size)",
            "+                & (h >= cfg.min_bbox_size),",
            "+                as_tuple=False).squeeze()",
            "if valid_inds.sum().item() != len(proposals):",
            "proposals = proposals[valid_inds, :]",
            "scores = scores[valid_inds]"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=638797)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=638798)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'as_tuple'), position=0, insert_id=638799)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=638800)",
            "Insert(target_node=IN(type=keyword_argument), node=('false', 'False'), position=2, insert_id=638801)"
        ],
        "plus_line": 4,
        "minus_line": 2,
        "AST_diff_line": 5,
        "number": 4401,
        "neg_line": [
            "-valid_inds = torch.nonzero((w >= cfg.min_bbox_size)",
            "-& (h >= cfg.min_bbox_size)).squeeze()"
        ],
        "pos_line": [
            "+valid_inds = torch.nonzero(",
            "+(w >= cfg.min_bbox_size)",
            "+& (h >= cfg.min_bbox_size),",
            "+as_tuple=False).squeeze()"
        ],
        "core_change": "-valid_inds = torch.nonzero((w >= cfg.min_bbox_size) -& (h >= cfg.min_bbox_size)).squeeze() +valid_inds = torch.nonzero( +(w >= cfg.min_bbox_size) +& (h >= cfg.min_bbox_size), +as_tuple=False).squeeze()",
        "core_API": "nonzero"
    },
    {
        "commit_hash": "10c47dd8ce860826a0d85960bdcf709d437315d4",
        "index": "61cf99b7..694bd510 100644",
        "commit_message": "Fix mypy check\n\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class RandomResizedCrop(AugmentationBase):",
            "else:",
            "batch_shape = input.shape",
            "params = RandomResizedCrop.get_params(",
            "-                batch_size, batch_shape[-2:], self.size, self.scale, self.ratio)  # type: ignore",
            "+                batch_size, batch_shape[-2:], self.size, self.scale, self.ratio)",
            "return super().forward(input, params)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 4403,
        "neg_line": [
            "-batch_size, batch_shape[-2:], self.size, self.scale, self.ratio)  # type: ignore"
        ],
        "pos_line": [
            "+batch_size, batch_shape[-2:], self.size, self.scale, self.ratio)"
        ],
        "core_change": "-batch_size, batch_shape[-2:], self.size, self.scale, self.ratio)  # type: ignore +batch_size, batch_shape[-2:], self.size, self.scale, self.ratio)",
        "core_API": "get_params"
    },
    {
        "commit_hash": "4c59718f40a36cc1da102ec4d01c5ed6bbed407d",
        "index": "704f6641..65c9ecaa 100644",
        "commit_message": "fix initializer mode\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def resnet_group(l, name, block_func, features, count, stride):",
            "",
            "def resnet_backbone(image, num_blocks, group_func, block_func):",
            "with argscope(Conv2D, nl=tf.identity, use_bias=False,",
            "-                  W_init=tf.variance_scaling_initializer(scale=2.0, mode='FAN_OUT')):",
            "+                  W_init=tf.variance_scaling_initializer(scale=2.0, mode='fan_out')):",
            "logits = (LinearWrap(image)",
            ".Conv2D('conv0', 64, 7, stride=2, nl=BNReLU)",
            ".MaxPooling('pool0', shape=3, stride=2, padding='SAME')"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text='FAN_OUT'), value=\"'fan_out'\")"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 4404,
        "neg_line": [
            "-W_init=tf.variance_scaling_initializer(scale=2.0, mode='FAN_OUT')):"
        ],
        "pos_line": [
            "+W_init=tf.variance_scaling_initializer(scale=2.0, mode='fan_out')):"
        ],
        "core_change": "-W_init=tf.variance_scaling_initializer(scale=2.0, mode='FAN_OUT')): +W_init=tf.variance_scaling_initializer(scale=2.0, mode='fan_out')):",
        "core_API": "variance_scaling_initializer"
    },
    {
        "commit_hash": "a46368d016aad516ad119432e58ddbb39be2e7b9",
        "index": "6db82c06..7f1305d3 100644",
        "commit_message": "Fix LKJ sample shape bug (#2617)\n\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class LKJCorrCholesky(TorchDistribution):",
            "super().__init__(torch.Size(), torch.Size((d, d)), validate_args=validate_args)",
            "",
            "def sample(self, sample_shape=torch.Size()):",
            "-        y = self._gen.sample(sample_shape=self.batch_shape + sample_shape).detach()",
            "+        with torch.no_grad():",
            "+            y = self._gen.sample(sample_shape=sample_shape + self.batch_shape)",
            "z = y.mul(2).add(-1.0)",
            "return _vector_to_l_cholesky(z)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=block), node=('with_statement', None), position=0, insert_id=692369)",
            "Insert(target_node=IN(type=with_statement), node=('with', 'with'), position=0, insert_id=692370)",
            "Insert(target_node=IN(type=with_statement), node=('with_clause', None), position=1, insert_id=692371)",
            "Insert(target_node=IN(type=with_statement), node=(':', ':'), position=2, insert_id=692372)",
            "Insert(target_node=IN(type=with_statement), node=('block', None), position=3, insert_id=692373)",
            "Insert(target_node=IN(type=with_clause), node=('with_item', None), position=0, insert_id=692374)",
            "Move(target_node=IN(type=block), node=ASTNode(type=expression_statement), position=0)",
            "Insert(target_node=IN(type=with_item), node=('call', None), position=0, insert_id=692375)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=692376)",
            "Move(target_node=IN(type=call), node=ASTNode(type=argument_list), position=1)",
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=call), position=2)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=692377)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=692378)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'no_grad'), position=2, insert_id=692379)",
            "Insert(target_node=ASTNode(type=binary_operator), node=('identifier', 'sample_shape'), position=0, insert_id=692380)",
            "Insert(target_node=ASTNode(type=binary_operator), node=('+', '+'), position=1, insert_id=692381)",
            "Delete(target_node=ASTNode(type=+, text=+))",
            "Delete(target_node=ASTNode(type=identifier, text=sample_shape))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=detach))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 22,
        "number": 4407,
        "neg_line": [
            "-y = self._gen.sample(sample_shape=self.batch_shape + sample_shape).detach()"
        ],
        "pos_line": [
            "+with torch.no_grad():",
            "+y = self._gen.sample(sample_shape=sample_shape + self.batch_shape)"
        ],
        "core_change": "-y = self._gen.sample(sample_shape=self.batch_shape + sample_shape).detach() +with torch.no_grad(): +y = self._gen.sample(sample_shape=sample_shape + self.batch_shape)",
        "core_API": "Size"
    },
    {
        "commit_hash": "148837b8e9e98ee82e21585e2fc1672d4a6c60c1",
        "index": "65541594a..99986d6f7 100644",
        "commit_message": "Fix uploading to s3 error handling\n\n",
        "file": "PySyft.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "def upload_to_s3_using_presigned(",
            ")  # maintain list of part no and ETag",
            "",
            "# Step 4 - Send a message to PyGrid informing about dataset upload complete!",
            "-        upload_response = client.datasets.perform_request(",
            "+        client.datasets.perform_request(",
            "syft_msg=UploadDataCompleteMessage,",
            "content={",
            "\"upload_id\": upload_response.payload.upload_id,"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=upload_response))",
            "Delete(target_node=ASTNode(type==, text==))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 4411,
        "neg_line": [
            "-upload_response = client.datasets.perform_request("
        ],
        "pos_line": [
            "+client.datasets.perform_request("
        ],
        "core_change": "-upload_response = client.datasets.perform_request( +client.datasets.perform_request(",
        "core_API": "perform_request"
    },
    {
        "commit_hash": "c1491b398006fe4252aa17a20002fdd020712002",
        "index": "b97261a1..8481e1ea 100644",
        "commit_message": "fix a broken link and minor update doc example (#425)\n\n* fix a broken link.\n\n* fix a broken link.\n\n* fix a broken link.\n\n* add placeholder for easy example reference.\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tensorflow",
        "change": [
            "class ExpandDimsLayer(Layer):",
            "class TileLayer(Layer):",
            "\"\"\"",
            "The :class:`TileLayer` class constructs a tensor by tiling a given tensor,",
            "-    see `tf.tile() <https://www.tensorflow.org/api_docs/python/array_ops/slicing_and_joining#tile>`__ .",
            "+    see `tf.tile() <https://www.tensorflow.org/api_docs/python/tf/tile>`__ .",
            "",
            "Parameters"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=`tf.tile() <https://www.tensorflow.org/api_docs/python/array_ops/slicing_and_joining#tile>`), value='`tf.tile() <https://www.tensorflow.org/api_docs/python/tf/tile>`')",
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=identifier, text=__), position=15)",
            "Delete(target_node=ASTNode(type=ERROR))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 3,
        "number": 4416,
        "neg_line": [
            "-see `tf.tile() <https://www.tensorflow.org/api_docs/python/array_ops/slicing_and_joining#tile>`__ ."
        ],
        "pos_line": [
            "+see `tf.tile() <https://www.tensorflow.org/api_docs/python/tf/tile>`__ ."
        ],
        "core_change": "-see `tf.tile() <https://www.tensorflow.org/api_docs/python/array_ops/slicing_and_joining#tile>`__ . +see `tf.tile() <https://www.tensorflow.org/api_docs/python/tf/tile>`__ .",
        "core_API": "tile"
    },
    {
        "commit_hash": "934d9fa8aa341bfcfd3106dbf77d5378a21c3f18",
        "index": "8a433a8cfe..64230a679f 100644",
        "commit_message": "Fix optional typehint (continued) (#11923)\n\nCo-authored-by: @AnnaTz\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def thresholded_relu(",
            "x: Tensor,",
            "/,",
            "*,",
            "-    threshold: Optional[Union[int, float]] = 0,",
            "+    threshold: Union[int, float] = 0,",
            "out: Optional[Tensor] = None,",
            ") -> Tensor:",
            "return tf.where(x > threshold, x, 0)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=Optional))",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=], text=]))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 3,
        "number": 4417,
        "neg_line": [
            "-threshold: Optional[Union[int, float]] = 0,"
        ],
        "pos_line": [
            "+threshold: Union[int, float] = 0,"
        ],
        "core_change": "-threshold: Optional[Union[int, float]] = 0, +threshold: Union[int, float] = 0,",
        "core_API": "where"
    },
    {
        "commit_hash": "df3e2cb4cceb47a36ceed0a261cceeb27e7a6e00",
        "index": "2430d5ac..3869cfea 100644",
        "commit_message": "Layer API Refactoring (#675)\n\n* Layer API Refactoring\n\n* private method decorator added\n\n* Input Layers added\n\n* Lazy Import on ROI pooling\n\n* @zsdonghao change proposal added\n\n* UTF-8 header added\n\n* gitignore updated\n\n* Python2 error correction\n\n* Changelog Updated\n\n* Python Headers Uniformized + Codacy Error Fix + LSTMStateTuple missing import\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TimeDistributedLayer(Layer):",
            "",
            "self.outputs = tf.stack(x, axis=1, name=name)",
            "",
            "-        self.all_layers.append(self.outputs)",
            "-        self.all_params.extend(variables)",
            "+        self._add_layers(self.outputs)",
            "+        self._add_params(variables)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=all_layers), value='_add_layers')",
            "Update(target_node=ASTNode(type=identifier, text=all_params), value='_add_params')",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=append))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=extend))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 10,
        "number": 4420,
        "neg_line": [
            "-self.all_layers.append(self.outputs)",
            "-self.all_params.extend(variables)"
        ],
        "pos_line": [
            "+self._add_layers(self.outputs)",
            "+self._add_params(variables)"
        ],
        "core_change": "-self.all_layers.append(self.outputs) -self.all_params.extend(variables) +self._add_layers(self.outputs) +self._add_params(variables)",
        "core_API": "stack"
    },
    {
        "commit_hash": "b2b58c0e9eb32f5d9a04472992bf86c6f3be8351",
        "index": "3cbc0b8e..1a86f756 100755",
        "commit_message": "Fixing example code\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def linear_layer(x, size):",
            "",
            "",
            "def dense_layer(x, size):",
            "-    with tf.variable_scope('linear'):",
            "+    with tf.variable_scope('dense'):",
            "weights = tf.Variable(initial_value=tf.random_normal(shape=(x.get_shape()[1].value, size), stddev=sqrt(2.0 / (x.get_shape()[1].value + size))))",
            "x = tf.matmul(a=x, b=weights)",
            "x = tf.nn.relu(features=x)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text='linear'), value=\"'dense'\")"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 4421,
        "neg_line": [
            "-with tf.variable_scope('linear'):"
        ],
        "pos_line": [
            "+with tf.variable_scope('dense'):"
        ],
        "core_change": "-with tf.variable_scope('linear'): +with tf.variable_scope('dense'):",
        "core_API": "variable_scope"
    },
    {
        "commit_hash": "2f3a4210185f5311f6cfab3c91b30616c9a30fc8",
        "index": "aca167085..00106627a 100644",
        "commit_message": "Fix other PyTorch models\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DistilBertModel(DistilBertPreTrainedModel):",
            "else:",
            "raise ValueError(\"You have to specify either input_ids or inputs_embeds\")",
            "",
            "+        device = input_ids.device if input_ids is not None else inputs_embeds.device",
            "+",
            "if attention_mask is None:",
            "-            attention_mask = torch.ones(input_shape) # (bs, seq_length)",
            "+            attention_mask = torch.ones(input_shape, device=device) # (bs, seq_length)",
            "",
            "# Prepare head mask if needed",
            "# 1.0 in head_mask indicate we keep the head"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=2, insert_id=1245128)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1245129)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'device'), position=0, insert_id=1245130)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1245131)",
            "Insert(target_node=IN(type=assignment), node=('conditional_expression', None), position=2, insert_id=1245132)",
            "Insert(target_node=IN(type=conditional_expression), node=('attribute', None), position=0, insert_id=1245133)",
            "Insert(target_node=IN(type=conditional_expression), node=('if', 'if'), position=1, insert_id=1245134)",
            "Insert(target_node=IN(type=conditional_expression), node=('comparison_operator', None), position=2, insert_id=1245135)",
            "Insert(target_node=IN(type=conditional_expression), node=('else', 'else'), position=3, insert_id=1245136)",
            "Insert(target_node=IN(type=conditional_expression), node=('attribute', None), position=4, insert_id=1245137)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'input_ids'), position=0, insert_id=1245138)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1245139)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=1245140)",
            "Insert(target_node=IN(type=comparison_operator), node=('identifier', 'input_ids'), position=0, insert_id=1245141)",
            "Insert(target_node=IN(type=comparison_operator), node=('is not', 'is'), position=1, insert_id=1245142)",
            "Insert(target_node=IN(type=comparison_operator), node=('is not', 'not'), position=2, insert_id=1245143)",
            "Insert(target_node=IN(type=comparison_operator), node=('none', 'None'), position=3, insert_id=1245144)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'inputs_embeds'), position=0, insert_id=1245145)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1245146)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=1245147)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1245148)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1245149)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=1245150)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1245151)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=2, insert_id=1245152)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 25,
        "number": 4424,
        "neg_line": [
            "-attention_mask = torch.ones(input_shape) # (bs, seq_length)"
        ],
        "pos_line": [
            "+device = input_ids.device if input_ids is not None else inputs_embeds.device",
            "+",
            "+attention_mask = torch.ones(input_shape, device=device) # (bs, seq_length)"
        ],
        "core_change": "+device = input_ids.device if input_ids is not None else inputs_embeds.device + -attention_mask = torch.ones(input_shape) # (bs, seq_length) +attention_mask = torch.ones(input_shape, device=device) # (bs, seq_length)",
        "core_API": "ones"
    },
    {
        "commit_hash": "8fdecfab00ad672d4070c2c672ac0442f2f1ffca",
        "index": "e77d80cb..bd9688cc 100644",
        "commit_message": "fix noise device\n\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class LatentDiffusion(DiffusionPipeline):",
            "# 3. optionally sample variance",
            "variance = 0",
            "if eta > 0:",
            "-                noise = torch.randn(image.shape, generator=generator, device=image.device)",
            "+                noise = torch.randn(image.shape, generator=generator)to(image.device)",
            "variance = self.noise_scheduler.get_variance(t, num_inference_steps).sqrt() * eta * noise",
            "",
            "# 4. set current image to prev_image: x_t -> x_t-1"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('call', None), position=0, insert_id=113267)",
            "Insert(target_node=ASTNode(type=call), node=('ERROR', None), position=1, insert_id=113268)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=2, insert_id=113269)",
            "Move(target_node=IN(type=call), node=ASTNode(type=attribute), position=0)",
            "Move(target_node=IN(type=call), node=ASTNode(type=argument_list), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=device), value='to')",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=identifier, text=device), position=0)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=113270)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=attribute), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=113271)",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 13,
        "number": 4425,
        "neg_line": [
            "-noise = torch.randn(image.shape, generator=generator, device=image.device)"
        ],
        "pos_line": [
            "+noise = torch.randn(image.shape, generator=generator)to(image.device)"
        ],
        "core_change": "-noise = torch.randn(image.shape, generator=generator, device=image.device) +noise = torch.randn(image.shape, generator=generator)to(image.device)",
        "core_API": "randn"
    },
    {
        "commit_hash": "27bae6956705cf91cd304cd2071a480eadc55757",
        "index": "1458d24c..04b1499c 100644",
        "commit_message": "Fix EmbedSequence\n\n",
        "file": "ludwig.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class EmbedSequence(Layer):",
            "self.dropout = None",
            "",
            "def call(self, inputs, training=None, mask=None):",
            "-        embedded = self.embed(",
            "-            inputs, training=None, mask=None",
            "+        embedded = tf.nn.embedding_lookup(",
            "+            self.embeddings, inputs, name='embeddings_lookup'",
            ")",
            "",
            "# TODO use tf2 mechanism for masking"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1420314)",
            "Move(target_node=ASTNode(type=identifier, text=inputs), node=ASTNode(type=argument_list), position=2)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1420315)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1420316)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'embedding_lookup'), position=2, insert_id=1420317)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=attribute), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=1420318)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1420319)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'nn'), position=2, insert_id=1420320)",
            "Update(target_node=ASTNode(type=identifier, text=embed), value='embeddings')",
            "Update(target_node=ASTNode(type=identifier, text=mask), value='name')",
            "Insert(target_node=ASTNode(type=keyword_argument), node=('string', \"'embeddings_lookup'\"), position=2, insert_id=1420321)",
            "Delete(target_node=ASTNode(type=identifier, text=training))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=none, text=None))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=none, text=None))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 4428,
        "neg_line": [
            "-embedded = self.embed(",
            "-inputs, training=None, mask=None"
        ],
        "pos_line": [
            "+embedded = tf.nn.embedding_lookup(",
            "+self.embeddings, inputs, name='embeddings_lookup'"
        ],
        "core_change": "-embedded = self.embed( -inputs, training=None, mask=None +embedded = tf.nn.embedding_lookup( +self.embeddings, inputs, name='embeddings_lookup'",
        "core_API": "embed"
    },
    {
        "commit_hash": "44cb00e468472c5a927eea00a0bb61ef4a279199",
        "index": "092a8ee..77e2260 100644",
        "commit_message": "lstsq fix in circle fitting for old PyTorch\n\nSummary: the pytorch3d.compat.lstsq function needs a 2D rhs.\n\nReviewed By: patricklabatut\n\nDifferential Revision: D36195826\n\nfbshipit-source-id: 9dbafea2057035cc04973f56729dc97b47dcac83\n\n",
        "file": "pytorch3d.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def fit_circle_in_2d(",
            "n_provided = points2d.shape[0]",
            "if n_provided < 3:",
            "raise ValueError(f\"{n_provided} points are not enough to determine a circle\")",
            "-    solution = lstsq(design, rhs)",
            "-    center = solution[:2] / 2",
            "-    radius = torch.sqrt(solution[2] + (center ** 2).sum())",
            "+    solution = lstsq(design, rhs[:, None])",
            "+    center = solution[:2, 0] / 2",
            "+    radius = torch.sqrt(solution[2, 0] + (center ** 2).sum())",
            "if n_points > 0:",
            "if angles is not None:",
            "warnings.warn(\"n_points ignored because angles provided\")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=subscript), node=(',', ','), position=3, insert_id=913565)",
            "Insert(target_node=ASTNode(type=subscript), node=('integer', '0'), position=4, insert_id=913566)",
            "Insert(target_node=ASTNode(type=argument_list), node=('subscript', None), position=3, insert_id=913567)",
            "Move(target_node=IN(type=subscript), node=ASTNode(type=identifier, text=rhs), position=0)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=913568)",
            "Insert(target_node=IN(type=subscript), node=('slice', None), position=2, insert_id=913569)",
            "Insert(target_node=IN(type=subscript), node=(',', ','), position=3, insert_id=913570)",
            "Insert(target_node=IN(type=subscript), node=('none', 'None'), position=4, insert_id=913571)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=5, insert_id=913572)",
            "Insert(target_node=IN(type=slice), node=(':', ':'), position=0, insert_id=913573)",
            "Insert(target_node=ASTNode(type=subscript), node=(',', ','), position=3, insert_id=913574)",
            "Insert(target_node=ASTNode(type=subscript), node=('integer', '0'), position=4, insert_id=913575)"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 12,
        "number": 4429,
        "neg_line": [
            "-solution = lstsq(design, rhs)",
            "-center = solution[:2] / 2",
            "-radius = torch.sqrt(solution[2] + (center ** 2).sum())"
        ],
        "pos_line": [
            "+solution = lstsq(design, rhs[:, None])",
            "+center = solution[:2, 0] / 2",
            "+radius = torch.sqrt(solution[2, 0] + (center ** 2).sum())"
        ],
        "core_change": "-solution = lstsq(design, rhs) -center = solution[:2] / 2 -radius = torch.sqrt(solution[2] + (center ** 2).sum()) +solution = lstsq(design, rhs[:, None]) +center = solution[:2, 0] / 2 +radius = torch.sqrt(solution[2, 0] + (center ** 2).sum())",
        "core_API": "sqrt"
    },
    {
        "commit_hash": "093e901c979c0f8b0af92f79010b53db1ddcc972",
        "index": "5e651f7..8405216 100644",
        "commit_message": "fix deprecation warnings; remove pydoop dependency; update README\n\n",
        "file": "TensorFlowOnSpark.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def _generate_image_and_label_batch(image, label, min_queue_examples,",
            "capacity=min_queue_examples + 3 * batch_size)",
            "",
            "# Display the training images in the visualizer.",
            "-  tf.image_summary('images', images)",
            "+  tf.summary.image('images', images)",
            "",
            "return images, tf.reshape(label_batch, [batch_size])"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=image_summary), value='summary')",
            "Insert(target_node=ASTNode(type=ERROR), node=('.', '.'), position=6, insert_id=2213606)",
            "Insert(target_node=ASTNode(type=ERROR), node=('identifier', 'image'), position=7, insert_id=2213607)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 3,
        "number": 4431,
        "neg_line": [
            "-tf.image_summary('images', images)"
        ],
        "pos_line": [
            "+tf.summary.image('images', images)"
        ],
        "core_change": "-tf.image_summary('images', images) +tf.summary.image('images', images)",
        "core_API": "image_summary"
    },
    {
        "commit_hash": "3c726544d240f610cd35ea264d893d6a6ada074a",
        "index": "83b6d4d9..0d5804c8 100644",
        "commit_message": "fix issue where is_initialized is not available in single-worker paradigm (#2801)\n\nSummary:\n# Before submitting\n\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\n- [ ] Did you make sure to update the docs?\n- [ ] Did you write any new necessary tests?\n\n## What does this PR do?\nFixes https://github.com/pytorch/fairseq/issues/1205\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding \n\nPull Request resolved: https://github.com/pytorch/fairseq/pull/2801\n\nReviewed By: alexeib\n\nDifferential Revision: D24579193\n\nPulled By: myleott\n\nfbshipit-source-id: bcb14bb588d4538398bff4114e0a387fd29818c5\n\n",
        "file": "fairseq.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def distributed_init(cfg: FairseqConfig):",
            "cfg = convert_namespace_to_omegaconf(cfg)",
            "",
            "if not cfg.common.tpu:",
            "-        if torch.distributed.is_initialized():",
            "+        if torch.distributed.is_available() and torch.distributed.is_initialized():",
            "warnings.warn(",
            "\"Distributed is already initialized, cannot initialize twice!\"",
            ")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=if_statement), node=('boolean_operator', None), position=1, insert_id=210296)",
            "Insert(target_node=IN(type=boolean_operator), node=('call', None), position=0, insert_id=210297)",
            "Insert(target_node=IN(type=boolean_operator), node=('and', 'and'), position=1, insert_id=210298)",
            "Move(target_node=IN(type=boolean_operator), node=ASTNode(type=call), position=2)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=210299)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=210300)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=210301)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=210302)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'is_available'), position=2, insert_id=210303)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=210304)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=210305)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=210306)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=210307)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'distributed'), position=2, insert_id=210308)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 14,
        "number": 4434,
        "neg_line": [
            "-if torch.distributed.is_initialized():"
        ],
        "pos_line": [
            "+if torch.distributed.is_available() and torch.distributed.is_initialized():"
        ],
        "core_change": "-if torch.distributed.is_initialized(): +if torch.distributed.is_available() and torch.distributed.is_initialized():",
        "core_API": "is_initialized"
    },
    {
        "commit_hash": "5f4c6e830f603830117877b8990a0e65a2386aa6",
        "index": "554b25b97..f0883f211 100644",
        "commit_message": "Quick fix :) (#606)\n\n* Changing the name\n\n* style + quality\n\n* update doc and logo\n\n* clean up\n\n* circle-CI on the branche for now\n\n* fix daily dialog dataset\n\n* fix urls\n\nCo-authored-by: Quentin Lhoest <lhoest.q@gmail.com>\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "def disable_progress_bar():",
            "",
            "Usage:",
            "",
            "-    nlp.disable_progress_bar()",
            "+    datasets.disable_progress_bar()",
            "\"\"\"",
            "# Replace tqdm",
            "global _active"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=nlp), value='datasets')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 4436,
        "neg_line": [
            "-nlp.disable_progress_bar()"
        ],
        "pos_line": [
            "+datasets.disable_progress_bar()"
        ],
        "core_change": "-nlp.disable_progress_bar() +datasets.disable_progress_bar()",
        "core_API": "disable_progress_bar"
    },
    {
        "commit_hash": "bec166ddce24ed9203f68a19c5dc0c1cbf2d5651",
        "index": "9bebcbc0..0a67db13 100644",
        "commit_message": "Use NewSessionCreate in PredictConfig, so that variables are initialized (fix #1225)\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class PredictConfig(object):",
            "assert_type(self.session_init, SessionInit, 'session_init')",
            "",
            "if session_creator is None:",
            "-            self.session_creator = tf.train.ChiefSessionCreator(config=get_default_sess_config())",
            "+            self.session_creator = NewSessionCreator(config=get_default_sess_config())",
            "else:",
            "self.session_creator = session_creator"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tf), value='NewSessionCreator')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=tf), position=0)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=train))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=ChiefSessionCreator))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 4437,
        "neg_line": [
            "-self.session_creator = tf.train.ChiefSessionCreator(config=get_default_sess_config())"
        ],
        "pos_line": [
            "+self.session_creator = NewSessionCreator(config=get_default_sess_config())"
        ],
        "core_change": "-self.session_creator = tf.train.ChiefSessionCreator(config=get_default_sess_config()) +self.session_creator = NewSessionCreator(config=get_default_sess_config())",
        "core_API": "ChiefSessionCreator"
    },
    {
        "commit_hash": "8271406a04e380421c6226cbec1b6ea55189ea72",
        "index": "2a7e60713..e7b4b5125 100644",
        "commit_message": "[RLLib] Fix MultiDiscrete not being one-hotted correctly (#26558)\n\nCo-authored-by: Jun Gong <jungong@anyscale.com>\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ComplexInputNetwork(TorchModelV2, nn.Module):",
            "cnn_out, _ = self.cnns[i](SampleBatch({SampleBatch.OBS: component}))",
            "outs.append(cnn_out)",
            "elif i in self.one_hot:",
            "-                if component.dtype in [torch.int32, torch.int64, torch.uint8]:",
            "+                if component.dtype in [",
            "+                    torch.int8,",
            "+                    torch.int16,",
            "+                    torch.int32,",
            "+                    torch.int64,",
            "+                    torch.uint8,",
            "+                ]:",
            "one_hot_in = {",
            "SampleBatch.OBS: one_hot(",
            "component, self.flattened_input_space[i]"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=list), node=('attribute', None), position=1, insert_id=1110048)",
            "Insert(target_node=ASTNode(type=list), node=(',', ','), position=2, insert_id=1110049)",
            "Insert(target_node=ASTNode(type=list), node=('attribute', None), position=3, insert_id=1110050)",
            "Insert(target_node=ASTNode(type=list), node=(',', ','), position=4, insert_id=1110051)",
            "Insert(target_node=ASTNode(type=list), node=(',', ','), position=10, insert_id=1110052)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1110053)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1110054)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'int8'), position=2, insert_id=1110055)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1110056)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1110057)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'int16'), position=2, insert_id=1110058)"
        ],
        "plus_line": 7,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 4445,
        "neg_line": [
            "-if component.dtype in [torch.int32, torch.int64, torch.uint8]:"
        ],
        "pos_line": [
            "+if component.dtype in [",
            "+torch.int8,",
            "+torch.int16,",
            "+torch.int32,",
            "+torch.int64,",
            "+torch.uint8,",
            "+]:"
        ],
        "core_change": "-if component.dtype in [torch.int32, torch.int64, torch.uint8]: +if component.dtype in [ +torch.int8, +torch.int16, +torch.int32, +torch.int64, +torch.uint8, +]:",
        "core_API": "append"
    },
    {
        "commit_hash": "f0ce6bcc1ba644c87ac74455f95ffbd865a9c3ca",
        "index": "cc42d6a..a769c55 100644",
        "commit_message": "fix(activation): add missing comma\n",
        "file": "gpt-neo.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "CUSTOM_FNS = {'lrelu001': lambda x: mtf.leaky_relu(x, alpha=0.01),",
            "'spike2': lambda x: mtf.exp(-x ** 2),",
            "'tanhshrink': lambda x: x - tanh(x),",
            "'softsign': lambda x: x / (mtf.abs(x) + 1),",
            "-              'softmax': lambda x: mtf.softmax(x, x.shape[-1])",
            "+              'softmax': lambda x: mtf.softmax(x, x.shape[-1]),",
            "'logsoftmax': lambda x: mtf.log_softmax(x, x.shape[-1]),",
            "'bipolarsigmoid': lambda x: mtf.sigmoid(x) * 2 - 1,",
            "'rrelu': _rrelu,"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=ERROR), node=('pair', None), position=11, insert_id=1933885)",
            "Insert(target_node=ASTNode(type=ERROR), node=(',', ','), position=12, insert_id=1933886)",
            "Move(target_node=IN(type=pair), node=ASTNode(type=string, text='softmax'), position=0)",
            "Move(target_node=IN(type=pair), node=ASTNode(type=:, text=:), position=1)",
            "Move(target_node=IN(type=pair), node=ASTNode(type=lambda), position=2)",
            "Move(target_node=ASTNode(type=pair), node=ASTNode(type=string, text='logsoftmax'), position=0)",
            "Move(target_node=ASTNode(type=pair), node=ASTNode(type=:, text=:), position=1)",
            "Move(target_node=ASTNode(type=lambda), node=ASTNode(type=call), position=3)",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=ERROR))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 4446,
        "neg_line": [
            "-'softmax': lambda x: mtf.softmax(x, x.shape[-1])"
        ],
        "pos_line": [
            "+'softmax': lambda x: mtf.softmax(x, x.shape[-1]),"
        ],
        "core_change": "-'softmax': lambda x: mtf.softmax(x, x.shape[-1]) +'softmax': lambda x: mtf.softmax(x, x.shape[-1]),",
        "core_API": "leaky_relu"
    },
    {
        "commit_hash": "ba71bf4caedaa90a11f02735eb8bb375aabe70f5",
        "index": "d55ef5447..c8d688282 100755",
        "commit_message": "fix: renamed variable name (#18850)\n\nThe sequence_masked variable is actually the part of the sequence that is kept unmasked for the encoder. This commit renames the variable.\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ViTMAEEmbeddings(nn.Module):",
            "# unshuffle to get the binary mask",
            "mask = torch.gather(mask, dim=1, index=ids_restore)",
            "",
            "-        return sequence_masked, mask, ids_restore",
            "+        return sequence_unmasked, mask, ids_restore",
            "",
            "def forward(self, pixel_values, noise=None):",
            "batch_size, num_channels, height, width = pixel_values.shape"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=sequence_masked), value='sequence_unmasked')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 4448,
        "neg_line": [
            "-return sequence_masked, mask, ids_restore"
        ],
        "pos_line": [
            "+return sequence_unmasked, mask, ids_restore"
        ],
        "core_change": "-return sequence_masked, mask, ids_restore +return sequence_unmasked, mask, ids_restore",
        "core_API": "gather"
    },
    {
        "commit_hash": "03aaac35021dead4fb7ad354fe9c986d16869f03",
        "index": "10250a1d6..3725c5e77 100644",
        "commit_message": "Fix TVLT (torch device issue) (#21710)\n\n* fix tvlt ci\n\n* fix tvlt ci\n\n* fix tvlt ci\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def generate_audio_mask_noise(audio_values, audio_mask=None, mask_ratio=0.75, ma",
            "if mask_type == \"frame-level\":",
            "num_time_patches = seq_len // freq_len",
            "noise = (",
            "-            torch.rand(batch_size, num_time_patches).unsqueeze(-1).repeat(1, 1, freq_len).view(batch_size, seq_len)",
            "+            torch.rand(batch_size, num_time_patches, device=audio_values.device)",
            "+            .unsqueeze(-1)",
            "+            .repeat(1, 1, freq_len)",
            "+            .view(batch_size, seq_len)",
            ")  # noise in [0, 1]",
            "elif mask_type == \"patch-level\":",
            "-        noise = torch.rand(batch_size, seq_len)  # noise in [0, 1]",
            "+        noise = torch.rand(batch_size, seq_len, device=audio_values.device)  # noise in [0, 1]",
            "len_keep = int(seq_len * (1 - mask_ratio))",
            "return noise, len_keep"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=1176244)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=1176245)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=1176246)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1176247)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=1176248)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'audio_values'), position=0, insert_id=1176249)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1176250)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=1176251)",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1176252)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1176253)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1176254)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'rand'), position=2, insert_id=1176255)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=1176256)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=1176257)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=1176258)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1176259)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=1176260)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'audio_values'), position=0, insert_id=1176261)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1176262)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=1176263)",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=rand))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 5,
        "minus_line": 2,
        "AST_diff_line": 25,
        "number": 4450,
        "neg_line": [
            "-torch.rand(batch_size, num_time_patches).unsqueeze(-1).repeat(1, 1, freq_len).view(batch_size, seq_len)",
            "-noise = torch.rand(batch_size, seq_len)  # noise in [0, 1]"
        ],
        "pos_line": [
            "+torch.rand(batch_size, num_time_patches, device=audio_values.device)",
            "+.unsqueeze(-1)",
            "+.repeat(1, 1, freq_len)",
            "+.view(batch_size, seq_len)",
            "+noise = torch.rand(batch_size, seq_len, device=audio_values.device)  # noise in [0, 1]"
        ],
        "core_change": "-torch.rand(batch_size, num_time_patches).unsqueeze(-1).repeat(1, 1, freq_len).view(batch_size, seq_len) +torch.rand(batch_size, num_time_patches, device=audio_values.device) +.unsqueeze(-1) +.repeat(1, 1, freq_len) +.view(batch_size, seq_len) -noise = torch.rand(batch_size, seq_len)  # noise in [0, 1] +noise = torch.rand(batch_size, seq_len, device=audio_values.device)  # noise in [0, 1]",
        "core_API": "rand"
    },
    {
        "commit_hash": "d0fab84e4dfdddc64cb4275ff98cf67f84ff86a0",
        "index": "877390a1c..8b56b87f6 100644",
        "commit_message": "[RLlib] DDPG PyTorch version. (#7953)\n\nThe DDPG/TD3 algorithms currently do not have a PyTorch implementation. This PR adds PyTorch support for DDPG/TD3 to RLlib.\nThis PR:\n- Depends on the re-factor PR for DDPG (Functional Algorithm API).\n- Adds learning regression tests for the PyTorch version of DDPG and a DDPG (torch)\n- Updates the documentation to reflect that DDPG and TD3 now support PyTorch.\n\n* Learning Pendulum-v0 on torch version (same config as tf). Wall time a little slower (~20% than tf).\n* Fix GPU target model problem.\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class OrnsteinUhlenbeckNoise(GaussianNoise):",
            "ou_new = self.ou_theta * -self.ou_state + \\",
            "self.ou_sigma * gaussian_sample",
            "self.ou_state += ou_new",
            "-                high_low = torch.from_numpy(self.action_space.high -",
            "-                                            self.action_space.low).to(",
            "-                                                self.device)",
            "-                noise = scale * self.ou_base_scale * self.ou_state * high_low",
            "+                high_m_low = torch.from_numpy(",
            "+                    self.action_space.high - self.action_space.low). \\",
            "+                    to(self.device)",
            "+                noise = scale * self.ou_base_scale * self.ou_state * high_m_low",
            "action = torch.clamp(det_actions + noise,",
            "self.action_space.low[0],",
            "self.action_space.high[0])"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=high_low), value='high_m_low')",
            "Update(target_node=ASTNode(type=identifier, text=high_low), value='high_m_low')"
        ],
        "plus_line": 4,
        "minus_line": 4,
        "AST_diff_line": 2,
        "number": 4451,
        "neg_line": [
            "-high_low = torch.from_numpy(self.action_space.high -",
            "-self.action_space.low).to(",
            "-self.device)",
            "-noise = scale * self.ou_base_scale * self.ou_state * high_low"
        ],
        "pos_line": [
            "+high_m_low = torch.from_numpy(",
            "+self.action_space.high - self.action_space.low). \\",
            "+to(self.device)",
            "+noise = scale * self.ou_base_scale * self.ou_state * high_m_low"
        ],
        "core_change": "-high_low = torch.from_numpy(self.action_space.high - -self.action_space.low).to( -self.device) -noise = scale * self.ou_base_scale * self.ou_state * high_low +high_m_low = torch.from_numpy( +self.action_space.high - self.action_space.low). \\ +to(self.device) +noise = scale * self.ou_base_scale * self.ou_state * high_m_low",
        "core_API": "from_numpy"
    },
    {
        "commit_hash": "5a2b77a6c1dc54372d0569c6a681c69895eab904",
        "index": "52cc6585a..6ad86071e 100644",
        "commit_message": "Fix error in mixed precision training of `TFCvtModel` (#22267)\n\n* Make sure CVT can be trained using mixed precision\n\n* Add test for keras-fit with mixed-precision\n\n* Update tests/models/cvt/test_modeling_tf_cvt.py\n\nCo-authored-by: Matt <Rocketknight1@users.noreply.github.com>\n\n---------\n\nCo-authored-by: gcuder <Gerald.Cuder@iacapps.com>\nCo-authored-by: Matt <Rocketknight1@users.noreply.github.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFCvtDropPath(tf.keras.layers.Layer):",
            "return x",
            "keep_prob = 1 - self.drop_prob",
            "shape = (tf.shape(x)[0],) + (1,) * (len(tf.shape(x)) - 1)",
            "-        random_tensor = keep_prob + tf.random.uniform(shape, 0, 1)",
            "+        random_tensor = keep_prob + tf.random.uniform(shape, 0, 1, dtype=self.compute_dtype)",
            "random_tensor = tf.floor(random_tensor)",
            "return (x / keep_prob) * random_tensor"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=6, insert_id=2357016)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=7, insert_id=2357017)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=2357018)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2357019)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=2357020)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=2357021)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2357022)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'compute_dtype'), position=2, insert_id=2357023)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 4456,
        "neg_line": [
            "-random_tensor = keep_prob + tf.random.uniform(shape, 0, 1)"
        ],
        "pos_line": [
            "+random_tensor = keep_prob + tf.random.uniform(shape, 0, 1, dtype=self.compute_dtype)"
        ],
        "core_change": "-random_tensor = keep_prob + tf.random.uniform(shape, 0, 1) +random_tensor = keep_prob + tf.random.uniform(shape, 0, 1, dtype=self.compute_dtype)",
        "core_API": "shape"
    },
    {
        "commit_hash": "706014843879fc7792c9743de1cac75668c16268",
        "index": "3807dc0ea..07c355d6e 100644",
        "commit_message": "fix\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class FrameScoreFeats(AbsFeatsExtract):",
            "pad = self.win_length // 2",
            "input_lengths = input_lengths + 2 * pad",
            "",
            "-            olens = (",
            "-                torch.div(",
            "-                    (input_lengths - self.win_length),",
            "-                    self.hop_length,",
            "-                    rounding_mode=\"floor\",",
            "-                )",
            "-                + 1",
            "-            )",
            "+            olens = (input_lengths - self.win_length) // self.hop_length + 1",
            "output.masked_fill_(make_pad_mask(olens, output, 1), 0.0)",
            "else:",
            "olens = None"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('binary_operator', None), position=2, insert_id=119829)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=binary_operator), position=0)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=+, text=+), position=1)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=integer, text=1), position=2)",
            "Move(target_node=ASTNode(type=binary_operator), node=ASTNode(type=parenthesized_expression), position=0)",
            "Insert(target_node=ASTNode(type=binary_operator), node=('//', '//'), position=1, insert_id=119830)",
            "Move(target_node=ASTNode(type=binary_operator), node=ASTNode(type=attribute), position=2)",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=div))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=rounding_mode))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=string, text=\"floor\"))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=parenthesized_expression))"
        ],
        "plus_line": 1,
        "minus_line": 8,
        "AST_diff_line": 25,
        "number": 4459,
        "neg_line": [
            "-olens = (",
            "-torch.div(",
            "-(input_lengths - self.win_length),",
            "-self.hop_length,",
            "-rounding_mode=\"floor\",",
            "-)",
            "-+ 1",
            "-)"
        ],
        "pos_line": [
            "+olens = (input_lengths - self.win_length) // self.hop_length + 1"
        ],
        "core_change": "-olens = ( -torch.div( -(input_lengths - self.win_length), -self.hop_length, -rounding_mode=\"floor\", -) -+ 1 -) +olens = (input_lengths - self.win_length) // self.hop_length + 1",
        "core_API": "div"
    },
    {
        "commit_hash": "f7bb107b417905c778215344e9c63eb4d1f4b6ac",
        "index": "c73f036..a7fb25b 100644",
        "commit_message": "fix some issue with torch.pi and torch jit script\n\n",
        "file": "imagen-pytorch.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def beta_linear_log_snr(t):",
            "",
            "@torch.jit.script",
            "def alpha_cosine_log_snr(t, s: float = 0.008):",
            "-    return -log((torch.cos((t + s) / (1 + s) * torch.pi * 0.5) ** -2) - 1)",
            "+    return -log((torch.cos((t + s) / (1 + s) * math.pi * 0.5) ** -2) - 1)",
            "",
            "def log_snr_to_alpha_sigma(log_snr):",
            "return torch.sqrt(torch.sigmoid(log_snr)), torch.sqrt(torch.sigmoid(-log_snr))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=torch), value='math')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 4460,
        "neg_line": [
            "-return -log((torch.cos((t + s) / (1 + s) * torch.pi * 0.5) ** -2) - 1)"
        ],
        "pos_line": [
            "+return -log((torch.cos((t + s) / (1 + s) * math.pi * 0.5) ** -2) - 1)"
        ],
        "core_change": "-return -log((torch.cos((t + s) / (1 + s) * torch.pi * 0.5) ** -2) - 1) +return -log((torch.cos((t + s) / (1 + s) * math.pi * 0.5) ** -2) - 1)",
        "core_API": "cos"
    },
    {
        "commit_hash": "e6b34ef90d5f3eac70154b305b476614a64f1981",
        "index": "c1d598dc7..978ac5df7 100644",
        "commit_message": "[WIP] Reduction when batch size < num gpus (#1609)\n\n* reduce if <= num_gpus\n\n* add test with explanation\n\n* chlog\n\n* fix changelog\n\nCo-authored-by: J. Borovec <jirka.borovec@seznam.cz>\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TrainerLoggingMixin(ABC):",
            "elif isinstance(output[k], torch.Tensor) and output[k].dim() == 0:",
            "pass",
            "",
            "-            # reduce only metrics that have the same number of gpus",
            "-            elif output[k].size(0) == num_gpus:",
            "-                reduced = torch.mean(output[k])",
            "-                output[k] = reduced",
            "+            # do not reduce metrics that have batch size > num gpus",
            "+            elif output[k].size(0) <= num_gpus:",
            "+                output[k] = torch.mean(output[k])",
            "+",
            "return output"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=comparison_operator), node=('<=', '<='), position=1, insert_id=582587)",
            "Move(target_node=ASTNode(type=comparison_operator), node=ASTNode(type=subscript), position=4)",
            "Delete(target_node=ASTNode(type===, text===))",
            "Delete(target_node=ASTNode(type=identifier, text=reduced))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=reduced))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 3,
        "minus_line": 4,
        "AST_diff_line": 8,
        "number": 4464,
        "neg_line": [
            "-# reduce only metrics that have the same number of gpus",
            "-elif output[k].size(0) == num_gpus:",
            "-reduced = torch.mean(output[k])",
            "-output[k] = reduced"
        ],
        "pos_line": [
            "+# do not reduce metrics that have batch size > num gpus",
            "+elif output[k].size(0) <= num_gpus:",
            "+output[k] = torch.mean(output[k])",
            "+"
        ],
        "core_change": "-# reduce only metrics that have the same number of gpus -elif output[k].size(0) == num_gpus: -reduced = torch.mean(output[k]) -output[k] = reduced +# do not reduce metrics that have batch size > num gpus +elif output[k].size(0) <= num_gpus: +output[k] = torch.mean(output[k]) +",
        "core_API": "mean"
    },
    {
        "commit_hash": "95b6d985ffb231d9a2047c039c95660913063b9d",
        "index": "7e8d6d3b..dc49e1b5 100644",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestEncoderBase(AllenNlpTestCase):",
            ")",
            "",
            "# Check that error is raised if mask has wrong batch size.",
            "-        bad_mask = torch.FloatTensor([1, 1, 0])",
            "+        bad_mask = torch.BoolTensor([True, True, False])",
            "with self.assertRaises(ValueError):",
            "self.encoder_base.reset_states(bad_mask)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=FloatTensor), value='BoolTensor')",
            "Insert(target_node=ASTNode(type=list), node=('true', 'True'), position=1, insert_id=19740)",
            "Insert(target_node=ASTNode(type=list), node=('true', 'True'), position=4, insert_id=19741)",
            "Insert(target_node=ASTNode(type=list), node=(',', ','), position=5, insert_id=19742)",
            "Insert(target_node=ASTNode(type=list), node=('false', 'False'), position=6, insert_id=19743)",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=integer, text=0))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 4466,
        "neg_line": [
            "-bad_mask = torch.FloatTensor([1, 1, 0])"
        ],
        "pos_line": [
            "+bad_mask = torch.BoolTensor([True, True, False])"
        ],
        "core_change": "-bad_mask = torch.FloatTensor([1, 1, 0]) +bad_mask = torch.BoolTensor([True, True, False])",
        "core_API": "FloatTensor"
    },
    {
        "commit_hash": "e3cc4487fe66e03ec85970ea2db8e5fb34c455f4",
        "index": "c4dd39918..dbefa1edd 100644",
        "commit_message": "Fix CIs for PyTorch 1.13 (#20686)\n\n* fix 1\n\n* fix 2\n\n* fix 3\n\n* fix 4\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class CLIPSegTextTransformer(nn.Module):",
            "# take features from the eot embedding (eot_token is the highest number in each sequence)",
            "# casting to torch.int for onnx compatibility: argmax doesn't support int64 inputs with opset 14",
            "pooled_output = last_hidden_state[",
            "-            torch.arange(last_hidden_state.shape[0], device=input_ids.device), input_ids.to(torch.int).argmax(dim=-1)",
            "+            torch.arange(last_hidden_state.shape[0], device=last_hidden_state.device),",
            "+            input_ids.to(dtype=torch.int, device=last_hidden_state.device).argmax(dim=-1),",
            "]",
            "",
            "if not return_dict:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=subscript), node=(',', ','), position=5, insert_id=1181331)",
            "Update(target_node=ASTNode(type=identifier, text=input_ids), value='last_hidden_state')",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=1, insert_id=1181332)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1181333)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1181334)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=1181335)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1181336)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=attribute), position=2)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=1181337)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1181338)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=1181339)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'last_hidden_state'), position=0, insert_id=1181340)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1181341)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=1181342)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 14,
        "number": 4467,
        "neg_line": [
            "-torch.arange(last_hidden_state.shape[0], device=input_ids.device), input_ids.to(torch.int).argmax(dim=-1)"
        ],
        "pos_line": [
            "+torch.arange(last_hidden_state.shape[0], device=last_hidden_state.device),",
            "+input_ids.to(dtype=torch.int, device=last_hidden_state.device).argmax(dim=-1),"
        ],
        "core_change": "-torch.arange(last_hidden_state.shape[0], device=input_ids.device), input_ids.to(torch.int).argmax(dim=-1) +torch.arange(last_hidden_state.shape[0], device=last_hidden_state.device), +input_ids.to(dtype=torch.int, device=last_hidden_state.device).argmax(dim=-1),",
        "core_API": "arange"
    },
    {
        "commit_hash": "58cb31362f27eb6502e917b091e14cc9b1c0d65a",
        "index": "9c36e83..12b3d7f 100644",
        "commit_message": "Fix parsing Keras version identifiers like `2.2.4-tf` (#3794)\n\nSigned-off-by: Max H. Gerlach <git@maxgerlach.de>\n",
        "file": "horovod.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "keras",
        "change": [
            "from horovod.common.util  import is_version_greater_equal_than",
            "",
            "if is_version_greater_equal_than(tf.__version__, \"2.6.0\"):",
            "from keras import backend as K",
            "-    if version.parse(keras.__version__) < version.parse(\"2.9.0\"):",
            "+    if version.parse(keras.__version__.replace(\"-tf\", \"+tf\")) < version.parse(\"2.9.0\"):",
            "from keras.optimizer_v2 import optimizer_v2",
            "else:",
            "from keras.optimizers.optimizer_v2 import optimizer_v2"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=1, insert_id=2484607)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=2, insert_id=2484608)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2484609)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2484610)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2484611)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'replace'), position=2, insert_id=2484612)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2484613)",
            "Insert(target_node=IN(type=argument_list), node=('string', '\"-tf\"'), position=1, insert_id=2484614)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=2484615)",
            "Insert(target_node=IN(type=argument_list), node=('string', '\"+tf\"'), position=3, insert_id=2484616)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=), text=)), position=4)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 12,
        "number": 4474,
        "neg_line": [
            "-if version.parse(keras.__version__) < version.parse(\"2.9.0\"):"
        ],
        "pos_line": [
            "+if version.parse(keras.__version__.replace(\"-tf\", \"+tf\")) < version.parse(\"2.9.0\"):"
        ],
        "core_change": "-if version.parse(keras.__version__) < version.parse(\"2.9.0\"): +if version.parse(keras.__version__.replace(\"-tf\", \"+tf\")) < version.parse(\"2.9.0\"):",
        "core_API": "parse"
    },
    {
        "commit_hash": "2fe1a3ad30cb5fa7226eee375f1008f1b647b1d3",
        "index": "3a181e0e..e3233951 100644",
        "commit_message": "fix mypy error\n\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def tversky_loss(input: torch.Tensor, target: torch.Tensor,",
            "# compute the actual dice score",
            "dims = (1, 2, 3)",
            "intersection = torch.sum(input_soft * target_one_hot, dims)",
            "-    fps = torch.sum(input_soft * (1. - target_one_hot), dims)",
            "-    fns = torch.sum((1. - input_soft) * target_one_hot, dims)",
            "+    fps = torch.sum(input_soft * (-target_one_hot + 1.), dims)",
            "+    fns = torch.sum((-input_soft + 1.) * target_one_hot, dims)",
            "",
            "numerator = intersection",
            "denominator = intersection + alpha * fps + beta * fns",
            "tversky_loss = numerator / (denominator + eps)",
            "-    return torch.mean(1. - tversky_loss)",
            "+    return torch.mean(-tversky_loss + 1.)",
            "",
            "",
            "class TverskyLoss(nn.Module):"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=binary_operator), node=('unary_operator', None), position=0, insert_id=453879)",
            "Insert(target_node=ASTNode(type=binary_operator), node=('+', '+'), position=1, insert_id=453880)",
            "Insert(target_node=ASTNode(type=binary_operator), node=('float', '1.'), position=2, insert_id=453881)",
            "Insert(target_node=ASTNode(type=binary_operator), node=('unary_operator', None), position=0, insert_id=453882)",
            "Insert(target_node=ASTNode(type=binary_operator), node=('+', '+'), position=1, insert_id=453883)",
            "Insert(target_node=ASTNode(type=binary_operator), node=('float', '1.'), position=2, insert_id=453884)",
            "Move(target_node=IN(type=unary_operator), node=ASTNode(type=-, text=-), position=0)",
            "Move(target_node=IN(type=unary_operator), node=ASTNode(type=identifier, text=tversky_loss), position=1)",
            "Move(target_node=IN(type=unary_operator), node=ASTNode(type=-, text=-), position=0)",
            "Move(target_node=IN(type=unary_operator), node=ASTNode(type=identifier, text=input_soft), position=1)",
            "Insert(target_node=ASTNode(type=binary_operator), node=('unary_operator', None), position=0, insert_id=453885)",
            "Insert(target_node=ASTNode(type=binary_operator), node=('+', '+'), position=1, insert_id=453886)",
            "Insert(target_node=ASTNode(type=binary_operator), node=('float', '1.'), position=2, insert_id=453887)",
            "Move(target_node=IN(type=unary_operator), node=ASTNode(type=-, text=-), position=0)",
            "Move(target_node=IN(type=unary_operator), node=ASTNode(type=identifier, text=target_one_hot), position=1)",
            "Delete(target_node=ASTNode(type=float, text=1.))",
            "Delete(target_node=ASTNode(type=float, text=1.))",
            "Delete(target_node=ASTNode(type=float, text=1.))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 18,
        "number": 4476,
        "neg_line": [
            "-fps = torch.sum(input_soft * (1. - target_one_hot), dims)",
            "-fns = torch.sum((1. - input_soft) * target_one_hot, dims)",
            "-return torch.mean(1. - tversky_loss)"
        ],
        "pos_line": [
            "+fps = torch.sum(input_soft * (-target_one_hot + 1.), dims)",
            "+fns = torch.sum((-input_soft + 1.) * target_one_hot, dims)",
            "+return torch.mean(-tversky_loss + 1.)"
        ],
        "core_change": "-fps = torch.sum(input_soft * (1. - target_one_hot), dims) -fns = torch.sum((1. - input_soft) * target_one_hot, dims) +fps = torch.sum(input_soft * (-target_one_hot + 1.), dims) +fns = torch.sum((-input_soft + 1.) * target_one_hot, dims) -return torch.mean(1. - tversky_loss) +return torch.mean(-tversky_loss + 1.)",
        "core_API": "sum"
    },
    {
        "commit_hash": "da6dbc8d1d128cf783d7151b012a5502bbd52bf5",
        "index": "394e4285d..82f328a92 100644",
        "commit_message": "PoC: Accelerator refactor (#5743)\n\n* restoring the result from subprocess\n\n* fix queue.get() order for results\n\n* add missing \"block_backward_sync\" context manager\n\n* add missing \"block_backward_sync\" context manager\n\n* fix sync_batchnorm\n\n* fix supported gpu-ids for tuple\n\n* fix clip gradients and inf recursion\n\n* accelerator selection: added cluster_environment plugin\n\n* fix torchelastic test\n\n* fix reduce early stopping decision for DDP\n\n* fix tests: callbacks, conversion to lightning optimizer\n\n* fix lightning optimizer does not pickle\n\n* fix setting benchmark and deterministic option\n\n* fix slurm amp test\n\n* fix prepare_data test and determine node_rank\n\n* fix retrieving last path when testing\n\n* remove obsolete plugin argument\n\n* fix test: test_trainer_config\n\n* fix torchscript tests\n\n* fix trainer.model access\n\n* move properties\n\n* fix test_transfer_batch_hook\n\n* fix auto_select_gpus\n\n* fix omegaconf test\n\n* fix test that needs to simulate slurm ddp\n\n* add horovod plugin\n\n* fix test with named arguments\n\n* clean up whitespace\n\n* fix datamodules test\n\n* remove old accelerators\n\n* fix naming\n\n* move old plugins\n\n* move to plugins\n\n* create precision subpackage\n\n* create training_type subpackage\n\n* fix all new import errors\n\n* fix wrong arguments order passed to test\n\n* fix LR finder\n\n* Added sharded training type and amp plugin\n\n* Move clip grad to precision plugin\n\n* Added sharded spawn, select accelerators based on distributed_backend + enable custom fp16 plugin automatically\n\n* Fix import issue, attempting to fix tests\n\n* Fix initial test\n\n* Reflect hook logic from master, should wrap model after move to device\n\n* Optional state consolidation, since master has optimizers not wrapped\n\n* change attribute for instance test\n\n* reset optimizers\n\noptimizers are not used in main process, so state would be wrong.\n\n* legacy\n\n* imports in accel\n\n* legacy2\n\n* trainer imports\n\n* fix import errors after rebase\n\n* move hook to new setup location\n\n* provide unwrapping logic\n\n* fix trainer callback system\n\n* added ddp2 implementation\n\n* fix imports .legacy\n\n* move plugins\n\n* restore legacy\n\n* drop test.py from root\n\n* add tpu accelerator and plugins\n\n* fixes\n\n* fix lightning optimizer merge\n\n* reset bugreportmodel\n\n* unwrapping\n\n* step routing forward\n\n* model access\n\n* unwrap\n\n* opt\n\n* integrate distrib_type\n\n* sync changes\n\n* sync\n\n* fixes\n\n* add forgotten generators\n\n* add missing logic\n\n* update\n\n* import\n\n* missed imports\n\n* import fixes\n\n* isort\n\n* mv f\n\n* changelog\n\n* format\n\n* move helper to parallel plugin\n\n* d\n\n* add world size\n\n* clean up\n\n* duplicate\n\n* activate ddp_sharded and tpu\n\n* set nvidia flags\n\n* remove unused colab var\n\n* use_tpu <-> on_tpu attrs\n\n* make some ddp_cpu and clusterplugin tests pass\n\n* Ref/accelerator connector (#5742)\n\n* final cleanup\n\nCo-authored-by: Adrian Wlchli <aedu.waelchli@gmail.com>\n\n* connector cleanup\n\nCo-authored-by: Adrian Wlchli <aedu.waelchli@gmail.com>\n\n* trainer cleanup\n\nCo-authored-by: Adrian Wlchli <aedu.waelchli@gmail.com>\n\n* accelerator cleanup + missing logic in accelerator connector\n\nCo-authored-by: Adrian Wlchli <aedu.waelchli@gmail.com>\n\n* add missing changes to callbacks\n\nCo-authored-by: Adrian Wlchli <aedu.waelchli@gmail.com>\n\n* reflect accelerator changes to lightning module\n\nCo-authored-by: Adrian Wlchli <aedu.waelchli@gmail.com>\n\n* clean cluster envs\n\nCo-authored-by: Adrian Wlchli <aedu.waelchli@gmail.com>\n\n* cleanup plugins\n\nCo-authored-by: Adrian Wlchli <aedu.waelchli@gmail.com>\n\n* add broadcasting\n\nCo-authored-by: Adrian Wlchli <aedu.waelchli@gmail.com>\n\n* yapf\n\n* remove plugin connector\n\nCo-authored-by: Adrian Wlchli <aedu.waelchli@gmail.com>\n\n* plugins\n\n* manual optimization\n\n* update optimizer routing\n\n* add rank to torchelastic\n\n* fix memory mixed precision\n\n* setstate on trainer for pickling in ddp spawn\n\n* add predict method\n\n* add back commented accelerator code\n\n* adapt test for sync_batch_norm to new plugin\n\n* fix deprecated tests\n\n* fix ddp cpu choice when no num_processes are given\n\n* yapf format\n\n* skip a memory test that cannot pass anymore\n\n* fix pickle error in spawn plugin\n\n* x\n\n* avoid\n\n* x\n\n* fix cyclic import in docs build\n\n* add support for sharded\n\n* update typing\n\n* add sharded and sharded_spawn to distributed types\n\n* make unwrap model default\n\n* refactor LightningShardedDataParallel similar to LightningDistributedDataParallel\n\n* update sharded spawn to reflect changes\n\n* update sharded to reflect changes\n\n* Merge 1.1.5 changes\n\n* fix merge\n\n* fix merge\n\n* yapf isort\n\n* fix merge\n\n* yapf isort\n\n* fix indentation in test\n\n* copy over reinit scheduler implementation from dev1.2\n\n* fix apex tracking calls with dev_debugger\n\n* reduce diff to dev1.2, clean up\n\n* fix trainer config test  when gpus>0 and num_processes >0 and ddp_cpu\n\n* sort plugin tests legacy/new\n\n* fix error handling for amp on cpu\n\n* fix merge\n\n\nfix merge\n\n\nfix merge\n\n* [Feat] Resolve manual_backward (#5837)\n\n* resolve manual_backward\n\n* resolve flake8\n\n* update\n\n* resolve for ddp_spawn\n\n* resolve flake8\n\n* resolve flake8\n\n* resolve flake8\n\nCo-authored-by: Ubuntu <ubuntu@ip-172-31-88-60.ec2.internal>\n\n* fix tests/accelerator tests on cpu\n\n* [BugFix] Resolve manual optimization (#5852)\n\n* resolve manual_optimization\n\n* update\n\n* update\n\nCo-authored-by: Ubuntu <ubuntu@ip-172-31-88-60.ec2.internal>\n\n* Remove copy trainer parameters to happen earlier within the loop and add safe guard to get ref model (#5856)\n\n* resovle a bug\n\n* Accelerator refactor sharded rpc (#5854)\n\n* rpc branch\n\n* merge\n\n* update handling of rpc\n\n* make devices etc. Optional in RPC\n\n* set devices etc. later if necessary\n\n* remove devices from sequential\n\n* make devices optional in rpc\n\n* fix import\n\n* uncomment everything\n\n* fix cluster selection\n\nCo-authored-by: Ubuntu <ubuntu@ip-172-31-88-60.ec2.internal>\n\n* resolve bug\n\n* fix assert in rpc test\n\n* resolve a test\n\n* fix docs compilation\n\n* accelerator refactor - fix for sharded parity test (#5866)\n\n* fix memory issue with ddp_spawn\n\n* x\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n* x\n\n* Remove DDP2 as this does not apply\n\n* Add missing pre optimizer hook to ensure lambda closure is called\n\n* fix apex docstring\n\n* [accelerator][BugFix] Resolve some test for 1 gpu (#5863)\n\n* update\n\n* revert init\n\n* resolve a bug\n\n* update\n\n* resolve flake8\n\n* update\n\n* update\n\n* update\n\n* revert init\n\n* resolve a bug\n\n* update\n\n* resolve flake8\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* revert init\n\n* resolve a bug\n\n* update\n\n* resolve flake8\n\n* update\n\n* update\n\n* update\n\n* revert init\n\n* update\n\n* resolve flake8\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* all_gather\n\n* update\n\n* make plugins work, add misconfig for RPC\n\n* update\n\n* update\n\n* remove breaking test\n\n* resolve some tests\n\n* resolve flake8\n\n* revert to ddp_spawn\n\nCo-authored-by: root <root@ip-172-31-88-60.ec2.internal>\nCo-authored-by: Ubuntu <ubuntu@ip-172-31-88-60.ec2.internal>\nCo-authored-by: Justus Schock <justus.schock@rwth-aachen.de>\n\n* yapf isort\n\n* resolve flake8\n\n* fix apex doctests\n\n* fix apex doctests 2\n\n* resolve docs\n\n* update drone\n\n* clean env\n\n* update\n\n* update\n\n* update\n\n* update\n\n* merge\n\n* Fix RPC related tests, clean out old API, update for new accelerator API [skip ci] (#5881)\n\n* Fix RPC related tests, clean out old API, update for new accelerator API\n\n* Move tests out of legacy folder, update paths and names\n\n* Update test_remove_1-4.py\n\n* Expose properties for tpu cores/gpus/num_gpus\n\n* Add root GPU property\n\n* Move properties to properties.py\n\n* move tests that were previously in drone\n\n* Fix root GPU property (#5908)\n\n* Move root GPU to property, remove horovod set as this is handled in horovod plugin, ensure we mock correctly to set GPU accelerator\n\n* Add missing tests back\n\n* fix best model path transfer when no checkpoint callback available\n\n* Fix setup hook order [wip] (#5858)\n\n* Call trainer setup hook before accelerator setup\n\n* Add test case\n\n* add new test\n\n* typo\n\n* fix callback order in test\n\nCo-authored-by: tchaton <thomas@grid.ai>\nCo-authored-by: Adrian Wlchli <aedu.waelchli@gmail.com>\n\n* rename ddp sequential -> rpc sequential for special test\n\n* revert\n\n* fix stupid merge problem\n\n* Use property in connector for sampler (#5913)\n\n* merge the import conflicts\n\n* fix spawning of processes in slurm\n\n* [wip] Fix some bugs for TPU [skip ci] (#5878)\n\n* fixed for single tpu\n\n* fixed spawn\n\n* fixed spawn\n\n* update\n\n* update\n\n* wip\n\n* resolve bugs\n\n* resolve bug\n\n* update on comment\n\n* removed decorator\n\n* resolve comments\n\n* set to 4\n\n* update\n\n* update\n\n* need cleaning\n\n* update\n\n* update\n\n* update\n\n* resolve flake8\n\n* resolve bugs\n\n* exclude broadcast\n\n* resolve bugs\n\n* change test\n\n* update\n\n* update\n\n* skip if meet fails\n\n* properly raise trace\n\n* update\n\n* add catch\n\n* wrap test\n\n* resolve typo\n\n* update\n\n* typo\n\nCo-authored-by: Lezwon Castelino <lezwon@gmail.com>\nCo-authored-by: Your Name <you@example.com>\n\n* resolve some tests\n\n* update\n\n* fix imports\n\n* update\n\n* resolve flake8\n\n* update azure pipeline\n\n* skip a sharded test on cpu that requires a gpu\n\n* resolve tpus\n\n* resolve bug\n\n* resolve flake8\n\n* update\n\n* updat utils\n\n* revert permission change on files\n\n* suggestions from carlos\n\nCo-authored-by: Carlos Mochol <carlossmocholi@gmail.com>\n\n* remove unrelated formatting changes\n\n* remove incomplete comment\n\n* Update pytorch_lightning/accelerators/__init__.py\n\nCo-authored-by: Carlos Mochol <carlossmocholi@gmail.com>\n\n* remove unrelated formatting change\n\n* add types\n\n* warn 1.7 ddp manual backward only if ddp kwarg unset\n\n* yapf + isort\n\n* pep8 unused imports\n\n* fix cyclic import in docs\n\n* Apply suggestions from code review\n\n* typer in accelerator.py\n\n* typo\n\n* Apply suggestions from code review\n\n* formatting\n\n* update on comments\n\n* update typo\n\n* Update pytorch_lightning/trainer/properties.py\n\nCo-authored-by: Adrian Wlchli <aedu.waelchli@gmail.com>\n\n* update\n\n* suggestion from code review\n\n* suggestion from code review\n\nCo-authored-by: Adrian Wlchli <aedu.waelchli@gmail.com>\nCo-authored-by: SeanNaren <sean@grid.ai>\nCo-authored-by: Jirka Borovec <jirka.borovec@seznam.cz>\nCo-authored-by: chaton <thomas@grid.ai>\nCo-authored-by: Ubuntu <ubuntu@ip-172-31-88-60.ec2.internal>\nCo-authored-by: Sean Naren <sean.narenthiran@gmail.com>\nCo-authored-by: root <root@ip-172-31-88-60.ec2.internal>\nCo-authored-by: Lezwon Castelino <lezwon@gmail.com>\nCo-authored-by: Your Name <you@example.com>\nCo-authored-by: Carlos Mochol <carlossmocholi@gmail.com>\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\nCo-authored-by: mergify[bot] <37929162+mergify[bot]@users.noreply.github.com>\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class MetricsHolder:",
            "else:",
            "current = torch.tensor(current, device=device, dtype=torch.float)",
            "",
            "-        if use_tpu and _TPU_AVAILABLE:",
            "+        if isinstance(current, torch.Tensor) and current.device.type == \"xla\":",
            "current = current.cpu()",
            "",
            "return current"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=boolean_operator), node=('call', None), position=0, insert_id=547439)",
            "Insert(target_node=ASTNode(type=boolean_operator), node=('comparison_operator', None), position=3, insert_id=547440)",
            "Update(target_node=ASTNode(type=identifier, text=use_tpu), value='isinstance')",
            "Move(target_node=IN(type=call), node=ASTNode(type=identifier, text=use_tpu), position=0)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=547441)",
            "Insert(target_node=IN(type=comparison_operator), node=('attribute', None), position=0, insert_id=547442)",
            "Insert(target_node=IN(type=comparison_operator), node=('==', '=='), position=1, insert_id=547443)",
            "Insert(target_node=IN(type=comparison_operator), node=('string', '\"xla\"'), position=2, insert_id=547444)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=547445)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'current'), position=1, insert_id=547446)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=547447)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=3, insert_id=547448)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=547449)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=547450)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=547451)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'type'), position=2, insert_id=547452)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=547453)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=547454)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Tensor'), position=2, insert_id=547455)",
            "Update(target_node=ASTNode(type=identifier, text=_TPU_AVAILABLE), value='current')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=_TPU_AVAILABLE), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=547456)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=547457)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 23,
        "number": 4478,
        "neg_line": [
            "-if use_tpu and _TPU_AVAILABLE:"
        ],
        "pos_line": [
            "+if isinstance(current, torch.Tensor) and current.device.type == \"xla\":"
        ],
        "core_change": "-if use_tpu and _TPU_AVAILABLE: +if isinstance(current, torch.Tensor) and current.device.type == \"xla\":",
        "core_API": "tensor"
    },
    {
        "commit_hash": "c95de29e31c13b7836fb55fdea57c761cc120650",
        "index": "e11d79e43..a3546206b 100644",
        "commit_message": ":pencil2: Fix typo (#9020)\n\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class TFEncoderLayer(tf.keras.layers.Layer):",
            "if self.normalize_before:",
            "x = self.final_layer_norm(x)",
            "x = self.activation_fn(self.fc1(x))",
            "-        x = tf.nn.dropout(x, rate=self.self.activation_dropout if training else 0)",
            "+        x = tf.nn.dropout(x, rate=self.activation_dropout if training else 0)",
            "x = self.fc2(x)",
            "x = tf.nn.dropout(x, rate=self.dropout if training else 0)",
            "x = residual + x"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 4483,
        "neg_line": [
            "-x = tf.nn.dropout(x, rate=self.self.activation_dropout if training else 0)"
        ],
        "pos_line": [
            "+x = tf.nn.dropout(x, rate=self.activation_dropout if training else 0)"
        ],
        "core_change": "-x = tf.nn.dropout(x, rate=self.self.activation_dropout if training else 0) +x = tf.nn.dropout(x, rate=self.activation_dropout if training else 0)",
        "core_API": "final_layer_norm"
    },
    {
        "commit_hash": "fec3112cbacf19cd211109b1bc0b33a502595aca",
        "index": "ab2dec09..feedf4b1 100644",
        "commit_message": "Update to PyTorch 1.9.0 (#2887)\n\n* Update to PyTorch 1.9.0\n\n* Update torch.cholesky, torch.symeig\n\n* Fix torch.tensordot, torch.qr, funsor dependency\n\n* Fix torch import, AutoGuideList usage\n\n* Ignore bug in PyTorch jit\n\n* Ignore tracer warnings\n\n* Replace torch.solve with torch.linalg.solve\n\n* Xfail vectorized markov funsor tests\n\n* Update funsor version\n\n* Switch MNIST mirrors\n\n* Pin pillow version\n\n* Bump torchvision version\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_mean_var(batch_shape):",
            "dim = 2",
            "loc = torch.randn(batch_shape + (dim,))",
            "A = torch.randn(batch_shape + (dim, dim + dim))",
            "-    scale_tril = A.matmul(A.transpose(-2, -1)).cholesky()",
            "+    scale_tril = torch.linalg.cholesky(A.matmul(A.transpose(-2, -1)))",
            "df = torch.randn(batch_shape).exp() + 4",
            "num_samples = 100000",
            "d = MultivariateStudentT(df, loc, scale_tril)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=677107)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=677108)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=677109)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=677110)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cholesky'), position=2, insert_id=677111)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=677112)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=call), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=677113)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=677114)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=677115)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'linalg'), position=2, insert_id=677116)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=cholesky))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 4484,
        "neg_line": [
            "-scale_tril = A.matmul(A.transpose(-2, -1)).cholesky()"
        ],
        "pos_line": [
            "+scale_tril = torch.linalg.cholesky(A.matmul(A.transpose(-2, -1)))"
        ],
        "core_change": "-scale_tril = A.matmul(A.transpose(-2, -1)).cholesky() +scale_tril = torch.linalg.cholesky(A.matmul(A.transpose(-2, -1)))",
        "core_API": "randn"
    },
    {
        "commit_hash": "b4a1c53cb1446d7cf8a646e2b90b60bd2d78a3b0",
        "index": "d52b147f..5d38407f 100644",
        "commit_message": "fix warper.compute_subpixel_step and _compute_projection to not crash\n\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DepthWarper(nn.Module):",
            "1.0 - delta_d)",
            "xy_p1 = self._compute_projection(self.width / 2, self.height / 2,",
            "1.0 + delta_d)",
            "-        dx = torch.norm((xy_p1 - xy_m1), 2, dim=2) / 2.0",
            "+        dx = torch.norm((xy_p1 - xy_m1), 2, dim=-1) / 2.0",
            "dxdd = dx / (delta_d)  # pixel*(1/meter)",
            "# half pixel sampling, we're interested in the min for all cameras",
            "return torch.min(0.5 / dxdd)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=keyword_argument), node=('unary_operator', '-1'), position=2, insert_id=475657)",
            "Delete(target_node=ASTNode(type=integer, text=2))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 4485,
        "neg_line": [
            "-dx = torch.norm((xy_p1 - xy_m1), 2, dim=2) / 2.0"
        ],
        "pos_line": [
            "+dx = torch.norm((xy_p1 - xy_m1), 2, dim=-1) / 2.0"
        ],
        "core_change": "-dx = torch.norm((xy_p1 - xy_m1), 2, dim=2) / 2.0 +dx = torch.norm((xy_p1 - xy_m1), 2, dim=-1) / 2.0",
        "core_API": "_compute_projection"
    },
    {
        "commit_hash": "5076675f8522d08949b8ff94e2ff61af05292b17",
        "index": "049df73..09e4cc8 100644",
        "commit_message": "Fix BN momentum in inception*\n\n",
        "file": "pretrained-models.pytorch.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class BasicConv2d(nn.Module):",
            "",
            "def __init__(self, in_planes, out_planes, kernel_size, stride, padding=0):",
            "super(BasicConv2d, self).__init__()",
            "-        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, bias=False) # verify bias false",
            "-        self.bn = nn.BatchNorm2d(out_planes, eps=0.001, momentum=0, affine=True)",
            "+        self.conv = nn.Conv2d(in_planes, out_planes,",
            "+                              kernel_size=kernel_size, stride=stride,",
            "+                              padding=padding, bias=False) # verify bias false",
            "+        self.bn = nn.BatchNorm2d(out_planes,",
            "+                                 eps=0.001, # value found in tensorflow",
            "+                                 momentum=0.1, # default pytorch value",
            "+                                 affine=True)",
            "self.relu = nn.ReLU(inplace=True)",
            "",
            "def forward(self, x):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=keyword_argument), node=('float', '0.1'), position=2, insert_id=1440548)",
            "Delete(target_node=ASTNode(type=integer, text=0))"
        ],
        "plus_line": 7,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 4487,
        "neg_line": [
            "-self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, bias=False) # verify bias false",
            "-self.bn = nn.BatchNorm2d(out_planes, eps=0.001, momentum=0, affine=True)"
        ],
        "pos_line": [
            "+self.conv = nn.Conv2d(in_planes, out_planes,",
            "+kernel_size=kernel_size, stride=stride,",
            "+padding=padding, bias=False) # verify bias false",
            "+self.bn = nn.BatchNorm2d(out_planes,",
            "+eps=0.001, # value found in tensorflow",
            "+momentum=0.1, # default pytorch value",
            "+affine=True)"
        ],
        "core_change": "-self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, bias=False) # verify bias false -self.bn = nn.BatchNorm2d(out_planes, eps=0.001, momentum=0, affine=True) +self.conv = nn.Conv2d(in_planes, out_planes, +kernel_size=kernel_size, stride=stride, +padding=padding, bias=False) # verify bias false +self.bn = nn.BatchNorm2d(out_planes, +eps=0.001, # value found in tensorflow +momentum=0.1, # default pytorch value +affine=True)",
        "core_API": "Conv2d"
    },
    {
        "commit_hash": "93e528979365be809f47540f56037f99581f2911",
        "index": "17946140..84be2012 100644",
        "commit_message": "Fix inference CI device error (#2824)\n\n\n",
        "file": "DeepSpeed.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestAutoTensorParallelism(DistributedTest):",
            "",
            "# We have to load these large models on CPU with pipeline because not",
            "# enough GPU memory",
            "-        pipe = pipeline(task, model=model, device=-1, framework=\"pt\")",
            "+        pipe = pipeline(task, model=model, device=torch.device(\"cpu\"), framework=\"pt\")",
            "bs_output = pipe(query, **inf_kwargs)",
            "",
            "pipe.model = deepspeed.init_inference(pipe.model,"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=keyword_argument), node=('call', None), position=2, insert_id=74744)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=74745)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=74746)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=74747)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=74748)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=74749)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=74750)",
            "Insert(target_node=IN(type=argument_list), node=('string', '\"cpu\"'), position=1, insert_id=74751)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=74752)",
            "Delete(target_node=ASTNode(type=unary_operator, text=-1))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 4490,
        "neg_line": [
            "-pipe = pipeline(task, model=model, device=-1, framework=\"pt\")"
        ],
        "pos_line": [
            "+pipe = pipeline(task, model=model, device=torch.device(\"cpu\"), framework=\"pt\")"
        ],
        "core_change": "-pipe = pipeline(task, model=model, device=-1, framework=\"pt\") +pipe = pipeline(task, model=model, device=torch.device(\"cpu\"), framework=\"pt\")",
        "core_API": "device"
    },
    {
        "commit_hash": "7f320e0eb8b167aef06a1753eed9e04f0cf5b519",
        "index": "497d9e7..088f4b4 100644",
        "commit_message": "added fixes\n\nPiperOrigin-RevId: 325186954\n\n",
        "file": "hub.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "\"source\": [",
            "\"model.compile(optimizer='adam',\\n\",",
            "\"              loss=tf.losses.BinaryCrossentropy(from_logits=True),\\n\",",
            "-        \"              metrics=['accuracy'])\"",
            "+        \"              metrics=[tf.metrics.BinaryAccuracy(threshold=0.0, name='accuracy')])\"",
            "]",
            "},",
            "{"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"              metrics=['accuracy'])\"), value='\"              metrics=[tf.metrics.BinaryAccuracy(threshold=0.0, name=\\'accuracy\\')])\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 4492,
        "neg_line": [
            "-\"              metrics=['accuracy'])\""
        ],
        "pos_line": [
            "+\"              metrics=[tf.metrics.BinaryAccuracy(threshold=0.0, name='accuracy')])\""
        ],
        "core_change": "-\"              metrics=['accuracy'])\" +\"              metrics=[tf.metrics.BinaryAccuracy(threshold=0.0, name='accuracy')])\"",
        "core_API": "compile"
    },
    {
        "commit_hash": "db92d23866a083b6a8107f38c112df7fa52c60e3",
        "index": "2a374497..22bac8e7 100644",
        "commit_message": "Remove type ignore from the codebase (#2030)\n\n* Remove type ignore from the codebase\n\n* undo `NamedTuple` property with `List[Self]`\n\n- This is causing `Segmentation fault (core dumped)` when running `mypy --cobertura-xml-report ./`\n\n* add TODO for next mypy release\n\n* fix typo\n\n* fix F401\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def random_intrinsics(low: Union[float, torch.Tensor], high: Union[float, torch.",
            "the random camera matrix with the shape of :math:`(1, 3, 3)`.",
            "\"\"\"",
            "sampler = torch.distributions.Uniform(low, high)",
            "-    fx, fy, cx, cy = (sampler.sample((1,)) for _ in range(4))",
            "+    fx, fy, cx, cy = (sampler.sample(torch.Size((1,))) for _ in range(4))",
            "zeros, ones = torch.zeros_like(fx), torch.ones_like(fx)",
            "camera_matrix: torch.Tensor = torch.cat([fx, zeros, cx, zeros, fy, cy, zeros, zeros, ones])",
            "return camera_matrix.view(1, 3, 3)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('(', '('), position=0, insert_id=395013)",
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=1, insert_id=395014)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=2, insert_id=395015)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=395016)",
            "Move(target_node=IN(type=call), node=ASTNode(type=argument_list), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=395017)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=395018)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Size'), position=2, insert_id=395019)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 4493,
        "neg_line": [
            "-fx, fy, cx, cy = (sampler.sample((1,)) for _ in range(4))"
        ],
        "pos_line": [
            "+fx, fy, cx, cy = (sampler.sample(torch.Size((1,))) for _ in range(4))"
        ],
        "core_change": "-fx, fy, cx, cy = (sampler.sample((1,)) for _ in range(4)) +fx, fy, cx, cy = (sampler.sample(torch.Size((1,))) for _ in range(4))",
        "core_API": "Uniform"
    },
    {
        "commit_hash": "1e0ccad60611f5e47d4eddbf950f120c30020dd9",
        "index": "0865d1d8..1ef0942e 100644",
        "commit_message": "Moving bbox-related functionality to bbox module (#1103)\n\n* Moving bbox-related functionality to bbox module\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* With transform boxes test moved to test/bbox\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix importing bbox methods across the library\n\n* With backward compatibility and deprecation warnings included\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Backward compatibility with fixed names\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* With lost import\n\n* With docs entry for bbox module\n\n* Naming changed from _3d to 3d\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class AugmentationSequential(ImageSequential):",
            ") -> torch.Tensor:",
            "if isinstance(module, GeometricAugmentationBase2D):",
            "transform = module.compute_inverse_transformation(module.get_transformation_matrix(input, param))",
            "-            input = transform_boxes(torch.as_tensor(transform, device=input.device, dtype=input.dtype), input, mode)",
            "+            input = transform_bbox(torch.as_tensor(transform, device=input.device, dtype=input.dtype), input, mode)",
            "return input",
            "",
            "def inverse_keypoints("
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=transform_boxes), value='transform_bbox')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 4494,
        "neg_line": [
            "-input = transform_boxes(torch.as_tensor(transform, device=input.device, dtype=input.dtype), input, mode)"
        ],
        "pos_line": [
            "+input = transform_bbox(torch.as_tensor(transform, device=input.device, dtype=input.dtype), input, mode)"
        ],
        "core_change": "-input = transform_boxes(torch.as_tensor(transform, device=input.device, dtype=input.dtype), input, mode) +input = transform_bbox(torch.as_tensor(transform, device=input.device, dtype=input.dtype), input, mode)",
        "core_API": "compute_inverse_transformation"
    },
    {
        "commit_hash": "eb3ada8bc4cf4c9b9b18aea17c36491d96b295ef",
        "index": "6dc2b60708..62747d0c28 100644",
        "commit_message": "Fixed failing tests for meta torch (#3652)\n\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def divide(",
            "x1, x2 = ivy.promote_types_of_inputs(x1, x2)",
            "ret = torch.div(x1, x2)",
            "if ivy.is_float_dtype(x1.dtype):",
            "-        ret = torch.tensor(ret, dtype=x1.dtype)",
            "+        ret = ret.to(x1.dtype)",
            "else:",
            "-        ret = torch.tensor(ret, dtype=ivy.default_float_dtype(as_native=True))",
            "+        ret = ret.to(ivy.default_float_dtype(as_native=True))",
            "return ret"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=torch), value='ret')",
            "Update(target_node=ASTNode(type=identifier, text=tensor), value='to')",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=attribute), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=torch), value='ret')",
            "Update(target_node=ASTNode(type=identifier, text=tensor), value='to')",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=call), position=1)",
            "Delete(target_node=ASTNode(type=identifier, text=ret))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=dtype))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=identifier, text=ret))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=dtype))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 16,
        "number": 4495,
        "neg_line": [
            "-ret = torch.tensor(ret, dtype=x1.dtype)",
            "-ret = torch.tensor(ret, dtype=ivy.default_float_dtype(as_native=True))"
        ],
        "pos_line": [
            "+ret = ret.to(x1.dtype)",
            "+ret = ret.to(ivy.default_float_dtype(as_native=True))"
        ],
        "core_change": "-ret = torch.tensor(ret, dtype=x1.dtype) +ret = ret.to(x1.dtype) -ret = torch.tensor(ret, dtype=ivy.default_float_dtype(as_native=True)) +ret = ret.to(ivy.default_float_dtype(as_native=True))",
        "core_API": "promote_types_of_inputs"
    },
    {
        "commit_hash": "9cb6fe4c93c17f1d4e327ea345cd1c653432c76c",
        "index": "5e87b573..0b70a973 100644",
        "commit_message": "Copy to local before loading checkpoint\n\nSummary: Follow-up for \"Fix FSDP optim state loading (#1819)\". Update for remote file systems.\n\nReviewed By: sshleifer\n\nDifferential Revision: D28088088\n\nfbshipit-source-id: 5d2f3ea5084fbbb21564d053317d2c07565cf2bc\n\n",
        "file": "fairseq.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Trainer(object):",
            "last_optim_state = state.get(\"last_optimizer_state\", None)",
            "if last_optim_state == -1:",
            "master_path = re.sub(\"shard[0-9]+\", \"shard0\", filename)",
            "-                    last_optim_state = torch.load(master_path, map_location='cpu')['last_optimizer_state']",
            "+                    local_master_path = PathManager.get_local_path(master_path)",
            "+                    last_optim_state = torch.load(local_master_path, map_location='cpu')['last_optimizer_state']",
            "",
            "# If doing zero_sharding, do not broadcast global optimizer",
            "# state. Later we will broadcast sharded states to each rank"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=4, insert_id=207072)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=207073)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'local_master_path'), position=0, insert_id=207074)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=207075)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=207076)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=207077)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=207078)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'PathManager'), position=0, insert_id=207079)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=207080)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'get_local_path'), position=2, insert_id=207081)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=207082)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'master_path'), position=1, insert_id=207083)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=207084)",
            "Update(target_node=ASTNode(type=identifier, text=master_path), value='local_master_path')"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 14,
        "number": 4498,
        "neg_line": [
            "-last_optim_state = torch.load(master_path, map_location='cpu')['last_optimizer_state']"
        ],
        "pos_line": [
            "+local_master_path = PathManager.get_local_path(master_path)",
            "+last_optim_state = torch.load(local_master_path, map_location='cpu')['last_optimizer_state']"
        ],
        "core_change": "-last_optim_state = torch.load(master_path, map_location='cpu')['last_optimizer_state'] +local_master_path = PathManager.get_local_path(master_path) +last_optim_state = torch.load(local_master_path, map_location='cpu')['last_optimizer_state']",
        "core_API": "get"
    },
    {
        "commit_hash": "3e050bdb4ca4b65ea39c8f70293b3b8ce1c40ed7",
        "index": "39ccf25..ded3918 100644",
        "commit_message": "Fix pylint errors\n\n",
        "file": "seq2seq.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class ImageCaptioningInputPipeline(InputPipeline):",
            "self.params[\"caption_tokens_field\"]),",
            "\"target_len\": tfexample_decoder.ItemHandlerCallback(",
            "keys=[self.params[\"caption_tokens_field\"]],",
            "-            func=lambda dict: tf.size(dict[self.params[\"caption_tokens_field\"]]))",
            "+            func=lambda dict: tf.size(",
            "+                dict[self.params[\"caption_tokens_field\"]]))",
            "}",
            "",
            "decoder = TFSEquenceExampleDecoder("
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 4499,
        "neg_line": [
            "-func=lambda dict: tf.size(dict[self.params[\"caption_tokens_field\"]]))"
        ],
        "pos_line": [
            "+func=lambda dict: tf.size(",
            "+dict[self.params[\"caption_tokens_field\"]]))"
        ],
        "core_change": "-func=lambda dict: tf.size(dict[self.params[\"caption_tokens_field\"]])) +func=lambda dict: tf.size( +dict[self.params[\"caption_tokens_field\"]]))",
        "core_API": "ItemHandlerCallback"
    },
    {
        "commit_hash": "47d663f0fa97919cce60a64ffa977d9aed6cff45",
        "index": "b0058765..8266a9a0 100644",
        "commit_message": "Add docstrings of core modules and methods (#3120)\n\n* Add docstrings of core modules and methods\n\n* Update docs and fix comments\n\n* Complete docstrings\n\n* Resolve comments\n\n* reformat docstrings\n\n* resolve comments\n",
        "file": "mmdetection.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "from .base_bbox_coder import BaseBBoxCoder",
            "",
            "@BBOX_CODERS.register_module()",
            "class PseudoBBoxCoder(BaseBBoxCoder):",
            "+    \"\"\"Pseudo bounding box coder\"\"\"",
            "",
            "def __init__(self, **kwargs):",
            "super(BaseBBoxCoder, self).__init__(**kwargs)",
            "",
            "def encode(self, bboxes, gt_bboxes):",
            "+        \"\"\"torch.Tensor: return the given ``bboxes``\"\"\"",
            "return gt_bboxes",
            "",
            "def decode(self, bboxes, pred_bboxes):",
            "+        \"\"\"torch.Tensor: return the given ``pred_bboxes``\"\"\"",
            "return pred_bboxes"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=function_definition), node=('block', None), position=4, insert_id=636617)",
            "Insert(target_node=ASTNode(type=function_definition), node=('block', None), position=4, insert_id=636618)",
            "Insert(target_node=ASTNode(type=class_definition), node=('block', None), position=4, insert_id=636619)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=636620)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=636621)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=636622)",
            "Insert(target_node=IN(type=expression_statement), node=('string', '\"\"\"torch.Tensor: return the given ``bboxes``\"\"\"'), position=0, insert_id=636623)",
            "Insert(target_node=IN(type=expression_statement), node=('string', '\"\"\"torch.Tensor: return the given ``pred_bboxes``\"\"\"'), position=0, insert_id=636624)",
            "Insert(target_node=IN(type=expression_statement), node=('string', '\"\"\"Pseudo bounding box coder\"\"\"'), position=0, insert_id=636625)",
            "Delete(target_node=ASTNode(type=block, text=))",
            "Delete(target_node=ASTNode(type=block, text=))",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 3,
        "minus_line": 0,
        "AST_diff_line": 12,
        "number": 4501,
        "neg_line": [],
        "pos_line": [
            "+\"\"\"Pseudo bounding box coder\"\"\"",
            "+\"\"\"torch.Tensor: return the given ``bboxes``\"\"\"",
            "+\"\"\"torch.Tensor: return the given ``pred_bboxes``\"\"\""
        ],
        "core_change": "+\"\"\"Pseudo bounding box coder\"\"\" +\"\"\"torch.Tensor: return the given ``bboxes``\"\"\" +\"\"\"torch.Tensor: return the given ``pred_bboxes``\"\"\"",
        "core_API": "register_module"
    },
    {
        "commit_hash": "9b6385488617ee99188b60c0525cbe4ec7e8f2a0",
        "index": "34204b12..39cb7051 100644",
        "commit_message": "Improve reproduceability 2/3 (#1906)\n\n* [Repro] Correct reproducability\n\n* up\n\n* up\n\n* uP\n\n* up\n\n* need better image\n\n* allow conversion from no state dict checkpoints\n\n* up\n\n* up\n\n* up\n\n* up\n\n* check tensors\n\n* check tensors\n\n* check tensors\n\n* check tensors\n\n* next try\n\n* up\n\n* up\n\n* better name\n\n* up\n\n* up\n\n* Apply suggestions from code review\n\n* correct more\n\n* up\n\n* replace all torch randn\n\n* fix\n\n* correct\n\n* correct\n\n* finish\n\n* fix more\n\n* up\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class PNDMPipeline(DiffusionPipeline):",
            "# the official paper: https://arxiv.org/pdf/2202.09778.pdf",
            "",
            "# Sample gaussian noise to begin loop",
            "-        image = torch.randn(",
            "+        image = randn_tensor(",
            "(batch_size, self.unet.in_channels, self.unet.sample_size, self.unet.sample_size),",
            "generator=generator,",
            "+            device=self.device,",
            ")",
            "-        image = image.to(self.device)",
            "",
            "self.scheduler.set_timesteps(num_inference_steps)",
            "for t in self.progress_bar(self.scheduler.timesteps):"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=torch), value='randn_tensor')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=torch), position=0)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=91784)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=6, insert_id=91785)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=91786)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type==, text==), position=1)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=attribute), position=2)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=randn))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=identifier, text=image))",
            "Delete(target_node=ASTNode(type=identifier, text=image))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=to))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 21,
        "number": 4502,
        "neg_line": [
            "-image = torch.randn(",
            "-image = image.to(self.device)"
        ],
        "pos_line": [
            "+image = randn_tensor(",
            "+device=self.device,"
        ],
        "core_change": "-image = torch.randn( +image = randn_tensor( +device=self.device, -image = image.to(self.device)",
        "core_API": "randn"
    },
    {
        "commit_hash": "66d4b129903058365f6cf1d4fb4ac73814ca3bfc",
        "index": "e7a5837b..7d834575 100644",
        "commit_message": "Fix examples and tutorials to use the updated tensor API (#886)\n\n* Fix examples and tutorials to use the updated tensor API\n\n* small fixes\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def load_data():",
            "(X_np, Y), _ = multi_mnist(inpath, max_digits=2, canvas_size=50, seed=42)",
            "X_np = X_np.astype(np.float32)",
            "X_np /= 255.0",
            "-    X = Variable(torch.from_numpy(X_np))",
            "+    X = torch.from_numpy(X_np)",
            "# Using FloatTensor to allow comparison with values sampled from",
            "# Bernoulli.",
            "counts = torch.FloatTensor([len(objs) for objs in Y])"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=IN(type=root), node=ASTNode(type=call), position=0)",
            "Delete(target_node=ASTNode(type=identifier, text=Variable))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 4503,
        "neg_line": [
            "-X = Variable(torch.from_numpy(X_np))"
        ],
        "pos_line": [
            "+X = torch.from_numpy(X_np)"
        ],
        "core_change": "-X = Variable(torch.from_numpy(X_np)) +X = torch.from_numpy(X_np)",
        "core_API": "astype"
    },
    {
        "commit_hash": "e94d5bff7e4d761907eb8c2e99334a0f116d186d",
        "index": "7412a8ec05..a05dce28a3 100644",
        "commit_message": "Added Bfloat16Finfo to tensorflow and fixed failing tests for dtype finfo (#4577)\n\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def finfo(type: Union[DType, str, tf.Tensor, tf.Variable]) -> Finfo:",
            "if isinstance(type, tf.Tensor):",
            "type = type.dtype",
            "if ivy.as_native_dtype(type) == tf.bfloat16:",
            "-        return Finfo(tf.experimental.numpy.finfo(tf.float32))",
            "+        return Finfo(Bfloat16Finfo())",
            "return Finfo(tf.experimental.numpy.finfo(ivy.as_native_dtype(type)))"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tf), value='Bfloat16Finfo')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=tf), position=0)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=experimental))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=numpy))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=finfo))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=float32))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 15,
        "number": 4507,
        "neg_line": [
            "-return Finfo(tf.experimental.numpy.finfo(tf.float32))"
        ],
        "pos_line": [
            "+return Finfo(Bfloat16Finfo())"
        ],
        "core_change": "-return Finfo(tf.experimental.numpy.finfo(tf.float32)) +return Finfo(Bfloat16Finfo())",
        "core_API": "as_native_dtype"
    },
    {
        "commit_hash": "258844720a9bccd326c7b33735f7f81c2d483630",
        "index": "614f18c4..07e22c5c 100644",
        "commit_message": "Fix segment_ops to use int32 when calling scatter_nd. This is needed because scatter_nd does not support boolean updates when running on GPU.\n\nPiperOrigin-RevId: 268456498\n\n",
        "file": "tf-quant-finance.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def segment_diff(x,",
            "",
            "needs_fix = tf.scatter_nd(",
            "fix_indices,",
            "-        tf.reshape(tf.ones_like(fix_indices, dtype=tf.bool), [-1]),",
            "+        # Unfortunately, scatter_nd doesn't support bool on GPUs so we need to",
            "+        # do ints here and then convert to bool.",
            "+        tf.reshape(tf.ones_like(fix_indices, dtype=tf.int32), [-1]),",
            "shape=tf.shape(x))",
            "# If exclusive is False, then needs_fix means we need to replace the values",
            "# in raw_diffs at those locations with the values in x.",
            "+    needs_fix = tf.cast(needs_fix, dtype=tf.bool)",
            "if not exclusive:",
            "return tf.where(needs_fix, x, raw_diffs)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2348724)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=argument_list), position=1)",
            "Insert(target_node=IN(type=attribute), node=('call', None), position=0, insert_id=2348725)",
            "Insert(target_node=IN(type=attribute), node=('ERROR', None), position=1, insert_id=2348726)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=2, insert_id=2348727)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cast'), position=3, insert_id=2348728)",
            "Update(target_node=ASTNode(type=identifier, text=fix_indices), value='needs_fix')",
            "Move(target_node=IN(type=call), node=ASTNode(type=attribute), position=0)",
            "Move(target_node=IN(type=call), node=ASTNode(type=argument_list), position=1)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'needs_fix'), position=0, insert_id=2348729)",
            "Insert(target_node=IN(type=ERROR), node=('=', '='), position=1, insert_id=2348730)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'tf'), position=2, insert_id=2348731)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=2348732)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2348733)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'fix_indices'), position=1, insert_id=2348734)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=2348735)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=3, insert_id=2348736)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=2348737)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=2348738)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2348739)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=2348740)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2348741)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2348742)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'int32'), position=2, insert_id=2348743)"
        ],
        "plus_line": 4,
        "minus_line": 1,
        "AST_diff_line": 24,
        "number": 4508,
        "neg_line": [
            "-tf.reshape(tf.ones_like(fix_indices, dtype=tf.bool), [-1]),"
        ],
        "pos_line": [
            "+# Unfortunately, scatter_nd doesn't support bool on GPUs so we need to",
            "+# do ints here and then convert to bool.",
            "+tf.reshape(tf.ones_like(fix_indices, dtype=tf.int32), [-1]),",
            "+needs_fix = tf.cast(needs_fix, dtype=tf.bool)"
        ],
        "core_change": "-tf.reshape(tf.ones_like(fix_indices, dtype=tf.bool), [-1]), +# Unfortunately, scatter_nd doesn't support bool on GPUs so we need to +# do ints here and then convert to bool. +tf.reshape(tf.ones_like(fix_indices, dtype=tf.int32), [-1]), +needs_fix = tf.cast(needs_fix, dtype=tf.bool)",
        "core_API": "scatter_nd"
    },
    {
        "commit_hash": "d5319793c47326655cf25025ceb13a97afa00aad",
        "index": "148bc2bd1..7c2c6f460 100644",
        "commit_message": "Fix BERT\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class BertModel(BertPreTrainedModel):",
            "device = input_ids.device if input_ids is not None else inputs_embeds.device",
            "",
            "if attention_mask is None:",
            "-            attention_mask = torch.ones(input_shape)",
            "+            attention_mask = torch.ones(input_shape, device=device)",
            "if encoder_attention_mask is None:",
            "-            encoder_attention_mask = torch.ones(input_shape)",
            "+            encoder_attention_mask = torch.ones(input_shape, device=device)",
            "if token_type_ids is None:",
            "-            token_type_ids = torch.zeros(input_shape, dtype=torch.long)",
            "+            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)",
            "",
            "# We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]",
            "# ourselves in which case we just need to make it broadcastable to all heads."
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1245176)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1245177)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1245178)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'ones'), position=2, insert_id=1245179)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1245180)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1245181)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1245182)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1245183)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=1245184)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=1245185)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=1245186)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1245187)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=2, insert_id=1245188)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=1245189)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1245190)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=2, insert_id=1245191)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=1245192)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1245193)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=2, insert_id=1245194)",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=ones))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 24,
        "number": 4510,
        "neg_line": [
            "-attention_mask = torch.ones(input_shape)",
            "-encoder_attention_mask = torch.ones(input_shape)",
            "-token_type_ids = torch.zeros(input_shape, dtype=torch.long)"
        ],
        "pos_line": [
            "+attention_mask = torch.ones(input_shape, device=device)",
            "+encoder_attention_mask = torch.ones(input_shape, device=device)",
            "+token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)"
        ],
        "core_change": "-attention_mask = torch.ones(input_shape) +attention_mask = torch.ones(input_shape, device=device) -encoder_attention_mask = torch.ones(input_shape) +encoder_attention_mask = torch.ones(input_shape, device=device) -token_type_ids = torch.zeros(input_shape, dtype=torch.long) +token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)",
        "core_API": "ones"
    },
    {
        "commit_hash": "4c293283636e5992826f450f1a045b9f83d9f5d1",
        "index": "c6b038e..e4d1323 100644",
        "commit_message": "fix frequency in rotary vision transformer\n\n",
        "file": "vit-pytorch.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class AxialRotaryEmbedding(nn.Module):",
            "def __init__(self, dim, max_freq = 10):",
            "super().__init__()",
            "self.dim = dim",
            "-        scales = torch.logspace(1., log(max_freq / 2) / log(2), self.dim // 4, base = 2)",
            "+        scales = torch.logspace(0., log(max_freq / 2) / log(2), self.dim // 4, base = 2)",
            "self.register_buffer('scales', scales)",
            "",
            "def forward(self, x):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=float, text=1.), value='0.')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 4511,
        "neg_line": [
            "-scales = torch.logspace(1., log(max_freq / 2) / log(2), self.dim // 4, base = 2)"
        ],
        "pos_line": [
            "+scales = torch.logspace(0., log(max_freq / 2) / log(2), self.dim // 4, base = 2)"
        ],
        "core_change": "-scales = torch.logspace(1., log(max_freq / 2) / log(2), self.dim // 4, base = 2) +scales = torch.logspace(0., log(max_freq / 2) / log(2), self.dim // 4, base = 2)",
        "core_API": "logspace"
    },
    {
        "commit_hash": "de4159a318f9a4f2f474733306c00ac326e643a7",
        "index": "364b51502..da25fa582 100755",
        "commit_message": "More TF int dtype fixes (#20384)\n\n* Add a test to ensure int dummy inputs are int64\n\n* Move the test into the existing int64 test and update a lot of existing dummies\n\n* Fix remaining dummies\n\n* Fix remaining dummies\n\n* Test for int64 serving sigs as well\n\n* Update core tests to use tf.int64\n\n* Add better messages to the assertions\n\n* Update all serving sigs to int64\n\n* More sneaky hiding tf.int32s\n\n* Add an optional int32 signature in save_pretrained\n\n* make fixup\n\n* Add Amy's suggestions\n\n* Switch all serving sigs back to tf.int32\n\n* Switch all dummies to tf.int32\n\n* Adjust tests to check for tf.int32 instead of tf.int64\n\n* Fix base dummy_inputs dtype\n\n* Start casting to tf.int32 in input_processing\n\n* Change dtype for unpack_inputs test\n\n* Add proper tf.int32 test\n\n* Make the alternate serving signature int64\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFSpeech2TextPreTrainedModel(TFPreTrainedModel):",
            "input_signature=[",
            "{",
            "\"input_features\": tf.TensorSpec((None, None, None), tf.float32, name=\"input_features\"),",
            "-                \"attention_mask\": tf.TensorSpec((None, None), tf.int64, name=\"attention_mask\"),",
            "-                \"decoder_input_ids\": tf.TensorSpec((None, None), tf.int64, name=\"decoder_input_ids\"),",
            "-                \"decoder_attention_mask\": tf.TensorSpec((None, None), tf.int64, name=\"decoder_attention_mask\"),",
            "+                \"attention_mask\": tf.TensorSpec((None, None), tf.int32, name=\"attention_mask\"),",
            "+                \"decoder_input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"decoder_input_ids\"),",
            "+                \"decoder_attention_mask\": tf.TensorSpec((None, None), tf.int32, name=\"decoder_attention_mask\"),",
            "}",
            "]",
            ")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2358159)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2358160)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2358161)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'TensorSpec'), position=2, insert_id=2358162)",
            "Insert(target_node=ASTNode(type=argument_list), node=('tuple', None), position=1, insert_id=2358163)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=tuple), position=1)",
            "Insert(target_node=IN(type=tuple), node=('(', '('), position=0, insert_id=2358164)",
            "Insert(target_node=IN(type=tuple), node=('none', 'None'), position=1, insert_id=2358165)",
            "Insert(target_node=IN(type=tuple), node=(',', ','), position=2, insert_id=2358166)",
            "Insert(target_node=IN(type=tuple), node=('none', 'None'), position=3, insert_id=2358167)",
            "Insert(target_node=IN(type=tuple), node=(')', ')'), position=4, insert_id=2358168)",
            "Update(target_node=ASTNode(type=identifier, text=int64), value='int32')",
            "Update(target_node=ASTNode(type=identifier, text=int64), value='int32')",
            "Update(target_node=ASTNode(type=identifier, text=int64), value='int32')",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=TensorSpec))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=none, text=None))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=none, text=None))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=tuple))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 25,
        "number": 4515,
        "neg_line": [
            "-\"attention_mask\": tf.TensorSpec((None, None), tf.int64, name=\"attention_mask\"),",
            "-\"decoder_input_ids\": tf.TensorSpec((None, None), tf.int64, name=\"decoder_input_ids\"),",
            "-\"decoder_attention_mask\": tf.TensorSpec((None, None), tf.int64, name=\"decoder_attention_mask\"),"
        ],
        "pos_line": [
            "+\"attention_mask\": tf.TensorSpec((None, None), tf.int32, name=\"attention_mask\"),",
            "+\"decoder_input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"decoder_input_ids\"),",
            "+\"decoder_attention_mask\": tf.TensorSpec((None, None), tf.int32, name=\"decoder_attention_mask\"),"
        ],
        "core_change": "-\"attention_mask\": tf.TensorSpec((None, None), tf.int64, name=\"attention_mask\"), -\"decoder_input_ids\": tf.TensorSpec((None, None), tf.int64, name=\"decoder_input_ids\"), -\"decoder_attention_mask\": tf.TensorSpec((None, None), tf.int64, name=\"decoder_attention_mask\"), +\"attention_mask\": tf.TensorSpec((None, None), tf.int32, name=\"attention_mask\"), +\"decoder_input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"decoder_input_ids\"), +\"decoder_attention_mask\": tf.TensorSpec((None, None), tf.int32, name=\"decoder_attention_mask\"),",
        "core_API": "TensorSpec"
    },
    {
        "commit_hash": "2e7e4280aa6f380a4e3afad6524295a17901c56c",
        "index": "4e4b0d963..6bc306a6e 100644",
        "commit_message": "Traced models serialization and torchscripting fix (#17206)\n\n* Fix torch.jit.script and pickling issues\n\n* Fix get_attr issues\n\n* Fix import in function\n\n* Fix GPT-J and T5 tracing for torch=1.11\n\n* Gate graph surgery on torch version\n\n* Modeling minor changes to enable TorchScripting\n\n* Model serialization / deserialization test\n\n* Remove _assert_is_none users\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "functional",
        "change": [
            "class MobileBertEmbeddings(nn.Module):",
            "# dimensional output.",
            "inputs_embeds = torch.cat(",
            "[",
            "-                    nn.functional.pad(inputs_embeds[:, 1:], [0, 0, 0, 1, 0, 0], value=0),",
            "+                    nn.functional.pad(inputs_embeds[:, 1:], [0, 0, 0, 1, 0, 0], value=0.0),",
            "inputs_embeds,",
            "-                    nn.functional.pad(inputs_embeds[:, :-1], [0, 0, 1, 0, 0, 0], value=0),",
            "+                    nn.functional.pad(inputs_embeds[:, :-1], [0, 0, 1, 0, 0, 0], value=0.0),",
            "],",
            "dim=2,",
            ")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1755026)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1755027)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1755028)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'pad'), position=2, insert_id=1755029)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'nn'), position=0, insert_id=1755030)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1755031)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'functional'), position=2, insert_id=1755032)",
            "Insert(target_node=ASTNode(type=keyword_argument), node=('float', '0.0'), position=2, insert_id=1755033)",
            "Insert(target_node=ASTNode(type=keyword_argument), node=('float', '0.0'), position=2, insert_id=1755034)",
            "Delete(target_node=ASTNode(type=integer, text=0))",
            "Delete(target_node=ASTNode(type=identifier, text=nn))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=functional))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=pad))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=integer, text=0))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 19,
        "number": 4518,
        "neg_line": [
            "-nn.functional.pad(inputs_embeds[:, 1:], [0, 0, 0, 1, 0, 0], value=0),",
            "-nn.functional.pad(inputs_embeds[:, :-1], [0, 0, 1, 0, 0, 0], value=0),"
        ],
        "pos_line": [
            "+nn.functional.pad(inputs_embeds[:, 1:], [0, 0, 0, 1, 0, 0], value=0.0),",
            "+nn.functional.pad(inputs_embeds[:, :-1], [0, 0, 1, 0, 0, 0], value=0.0),"
        ],
        "core_change": "-nn.functional.pad(inputs_embeds[:, 1:], [0, 0, 0, 1, 0, 0], value=0), +nn.functional.pad(inputs_embeds[:, 1:], [0, 0, 0, 1, 0, 0], value=0.0), -nn.functional.pad(inputs_embeds[:, :-1], [0, 0, 1, 0, 0, 0], value=0), +nn.functional.pad(inputs_embeds[:, :-1], [0, 0, 1, 0, 0, 0], value=0.0),",
        "core_API": "cat"
    },
    {
        "commit_hash": "87eb2a8df326148df6d704044952e49b598668e5",
        "index": "f1e79052..4f0dc4e1 100644",
        "commit_message": "Fix GPU device placement when calling tensor.cuda() (#624)\n\n* Correctly set cuda device when constructing cuda tensors\n\n* Fix .enumerate_support() methods\n\n* Revert to older invocation of torch.arange(-, -)\n\n* Support pytorch 0.2 invocation of .expand()\n\n* Fix type of Bernoulli sample\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Bernoulli(Distribution):",
            "sample.",
            ":rtype: torch.autograd.Variable.",
            "\"\"\"",
            "-        return Variable(torch.stack([torch.Tensor([t]).expand_as(self.ps) for t in [0, 1]]))",
            "+        result = torch.arange(0, 2)  # TODO Convert to LongTensor or ByteTensor.",
            "+        result = result.view((-1,) + (1,) * self.ps.dim()).expand((2,) + self.ps.size())",
            "+        if self.ps.is_cuda:",
            "+            result = result.cuda(self.ps.get_device())",
            "+        return Variable(result)",
            "",
            "def analytic_mean(self):",
            "\"\"\""
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"\"\"\n        return Variable(torch.stack([torch.Tensor([t]).expand_as(self.ps) for t in [0, 1]]))\n\ndef analytic_mean(self):\n\"\"\"), value='\"\"\"\\n        result = torch.arange(0, 2)  # TODO Convert to LongTensor or ByteTensor.\\n        result = result.view((-1,) + (1,) * self.ps.dim()).expand((2,) + self.ps.size())\\n        if self.ps.is_cuda:\\n            result = result.cuda(self.ps.get_device())\\n        return Variable(result)\\n\\ndef analytic_mean(self):\\n\"\"\"')"
        ],
        "plus_line": 5,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 4520,
        "neg_line": [
            "-return Variable(torch.stack([torch.Tensor([t]).expand_as(self.ps) for t in [0, 1]]))"
        ],
        "pos_line": [
            "+result = torch.arange(0, 2)  # TODO Convert to LongTensor or ByteTensor.",
            "+result = result.view((-1,) + (1,) * self.ps.dim()).expand((2,) + self.ps.size())",
            "+if self.ps.is_cuda:",
            "+result = result.cuda(self.ps.get_device())",
            "+return Variable(result)"
        ],
        "core_change": "-return Variable(torch.stack([torch.Tensor([t]).expand_as(self.ps) for t in [0, 1]])) +result = torch.arange(0, 2)  # TODO Convert to LongTensor or ByteTensor. +result = result.view((-1,) + (1,) * self.ps.dim()).expand((2,) + self.ps.size()) +if self.ps.is_cuda: +result = result.cuda(self.ps.get_device()) +return Variable(result)",
        "core_API": "stack"
    },
    {
        "commit_hash": "26ae658301874aadefcea4b2463f31d6fad28efe",
        "index": "372c833a..27adbb92 100644",
        "commit_message": "GH-1624: fix numpy warning\n\n",
        "file": "flair.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class FastTextEmbeddings(TokenEmbeddings):",
            "word_embedding = np.zeros(self.embedding_length, dtype=\"float\")",
            "",
            "word_embedding = torch.tensor(",
            "-            word_embedding, device=flair.device, dtype=torch.float",
            "+            word_embedding.tolist(), device=flair.device, dtype=torch.float",
            ")",
            "return word_embedding"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=1, insert_id=237501)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=237502)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=237503)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=word_embedding), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=237504)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tolist'), position=2, insert_id=237505)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=237506)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=237507)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 4521,
        "neg_line": [
            "-word_embedding, device=flair.device, dtype=torch.float"
        ],
        "pos_line": [
            "+word_embedding.tolist(), device=flair.device, dtype=torch.float"
        ],
        "core_change": "-word_embedding, device=flair.device, dtype=torch.float +word_embedding.tolist(), device=flair.device, dtype=torch.float",
        "core_API": "zeros"
    },
    {
        "commit_hash": "e47f59865fe6deaeb08a8580dd581cf070ba6e74",
        "index": "f9ade3d2..78b32368 100644",
        "commit_message": "[Fix] Fixes jit tests in feature for pytorch 1.6 (#858)\n\n* fix test jit issues in feature module\n\n* add github actions job for fast test pytorch versions\n\n* fixed orientation module mypy issues\n\n* add torch.jit.annotation for mypy issues\n\n* remove import cast\n\n* preset self.angle_detector as nn.Module\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class HardNet(nn.Module):",
            "# training totally unstable.",
            "return (x - mp.detach()) / (sp.detach() + eps)",
            "",
            "-    def forward(self, input: torch.Tensor) -> torch.Tensor:   # type: ignore",
            "+    def forward(self, input: torch.Tensor) -> torch.Tensor:",
            "x_norm: torch.Tensor = self._normalize_input(input)",
            "x_features: torch.Tensor = self.features(x_norm)",
            "x_out = x_features.view(x_features.size(0), -1)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 4524,
        "neg_line": [
            "-def forward(self, input: torch.Tensor) -> torch.Tensor:   # type: ignore"
        ],
        "pos_line": [
            "+def forward(self, input: torch.Tensor) -> torch.Tensor:"
        ],
        "core_change": "-def forward(self, input: torch.Tensor) -> torch.Tensor:   # type: ignore +def forward(self, input: torch.Tensor) -> torch.Tensor:",
        "core_API": "detach"
    },
    {
        "commit_hash": "22ccc43670dac93eb7fe81520a84cf3979d05693",
        "index": "b915bb49e6..04951f978b 100644",
        "commit_message": "[RLlib] DQN torch version. (#7597)\n\n* Fix.\n\n* Rollback.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* Fix.\n\n* Fix.\n\n* Fix.\n\n* Fix.\n\n* Fix.\n\n* WIP.\n\n* WIP.\n\n* Fix.\n\n* Test case fixes.\n\n* Test case fixes and LINT.\n\n* Test case fixes and LINT.\n\n* Rollback.\n\n* WIP.\n\n* WIP.\n\n* Test case fixes.\n\n* Fix.\n\n* Fix.\n\n* Fix.\n\n* Add regression test for DQN w/ param noise.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* Comment\n\n* Regression test case.\n\n* WIP.\n\n* WIP.\n\n* LINT.\n\n* LINT.\n\n* WIP.\n\n* Fix.\n\n* Fix.\n\n* Fix.\n\n* LINT.\n\n* Fix (SAC does currently not support eager).\n\n* Fix.\n\n* WIP.\n\n* LINT.\n\n* Update rllib/evaluation/sampler.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* Update rllib/evaluation/sampler.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* Update rllib/utils/exploration/exploration.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* Update rllib/utils/exploration/exploration.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* WIP.\n\n* WIP.\n\n* Fix.\n\n* LINT.\n\n* LINT.\n\n* Fix and LINT.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* Fix.\n\n* LINT.\n\n* Fix.\n\n* Fix and LINT.\n\n* Update rllib/utils/exploration/exploration.py\n\n* Update rllib/policy/dynamic_tf_policy.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* Update rllib/policy/dynamic_tf_policy.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* Update rllib/policy/dynamic_tf_policy.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* Fixes.\n\n* WIP.\n\n* LINT.\n\n* Fixes and LINT.\n\n* LINT and fixes.\n\n* LINT.\n\n* Move action_dist back into torch extra_action_out_fn and LINT.\n\n* Working SimpleQ learning cartpole on both torch AND tf.\n\n* Working Rainbow learning cartpole on tf.\n\n* Working Rainbow learning cartpole on tf.\n\n* WIP.\n\n* LINT.\n\n* LINT.\n\n* Update docs and add torch to APEX test.\n\n* LINT.\n\n* Fix.\n\n* LINT.\n\n* Fix.\n\n* Fix.\n\n* Fix and docstrings.\n\n* Fix broken RLlib tests in master.\n\n* Split BAZEL learning tests into cartpole and pendulum (reached the 60min barrier).\n\n* Fix error_outputs option in BAZEL for RLlib regression tests.\n\n* Fix.\n\n* Tune param-noise tests.\n\n* LINT.\n\n* Fix.\n\n* Fix.\n\n* test\n\n* test\n\n* test\n\n* Fix.\n\n* Fix.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* LINT.\n\n* WIP.\n\nCo-authored-by: Eric Liang <ekhliang@gmail.com>\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "layers",
        "change": [
            "def do_test_log_likelihood(run,",
            "layer_key[0])])",
            "else:",
            "expected_mean_logstd = fc(",
            "-                        fc(obs_batch,",
            "-                           vars[\"_hidden_layers.0._model.0.weight\"]),",
            "-                        vars[\"_logits._model.0.weight\"])",
            "+                        fc(",
            "+                            obs_batch,",
            "+                            np.transpose(",
            "+                                vars[\"_hidden_layers.0._model.0.weight\"])),",
            "+                        np.transpose(vars[\"_logits._model.0.weight\"]))",
            "mean, log_std = np.split(expected_mean_logstd, 2, axis=-1)",
            "if logp_func is None:",
            "expected_logp = np.log(norm.pdf(a, mean, np.exp(log_std)))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=1, insert_id=2620253)",
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=4, insert_id=2620254)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=5, insert_id=2620255)",
            "Insert(target_node=IN(type=call), node=('identifier', 'fc'), position=0, insert_id=2620256)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2620257)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2620258)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2620259)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2620260)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=obs_batch), position=1)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=,, text=,), position=2)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=call), position=3)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=2620261)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'np'), position=0, insert_id=2620262)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2620263)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'transpose'), position=2, insert_id=2620264)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2620265)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=subscript), position=1)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=), text=)), position=2)",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2620266)",
            "Update(target_node=ASTNode(type=identifier, text=fc), value='np')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=fc), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2620267)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'transpose'), position=2, insert_id=2620268)"
        ],
        "plus_line": 5,
        "minus_line": 3,
        "AST_diff_line": 23,
        "number": 4528,
        "neg_line": [
            "-fc(obs_batch,",
            "-vars[\"_hidden_layers.0._model.0.weight\"]),",
            "-vars[\"_logits._model.0.weight\"])"
        ],
        "pos_line": [
            "+fc(",
            "+obs_batch,",
            "+np.transpose(",
            "+vars[\"_hidden_layers.0._model.0.weight\"])),",
            "+np.transpose(vars[\"_logits._model.0.weight\"]))"
        ],
        "core_change": "-fc(obs_batch, -vars[\"_hidden_layers.0._model.0.weight\"]), -vars[\"_logits._model.0.weight\"]) +fc( +obs_batch, +np.transpose( +vars[\"_hidden_layers.0._model.0.weight\"])), +np.transpose(vars[\"_logits._model.0.weight\"]))",
        "core_API": "transpose"
    },
    {
        "commit_hash": "de4159a318f9a4f2f474733306c00ac326e643a7",
        "index": "5a8fb1ece..a13d18806 100644",
        "commit_message": "More TF int dtype fixes (#20384)\n\n* Add a test to ensure int dummy inputs are int64\n\n* Move the test into the existing int64 test and update a lot of existing dummies\n\n* Fix remaining dummies\n\n* Fix remaining dummies\n\n* Test for int64 serving sigs as well\n\n* Update core tests to use tf.int64\n\n* Add better messages to the assertions\n\n* Update all serving sigs to int64\n\n* More sneaky hiding tf.int32s\n\n* Add an optional int32 signature in save_pretrained\n\n* make fixup\n\n* Add Amy's suggestions\n\n* Switch all serving sigs back to tf.int32\n\n* Switch all dummies to tf.int32\n\n* Adjust tests to check for tf.int32 instead of tf.int64\n\n* Fix base dummy_inputs dtype\n\n* Start casting to tf.int32 in input_processing\n\n* Change dtype for unpack_inputs test\n\n* Add proper tf.int32 test\n\n* Make the alternate serving signature int64\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFEncoderDecoderModel(TFPreTrainedModel, TFCausalLanguageModelingLoss):",
            "`Dict[str, tf.Tensor]`: The dummy inputs.",
            "\"\"\"",
            "# Add `decoder_input_ids` because `self.decoder` requires it.",
            "-        input_ids = tf.constant(DUMMY_INPUTS)",
            "+        input_ids = tf.constant(DUMMY_INPUTS, dtype=tf.int32)",
            "dummy = {\"input_ids\": input_ids, \"decoder_input_ids\": input_ids}",
            "return dummy"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 4530,
        "neg_line": [
            "-input_ids = tf.constant(DUMMY_INPUTS)"
        ],
        "pos_line": [
            "+input_ids = tf.constant(DUMMY_INPUTS, dtype=tf.int32)"
        ],
        "core_change": "-input_ids = tf.constant(DUMMY_INPUTS) +input_ids = tf.constant(DUMMY_INPUTS, dtype=tf.int32)",
        "core_API": "constant"
    },
    {
        "commit_hash": "1bd8a7835ab66f266daf11cff2aa56ed6a46a001",
        "index": "694483d..4301a54 100644",
        "commit_message": "attempting to fix issue with deepspeed fp16 seeing overflowing gradient\n\n",
        "file": "DALLE2-pytorch.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def discretized_gaussian_log_likelihood(x, *, means, log_scales, thres = 0.999):",
            "log_cdf_plus,",
            "torch.where(x > thres,",
            "log_one_minus_cdf_min,",
            "-            log(cdf_delta)))",
            "+            log(cdf_delta, eps = eps)))",
            "",
            "return log_probs"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=63441)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=63442)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'eps'), position=0, insert_id=63443)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=63444)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'eps'), position=2, insert_id=63445)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 4531,
        "neg_line": [
            "-log(cdf_delta)))"
        ],
        "pos_line": [
            "+log(cdf_delta, eps = eps)))"
        ],
        "core_change": "-log(cdf_delta))) +log(cdf_delta, eps = eps)))",
        "core_API": "where"
    },
    {
        "commit_hash": "70996a5420f6b28cb0330e373b99f75893c8fbb3",
        "index": "78ccd0723..e5c1f340e 100644",
        "commit_message": "WIP: Support for Training with BF16 (#13207)\n\n* started bf16 integration\n\n* minor changes\n\n* code now runs\n\n* style\n\n* lay foundation for bf16 testing\n\n* lay foundation for bf16 testing\n\n* start the tests\n\n* better bf16 check\n\n* style\n\n* 2 separate checkers - one for bf16 support, another for bf16+autocast\n\n* Update src/transformers/training_args.py\n\nCo-authored-by: Stas Bekman <stas00@users.noreply.github.com>\n\n* a couple of comment resolutions\n\n* more comment resolutions\n\n* resolved a small bug\n\n* just some print statemtns\n\n* added todo marking\n\n* added a todo\n\n* adjust for API change s/fast_dtype/dtype/\n\n* fix style\n\n* merge 2 bf16 util functions\n\n* bf16 now does scaling too\n\n* Add support for bfloat16\n\n* Revert T5 layernorm to float32\n\nThis is based on the comment at https://github.com/huggingface/transformers/pull/14448/files#r752660929 and the PyTorch PR https://github.com/pytorch/pytorch/pull/66920 .\n\n* Add comment about conversion to float32 before returning the numpy data\n\n* Add comment about AMP-bfloat16 incompatibility\n\n* Fix formatting\n\n* typo\n\n* reformer / bf16\n\n* cleanup\n\n* require at least pt-1.10\n\n* fix\n\n* will deal with deepspeed separately\n\n* cleanup\n\n* revert\n\n* cleanup\n\n* fp16_full_eval and bf16_full_eval are separate modes\n\n* proper deprecation\n\n* cleanup\n\n* test and fixes\n\n* spelling\n\n* cleanup\n\n* add a note that this API is experimental\n\nCo-authored-by: jamie <jamie@cortx.com>\nCo-authored-by: Stas Bekman <stas@stason.org>\nCo-authored-by: Stas Bekman <stas00@users.noreply.github.com>\nCo-authored-by: suriya <suriya@cortx.com>\nCo-authored-by: Manuel R. Ciosici <manuelrciosici@gmail.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class T5LayerNorm(nn.Module):",
            "variance = hidden_states.to(torch.float32).pow(2).mean(-1, keepdim=True)",
            "hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)",
            "",
            "-        # convert into float16 if necessary",
            "-        if self.weight.dtype == torch.float16:",
            "-            hidden_states = hidden_states.to(torch.float16)",
            "+        # convert into half-precision if necessary",
            "+        if self.weight.dtype in [torch.float16, torch.bfloat16]:",
            "+            hidden_states = hidden_states.to(self.weight.dtype)",
            "+",
            "return self.weight * hidden_states"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=comparison_operator), node=('attribute', None), position=0, insert_id=1209260)",
            "Insert(target_node=ASTNode(type=comparison_operator), node=('in', 'in'), position=1, insert_id=1209261)",
            "Insert(target_node=ASTNode(type=comparison_operator), node=('list', None), position=2, insert_id=1209262)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1209263)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1209264)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dtype'), position=2, insert_id=1209265)",
            "Insert(target_node=IN(type=list), node=('[', '['), position=0, insert_id=1209266)",
            "Move(target_node=IN(type=list), node=ASTNode(type=attribute), position=1)",
            "Insert(target_node=IN(type=list), node=(',', ','), position=2, insert_id=1209267)",
            "Insert(target_node=IN(type=list), node=('attribute', None), position=3, insert_id=1209268)",
            "Insert(target_node=IN(type=list), node=(']', ']'), position=4, insert_id=1209269)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=1209270)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1209271)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'weight'), position=2, insert_id=1209272)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1209273)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1209274)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'bfloat16'), position=2, insert_id=1209275)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=attribute), position=1)",
            "Delete(target_node=ASTNode(type===, text===))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=float16))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 23,
        "number": 4532,
        "neg_line": [
            "-# convert into float16 if necessary",
            "-if self.weight.dtype == torch.float16:",
            "-hidden_states = hidden_states.to(torch.float16)"
        ],
        "pos_line": [
            "+# convert into half-precision if necessary",
            "+if self.weight.dtype in [torch.float16, torch.bfloat16]:",
            "+hidden_states = hidden_states.to(self.weight.dtype)",
            "+"
        ],
        "core_change": "-# convert into float16 if necessary -if self.weight.dtype == torch.float16: -hidden_states = hidden_states.to(torch.float16) +# convert into half-precision if necessary +if self.weight.dtype in [torch.float16, torch.bfloat16]: +hidden_states = hidden_states.to(self.weight.dtype) +",
        "core_API": "to"
    },
    {
        "commit_hash": "ee99a6c1e270a78d6fb755f2478ede73399b4fd6",
        "index": "5694fe4d..b9cebb5a 100644",
        "commit_message": "Fix voice conversion inference (#1583)\n\n* Add voice conversion zoo test\n\n* Fix style\n\n* Fix unit test\n\n",
        "file": "TTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestVits(unittest.TestCase):",
            "args = VitsArgs(num_speakers=num_speakers, use_speaker_embedding=True)",
            "model = Vits(args)",
            "",
            "-        ref_inp = torch.randn(1, spec_len, 513)",
            "+        ref_inp = torch.randn(1, 513, spec_len)",
            "ref_inp_len = torch.randint(1, spec_effective_len, (1,))",
            "ref_spk_id = torch.randint(1, num_speakers, (1,))",
            "tgt_spk_id = torch.randint(1, num_speakers, (1,))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=identifier, text=spec_len), node=ASTNode(type=argument_list), position=5)",
            "Move(target_node=ASTNode(type=,, text=,), node=ASTNode(type=argument_list), position=6)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 4533,
        "neg_line": [
            "-ref_inp = torch.randn(1, spec_len, 513)"
        ],
        "pos_line": [
            "+ref_inp = torch.randn(1, 513, spec_len)"
        ],
        "core_change": "-ref_inp = torch.randn(1, spec_len, 513) +ref_inp = torch.randn(1, 513, spec_len)",
        "core_API": "randn"
    },
    {
        "commit_hash": "ca9ab1201ff56815652b7b85496d262c2d89d6c1",
        "index": "36ad4789..b14ac446 100644",
        "commit_message": "ds_report bug fix on cpu and guard torch import in setup.py (#524)\n\n* on cpu box error gracefully if cuda home doesn't exist\n\n* gaurd against torch import issue\n\n* fix sytax error\n\n* fix import\n",
        "file": "DeepSpeed.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def ninja_installed():",
            "def nvcc_version():",
            "import torch.utils.cpp_extension",
            "cuda_home = torch.utils.cpp_extension.CUDA_HOME",
            "+    if cuda_home is None:",
            "+        return f\"{RED} [FAIL] cannot find CUDA_HOME via torch.utils.cpp_extension.CUDA_HOME={torch.utils.cpp_extension.CUDA_HOME} {END}\"",
            "try:",
            "output = subprocess.check_output([cuda_home + \"/bin/nvcc\",",
            "\"-V\"],"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=4, insert_id=84714)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=84715)",
            "Insert(target_node=IN(type=if_statement), node=('comparison_operator', None), position=1, insert_id=84716)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=84717)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=84718)",
            "Insert(target_node=IN(type=comparison_operator), node=('identifier', 'cuda_home'), position=0, insert_id=84719)",
            "Insert(target_node=IN(type=comparison_operator), node=('is', 'is'), position=1, insert_id=84720)",
            "Insert(target_node=IN(type=comparison_operator), node=('none', 'None'), position=2, insert_id=84721)",
            "Insert(target_node=IN(type=block), node=('return_statement', None), position=0, insert_id=84722)",
            "Insert(target_node=IN(type=return_statement), node=('return', 'return'), position=0, insert_id=84723)",
            "Insert(target_node=IN(type=return_statement), node=('string', 'f\"{RED} [FAIL] cannot find CUDA_HOME via torch.utils.cpp_extension.CUDA_HOME={torch.utils.cpp_extension.CUDA_HOME} {END}\"'), position=1, insert_id=84724)"
        ],
        "plus_line": 2,
        "minus_line": 0,
        "AST_diff_line": 11,
        "number": 4537,
        "neg_line": [],
        "pos_line": [
            "+if cuda_home is None:",
            "+return f\"{RED} [FAIL] cannot find CUDA_HOME via torch.utils.cpp_extension.CUDA_HOME={torch.utils.cpp_extension.CUDA_HOME} {END}\""
        ],
        "core_change": "+if cuda_home is None: +return f\"{RED} [FAIL] cannot find CUDA_HOME via torch.utils.cpp_extension.CUDA_HOME={torch.utils.cpp_extension.CUDA_HOME} {END}\"",
        "core_API": "check_output"
    },
    {
        "commit_hash": "aa1355b4509a5dccc0febb471cb50048f67561f2",
        "index": "ed1bf765..778962f0 100644",
        "commit_message": "Fix beta log likelihood\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Beta(Distribution):",
            "def log_probability(self, action):",
            "action = (action - self.min_value) / (self.max_value - self.min_value)",
            "action = tf.minimum(x=action, y=(1.0 - util.epsilon))",
            "-        return (self.alpha - 1.0) * tf.log(action) + (self.beta - 1.0) * tf.log1p(-action) - self.log_norm",
            "+        return (self.alpha - 1.0) * tf.log(action + util.epsilon) +\\",
            "+               (self.beta - 1.0) * tf.log1p(-action) - self.log_norm",
            "",
            "def kl_divergence(self, other):",
            "assert isinstance(other, Beta)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('binary_operator', None), position=1, insert_id=2243455)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=identifier, text=action), position=0)",
            "Insert(target_node=IN(type=binary_operator), node=('+', '+'), position=1, insert_id=2243456)",
            "Insert(target_node=IN(type=binary_operator), node=('attribute', None), position=2, insert_id=2243457)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'util'), position=0, insert_id=2243458)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2243459)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'epsilon'), position=2, insert_id=2243460)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 4538,
        "neg_line": [
            "-return (self.alpha - 1.0) * tf.log(action) + (self.beta - 1.0) * tf.log1p(-action) - self.log_norm"
        ],
        "pos_line": [
            "+return (self.alpha - 1.0) * tf.log(action + util.epsilon) +\\",
            "+(self.beta - 1.0) * tf.log1p(-action) - self.log_norm"
        ],
        "core_change": "-return (self.alpha - 1.0) * tf.log(action) + (self.beta - 1.0) * tf.log1p(-action) - self.log_norm +return (self.alpha - 1.0) * tf.log(action + util.epsilon) +\\ +(self.beta - 1.0) * tf.log1p(-action) - self.log_norm",
        "core_API": "minimum"
    },
    {
        "commit_hash": "3dda5ac9fcf390d8b83eb855249360711707c11c",
        "index": "a6f27b9f..b91a68e2 100644",
        "commit_message": "for discussion: incorporate black code formatter (#3308)\n\n* setup files\n\n* run black\n\n* undo\n\n* update CONTRIBUTING.md\n\n* fix quotes in test_other_modules\n\n* make flake8 happy\n\n* set black to 100 characters per line\n\n* move type: ignore to where mypy wants them\n\n* more flake8\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "from allennlp.modules.attention.dot_product_attention import DotProductAttention",
            "",
            "",
            "class TestDotProductAttention(AllenNlpTestCase):",
            "-",
            "def test_can_init_dot(self):",
            "legacy_attention = Attention.from_params(Params({\"type\": \"dot_product\"}))",
            "isinstance(legacy_attention, DotProductAttention)",
            "",
            "def test_dot_product_similarity(self):",
            "linear = DotProductAttention(normalize=False)",
            "-        output = linear(torch.FloatTensor([[0, 0, 0], [1, 1, 1]]),",
            "-                        torch.FloatTensor([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]]))",
            "+        output = linear(",
            "+            torch.FloatTensor([[0, 0, 0], [1, 1, 1]]),",
            "+            torch.FloatTensor([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]]),",
            "+        )",
            "",
            "-        assert_almost_equal(output.numpy(),",
            "-                            numpy.array([[0.0, 0.0], [24.0, 33.0]]), decimal=2)",
            "+        assert_almost_equal(output.numpy(), numpy.array([[0.0, 0.0], [24.0, 33.0]]), decimal=2)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=23825)"
        ],
        "plus_line": 5,
        "minus_line": 4,
        "AST_diff_line": 1,
        "number": 4540,
        "neg_line": [
            "-",
            "-output = linear(torch.FloatTensor([[0, 0, 0], [1, 1, 1]]),",
            "-torch.FloatTensor([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]]))",
            "-assert_almost_equal(output.numpy(),",
            "-numpy.array([[0.0, 0.0], [24.0, 33.0]]), decimal=2)"
        ],
        "pos_line": [
            "+output = linear(",
            "+torch.FloatTensor([[0, 0, 0], [1, 1, 1]]),",
            "+torch.FloatTensor([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]]),",
            "+)",
            "+assert_almost_equal(output.numpy(), numpy.array([[0.0, 0.0], [24.0, 33.0]]), decimal=2)"
        ],
        "core_change": "- -output = linear(torch.FloatTensor([[0, 0, 0], [1, 1, 1]]), -torch.FloatTensor([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])) +output = linear( +torch.FloatTensor([[0, 0, 0], [1, 1, 1]]), +torch.FloatTensor([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]]), +) -assert_almost_equal(output.numpy(), -numpy.array([[0.0, 0.0], [24.0, 33.0]]), decimal=2) +assert_almost_equal(output.numpy(), numpy.array([[0.0, 0.0], [24.0, 33.0]]), decimal=2)",
        "core_API": "from_params"
    },
    {
        "commit_hash": "163ae9681e4ec19de40be444ebf4cdcbca759307",
        "index": "29c89c6d..a3e66406 100644",
        "commit_message": "Fix typo in test\n\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "layers",
        "change": [
            "class TokenClassificationIntegrationTest(test_combinations.TestCase):",
            "keras.layers.Conv1D(4, 5, padding='same', activation='relu'),",
            "keras.layers.Conv1D(8, 5, padding='same'),",
            "keras.layers.BatchNormalization(),",
            "-        keras.layers.Conv2D(3, 5, padding='same', activation='softmax'),",
            "+        keras.layers.Conv1D(3, 5, padding='same', activation='softmax'),",
            "]",
            "model = test_utils.get_model_from_layers(",
            "layers, input_shape=(None,))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=Conv2D), value='Conv1D')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 4541,
        "neg_line": [
            "-keras.layers.Conv2D(3, 5, padding='same', activation='softmax'),"
        ],
        "pos_line": [
            "+keras.layers.Conv1D(3, 5, padding='same', activation='softmax'),"
        ],
        "core_change": "-keras.layers.Conv2D(3, 5, padding='same', activation='softmax'), +keras.layers.Conv1D(3, 5, padding='same', activation='softmax'),",
        "core_API": "Conv1D"
    },
    {
        "commit_hash": "73747abe72df28ea535044c8621e7a62d35b3e0f",
        "index": "9b3aa468d..333cce4e5 100644",
        "commit_message": "fix model(Remove unwanted relu layer.) (#1644)\n\nA relu layer was inserted just before the cross entropy error was calculated, and it seemed that the loss was not calculated correctly, so I removed the relu layer.\nAccordingly, the structure of the NeuralNetwork class in buildmodel_tutorial.py was also changed.\n\nFYI:\nAfter fixing the model, we tried the task in the environment at hand, and the accuracy went from about 52% to about 71%.\n\nIf you have time, I hope you will take a look.\n",
        "file": "tutorials.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class NeuralNetwork(nn.Module):",
            "nn.Linear(512, 512),",
            "nn.ReLU(),",
            "nn.Linear(512, 10),",
            "-            nn.ReLU()",
            ")",
            "",
            "def forward(self, x):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=nn))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=ReLU))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 4545,
        "neg_line": [
            "-nn.ReLU()"
        ],
        "pos_line": [],
        "core_change": "-nn.ReLU()",
        "core_API": "Linear"
    },
    {
        "commit_hash": "03f5f8c4b18a389e5ea21c9d8f2243a4833b64c1",
        "index": "9b6e54e..0f6d7f4 100644",
        "commit_message": "Fix issue #452 for PyTorch 0.4.0\n\n",
        "file": "faster-rcnn.pytorch.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class roibatchLoader(data.Dataset):",
            "# for ratio cross 1, we make it to be 1.",
            "target_ratio = 1",
            "",
            "-        self.ratio_list_batch[left_idx:(right_idx+1)] = target_ratio",
            "+        self.ratio_list_batch[left_idx:(right_idx+1)] = torch.tensor(target_ratio.astype(np.float64)) # trainset ratio list ,each batch is same number",
            "",
            "",
            "def __getitem__(self, index):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('call', None), position=2, insert_id=226318)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=226319)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=226320)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=226321)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=226322)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tensor'), position=2, insert_id=226323)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=226324)",
            "Insert(target_node=IN(type=argument_list), node=('call', None), position=1, insert_id=226325)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=226326)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=226327)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=226328)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=target_ratio), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=226329)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'astype'), position=2, insert_id=226330)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=226331)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=226332)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=226333)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'np'), position=0, insert_id=226334)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=226335)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'float64'), position=2, insert_id=226336)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 20,
        "number": 4555,
        "neg_line": [
            "-self.ratio_list_batch[left_idx:(right_idx+1)] = target_ratio"
        ],
        "pos_line": [
            "+self.ratio_list_batch[left_idx:(right_idx+1)] = torch.tensor(target_ratio.astype(np.float64)) # trainset ratio list ,each batch is same number"
        ],
        "core_change": "-self.ratio_list_batch[left_idx:(right_idx+1)] = target_ratio +self.ratio_list_batch[left_idx:(right_idx+1)] = torch.tensor(target_ratio.astype(np.float64)) # trainset ratio list ,each batch is same number",
        "core_API": "tensor"
    },
    {
        "commit_hash": "8973a322b96b93d4ea421955bb763e2ce00e365d",
        "index": "8aa9a782..1049dd8b 100644",
        "commit_message": "Fixed scale parameter generation (#1400)\n\n* Fixed scale parameter generation\n\n* Minor fix\n\n* Added test for Random Affine 3d transformation\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Flake issue fixed\n\n* Refactored random affine 3d test into TestRandomAffine3D class\n\n* Newline addition\n\n* Addressing PR comments\n\n* Minor fixes\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def random_affine_generator3d(",
            "dim=1,",
            ")",
            "else:",
            "-        scale = torch.ones(batch_size, device=device, dtype=dtype).repeat(1, 3)",
            "+        scale = torch.ones(batch_size, device=device, dtype=dtype).reshape(batch_size, 1).repeat(1, 3)",
            "",
            "if translate is not None:",
            "if translate.shape != torch.Size([3]):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=413918)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=413919)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=413920)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'reshape'), position=2, insert_id=413921)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=413922)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'batch_size'), position=1, insert_id=413923)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=413924)",
            "Insert(target_node=IN(type=argument_list), node=('integer', '1'), position=3, insert_id=413925)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=413926)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 4559,
        "neg_line": [
            "-scale = torch.ones(batch_size, device=device, dtype=dtype).repeat(1, 3)"
        ],
        "pos_line": [
            "+scale = torch.ones(batch_size, device=device, dtype=dtype).reshape(batch_size, 1).repeat(1, 3)"
        ],
        "core_change": "-scale = torch.ones(batch_size, device=device, dtype=dtype).repeat(1, 3) +scale = torch.ones(batch_size, device=device, dtype=dtype).reshape(batch_size, 1).repeat(1, 3)",
        "core_API": "ones"
    },
    {
        "commit_hash": "d8361505f7fbfd0eb21e3cd6d167fcbc717dcf5a",
        "index": "a2b165f..4cc3044 100644",
        "commit_message": "Fix #30\n\n",
        "file": "CapsNet-Tensorflow.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class CapsNet(object):",
            "assert self.masked_v.get_shape() == [cfg.batch_size, 1, 16, 1]",
            "# Method 2. masking with true label, default mode",
            "else:",
            "-                self.masked_v = tf.matmul(tf.squeeze(self.caps2), tf.reshape(self.Y, (-1, 10, 1)), transpose_a=True)",
            "+                # self.masked_v = tf.matmul(tf.squeeze(self.caps2), tf.reshape(self.Y, (-1, 10, 1)), transpose_a=True)",
            "+                self.masked_v = tf.multply(tf.squeeze(self.caps2), tf.reshape(self.Y, (-1, 10, 1)))",
            "self.v_length = tf.sqrt(tf.reduce_sum(tf.square(self.caps2), axis=2, keep_dims=True) + epsilon)",
            "",
            "# 2. Reconstructe the MNIST images with 3 FC layers"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=matmul), value='multply')",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=transpose_a))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=true, text=True))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 4561,
        "neg_line": [
            "-self.masked_v = tf.matmul(tf.squeeze(self.caps2), tf.reshape(self.Y, (-1, 10, 1)), transpose_a=True)"
        ],
        "pos_line": [
            "+# self.masked_v = tf.matmul(tf.squeeze(self.caps2), tf.reshape(self.Y, (-1, 10, 1)), transpose_a=True)",
            "+self.masked_v = tf.multply(tf.squeeze(self.caps2), tf.reshape(self.Y, (-1, 10, 1)))"
        ],
        "core_change": "-self.masked_v = tf.matmul(tf.squeeze(self.caps2), tf.reshape(self.Y, (-1, 10, 1)), transpose_a=True) +# self.masked_v = tf.matmul(tf.squeeze(self.caps2), tf.reshape(self.Y, (-1, 10, 1)), transpose_a=True) +self.masked_v = tf.multply(tf.squeeze(self.caps2), tf.reshape(self.Y, (-1, 10, 1)))",
        "core_API": "get_shape"
    },
    {
        "commit_hash": "2f59829dab71a25f9fd758d186235e546a67fb0b",
        "index": "3599f96e..e68b8438 100644",
        "commit_message": "fix bugs shift_rgb (#1861)\n\n* fix bugs shift_rgb\n\n* add tests for TestRGBShift and TestRandomRGBShift\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def shift_rgb(image: torch.Tensor, r_shift: torch.Tensor, g_shift: torch.Tensor,",
            "",
            "shifts = [r_shift, g_shift, b_shift]",
            "",
            "-    shifted = (image + torch.Tensor(shifts).view(1, 3, 1, 1).to(image)).clamp_(min=0, max=1)",
            "+    shifted = (image + torch.stack(shifts).view(-1, 3, 1, 1).to(image)).clamp_(min=0, max=1)",
            "",
            "return shifted"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('unary_operator', '-1'), position=1, insert_id=401274)",
            "Update(target_node=ASTNode(type=identifier, text=Tensor), value='stack')",
            "Delete(target_node=ASTNode(type=integer, text=1))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 3,
        "number": 4563,
        "neg_line": [
            "-shifted = (image + torch.Tensor(shifts).view(1, 3, 1, 1).to(image)).clamp_(min=0, max=1)"
        ],
        "pos_line": [
            "+shifted = (image + torch.stack(shifts).view(-1, 3, 1, 1).to(image)).clamp_(min=0, max=1)"
        ],
        "core_change": "-shifted = (image + torch.Tensor(shifts).view(1, 3, 1, 1).to(image)).clamp_(min=0, max=1) +shifted = (image + torch.stack(shifts).view(-1, 3, 1, 1).to(image)).clamp_(min=0, max=1)",
        "core_API": "Tensor"
    },
    {
        "commit_hash": "5b57be292c7d193419d7c8436441aff907068d5e",
        "index": "08212ee1..d366f473 100644",
        "commit_message": "Adding normalization bias verification (#4990)\n\n* adding batchnorm verification\n\n* Adding trainer callback\n\n* updating changelog\n\n* renaming class\n\n* detailed message for sanity check\n\n* run sanity checks by default\n\n* fix normalization bias issue in image embeddings\n\n* update docstring\n\n* fix test\n\nCo-authored-by: Dirk Groeneveld <dirkg@allenai.org>\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestImageFeatureEmbeddings(AllenNlpTestCase):",
            "super().__init__()",
            "",
            "self.image_embeddings = torch.nn.Linear(feature_size, embedding_size)",
            "-                self.image_location_embeddings = torch.nn.Linear(4, embedding_size)",
            "+                self.image_location_embeddings = torch.nn.Linear(4, embedding_size, bias=False)",
            "self.layer_norm = torch.nn.LayerNorm(embedding_size, eps=1e-12)",
            "self.dropout = torch.nn.Dropout(dropout)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=4583)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=4584)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'bias'), position=0, insert_id=4585)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=4586)",
            "Insert(target_node=IN(type=keyword_argument), node=('false', 'False'), position=2, insert_id=4587)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 4564,
        "neg_line": [
            "-self.image_location_embeddings = torch.nn.Linear(4, embedding_size)"
        ],
        "pos_line": [
            "+self.image_location_embeddings = torch.nn.Linear(4, embedding_size, bias=False)"
        ],
        "core_change": "-self.image_location_embeddings = torch.nn.Linear(4, embedding_size) +self.image_location_embeddings = torch.nn.Linear(4, embedding_size, bias=False)",
        "core_API": "Linear"
    },
    {
        "commit_hash": "f7dfb86af100ebbc15984646fc12bd574b47c81d",
        "index": "37962237..33d543b4 100644",
        "commit_message": "fix bug in last commit\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class FastRCNNHead(object):",
            "",
            "@memoized",
            "def decoded_output_boxes_class_agnostic(self):",
            "+        \"\"\" Returns: Nx4 \"\"\"",
            "assert self._bbox_class_agnostic",
            "box_logits = tf.reshape(self.box_logits, [-1, 4])",
            "decoded = decode_bbox_target("
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('block', None), position=0, insert_id=2281002)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=2281003)",
            "Insert(target_node=IN(type=expression_statement), node=('string', '\"\"\" Returns: Nx4 \"\"\"'), position=0, insert_id=2281004)",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 4,
        "number": 4565,
        "neg_line": [],
        "pos_line": [
            "+\"\"\" Returns: Nx4 \"\"\""
        ],
        "core_change": "+\"\"\" Returns: Nx4 \"\"\"",
        "core_API": "reshape"
    },
    {
        "commit_hash": "cb3e5c33f7c2131f723ab79b88f98f8e1df6d927",
        "index": "4bb8de178..7782d6433 100644",
        "commit_message": "Fix a few last paths for the new repo org (#8666)\n\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "models",
        "change": [
            "def make_support(question, source=\"wiki40b\", method=\"dense\", n_results=10):",
            "return question_doc, support_list",
            "",
            "",
            "-@st.cache(hash_funcs={torch.Tensor: (lambda _: None), transformers.tokenization_bart.BartTokenizer: (lambda _: None)})",
            "+@st.cache(",
            "+    hash_funcs={",
            "+        torch.Tensor: (lambda _: None),",
            "+        transformers.models.bart.tokenization_bart.BartTokenizer: (lambda _: None),",
            "+    }",
            "+)",
            "def answer_question(",
            "question_doc, s2s_model, s2s_tokenizer, min_len=64, max_len=256, sampling=False, n_beams=2, top_p=0.95, temp=0.8",
            "):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=dictionary), node=(',', ','), position=4, insert_id=2688809)",
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=2688810)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=2688811)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2688812)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2688813)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'bart'), position=2, insert_id=2688814)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=transformers), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'models'), position=2, insert_id=2688815)"
        ],
        "plus_line": 4,
        "minus_line": 0,
        "AST_diff_line": 9,
        "number": 4567,
        "neg_line": [
            "-@st.cache(hash_funcs={torch.Tensor: (lambda _: None), transformers.tokenization_bart.BartTokenizer: (lambda _: None)})"
        ],
        "pos_line": [
            "+@st.cache(",
            "+hash_funcs={",
            "+torch.Tensor: (lambda _: None),",
            "+transformers.models.bart.tokenization_bart.BartTokenizer: (lambda _: None),",
            "+}",
            "+)"
        ],
        "core_change": "-@st.cache(hash_funcs={torch.Tensor: (lambda _: None), transformers.tokenization_bart.BartTokenizer: (lambda _: None)}) +@st.cache( +hash_funcs={ +torch.Tensor: (lambda _: None), +transformers.models.bart.tokenization_bart.BartTokenizer: (lambda _: None), +} +)",
        "core_API": "cache"
    },
    {
        "commit_hash": "348dade16393ff0441ae3416158aad4e91d1be59",
        "index": "103dd196..f558f683 100755",
        "commit_message": "small fix related to entropy summary\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class DistributionModel(MemoryModel):",
            "collapsed_size = util.prod(util.shape(entropy)[1:])",
            "entropy = tf.reshape(tensor=entropy, shape=(-1, collapsed_size))",
            "entropies.append(entropy)",
            "-",
            "entropy_per_instance = tf.reduce_mean(input_tensor=tf.concat(values=entropies, axis=1), axis=1)",
            "entropy = tf.reduce_mean(input_tensor=entropy_per_instance, axis=0)",
            "-            if 'entropy' in self.summary_labels:",
            "-                tf.contrib.summary.scalar(name='entropy', tensor=entropy)",
            "+",
            "+        if 'entropy' in self.summary_labels:",
            "+            tf.contrib.summary.scalar(name='entropy', tensor=entropy)",
            "+        if self.entropy_regularization is not None and self.entropy_regularization > 0.0:",
            "losses['entropy'] = -self.entropy_regularization * entropy",
            "",
            "return losses"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=7, insert_id=2233898)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=2233899)",
            "Insert(target_node=IN(type=if_statement), node=('boolean_operator', None), position=1, insert_id=2233900)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=2233901)",
            "Insert(target_node=IN(type=if_statement), node=('block', ''), position=3, insert_id=2233902)",
            "Insert(target_node=IN(type=boolean_operator), node=('comparison_operator', None), position=0, insert_id=2233903)",
            "Insert(target_node=IN(type=boolean_operator), node=('and', 'and'), position=1, insert_id=2233904)",
            "Insert(target_node=IN(type=boolean_operator), node=('comparison_operator', None), position=2, insert_id=2233905)",
            "Insert(target_node=IN(type=comparison_operator), node=('attribute', None), position=0, insert_id=2233906)",
            "Insert(target_node=IN(type=comparison_operator), node=('is not', 'is'), position=1, insert_id=2233907)",
            "Insert(target_node=IN(type=comparison_operator), node=('is not', 'not'), position=2, insert_id=2233908)",
            "Insert(target_node=IN(type=comparison_operator), node=('none', 'None'), position=3, insert_id=2233909)",
            "Insert(target_node=IN(type=comparison_operator), node=('attribute', None), position=0, insert_id=2233910)",
            "Insert(target_node=IN(type=comparison_operator), node=('>', '>'), position=1, insert_id=2233911)",
            "Insert(target_node=IN(type=comparison_operator), node=('float', '0.0'), position=2, insert_id=2233912)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=2233913)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2233914)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'entropy_regularization'), position=2, insert_id=2233915)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=2233916)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2233917)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'entropy_regularization'), position=2, insert_id=2233918)"
        ],
        "plus_line": 3,
        "minus_line": 2,
        "AST_diff_line": 21,
        "number": 4568,
        "neg_line": [
            "-",
            "-if 'entropy' in self.summary_labels:",
            "-tf.contrib.summary.scalar(name='entropy', tensor=entropy)"
        ],
        "pos_line": [
            "+",
            "+if 'entropy' in self.summary_labels:",
            "+tf.contrib.summary.scalar(name='entropy', tensor=entropy)",
            "+if self.entropy_regularization is not None and self.entropy_regularization > 0.0:"
        ],
        "core_change": "- -if 'entropy' in self.summary_labels: -tf.contrib.summary.scalar(name='entropy', tensor=entropy) + +if 'entropy' in self.summary_labels: +tf.contrib.summary.scalar(name='entropy', tensor=entropy) +if self.entropy_regularization is not None and self.entropy_regularization > 0.0:",
        "core_API": "prod"
    },
    {
        "commit_hash": "c51dc4f92755c67a83f3fc8a0bd6b3e64df199e4",
        "index": "6ad6bdad6..6a16b72c7 100755",
        "commit_message": "[torch] remove deprecated uint8 in favor of bool (#21384)\n\n* uint8 -> bool\n\n* fix copies\n\n* style\n\n* update test modeling commen when checking attention buffers\n\n* style\n\n* use logical not on random mask instead of subtraction with 1\n\n* remove torch uint8\n\n* quality\n\n* remove modified modeling utils\n\n* Update based on review\n\nCo-authored-by: sgugger <sylvain.gugger@gmail.com>\n\n---------\n\nCo-authored-by: sgugger <sylvain.gugger@gmail.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def _compute_global_attention_mask(input_ids, sep_token_id, before_sep_token=Tru",
            "# bool attention mask with True in locations of global attention",
            "attention_mask = torch.arange(input_ids.shape[1], device=input_ids.device)",
            "if before_sep_token is True:",
            "-        attention_mask = (attention_mask.expand_as(input_ids) < question_end_index).to(torch.uint8)",
            "+        attention_mask = (attention_mask.expand_as(input_ids) < question_end_index).to(torch.bool)",
            "else:",
            "# last token is separation token and should not be counted and in the middle are two separation tokens",
            "-        attention_mask = (attention_mask.expand_as(input_ids) > (question_end_index + 1)).to(torch.uint8) * (",
            "+        attention_mask = (attention_mask.expand_as(input_ids) > (question_end_index + 1)).to(torch.bool) * (",
            "attention_mask.expand_as(input_ids) < input_ids.shape[-1]",
            "-        ).to(torch.uint8)",
            "+        ).to(torch.bool)",
            "",
            "return attention_mask"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=uint8), value='bool')",
            "Update(target_node=ASTNode(type=identifier, text=uint8), value='bool')",
            "Update(target_node=ASTNode(type=identifier, text=uint8), value='bool')"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 3,
        "number": 4569,
        "neg_line": [
            "-attention_mask = (attention_mask.expand_as(input_ids) < question_end_index).to(torch.uint8)",
            "-attention_mask = (attention_mask.expand_as(input_ids) > (question_end_index + 1)).to(torch.uint8) * (",
            "-).to(torch.uint8)"
        ],
        "pos_line": [
            "+attention_mask = (attention_mask.expand_as(input_ids) < question_end_index).to(torch.bool)",
            "+attention_mask = (attention_mask.expand_as(input_ids) > (question_end_index + 1)).to(torch.bool) * (",
            "+).to(torch.bool)"
        ],
        "core_change": "-attention_mask = (attention_mask.expand_as(input_ids) < question_end_index).to(torch.uint8) +attention_mask = (attention_mask.expand_as(input_ids) < question_end_index).to(torch.bool) -attention_mask = (attention_mask.expand_as(input_ids) > (question_end_index + 1)).to(torch.uint8) * ( +attention_mask = (attention_mask.expand_as(input_ids) > (question_end_index + 1)).to(torch.bool) * ( -).to(torch.uint8) +).to(torch.bool)",
        "core_API": "arange"
    },
    {
        "commit_hash": "d3ea0df8b9f923685ce5f2555c303b8eddbf83fd",
        "index": "0317c75..cb32d01 100644",
        "commit_message": "New YOLOv5 Classification Models (#8956)\n\n* Update\n\n* Logger step fix: Increment step with epochs (#8654)\n\n* enhance\n\n* revert\n\n* allow training from scratch\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Update --img argument from train.py \n\nsingle line\n\n* fix image size from 640 to 128\n\n* suport custom dataloader and augmentation\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* format\n\n* Update dataloaders.py\n\n* Single line return, single line comment, remove unused argument\n\n* address PR comments\n\n* fix spelling\n\n* don't augment eval set\n\n* use fstring\n\n* update augmentations.py\n\n* new maning convention for transforms\n\n* reverse if statement, inline ops\n\n* reverse if statement, inline ops\n\n* updates\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* update dataloaders\n\n* Remove additional if statement\n\n* Remove is_train as redundant\n\n* Cleanup\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Cleanup2\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Update classifier.py\n\n* Update augmentations.py\n\n* fix: imshow clip warning\n\n* update\n\n* Revert ToTensorV2 removal\n\n* Update classifier.py\n\n* Update normalize values, revert uint8\n\n* normalize image using cv2\n\n* remove dedundant comment\n\n* Update classifier.py\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* replace print with logger\n\n* commit steps\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Allow logging models from GenericLogger (#8676)\n\n* enhance\n\n* revert\n\n* allow training from scratch\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Update --img argument from train.py \n\nsingle line\n\n* fix image size from 640 to 128\n\n* suport custom dataloader and augmentation\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* format\n\n* Update dataloaders.py\n\n* Single line return, single line comment, remove unused argument\n\n* address PR comments\n\n* fix spelling\n\n* don't augment eval set\n\n* use fstring\n\n* update augmentations.py\n\n* new maning convention for transforms\n\n* reverse if statement, inline ops\n\n* reverse if statement, inline ops\n\n* updates\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* update dataloaders\n\n* Remove additional if statement\n\n* Remove is_train as redundant\n\n* Cleanup\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Cleanup2\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Update classifier.py\n\n* Update augmentations.py\n\n* fix: imshow clip warning\n\n* update\n\n* Revert ToTensorV2 removal\n\n* Update classifier.py\n\n* Update normalize values, revert uint8\n\n* normalize image using cv2\n\n* remove dedundant comment\n\n* Update classifier.py\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* replace print with logger\n\n* commit steps\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* support final model logging\n\n* update\n\n* update\n\n* update\n\n* update\n\n* remove curses\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Update classifier.py\n\n* Update __init__.py\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update dataset download\n\n* Update dataset download\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Pass imgsz to classify_transforms()\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Cos scheduler\n\n* Cos scheduler\n\n* Remove unused args\n\n* Update\n\n* Add seed\n\n* Add seed\n\n* Update\n\n* Update\n\n* Add run(), main()\n\n* Merge master\n\n* Merge master\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Create YOLOv5 BaseModel class (#8829)\n\n* Create BaseModel\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix\n\n* Hub load device fix\n\n* Update\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* Add experiment\n\n* Merge master\n\n* Attach names\n\n* weight decay = 1e-4\n\n* weight decay = 5e-5\n\n* update smart_optimizer console printout\n\n* fashion-mnist fix\n\n* Merge master\n\n* Update Table\n\n* Update Table\n\n* Remove destroy process group\n\n* add kwargs to forward()\n\n* fuse fix for resnet50\n\n* nc, names fix for resnet50\n\n* nc, names fix for resnet50\n\n* ONNX CPU inference fix\n\n* revert\n\n* cuda\n\n* if augment or visualize\n\n* if augment or visualize\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* New smart_inference_mode()\n\n* Update README\n\n* Refactor into /classify dir\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* reset defaults\n\n* reset defaults\n\n* fix gpu predict\n\n* warmup\n\n* ema half fix\n\n* spacing\n\n* remove data\n\n* remove cache\n\n* remove denormalize\n\n* save run settings\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* verbose false on initial plots\n\n* new save_yaml() function\n\n* Update ci-testing.yml\n\n* Path(data) CI fix\n\n* Separate classification CI\n\n* fix val\n\n* fix val\n\n* fix val\n\n* smartCrossEntropyLoss\n\n* skip validation on hub load\n\n* autodownload with working dir root\n\n* str(data)\n\n* Dataset usage example\n\n* im_show normalize\n\n* im_show normalize\n\n* add imagenet simple names to multibackend\n\n* Add validation speeds\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* 24-space names\n\n* Update bash scripts\n\n* Update permissions\n\n* Add bash script arguments\n\n* remove verbose\n\n* TRT data fix\n\n* names generator fix\n\n* optimize if names\n\n* update usage\n\n* Add local loading\n\n* Verbose=False\n\n* update names printing\n\n* Add Usage examples\n\n* Add Usage examples\n\n* Add Usage examples\n\n* Add Usage examples\n\n* named_children\n\n* reshape_classifier_outputs\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* update\n\n* update\n\n* fix CI\n\n* fix incorrect class substitution\n\n* fix incorrect class substitution\n\n* remove denormalize\n\n* ravel fix\n\n* cleanup\n\n* update opt file printing\n\n* update opt file printing\n\n* update defaults\n\n* add opt to checkpoint\n\n* Add warning\n\n* Add comment\n\n* plot half bug fix\n\n* Use NotImplementedError\n\n* fix export shape report\n\n* Fix TRT load\n\n* cleanup CI\n\n* profile comment\n\n* CI fix\n\n* Add cls models\n\n* avoid inplace error\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix usage examples\n\n* Update README\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Update README\n\n* Update README\n\n* Update README\n\n* Update README\n\n* Update README\n\n* Update README\n\n* Update README\n\n* Update README\n\n* Update README\n\n* Update README\n\n* Update README\n\n* Update README\n\n* Update README\n\n* Update README\n\n* Update README\n\n* Update README\n\nCo-authored-by: Ayush Chaurasia <ayush.chaurarsia@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "yolov5.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def attempt_load(weights, device=None, inplace=True, fuse=True):",
            "elif t is nn.Upsample and not hasattr(m, 'recompute_scale_factor'):",
            "m.recompute_scale_factor = None  # torch 1.11.0 compatibility",
            "",
            "+    # Return model",
            "if len(model) == 1:",
            "-        return model[-1]  # return model",
            "+        return model[-1]",
            "+",
            "+    # Return detection ensemble",
            "print(f'Ensemble created with {weights}\\n')",
            "for k in 'names', 'nc', 'yaml':",
            "setattr(model, k, getattr(model[0], k))",
            "model.stride = model[torch.argmax(torch.tensor([m.stride.max() for m in model])).int()].stride  # max stride",
            "assert all(model[0].nc == m.nc for m in model), f'Models have different class counts: {[m.nc for m in model]}'",
            "-    return model  # return ensemble",
            "+    return model"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 4,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 4573,
        "neg_line": [
            "-return model[-1]  # return model",
            "-return model  # return ensemble"
        ],
        "pos_line": [
            "+# Return model",
            "+return model[-1]",
            "+",
            "+# Return detection ensemble",
            "+return model"
        ],
        "core_change": "+# Return model -return model[-1]  # return model +return model[-1] + +# Return detection ensemble -return model  # return ensemble +return model",
        "core_API": "argmax"
    },
    {
        "commit_hash": "91c98330a16a2b957957e12344b33f0a4a4ea0de",
        "index": "a60020a2..4adec15e 100644",
        "commit_message": "Fix index_lookup_test in tf v1 setting.\n\nPiperOrigin-RevId: 394765449\n\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class IndexLookupOutputTest(keras_parameterized.TestCase,",
            ")  # pyformat: disable",
            "def test_int_output(self, shape, input_array, expected_output):",
            "vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]",
            "+    if callable(input_array):",
            "+      input_array = input_array()",
            "",
            "layer = index_lookup.IndexLookup(",
            "max_tokens=None,"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=2, insert_id=2080310)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=2080311)",
            "Insert(target_node=IN(type=if_statement), node=('call', None), position=1, insert_id=2080312)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=2080313)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=2080314)",
            "Insert(target_node=IN(type=call), node=('identifier', 'callable'), position=0, insert_id=2080315)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2080316)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=2080317)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2080318)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'input_array'), position=1, insert_id=2080319)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2080320)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=2080321)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'input_array'), position=0, insert_id=2080322)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=2080323)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=2080324)",
            "Insert(target_node=IN(type=call), node=('identifier', 'input_array'), position=0, insert_id=2080325)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2080326)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2080327)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=2080328)"
        ],
        "plus_line": 2,
        "minus_line": 0,
        "AST_diff_line": 19,
        "number": 4575,
        "neg_line": [],
        "pos_line": [
            "+if callable(input_array):",
            "+input_array = input_array()"
        ],
        "core_change": "+if callable(input_array): +input_array = input_array()",
        "core_API": "IndexLookup"
    },
    {
        "commit_hash": "bb293902b62d4cb3777f533818220e71b0b10a2f",
        "index": "= 0",
        "commit_message": "Fix for creating tensor from tensors\n\n",
        "file": "PySyft.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def perform_analysis_torch(preds, indices, noise_eps=0.1, delta=1e-5, moments=8,",
            "data_ind_eps_list = (data_ind_log_mgf - math.log(delta)) / l_list",
            "",
            "return min(eps_list_nm), min(data_ind_eps_list)",
            "+",
            "+",
            "+",
            "+"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 0,
        "minus_line": 0,
        "AST_diff_line": 17,
        "number": 4580,
        "neg_line": [],
        "pos_line": [
            "+",
            "+",
            "+",
            "+"
        ],
        "core_change": "+ + + +",
        "core_API": "log"
    },
    {
        "commit_hash": "084a3890db88043634ccbb3974f1ef584e19dfd5",
        "index": "f45ba132..c222eb29 100644",
        "commit_message": "fix a potential bug in GroupSampler (#955)\n\n\n",
        "file": "mmdetection.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class GroupSampler(Sampler):",
            "range(len(indices) // self.samples_per_gpu))",
            "]",
            "indices = np.concatenate(indices)",
            "-        indices = torch.from_numpy(indices).long()",
            "+        indices = indices.astype(np.int64).tolist()",
            "assert len(indices) == self.num_samples",
            "return iter(indices)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=long), value='tolist')",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=indices), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=from_numpy), value='astype')",
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=1, insert_id=644358)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'np'), position=0, insert_id=644359)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=644360)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'int64'), position=2, insert_id=644361)",
            "Delete(target_node=ASTNode(type=identifier, text=torch))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 4583,
        "neg_line": [
            "-indices = torch.from_numpy(indices).long()"
        ],
        "pos_line": [
            "+indices = indices.astype(np.int64).tolist()"
        ],
        "core_change": "-indices = torch.from_numpy(indices).long() +indices = indices.astype(np.int64).tolist()",
        "core_API": "concatenate"
    },
    {
        "commit_hash": "f62cb8313c2d7051e38f845823c1f4a7307aac3e",
        "index": "ddc223637..799d0ef04 100755",
        "commit_message": "Adds CLIP to models exportable with ONNX (#18515)\n\n* onnx config for clip\n\n* default opset as 14\n\n* changes from the original repo\n\n* input values order fix\n\n* outputs fix\n\n* remove unused import\n\n* ran make fix-copies\n\n* black format\n\n* review comments: forward ref, import fix, model change revert, .to cleanup\n\n* make style\n\n* formatting fixes\n\n* revert groupvit\n\n* comment for cast to int32\n\n* comment fix\n\n* make .T as .t() for onnx conversion\n\n* ran make fix-copies\n\n* remove unneeded comment\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n\n* fix copies\n\n* remove comment\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class CLIPModel(CLIPPreTrainedModel):",
            "# cosine similarity as logits",
            "logit_scale = self.logit_scale.exp()",
            "logits_per_text = torch.matmul(text_embeds, image_embeds.t()) * logit_scale",
            "-        logits_per_image = logits_per_text.T",
            "+        logits_per_image = logits_per_text.t()",
            "",
            "loss = None",
            "if return_loss:"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('call', None), position=2, insert_id=1193249)",
            "Move(target_node=IN(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1193250)",
            "Update(target_node=ASTNode(type=identifier, text=T), value='t')",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1193251)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1193252)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 4584,
        "neg_line": [
            "-logits_per_image = logits_per_text.T"
        ],
        "pos_line": [
            "+logits_per_image = logits_per_text.t()"
        ],
        "core_change": "-logits_per_image = logits_per_text.T +logits_per_image = logits_per_text.t()",
        "core_API": "exp"
    },
    {
        "commit_hash": "95b6d985ffb231d9a2047c039c95660913063b9d",
        "index": "5e3777fe..298c70ff 100644",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class BooleanAccuracy(Metric):",
            "",
            "# We want to skip predictions that are completely masked;",
            "# so we'll keep predictions that aren't.",
            "-            keep = mask.view(batch_size, -1).max(dim=1)[0].float()",
            "+            keep = mask.view(batch_size, -1).max(dim=1)[0]",
            "else:",
            "-            keep = torch.ones(batch_size, device=predictions.device).float()",
            "+            keep = torch.ones(batch_size, device=predictions.device).bool()",
            "",
            "predictions = predictions.view(batch_size, -1)",
            "gold_labels = gold_labels.view(batch_size, -1)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=argument_list), position=1)",
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=subscript), position=2)",
            "Update(target_node=ASTNode(type=identifier, text=float), value='bool')",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=float))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 10,
        "number": 4585,
        "neg_line": [
            "-keep = mask.view(batch_size, -1).max(dim=1)[0].float()",
            "-keep = torch.ones(batch_size, device=predictions.device).float()"
        ],
        "pos_line": [
            "+keep = mask.view(batch_size, -1).max(dim=1)[0]",
            "+keep = torch.ones(batch_size, device=predictions.device).bool()"
        ],
        "core_change": "-keep = mask.view(batch_size, -1).max(dim=1)[0].float() +keep = mask.view(batch_size, -1).max(dim=1)[0] -keep = torch.ones(batch_size, device=predictions.device).float() +keep = torch.ones(batch_size, device=predictions.device).bool()",
        "core_API": "view"
    },
    {
        "commit_hash": "c05ace221339102abb1c8b0933b9cbfd51ddd4a3",
        "index": "30403ba75..c57668f66 100644",
        "commit_message": "Use f-strings in the dataset scripts (#3291)\n\n* Finishes #3257\n\nUsed f-strings to format the .py files in the dataset folder\n\n* Fix style\n\n* Fix hkcancor dataset\n\nCo-authored-by: Mario ako <mario@huggingface.co>\nCo-authored-by: mariosasko <mariosasko777@gmail.com>\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class StyleChangeDetection(datasets.GeneratorBasedBuilder):",
            "",
            "if not os.path.exists(train_dir):",
            "raise FileNotFoundError(",
            "-                \"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('style_change_detection', data_dir=...)` that includes {}. Manual download instructions: {}\".format(",
            "-                    train_dir, train_dir, self.manual_download_instructions",
            "-                )",
            "+                f\"{train_dir} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('style_change_detection', data_dir=...)` that includes {train_dir}. Manual download instructions: {self.manual_download_instructions}\"",
            ")",
            "",
            "return ["
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('string', 'f\"{train_dir} does not exist. Make sure you insert a manual dir via `datasets.load_dataset(\\'style_change_detection\\', data_dir=...)` that includes {train_dir}. Manual download instructions: {self.manual_download_instructions}\"'), position=1, insert_id=1781649)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=), text=)), position=2)",
            "Delete(target_node=ASTNode(type=string, text=\"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('style_change_detection', data_dir=...)` that includes {}. Manual download instructions: {}\"))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=format))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=train_dir))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=train_dir))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=manual_download_instructions))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=), text=)))"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 18,
        "number": 4586,
        "neg_line": [
            "-\"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('style_change_detection', data_dir=...)` that includes {}. Manual download instructions: {}\".format(",
            "-train_dir, train_dir, self.manual_download_instructions",
            "-)"
        ],
        "pos_line": [
            "+f\"{train_dir} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('style_change_detection', data_dir=...)` that includes {train_dir}. Manual download instructions: {self.manual_download_instructions}\""
        ],
        "core_change": "-\"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('style_change_detection', data_dir=...)` that includes {}. Manual download instructions: {}\".format( -train_dir, train_dir, self.manual_download_instructions -) +f\"{train_dir} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('style_change_detection', data_dir=...)` that includes {train_dir}. Manual download instructions: {self.manual_download_instructions}\"",
        "core_API": "exists"
    },
    {
        "commit_hash": "0c166dcfb8b9e9d79d013b7967a59dfb1ac4b804",
        "index": "1f36f827..e595c5d3 100644",
        "commit_message": "define and apply YAPF formatting  (#1039)\n\n* define YAPF\n\n* yapf kornia -rip\n\n* yapf examples -rip\n\n* yapf test -rip\n\n* Update CONTRIBUTING.rst\n\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\n\n* update config\n\n* fix 4lint\n\n* mypy\n\n* mypy\n\n* mypy\n\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def cross_product_matrix(x: torch.Tensor) -> torch.Tensor:",
            "",
            "# construct the matrix, reshape to 3x3 and return",
            "zeros = torch.zeros_like(x0)",
            "-    cross_product_matrix_flat = torch.stack([",
            "-        zeros, -x2, x1,",
            "-        x2, zeros, -x0,",
            "-        -x1, x0, zeros], dim=-1)",
            "+    cross_product_matrix_flat = torch.stack([zeros, -x2, x1, x2, zeros, -x0, -x1, x0, zeros], dim=-1)",
            "return cross_product_matrix_flat.view(-1, 3, 3)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 4,
        "AST_diff_line": 17,
        "number": 4588,
        "neg_line": [
            "-cross_product_matrix_flat = torch.stack([",
            "-zeros, -x2, x1,",
            "-x2, zeros, -x0,",
            "--x1, x0, zeros], dim=-1)"
        ],
        "pos_line": [
            "+cross_product_matrix_flat = torch.stack([zeros, -x2, x1, x2, zeros, -x0, -x1, x0, zeros], dim=-1)"
        ],
        "core_change": "-cross_product_matrix_flat = torch.stack([ -zeros, -x2, x1, -x2, zeros, -x0, --x1, x0, zeros], dim=-1) +cross_product_matrix_flat = torch.stack([zeros, -x2, x1, x2, zeros, -x0, -x1, x0, zeros], dim=-1)",
        "core_API": "zeros_like"
    },
    {
        "commit_hash": "aba604ed1d3d2047a68a74872403db43e9d19037",
        "index": "5e8397bab..257896ae4 100644",
        "commit_message": "Add HF_ prefix to env var MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES (#2409)\n\n* add HF_ prefix to env var MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES\n\n* update tests\n\n* style\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "def test_load_from_disk_with_default_in_memory(",
            "current_dataset_size = 512  # arrow file size = 512, in-memory dataset size = 148",
            "if max_in_memory_dataset_size == \"default\":",
            "# default = 250 * 2 ** 20",
            "-        max_in_memory_dataset_size = datasets.config.MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES",
            "+        max_in_memory_dataset_size = datasets.config.HF_MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES",
            "else:",
            "-        monkeypatch.setattr(datasets.config, \"MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES\", max_in_memory_dataset_size)",
            "+        monkeypatch.setattr(datasets.config, \"HF_MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES\", max_in_memory_dataset_size)",
            "if max_in_memory_dataset_size:",
            "expected_in_memory = current_dataset_size < max_in_memory_dataset_size",
            "else:"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=1785144)",
            "Update(target_node=ASTNode(type=identifier, text=MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES), value='HF_MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES')",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'datasets'), position=0, insert_id=1785145)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1785146)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'config'), position=2, insert_id=1785147)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=attribute), position=1)",
            "Update(target_node=ASTNode(type=string, text=\"MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES\"), value='\"HF_MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES\"')",
            "Delete(target_node=ASTNode(type=identifier, text=datasets))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=config))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 11,
        "number": 4589,
        "neg_line": [
            "-max_in_memory_dataset_size = datasets.config.MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES",
            "-monkeypatch.setattr(datasets.config, \"MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES\", max_in_memory_dataset_size)"
        ],
        "pos_line": [
            "+max_in_memory_dataset_size = datasets.config.HF_MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES",
            "+monkeypatch.setattr(datasets.config, \"HF_MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES\", max_in_memory_dataset_size)"
        ],
        "core_change": "-max_in_memory_dataset_size = datasets.config.MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES +max_in_memory_dataset_size = datasets.config.HF_MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES -monkeypatch.setattr(datasets.config, \"MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES\", max_in_memory_dataset_size) +monkeypatch.setattr(datasets.config, \"HF_MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES\", max_in_memory_dataset_size)",
        "core_API": "setattr"
    },
    {
        "commit_hash": "ad22997614a80d26ac26fc36d667c4e513a1180c",
        "index": "229b9ef..21ce07e 100644",
        "commit_message": "fixed the issues #372 (#379)\n\ntorch.nn.functional.tanh()torch.tanh()\n",
        "file": "MockingBird.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class STL(nn.Module):",
            "def forward(self, inputs):",
            "N = inputs.size(0)",
            "query = inputs.unsqueeze(1)  # [N, 1, E//2]",
            "-        keys = tFunctional.tanh(self.embed).unsqueeze(0).expand(N, -1, -1)  # [N, token_num, E // num_heads]",
            "+        keys = torch.tanh(self.embed).unsqueeze(0).expand(N, -1, -1)  # [N, token_num, E // num_heads]",
            "style_embed = self.attention(query, keys)",
            "",
            "return style_embed"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tFunctional), value='torch')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 4591,
        "neg_line": [
            "-keys = tFunctional.tanh(self.embed).unsqueeze(0).expand(N, -1, -1)  # [N, token_num, E // num_heads]"
        ],
        "pos_line": [
            "+keys = torch.tanh(self.embed).unsqueeze(0).expand(N, -1, -1)  # [N, token_num, E // num_heads]"
        ],
        "core_change": "-keys = tFunctional.tanh(self.embed).unsqueeze(0).expand(N, -1, -1)  # [N, token_num, E // num_heads] +keys = torch.tanh(self.embed).unsqueeze(0).expand(N, -1, -1)  # [N, token_num, E // num_heads]",
        "core_API": "size"
    },
    {
        "commit_hash": "6a0a312370538d8cd0562337e76696774978d02a",
        "index": "6258933d..e27b793b 100644",
        "commit_message": "Fix bug in half precision for DPMSolverMultistepScheduler (#1349)\n\n* cast to float for quantile method\n\n* add fp16 test for DPMSolverMultistepScheduler fix\n\n* formatting update\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DPMSolverMultistepScheduler(SchedulerMixin, ConfigMixin):",
            "self.config.sample_max_value * torch.ones_like(dynamic_max_val).to(dynamic_max_val.device),",
            ")[(...,) + (None,) * (x0_pred.ndim - 1)]",
            "x0_pred = torch.clamp(x0_pred, -dynamic_max_val, dynamic_max_val) / dynamic_max_val",
            "+                x0_pred = x0_pred.type(orig_dtype)",
            "return x0_pred",
            "# DPM-Solver needs to solve an integral of the noise prediction model.",
            "elif self.config.algorithm_type == \"dpmsolver\":"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=5, insert_id=96385)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=96386)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'x0_pred'), position=0, insert_id=96387)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=96388)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=96389)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=96390)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=96391)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'x0_pred'), position=0, insert_id=96392)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=96393)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'type'), position=2, insert_id=96394)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=96395)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'orig_dtype'), position=1, insert_id=96396)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=96397)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 13,
        "number": 4593,
        "neg_line": [],
        "pos_line": [
            "+x0_pred = x0_pred.type(orig_dtype)"
        ],
        "core_change": "+x0_pred = x0_pred.type(orig_dtype)",
        "core_API": "ones_like"
    },
    {
        "commit_hash": "da2f1325014a158be36d62bdd20293ef97e574f4",
        "index": "12abbfdd..6b2569f6 100644",
        "commit_message": "enable black in the precommit (#1777)\n\n* enable black in the precommit\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add some fixes\n\n* update libface detection url\n\n* added url from kornia checkpoint\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def nms(boxes: torch.Tensor, scores: torch.Tensor, iou_threshold: float) -> torc",
            "xx2 = torch.min(x2[i], x2[order[1:]])",
            "yy2 = torch.min(y2[i], y2[order[1:]])",
            "",
            "-        w = torch.clamp(xx2 - xx1, min=0.)",
            "-        h = torch.clamp(yy2 - yy1, min=0.)",
            "+        w = torch.clamp(xx2 - xx1, min=0.0)",
            "+        h = torch.clamp(yy2 - yy1, min=0.0)",
            "inter = w * h",
            "ovr = inter / (areas[i] + areas[order[1:]] - inter)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=float, text=0.), value='0.0')",
            "Update(target_node=ASTNode(type=float, text=0.), value='0.0')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 4596,
        "neg_line": [
            "-w = torch.clamp(xx2 - xx1, min=0.)",
            "-h = torch.clamp(yy2 - yy1, min=0.)"
        ],
        "pos_line": [
            "+w = torch.clamp(xx2 - xx1, min=0.0)",
            "+h = torch.clamp(yy2 - yy1, min=0.0)"
        ],
        "core_change": "-w = torch.clamp(xx2 - xx1, min=0.) -h = torch.clamp(yy2 - yy1, min=0.) +w = torch.clamp(xx2 - xx1, min=0.0) +h = torch.clamp(yy2 - yy1, min=0.0)",
        "core_API": "min"
    },
    {
        "commit_hash": "722896f70a45c540f8f6ea1ffdd43e1ad8d43dc8",
        "index": "d4d27875..83e958b3 100644",
        "commit_message": "bug fix\n\n",
        "file": "TTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def create_attn_mask(N, T, g=0.05):",
            "M[n, t] = val",
            "e_x = np.exp(M - np.max(M))",
            "M = e_x / e_x.sum(axis=0) # only difference",
            "-    M = Variable(torch.FloatTensor(M).t()).cuda()",
            "+    M = torch.FloatTensor(M).t().cuda()",
            "M = torch.stack([M]*32)",
            "return M"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=Variable))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 4600,
        "neg_line": [
            "-M = Variable(torch.FloatTensor(M).t()).cuda()"
        ],
        "pos_line": [
            "+M = torch.FloatTensor(M).t().cuda()"
        ],
        "core_change": "-M = Variable(torch.FloatTensor(M).t()).cuda() +M = torch.FloatTensor(M).t().cuda()",
        "core_API": "exp"
    },
    {
        "commit_hash": "d4ea3ba50cb4da5b5f92d9b11504c5c2814da2ac",
        "index": "9c2b42f8cb..3830bd8e08 100644",
        "commit_message": "Fixed failing ivy tests for test_vmap by using handle_test and adding fn_tree (#11356)\n\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "functional",
        "change": [
            "def _fn3(x, y):",
            "",
            "",
            "# vmap",
            "-@given(",
            "+@handle_test(",
            "+    fn_tree=\"functional.ivy.vmap\",",
            "func=st.sampled_from([_fn1, _fn2, _fn3]),",
            "dtype_and_arrays_and_axes=helpers.arrays_and_axes(",
            "allow_none=False,"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=given), value='handle_test')",
            "Insert(target_node=ASTNode(type=ERROR), node=('keyword_argument', None), position=3, insert_id=1627999)",
            "Insert(target_node=ASTNode(type=ERROR), node=(',', ','), position=4, insert_id=1628000)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'fn_tree'), position=0, insert_id=1628001)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1628002)",
            "Insert(target_node=IN(type=keyword_argument), node=('string', '\"functional.ivy.vmap\"'), position=2, insert_id=1628003)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 6,
        "number": 4609,
        "neg_line": [
            "-@given("
        ],
        "pos_line": [
            "+@handle_test(",
            "+fn_tree=\"functional.ivy.vmap\","
        ],
        "core_change": "-@given( +@handle_test( +fn_tree=\"functional.ivy.vmap\",",
        "core_API": "sampled_from"
    },
    {
        "commit_hash": "51b8c5e88ce2681dd5b9b61b78cc81adc11399aa",
        "index": "192b999ce..792514008 100644",
        "commit_message": "Fix test scripts and refine some code\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def transfer_lm(ch_rnnlm, th_rnnlm):",
            "",
            "def test_lm():",
            "n_vocab = 3",
            "+    n_layers = 2",
            "n_units = 2",
            "batchsize = 5",
            "-    rnnlm_ch = lm_chainer.ClassifierWithState(lm_chainer.RNNLM(n_vocab, n_units))",
            "-    rnnlm_th = lm_pytorch.ClassifierWithState(lm_pytorch.RNNLM(n_vocab, n_units))",
            "+    rnnlm_ch = lm_chainer.ClassifierWithState(lm_chainer.RNNLM(n_vocab, n_layers, n_units))",
            "+    rnnlm_th = lm_pytorch.ClassifierWithState(lm_pytorch.RNNLM(n_vocab, n_layers, n_units))",
            "transfer_lm(rnnlm_ch.predictor, rnnlm_th.predictor)",
            "import numpy",
            "# TODO(karita) implement weight transfer"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=179790)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=179791)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'n_layers'), position=0, insert_id=179792)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=179793)",
            "Insert(target_node=IN(type=assignment), node=('integer', '2'), position=2, insert_id=179794)",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'n_layers'), position=3, insert_id=179795)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=179796)",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'n_layers'), position=3, insert_id=179797)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=179798)"
        ],
        "plus_line": 3,
        "minus_line": 2,
        "AST_diff_line": 9,
        "number": 4611,
        "neg_line": [
            "-rnnlm_ch = lm_chainer.ClassifierWithState(lm_chainer.RNNLM(n_vocab, n_units))",
            "-rnnlm_th = lm_pytorch.ClassifierWithState(lm_pytorch.RNNLM(n_vocab, n_units))"
        ],
        "pos_line": [
            "+n_layers = 2",
            "+rnnlm_ch = lm_chainer.ClassifierWithState(lm_chainer.RNNLM(n_vocab, n_layers, n_units))",
            "+rnnlm_th = lm_pytorch.ClassifierWithState(lm_pytorch.RNNLM(n_vocab, n_layers, n_units))"
        ],
        "core_change": "+n_layers = 2 -rnnlm_ch = lm_chainer.ClassifierWithState(lm_chainer.RNNLM(n_vocab, n_units)) -rnnlm_th = lm_pytorch.ClassifierWithState(lm_pytorch.RNNLM(n_vocab, n_units)) +rnnlm_ch = lm_chainer.ClassifierWithState(lm_chainer.RNNLM(n_vocab, n_layers, n_units)) +rnnlm_th = lm_pytorch.ClassifierWithState(lm_pytorch.RNNLM(n_vocab, n_layers, n_units))",
        "core_API": "ClassifierWithState"
    },
    {
        "commit_hash": "9325c85f780c569d1823e422eaf51b2e497e0d3e",
        "index": "192883b2..491312b4 100644",
        "commit_message": "fixed dropbox update\n\n",
        "file": "stable-diffusion-webui.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class FrozenCLIPEmbedderWithCustomWords(torch.nn.Module):",
            "image_embs_name = None",
            "if image_embs_name is not None and self.image_embs_name != image_embs_name:",
            "self.image_embs_name = image_embs_name",
            "-            self.image_embs = torch.load(aesthetic_embeddings[self.image_embs_name], map_location=device)",
            "+            self.image_embs = torch.load(shared.aesthetic_embeddings[self.image_embs_name], map_location=device)",
            "self.image_embs /= self.image_embs.norm(dim=-1, keepdim=True)",
            "self.image_embs.requires_grad_(False)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=subscript), node=('attribute', None), position=0, insert_id=1139582)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'shared'), position=0, insert_id=1139583)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1139584)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=aesthetic_embeddings), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 4613,
        "neg_line": [
            "-self.image_embs = torch.load(aesthetic_embeddings[self.image_embs_name], map_location=device)"
        ],
        "pos_line": [
            "+self.image_embs = torch.load(shared.aesthetic_embeddings[self.image_embs_name], map_location=device)"
        ],
        "core_change": "-self.image_embs = torch.load(aesthetic_embeddings[self.image_embs_name], map_location=device) +self.image_embs = torch.load(shared.aesthetic_embeddings[self.image_embs_name], map_location=device)",
        "core_API": "load"
    },
    {
        "commit_hash": "875ab806bb71e7f69660dca7a106065ec6371869",
        "index": "8add481b..396cb18f 100644",
        "commit_message": "[Feat] improve testing framework (#560)\n\n* Parametrize device and dtype fixtures using CLI options\n\n* improve tests parametrise n MakeFile\n\n* add coverage tot tests and update ci config file\n\n* test pep8 using pytest-flak8\n\n* add pytest-mypy, update failing tests and remove verify script\n\n* update build-docs command in MakeFile\n\n* fix performance test issue with non cuda device\n\n* add pydocstyle, adapt normalisation module to pydocsyle\n\n* fix augmentation  docs rendering\n\nCo-authored-by: Aiden Nibali <dismaldenizen@gmail.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestCenterCrop:",
            "def test_jit_trace(self, device):",
            "@torch.jit.script",
            "def op_script(input: torch.Tensor,",
            "-                      size: Tuple[torch.Tensor, torch.Tensor]) -> torch.Tensor:",
            "+                      size: Tuple[int, int]) -> torch.Tensor:",
            "return kornia.center_crop(input, size)",
            "# 1. Trace op",
            "batch_size, channels, height, width = 1, 2, 5, 4"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=torch), value='int')",
            "Move(target_node=ASTNode(type=subscript), node=ASTNode(type=identifier, text=torch), position=2)",
            "Update(target_node=ASTNode(type=identifier, text=torch), value='int')",
            "Move(target_node=ASTNode(type=subscript), node=ASTNode(type=identifier, text=torch), position=5)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=Tensor))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=Tensor))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 4614,
        "neg_line": [
            "-size: Tuple[torch.Tensor, torch.Tensor]) -> torch.Tensor:"
        ],
        "pos_line": [
            "+size: Tuple[int, int]) -> torch.Tensor:"
        ],
        "core_change": "-size: Tuple[torch.Tensor, torch.Tensor]) -> torch.Tensor: +size: Tuple[int, int]) -> torch.Tensor:",
        "core_API": "center_crop"
    },
    {
        "commit_hash": "645c7c386e62d2fb1d50f4621c1a52645a13869f",
        "index": "c99361c..e1e17e1 100644",
        "commit_message": "codemod for 0.4 (#331) (#336)\n\n* codemod for 0.4\n\n* better IN key removal\n\n* fix ac\n\n* update to use some device\n\n* rnn flatten_param\n",
        "file": "examples.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class UpsampleConvLayer(torch.nn.Module):",
            "super(UpsampleConvLayer, self).__init__()",
            "self.upsample = upsample",
            "if upsample:",
            "-            self.upsample_layer = torch.nn.UpsamplingNearest2d(scale_factor=upsample)",
            "+            self.upsample_layer = torch.nn.Upsample(mode='nearest', scale_factor=upsample)",
            "reflection_padding = kernel_size // 2",
            "self.reflection_pad = torch.nn.ReflectionPad2d(reflection_padding)",
            "self.conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=UpsamplingNearest2d), value='Upsample')",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=1, insert_id=1346604)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1346605)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'mode'), position=0, insert_id=1346606)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1346607)",
            "Insert(target_node=IN(type=keyword_argument), node=('string', \"'nearest'\"), position=2, insert_id=1346608)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 4615,
        "neg_line": [
            "-self.upsample_layer = torch.nn.UpsamplingNearest2d(scale_factor=upsample)"
        ],
        "pos_line": [
            "+self.upsample_layer = torch.nn.Upsample(mode='nearest', scale_factor=upsample)"
        ],
        "core_change": "-self.upsample_layer = torch.nn.UpsamplingNearest2d(scale_factor=upsample) +self.upsample_layer = torch.nn.Upsample(mode='nearest', scale_factor=upsample)",
        "core_API": "UpsamplingNearest2d"
    },
    {
        "commit_hash": "df3e2cb4cceb47a36ceed0a261cceeb27e7a6e00",
        "index": "eea66a4d..f9778ab9 100644",
        "commit_message": "Layer API Refactoring (#675)\n\n* Layer API Refactoring\n\n* private method decorator added\n\n* Input Layers added\n\n* Lazy Import on ROI pooling\n\n* @zsdonghao change proposal added\n\n* UTF-8 header added\n\n* gitignore updated\n\n* Python2 error correction\n\n* Changelog Updated\n\n* Python Headers Uniformized + Codacy Error Fix + LSTMStateTuple missing import\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "layers",
        "change": [
            "class PReluLayer(Layer):",
            "except Exception:  # TF 0.12",
            "self.outputs = tf.nn.relu(self.inputs) + tf.mul(alphas, (self.inputs - tf.abs(self.inputs))) * 0.5",
            "",
            "-        self.all_layers.append(self.outputs)",
            "-        self.all_params.extend([alphas])",
            "+        self._add_layers(self.outputs)",
            "+        self._add_params([alphas])"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=all_layers), value='_add_layers')",
            "Update(target_node=ASTNode(type=identifier, text=all_params), value='_add_params')",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=append))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=extend))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 10,
        "number": 4617,
        "neg_line": [
            "-self.all_layers.append(self.outputs)",
            "-self.all_params.extend([alphas])"
        ],
        "pos_line": [
            "+self._add_layers(self.outputs)",
            "+self._add_params([alphas])"
        ],
        "core_change": "-self.all_layers.append(self.outputs) -self.all_params.extend([alphas]) +self._add_layers(self.outputs) +self._add_params([alphas])",
        "core_API": "relu"
    },
    {
        "commit_hash": "43043ee4d5c672f7fbc22ac9559e8164731fd053",
        "index": "abdef2fc8c..c6d55fa0f6 100644",
        "commit_message": "[RLlib] Tf2x preparation; part 2 (upgrading `try_import_tf()`). (#9136)\n\n* WIP.\n\n* Fixes.\n\n* LINT.\n\n* WIP.\n\n* WIP.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* WIP.\n\n* Fixes.\n\n* Test\n\n* Fix.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* LINT.\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def scope_vars(scope, trainable_only=False):",
            "vars: [tf.Variable]",
            "list of variables in `scope`.",
            "\"\"\"",
            "-    return tf.get_collection(",
            "-        tf.GraphKeys.TRAINABLE_VARIABLES",
            "-        if trainable_only else tf.GraphKeys.VARIABLES,",
            "+    return tf1.get_collection(",
            "+        tf1.GraphKeys.TRAINABLE_VARIABLES",
            "+        if trainable_only else tf1.GraphKeys.VARIABLES,",
            "scope=scope if isinstance(scope, str) else scope.name)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tf), value='tf1')",
            "Update(target_node=ASTNode(type=identifier, text=tf), value='tf1')",
            "Update(target_node=ASTNode(type=identifier, text=tf), value='tf1')"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 3,
        "number": 4620,
        "neg_line": [
            "-return tf.get_collection(",
            "-tf.GraphKeys.TRAINABLE_VARIABLES",
            "-if trainable_only else tf.GraphKeys.VARIABLES,"
        ],
        "pos_line": [
            "+return tf1.get_collection(",
            "+tf1.GraphKeys.TRAINABLE_VARIABLES",
            "+if trainable_only else tf1.GraphKeys.VARIABLES,"
        ],
        "core_change": "-return tf.get_collection( -tf.GraphKeys.TRAINABLE_VARIABLES -if trainable_only else tf.GraphKeys.VARIABLES, +return tf1.get_collection( +tf1.GraphKeys.TRAINABLE_VARIABLES +if trainable_only else tf1.GraphKeys.VARIABLES,",
        "core_API": "get_collection"
    },
    {
        "commit_hash": "89879e2c76e86c685e44c47a6cdb82f7e645c142",
        "index": "64c33ea9..93d8ba26 100644",
        "commit_message": "Fix keras docstrings\n\nPiperOrigin-RevId: 424275818\n\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Optimizer(_BaseOptimizer):",
            ">>> opt = tf.keras.optimizers.experimental.SGD(learning_rate=1, clipvalue=1)",
            ">>> var1, var2 = tf.Variable(2.0), tf.Variable(2.0)",
            ">>> with tf.GradientTape() as tape:",
            "-    ... loss = 2 * var1 + 2 * var2",
            "+  ...   loss = 2 * var1 + 2 * var2",
            ">>> grads = tape.gradient(loss, [var1, var2])",
            ">>> print([grads[0].numpy(), grads[1].numpy()])",
            "-  [2.0., 2.0]",
            "+  [2.0, 2.0]",
            ">>> opt.apply_gradients(zip(grads, [var1, var2]))",
            ">>> # Without clipping, we should get [0, 0], but as gradients are clipped to",
            ">>> # have max value 1, we get [1.0, 1.0]."
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=list), node=ASTNode(type=float, text=2.0), position=1)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 4,
        "number": 4622,
        "neg_line": [
            "-... loss = 2 * var1 + 2 * var2",
            "-[2.0., 2.0]"
        ],
        "pos_line": [
            "+...   loss = 2 * var1 + 2 * var2",
            "+[2.0, 2.0]"
        ],
        "core_change": "-... loss = 2 * var1 + 2 * var2 +...   loss = 2 * var1 + 2 * var2 -[2.0., 2.0] +[2.0, 2.0]",
        "core_API": "SGD"
    },
    {
        "commit_hash": "f5dbcd8128f21a8fca7e2b036f8162319cd162a8",
        "index": "e461d502..160a20de 100644",
        "commit_message": "Fix layer choice on IT and deprecate \"choices\" and \"length\" (#2386)\n\n\n",
        "file": "nni.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Mutator(BaseMutator):",
            "\"\"\"",
            "if self._connect_all:",
            "return self._all_connect_tensor_reduction(mutable.reduction,",
            "-                                                      [op(*args, **kwargs) for op in mutable.choices]), \\",
            "-                torch.ones(mutable.length)",
            "+                                                      [op(*args, **kwargs) for op in mutable]), \\",
            "+                torch.ones(len(mutable))",
            "",
            "def _map_fn(op, args, kwargs):",
            "return op(*args, **kwargs)",
            "",
            "mask = self._get_decision(mutable)",
            "-        assert len(mask) == len(mutable.choices), \\",
            "-            \"Invalid mask, expected {} to be of length {}.\".format(mask, len(mutable.choices))",
            "-        out = self._select_with_mask(_map_fn, [(choice, args, kwargs) for choice in mutable.choices], mask)",
            "+        assert len(mask) == len(mutable), \\",
            "+            \"Invalid mask, expected {} to be of length {}.\".format(mask, len(mutable))",
            "+        out = self._select_with_mask(_map_fn, [(choice, args, kwargs) for choice in mutable], mask)",
            "return self._tensor_reduction(mutable.reduction, out), mask",
            "",
            "def on_forward_input_choice(self, mutable, tensor_list):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"\"\"\nif self._connect_all:\nreturn self._all_connect_tensor_reduction(mutable.reduction,\n                                                      [op(*args, **kwargs) for op in mutable.choices]), \\\n                torch.ones(mutable.length)\n\ndef _map_fn(op, args, kwargs):\nreturn op(*args, **kwargs)\n\nmask = self._get_decision(mutable)\n        assert len(mask) == len(mutable.choices), \\\n            \"Invalid mask, expected {} to be of length {}.\"), value='\"\"\"\\nif self._connect_all:\\nreturn self._all_connect_tensor_reduction(mutable.reduction,\\n                                                      [op(*args, **kwargs) for op in mutable]), \\\\\\n                torch.ones(len(mutable))\\n\\ndef _map_fn(op, args, kwargs):\\nreturn op(*args, **kwargs)\\n\\nmask = self._get_decision(mutable)\\n        assert len(mask) == len(mutable), \\\\\\n            \"Invalid mask, expected {} to be of length {}.\"')",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=mutable), position=1)",
            "Move(target_node=ASTNode(type=for_in_clause), node=ASTNode(type=identifier, text=mutable), position=3)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=choices))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=choices))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 5,
        "minus_line": 5,
        "AST_diff_line": 9,
        "number": 4623,
        "neg_line": [
            "-[op(*args, **kwargs) for op in mutable.choices]), \\",
            "-torch.ones(mutable.length)",
            "-assert len(mask) == len(mutable.choices), \\",
            "-\"Invalid mask, expected {} to be of length {}.\".format(mask, len(mutable.choices))",
            "-out = self._select_with_mask(_map_fn, [(choice, args, kwargs) for choice in mutable.choices], mask)"
        ],
        "pos_line": [
            "+[op(*args, **kwargs) for op in mutable]), \\",
            "+torch.ones(len(mutable))",
            "+assert len(mask) == len(mutable), \\",
            "+\"Invalid mask, expected {} to be of length {}.\".format(mask, len(mutable))",
            "+out = self._select_with_mask(_map_fn, [(choice, args, kwargs) for choice in mutable], mask)"
        ],
        "core_change": "-[op(*args, **kwargs) for op in mutable.choices]), \\ -torch.ones(mutable.length) +[op(*args, **kwargs) for op in mutable]), \\ +torch.ones(len(mutable)) -assert len(mask) == len(mutable.choices), \\ -\"Invalid mask, expected {} to be of length {}.\".format(mask, len(mutable.choices)) -out = self._select_with_mask(_map_fn, [(choice, args, kwargs) for choice in mutable.choices], mask) +assert len(mask) == len(mutable), \\ +\"Invalid mask, expected {} to be of length {}.\".format(mask, len(mutable)) +out = self._select_with_mask(_map_fn, [(choice, args, kwargs) for choice in mutable], mask)",
        "core_API": "_all_connect_tensor_reduction"
    },
    {
        "commit_hash": "58ec8c0f28250419ea10dded02ceee9b74626e24",
        "index": "5ad0a065..c45d0e74 100644",
        "commit_message": "Fix MHA with bias=False (fixes #1527)\n\nSummary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/1528\n\nDifferential Revision: D19178445\n\nPulled By: myleott\n\nfbshipit-source-id: 2e08012205de825ded334222d29797f2c125f15e\n\n",
        "file": "fairseq.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class MultiheadAttention(nn.Module):",
            "nn.init.xavier_uniform_(self.q_proj.weight)",
            "",
            "nn.init.xavier_uniform_(self.out_proj.weight)",
            "-        nn.init.constant_(self.out_proj.bias, 0.)",
            "+        if self.out_proj.bias is not None:",
            "+            nn.init.constant_(self.out_proj.bias, 0.)",
            "if self.bias_k is not None:",
            "nn.init.xavier_normal_(self.bias_k)",
            "if self.bias_v is not None:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=3, insert_id=1356874)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=1356875)",
            "Insert(target_node=IN(type=if_statement), node=('comparison_operator', None), position=1, insert_id=1356876)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=1356877)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=1356878)",
            "Insert(target_node=IN(type=comparison_operator), node=('attribute', None), position=0, insert_id=1356879)",
            "Insert(target_node=IN(type=comparison_operator), node=('is not', 'is'), position=1, insert_id=1356880)",
            "Insert(target_node=IN(type=comparison_operator), node=('is not', 'not'), position=2, insert_id=1356881)",
            "Insert(target_node=IN(type=comparison_operator), node=('none', 'None'), position=3, insert_id=1356882)",
            "Move(target_node=IN(type=block), node=ASTNode(type=expression_statement), position=0)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1356883)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1356884)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'bias'), position=2, insert_id=1356885)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=1356886)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1356887)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'out_proj'), position=2, insert_id=1356888)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 16,
        "number": 4624,
        "neg_line": [
            "-nn.init.constant_(self.out_proj.bias, 0.)"
        ],
        "pos_line": [
            "+if self.out_proj.bias is not None:",
            "+nn.init.constant_(self.out_proj.bias, 0.)"
        ],
        "core_change": "-nn.init.constant_(self.out_proj.bias, 0.) +if self.out_proj.bias is not None: +nn.init.constant_(self.out_proj.bias, 0.)",
        "core_API": "xavier_uniform_"
    },
    {
        "commit_hash": "edb78974b84b324df08ec198e6ff7f267ee23d33",
        "index": "a7d4e65a..ff5d3a60 100644",
        "commit_message": "upgrade detection / some v2 compat fix\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def encode_bbox_target(boxes, anchors):",
            "",
            "# Note that here not all boxes are valid. Some may be zero",
            "txty = (xbyb - xaya) / waha",
            "-    twth = tf.log(wbhb / waha)  # may contain -inf for invalid boxes",
            "+    twth = tf.math.log(wbhb / waha)  # may contain -inf for invalid boxes",
            "encoded = tf.concat([txty, twth], axis=1)  # (-1x2x2)",
            "return tf.reshape(encoded, tf.shape(boxes))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=2273097)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=2273098)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=tf), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'math'), position=2, insert_id=2273099)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 4625,
        "neg_line": [
            "-twth = tf.log(wbhb / waha)  # may contain -inf for invalid boxes"
        ],
        "pos_line": [
            "+twth = tf.math.log(wbhb / waha)  # may contain -inf for invalid boxes"
        ],
        "core_change": "-twth = tf.log(wbhb / waha)  # may contain -inf for invalid boxes +twth = tf.math.log(wbhb / waha)  # may contain -inf for invalid boxes",
        "core_API": "log"
    },
    {
        "commit_hash": "1fcef7cabce7d03f10ec892641c777a2fd25e52d",
        "index": "8a53afa..ce802d9 100644",
        "commit_message": "Fix Leaky ReLU activation (#784)\n\n* Fix leaky ReLU activation\n\n- fix bug in activations.leaky_relu that is making it act like an ordinary ReLU\n\n* Add tests for leaky_relu\n\n",
        "file": "tflearn.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def leaky_relu(x, alpha=0.1, name=\"LeakyReLU\"):",
            "\"\"\"",
            "",
            "with tf.name_scope(name) as scope:",
            "-        x = tf.nn.relu(x)",
            "m_x = tf.nn.relu(-x)",
            "+        x = tf.nn.relu(x)",
            "x -= alpha * m_x",
            "",
            "x.scope = scope"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=module), node=ASTNode(type=expression_statement), position=4)",
            "Insert(target_node=ASTNode(type=with_statement), node=('block', ''), position=3, insert_id=2350781)",
            "Delete(target_node=ASTNode(type=block))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 3,
        "number": 4626,
        "neg_line": [
            "-x = tf.nn.relu(x)"
        ],
        "pos_line": [
            "+x = tf.nn.relu(x)"
        ],
        "core_change": "-x = tf.nn.relu(x) +x = tf.nn.relu(x)",
        "core_API": "name_scope"
    },
    {
        "commit_hash": "a3efa433eac5feba842350c38a1db29244963fb5",
        "index": "0f1a4022..612c24f8 100644",
        "commit_message": "Fix DDIM on Windows not using int64 for timesteps (#819)\n\n\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DDIMScheduler(SchedulerMixin, ConfigMixin):",
            "step_ratio = self.config.num_train_timesteps // self.num_inference_steps",
            "# creates integer timesteps by multiplying by ratio",
            "# casting to int to avoid issues when num_inference_step is power of 3",
            "-        timesteps = (np.arange(0, num_inference_steps) * step_ratio).round()[::-1].copy()",
            "+        timesteps = (np.arange(0, num_inference_steps) * step_ratio).round()[::-1].copy().astype(np.int64)",
            "self.timesteps = torch.from_numpy(timesteps).to(device)",
            "self.timesteps += offset"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=102323)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=102324)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=102325)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'astype'), position=2, insert_id=102326)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=102327)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=102328)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=102329)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'np'), position=0, insert_id=102330)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=102331)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'int64'), position=2, insert_id=102332)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 4631,
        "neg_line": [
            "-timesteps = (np.arange(0, num_inference_steps) * step_ratio).round()[::-1].copy()"
        ],
        "pos_line": [
            "+timesteps = (np.arange(0, num_inference_steps) * step_ratio).round()[::-1].copy().astype(np.int64)"
        ],
        "core_change": "-timesteps = (np.arange(0, num_inference_steps) * step_ratio).round()[::-1].copy() +timesteps = (np.arange(0, num_inference_steps) * step_ratio).round()[::-1].copy().astype(np.int64)",
        "core_API": "arange"
    },
    {
        "commit_hash": "2295d783d5787bcd4c99ea0ddb2a9403697fc126",
        "index": "b5e02fb64..d78d582f3 100644",
        "commit_message": "Copy tokenizer files in each of their repo (#10624)\n\n* Move tokenizer files in each repo\n\n* Fix mBART50 tests\n\n* Fix mBART tests\n\n* Fix Marian tests\n\n* Update templates\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "models",
        "change": [
            "class MarianTokenizationTest(TokenizerTesterMixin, unittest.TestCase):",
            "vocab = [\"</s>\", \"<unk>\", \"This\", \"is\", \"a\", \"t\", \"est\", \"\\u0120\", \"<pad>\"]",
            "vocab_tokens = dict(zip(vocab, range(len(vocab))))",
            "save_dir = Path(self.tmpdirname)",
            "-        save_json(vocab_tokens, save_dir / vocab_files_names[\"vocab\"])",
            "-        save_json(mock_tokenizer_config, save_dir / vocab_files_names[\"tokenizer_config_file\"])",
            "-        if not (save_dir / vocab_files_names[\"source_spm\"]).exists():",
            "-            copyfile(SAMPLE_SP, save_dir / vocab_files_names[\"source_spm\"])",
            "-            copyfile(SAMPLE_SP, save_dir / vocab_files_names[\"target_spm\"])",
            "+        save_json(vocab_tokens, save_dir / VOCAB_FILES_NAMES[\"vocab\"])",
            "+        save_json(mock_tokenizer_config, save_dir / VOCAB_FILES_NAMES[\"tokenizer_config_file\"])",
            "+        if not (save_dir / VOCAB_FILES_NAMES[\"source_spm\"]).exists():",
            "+            copyfile(SAMPLE_SP, save_dir / VOCAB_FILES_NAMES[\"source_spm\"])",
            "+            copyfile(SAMPLE_SP, save_dir / VOCAB_FILES_NAMES[\"target_spm\"])",
            "",
            "tokenizer = MarianTokenizer.from_pretrained(self.tmpdirname)",
            "tokenizer.save_pretrained(self.tmpdirname)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=vocab_files_names), value='VOCAB_FILES_NAMES')",
            "Update(target_node=ASTNode(type=identifier, text=vocab_files_names), value='VOCAB_FILES_NAMES')",
            "Update(target_node=ASTNode(type=identifier, text=vocab_files_names), value='VOCAB_FILES_NAMES')",
            "Update(target_node=ASTNode(type=identifier, text=vocab_files_names), value='VOCAB_FILES_NAMES')",
            "Update(target_node=ASTNode(type=identifier, text=vocab_files_names), value='VOCAB_FILES_NAMES')"
        ],
        "plus_line": 5,
        "minus_line": 5,
        "AST_diff_line": 5,
        "number": 4640,
        "neg_line": [
            "-save_json(vocab_tokens, save_dir / vocab_files_names[\"vocab\"])",
            "-save_json(mock_tokenizer_config, save_dir / vocab_files_names[\"tokenizer_config_file\"])",
            "-if not (save_dir / vocab_files_names[\"source_spm\"]).exists():",
            "-copyfile(SAMPLE_SP, save_dir / vocab_files_names[\"source_spm\"])",
            "-copyfile(SAMPLE_SP, save_dir / vocab_files_names[\"target_spm\"])"
        ],
        "pos_line": [
            "+save_json(vocab_tokens, save_dir / VOCAB_FILES_NAMES[\"vocab\"])",
            "+save_json(mock_tokenizer_config, save_dir / VOCAB_FILES_NAMES[\"tokenizer_config_file\"])",
            "+if not (save_dir / VOCAB_FILES_NAMES[\"source_spm\"]).exists():",
            "+copyfile(SAMPLE_SP, save_dir / VOCAB_FILES_NAMES[\"source_spm\"])",
            "+copyfile(SAMPLE_SP, save_dir / VOCAB_FILES_NAMES[\"target_spm\"])"
        ],
        "core_change": "-save_json(vocab_tokens, save_dir / vocab_files_names[\"vocab\"]) -save_json(mock_tokenizer_config, save_dir / vocab_files_names[\"tokenizer_config_file\"]) -if not (save_dir / vocab_files_names[\"source_spm\"]).exists(): -copyfile(SAMPLE_SP, save_dir / vocab_files_names[\"source_spm\"]) -copyfile(SAMPLE_SP, save_dir / vocab_files_names[\"target_spm\"]) +save_json(vocab_tokens, save_dir / VOCAB_FILES_NAMES[\"vocab\"]) +save_json(mock_tokenizer_config, save_dir / VOCAB_FILES_NAMES[\"tokenizer_config_file\"]) +if not (save_dir / VOCAB_FILES_NAMES[\"source_spm\"]).exists(): +copyfile(SAMPLE_SP, save_dir / VOCAB_FILES_NAMES[\"source_spm\"]) +copyfile(SAMPLE_SP, save_dir / VOCAB_FILES_NAMES[\"target_spm\"])",
        "core_API": "from_pretrained"
    },
    {
        "commit_hash": "d0bcf1a87ee87539ceb29bcc976d4da063ffc47b",
        "index": "1ad82af..f395d93 100644",
        "commit_message": "tensorflow: fix bug in broadcast_variables (#416)\n\nWhen there's only one rank in total, broadcast_variables should still\nreturn a tf operation.\n\nSigned-off-by: Yulu Jia <yulu.jia@bytedance.com>\n",
        "file": "byteps.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def broadcast_variables(variables, root_rank, scope=''):",
            "scope: the graph name scope",
            "\"\"\"",
            "if size() <= 1:",
            "-        return variables",
            "+        return tf.group(*variables)",
            "_assign = tf.assign if hasattr(tf, 'assign') else tf.compat.v1.assign",
            "return tf.group(*[_assign(var, broadcast(var, root_rank, scope))",
            "for var in variables])"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=return_statement), node=('call', None), position=1, insert_id=1907959)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1907960)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1907961)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=1907962)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1907963)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'group'), position=2, insert_id=1907964)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1907965)",
            "Insert(target_node=IN(type=argument_list), node=('list_splat', None), position=1, insert_id=1907966)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1907967)",
            "Insert(target_node=IN(type=list_splat), node=('*', '*'), position=0, insert_id=1907968)",
            "Move(target_node=IN(type=list_splat), node=ASTNode(type=identifier, text=variables), position=1)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 4642,
        "neg_line": [
            "-return variables"
        ],
        "pos_line": [
            "+return tf.group(*variables)"
        ],
        "core_change": "-return variables +return tf.group(*variables)",
        "core_API": "group"
    },
    {
        "commit_hash": "63a997cdd385d50b86c4a9f34aaa61cac7ffd982",
        "index": "f072ef56..7f6fdbf2 100755",
        "commit_message": "Fix seed unittest, internal Tensorforce model improvements\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Queue(Memory):",
            "return lengths, successor_indices, mask",
            "",
            "lengths = tf.ones_like(input=indices, dtype=tf_util.get_dtype(type='int'))",
            "-        successor_indices = tf.expand_dims(input=indices, axis=1)",
            "+        successor_indices = tf.math.mod(x=tf.expand_dims(input=indices, axis=1), y=capacity)",
            "mask = tf.ones_like(input=successor_indices, dtype=tf_util.get_dtype(type='bool'))",
            "shape = tf.TensorShape(dims=((None, None)))"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2222198)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=2222199)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2222200)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2222201)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'mod'), position=2, insert_id=2222202)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2222203)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=1, insert_id=2222204)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=2222205)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=3, insert_id=2222206)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=2222207)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2222208)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2222209)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'math'), position=2, insert_id=2222210)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'x'), position=0, insert_id=2222211)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2222212)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=call), position=2)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'y'), position=0, insert_id=2222213)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2222214)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'capacity'), position=2, insert_id=2222215)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 19,
        "number": 4644,
        "neg_line": [
            "-successor_indices = tf.expand_dims(input=indices, axis=1)"
        ],
        "pos_line": [
            "+successor_indices = tf.math.mod(x=tf.expand_dims(input=indices, axis=1), y=capacity)"
        ],
        "core_change": "-successor_indices = tf.expand_dims(input=indices, axis=1) +successor_indices = tf.math.mod(x=tf.expand_dims(input=indices, axis=1), y=capacity)",
        "core_API": "ones_like"
    },
    {
        "commit_hash": "3f0f2a059743593fd07b629c261b609bd9a767e6",
        "index": "bc9ce689..1f241ae3 100644",
        "commit_message": "[Feature] Support efficientnet in mmdetection. (#7514)\n\n* Initial implementation\n\n* Add missing import\n\n* Add MemoryEfficientSwishImplementation. Add docstrings\n\n* Add efficientnet2mmdet tool\n\n* Add config folder\n\n* Flake8\n\n* Flake8\n\n* Flake8\n\n* Fix config\n\n* Requested changes\n\n* docformatter\n\n* Update train config from https://github.com/google/automl/blob/master/efficientdet\n\n* Run pre-commit\n\n* Fix schedule\n\n* Set by_epoch=False in scheduler\n\n* Train 80 epochs\n\n* Remove duplicated arg\n\n* Update README.md\n\n* efficient3 efficient0\n\n* efficientNet imports\n\n* efficientNet\n\n* config edit path for eff3 and dropout for eff0\n\n* efficientnet review2\n\n* fix model_converter location and drop path\n\n* fix model converter  and efficientnet import\n\n* register memoryefficietnswish\n\n* eff0, eff3\n\n* fix  flake8 yapf isort\n\n* same padding in tensorflow and edit drop path rate\n\n* fix init of utils\n\n* Align mmdet utils with mmcls\n\n* Align mmdet.models.utils with mmcls\n\n* Use mmcls efficientnet backbone\n\n* Update\n\n* Update\n\n* Update metafile\n\nCo-authored-by: David de la Iglesia Castro <daviddelaiglesiacastro@gmail.com>\nCo-authored-by: David de la Iglesia Castro <diglesia@gradiant.org>\nCo-authored-by: jiangyitong <jiangyitong1@sensetime.com>\nCo-authored-by: jiangyitong <jiangyitong1998@outlook.com>\n",
        "file": "mmdetection.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class InvertedResidual(BaseModule):",
            "out = self.linear_conv(out)",
            "",
            "if self.with_res_shortcut:",
            "-                return x + out",
            "+                return x + self.drop_path(out)",
            "else:",
            "return out"
        ],
        "hunk_index": 4,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=binary_operator), node=('call', None), position=2, insert_id=1423643)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1423644)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1423645)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=1423646)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1423647)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'drop_path'), position=2, insert_id=1423648)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1423649)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=out), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1423650)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 4646,
        "neg_line": [
            "-return x + out"
        ],
        "pos_line": [
            "+return x + self.drop_path(out)"
        ],
        "core_change": "-return x + out +return x + self.drop_path(out)",
        "core_API": "linear_conv"
    },
    {
        "commit_hash": "d8c1f685b60456401acb6137c2629332c4a73d2b",
        "index": "e60f93ab..1ae38f70 100644",
        "commit_message": "Change to use F-string (#2531)\n\n* Change .format to f-string\n\n* Resolve comments\n\n* Fix missing change\n",
        "file": "mmdetection.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_nn_op_forward_called():",
            "torch.__version__ = '1.4.1'",
            "",
            "for m in ['Conv2d', 'ConvTranspose2d', 'MaxPool2d']:",
            "-        with patch('torch.nn.{}.forward'.format(m)) as nn_module_forward:",
            "+        with patch(f'torch.nn.{m}.forward') as nn_module_forward:",
            "# randn input",
            "x_empty = torch.randn(0, 3, 10, 10)",
            "wrapper = eval(m)(3, 2, 1)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('string', \"f'torch.nn.{m}.forward'\"), position=1, insert_id=639084)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=), text=)), position=2)",
            "Delete(target_node=ASTNode(type=string, text='torch.nn.{}.forward'))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=format))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=m))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=), text=)))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 4647,
        "neg_line": [
            "-with patch('torch.nn.{}.forward'.format(m)) as nn_module_forward:"
        ],
        "pos_line": [
            "+with patch(f'torch.nn.{m}.forward') as nn_module_forward:"
        ],
        "core_change": "-with patch('torch.nn.{}.forward'.format(m)) as nn_module_forward: +with patch(f'torch.nn.{m}.forward') as nn_module_forward:",
        "core_API": "randn"
    },
    {
        "commit_hash": "92d1ddc470f555dbbc1831005cf970ad1e34ad0b",
        "index": "1f0d587..a6802ee 100644",
        "commit_message": "Fix classification with only one class\n\n",
        "file": "PyTorch-YOLOv3.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def compute_loss(predictions, targets, model):  # predictions, targets, model",
            "tobj[b, anchor, grid_j, grid_i] = (1.0 - model.gr) + model.gr * iou.detach().clamp(0).type(tobj.dtype)  # iou ratio",
            "",
            "# Classification",
            "-            t = torch.full_like(ps[:, 5:], cn, device=device)  # targets",
            "-            t[range(num_targets), tcls[layer_index]] = cp",
            "-            lcls += BCEcls(ps[:, 5:], t)  # BCE",
            "+            if ps.size(1) - 5 > 1:",
            "+                t = torch.full_like(ps[:, 5:], cn, device=device)  # targets",
            "+                t[range(num_targets), tcls[layer_index]] = cp",
            "+                lcls += BCEcls(ps[:, 5:], t)  # BCE",
            "",
            "lobj += BCEobj(layer_predictions[..., 4], tobj) * balance[layer_index]  # obj loss"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=2, insert_id=905570)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=905571)",
            "Insert(target_node=IN(type=if_statement), node=('comparison_operator', None), position=1, insert_id=905572)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=905573)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=905574)",
            "Insert(target_node=IN(type=comparison_operator), node=('binary_operator', None), position=0, insert_id=905575)",
            "Insert(target_node=IN(type=comparison_operator), node=('>', '>'), position=1, insert_id=905576)",
            "Insert(target_node=IN(type=comparison_operator), node=('integer', '1'), position=2, insert_id=905577)",
            "Move(target_node=IN(type=block), node=ASTNode(type=expression_statement), position=0)",
            "Move(target_node=IN(type=block), node=ASTNode(type=expression_statement), position=1)",
            "Move(target_node=IN(type=block), node=ASTNode(type=expression_statement), position=2)",
            "Insert(target_node=IN(type=binary_operator), node=('call', None), position=0, insert_id=905578)",
            "Insert(target_node=IN(type=binary_operator), node=('-', '-'), position=1, insert_id=905579)",
            "Insert(target_node=IN(type=binary_operator), node=('integer', '5'), position=2, insert_id=905580)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=905581)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=905582)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'ps'), position=0, insert_id=905583)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=905584)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'size'), position=2, insert_id=905585)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=905586)",
            "Insert(target_node=IN(type=argument_list), node=('integer', '1'), position=1, insert_id=905587)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=905588)"
        ],
        "plus_line": 4,
        "minus_line": 3,
        "AST_diff_line": 22,
        "number": 4650,
        "neg_line": [
            "-t = torch.full_like(ps[:, 5:], cn, device=device)  # targets",
            "-t[range(num_targets), tcls[layer_index]] = cp",
            "-lcls += BCEcls(ps[:, 5:], t)  # BCE"
        ],
        "pos_line": [
            "+if ps.size(1) - 5 > 1:",
            "+t = torch.full_like(ps[:, 5:], cn, device=device)  # targets",
            "+t[range(num_targets), tcls[layer_index]] = cp",
            "+lcls += BCEcls(ps[:, 5:], t)  # BCE"
        ],
        "core_change": "-t = torch.full_like(ps[:, 5:], cn, device=device)  # targets -t[range(num_targets), tcls[layer_index]] = cp -lcls += BCEcls(ps[:, 5:], t)  # BCE +if ps.size(1) - 5 > 1: +t = torch.full_like(ps[:, 5:], cn, device=device)  # targets +t[range(num_targets), tcls[layer_index]] = cp +lcls += BCEcls(ps[:, 5:], t)  # BCE",
        "core_API": "detach"
    },
    {
        "commit_hash": "679910496e5244409fb3d4da85cef19af8306545",
        "index": "20e6f17411..2fc953a58b 100644",
        "commit_message": "fix policy gradients for mujoco domains (#589)\n\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Categorical(object):",
            "class DiagGaussian(object):",
            "def __init__(self, flat):",
            "self.flat = flat",
            "-    mean, logstd = tf.split(1, 2, flat)",
            "+    mean, logstd = tf.split(flat, 2, axis=1)",
            "self.mean = mean",
            "self.logstd = logstd",
            "self.std = tf.exp(logstd)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=integer, text=2), node=ASTNode(type=argument_list), position=3)",
            "Move(target_node=ASTNode(type=,, text=,), node=ASTNode(type=argument_list), position=4)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=2156329)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=2156330)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'axis'), position=0, insert_id=2156331)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2156332)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=integer, text=1), position=2)",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 4654,
        "neg_line": [
            "-mean, logstd = tf.split(1, 2, flat)"
        ],
        "pos_line": [
            "+mean, logstd = tf.split(flat, 2, axis=1)"
        ],
        "core_change": "-mean, logstd = tf.split(1, 2, flat) +mean, logstd = tf.split(flat, 2, axis=1)",
        "core_API": "split"
    },
    {
        "commit_hash": "5ec6553f942b60ce5c42718b0af6914f19539b77",
        "index": "267a449226..54eb5b4aac 100644",
        "commit_message": "small fix to torch.device to fix torch related tests (#2230)\n\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "torch_scatter = None",
            "def dev(x: torch.Tensor, as_native: bool = False) -> Union[ivy.Device, torch.device]:",
            "dv = x.device",
            "if as_native:",
            "-        return torch.device(dv.replace(\"gpu\", \"cuda\"))",
            "+        return torch.device(dv.type.replace(\"gpu\", \"cuda\"))",
            "return as_ivy_dev(dv)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=347848)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=347849)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=dv), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'type'), position=2, insert_id=347850)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 4655,
        "neg_line": [
            "-return torch.device(dv.replace(\"gpu\", \"cuda\"))"
        ],
        "pos_line": [
            "+return torch.device(dv.type.replace(\"gpu\", \"cuda\"))"
        ],
        "core_change": "-return torch.device(dv.replace(\"gpu\", \"cuda\")) +return torch.device(dv.type.replace(\"gpu\", \"cuda\"))",
        "core_API": "device"
    },
    {
        "commit_hash": "f48b1101ebbb160d000180668bf4abd70416375f",
        "index": "02577af5..972a494a 100644",
        "commit_message": "Add name for shuflle layer.\n\nfix a bug.\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Shuffle(Layer):",
            "assert in_channel % self.group == 0",
            "temp = tf.reshape(inputs, [-1, h, w, in_channel // self.group, self.group])",
            "temp = tf.transpose(temp, [0, 1, 2, 4, 3])",
            "-        outputs = tf.reshape(temp, [-1, h, w, in_channel])",
            "+        outputs = tf.reshape(temp, [-1, h, w, in_channel],name=self.name)",
            "return outputs"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=2256167)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=2256168)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'name'), position=0, insert_id=2256169)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2256170)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=2256171)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=2256172)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2256173)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'name'), position=2, insert_id=2256174)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 4661,
        "neg_line": [
            "-outputs = tf.reshape(temp, [-1, h, w, in_channel])"
        ],
        "pos_line": [
            "+outputs = tf.reshape(temp, [-1, h, w, in_channel],name=self.name)"
        ],
        "core_change": "-outputs = tf.reshape(temp, [-1, h, w, in_channel]) +outputs = tf.reshape(temp, [-1, h, w, in_channel],name=self.name)",
        "core_API": "reshape"
    },
    {
        "commit_hash": "32484500befa825b3def2cdc15cb12eb2251f681",
        "index": "2a953db..bf9216a 100644",
        "commit_message": "tutorial fixes from #336. Wheels with cuda10.1.\n\nSummary:\nAdd a document to explain how to run the tutorials.\nFix API of TexturesVertex in fit_textured_mesh.\nPrepare cuda 10.1 wheels (not 10.2) for linux to be available on pypi - this matches what colab has.\nChange the tutorials to use these new wheels.\n\nReviewed By: gkioxari\n\nDifferential Revision: D23324479\n\nfbshipit-source-id: 60e92a3f46a2d878f811b7703638f8d1dae143d9\n\n",
        "file": "pytorch3d.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "\"outputs\": [],",
            "\"source\": [",
            "\"!pip install torch torchvision\\n\",",
            "-    \"!pip install 'git+https://github.com/facebookresearch/pytorch3d.git@stable'\"",
            "+    \"import sys\\n\",",
            "+    \"if torch.__version__=='1.6.0+cu101' and sys.platform.startswith('linux'):\\n\",",
            "+    \"    !pip install pytorch3d\\n\",",
            "+    \"else:\\n\",",
            "+    \"    !pip install 'git+https://github.com/facebookresearch/pytorch3d.git@stable'\"",
            "]",
            "},",
            "{"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"!pip install 'git+https://github.com/facebookresearch/pytorch3d.git@stable'\"), value='\"import sys\\\\n\"')",
            "Insert(target_node=ASTNode(type=subscript), node=(',', ','), position=6, insert_id=922873)",
            "Insert(target_node=ASTNode(type=subscript), node=('string', '\"if torch.__version__==\\'1.6.0+cu101\\' and sys.platform.startswith(\\'linux\\'):\\\\n\"'), position=7, insert_id=922874)",
            "Insert(target_node=ASTNode(type=subscript), node=(',', ','), position=8, insert_id=922875)",
            "Insert(target_node=ASTNode(type=subscript), node=('string', '\"    !pip install pytorch3d\\\\n\"'), position=9, insert_id=922876)",
            "Insert(target_node=ASTNode(type=subscript), node=(',', ','), position=10, insert_id=922877)",
            "Insert(target_node=ASTNode(type=subscript), node=('string', '\"else:\\\\n\"'), position=11, insert_id=922878)",
            "Insert(target_node=ASTNode(type=subscript), node=(',', ','), position=12, insert_id=922879)",
            "Insert(target_node=ASTNode(type=subscript), node=('string', '\"    !pip install \\'git+https://github.com/facebookresearch/pytorch3d.git@stable\\'\"'), position=13, insert_id=922880)"
        ],
        "plus_line": 5,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 4662,
        "neg_line": [
            "-\"!pip install 'git+https://github.com/facebookresearch/pytorch3d.git@stable'\""
        ],
        "pos_line": [
            "+\"import sys\\n\",",
            "+\"if torch.__version__=='1.6.0+cu101' and sys.platform.startswith('linux'):\\n\",",
            "+\"    !pip install pytorch3d\\n\",",
            "+\"else:\\n\",",
            "+\"    !pip install 'git+https://github.com/facebookresearch/pytorch3d.git@stable'\""
        ],
        "core_change": "-\"!pip install 'git+https://github.com/facebookresearch/pytorch3d.git@stable'\" +\"import sys\\n\", +\"if torch.__version__=='1.6.0+cu101' and sys.platform.startswith('linux'):\\n\", +\"    !pip install pytorch3d\\n\", +\"else:\\n\", +\"    !pip install 'git+https://github.com/facebookresearch/pytorch3d.git@stable'\"",
        "core_API": "startswith"
    },
    {
        "commit_hash": "84790b78f60221fe6819b5b01ed27c65da12e37a",
        "index": "025744a4..2bb3cc80 100755",
        "commit_message": "re-organize predict/; fix TF incompatibile change of sparse_softmax_cross_entropy_loss\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Model(ModelDesc):",
            "",
            "logits = FullyConnected('linear', l, out_dim=1000, nl=tf.identity)",
            "prob = tf.nn.softmax(logits, name='output')",
            "-        loss3 = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, label)",
            "+        loss3 = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)",
            "loss3 = tf.reduce_mean(loss3, name='loss3')",
            "",
            "cost = tf.add_n([loss3, 0.3 * loss2, 0.3 * loss1], name='weighted_cost')"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=1, insert_id=2307790)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=4, insert_id=2307791)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=identifier, text=logits), position=0)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2307792)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'logits'), position=2, insert_id=2307793)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'labels'), position=0, insert_id=2307794)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2307795)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=identifier, text=label), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 4666,
        "neg_line": [
            "-loss3 = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, label)"
        ],
        "pos_line": [
            "+loss3 = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)"
        ],
        "core_change": "-loss3 = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, label) +loss3 = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)",
        "core_API": "softmax"
    },
    {
        "commit_hash": "433349234142a476ba0b05a4d0d3dd11dbd5d571",
        "index": "b9a03af1..9933df6b 100644",
        "commit_message": "Fix BCE loss issue (#1872)\n\n* Fix BCE loss issue\n\n* Remove import\n",
        "file": "TTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class BCELossMasked(nn.Module):",
            "",
            "def __init__(self, pos_weight: float = None):",
            "super().__init__()",
            "-        self.pos_weight = torch.tensor([pos_weight])",
            "+        self.pos_weight = nn.Parameter(torch.tensor([pos_weight]), requires_grad=False)",
            "",
            "def forward(self, x, target, length):",
            "\"\"\""
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1548574)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=1548575)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'nn'), position=0, insert_id=1548576)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1548577)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Parameter'), position=2, insert_id=1548578)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1548579)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=call), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=1548580)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1548581)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=1548582)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'requires_grad'), position=0, insert_id=1548583)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1548584)",
            "Insert(target_node=IN(type=keyword_argument), node=('false', 'False'), position=2, insert_id=1548585)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 13,
        "number": 4667,
        "neg_line": [
            "-self.pos_weight = torch.tensor([pos_weight])"
        ],
        "pos_line": [
            "+self.pos_weight = nn.Parameter(torch.tensor([pos_weight]), requires_grad=False)"
        ],
        "core_change": "-self.pos_weight = torch.tensor([pos_weight]) +self.pos_weight = nn.Parameter(torch.tensor([pos_weight]), requires_grad=False)",
        "core_API": "tensor"
    },
    {
        "commit_hash": "b73bfafe54ce3badeec6deb9f936dccfa198e098",
        "index": "09c2a0d..0a48766 100644",
        "commit_message": "Fix beam search input transform\n\n",
        "file": "seq2seq.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class AttentionDecoder(DecoderBase):",
            "initial_call=False,",
            "predictions=outputs)",
            "",
            "-    next_inputs = tf.concat([next_inputs, attention_context], 1)",
            "+    next_inputs = self.transform_inputs(next_inputs, outputs)",
            "",
            "return (outputs, cell_state, next_inputs, finished)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tf), value='self')",
            "Update(target_node=ASTNode(type=identifier, text=concat), value='transform_inputs')",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=next_inputs), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=attention_context), value='outputs')",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=attention_context), position=4)",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=list))",
            "Delete(target_node=ASTNode(type=integer, text=1))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 4672,
        "neg_line": [
            "-next_inputs = tf.concat([next_inputs, attention_context], 1)"
        ],
        "pos_line": [
            "+next_inputs = self.transform_inputs(next_inputs, outputs)"
        ],
        "core_change": "-next_inputs = tf.concat([next_inputs, attention_context], 1) +next_inputs = self.transform_inputs(next_inputs, outputs)",
        "core_API": "concat"
    },
    {
        "commit_hash": "a16fe2b4f917b6b7a6f81c63832429740ff02311",
        "index": "5a68fb1b..6bd0015c 100644",
        "commit_message": "[Enhance] pytorch nightly fixes (#861)\n\n* Fixed Uniform bug for validation and added extra dimensions\n\n* Fixed geometry tests\n\n* Fixed beta distribution bug\n\n* Fixed linting\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestTransformBoxes:",
            "",
            "boxes = torch.tensor([[139.2640, 103.0150, 397.3120, 410.5225]], device=device, dtype=dtype)",
            "",
            "-        expected = torch.tensor([372.7360, 103.0150, 114.6880, 410.5225], device=device, dtype=dtype)",
            "+        expected = torch.tensor([[372.7360, 103.0150, 114.6880, 410.5225]], device=device, dtype=dtype)",
            "",
            "trans_mat = torch.tensor([[[-1., 0., 512.],",
            "[0., 1., 0.],"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=list), node=('[', '['), position=0, insert_id=430409)",
            "Move(target_node=ASTNode(type=list), node=ASTNode(type=list), position=1)",
            "Insert(target_node=ASTNode(type=list), node=(']', ']'), position=2, insert_id=430410)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 3,
        "number": 4673,
        "neg_line": [
            "-expected = torch.tensor([372.7360, 103.0150, 114.6880, 410.5225], device=device, dtype=dtype)"
        ],
        "pos_line": [
            "+expected = torch.tensor([[372.7360, 103.0150, 114.6880, 410.5225]], device=device, dtype=dtype)"
        ],
        "core_change": "-expected = torch.tensor([372.7360, 103.0150, 114.6880, 410.5225], device=device, dtype=dtype) +expected = torch.tensor([[372.7360, 103.0150, 114.6880, 410.5225]], device=device, dtype=dtype)",
        "core_API": "tensor"
    },
    {
        "commit_hash": "52ddd740f0696d950fdbe9da83c31f681007c597",
        "index": "270396da..0bb052c9 100644",
        "commit_message": "fix tests and warnings (#1834)\n\n* fix tests and warnings\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix tests and warnings\n\n* fix tests and warnings\n\n* replace `torch.testinig.assert_close` with `assert_close`\n\nCo-authored-by: Anton Shevtsov <aeshevtsov@avito.ru>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestConvDistanceTransform:",
            "output1 = kornia.contrib.distance_transform(sample1, kernel_size, h)",
            "assert_close(expected_output1, output1)",
            "",
            "-    def test_gradcheck(self, device, dtype):",
            "+    def test_gradcheck(self, device):",
            "B, C, H, W = 1, 1, 32, 32",
            "-        sample1 = torch.ones(B, C, H, W, device=device, dtype=dtype, requires_grad=True)",
            "+        sample1 = torch.ones(B, C, H, W, device=device, dtype=torch.float64, requires_grad=True)",
            "assert gradcheck(kornia.contrib.distance_transform, (sample1), raise_exception=True)",
            "",
            "def test_loss_grad(self, device, dtype):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=keyword_argument), node=('attribute', None), position=2, insert_id=401331)",
            "Update(target_node=ASTNode(type=identifier, text=dtype), value='torch')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=dtype), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=401332)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'float64'), position=2, insert_id=401333)",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=dtype))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 7,
        "number": 4674,
        "neg_line": [
            "-def test_gradcheck(self, device, dtype):",
            "-sample1 = torch.ones(B, C, H, W, device=device, dtype=dtype, requires_grad=True)"
        ],
        "pos_line": [
            "+def test_gradcheck(self, device):",
            "+sample1 = torch.ones(B, C, H, W, device=device, dtype=torch.float64, requires_grad=True)"
        ],
        "core_change": "-def test_gradcheck(self, device, dtype): +def test_gradcheck(self, device): -sample1 = torch.ones(B, C, H, W, device=device, dtype=dtype, requires_grad=True) +sample1 = torch.ones(B, C, H, W, device=device, dtype=torch.float64, requires_grad=True)",
        "core_API": "distance_transform"
    },
    {
        "commit_hash": "d89453018099062ad0a15d6df2db9a3c65fd3a1a",
        "index": "d185ad3..d9b51e2 100644",
        "commit_message": "Fix(core): fix memory leak issue and switch to subprocess backend (#216)\n\n* fix RAM leak error\n\n* fix another memory leak\n\n* update boxes.py\n\n* make flake8 happy\n\n",
        "file": "YOLOX.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def all_reduce(py_dict, op=\"sum\", group=None):",
            "flatten_tensor /= world_size",
            "",
            "split_tensors = [",
            "-        x.reshape(shape) for x, shape in zip(",
            "-            torch.split(flatten_tensor, tensor_numels), tensor_shapes",
            "-        )",
            "+        x.reshape(shape)",
            "+        for x, shape in zip(torch.split(flatten_tensor, tensor_numels), tensor_shapes)",
            "]",
            "return OrderedDict({k: v for k, v in zip(py_key, split_tensors)})"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 3,
        "AST_diff_line": 17,
        "number": 4675,
        "neg_line": [
            "-x.reshape(shape) for x, shape in zip(",
            "-torch.split(flatten_tensor, tensor_numels), tensor_shapes",
            "-)"
        ],
        "pos_line": [
            "+x.reshape(shape)",
            "+for x, shape in zip(torch.split(flatten_tensor, tensor_numels), tensor_shapes)"
        ],
        "core_change": "-x.reshape(shape) for x, shape in zip( -torch.split(flatten_tensor, tensor_numels), tensor_shapes -) +x.reshape(shape) +for x, shape in zip(torch.split(flatten_tensor, tensor_numels), tensor_shapes)",
        "core_API": "reshape"
    },
    {
        "commit_hash": "fec3112cbacf19cd211109b1bc0b33a502595aca",
        "index": "4714d309..8bd14af6 100644",
        "commit_message": "Update to PyTorch 1.9.0 (#2887)\n\n* Update to PyTorch 1.9.0\n\n* Update torch.cholesky, torch.symeig\n\n* Fix torch.tensordot, torch.qr, funsor dependency\n\n* Fix torch import, AutoGuideList usage\n\n* Ignore bug in PyTorch jit\n\n* Ignore tracer warnings\n\n* Replace torch.solve with torch.linalg.solve\n\n* Xfail vectorized markov funsor tests\n\n* Update funsor version\n\n* Switch MNIST mirrors\n\n* Pin pillow version\n\n* Bump torchvision version\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def sqrt(x):",
            "# is upper triangular) using some `flip` operators:",
            "#   flip(cholesky(flip(schur_complement)))",
            "try:",
            "-            top_left = torch.flip(torch.cholesky(torch.flip(schur_complement, (-2, -1))), (-2, -1))",
            "+            top_left = torch.flip(",
            "+                torch.linalg.cholesky(torch.flip(schur_complement, (-2, -1))), (-2, -1)",
            "+            )",
            "break",
            "except RuntimeError:",
            "B = B / 2"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=677023)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=677024)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=torch), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'linalg'), position=2, insert_id=677025)"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 4676,
        "neg_line": [
            "-top_left = torch.flip(torch.cholesky(torch.flip(schur_complement, (-2, -1))), (-2, -1))"
        ],
        "pos_line": [
            "+top_left = torch.flip(",
            "+torch.linalg.cholesky(torch.flip(schur_complement, (-2, -1))), (-2, -1)",
            "+)"
        ],
        "core_change": "-top_left = torch.flip(torch.cholesky(torch.flip(schur_complement, (-2, -1))), (-2, -1)) +top_left = torch.flip( +torch.linalg.cholesky(torch.flip(schur_complement, (-2, -1))), (-2, -1) +)",
        "core_API": "flip"
    },
    {
        "commit_hash": "cf113d705b9054d329c67cf9bb29cbc3f191015d",
        "index": "03dd1f9d..ac38a179 100644",
        "commit_message": "Changes and improvements to how we initialize transformer modules from pretrained models (#5200)\n\n* updates\n\n* rename 'load_state_dict' -> 'read_state_dict'\n\n* fix TransformerStack\n\n* more fixes\n\n* fix embeddings\n\n* fix toolkit tests\n\n* fix self attention\n\n* fix bimodal encoder tests\n\n* fix more tests\n\n* fix T5!\n\n* fixes\n\n* fix backbone\n\n* fix\n\n* fixes\n\n* fix\n\n* doc fixes\n\n* name changes\n\n* patch models branch temporarily\n\n* update CHANGELOG\n\n* change default dist loading strategy to 'MEM_EFFICIENT' for T5\n\n* fix distilbert test\n\n* always use memory efficient distributed loading strategy\n\n* Update .github/workflows/ci.yml\n\nCo-authored-by: Pete <petew@allenai.org>\n\nCo-authored-by: Akshita Bhagia <akshita23bhagia@gmail.com>\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "import torch",
            "from allennlp.common import FromParams",
            "",
            "from allennlp.modules.transformer.transformer_module import TransformerModule",
            "+from allennlp.modules.transformer.layer_norm import LayerNorm",
            "",
            "",
            "class OutputLayer(TransformerModule, FromParams):",
            "",
            "-    _huggingface_mapping = {\"LayerNorm\": \"layer_norm\"}",
            "+    _pretrained_mapping = {\"LayerNorm\": \"layer_norm\"}",
            "",
            "def __init__(self, input_size: int, hidden_size: int, dropout: float):",
            "super().__init__()",
            "self.dense = torch.nn.Linear(input_size, hidden_size)",
            "-        self.layer_norm = torch.nn.LayerNorm(hidden_size, eps=1e-12)",
            "+        self.layer_norm = LayerNorm(hidden_size, eps=1e-12)",
            "self.dropout = torch.nn.Dropout(dropout)",
            "",
            "def forward(self, hidden_states, input_tensor):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('import_from_statement', None), position=3, insert_id=2733)",
            "Insert(target_node=IN(type=import_from_statement), node=('from', 'from'), position=0, insert_id=2734)",
            "Insert(target_node=IN(type=import_from_statement), node=('dotted_name', None), position=1, insert_id=2735)",
            "Insert(target_node=IN(type=import_from_statement), node=('import', 'import'), position=2, insert_id=2736)",
            "Insert(target_node=IN(type=import_from_statement), node=('dotted_name', None), position=3, insert_id=2737)",
            "Insert(target_node=IN(type=dotted_name), node=('identifier', 'allennlp'), position=0, insert_id=2738)",
            "Insert(target_node=IN(type=dotted_name), node=('.', '.'), position=1, insert_id=2739)",
            "Insert(target_node=IN(type=dotted_name), node=('identifier', 'modules'), position=2, insert_id=2740)",
            "Insert(target_node=IN(type=dotted_name), node=('.', '.'), position=3, insert_id=2741)",
            "Insert(target_node=IN(type=dotted_name), node=('identifier', 'transformer'), position=4, insert_id=2742)",
            "Insert(target_node=IN(type=dotted_name), node=('.', '.'), position=5, insert_id=2743)",
            "Insert(target_node=IN(type=dotted_name), node=('identifier', 'layer_norm'), position=6, insert_id=2744)",
            "Insert(target_node=IN(type=dotted_name), node=('identifier', 'LayerNorm'), position=0, insert_id=2745)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=LayerNorm), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=_huggingface_mapping), value='_pretrained_mapping')",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=nn))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 21,
        "number": 4677,
        "neg_line": [
            "-_huggingface_mapping = {\"LayerNorm\": \"layer_norm\"}",
            "-self.layer_norm = torch.nn.LayerNorm(hidden_size, eps=1e-12)"
        ],
        "pos_line": [
            "+from allennlp.modules.transformer.layer_norm import LayerNorm",
            "+_pretrained_mapping = {\"LayerNorm\": \"layer_norm\"}",
            "+self.layer_norm = LayerNorm(hidden_size, eps=1e-12)"
        ],
        "core_change": "+from allennlp.modules.transformer.layer_norm import LayerNorm -_huggingface_mapping = {\"LayerNorm\": \"layer_norm\"} +_pretrained_mapping = {\"LayerNorm\": \"layer_norm\"} -self.layer_norm = torch.nn.LayerNorm(hidden_size, eps=1e-12) +self.layer_norm = LayerNorm(hidden_size, eps=1e-12)",
        "core_API": "Linear"
    },
    {
        "commit_hash": "b5e2b183af5e40e33a4dc7659e697d137259d56e",
        "index": "77e34b792..5ab432015 100755",
        "commit_message": "Doc styler examples (#14953)\n\n* Fix bad examples\n\n* Add black formatting to style_doc\n\n* Use first nonempty line\n\n* Put it at the right place\n\n* Don't add spaces to empty lines\n\n* Better templates\n\n* Deal with triple quotes in docstrings\n\n* Result of style_doc\n\n* Enable mdx treatment and fix code examples in MDXs\n\n* Result of doc styler on doc source files\n\n* Last fixes\n\n* Break copy from\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class XLMForQuestionAnswering(XLMPreTrainedModel):",
            ">>> from transformers import XLMTokenizer, XLMForQuestionAnswering",
            ">>> import torch",
            "",
            "-        >>> tokenizer = XLMTokenizer.from_pretrained('xlm-mlm-en-2048')",
            "-        >>> model = XLMForQuestionAnswering.from_pretrained('xlm-mlm-en-2048')",
            "+        >>> tokenizer = XLMTokenizer.from_pretrained(\"xlm-mlm-en-2048\")",
            "+        >>> model = XLMForQuestionAnswering.from_pretrained(\"xlm-mlm-en-2048\")",
            "",
            "-        >>> input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1",
            "+        >>> input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(",
            "+        ...     0",
            "+        >>> )  # Batch size 1",
            ">>> start_positions = torch.tensor([1])",
            ">>> end_positions = torch.tensor([3])"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=module), position=12)",
            "Insert(target_node=ASTNode(type=module), node=('ERROR', None), position=1, insert_id=1208570)",
            "Insert(target_node=IN(type=ERROR), node=('>>', '>>'), position=0, insert_id=1208571)",
            "Insert(target_node=IN(type=ERROR), node=('>', '>'), position=1, insert_id=1208572)",
            "Update(target_node=ASTNode(type=string, text='xlm-mlm-en-2048'), value='\"xlm-mlm-en-2048\"')",
            "Update(target_node=ASTNode(type=string, text='xlm-mlm-en-2048'), value='\"xlm-mlm-en-2048\"')",
            "Insert(target_node=ASTNode(type=argument_list), node=('ellipsis', '...'), position=1, insert_id=1208573)",
            "Insert(target_node=ASTNode(type=argument_list), node=('ERROR', None), position=2, insert_id=1208574)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=integer, text=0), position=0)",
            "Insert(target_node=IN(type=ERROR), node=('>>', '>>'), position=1, insert_id=1208575)",
            "Insert(target_node=IN(type=ERROR), node=('>', '>'), position=2, insert_id=1208576)",
            "Delete(target_node=ASTNode(type=>>, text=>>))",
            "Delete(target_node=ASTNode(type=>, text=>))",
            "Delete(target_node=ASTNode(type=ERROR))"
        ],
        "plus_line": 5,
        "minus_line": 3,
        "AST_diff_line": 14,
        "number": 4679,
        "neg_line": [
            "->>> tokenizer = XLMTokenizer.from_pretrained('xlm-mlm-en-2048')",
            "->>> model = XLMForQuestionAnswering.from_pretrained('xlm-mlm-en-2048')",
            "->>> input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1"
        ],
        "pos_line": [
            "+>>> tokenizer = XLMTokenizer.from_pretrained(\"xlm-mlm-en-2048\")",
            "+>>> model = XLMForQuestionAnswering.from_pretrained(\"xlm-mlm-en-2048\")",
            "+>>> input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(",
            "+...     0",
            "+>>> )  # Batch size 1"
        ],
        "core_change": "->>> tokenizer = XLMTokenizer.from_pretrained('xlm-mlm-en-2048') ->>> model = XLMForQuestionAnswering.from_pretrained('xlm-mlm-en-2048') +>>> tokenizer = XLMTokenizer.from_pretrained(\"xlm-mlm-en-2048\") +>>> model = XLMForQuestionAnswering.from_pretrained(\"xlm-mlm-en-2048\") ->>> input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1 +>>> input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze( +...     0 +>>> )  # Batch size 1",
        "core_API": "from_pretrained"
    },
    {
        "commit_hash": "188df74efe0213fcddccc49cfed8904d94d776eb",
        "index": "00000000..5c509f4c",
        "commit_message": "Add Homography Tracker API (#1389)\n\n* rebase commit\n\n* init\n\n* added test data for loftr and image registrator\n\n* real ransac tests\n\n* save\n\n* works\n\n* lint\n\n* pdated example\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* docs\n\n* docs\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* stupid CI OOM\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* lint\n\n* fix dtype in tests\n\n* fix test dtype\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* no-grad for homotest\n\n* fix\n\n* fix\n\n* fix random seed for ransac test\n\n* some formatting\n\n* codespell\n\n* fix signature defaults\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add fixture with data\n\n* remove files\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix deepsource\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* deepsource\n\n* fix super\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* identation fix\n\n* fix positional\n\n* fix mypy\n\n* refactor some tests\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* remove doctest\n\n* fix ransac tests\n\n* fix similarity tests\n\n* fix sha path\n\n* fix loftr precision\n\n* xfail some ransac tests\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "+import pytest",
            "+import torch",
            "+",
            "+",
            "+@pytest.fixture",
            "+def data_loftr():",
            "+    url = 'https://github.com/kornia/data_test/blob/main/loftr_outdoor_and_homography_data.pt?raw=true'",
            "+    return torch.hub.load_state_dict_from_url(url)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 0,
        "AST_diff_line": 17,
        "number": 4680,
        "neg_line": [],
        "pos_line": [
            "+import pytest",
            "+import torch",
            "+",
            "+",
            "+@pytest.fixture",
            "+def data_loftr():",
            "+url = 'https://github.com/kornia/data_test/blob/main/loftr_outdoor_and_homography_data.pt?raw=true'",
            "+return torch.hub.load_state_dict_from_url(url)"
        ],
        "core_change": "+import pytest +import torch + + +@pytest.fixture +def data_loftr(): +url = 'https://github.com/kornia/data_test/blob/main/loftr_outdoor_and_homography_data.pt?raw=true' +return torch.hub.load_state_dict_from_url(url)",
        "core_API": "load_state_dict_from_url"
    },
    {
        "commit_hash": "296fe9b4efe3674a6e1f5fe1b0fb629828c56a63",
        "index": "d64b3a3..6892450 100644",
        "commit_message": "Fixed some of the model definitions\n",
        "file": "facenet.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def main(args):",
            "",
            "# Start running operations on the Graph.",
            "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=args.gpu_memory_fraction)",
            "-        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))",
            "+        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))",
            "sess.run(tf.initialize_all_variables())",
            "sess.run(tf.initialize_local_variables())",
            "summary_writer = tf.train.SummaryWriter(log_dir, sess.graph)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1929626)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1929627)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'log_device_placement'), position=0, insert_id=1929628)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1929629)",
            "Insert(target_node=IN(type=keyword_argument), node=('false', 'False'), position=2, insert_id=1929630)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 4681,
        "neg_line": [
            "-sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))"
        ],
        "pos_line": [
            "+sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))"
        ],
        "core_change": "-sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) +sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))",
        "core_API": "GPUOptions"
    },
    {
        "commit_hash": "b8385d8a118f19ff31a49fd463561e8d2d7f098d",
        "index": "8dddcfa63..30c514f6c 100644",
        "commit_message": "TF Seq2Seq int dtype fix (#13496)\n\nFixes problems with passing int64 input to TF Seq2Seq models.\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFMarianMTModel(TFMarianPreTrainedModel, TFCausalLanguageModelingLoss):",
            "if inputs[\"labels\"] is not None:",
            "inputs[\"labels\"] = tf.where(",
            "inputs[\"labels\"] == self.config.pad_token_id,",
            "-                tf.fill(shape_list(inputs[\"labels\"]), -100),",
            "+                tf.fill(shape_list(inputs[\"labels\"]), tf.cast(-100, inputs[\"labels\"].dtype)),",
            "inputs[\"labels\"],",
            ")",
            "inputs[\"use_cache\"] = False"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=3, insert_id=2369284)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=4, insert_id=2369285)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2369286)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2369287)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2369288)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2369289)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cast'), position=2, insert_id=2369290)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2369291)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=unary_operator, text=-100), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=2369292)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=3, insert_id=2369293)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=), text=)), position=4)",
            "Insert(target_node=IN(type=attribute), node=('subscript', None), position=0, insert_id=2369294)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2369295)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dtype'), position=2, insert_id=2369296)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'inputs'), position=0, insert_id=2369297)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=2369298)",
            "Insert(target_node=IN(type=subscript), node=('string', '\"labels\"'), position=2, insert_id=2369299)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=2369300)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 19,
        "number": 4682,
        "neg_line": [
            "-tf.fill(shape_list(inputs[\"labels\"]), -100),"
        ],
        "pos_line": [
            "+tf.fill(shape_list(inputs[\"labels\"]), tf.cast(-100, inputs[\"labels\"].dtype)),"
        ],
        "core_change": "-tf.fill(shape_list(inputs[\"labels\"]), -100), +tf.fill(shape_list(inputs[\"labels\"]), tf.cast(-100, inputs[\"labels\"].dtype)),",
        "core_API": "where"
    },
    {
        "commit_hash": "d9eeaaf00a4bc2fd0cf597896f97ffa1567dbb8b",
        "index": "11163f7226..28575f5466 100755",
        "commit_message": "[tune] Fix bug in example where config hyperparameters were ignored (#2860)\n\nA fix to an example for tune (`python/ray/tune/examples/pbt_tune_cifar10_with_keras.py`) where the hyperparameters for the optimizer, learning rate and decay, were not being passed into the optimizer. \n\nThis means that the current optimizer uses default values for the hyperparameters no matter the config.\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Cifar10Model(Trainable):",
            "x_train = self.train_data[0]",
            "model = self._build_model(x_train.shape[1:])",
            "",
            "-        opt = tf.keras.optimizers.Adadelta()",
            "+        opt = tf.keras.optimizers.Adadelta(",
            "+            lr=self.config[\"lr\"], decay=self.config[\"decay\"])",
            "model.compile(",
            "loss=\"categorical_crossentropy\",",
            "optimizer=opt,"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=1, insert_id=2153488)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=2153489)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=2153490)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'lr'), position=0, insert_id=2153491)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2153492)",
            "Insert(target_node=IN(type=keyword_argument), node=('subscript', None), position=2, insert_id=2153493)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'decay'), position=0, insert_id=2153494)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2153495)",
            "Insert(target_node=IN(type=keyword_argument), node=('subscript', None), position=2, insert_id=2153496)",
            "Insert(target_node=IN(type=subscript), node=('attribute', None), position=0, insert_id=2153497)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=2153498)",
            "Insert(target_node=IN(type=subscript), node=('string', '\"lr\"'), position=2, insert_id=2153499)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=2153500)",
            "Insert(target_node=IN(type=subscript), node=('attribute', None), position=0, insert_id=2153501)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=2153502)",
            "Insert(target_node=IN(type=subscript), node=('string', '\"decay\"'), position=2, insert_id=2153503)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=2153504)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=2153505)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2153506)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'config'), position=2, insert_id=2153507)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=2153508)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2153509)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'config'), position=2, insert_id=2153510)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 23,
        "number": 4685,
        "neg_line": [
            "-opt = tf.keras.optimizers.Adadelta()"
        ],
        "pos_line": [
            "+opt = tf.keras.optimizers.Adadelta(",
            "+lr=self.config[\"lr\"], decay=self.config[\"decay\"])"
        ],
        "core_change": "-opt = tf.keras.optimizers.Adadelta() +opt = tf.keras.optimizers.Adadelta( +lr=self.config[\"lr\"], decay=self.config[\"decay\"])",
        "core_API": "_build_model"
    },
    {
        "commit_hash": "f4781a0b27ffb3ea61ecd25b0b87305e0960304e",
        "index": "69468efb..7cce0ed1 100644",
        "commit_message": "update expected results of slow tests (#268)\n\n* update expected results of slow tests\n\n* relax sum and mean tests\n\n* Print shapes when reporting exception\n\n* formatting\n\n* fix sentence\n\n* relax test_stable_diffusion_fast_ddim for gpu fp16\n\n* relax flakey tests on GPU\n\n* added comment on large tolerences\n\n* black\n\n* format\n\n* set scheduler seed\n\n* added generator\n\n* use np.isclose\n\n* set num_inference_steps to 50\n\n* fix dep. warning\n\n* update expected_slice\n\n* preprocess if image\n\n* updated expected results\n\n* updated expected from CI\n\n* pass generator to VAE\n\n* undo change back to orig\n\n* use orignal\n\n* revert back the expected on cpu\n\n* revert back values for CPU\n\n* more undo\n\n* update result after using gen\n\n* update mean\n\n* set generator for mps\n\n* update expected on CI server\n\n* undo\n\n* use new seed every time\n\n* cpu manual seed\n\n* reduce num_inference_steps\n\n* style\n\n* use generator for randn\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class VQModelTests(ModelTesterMixin, unittest.TestCase):",
            "# fmt: off",
            "expected_output_slice = torch.tensor([-0.0153, -0.4044, -0.1880, -0.5161, -0.2418, -0.4072, -0.1612, -0.0633, -0.0143])",
            "# fmt: on",
            "-        self.assertTrue(torch.allclose(output_slice, expected_output_slice, rtol=1e-2))",
            "+        self.assertTrue(torch.allclose(output_slice, expected_output_slice, atol=1e-3))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=rtol), value='atol')",
            "Update(target_node=ASTNode(type=float, text=1e-2), value='1e-3')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 4693,
        "neg_line": [
            "-self.assertTrue(torch.allclose(output_slice, expected_output_slice, rtol=1e-2))"
        ],
        "pos_line": [
            "+self.assertTrue(torch.allclose(output_slice, expected_output_slice, atol=1e-3))"
        ],
        "core_change": "-self.assertTrue(torch.allclose(output_slice, expected_output_slice, rtol=1e-2)) +self.assertTrue(torch.allclose(output_slice, expected_output_slice, atol=1e-3))",
        "core_API": "tensor"
    },
    {
        "commit_hash": "3342fb622e73f1287894c353b16241fd16a6ab1d",
        "index": "909d19c9..d9c094e8 100644",
        "commit_message": "fix cluster gcn test\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_cluster_gcn_conv():",
            "",
            "t = '(Tensor, SparseTensor, Size) -> Tensor'",
            "jit = torch.jit.script(conv.jittable(t))",
            "-    assert torch.allclose(jit(x, adj.t()), out)",
            "+    assert torch.allclose(jit(x, adj.t()), out, atol=1e-5)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=1008538)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=1008539)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'atol'), position=0, insert_id=1008540)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1008541)",
            "Insert(target_node=IN(type=keyword_argument), node=('float', '1e-5'), position=2, insert_id=1008542)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 4694,
        "neg_line": [
            "-assert torch.allclose(jit(x, adj.t()), out)"
        ],
        "pos_line": [
            "+assert torch.allclose(jit(x, adj.t()), out, atol=1e-5)"
        ],
        "core_change": "-assert torch.allclose(jit(x, adj.t()), out) +assert torch.allclose(jit(x, adj.t()), out, atol=1e-5)",
        "core_API": "script"
    },
    {
        "commit_hash": "e94abf66783f5e24ab37adc754ac902e827a4b4d",
        "index": "412e677f..3f3b3b2f 100644",
        "commit_message": "fix TF deprecation of concat/split/pack/unpack\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def batch_flatten(x):",
            "shape = x.get_shape().as_list()[1:]",
            "if None not in shape:",
            "return tf.reshape(x, [-1, int(np.prod(shape))])",
            "-    return tf.reshape(x, tf.pack([tf.shape(x)[0], -1]))",
            "+    return tf.reshape(x, tf.stack([tf.shape(x)[0], -1]))",
            "",
            "",
            "def class_balanced_cross_entropy(pred, label, name='cross_entropy_loss'):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=pack), value='stack')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 4703,
        "neg_line": [
            "-return tf.reshape(x, tf.pack([tf.shape(x)[0], -1]))"
        ],
        "pos_line": [
            "+return tf.reshape(x, tf.stack([tf.shape(x)[0], -1]))"
        ],
        "core_change": "-return tf.reshape(x, tf.pack([tf.shape(x)[0], -1])) +return tf.reshape(x, tf.stack([tf.shape(x)[0], -1]))",
        "core_API": "get_shape"
    },
    {
        "commit_hash": "c0f80751543d42d0ed343516fa766d25854bdbc7",
        "index": "b48c6cd3a..3d66f1adb 100644",
        "commit_message": "Revert \"Fix Conv1D block for FastSpeech\"\n\nThis reverts commit 5f5fa7d8d0c3e74bf9c5460eb6ca3754dec708eb.\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class MultiLayeredConv1d(torch.nn.Module):",
            "",
            "\"\"\"",
            "x = torch.relu(self.w_1(x.transpose(-1, 1))).transpose(-1, 1)",
            "-        x = torch.relu(self.w_2(x.transpose(-1, 1))).transpose(-1, 1)",
            "-        return self.dropout(x)",
            "+        return self.w_2(self.dropout(x).transpose(-1, 1)).transpose(-1, 1)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('return_statement', None), position=3, insert_id=166397)",
            "Insert(target_node=IN(type=return_statement), node=('return', 'return'), position=0, insert_id=166398)",
            "Move(target_node=IN(type=return_statement), node=ASTNode(type=call), position=1)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=166399)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=argument_list), position=1)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=transpose), position=2)",
            "Delete(target_node=ASTNode(type=identifier, text=x))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=relu))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=x))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=return, text=return))",
            "Delete(target_node=ASTNode(type=return_statement))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 25,
        "number": 4706,
        "neg_line": [
            "-x = torch.relu(self.w_2(x.transpose(-1, 1))).transpose(-1, 1)",
            "-return self.dropout(x)"
        ],
        "pos_line": [
            "+return self.w_2(self.dropout(x).transpose(-1, 1)).transpose(-1, 1)"
        ],
        "core_change": "-x = torch.relu(self.w_2(x.transpose(-1, 1))).transpose(-1, 1) -return self.dropout(x) +return self.w_2(self.dropout(x).transpose(-1, 1)).transpose(-1, 1)",
        "core_API": "relu"
    },
    {
        "commit_hash": "51139ed37c5a64a87c916e6a6675385f9b700535",
        "index": "50f20e39f..79217bcc0 100644",
        "commit_message": "[SGD] Fix process group timeout units (#12477)\n\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TorchTrainer:",
            "if backend == \"auto\":",
            "backend = \"nccl\" if use_gpu else \"gloo\"",
            "",
            "+        if backend == \"nccl\":",
            "+            timeout_s = NCCL_TIMEOUT_S",
            "+",
            "logger.debug(f\"Using {backend} as backend.\")",
            "self.backend = backend",
            "self.num_cpus_per_worker = num_cpus_per_worker"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=3, insert_id=1119306)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=1119307)",
            "Insert(target_node=IN(type=if_statement), node=('comparison_operator', None), position=1, insert_id=1119308)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=1119309)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=1119310)",
            "Insert(target_node=IN(type=comparison_operator), node=('identifier', 'backend'), position=0, insert_id=1119311)",
            "Insert(target_node=IN(type=comparison_operator), node=('==', '=='), position=1, insert_id=1119312)",
            "Insert(target_node=IN(type=comparison_operator), node=('string', '\"nccl\"'), position=2, insert_id=1119313)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=1119314)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1119315)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'timeout_s'), position=0, insert_id=1119316)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1119317)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'NCCL_TIMEOUT_S'), position=2, insert_id=1119318)"
        ],
        "plus_line": 2,
        "minus_line": 0,
        "AST_diff_line": 13,
        "number": 4707,
        "neg_line": [],
        "pos_line": [
            "+if backend == \"nccl\":",
            "+timeout_s = NCCL_TIMEOUT_S",
            "+"
        ],
        "core_change": "+if backend == \"nccl\": +timeout_s = NCCL_TIMEOUT_S +",
        "core_API": "debug"
    },
    {
        "commit_hash": "129350f37c96be5717d9c195b701da57692fd24e",
        "index": "cf43c1d3..5cee3393 100755",
        "commit_message": "fixed problem with multi-state/action networks and scoping\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Bernoulli(Distribution):",
            "self.shape = shape",
            "action_size = util.prod(self.shape)",
            "",
            "-        with tf.name_scope(name=scope):",
            "-            self.logit = Linear(size=action_size, bias=log(probability), scope='logit')",
            "+        self.logit = Linear(size=action_size, bias=log(probability), scope='logit')",
            "",
            "super(Bernoulli, self).__init__(scope, summary_labels)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=module), node=ASTNode(type=expression_statement), position=3)",
            "Delete(target_node=ASTNode(type=with, text=with))",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=name_scope))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=name))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=scope))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=with_item))",
            "Delete(target_node=ASTNode(type=with_clause))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=block))",
            "Delete(target_node=ASTNode(type=with_statement))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 19,
        "number": 4708,
        "neg_line": [
            "-with tf.name_scope(name=scope):",
            "-self.logit = Linear(size=action_size, bias=log(probability), scope='logit')"
        ],
        "pos_line": [
            "+self.logit = Linear(size=action_size, bias=log(probability), scope='logit')"
        ],
        "core_change": "-with tf.name_scope(name=scope): -self.logit = Linear(size=action_size, bias=log(probability), scope='logit') +self.logit = Linear(size=action_size, bias=log(probability), scope='logit')",
        "core_API": "prod"
    },
    {
        "commit_hash": "6ba2231d7227825dfb76e9df161824d04234e69f",
        "index": "44514da0..9c4c2f2d 100644",
        "commit_message": "Reproducibility 3/3 (#1924)\n\n* make tests deterministic\n\n* run slow tests\n\n* prepare for testing\n\n* finish\n\n* refactor\n\n* add print statements\n\n* finish more\n\n* correct some test failures\n\n* more fixes\n\n* set up to correct tests\n\n* more corrections\n\n* up\n\n* fix more\n\n* more prints\n\n* add\n\n* up\n\n* up\n\n* up\n\n* uP\n\n* uP\n\n* more fixes\n\n* uP\n\n* up\n\n* up\n\n* up\n\n* up\n\n* fix more\n\n* up\n\n* up\n\n* clean tests\n\n* up\n\n* up\n\n* up\n\n* more fixes\n\n* Apply suggestions from code review\n\nCo-authored-by: Suraj Patil <surajp815@gmail.com>\n\n* make\n\n* correct\n\n* finish\n\n* finish\n\nCo-authored-by: Suraj Patil <surajp815@gmail.com>\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class VersatileDiffusionTextToImagePipelineIntegrationTests(unittest.TestCase):",
            "image_slice = image[0, 253:256, 253:256, -1]",
            "",
            "assert image.shape == (1, 512, 512, 3)",
            "-        expected_slice = np.array([0.0408, 0.0181, 0.0, 0.0388, 0.0046, 0.0461, 0.0411, 0.0, 0.0222])",
            "+        expected_slice = np.array([0.3493, 0.3757, 0.4093, 0.4495, 0.4233, 0.4102, 0.4507, 0.4756, 0.4787])",
            "+",
            "assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-2"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=ASTNode(type=float, text=0.0), node=ASTNode(type=list), position=16)",
            "Update(target_node=ASTNode(type=float, text=0.0408), value='0.3493')",
            "Update(target_node=ASTNode(type=float, text=0.0181), value='0.3757')",
            "Update(target_node=ASTNode(type=float, text=0.0), value='0.4093')",
            "Update(target_node=ASTNode(type=float, text=0.0388), value='0.4495')",
            "Update(target_node=ASTNode(type=float, text=0.0046), value='0.4233')",
            "Update(target_node=ASTNode(type=float, text=0.0461), value='0.4102')",
            "Update(target_node=ASTNode(type=float, text=0.0411), value='0.4507')",
            "Update(target_node=ASTNode(type=float, text=0.0), value='0.4756')",
            "Insert(target_node=ASTNode(type=list), node=(',', ','), position=16, insert_id=90061)",
            "Update(target_node=ASTNode(type=float, text=0.0222), value='0.4787')",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 12,
        "number": 4710,
        "neg_line": [
            "-expected_slice = np.array([0.0408, 0.0181, 0.0, 0.0388, 0.0046, 0.0461, 0.0411, 0.0, 0.0222])"
        ],
        "pos_line": [
            "+expected_slice = np.array([0.3493, 0.3757, 0.4093, 0.4495, 0.4233, 0.4102, 0.4507, 0.4756, 0.4787])",
            "+"
        ],
        "core_change": "-expected_slice = np.array([0.0408, 0.0181, 0.0, 0.0388, 0.0046, 0.0461, 0.0411, 0.0, 0.0222]) +expected_slice = np.array([0.3493, 0.3757, 0.4093, 0.4495, 0.4233, 0.4102, 0.4507, 0.4756, 0.4787]) +",
        "core_API": "array"
    },
    {
        "commit_hash": "60a822d7ac527632ee9cbb7c1f1dd427ac8bc84f",
        "index": "66680c19..d7d9e2a9 100644",
        "commit_message": "Removing old BERT files (#3746)\n\n* Removed old bert files\n\n* removing pretrained model caching\n\n* fix bug\n\n* black, flake\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestBasicTextFieldEmbedder(AllenNlpTestCase):",
            "token_embedder = BasicTextFieldEmbedder.from_params(vocab=self.vocab, params=params)",
            "inputs = {",
            "\"bert\": {",
            "-                \"input_ids\": (torch.rand(3, 5) * 10).long(),",
            "-                \"offsets\": (torch.rand(3, 5) * 1).long(),",
            "+                \"token_ids\": (torch.rand(3, 5) * 10).long(),",
            "+                \"mask\": (torch.rand(3, 5) * 1).long(),",
            "},",
            "\"token_characters\": {\"token_characters\": (torch.rand(3, 5, 5) * 1).long()},",
            "}"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"input_ids\"), value='\"token_ids\"')",
            "Update(target_node=ASTNode(type=string, text=\"offsets\"), value='\"mask\"')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 4711,
        "neg_line": [
            "-\"input_ids\": (torch.rand(3, 5) * 10).long(),",
            "-\"offsets\": (torch.rand(3, 5) * 1).long(),"
        ],
        "pos_line": [
            "+\"token_ids\": (torch.rand(3, 5) * 10).long(),",
            "+\"mask\": (torch.rand(3, 5) * 1).long(),"
        ],
        "core_change": "-\"input_ids\": (torch.rand(3, 5) * 10).long(), -\"offsets\": (torch.rand(3, 5) * 1).long(), +\"token_ids\": (torch.rand(3, 5) * 10).long(), +\"mask\": (torch.rand(3, 5) * 1).long(),",
        "core_API": "from_params"
    },
    {
        "commit_hash": "5a6d367d5a4c9879f44fcba7b8f1afd55c2852ad",
        "index": "8b34e9439b..26eded5d28 100644",
        "commit_message": "Fixed matrix_rank implementation of backends to adhere to the standard (#4827)\n\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def matrix_rank(",
            ") -> torch.Tensor:",
            "# ToDo: add support for default rtol value here, for the case where None is provided",
            "ret = torch.linalg.matrix_rank(x, rtol=rtol, out=out)",
            "-    return torch.tensor(ret, dtype=ivy.default_int_dtype(as_native=True))",
            "+    return ret.to(dtype=x.dtype)",
            "",
            "",
            "matrix_rank.unsupported_dtypes = ("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=ret), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=tensor), value='to')",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=), text=)), position=3)",
            "Insert(target_node=ASTNode(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=322978)",
            "Move(target_node=ASTNode(type=keyword_argument), node=ASTNode(type=attribute), position=3)",
            "Update(target_node=ASTNode(type=identifier, text=ivy), value='x')",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=dtype), position=2)",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=default_int_dtype))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=as_native))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=true, text=True))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=), text=)))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 18,
        "number": 4714,
        "neg_line": [
            "-return torch.tensor(ret, dtype=ivy.default_int_dtype(as_native=True))"
        ],
        "pos_line": [
            "+return ret.to(dtype=x.dtype)"
        ],
        "core_change": "-return torch.tensor(ret, dtype=ivy.default_int_dtype(as_native=True)) +return ret.to(dtype=x.dtype)",
        "core_API": "matrix_rank"
    },
    {
        "commit_hash": "834abcff51bc6050342cd35d445766643fd7e1d6",
        "index": "3f66d264..b318b234 100644",
        "commit_message": "fix cuda tests failing (#1941)\n\n* fix cuda tests failing\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def adalam_core(",
            "final_matches, idxs, counts = torch.unique(final_matches, dim=0, return_inverse=True, return_counts=True)",
            "_, ind_sorted = torch.sort(idxs)",
            "cum_sum = counts.cumsum(0)",
            "-        cum_sum = torch.cat((torch.tensor([0]), cum_sum[:-1]))",
            "+        cum_sum = torch.cat((torch.tensor([0], dtype=cum_sum.dtype, device=cum_sum.device), cum_sum[:-1]))",
            "first_indicies = ind_sorted[cum_sum]",
            "accepted_dist = accepted_dist[first_indicies]",
            "if return_dist:"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=396383)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=396384)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=396385)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=396386)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=396387)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=396388)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=396389)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=396390)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=396391)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=396392)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cum_sum'), position=0, insert_id=396393)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=396394)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dtype'), position=2, insert_id=396395)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cum_sum'), position=0, insert_id=396396)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=396397)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=396398)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 16,
        "number": 4715,
        "neg_line": [
            "-cum_sum = torch.cat((torch.tensor([0]), cum_sum[:-1]))"
        ],
        "pos_line": [
            "+cum_sum = torch.cat((torch.tensor([0], dtype=cum_sum.dtype, device=cum_sum.device), cum_sum[:-1]))"
        ],
        "core_change": "-cum_sum = torch.cat((torch.tensor([0]), cum_sum[:-1])) +cum_sum = torch.cat((torch.tensor([0], dtype=cum_sum.dtype, device=cum_sum.device), cum_sum[:-1]))",
        "core_API": "unique"
    },
    {
        "commit_hash": "015a0bcb255c1ed808455e1a9dd35e8eafe2fc39",
        "index": "0920c816..7701ebf4 100644",
        "commit_message": "Keras training: Give up on inferring steps if the dataset is not a `tf.data.Dataset` such as a per-worker dataset (thus doesn't have a variant tensor).\n\nThis fixes the cases where the dataset is trained with a per-worker dataset.\n\nPiperOrigin-RevId: 463620470\n\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class DataHandler:",
            "if adapter_steps is not None:",
            "return adapter_steps",
            "",
            "+        # tf.distribute's `PerWorkerDataset` does not inherit from",
            "+        # `tf.data.Dataset` and in those cases we give up on inferring steps.",
            "+        if not isinstance(dataset, tf.data.Dataset):",
            "+            return None",
            "+",
            "size = tf.data.experimental.cardinality(dataset)",
            "if size == tf.data.experimental.INFINITE_CARDINALITY and steps is None:",
            "raise ValueError("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=3, insert_id=2052243)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=2052244)",
            "Insert(target_node=IN(type=if_statement), node=('not_operator', None), position=1, insert_id=2052245)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=2052246)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=2052247)",
            "Insert(target_node=IN(type=not_operator), node=('not', 'not'), position=0, insert_id=2052248)",
            "Insert(target_node=IN(type=not_operator), node=('call', None), position=1, insert_id=2052249)",
            "Insert(target_node=IN(type=block), node=('return_statement', None), position=0, insert_id=2052250)",
            "Insert(target_node=IN(type=call), node=('identifier', 'isinstance'), position=0, insert_id=2052251)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2052252)",
            "Insert(target_node=IN(type=return_statement), node=('return', 'return'), position=0, insert_id=2052253)",
            "Insert(target_node=IN(type=return_statement), node=('none', 'None'), position=1, insert_id=2052254)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2052255)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'dataset'), position=1, insert_id=2052256)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=2052257)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=3, insert_id=2052258)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=2052259)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2052260)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2052261)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Dataset'), position=2, insert_id=2052262)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2052263)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2052264)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'data'), position=2, insert_id=2052265)"
        ],
        "plus_line": 4,
        "minus_line": 0,
        "AST_diff_line": 23,
        "number": 4716,
        "neg_line": [],
        "pos_line": [
            "+# tf.distribute's `PerWorkerDataset` does not inherit from",
            "+# `tf.data.Dataset` and in those cases we give up on inferring steps.",
            "+if not isinstance(dataset, tf.data.Dataset):",
            "+return None",
            "+"
        ],
        "core_change": "+# tf.distribute's `PerWorkerDataset` does not inherit from +# `tf.data.Dataset` and in those cases we give up on inferring steps. +if not isinstance(dataset, tf.data.Dataset): +return None +",
        "core_API": "cardinality"
    },
    {
        "commit_hash": "9bd557916e3c8782d2d1ce5d89b16adafe241f6f",
        "index": "a418a79..85bf6f1 100644",
        "commit_message": "Use codecs utf-8 writer for decode_and_evaluate. Fix #38.\n\nPiperOrigin-RevId: 164206518\n\n",
        "file": "nmt.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def decode_and_evaluate(name,",
            "",
            "start_time = time.time()",
            "num_sentences = 0",
            "-    with tf.gfile.GFile(trans_file, mode=\"w\") as trans_f:",
            "+    with codecs.getwriter(\"utf-8\")(",
            "+        tf.gfile.GFile(trans_file, mode=\"w\")) as trans_f:",
            "trans_f.write(\"\")  # Write empty string to ensure file is created.",
            "",
            "while True:"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('call', None), position=0, insert_id=2128289)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=2128290)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2128291)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2128292)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2128293)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=call), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2128294)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'codecs'), position=0, insert_id=2128295)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2128296)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'getwriter'), position=2, insert_id=2128297)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2128298)",
            "Insert(target_node=IN(type=argument_list), node=('string', '\"utf-8\"'), position=1, insert_id=2128299)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2128300)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 13,
        "number": 4718,
        "neg_line": [
            "-with tf.gfile.GFile(trans_file, mode=\"w\") as trans_f:"
        ],
        "pos_line": [
            "+with codecs.getwriter(\"utf-8\")(",
            "+tf.gfile.GFile(trans_file, mode=\"w\")) as trans_f:"
        ],
        "core_change": "-with tf.gfile.GFile(trans_file, mode=\"w\") as trans_f: +with codecs.getwriter(\"utf-8\")( +tf.gfile.GFile(trans_file, mode=\"w\")) as trans_f:",
        "core_API": "time"
    },
    {
        "commit_hash": "d08bc09495a289519a774e881f399cf864178f9b",
        "index": "d32967f1..6fafd25a 100644",
        "commit_message": "Fix unused variable found (#1250)\n\n* Fix Unused variable found\n\n* more fixes\n\n* Update kornia/enhance/histogram.py\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestRandomCropSizeGen(RandomGeneratorBaseTests):",
            "",
            "def test_same_on_batch(self, device, dtype):",
            "torch.manual_seed(42)",
            "-        degrees = torch.tensor([10, 20])",
            "res = random_crop_size_generator(",
            "batch_size=8,",
            "size=(100, 100),"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=degrees))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=tensor))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=integer, text=10))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=integer, text=20))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=list))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 18,
        "number": 4722,
        "neg_line": [
            "-degrees = torch.tensor([10, 20])"
        ],
        "pos_line": [],
        "core_change": "-degrees = torch.tensor([10, 20])",
        "core_API": "manual_seed"
    },
    {
        "commit_hash": "0dfd6b624081fc4e1c72fc74ae0cd2de199c334c",
        "index": "f007776a..578f03d9 100644",
        "commit_message": "Add linting with black (#2678)\n\nSummary:\n# Before submitting\n\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\n- [ ] Did you make sure to update the docs?\n- [ ] Did you write any new necessary tests?\n\n## What does this PR do?\nFixes # (issue).\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding \n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2678\n\nReviewed By: Mortimerp9\n\nDifferential Revision: D32653381\n\nPulled By: dianaml0\n\nfbshipit-source-id: 2810d14867cd7d64f4d340740e2b590b82de47fe\n\n",
        "file": "fairseq.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TransformerEncoderBase(FairseqEncoder):",
            "# `forward` so we use a dictionary instead.",
            "# TorchScript does not support mixed values so the values are all lists.",
            "# The empty list is equivalent to None.",
            "-        src_lengths = src_tokens.ne(self.padding_idx).sum(dim=1, dtype=torch.int32).reshape(-1, 1).contiguous()",
            "+        src_lengths = (",
            "+            src_tokens.ne(self.padding_idx)",
            "+            .sum(dim=1, dtype=torch.int32)",
            "+            .reshape(-1, 1)",
            "+            .contiguous()",
            "+        )",
            "return {",
            "\"encoder_out\": [x],  # T x B x C",
            "\"encoder_padding_mask\": [encoder_padding_mask],  # B x T"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('parenthesized_expression', None), position=2, insert_id=205265)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('(', '('), position=0, insert_id=205266)",
            "Move(target_node=IN(type=parenthesized_expression), node=ASTNode(type=call), position=1)",
            "Insert(target_node=IN(type=parenthesized_expression), node=(')', ')'), position=2, insert_id=205267)"
        ],
        "plus_line": 6,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 4725,
        "neg_line": [
            "-src_lengths = src_tokens.ne(self.padding_idx).sum(dim=1, dtype=torch.int32).reshape(-1, 1).contiguous()"
        ],
        "pos_line": [
            "+src_lengths = (",
            "+src_tokens.ne(self.padding_idx)",
            "+.sum(dim=1, dtype=torch.int32)",
            "+.reshape(-1, 1)",
            "+.contiguous()",
            "+)"
        ],
        "core_change": "-src_lengths = src_tokens.ne(self.padding_idx).sum(dim=1, dtype=torch.int32).reshape(-1, 1).contiguous() +src_lengths = ( +src_tokens.ne(self.padding_idx) +.sum(dim=1, dtype=torch.int32) +.reshape(-1, 1) +.contiguous() +)",
        "core_API": "ne"
    },
    {
        "commit_hash": "e9dc54611923f6cff85ccde42bed7c1005fdfdd8",
        "index": "cf5d2d37..016c91fb 100644",
        "commit_message": "Fix TorchScript support in `DNAConv` and `FAConv` (#6754)\n\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_fa_conv():",
            "",
            "result = conv(x, x_0, adj2.t(), return_attention_weights=True)",
            "assert torch.allclose(result[0], out)",
            "-    assert result[1].size() == torch.Size([4, 4]) and result[1]._nnz() == 10",
            "+    assert result[1][0].size() == torch.Size([4, 4])",
            "+    assert result[1][0]._nnz() == 10",
            "assert conv._alpha is None",
            "",
            "if is_full_test():"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('assert_statement', None), position=3, insert_id=931566)",
            "Insert(target_node=ASTNode(type=module), node=('assert_statement', None), position=4, insert_id=931567)",
            "Move(target_node=IN(type=assert_statement), node=ASTNode(type=assert, text=assert), position=0)",
            "Move(target_node=IN(type=assert_statement), node=ASTNode(type=comparison_operator), position=1)",
            "Insert(target_node=IN(type=assert_statement), node=('assert', 'assert'), position=0, insert_id=931568)",
            "Move(target_node=IN(type=assert_statement), node=ASTNode(type=comparison_operator), position=1)",
            "Insert(target_node=ASTNode(type=attribute), node=('subscript', None), position=0, insert_id=931569)",
            "Insert(target_node=ASTNode(type=attribute), node=('subscript', None), position=0, insert_id=931570)",
            "Move(target_node=IN(type=subscript), node=ASTNode(type=subscript), position=0)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=931571)",
            "Insert(target_node=IN(type=subscript), node=('integer', '0'), position=2, insert_id=931572)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=931573)",
            "Move(target_node=IN(type=subscript), node=ASTNode(type=subscript), position=0)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=931574)",
            "Insert(target_node=IN(type=subscript), node=('integer', '0'), position=2, insert_id=931575)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=931576)",
            "Delete(target_node=ASTNode(type=and, text=and))",
            "Delete(target_node=ASTNode(type=boolean_operator))",
            "Delete(target_node=ASTNode(type=assert_statement))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 19,
        "number": 4727,
        "neg_line": [
            "-assert result[1].size() == torch.Size([4, 4]) and result[1]._nnz() == 10"
        ],
        "pos_line": [
            "+assert result[1][0].size() == torch.Size([4, 4])",
            "+assert result[1][0]._nnz() == 10"
        ],
        "core_change": "-assert result[1].size() == torch.Size([4, 4]) and result[1]._nnz() == 10 +assert result[1][0].size() == torch.Size([4, 4]) +assert result[1][0]._nnz() == 10",
        "core_API": "t"
    },
    {
        "commit_hash": "6d4f83cae02129b7f49acf022561711cd937e8b8",
        "index": "7ae43e0e..7c684bb3 100644",
        "commit_message": "Drop JIT support for `core.check`, `Boxes`, and others (#2219)\n\n* Drop JIT support for `core.check` API\n\n- Consequently for this, we drop support of JIT on the following items: (in of dynamo)\n  - enhance\n    - AdjustSigmoid\n    - AdjustLog\n    - AddWeighted\n  - geometry\n    - UndistortPoints\n    - bbox and Boxes - follow up on #2218\n    - EuclideanDistance\n    - TransformPoints\n    - HomographyWarper\n    - WarpPerspective\n    - UpscaleDouble\n  - losses\n\n* Update typing with pyupgrade\n* drop all jit related from bbox and boxes\n\nfrom #2218\n* fix/skip failing dynamo tests\n* fix loss hd\n* fix typing\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestEuclideanDistance(BaseTester):",
            "pt2 = torch.rand(2, 3, device=device, dtype=torch.float64, requires_grad=True)",
            "assert gradcheck(kgl.euclidean_distance, (pt1, pt2), raise_exception=True, fast_mode=True)",
            "",
            "-    def test_jit(self, device, dtype):",
            "+    def test_dynamo(self, device, dtype, torch_optimizer):",
            "pt1 = torch.rand(2, 3, device=device, dtype=dtype)",
            "pt2 = torch.rand(2, 3, device=device, dtype=dtype)",
            "op = kgl.euclidean_distance",
            "-        op_jit = torch.jit.script(op)",
            "-        self.assert_close(op(pt1, pt2), op_jit(pt1, pt2))",
            "+        op_optimized = torch_optimizer(op)",
            "+        self.assert_close(op(pt1, pt2), op_optimized(pt1, pt2))",
            "",
            "def test_module(self, device, dtype):",
            "pass"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=test_jit), value='test_dynamo')",
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=6, insert_id=388598)",
            "Insert(target_node=ASTNode(type=parameters), node=('identifier', 'torch_optimizer'), position=7, insert_id=388599)",
            "Update(target_node=ASTNode(type=identifier, text=op_jit), value='op_optimized')",
            "Update(target_node=ASTNode(type=identifier, text=torch), value='torch_optimizer')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=torch), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=op_jit), value='op_optimized')",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=jit))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=script))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 13,
        "number": 4728,
        "neg_line": [
            "-def test_jit(self, device, dtype):",
            "-op_jit = torch.jit.script(op)",
            "-self.assert_close(op(pt1, pt2), op_jit(pt1, pt2))"
        ],
        "pos_line": [
            "+def test_dynamo(self, device, dtype, torch_optimizer):",
            "+op_optimized = torch_optimizer(op)",
            "+self.assert_close(op(pt1, pt2), op_optimized(pt1, pt2))"
        ],
        "core_change": "-def test_jit(self, device, dtype): +def test_dynamo(self, device, dtype, torch_optimizer): -op_jit = torch.jit.script(op) -self.assert_close(op(pt1, pt2), op_jit(pt1, pt2)) +op_optimized = torch_optimizer(op) +self.assert_close(op(pt1, pt2), op_optimized(pt1, pt2))",
        "core_API": "rand"
    },
    {
        "commit_hash": "68ca6f21ee0d163fa17c5e303f470da84c0c4c97",
        "index": "f431812e..c4db4c77 100644",
        "commit_message": "Fix bug Issue4592 (#4614)\n\n\n",
        "file": "nni.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class MNASNet(nn.Module):",
            "nn.ReLU(inplace=True),",
            "]",
            "self.layers = nn.Sequential(*layers)",
            "-        self.classifier = nn.Sequential(nn.Dropout(p=dropout, inplace=True),",
            "+        self.classifier = nn.Sequential(nn.Dropout(p=dropout),",
            "nn.Linear(1280, num_classes))",
            "self._initialize_weights()",
            "#self.for_test = 10"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=inplace))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=true, text=True))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 4729,
        "neg_line": [
            "-self.classifier = nn.Sequential(nn.Dropout(p=dropout, inplace=True),"
        ],
        "pos_line": [
            "+self.classifier = nn.Sequential(nn.Dropout(p=dropout),"
        ],
        "core_change": "-self.classifier = nn.Sequential(nn.Dropout(p=dropout, inplace=True), +self.classifier = nn.Sequential(nn.Dropout(p=dropout),",
        "core_API": "ReLU"
    },
    {
        "commit_hash": "9ca9691cdca8b8245b635b01091da0be43078b62",
        "index": "7c446aa974..adf79a0603 100644",
        "commit_message": "Fix mnist sgd jenkins tests on master (#4168)\n\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tensorflow",
        "change": [
            "def sum_grad_and_var_all_reduce(grad_and_vars,",
            "#   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))",
            "scaled_grads = [g for g, _ in grad_and_vars]",
            "if alg == 'nccl':",
            "-            summed_grads = nccl.all_sum(scaled_grads)",
            "+            from tensorflow.python.ops import nccl_ops",
            "+            summed_grads = nccl_ops.all_sum(scaled_grads)",
            "elif alg == 'simple':",
            "summed_grads = build_reduce_sum(scaled_grads)",
            "elif alg == 'trivial':"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=ERROR), node=('from', 'from'), position=2, insert_id=2454548)",
            "Insert(target_node=ASTNode(type=ERROR), node=('attribute', None), position=3, insert_id=2454549)",
            "Insert(target_node=ASTNode(type=ERROR), node=('identifier', 'import'), position=4, insert_id=2454550)",
            "Insert(target_node=ASTNode(type=ERROR), node=('identifier', 'nccl_ops'), position=5, insert_id=2454551)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2454552)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2454553)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'ops'), position=2, insert_id=2454554)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tensorflow'), position=0, insert_id=2454555)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2454556)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'python'), position=2, insert_id=2454557)",
            "Update(target_node=ASTNode(type=identifier, text=nccl), value='nccl_ops')"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 4734,
        "neg_line": [
            "-summed_grads = nccl.all_sum(scaled_grads)"
        ],
        "pos_line": [
            "+from tensorflow.python.ops import nccl_ops",
            "+summed_grads = nccl_ops.all_sum(scaled_grads)"
        ],
        "core_change": "-summed_grads = nccl.all_sum(scaled_grads) +from tensorflow.python.ops import nccl_ops +summed_grads = nccl_ops.all_sum(scaled_grads)",
        "core_API": "all_sum"
    },
    {
        "commit_hash": "144ccd1e7833f99a78a6c5bffe349818ef35a7c4",
        "index": "f4f543b8..52e4b0b8 100755",
        "commit_message": "Fix lstm return state shape.\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Lstm(Layer):",
            "",
            "# This distinction is so we can stack multiple LSTM layers",
            "if self.return_final_state:",
            "-            return tf.stack(values=(state.c, state.h), axis=1)",
            "+            return tf.concat(values=(state.c, state.h), axis=1)",
            "else:",
            "-            return x",
            "\\ No newline at end of file",
            "+            return x"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('return_statement', None), position=2, insert_id=2237482)",
            "Insert(target_node=IN(type=return_statement), node=('return', 'return'), position=0, insert_id=2237483)",
            "Insert(target_node=IN(type=return_statement), node=('identifier', 'x'), position=1, insert_id=2237484)",
            "Move(target_node=ASTNode(type=else_clause), node=ASTNode(type=ERROR), position=2)",
            "Insert(target_node=ASTNode(type=else_clause), node=('block', None), position=3, insert_id=2237485)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=2237486)",
            "Move(target_node=IN(type=expression_statement), node=ASTNode(type=identifier, text=file), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=stack), value='concat')",
            "Delete(target_node=ASTNode(type=return, text=return))",
            "Delete(target_node=ASTNode(type=identifier, text=x))",
            "Delete(target_node=ASTNode(type=return_statement))",
            "Delete(target_node=ASTNode(type=block))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 12,
        "number": 4738,
        "neg_line": [
            "-return tf.stack(values=(state.c, state.h), axis=1)",
            "-return x"
        ],
        "pos_line": [
            "+return tf.concat(values=(state.c, state.h), axis=1)",
            "+return x"
        ],
        "core_change": "-return tf.stack(values=(state.c, state.h), axis=1) +return tf.concat(values=(state.c, state.h), axis=1) -return x +return x",
        "core_API": "stack"
    },
    {
        "commit_hash": "1b36393d6eb1e909efb321eaea7facf473c7e1c4",
        "index": "ddce75f..8432653 100644",
        "commit_message": "Update trainer.py (#1098)\n\nfixes #1093\n",
        "file": "examples.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "dataloader",
        "change": [
            "class Trainer:",
            "return loss.item()",
            "",
            "def _run_epoch(self, epoch: int, dataloader: DataLoader, train: bool = True):",
            "-        self.dataloader.sampler.set_epoch(epoch)",
            "+        dataloader.sampler.set_epoch(epoch)",
            "for iter, (source, targets) in enumerate(dataloader):",
            "step_type = \"Train\" if train else \"Eval\"",
            "source = source.to(self.local_rank)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=dataloader), position=0)",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 4742,
        "neg_line": [
            "-self.dataloader.sampler.set_epoch(epoch)"
        ],
        "pos_line": [
            "+dataloader.sampler.set_epoch(epoch)"
        ],
        "core_change": "-self.dataloader.sampler.set_epoch(epoch) +dataloader.sampler.set_epoch(epoch)",
        "core_API": "item"
    },
    {
        "commit_hash": "2fd0f59325d32fc84ed39c4e49b3ee937c7be95c",
        "index": "304e229075..bbec3c6c37 100644",
        "commit_message": "Fixed failing test for clip_vector_norm and a bug with execute_with_gradients (#7582)\n\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def execute_with_gradients(",
            "return grads",
            "",
            "if isinstance(y, ivy.NativeArray):",
            "-        grads = grad_func(torch.clone(y))",
            "+        grads = _set_duplicates(",
            "+            grad_func(torch.clone(y)), required_duplicate_index_chains",
            "+        )",
            "else:",
            "# ToDo: use functorch.jacrev if it fixes the issue with broken memory reference",
            "array_idxs = ivy.nested_argwhere(y, lambda x: ivy.is_native_array(x))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('identifier', '_set_duplicates'), position=0, insert_id=291864)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=291865)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=291866)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=call), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=291867)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'required_duplicate_index_chains'), position=3, insert_id=291868)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=291869)"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 4743,
        "neg_line": [
            "-grads = grad_func(torch.clone(y))"
        ],
        "pos_line": [
            "+grads = _set_duplicates(",
            "+grad_func(torch.clone(y)), required_duplicate_index_chains",
            "+)"
        ],
        "core_change": "-grads = grad_func(torch.clone(y)) +grads = _set_duplicates( +grad_func(torch.clone(y)), required_duplicate_index_chains +)",
        "core_API": "clone"
    },
    {
        "commit_hash": "ed63b7ee9e68ba120893aaaaca0d4525a56ed484",
        "index": "88a83da2..c9c777c2 100644",
        "commit_message": "upgrade to pytorch 0.4.0 (#1126)\n\n* bump pytorch to 0.4 + fix sanitize\n\n* remove check for Variable in block_orthogonal\n\n* rename parameter split_size=\n\n* remove checks for Variable\n\n* fix more tests\n\n* fixes\n\n* get tests to pass\n\n* fix warnings\n\n* get rid of some of the Variables\n\n* more tests passing\n\n* more elimination of variables\n\n* finish removing all Variables\n\n* pylint and such\n\n* a few fixes\n\n* move torch.no_grad into model.forward_on_instances\n\n* more pytorch 0.4 changes\n\n* detach() -> data\n\n* pylint\n\n* fix bad tensor creation\n\n* fix types\n\n* remove print statement\n\n* more 0.4 goodness\n\n* factor out is_tensor\n\n* add no_grad to elmo command\n\n* cleanup\n\n* remove TODO\n\n* address PR feedback\n\n* replace all() with item()\n\n* more cleanup\n\n* further cleanup\n\n* remove Variable\n\n* really fix merge conflict\n\n* fix pylint\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class TestCnnEncoder(AllenNlpTestCase):",
            "num_filters=13,",
            "ngram_filter_sizes=(1, 2, 3, 4, 5),",
            "output_dim=30)",
            "-        tensor = Variable(torch.rand(4, 8, 7))",
            "+        tensor = torch.rand(4, 8, 7)",
            "assert encoder(tensor, None).size() == (4, 30)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=IN(type=root), node=ASTNode(type=call), position=0)",
            "Delete(target_node=ASTNode(type=identifier, text=Variable))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 4750,
        "neg_line": [
            "-tensor = Variable(torch.rand(4, 8, 7))"
        ],
        "pos_line": [
            "+tensor = torch.rand(4, 8, 7)"
        ],
        "core_change": "-tensor = Variable(torch.rand(4, 8, 7)) +tensor = torch.rand(4, 8, 7)",
        "core_API": "rand"
    },
    {
        "commit_hash": "3f937d9b76d1ca3158ba5e33b1abe358f23f7af6",
        "index": "abf5154995..f0eb34034e 100644",
        "commit_message": "lintfixbot: Auto-commit fixed lint errors in codebase\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def as_ivy_dtype(dtype_in: Union[torch.dtype, str, bool, int, float], /) -> ivy.",
            "",
            "",
            "@with_unsupported_dtypes({\"1.11.0 and below\": (\"uint16\",)}, backend_version)",
            "-def as_native_dtype(dtype_in: Union[torch.dtype, str, bool, int, float],",
            "-                    /) -> torch.dtype:",
            "+def as_native_dtype(",
            "+    dtype_in: Union[torch.dtype, str, bool, int, float], /",
            "+) -> torch.dtype:",
            "if dtype_in is int:",
            "return ivy.default_int_dtype(as_native=True)",
            "if dtype_in is float:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=1, insert_id=271445)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=271446)",
            "Move(target_node=IN(type=if_statement), node=ASTNode(type=comparison_operator), position=1)",
            "Move(target_node=IN(type=if_statement), node=ASTNode(type=:, text=:), position=2)",
            "Insert(target_node=IN(type=if_statement), node=('block', ''), position=3, insert_id=271447)",
            "Insert(target_node=ASTNode(type=block), node=('ERROR', None), position=1, insert_id=271448)",
            "Insert(target_node=ASTNode(type=expression_statement), node=('ERROR', None), position=2, insert_id=271449)",
            "Move(target_node=ASTNode(type=expression_statement), node=ASTNode(type=attribute), position=3)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=:, text=:), position=0)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=/, text=/), position=0)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=), text=)), position=1)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=->, text=->), position=2)",
            "Delete(target_node=ASTNode(type=identifier, text=if))",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=type))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=ERROR))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 4755,
        "neg_line": [
            "-def as_native_dtype(dtype_in: Union[torch.dtype, str, bool, int, float],",
            "-/) -> torch.dtype:"
        ],
        "pos_line": [
            "+def as_native_dtype(",
            "+dtype_in: Union[torch.dtype, str, bool, int, float], /",
            "+) -> torch.dtype:"
        ],
        "core_change": "-def as_native_dtype(dtype_in: Union[torch.dtype, str, bool, int, float], -/) -> torch.dtype: +def as_native_dtype( +dtype_in: Union[torch.dtype, str, bool, int, float], / +) -> torch.dtype:",
        "core_API": "default_int_dtype"
    },
    {
        "commit_hash": "a979a3f34eebbea7e6ffa7506aaf83aa6c47e7a1",
        "index": "feb49e39..197bd5a1 100755",
        "commit_message": "fix image summary\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Model(ModelDesc):",
            "W_init=tf.truncated_normal_initializer(stddev=0.02)):",
            "with tf.variable_scope('gen'):",
            "image_gen = self.generator(z)",
            "-                tf.summary.image('gen', image_gen, max_images=30)",
            "+                tf.summary.image('gen', image_gen, max_outputs=30)",
            "with tf.variable_scope('discrim'):",
            "vecpos, _ = self.discriminator(image_pos)",
            "with tf.variable_scope('discrim', reuse=True):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=max_images), value='max_outputs')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 4756,
        "neg_line": [
            "-tf.summary.image('gen', image_gen, max_images=30)"
        ],
        "pos_line": [
            "+tf.summary.image('gen', image_gen, max_outputs=30)"
        ],
        "core_change": "-tf.summary.image('gen', image_gen, max_images=30) +tf.summary.image('gen', image_gen, max_outputs=30)",
        "core_API": "truncated_normal_initializer"
    },
    {
        "commit_hash": "911786a0eee284be0d5979c5df383ad064dc37c5",
        "index": "01046c68..4b3d94ea 100644",
        "commit_message": "Remove duplicated functional of gp (#1607)\n\n* clean gp\n\n* rearange sgpr\n\n* kernel add -> sum in test_benchmark\n\n* nit\n\n* fix error\n\n* fix error during clean\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def svgp_multiclass(num_steps, whiten):",
            "whiten=whiten)",
            "",
            "gpmodel.fix_param(\"Xu\")",
            "-    gpmodel.kernel.get_subkernel(\"WhiteNoise\").fix_param(\"variance\")",
            "+    gpmodel.kernel.kern1.fix_param(\"variance\")",
            "",
            "gpmodel.optimize(optim.Adam({\"lr\": 0.0001}), num_steps=num_steps)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=get_subkernel), value='kern1')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 4757,
        "neg_line": [
            "-gpmodel.kernel.get_subkernel(\"WhiteNoise\").fix_param(\"variance\")"
        ],
        "pos_line": [
            "+gpmodel.kernel.kern1.fix_param(\"variance\")"
        ],
        "core_change": "-gpmodel.kernel.get_subkernel(\"WhiteNoise\").fix_param(\"variance\") +gpmodel.kernel.kern1.fix_param(\"variance\")",
        "core_API": "fix_param"
    },
    {
        "commit_hash": "925f014cb0104cefe4a53ba17c8ddc31400955f6",
        "index": "e3bf55710e..04ac1714ab 100644",
        "commit_message": "Fixed allclose to give consisten output of boolean arrays, fixed the docstring examples and added the correct decorators\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def allclose(",
            "equal_nan: Optional[bool] = False,",
            "out: Optional[torch.Tensor] = None,",
            ") -> bool:",
            "-    return torch.allclose(x1, x2, rtol=rtol, atol=atol, equal_nan=equal_nan)",
            "+    ret = torch.allclose(x1, x2, rtol=rtol, atol=atol, equal_nan=equal_nan)",
            "+    return torch.tensor(ret)",
            "",
            "",
            "@with_unsupported_dtypes({\"1.11.0 and below\": (\"float16\",)}, backend_version)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=block), node=('expression_statement', None), position=0, insert_id=283698)",
            "Insert(target_node=ASTNode(type=block), node=('return_statement', None), position=1, insert_id=283699)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=283700)",
            "Insert(target_node=IN(type=return_statement), node=('return', 'return'), position=0, insert_id=283701)",
            "Insert(target_node=IN(type=return_statement), node=('call', None), position=1, insert_id=283702)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'ret'), position=0, insert_id=283703)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=283704)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=call), position=2)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=283705)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=283706)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=283707)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=283708)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tensor'), position=2, insert_id=283709)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=283710)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'ret'), position=1, insert_id=283711)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=283712)",
            "Delete(target_node=ASTNode(type=return, text=return))",
            "Delete(target_node=ASTNode(type=return_statement))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 18,
        "number": 4759,
        "neg_line": [
            "-return torch.allclose(x1, x2, rtol=rtol, atol=atol, equal_nan=equal_nan)"
        ],
        "pos_line": [
            "+ret = torch.allclose(x1, x2, rtol=rtol, atol=atol, equal_nan=equal_nan)",
            "+return torch.tensor(ret)"
        ],
        "core_change": "-return torch.allclose(x1, x2, rtol=rtol, atol=atol, equal_nan=equal_nan) +ret = torch.allclose(x1, x2, rtol=rtol, atol=atol, equal_nan=equal_nan) +return torch.tensor(ret)",
        "core_API": "allclose"
    },
    {
        "commit_hash": "c7fd1d9fb18862318b133c9474d37c5085850070",
        "index": "7a4d77c2..8142f7b7 100755",
        "commit_message": "fix deprecations about casting & initializers in tf1.13\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class ResNetFPNModel(DetectionModel):",
            "maskrcnn_head_func = getattr(model_mrcnn, cfg.FPN.MRCNN_HEAD_FUNC)",
            "mask_logits = maskrcnn_head_func(",
            "'maskrcnn', roi_feature_maskrcnn, cfg.DATA.NUM_CATEGORY)   # #fg x #cat x 28 x 28",
            "-                indices = tf.stack([tf.range(tf.size(final_labels)), tf.to_int32(final_labels) - 1], axis=1)",
            "+                indices = tf.stack([tf.range(tf.size(final_labels)), tf.cast(final_labels, tf.int32) - 1], axis=1)",
            "final_mask_logits = tf.gather_nd(mask_logits, indices)   # #resultx28x28",
            "tf.sigmoid(final_mask_logits, name='output/masks')",
            "return []"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=to_int32), value='cast')",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=2278900)",
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=3, insert_id=2278901)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2278902)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2278903)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'int32'), position=2, insert_id=2278904)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 4764,
        "neg_line": [
            "-indices = tf.stack([tf.range(tf.size(final_labels)), tf.to_int32(final_labels) - 1], axis=1)"
        ],
        "pos_line": [
            "+indices = tf.stack([tf.range(tf.size(final_labels)), tf.cast(final_labels, tf.int32) - 1], axis=1)"
        ],
        "core_change": "-indices = tf.stack([tf.range(tf.size(final_labels)), tf.to_int32(final_labels) - 1], axis=1) +indices = tf.stack([tf.range(tf.size(final_labels)), tf.cast(final_labels, tf.int32) - 1], axis=1)",
        "core_API": "stack"
    },
    {
        "commit_hash": "7d87b71e235ba51051fed5fbc61ace41412737eb",
        "index": "5fb48a1b..7fda6e0e 100644",
        "commit_message": "Fix #235 replaying map_data (via dist.Subsample) (#239)\n\n* Add failing tests for #235\n\n* Implement iarange via dist.Subsample\n\n* Add documentation to subsample.py\n\n* Do not treat warnings as errors\n\n* Use pyro.util.log_gamma\n\n* Fix type error in Subsample.batch_log_pdf\n\n* Fix failing test that inspected sample sites\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def iarange(name, size, subsample_size=0):",
            "yield Variable(torch.LongTensor(list(range(size))))",
            "return",
            "",
            "-    subsample = Variable(torch.randperm(size)[0:subsample_size])",
            "+    subsample = sample(name, Subsample(size, subsample_size))",
            "if len(_PYRO_STACK) == 0:",
            "yield subsample",
            "else:"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=Variable), value='sample')",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'name'), position=1, insert_id=764918)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=764919)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=call), position=3)",
            "Update(target_node=ASTNode(type=identifier, text=torch), value='Subsample')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=torch), position=0)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=764920)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=subsample_size), position=3)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=randperm))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=integer, text=0))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=slice))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 4768,
        "neg_line": [
            "-subsample = Variable(torch.randperm(size)[0:subsample_size])"
        ],
        "pos_line": [
            "+subsample = sample(name, Subsample(size, subsample_size))"
        ],
        "core_change": "-subsample = Variable(torch.randperm(size)[0:subsample_size]) +subsample = sample(name, Subsample(size, subsample_size))",
        "core_API": "LongTensor"
    },
    {
        "commit_hash": "9f84a897eef892240c5004276f3d5e60c0015395",
        "index": "b8c8c52cf9..10ff99b268 100644",
        "commit_message": "more numpy tests fixes\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def argmin(",
            "keepdims: bool = False,",
            "out: Optional[torch.Tensor] = None,",
            ") -> torch.Tensor:",
            "-    x = torch.tensor(x)",
            "return torch.argmin(x, axis=axis, keepdim=keepdims, out=out)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('block', ''), position=0, insert_id=323912)",
            "Delete(target_node=ASTNode(type=identifier, text=x))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=tensor))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=x))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=block))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 15,
        "number": 4769,
        "neg_line": [
            "-x = torch.tensor(x)"
        ],
        "pos_line": [],
        "core_change": "-x = torch.tensor(x)",
        "core_API": "tensor"
    },
    {
        "commit_hash": "fdd1fd285744d76f6e8865838d46558e65bfed2e",
        "index": "7daff431..129a8fa9 100755",
        "commit_message": "Quick fix\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def SubpixelConv2d(net, scale=2, n_out_channel=None, act=tf.identity, name='subp",
            "bsize = tf.shape(X)[0] # Handling Dimension(None) type for undefined batch dim",
            "Xs=tf.split(X,r,3) #b*h*w*r*r",
            "Xr=tf.concat(Xs,2) #b*h*(r*w)*r",
            "-            X=tf.reshape(Xr,(b,r*h,r*w,c)) # b*(r*h)*(r*w)*c",
            "+            X=tf.reshape(Xr,(b,r*a,r*b,c)) # b*(r*h)*(r*w)*c",
            "else:",
            "print(_err_log)",
            "return X"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=h), value='a')",
            "Update(target_node=ASTNode(type=identifier, text=w), value='b')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 4770,
        "neg_line": [
            "-X=tf.reshape(Xr,(b,r*h,r*w,c)) # b*(r*h)*(r*w)*c"
        ],
        "pos_line": [
            "+X=tf.reshape(Xr,(b,r*a,r*b,c)) # b*(r*h)*(r*w)*c"
        ],
        "core_change": "-X=tf.reshape(Xr,(b,r*h,r*w,c)) # b*(r*h)*(r*w)*c +X=tf.reshape(Xr,(b,r*a,r*b,c)) # b*(r*h)*(r*w)*c",
        "core_API": "shape"
    },
    {
        "commit_hash": "c7fd1d9fb18862318b133c9474d37c5085850070",
        "index": "85737a35..af3c7859 100644",
        "commit_message": "fix deprecations about casting & initializers in tf1.13\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def resnet_fpn_backbone(image, num_blocks):",
            "freeze_at = cfg.BACKBONE.FREEZE_AT",
            "shape2d = tf.shape(image)[2:]",
            "mult = float(cfg.FPN.RESOLUTION_REQUIREMENT)",
            "-    new_shape2d = tf.to_int32(tf.ceil(tf.to_float(shape2d) / mult) * mult)",
            "+    new_shape2d = tf.cast(tf.ceil(tf.cast(shape2d, tf.float32) / mult) * mult, tf.int32)",
            "pad_shape2d = new_shape2d - shape2d",
            "assert len(num_blocks) == 4, num_blocks",
            "with backbone_scope(freeze=freeze_at > 0):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=to_int32), value='cast')",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=2278860)",
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=3, insert_id=2278861)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2278862)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2278863)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'int32'), position=2, insert_id=2278864)",
            "Update(target_node=ASTNode(type=identifier, text=to_float), value='cast')",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=2278865)",
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=3, insert_id=2278866)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2278867)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2278868)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'float32'), position=2, insert_id=2278869)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 12,
        "number": 4773,
        "neg_line": [
            "-new_shape2d = tf.to_int32(tf.ceil(tf.to_float(shape2d) / mult) * mult)"
        ],
        "pos_line": [
            "+new_shape2d = tf.cast(tf.ceil(tf.cast(shape2d, tf.float32) / mult) * mult, tf.int32)"
        ],
        "core_change": "-new_shape2d = tf.to_int32(tf.ceil(tf.to_float(shape2d) / mult) * mult) +new_shape2d = tf.cast(tf.ceil(tf.cast(shape2d, tf.float32) / mult) * mult, tf.int32)",
        "core_API": "shape"
    },
    {
        "commit_hash": "0e0b1ede051239b86015d57a6e4b29cc5628fab5",
        "index": "94c16b19..afa0dff3 100644",
        "commit_message": "fix entropy term in distributions.py (wrong isinstance check) (#3120)\n\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ExpandedDistribution(TorchDistribution):",
            "log_prob = log_prob.expand(shape)",
            "if isinstance(score_function, torch.Tensor):",
            "score_function = score_function.expand(shape)",
            "-            if isinstance(score_function, torch.Tensor):",
            "+            if isinstance(entropy_term, torch.Tensor):",
            "entropy_term = entropy_term.expand(shape)",
            "return ScoreParts(log_prob, score_function, entropy_term)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=score_function), value='entropy_term')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 4774,
        "neg_line": [
            "-if isinstance(score_function, torch.Tensor):"
        ],
        "pos_line": [
            "+if isinstance(entropy_term, torch.Tensor):"
        ],
        "core_change": "-if isinstance(score_function, torch.Tensor): +if isinstance(entropy_term, torch.Tensor):",
        "core_API": "expand"
    },
    {
        "commit_hash": "af6e54febe3b822118dfed28ddb797d8350e5d27",
        "index": "d14a8351b..fab1ef46a 100644",
        "commit_message": "Fix typo in train split name (#3751)\n\n* Fix typo in README guide\n\n* Fix split naming and table alignment in README guide\n\n* Fix split naming and table alignment in all datasets\n\n* Fix previously malformed dataset cards\n\n* Fix previously malformed dataset cards\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class DatasetBuilder:",
            "Please follow the manual download instructions:",
            "{self.manual_download_instructions}",
            "Manual data can be loaded with:",
            "-                     datasets.load_dataset({self.name}, data_dir='<path/to/manual/data>')\"\"\"",
            "+                     datasets.load_dataset(\"{self.name}\", data_dir=\"<path/to/manual/data>\")\"\"\"",
            ")",
            ")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('string', '\"{self.name}\"'), position=1, insert_id=1780859)",
            "Update(target_node=ASTNode(type=string, text='<path/to/manual/data>'), value='\"<path/to/manual/data>\"')",
            "Delete(target_node=ASTNode(type={, text={))",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=name))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=}, text=}))",
            "Delete(target_node=ASTNode(type=set))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 4778,
        "neg_line": [
            "-datasets.load_dataset({self.name}, data_dir='<path/to/manual/data>')\"\"\""
        ],
        "pos_line": [
            "+datasets.load_dataset(\"{self.name}\", data_dir=\"<path/to/manual/data>\")\"\"\""
        ],
        "core_change": "-datasets.load_dataset({self.name}, data_dir='<path/to/manual/data>')\"\"\" +datasets.load_dataset(\"{self.name}\", data_dir=\"<path/to/manual/data>\")\"\"\"",
        "core_API": "load_dataset"
    },
    {
        "commit_hash": "b41cffaa93e8205bd8bd309f82c33c07c420eefd",
        "index": "89fba7d..0a96098 100644",
        "commit_message": "Fix a few issues loading pretrained vit/bit npz weights w/ num_classes=0 __init__ arg. Missed a few other small classifier handling detail on Mlp, GhostNet, Levit. Should fix #713\n\n",
        "file": "pytorch-image-models.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "def _load_weights(model: VisionTransformer, checkpoint_path: str, prefix: str =",
            "model.pos_embed.copy_(pos_embed_w)",
            "model.norm.weight.copy_(_n2p(w[f'{prefix}Transformer/encoder_norm/scale']))",
            "model.norm.bias.copy_(_n2p(w[f'{prefix}Transformer/encoder_norm/bias']))",
            "-    if model.head.bias.shape[0] == w[f'{prefix}head/bias'].shape[-1]:",
            "+    if isinstance(model.head, nn.Linear) and model.head.bias.shape[0] == w[f'{prefix}head/bias'].shape[-1]:",
            "model.head.weight.copy_(_n2p(w[f'{prefix}head/kernel']))",
            "model.head.bias.copy_(_n2p(w[f'{prefix}head/bias']))",
            "for i, block in enumerate(model.blocks.children()):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=ERROR), node=('boolean_operator', None), position=0, insert_id=1477840)",
            "Insert(target_node=IN(type=boolean_operator), node=('call', None), position=0, insert_id=1477841)",
            "Insert(target_node=IN(type=boolean_operator), node=('and', 'and'), position=1, insert_id=1477842)",
            "Move(target_node=IN(type=boolean_operator), node=ASTNode(type=comparison_operator), position=2)",
            "Insert(target_node=IN(type=call), node=('identifier', 'isinstance'), position=0, insert_id=1477843)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1477844)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1477845)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=1477846)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=1477847)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=3, insert_id=1477848)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=1477849)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'model'), position=0, insert_id=1477850)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1477851)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'head'), position=2, insert_id=1477852)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'nn'), position=0, insert_id=1477853)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1477854)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Linear'), position=2, insert_id=1477855)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 4780,
        "neg_line": [
            "-if model.head.bias.shape[0] == w[f'{prefix}head/bias'].shape[-1]:"
        ],
        "pos_line": [
            "+if isinstance(model.head, nn.Linear) and model.head.bias.shape[0] == w[f'{prefix}head/bias'].shape[-1]:"
        ],
        "core_change": "-if model.head.bias.shape[0] == w[f'{prefix}head/bias'].shape[-1]: +if isinstance(model.head, nn.Linear) and model.head.bias.shape[0] == w[f'{prefix}head/bias'].shape[-1]:",
        "core_API": "copy_"
    },
    {
        "commit_hash": "cc2f9c127b20f6e4727891960be6aa122c44cbdc",
        "index": "a7e3218c..11d524d3 100644",
        "commit_message": "fix typo in freeze_variables\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def remap_variables(fn):",
            "def freeze_variables():",
            "\"\"\"",
            "Return a context, where all variables (reused or not) returned by",
            "-    ``get_variable`` will have no gradients (they  will be followed by ``tf.stop_gradient``).",
            "+    ``get_variable`` will have no gradients (they will be wrapped by ``tf.stop_gradient``).",
            "But they will still be in ``TRAINABLE_VARIABLES`` collections so they will get",
            "saved correctly. This is useful to fix certain variables for fine-tuning.",
            "",
            "Example:",
            ".. code-block:: python",
            "",
            "-            with varreplace.freeze_get_variable():",
            "+            with varreplace.freeze_variable():",
            "x = FullyConnected('fc', x, 1000)   # fc/* will not be trained",
            "\"\"\"",
            "return remap_variables(lambda v: tf.stop_gradient(v))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"\"\"\nReturn a context, where all variables (reused or not) returned by\n    ``get_variable`` will have no gradients (they  will be followed by ``tf.stop_gradient``).\nBut they will still be in ``TRAINABLE_VARIABLES`` collections so they will get\nsaved correctly. This is useful to fix certain variables for fine-tuning.\n\nExample:\n.. code-block:: python\n\n            with varreplace.freeze_get_variable():\nx = FullyConnected('fc', x, 1000)   # fc/* will not be trained\n\"\"\"), value='\"\"\"\\nReturn a context, where all variables (reused or not) returned by\\n    ``get_variable`` will have no gradients (they will be wrapped by ``tf.stop_gradient``).\\nBut they will still be in ``TRAINABLE_VARIABLES`` collections so they will get\\nsaved correctly. This is useful to fix certain variables for fine-tuning.\\n\\nExample:\\n.. code-block:: python\\n\\n            with varreplace.freeze_variable():\\nx = FullyConnected(\\'fc\\', x, 1000)   # fc/* will not be trained\\n\"\"\"')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 1,
        "number": 4783,
        "neg_line": [
            "-``get_variable`` will have no gradients (they  will be followed by ``tf.stop_gradient``).",
            "-with varreplace.freeze_get_variable():"
        ],
        "pos_line": [
            "+``get_variable`` will have no gradients (they will be wrapped by ``tf.stop_gradient``).",
            "+with varreplace.freeze_variable():"
        ],
        "core_change": "-``get_variable`` will have no gradients (they  will be followed by ``tf.stop_gradient``). +``get_variable`` will have no gradients (they will be wrapped by ``tf.stop_gradient``). -with varreplace.freeze_get_variable(): +with varreplace.freeze_variable():",
        "core_API": "freeze_get_variable"
    },
    {
        "commit_hash": "a93739431299871f29d8014a343be9ad10162377",
        "index": "2c7cdf686..256fac203 100644",
        "commit_message": "[Metrics] Unification of regression (#4166)\n\n* moved to utility\n\n* add files\n\n* unify\n\n* add desc\n\n* update\n\n* end of line\n\n* Apply suggestions from code review\n\nCo-authored-by: Justus Schock <12886177+justusschock@users.noreply.github.com>\n\n* Apply suggestions from code review\n\nCo-authored-by: Justus Schock <12886177+justusschock@users.noreply.github.com>\n\n* add back functional test in new interface\n\n* pep8\n\n* doctest fix\n\n* test name fix\n\n* unify psnr + add class psnr, TODO: psnr test refactor ala mean squared error\n\n* unify psnr\n\n* rm unused code\n\n* pep8\n\n* docs\n\n* unify ssim\n\n* lower tolerance for ssim\n\n* fix import\n\n* pep8\n\n* docs\n\n* flake8\n\n* test smaller images\n\n* trying to fix test\n\n* no ddp test for ssim\n\n* pep8\n\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\nCo-authored-by: Justus Schock <12886177+justusschock@users.noreply.github.com>\nCo-authored-by: Teddy Koker <teddy.koker@gmail.com>\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class MeanSquaredLogError(Metric):",
            "preds: Predictions from model",
            "target: Ground truth values",
            "\"\"\"",
            "-        self._check_same_shape(preds, target)",
            "-        squared_log_error = torch.pow(torch.log1p(preds) - torch.log1p(target), 2)",
            "+        sum_squared_log_error, n_obs = _mean_squared_log_error_update(preds, target)",
            "",
            "-        self.sum_squared_log_error += torch.sum(squared_log_error)",
            "-        self.total += target.numel()",
            "+        self.sum_squared_log_error += sum_squared_log_error",
            "+        self.total += n_obs",
            "",
            "def compute(self):",
            "\"\"\"",
            "Compute mean squared logarithmic error over state.",
            "\"\"\"",
            "-        return self.sum_squared_log_error / self.total",
            "+        return _mean_squared_log_error_compute(self.sum_squared_log_error, self.total)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"\"\"\n        self._check_same_shape(preds, target)\n        squared_log_error = torch.pow(torch.log1p(preds) - torch.log1p(target), 2)\n\n        self.sum_squared_log_error += torch.sum(squared_log_error)\n        self.total += target.numel()\n\ndef compute(self):\n\"\"\"), value='\"\"\"\\n        sum_squared_log_error, n_obs = _mean_squared_log_error_update(preds, target)\\n\\n        self.sum_squared_log_error += sum_squared_log_error\\n        self.total += n_obs\\n\\ndef compute(self):\\n\"\"\"')",
            "Insert(target_node=ASTNode(type=return_statement), node=('call', None), position=1, insert_id=559453)",
            "Insert(target_node=IN(type=call), node=('identifier', '_mean_squared_log_error_compute'), position=0, insert_id=559454)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=559455)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=559456)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=attribute), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=559457)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=attribute), position=3)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=559458)",
            "Delete(target_node=ASTNode(type=/, text=/))",
            "Delete(target_node=ASTNode(type=binary_operator))"
        ],
        "plus_line": 4,
        "minus_line": 5,
        "AST_diff_line": 11,
        "number": 4794,
        "neg_line": [
            "-self._check_same_shape(preds, target)",
            "-squared_log_error = torch.pow(torch.log1p(preds) - torch.log1p(target), 2)",
            "-self.sum_squared_log_error += torch.sum(squared_log_error)",
            "-self.total += target.numel()",
            "-return self.sum_squared_log_error / self.total"
        ],
        "pos_line": [
            "+sum_squared_log_error, n_obs = _mean_squared_log_error_update(preds, target)",
            "+self.sum_squared_log_error += sum_squared_log_error",
            "+self.total += n_obs",
            "+return _mean_squared_log_error_compute(self.sum_squared_log_error, self.total)"
        ],
        "core_change": "-self._check_same_shape(preds, target) -squared_log_error = torch.pow(torch.log1p(preds) - torch.log1p(target), 2) +sum_squared_log_error, n_obs = _mean_squared_log_error_update(preds, target) -self.sum_squared_log_error += torch.sum(squared_log_error) -self.total += target.numel() +self.sum_squared_log_error += sum_squared_log_error +self.total += n_obs -return self.sum_squared_log_error / self.total +return _mean_squared_log_error_compute(self.sum_squared_log_error, self.total)",
        "core_API": "_check_same_shape"
    },
    {
        "commit_hash": "2bbb3f7a400b92780451caf12d6a4c7729b5302d",
        "index": "18c74904..bf312db4 100644",
        "commit_message": "don't use sigmoid output for tacotron, fix bug for memory queue handling, remove maxout\n\n",
        "file": "TTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class Tacotron(nn.Module):",
            "forward_attn, trans_agent, forward_attn_mask,",
            "location_attn, separate_stopnet)",
            "self.postnet = PostCBHG(mel_dim)",
            "-        self.last_linear = nn.Sequential(",
            "-            nn.Linear(self.postnet.cbhg.gru_features * 2, linear_dim),",
            "-            nn.Sigmoid())",
            "+        self.last_linear = nn.Linear(self.postnet.cbhg.gru_features * 2, linear_dim)",
            "",
            "def forward(self, characters, text_lengths, mel_specs, speaker_ids=None):",
            "B = characters.size(0)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=nn))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=Sequential))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=nn))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=Sigmoid))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 16,
        "number": 4795,
        "neg_line": [
            "-self.last_linear = nn.Sequential(",
            "-nn.Linear(self.postnet.cbhg.gru_features * 2, linear_dim),",
            "-nn.Sigmoid())"
        ],
        "pos_line": [
            "+self.last_linear = nn.Linear(self.postnet.cbhg.gru_features * 2, linear_dim)"
        ],
        "core_change": "-self.last_linear = nn.Sequential( -nn.Linear(self.postnet.cbhg.gru_features * 2, linear_dim), -nn.Sigmoid()) +self.last_linear = nn.Linear(self.postnet.cbhg.gru_features * 2, linear_dim)",
        "core_API": "Sequential"
    },
    {
        "commit_hash": "e9dc54611923f6cff85ccde42bed7c1005fdfdd8",
        "index": "464c657c..d299ea96 100644",
        "commit_message": "Fix TorchScript support in `DNAConv` and `FAConv` (#6754)\n\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_dna_conv1():",
            "assert out.size() == (num_nodes, channels)",
            "",
            "if is_full_test():",
            "-        jit = torch.jit.script(conv.jittable())",
            "+        t = '(Tensor, Tensor, OptTensor) -> Tensor'",
            "+        jit = torch.jit.script(conv.jittable(t))",
            "assert jit(x, edge_index).tolist() == out.tolist()"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=block), node=('expression_statement', None), position=0, insert_id=931542)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=931543)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 't'), position=0, insert_id=931544)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=931545)",
            "Insert(target_node=IN(type=assignment), node=('string', \"'(Tensor, Tensor, OptTensor) -> Tensor'\"), position=2, insert_id=931546)",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 't'), position=1, insert_id=931547)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 4796,
        "neg_line": [
            "-jit = torch.jit.script(conv.jittable())"
        ],
        "pos_line": [
            "+t = '(Tensor, Tensor, OptTensor) -> Tensor'",
            "+jit = torch.jit.script(conv.jittable(t))"
        ],
        "core_change": "-jit = torch.jit.script(conv.jittable()) +t = '(Tensor, Tensor, OptTensor) -> Tensor' +jit = torch.jit.script(conv.jittable(t))",
        "core_API": "size"
    },
    {
        "commit_hash": "77534faf1a43447bf66b243dbdee792baca7d6c6",
        "index": "e95be49b..f5572b45 100644",
        "commit_message": "summaries updated and fixed\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class InternalLstm(TransformationBase):",
            "assert False",
            "",
            "specification[self.name] = dict(",
            "-            type='float', shape=(2, self.size), initial_state=tf.identity(x=self.initial_state)",
            "+            type='float', shape=(2, self.size), initial_state=self.initial_state",
            ")",
            "",
            "return specification"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=x), value='initial_state')",
            "Delete(target_node=ASTNode(type=identifier, text=initial_state))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=identity))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 4797,
        "neg_line": [
            "-type='float', shape=(2, self.size), initial_state=tf.identity(x=self.initial_state)"
        ],
        "pos_line": [
            "+type='float', shape=(2, self.size), initial_state=self.initial_state"
        ],
        "core_change": "-type='float', shape=(2, self.size), initial_state=tf.identity(x=self.initial_state) +type='float', shape=(2, self.size), initial_state=self.initial_state",
        "core_API": "identity"
    },
    {
        "commit_hash": "ce961a9fd26aef5130720cb6a171ddd5b51a8961",
        "index": "ed40f6d8..a4057f60 100644",
        "commit_message": "Fix logits masking bug introduced in #2469 (#2774)\n\nSummary:\n- Masking logits with float(inf) significantly degrades deconding\nperformance with language model and flashlight decoder. This fixes it.\n\n# Before submitting\n\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\n- [ ] Did you make sure to update the docs?\n- [ ] Did you write any new necessary tests?\n\n## What does this PR do?\nFixes logits masking bug introduced in https://github.com/fairinternal/fairseq-py/issues/2469\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding \n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2774\n\nReviewed By: arbabu123\n\nDifferential Revision: D33003148\n\nPulled By: alexeib\n\nfbshipit-source-id: 19bf3da4f5104b33357fd4941e5e76b95174ee28\n\n",
        "file": "fairseq.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Wav2VecCtc(BaseFairseqModel):",
            "",
            "if net_output[\"padding_mask\"] is not None and net_output[\"padding_mask\"].any():",
            "number_of_classes = logits.size(-1)",
            "-            masking_tensor = torch.ones(number_of_classes) * float(\"-inf\")",
            "-            masking_tensor[0] = float(\"inf\")",
            "+            masking_tensor = torch.ones(",
            "+                number_of_classes, device=logits.device",
            "+            ) * float(\"-inf\")",
            "+            masking_tensor[0] = 0",
            "logits[net_output[\"padding_mask\"].T] = masking_tensor.type_as(logits)",
            "",
            "if normalize:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('integer', '0'), position=2, insert_id=205191)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=205192)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=205193)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=205194)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=205195)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=205196)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'logits'), position=0, insert_id=205197)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=205198)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=205199)",
            "Delete(target_node=ASTNode(type=identifier, text=float))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=string, text=\"inf\"))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 4,
        "minus_line": 2,
        "AST_diff_line": 15,
        "number": 4798,
        "neg_line": [
            "-masking_tensor = torch.ones(number_of_classes) * float(\"-inf\")",
            "-masking_tensor[0] = float(\"inf\")"
        ],
        "pos_line": [
            "+masking_tensor = torch.ones(",
            "+number_of_classes, device=logits.device",
            "+) * float(\"-inf\")",
            "+masking_tensor[0] = 0"
        ],
        "core_change": "-masking_tensor = torch.ones(number_of_classes) * float(\"-inf\") -masking_tensor[0] = float(\"inf\") +masking_tensor = torch.ones( +number_of_classes, device=logits.device +) * float(\"-inf\") +masking_tensor[0] = 0",
        "core_API": "size"
    },
    {
        "commit_hash": "c6a0a2a96e255794dab9d403152b91b6bc7d64e4",
        "index": "01a0a5e2..22634726 100644",
        "commit_message": "dtype fix in LayerScale to support mixed precision\n\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class LayerScale(layers.Layer):",
            "",
            "def build(self, input_shape):",
            "self.gamma = tf.Variable(",
            "-            self.init_values * tf.ones((self.projection_dim,))",
            "+            self.init_values * tf.ones((self.projection_dim,)),",
            "+            dtype=self._compute_dtype_object",
            ")",
            "",
            "def call(self, x):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=2042209)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=2042210)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=2042211)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2042212)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=2042213)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=2042214)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2042215)",
            "Insert(target_node=IN(type=attribute), node=('identifier', '_compute_dtype_object'), position=2, insert_id=2042216)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 4799,
        "neg_line": [
            "-self.init_values * tf.ones((self.projection_dim,))"
        ],
        "pos_line": [
            "+self.init_values * tf.ones((self.projection_dim,)),",
            "+dtype=self._compute_dtype_object"
        ],
        "core_change": "-self.init_values * tf.ones((self.projection_dim,)) +self.init_values * tf.ones((self.projection_dim,)), +dtype=self._compute_dtype_object",
        "core_API": "Variable"
    },
    {
        "commit_hash": "8e942aed8527fabb4995a3c46918056a5308604b",
        "index": "3b181a64..e836225f 100644",
        "commit_message": "Fix the device of label in multiclass_nms (#5673)\n\n\n",
        "file": "mmdetection.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def multiclass_nms(multi_bboxes,",
            "",
            "scores = multi_scores[:, :-1]",
            "",
            "-    labels = torch.arange(num_classes, dtype=torch.long)",
            "+    labels = torch.arange(num_classes, dtype=torch.long, device=scores.device)",
            "labels = labels.view(1, -1).expand_as(scores)",
            "",
            "bboxes = bboxes.reshape(-1, 4)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=626030)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=626031)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=626032)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=626033)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=626034)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'scores'), position=0, insert_id=626035)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=626036)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=626037)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 4802,
        "neg_line": [
            "-labels = torch.arange(num_classes, dtype=torch.long)"
        ],
        "pos_line": [
            "+labels = torch.arange(num_classes, dtype=torch.long, device=scores.device)"
        ],
        "core_change": "-labels = torch.arange(num_classes, dtype=torch.long) +labels = torch.arange(num_classes, dtype=torch.long, device=scores.device)",
        "core_API": "arange"
    },
    {
        "commit_hash": "e94abf66783f5e24ab37adc754ac902e827a4b4d",
        "index": "f2809357..5615cfd0 100644",
        "commit_message": "fix TF deprecation of concat/split/pack/unpack\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def BilinearUpSample(x, shape):",
            "\"\"\"",
            "# inp_shape = tf.shape(x)",
            "# return tf.image.resize_bilinear(x,",
            "-    # tf.pack([inp_shape[1]*shape,inp_shape[2]*shape]),",
            "+    # tf.stack([inp_shape[1]*shape,inp_shape[2]*shape]),",
            "# align_corners=True)",
            "",
            "inp_shape = x.get_shape().as_list()"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 4803,
        "neg_line": [
            "-# tf.pack([inp_shape[1]*shape,inp_shape[2]*shape]),"
        ],
        "pos_line": [
            "+# tf.stack([inp_shape[1]*shape,inp_shape[2]*shape]),"
        ],
        "core_change": "-# tf.pack([inp_shape[1]*shape,inp_shape[2]*shape]), +# tf.stack([inp_shape[1]*shape,inp_shape[2]*shape]),",
        "core_API": "shape"
    },
    {
        "commit_hash": "cf5f31721ef489bb82bf4a7bb2d493713d48447d",
        "index": "bfe63acb..5c216be8 100644",
        "commit_message": "Sync for 4.0b4 release (#950)\n\n* sync for 4.0b4 release\n\n* fix extra space character in build.sh and add a simple prediction test for smoke testing on older macs\n",
        "file": "coremltools.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def run_compare_tf(",
            "graph, feed_dict, output_nodes, frontend, backend",
            ")",
            "else:",
            "-        with tf.Session(graph=graph) as sess:",
            "-            sess.run(tf.global_variables_initializer())",
            "-            tf_outputs = sess.run(output_nodes, feed_dict=feed_dict)",
            "+        if not tf_outputs:",
            "+            with tf.Session(graph=graph) as sess:",
            "+                sess.run(tf.global_variables_initializer())",
            "+                tf_outputs = sess.run(output_nodes, feed_dict=feed_dict)",
            "expected_outputs = {name: val for name, val in zip(output_names, tf_outputs)}",
            "",
            "for k,v in input_key_values.items():"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=block), node=('if_statement', None), position=0, insert_id=1913131)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=1913132)",
            "Insert(target_node=IN(type=if_statement), node=('not_operator', None), position=1, insert_id=1913133)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=1913134)",
            "Move(target_node=IN(type=if_statement), node=ASTNode(type=block), position=3)",
            "Insert(target_node=IN(type=not_operator), node=('not', 'not'), position=0, insert_id=1913135)",
            "Insert(target_node=IN(type=not_operator), node=('identifier', 'tf_outputs'), position=1, insert_id=1913136)"
        ],
        "plus_line": 4,
        "minus_line": 3,
        "AST_diff_line": 7,
        "number": 4821,
        "neg_line": [
            "-with tf.Session(graph=graph) as sess:",
            "-sess.run(tf.global_variables_initializer())",
            "-tf_outputs = sess.run(output_nodes, feed_dict=feed_dict)"
        ],
        "pos_line": [
            "+if not tf_outputs:",
            "+with tf.Session(graph=graph) as sess:",
            "+sess.run(tf.global_variables_initializer())",
            "+tf_outputs = sess.run(output_nodes, feed_dict=feed_dict)"
        ],
        "core_change": "-with tf.Session(graph=graph) as sess: -sess.run(tf.global_variables_initializer()) -tf_outputs = sess.run(output_nodes, feed_dict=feed_dict) +if not tf_outputs: +with tf.Session(graph=graph) as sess: +sess.run(tf.global_variables_initializer()) +tf_outputs = sess.run(output_nodes, feed_dict=feed_dict)",
        "core_API": "Session"
    },
    {
        "commit_hash": "11d86d3de420210073538c8d7e1e44f9492d02bc",
        "index": "69fb09b99..8303fef2d 100755",
        "commit_message": "[Deepspeed Wav2vec2] integration (#11638)\n\n* wip\n\n* wip - but working with https://github.com/microsoft/DeepSpeed/pull/1044\n\n* cleanup\n\n* workaround\n\n* working 5/8 modes\n\n* solve fp32 distributed zero3\n\n* style\n\n* sync\n\n* sync\n\n* rework\n\n* deprecation\n\n* cleanup\n\n* https://github.com/microsoft/DeepSpeed/pull/1044 pr was merged\n\n* clean up\n\n* add a guide\n\n* more prose\n\n* more prose\n\n* fix\n\n* more prose\n\n* sub_group_size was too big\n\n* Apply suggestions from code review\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n\n* refactor\n\n* bug fix\n\n* make the true check explicit\n\n* new deepspeed release\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Trainer:",
            "\"\"\"",
            "for k, v in inputs.items():",
            "if isinstance(v, torch.Tensor):",
            "-                inputs[k] = v.to(self.args.device)",
            "+                kwargs = dict(device=self.args.device)",
            "+                if self.deepspeed and inputs[k].dtype != torch.int64:",
            "+                    # NLP models inputs are int64 and those get adjusted to the right dtype of the",
            "+                    # embedding. Other models such as wav2vec2's inputs are already float and thus",
            "+                    # may need special handling to match the dtypes of the model",
            "+                    kwargs.update(dict(dtype=self.args.hf_deepspeed_config.dtype()))",
            "+",
            "+                inputs[k] = v.to(**kwargs)",
            "",
            "if self.args.past_index >= 0 and self._past is not None:",
            "inputs[\"mems\"] = self._past"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 7,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 4830,
        "neg_line": [
            "-inputs[k] = v.to(self.args.device)"
        ],
        "pos_line": [
            "+kwargs = dict(device=self.args.device)",
            "+if self.deepspeed and inputs[k].dtype != torch.int64:",
            "+# NLP models inputs are int64 and those get adjusted to the right dtype of the",
            "+# embedding. Other models such as wav2vec2's inputs are already float and thus",
            "+# may need special handling to match the dtypes of the model",
            "+kwargs.update(dict(dtype=self.args.hf_deepspeed_config.dtype()))",
            "+",
            "+inputs[k] = v.to(**kwargs)"
        ],
        "core_change": "-inputs[k] = v.to(self.args.device) +kwargs = dict(device=self.args.device) +if self.deepspeed and inputs[k].dtype != torch.int64: +# NLP models inputs are int64 and those get adjusted to the right dtype of the +# embedding. Other models such as wav2vec2's inputs are already float and thus +# may need special handling to match the dtypes of the model +kwargs.update(dict(dtype=self.args.hf_deepspeed_config.dtype())) + +inputs[k] = v.to(**kwargs)",
        "core_API": "items"
    },
    {
        "commit_hash": "b9e62bf053794c4d82cebf8fb4080bc0890daf40",
        "index": "a9768f2c9..9da8d42de 100644",
        "commit_message": "Fix RNNP\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class RNNP(torch.nn.Module):",
            "xs_pack = pack_padded_sequence(xs_pad, ilens, batch_first=True)",
            "rnn = getattr(self, (\"birnn\" if self.bidir else \"rnn\") + str(layer))",
            "rnn.flatten_parameters()",
            "-            if prev_state is not None and self.nbrnn.bidirectional:",
            "+            if prev_state is not None and rnn.bidirectional:",
            "prev_state = zero_backward_rnn_state(prev_state)",
            "ys, states = rnn(xs_pack, hx=prev_state)",
            "elayer_states.append(states)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=self), value='rnn')",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=self), position=0)",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Delete(target_node=ASTNode(type=identifier, text=nbrnn))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 4831,
        "neg_line": [
            "-if prev_state is not None and self.nbrnn.bidirectional:"
        ],
        "pos_line": [
            "+if prev_state is not None and rnn.bidirectional:"
        ],
        "core_change": "-if prev_state is not None and self.nbrnn.bidirectional: +if prev_state is not None and rnn.bidirectional:",
        "core_API": "flatten_parameters"
    },
    {
        "commit_hash": "62e3d1dbdc88af9ed8afb37f7e8ecc011f5486a3",
        "index": "81f75b03..d6d5541b 100644",
        "commit_message": "Scope TokenIndexer output by indexer name (#3597)\n\n* Indexer tests are now passing, at least\n\n* Fixed some masking issues, and padding keys for one config file\n\n* TextFieldEmbedder tests pass\n\n* Fixing fixtures, some more tests passing\n\n* Fixed weird ordering bug\n\n* All TokenEmbedder tests passing?\n\n* Update fixtures\n\n* Fix more hard-coded references\n\n* fix field tests\n\n* fix dataset reader tests\n\n* Fix iterator tests\n\n* More tests passing\n\n* fix hotflip and some other tests\n\n* more tests\n\n* more test fixes\n\n* more tests\n\n* most tests passing; I think the remaining ones are spacy model changes\n\n* hard-code POS tag test\n\n* last test, I think\n\n* black\n\n* updated black\n\n* flake8\n\n* mypy\n\n* black again\n\n* fix training configs\n\n* remove reference to embedder_to_indexer_map\n\n* Other fixes from PR comments\n\n* fix breakage from incorrect merge during rebase\n\n* flake, some docstring formatting\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestAutoRegressiveSeqDecoder(AllenNlpTestCase):",
            "",
            "encoded_state = torch.randn(batch_size, time_steps, decoder_inout_dim)",
            "source_mask = torch.ones(batch_size, time_steps).long()",
            "-        target_tokens = {\"tokens\": torch.ones(batch_size, time_steps).long()}",
            "+        target_tokens = {\"tokens\": {\"tokens\": torch.ones(batch_size, time_steps).long()}}",
            "source_mask[0, 1:] = 0",
            "encoder_out = {\"source_mask\": source_mask, \"encoder_outputs\": encoded_state}"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=pair), node=('string', '\"tokens\"'), position=0, insert_id=22723)",
            "Insert(target_node=ASTNode(type=pair), node=(':', ':'), position=1, insert_id=22724)",
            "Insert(target_node=ASTNode(type=pair), node=('dictionary', None), position=2, insert_id=22725)",
            "Insert(target_node=IN(type=dictionary), node=('{', '{'), position=0, insert_id=22726)",
            "Move(target_node=IN(type=dictionary), node=ASTNode(type=pair), position=1)",
            "Insert(target_node=IN(type=dictionary), node=('}', '}'), position=2, insert_id=22727)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 4837,
        "neg_line": [
            "-target_tokens = {\"tokens\": torch.ones(batch_size, time_steps).long()}"
        ],
        "pos_line": [
            "+target_tokens = {\"tokens\": {\"tokens\": torch.ones(batch_size, time_steps).long()}}"
        ],
        "core_change": "-target_tokens = {\"tokens\": torch.ones(batch_size, time_steps).long()} +target_tokens = {\"tokens\": {\"tokens\": torch.ones(batch_size, time_steps).long()}}",
        "core_API": "randn"
    },
    {
        "commit_hash": "4c0369fbf7a78db3dfce69cf8f2aa8e1103c002d",
        "index": "9009dbb57..d35342f28 100644",
        "commit_message": "fix flake8\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_encoder_cache(normalize_before):",
            "dropout_rate=0.0,",
            "input_layer=\"embed\")",
            "elayer = encoder.encoders[0]",
            "-    memory = torch.randn(2, 5, adim)",
            "-",
            "x = torch.randn(2, 5, adim)",
            "mask = subsequent_mask(x.shape[1]).unsqueeze(0)",
            "prev_mask = mask[:, :-1, :-1]"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=memory))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=randn))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=integer, text=2))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=integer, text=5))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=adim))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 4838,
        "neg_line": [
            "-memory = torch.randn(2, 5, adim)",
            "-"
        ],
        "pos_line": [],
        "core_change": "-memory = torch.randn(2, 5, adim) -",
        "core_API": "randn"
    },
    {
        "commit_hash": "44d2847610944f56a06b7cfa54faadb66e130a83",
        "index": "6ebdc660..5f231809 100644",
        "commit_message": "Metrics in distributed setting (#4525)\n\n* initial commit to ensure that metrics work correctly in distributed setting\n\n* updating global_distributed_metric to take metric object\n\n* adding distributed f1 score\n\n* adding distributed attachment scores\n\n* bug fix\n\n* adding distributed boolean accuracy\n\n* adding distributed entropy\n\n* adding distributed evalb\n\n* adding distributed mean_absolute_error\n\n* adding distributed sequence accuracy\n\n* adding distributed unigram recall\n\n* making models compatible with distributed metrics\n\n* adding distributed auc\n\n* adding distributed bleu\n\n* adding missing argument\n\n* initial commit to ensure that metrics work correctly in distributed setting\n\n* updating global_distributed_metric to take metric object\n\n* adding distributed f1 score\n\n* adding distributed attachment scores\n\n* bug fix\n\n* adding distributed boolean accuracy\n\n* adding distributed entropy\n\n* adding distributed evalb\n\n* adding distributed mean_absolute_error\n\n* adding distributed sequence accuracy\n\n* adding distributed unigram recall\n\n* making models compatible with distributed metrics\n\n* adding distributed auc\n\n* adding distributed bleu\n\n* adding missing argument\n\n* changing start method\n\n* removing unnecessary argument\n\n* adding remaining metrics, removing extra argument\n\n* allowing float values\n\n* bug fix\n\n* more bug fixes\n\n* changing average to return float\n\n* adding timeout for distributed test\n\n* testing unequal batches\n\n* adding distributed auc\n\n* adding distributed spearman correlation\n\n* adding distributed covariance and pearson correlation\n\n* changing distributed test to function, misc changes\n\n* checking batch lengths explicitly to raise errors\n\nCo-authored-by: Dirk Groeneveld <dirkg@allenai.org>\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SequenceAccuracy(Metric):",
            "accuracy = self.correct_count / self.total_count",
            "else:",
            "accuracy = 0",
            "-",
            "if reset:",
            "self.reset()",
            "-        return accuracy",
            "+        return {\"accuracy\": accuracy}",
            "",
            "@overrides",
            "def reset(self):"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=return_statement), node=('dictionary', None), position=1, insert_id=12677)",
            "Insert(target_node=IN(type=dictionary), node=('{', '{'), position=0, insert_id=12678)",
            "Insert(target_node=IN(type=dictionary), node=('pair', None), position=1, insert_id=12679)",
            "Insert(target_node=IN(type=dictionary), node=('}', '}'), position=2, insert_id=12680)",
            "Insert(target_node=IN(type=pair), node=('string', '\"accuracy\"'), position=0, insert_id=12681)",
            "Insert(target_node=IN(type=pair), node=(':', ':'), position=1, insert_id=12682)",
            "Move(target_node=IN(type=pair), node=ASTNode(type=identifier, text=accuracy), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 4839,
        "neg_line": [
            "-",
            "-return accuracy"
        ],
        "pos_line": [
            "+return {\"accuracy\": accuracy}"
        ],
        "core_change": "- -return accuracy +return {\"accuracy\": accuracy}",
        "core_API": "reset"
    },
    {
        "commit_hash": "a5b98f025bab56e28310108e355f226b514edad0",
        "index": "5de8cb548..48e213125 100644",
        "commit_message": "Fix docstring issues (#2072)\n\n* Fix docstring module name of filesystems\n\n* Fix docstring missing blank after comma\n\n* Fix docstring missing trailing dot\n\n* Fix docstring cross-referencing in Returns\n\n* Fix docstring cross-referencing in description\n\n* Fix docstring non-rendered args descriptions\n\n* Fix docstring cross-reference content prefix\n\n* Fix docstring line length\n\n* Fix docstring of SplitGenerator\n\n* Fix docstring of Split\n\n* Fix docstring document cross-reference in Split\n\n* Fix docstring of DatasetDict.shuffle\n\n* Fix docstring of DatasetBuilder\n\n* Add docstring attributes of DatasetInfo\n\n* Change docstring default rendering of Attributes in conf.py\n\n* Fix docstring of list_datasets\n\n* Fix docstring of load_dataset\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "def is_remote_filesystem(fs: fsspec.spec.AbstractFileSystem) -> bool:",
            "Validates if filesystem has remote protocol.",
            "",
            "Args:",
            "-        fs (``fsspec.spec.AbstractFileSystem``): An abstract super-class for pythonic file-systems, e.g. :code:`fsspec.filesystem(\\'file\\')` or :class:`datasets.filesystem.S3FileSystem`",
            "+        fs (``fsspec.spec.AbstractFileSystem``): An abstract super-class for pythonic file-systems, e.g. :code:`fsspec.filesystem(\\'file\\')` or :class:`datasets.filesystems.S3FileSystem`",
            "\"\"\"",
            "if fs is not None and fs.protocol != \"file\":",
            "return True"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=`datasets.filesystem.S3FileSystem`), value='`datasets.filesystems.S3FileSystem`')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 4845,
        "neg_line": [
            "-fs (``fsspec.spec.AbstractFileSystem``): An abstract super-class for pythonic file-systems, e.g. :code:`fsspec.filesystem(\\'file\\')` or :class:`datasets.filesystem.S3FileSystem`"
        ],
        "pos_line": [
            "+fs (``fsspec.spec.AbstractFileSystem``): An abstract super-class for pythonic file-systems, e.g. :code:`fsspec.filesystem(\\'file\\')` or :class:`datasets.filesystems.S3FileSystem`"
        ],
        "core_change": "-fs (``fsspec.spec.AbstractFileSystem``): An abstract super-class for pythonic file-systems, e.g. :code:`fsspec.filesystem(\\'file\\')` or :class:`datasets.filesystem.S3FileSystem` +fs (``fsspec.spec.AbstractFileSystem``): An abstract super-class for pythonic file-systems, e.g. :code:`fsspec.filesystem(\\'file\\')` or :class:`datasets.filesystems.S3FileSystem`",
        "core_API": "filesystem"
    },
    {
        "commit_hash": "680f1337c3c894b681ebbd24c18122a61e33715b",
        "index": "42d69ba44..8edcb4e69 100644",
        "commit_message": "MBartForConditionalGeneration (#6441)\n\n* add MBartForConditionalGeneration\n\n* style\n\n* rebase and fixes\n\n* add mbart test in TEST_FILES_WITH_NO_COMMON_TESTS\n\n* fix docs\n\n* don't ignore mbart\n\n* doc\n\n* fix mbart fairseq link\n\n* put mbart before bart\n\n* apply doc suggestions\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "models",
        "change": [
            "class BartConfig(PretrainedConfig):",
            "if self.normalize_before or self.add_final_layer_norm or self.scale_embedding:",
            "logger.info(\"This configuration is a mixture of MBART and BART settings\")",
            "return False",
            "-",
            "-",
            "-class MBartConfig(BartConfig):",
            "-    model_type = \"mbart\"",
            "-    \"\"\"See real config values at https://s3.amazonaws.com/models.huggingface.co/bert/facebook/mbart-large-en-ro/config.json.\"\"\""
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=class, text=class))",
            "Delete(target_node=ASTNode(type=identifier, text=MBartConfig))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=BartConfig))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=identifier, text=model_type))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=string, text=\"mbart\"))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=string, text=\"\"\"See real config values at https://s3.amazonaws.com/models.huggingface.co/bert/facebook/mbart-large-en-ro/config.json.\"\"\"))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=block))",
            "Delete(target_node=ASTNode(type=class_definition))"
        ],
        "plus_line": 0,
        "minus_line": 2,
        "AST_diff_line": 16,
        "number": 4848,
        "neg_line": [
            "-",
            "-",
            "-class MBartConfig(BartConfig):",
            "-model_type = \"mbart\"",
            "-\"\"\"See real config values at https://s3.amazonaws.com/models.huggingface.co/bert/facebook/mbart-large-en-ro/config.json.\"\"\""
        ],
        "pos_line": [],
        "core_change": "- - -class MBartConfig(BartConfig): -model_type = \"mbart\" -\"\"\"See real config values at https://s3.amazonaws.com/models.huggingface.co/bert/facebook/mbart-large-en-ro/config.json.\"\"\"",
        "core_API": "info"
    },
    {
        "commit_hash": "455c6390938a5c737fa63e78396cedae41e4e87e",
        "index": "d0f589ed1..b055bd99e 100644",
        "commit_message": "CDN urls (#4030)\n\n* [file_utils] use_cdn + documentation\n\n* Move to cdn. urls for weights\n\n* [urls] Hotfix for bert-base-japanese\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "models",
        "change": [
            "from .tokenization_utils import BatchEncoding",
            "logger = logging.getLogger(__name__)",
            "",
            "TF_TRANSFO_XL_PRETRAINED_MODEL_ARCHIVE_MAP = {",
            "-    \"transfo-xl-wt103\": \"https://s3.amazonaws.com/models.huggingface.co/bert/transfo-xl-wt103-tf_model.h5\",",
            "+    \"transfo-xl-wt103\": \"https://cdn.huggingface.co/transfo-xl-wt103-tf_model.h5\",",
            "}"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"https://s3.amazonaws.com/models.huggingface.co/bert/transfo-xl-wt103-tf_model.h5\"), value='\"https://cdn.huggingface.co/transfo-xl-wt103-tf_model.h5\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 4851,
        "neg_line": [
            "-\"transfo-xl-wt103\": \"https://s3.amazonaws.com/models.huggingface.co/bert/transfo-xl-wt103-tf_model.h5\","
        ],
        "pos_line": [
            "+\"transfo-xl-wt103\": \"https://cdn.huggingface.co/transfo-xl-wt103-tf_model.h5\","
        ],
        "core_change": "-\"transfo-xl-wt103\": \"https://s3.amazonaws.com/models.huggingface.co/bert/transfo-xl-wt103-tf_model.h5\", +\"transfo-xl-wt103\": \"https://cdn.huggingface.co/transfo-xl-wt103-tf_model.h5\",",
        "core_API": "getLogger"
    },
    {
        "commit_hash": "1d4823c0ec446e93d00df8ca654db4b45b63b3d4",
        "index": "7f13639cda..365ef17ad7 100644",
        "commit_message": "[RLlib] Add testing framework_iterator. (#7852)\n\n* Add testing framework_iterator.\n\n* LINT.\n\n* WIP.\n\n* Fix and LINT.\n\n* LINT fix.\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class QLoss:",
            "# priority is robust and insensitive to `prioritized_replay_alpha`",
            "self.td_error = tf.nn.softmax_cross_entropy_with_logits(",
            "labels=m, logits=q_logits_t_selected)",
            "-            self.loss = tf.reduce_mean(self.td_error * importance_weights)",
            "+            self.loss = tf.reduce_mean(",
            "+                self.td_error * tf.cast(importance_weights, tf.float32))",
            "self.stats = {",
            "# TODO: better Q stats for dist dqn",
            "\"mean_td_error\": tf.reduce_mean(self.td_error),"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=binary_operator), node=('call', None), position=2, insert_id=2146559)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2146560)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2146561)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2146562)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2146563)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cast'), position=2, insert_id=2146564)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2146565)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=importance_weights), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=2146566)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=3, insert_id=2146567)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=2146568)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2146569)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2146570)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'float32'), position=2, insert_id=2146571)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 14,
        "number": 4853,
        "neg_line": [
            "-self.loss = tf.reduce_mean(self.td_error * importance_weights)"
        ],
        "pos_line": [
            "+self.loss = tf.reduce_mean(",
            "+self.td_error * tf.cast(importance_weights, tf.float32))"
        ],
        "core_change": "-self.loss = tf.reduce_mean(self.td_error * importance_weights) +self.loss = tf.reduce_mean( +self.td_error * tf.cast(importance_weights, tf.float32))",
        "core_API": "softmax_cross_entropy_with_logits"
    },
    {
        "commit_hash": "8cecc66a74a66451279f4f495b7a9451eda95253",
        "index": "ade8ace0..ad3ab69f 100644",
        "commit_message": "Fix the bug that torch version less than 1.12 throws TypeError (#1671)\n\n\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "try:",
            "if _torch_available:",
            "import torch",
            "",
            "-        if torch.__version__ < version.Version(\"1.12\"):",
            "+        if version.Version(torch.__version__) < version.Version(\"1.12\"):",
            "raise ValueError(\"PyTorch should be >= 1.12\")",
            "logger.debug(f\"Successfully imported xformers version {_xformers_version}\")",
            "except importlib_metadata.PackageNotFoundError:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=comparison_operator), node=('call', None), position=0, insert_id=93730)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=93731)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=93732)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'version'), position=0, insert_id=93733)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=93734)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Version'), position=2, insert_id=93735)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=93736)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=attribute), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=93737)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 4861,
        "neg_line": [
            "-if torch.__version__ < version.Version(\"1.12\"):"
        ],
        "pos_line": [
            "+if version.Version(torch.__version__) < version.Version(\"1.12\"):"
        ],
        "core_change": "-if torch.__version__ < version.Version(\"1.12\"): +if version.Version(torch.__version__) < version.Version(\"1.12\"):",
        "core_API": "Version"
    },
    {
        "commit_hash": "d86c7fb53933f5b7558dd1ce202836d164becca5",
        "index": "038cd6250..f57913c29 100644",
        "commit_message": "Refac module factory + avoid etag requests for hub datasets (#2986)\n\n* refac module factory + avoid etag requests for hub datasets\n\n* fix tests\n\n* typing\n\n* fixes\n\n* prepare timeout\n\n* fix offline simulator with hugginggace_hub\n\n* add module factory tests (1/N)\n\n* add module factory test (2/N)\n\n* add data files tests (1/N)\n\n* add data fiels tests (2/N)\n\n* add data files tests (3/N)\n\n* style\n\n* docstrings\n\n* don't update counts when running tests\n\n* nump huggingface_hub\n\n* add timeouts for offline mode\n\n* minor\n\n* minor bis\n\n* install ruamel-yaml properly in the CI for windows\n\n* fix windows test\n\n* style\n\n* fix comet intensive calls patcher\n\n* warning message when loading from the master branch\n\n* style\n\n* albert's comments\n\n* remove unnecessary check\n\n* don't use master if HF_SCRIPTS_VERSION is specified\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class LocalMetricTest(parameterized.TestCase):",
            "",
            "def test_load_metric(self, metric_name):",
            "doctest.ELLIPSIS_MARKER = \"[...]\"",
            "-        metric_module = importlib.import_module(datasets.load.prepare_module(os.path.join(\"metrics\", metric_name))[0])",
            "+        metric_module = importlib.import_module(",
            "+            datasets.load.prepare_module(os.path.join(\"metrics\", metric_name), dataset=False)[0]",
            "+        )",
            "metric = datasets.load.import_main_class(metric_module.__name__, dataset=False)",
            "# check parameters",
            "parameters = inspect.signature(metric._compute).parameters"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1782641)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1782642)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dataset'), position=0, insert_id=1782643)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1782644)",
            "Insert(target_node=IN(type=keyword_argument), node=('false', 'False'), position=2, insert_id=1782645)"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 4862,
        "neg_line": [
            "-metric_module = importlib.import_module(datasets.load.prepare_module(os.path.join(\"metrics\", metric_name))[0])"
        ],
        "pos_line": [
            "+metric_module = importlib.import_module(",
            "+datasets.load.prepare_module(os.path.join(\"metrics\", metric_name), dataset=False)[0]",
            "+)"
        ],
        "core_change": "-metric_module = importlib.import_module(datasets.load.prepare_module(os.path.join(\"metrics\", metric_name))[0]) +metric_module = importlib.import_module( +datasets.load.prepare_module(os.path.join(\"metrics\", metric_name), dataset=False)[0] +)",
        "core_API": "import_module"
    },
    {
        "commit_hash": "399f084ff03a0788a2b145b62fa68d54ac841cb4",
        "index": "16edf754..9c82b45c 100644",
        "commit_message": "[Feat] Add elastic transform 2d (#853)\n\n* first\n\n* Update elastic_transform.py\n\n* updated elastic transform\n\n* now gradients flow through the gaussian function\n\n* cleaner elastic transform\n\n* minor fixes in augmentation documentation\n\n* implement functional elastic transform\n\n* set device and dtype for gaussian kernel\n\nCo-authored-by: IssamLaradji <issam.laradji@gmail.com>\nCo-authored-by: Issam H. Laradji <IssamLaradji@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestSimpleKD:",
            "def test_jit(self, device, dtype):",
            "batch_size, channels, ps = 1, 1, 19",
            "patches = torch.rand(batch_size, channels, ps, ps).to(device)",
            "-        model =  SimpleKD(patch_size=ps, kernel_type='polar', whitening='lw').to(patches.device, patches.dtype).eval()  # noqa",
            "-        model_jit = torch.jit.script( SimpleKD(patch_size=ps, kernel_type='polar', whitening='lw').to(patches.device, patches.dtype).eval())  # noqa",
            "+        model = SimpleKD(patch_size=ps, kernel_type='polar', whitening='lw').to(patches.device, patches.dtype).eval()  # noqa",
            "+        model_jit = torch.jit.script(SimpleKD(patch_size=ps, kernel_type='polar', whitening='lw').to(patches.device, patches.dtype).eval())  # noqa",
            "assert_allclose(model(patches), model_jit(patches))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 4864,
        "neg_line": [
            "-model =  SimpleKD(patch_size=ps, kernel_type='polar', whitening='lw').to(patches.device, patches.dtype).eval()  # noqa",
            "-model_jit = torch.jit.script( SimpleKD(patch_size=ps, kernel_type='polar', whitening='lw').to(patches.device, patches.dtype).eval())  # noqa"
        ],
        "pos_line": [
            "+model = SimpleKD(patch_size=ps, kernel_type='polar', whitening='lw').to(patches.device, patches.dtype).eval()  # noqa",
            "+model_jit = torch.jit.script(SimpleKD(patch_size=ps, kernel_type='polar', whitening='lw').to(patches.device, patches.dtype).eval())  # noqa"
        ],
        "core_change": "-model =  SimpleKD(patch_size=ps, kernel_type='polar', whitening='lw').to(patches.device, patches.dtype).eval()  # noqa -model_jit = torch.jit.script( SimpleKD(patch_size=ps, kernel_type='polar', whitening='lw').to(patches.device, patches.dtype).eval())  # noqa +model = SimpleKD(patch_size=ps, kernel_type='polar', whitening='lw').to(patches.device, patches.dtype).eval()  # noqa +model_jit = torch.jit.script(SimpleKD(patch_size=ps, kernel_type='polar', whitening='lw').to(patches.device, patches.dtype).eval())  # noqa",
        "core_API": "rand"
    },
    {
        "commit_hash": "94b57bf796022ad87f1ba8655e19575d98ff4ce6",
        "index": "70b3feff6..6f4e78908 100644",
        "commit_message": "[TF 2.2 compat]  use tf.VariableAggregation.ONLY_FIRST_REPLICA (#4283)\n\n* Fix the issue to properly run the accumulator with TF 2.2\n\n* Apply style\n\n* Fix training_args_tf for TF 2.2\n\n* Fix the TF training args when only one GPU is available\n\n* Remove the fixed version of TF in setup.py\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class GradientAccumulator(object):",
            "self._gradients.extend(",
            "[",
            "tf.Variable(",
            "-                        tf.zeros_like(gradient), trainable=False, synchronization=tf.VariableSynchronization.ON_READ,",
            "+                        tf.zeros_like(gradient),",
            "+                        trainable=False,",
            "+                        synchronization=tf.VariableSynchronization.ON_READ,",
            "+                        aggregation=tf.VariableAggregation.ONLY_FIRST_REPLICA,",
            ")",
            "for gradient in gradients",
            "]"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=7, insert_id=2382221)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=8, insert_id=2382222)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'aggregation'), position=0, insert_id=2382223)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2382224)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=2382225)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2382226)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2382227)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'ONLY_FIRST_REPLICA'), position=2, insert_id=2382228)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2382229)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2382230)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'VariableAggregation'), position=2, insert_id=2382231)"
        ],
        "plus_line": 4,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 4868,
        "neg_line": [
            "-tf.zeros_like(gradient), trainable=False, synchronization=tf.VariableSynchronization.ON_READ,"
        ],
        "pos_line": [
            "+tf.zeros_like(gradient),",
            "+trainable=False,",
            "+synchronization=tf.VariableSynchronization.ON_READ,",
            "+aggregation=tf.VariableAggregation.ONLY_FIRST_REPLICA,"
        ],
        "core_change": "-tf.zeros_like(gradient), trainable=False, synchronization=tf.VariableSynchronization.ON_READ, +tf.zeros_like(gradient), +trainable=False, +synchronization=tf.VariableSynchronization.ON_READ, +aggregation=tf.VariableAggregation.ONLY_FIRST_REPLICA,",
        "core_API": "extend"
    },
    {
        "commit_hash": "cced33542db5896afc0e8b3e3c9263dba44deb4f",
        "index": "15e9962e4..fe8b0b836 100644",
        "commit_message": "Disable non blocking to device with MPS (#14368)\n\n* disable non-blocking for mps due to race condition bug\n\n* fixed typo\n\n* fixed: unknown mps device for non arm systems\n\n* Removed unrobust test case\n\n* moved _MPS_DEVICES such that we used in apply_func\n\n* Resolve circular dependencies\n\n* Comment rewording\n\n* changed torchElasticEnvironment to a global import\n\n* simplified if statement to blocking device type\n\n* Added change to CHANGELOG\n\n* Update src/pytorch_lightning/utilities/apply_func.py\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fixed mypy not detecting casting of device\n\n* Moved check into if statement to mainain original behavior\n\nCo-authored-by: Carlos Mochol <carlossmocholi@gmail.com>\nCo-authored-by: Justus Schock <12886177+justusschock@users.noreply.github.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def move_data_to_device(batch: Any, device: Union[str, torch.device]) -> Any:",
            "",
            "kwargs = {}",
            "# Don't issue non-blocking transfers to CPU",
            "-        if isinstance(data, Tensor) and device not in _CPU_DEVICES:",
            "+        # Same with MPS due to a race condition bug: https://github.com/pytorch/pytorch/issues/83015",
            "+        if isinstance(data, Tensor) and isinstance(device, torch.device) and device.type not in _BLOCKING_DEVICE_TYPES:",
            "kwargs[\"non_blocking\"] = True",
            "data_output = data.to(device, **kwargs)",
            "if data_output is not None:"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=boolean_operator), node=('boolean_operator', None), position=0, insert_id=501808)",
            "Insert(target_node=ASTNode(type=boolean_operator), node=('and', 'and'), position=1, insert_id=501809)",
            "Move(target_node=IN(type=boolean_operator), node=ASTNode(type=call), position=0)",
            "Move(target_node=IN(type=boolean_operator), node=ASTNode(type=and, text=and), position=1)",
            "Insert(target_node=IN(type=boolean_operator), node=('call', None), position=2, insert_id=501810)",
            "Insert(target_node=ASTNode(type=comparison_operator), node=('attribute', None), position=0, insert_id=501811)",
            "Update(target_node=ASTNode(type=identifier, text=_CPU_DEVICES), value='_BLOCKING_DEVICE_TYPES')",
            "Insert(target_node=IN(type=call), node=('identifier', 'isinstance'), position=0, insert_id=501812)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=501813)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=device), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=501814)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'type'), position=2, insert_id=501815)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=501816)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'device'), position=1, insert_id=501817)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=501818)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=3, insert_id=501819)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=501820)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=501821)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=501822)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=501823)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 20,
        "number": 4871,
        "neg_line": [
            "-if isinstance(data, Tensor) and device not in _CPU_DEVICES:"
        ],
        "pos_line": [
            "+# Same with MPS due to a race condition bug: https://github.com/pytorch/pytorch/issues/83015",
            "+if isinstance(data, Tensor) and isinstance(device, torch.device) and device.type not in _BLOCKING_DEVICE_TYPES:"
        ],
        "core_change": "-if isinstance(data, Tensor) and device not in _CPU_DEVICES: +# Same with MPS due to a race condition bug: https://github.com/pytorch/pytorch/issues/83015 +if isinstance(data, Tensor) and isinstance(device, torch.device) and device.type not in _BLOCKING_DEVICE_TYPES:",
        "core_API": "to"
    },
    {
        "commit_hash": "856d4e5733451c7fe9b12f183b384e986699b1f8",
        "index": "2a45f43..d958a89 100644",
        "commit_message": "Fix `select_device()` for Multi-GPU (#6434)\n\n* Fix `select_device()` for Multi-GPU\n\nPossible fix for https://github.com/ultralytics/yolov5/issues/6431\n\n* Update torch_utils.py\n\n* Update torch_utils.py\n\n* Update torch_utils.py\n\n* Update torch_utils.py\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n",
        "file": "yolov5.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "cuda",
        "change": [
            "def select_device(device='', batch_size=0, newline=True):",
            "if cpu:",
            "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # force torch.cuda.is_available() = False",
            "elif device:  # non-cpu device requested",
            "-        nd = torch.cuda.device_count()  # number of CUDA devices",
            "-        assert torch.cuda.is_available(), 'CUDA is not available, use `--device cpu` or do not pass a --device'",
            "+        nd = device_count()  # number of CUDA devices",
            "assert nd > int(max(device.split(','))), f'Invalid `--device {device}` request, valid devices are 0 - {nd - 1}'",
            "-        os.environ['CUDA_VISIBLE_DEVICES'] = device  # set environment variable (must be after asserts)",
            "+        os.environ['CUDA_VISIBLE_DEVICES'] = device  # set environment variable - must be before assert is_available()",
            "+        assert torch.cuda.is_available(), 'CUDA is not available, use `--device cpu` or do not pass a --device'",
            "",
            "cuda = not cpu and torch.cuda.is_available()",
            "if cuda:"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=assert_statement), node=ASTNode(type=module), position=8)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=device_count), position=0)",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=cuda))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 8,
        "number": 4874,
        "neg_line": [
            "-nd = torch.cuda.device_count()  # number of CUDA devices",
            "-assert torch.cuda.is_available(), 'CUDA is not available, use `--device cpu` or do not pass a --device'",
            "-os.environ['CUDA_VISIBLE_DEVICES'] = device  # set environment variable (must be after asserts)"
        ],
        "pos_line": [
            "+nd = device_count()  # number of CUDA devices",
            "+os.environ['CUDA_VISIBLE_DEVICES'] = device  # set environment variable - must be before assert is_available()",
            "+assert torch.cuda.is_available(), 'CUDA is not available, use `--device cpu` or do not pass a --device'"
        ],
        "core_change": "-nd = torch.cuda.device_count()  # number of CUDA devices -assert torch.cuda.is_available(), 'CUDA is not available, use `--device cpu` or do not pass a --device' +nd = device_count()  # number of CUDA devices -os.environ['CUDA_VISIBLE_DEVICES'] = device  # set environment variable (must be after asserts) +os.environ['CUDA_VISIBLE_DEVICES'] = device  # set environment variable - must be before assert is_available() +assert torch.cuda.is_available(), 'CUDA is not available, use `--device cpu` or do not pass a --device'",
        "core_API": "is_available"
    },
    {
        "commit_hash": "b24ead87e1be6bce17e4ec5c953b6d028e4b3af7",
        "index": "add4410ca..065a3fef6 100644",
        "commit_message": "fix some typos in docs, comments, logging/errors (#11432)\n\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def symmetric_linear_quantization_params(num_bits, saturation_min, saturation_ma",
            "`saturation_max`.",
            "\"\"\"",
            "# in this part, we do not need any gradient computation,",
            "-    # in order to enfore this, we put torch.no_grad()",
            "+    # in order to enforce this, we put torch.no_grad()",
            "with torch.no_grad():",
            "n = 2 ** (num_bits - 1) - 1"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 4876,
        "neg_line": [
            "-# in order to enfore this, we put torch.no_grad()"
        ],
        "pos_line": [
            "+# in order to enforce this, we put torch.no_grad()"
        ],
        "core_change": "-# in order to enfore this, we put torch.no_grad() +# in order to enforce this, we put torch.no_grad()",
        "core_API": "no_grad"
    },
    {
        "commit_hash": "0d90e35f3b14bd242a64a7a2af744a258e7d0298",
        "index": "8ea7af42..2a8f3ef5 100644",
        "commit_message": "More unit test fixes\n\n",
        "file": "fairseq.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def main(args):",
            "print(args)",
            "",
            "use_cuda = torch.cuda.is_available() and not args.cpu",
            "-    if hasattr(torch, 'set_grad_enabled'):",
            "-        torch.set_grad_enabled(False)",
            "",
            "# Load dataset",
            "if args.replace_unk is None:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=if, text=if))",
            "Delete(target_node=ASTNode(type=identifier, text=hasattr))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=string, text='set_grad_enabled'))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=set_grad_enabled))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=false, text=False))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=block))",
            "Delete(target_node=ASTNode(type=if_statement))"
        ],
        "plus_line": 0,
        "minus_line": 2,
        "AST_diff_line": 22,
        "number": 4878,
        "neg_line": [
            "-if hasattr(torch, 'set_grad_enabled'):",
            "-torch.set_grad_enabled(False)"
        ],
        "pos_line": [],
        "core_change": "-if hasattr(torch, 'set_grad_enabled'): -torch.set_grad_enabled(False)",
        "core_API": "is_available"
    },
    {
        "commit_hash": "ed63b7ee9e68ba120893aaaaca0d4525a56ed484",
        "index": "6b6a2971..d6fae6a4 100644",
        "commit_message": "upgrade to pytorch 0.4.0 (#1126)\n\n* bump pytorch to 0.4 + fix sanitize\n\n* remove check for Variable in block_orthogonal\n\n* rename parameter split_size=\n\n* remove checks for Variable\n\n* fix more tests\n\n* fixes\n\n* get tests to pass\n\n* fix warnings\n\n* get rid of some of the Variables\n\n* more tests passing\n\n* more elimination of variables\n\n* finish removing all Variables\n\n* pylint and such\n\n* a few fixes\n\n* move torch.no_grad into model.forward_on_instances\n\n* more pytorch 0.4 changes\n\n* detach() -> data\n\n* pylint\n\n* fix bad tensor creation\n\n* fix types\n\n* remove print statement\n\n* more 0.4 goodness\n\n* factor out is_tensor\n\n* add no_grad to elmo command\n\n* cleanup\n\n* remove TODO\n\n* address PR feedback\n\n* replace all() with item()\n\n* more cleanup\n\n* further cleanup\n\n* remove Variable\n\n* really fix merge conflict\n\n* fix pylint\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestTimeDistributed(AllenNlpTestCase):",
            "def test_time_distributed_works_with_multiple_inputs(self):",
            "module = lambda x, y: x + y",
            "distributed = TimeDistributed(module)",
            "-        x_input = Variable(torch.LongTensor([[[1, 2], [3, 4]]]))",
            "-        y_input = Variable(torch.LongTensor([[[4, 2], [9, 1]]]))",
            "+        x_input = torch.LongTensor([[[1, 2], [3, 4]]])",
            "+        y_input = torch.LongTensor([[[4, 2], [9, 1]]])",
            "output = distributed(x_input, y_input)",
            "assert_almost_equal(output.data.numpy(), [[[5, 4], [12, 5]]])"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=call), position=2)",
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=call), position=2)",
            "Delete(target_node=ASTNode(type=identifier, text=Variable))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=identifier, text=Variable))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 12,
        "number": 4881,
        "neg_line": [
            "-x_input = Variable(torch.LongTensor([[[1, 2], [3, 4]]]))",
            "-y_input = Variable(torch.LongTensor([[[4, 2], [9, 1]]]))"
        ],
        "pos_line": [
            "+x_input = torch.LongTensor([[[1, 2], [3, 4]]])",
            "+y_input = torch.LongTensor([[[4, 2], [9, 1]]])"
        ],
        "core_change": "-x_input = Variable(torch.LongTensor([[[1, 2], [3, 4]]])) -y_input = Variable(torch.LongTensor([[[4, 2], [9, 1]]])) +x_input = torch.LongTensor([[[1, 2], [3, 4]]]) +y_input = torch.LongTensor([[[4, 2], [9, 1]]])",
        "core_API": "LongTensor"
    },
    {
        "commit_hash": "ca5819dec327be9e49412ce69909feea72f5d752",
        "index": "52bc74f..60083a7 100644",
        "commit_message": "revert padding bug fix for now\n\n",
        "file": "gpt-neo.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def test_pred_input(params, enc = None):",
            "bos = tf.constant(1, shape=[1, 1], dtype=tf.int64)",
            "src_seq = tf.random.uniform(shape=[1, length], minval=4, maxval=(params['n_vocab'] - 1), dtype=tf.int64)",
            "seq = tf.concat([bos, src_seq], axis=1)",
            "-    seq = tf.pad(seq, [[0, 0], [0, remaining]], constant_values=params['padding_id'])",
            "+    seq = tf.pad(seq, [[0, 0], [0, remaining]])",
            "dataset = tf.data.Dataset.from_tensors(seq)",
            "",
            "dataset = dataset.map(_dummy_labels)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=constant_values))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=params))",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=string, text='padding_id'))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 4882,
        "neg_line": [
            "-seq = tf.pad(seq, [[0, 0], [0, remaining]], constant_values=params['padding_id'])"
        ],
        "pos_line": [
            "+seq = tf.pad(seq, [[0, 0], [0, remaining]])"
        ],
        "core_change": "-seq = tf.pad(seq, [[0, 0], [0, remaining]], constant_values=params['padding_id']) +seq = tf.pad(seq, [[0, 0], [0, remaining]])",
        "core_API": "constant"
    },
    {
        "commit_hash": "ea8d58ea9186d3298c091de177fe9332199ac397",
        "index": "563f4f20..2eab5f3e 100644",
        "commit_message": "[MidBlock] Fix mid block (#78)\n\n* upload files\n\n* finish\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class UNetModel(ModelMixin, ConfigMixin):",
            "hs.append(self.down[i_level].downsample(hs[-1]))",
            "",
            "# middle",
            "-        h = self.mid(hs[-1], temb)",
            "-        #        h = self.mid.block_1(h, temb)",
            "-        #        h = self.mid.attn_1(h)",
            "-        #        h = self.mid.block_2(h, temb)",
            "+        h = self.mid_new(hs[-1], temb)",
            "",
            "# upsampling",
            "for i_level in reversed(range(self.num_resolutions)):"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=mid), value='mid_new')"
        ],
        "plus_line": 1,
        "minus_line": 4,
        "AST_diff_line": 1,
        "number": 4885,
        "neg_line": [
            "-h = self.mid(hs[-1], temb)",
            "-#        h = self.mid.block_1(h, temb)",
            "-#        h = self.mid.attn_1(h)",
            "-#        h = self.mid.block_2(h, temb)"
        ],
        "pos_line": [
            "+h = self.mid_new(hs[-1], temb)"
        ],
        "core_change": "-h = self.mid(hs[-1], temb) -#        h = self.mid.block_1(h, temb) -#        h = self.mid.attn_1(h) -#        h = self.mid.block_2(h, temb) +h = self.mid_new(hs[-1], temb)",
        "core_API": "append"
    },
    {
        "commit_hash": "b75255cd9d67f54f8b2df58536c99323a9f406fc",
        "index": "9339b98ea..b37e0cca3 100644",
        "commit_message": "[OPT/Galactica] Load large `galactica` models (#20390)\n\n* fix `opt` bias\n\n* revert unneeded assignment\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class OPTDecoder(OPTPreTrainedModel):",
            "# with checkpoints that have been fine-tuned before transformers v4.20.1",
            "# see https://github.com/facebookresearch/metaseq/pull/164",
            "if config.do_layer_norm_before and not config._remove_final_layer_norm:",
            "-            self.final_layer_norm = nn.LayerNorm(config.hidden_size)",
            "+            self.final_layer_norm = nn.LayerNorm(",
            "+                config.hidden_size, elementwise_affine=config.layer_norm_elementwise_affine",
            "+            )",
            "else:",
            "self.final_layer_norm = None"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1533368)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1533369)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'elementwise_affine'), position=0, insert_id=1533370)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1533371)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=1533372)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'config'), position=0, insert_id=1533373)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1533374)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'layer_norm_elementwise_affine'), position=2, insert_id=1533375)"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 4888,
        "neg_line": [
            "-self.final_layer_norm = nn.LayerNorm(config.hidden_size)"
        ],
        "pos_line": [
            "+self.final_layer_norm = nn.LayerNorm(",
            "+config.hidden_size, elementwise_affine=config.layer_norm_elementwise_affine",
            "+)"
        ],
        "core_change": "-self.final_layer_norm = nn.LayerNorm(config.hidden_size) +self.final_layer_norm = nn.LayerNorm( +config.hidden_size, elementwise_affine=config.layer_norm_elementwise_affine +)",
        "core_API": "LayerNorm"
    },
    {
        "commit_hash": "e924bc52ad8acb080b9a13eaa7bd4b9c3cb5f3da",
        "index": "e11add90..b412838a 100644",
        "commit_message": "max points bugfix\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class NormalizeRotation(object):",
            "C = torch.matmul(pos.t(), pos)",
            "e, v = torch.eig(C, eigenvectors=True)  # v[:,j] is j-th eigenvector",
            "",
            "-        data.pos = torch.matmul(pos, v)",
            "+        data.pos = torch.matmul(data.pos, v)",
            "",
            "if 'norm' in data:",
            "data.norm = F.normalize(torch.matmul(data.norm, v))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=1, insert_id=1059002)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'data'), position=0, insert_id=1059003)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1059004)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=pos), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 4889,
        "neg_line": [
            "-data.pos = torch.matmul(pos, v)"
        ],
        "pos_line": [
            "+data.pos = torch.matmul(data.pos, v)"
        ],
        "core_change": "-data.pos = torch.matmul(pos, v) +data.pos = torch.matmul(data.pos, v)",
        "core_API": "matmul"
    },
    {
        "commit_hash": "d08bc09495a289519a774e881f399cf864178f9b",
        "index": "84ee5c0e..d1f3a9ee 100644",
        "commit_message": "Fix unused variable found (#1250)\n\n* Fix Unused variable found\n\n* more fixes\n\n* Update kornia/enhance/histogram.py\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_deg2rad(batch_shape, device, dtype, atol, rtol):",
            "",
            "assert_close(x_deg, x_rad_to_deg, atol=atol, rtol=rtol)",
            "",
            "-    eps = torch.finfo(dtype).eps",
            "assert gradcheck(kornia.deg2rad, (tensor_to_gradcheck_var(x_deg),), raise_exception=True)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=eps))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=finfo))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=dtype))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=eps))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 16,
        "number": 4897,
        "neg_line": [
            "-eps = torch.finfo(dtype).eps"
        ],
        "pos_line": [],
        "core_change": "-eps = torch.finfo(dtype).eps",
        "core_API": "finfo"
    },
    {
        "commit_hash": "c05c9bf75e12d4169ae82e0ae3e6a47a4fa4746b",
        "index": "622cd8564..d8b1fa579 100644",
        "commit_message": "fixed_line (#1072)\n\n\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class NewDataset(datasets.GeneratorBasedBuilder):",
            "# data = datasets.load_dataset('my_dataset', 'first_domain')",
            "# data = datasets.load_dataset('my_dataset', 'second_domain')",
            "BUILDER_CONFIGS = [",
            "-        datasets.BuilderConfig(name=\"first_domain\", description=\"This part of my dataset covers a first domain\"),",
            "-        datasets.BuilderConfig(name=\"second_domain\", description=\"This part of my dataset covers a second domain\"),",
            "+        datasets.BuilderConfig(name=\"first_domain\", version=VERSION, description=\"This part of my dataset covers a first domain\"),",
            "+        datasets.BuilderConfig(name=\"second_domain\", version=VERSION, description=\"This part of my dataset covers a second domain\"),",
            "]",
            "",
            "DEFAULT_CONFIG_NAME = \"first_domain\"  # It's not mandatory to have a default configuration. Just use one if it make sense."
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1788941)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'datasets'), position=0, insert_id=1788942)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1788943)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'BuilderConfig'), position=2, insert_id=1788944)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1788945)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=1788946)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1788947)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=1788948)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'version'), position=0, insert_id=1788949)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1788950)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'VERSION'), position=2, insert_id=1788951)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'version'), position=0, insert_id=1788952)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1788953)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'VERSION'), position=2, insert_id=1788954)",
            "Delete(target_node=ASTNode(type=identifier, text=datasets))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=BuilderConfig))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 19,
        "number": 4898,
        "neg_line": [
            "-datasets.BuilderConfig(name=\"first_domain\", description=\"This part of my dataset covers a first domain\"),",
            "-datasets.BuilderConfig(name=\"second_domain\", description=\"This part of my dataset covers a second domain\"),"
        ],
        "pos_line": [
            "+datasets.BuilderConfig(name=\"first_domain\", version=VERSION, description=\"This part of my dataset covers a first domain\"),",
            "+datasets.BuilderConfig(name=\"second_domain\", version=VERSION, description=\"This part of my dataset covers a second domain\"),"
        ],
        "core_change": "-datasets.BuilderConfig(name=\"first_domain\", description=\"This part of my dataset covers a first domain\"), -datasets.BuilderConfig(name=\"second_domain\", description=\"This part of my dataset covers a second domain\"), +datasets.BuilderConfig(name=\"first_domain\", version=VERSION, description=\"This part of my dataset covers a first domain\"), +datasets.BuilderConfig(name=\"second_domain\", version=VERSION, description=\"This part of my dataset covers a second domain\"),",
        "core_API": "load_dataset"
    },
    {
        "commit_hash": "3fa67d721aef3fb635f81d9ee0a9cd63e8550f17",
        "index": "f9f19816..adced037 100644",
        "commit_message": "Fix HolidayCalendar tests on Travis\n\nPiperOrigin-RevId: 290737915\n\n",
        "file": "tf-quant-finance.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class HolidayCalendarTest(tf.test.TestCase, parameterized.TestCase):",
            "def test_skip_eager_reset(self):",
            "cal = dates.HolidayCalendar(start_year=2020, end_year=2021)",
            "cal.is_business_day(dates.DateTensor.from_tuples([]))  # Trigger caching.",
            "-    tf.reset_default_graph()",
            "+    tf.compat.v1.reset_default_graph()",
            "cal.reset()",
            "date_tensor = dates.DateTensor.from_tuples([(2020, 1, 3), (2020, 1, 4),",
            "(2021, 12, 24), (2021, 12, 25)])"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=2346322)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=2346323)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2346324)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2346325)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'v1'), position=2, insert_id=2346326)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=tf), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'compat'), position=2, insert_id=2346327)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 4899,
        "neg_line": [
            "-tf.reset_default_graph()"
        ],
        "pos_line": [
            "+tf.compat.v1.reset_default_graph()"
        ],
        "core_change": "-tf.reset_default_graph() +tf.compat.v1.reset_default_graph()",
        "core_API": "HolidayCalendar"
    },
    {
        "commit_hash": "dabf39bc53e3475fe491b3a5e902f31b33a22541",
        "index": "8c720219..26e9f8ac 100644",
        "commit_message": "RandomRotation3D cuda fix (#810)\n\n\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestRandomRotation3D:",
            "",
            "def test_same_on_batch(self, device, dtype):",
            "f = RandomRotation3D(degrees=40, same_on_batch=True)",
            "-        input = torch.eye(6, device=device, dtype=dtype).unsqueeze(dim=0).unsqueeze(dim=0).repeat(2, 3, 1, 1, 1)",
            "+        input = torch.eye(6, device=device, dtype=dtype).unsqueeze(dim=0).unsqueeze(dim=0).repeat(2, 3, 6, 1, 1)",
            "res = f(input)",
            "assert (res[0] == res[1]).all()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=integer, text=1), value='6')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 4901,
        "neg_line": [
            "-input = torch.eye(6, device=device, dtype=dtype).unsqueeze(dim=0).unsqueeze(dim=0).repeat(2, 3, 1, 1, 1)"
        ],
        "pos_line": [
            "+input = torch.eye(6, device=device, dtype=dtype).unsqueeze(dim=0).unsqueeze(dim=0).repeat(2, 3, 6, 1, 1)"
        ],
        "core_change": "-input = torch.eye(6, device=device, dtype=dtype).unsqueeze(dim=0).unsqueeze(dim=0).repeat(2, 3, 1, 1, 1) +input = torch.eye(6, device=device, dtype=dtype).unsqueeze(dim=0).unsqueeze(dim=0).repeat(2, 3, 6, 1, 1)",
        "core_API": "eye"
    },
    {
        "commit_hash": "4dd784c32f76fb8285f205b94e2a6ebde731a1cd",
        "index": "2ff256d78..c6a8e0014 100644",
        "commit_message": "Fix momentum and epsilon values (#19454)\n\nThe momentum value for PyTorch and TensorFlow batch normalization layers is not equivalent. The TensorFlow value should be (1 - pytorch_momentum) in order to ensure the correct updates are applied to the running mean and running variance calculations. We wouldn't observe a difference loading a pretrained model and performing inference, but evaluation outputs would change after some training steps.\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFSegformerDecodeHead(TFSegformerPreTrainedModel):",
            "self.linear_fuse = tf.keras.layers.Conv2D(",
            "filters=config.decoder_hidden_size, kernel_size=1, use_bias=False, name=\"linear_fuse\"",
            ")",
            "-        self.batch_norm = tf.keras.layers.BatchNormalization(epsilon=1e-5, momentum=0.1, name=\"batch_norm\")",
            "+        self.batch_norm = tf.keras.layers.BatchNormalization(epsilon=1e-5, momentum=0.9, name=\"batch_norm\")",
            "self.activation = tf.keras.layers.Activation(\"relu\")",
            "",
            "self.dropout = tf.keras.layers.Dropout(config.classifier_dropout_prob)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=float, text=0.1), value='0.9')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 4906,
        "neg_line": [
            "-self.batch_norm = tf.keras.layers.BatchNormalization(epsilon=1e-5, momentum=0.1, name=\"batch_norm\")"
        ],
        "pos_line": [
            "+self.batch_norm = tf.keras.layers.BatchNormalization(epsilon=1e-5, momentum=0.9, name=\"batch_norm\")"
        ],
        "core_change": "-self.batch_norm = tf.keras.layers.BatchNormalization(epsilon=1e-5, momentum=0.1, name=\"batch_norm\") +self.batch_norm = tf.keras.layers.BatchNormalization(epsilon=1e-5, momentum=0.9, name=\"batch_norm\")",
        "core_API": "Conv2D"
    },
    {
        "commit_hash": "3be2d048848df43e2dacb9127d2278443157ba78",
        "index": "92a5bd43b..93f0e96ab 100644",
        "commit_message": "fix consistency CrossEntropyLoss in modeling_bart (#6265)\n\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class BartForSequenceClassification(PretrainedBartModel):",
            "",
            "loss = None",
            "if labels is not None:",
            "-            loss = F.cross_entropy(logits.view(-1, self.config.num_labels), labels.view(-1))",
            "+            loss_fct = CrossEntropyLoss()",
            "+            loss = loss_fct(logits.view(-1, self.config.num_labels), labels.view(-1))",
            "",
            "if not return_dict:",
            "output = (logits,) + outputs[1:]"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=block), node=('expression_statement', None), position=0, insert_id=1543977)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1543978)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'loss_fct'), position=0, insert_id=1543979)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1543980)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=1543981)",
            "Insert(target_node=IN(type=call), node=('identifier', 'CrossEntropyLoss'), position=0, insert_id=1543982)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1543983)",
            "Update(target_node=ASTNode(type=identifier, text=F), value='loss_fct')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=F), position=0)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1543984)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1543985)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=cross_entropy))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 14,
        "number": 4907,
        "neg_line": [
            "-loss = F.cross_entropy(logits.view(-1, self.config.num_labels), labels.view(-1))"
        ],
        "pos_line": [
            "+loss_fct = CrossEntropyLoss()",
            "+loss = loss_fct(logits.view(-1, self.config.num_labels), labels.view(-1))"
        ],
        "core_change": "-loss = F.cross_entropy(logits.view(-1, self.config.num_labels), labels.view(-1)) +loss_fct = CrossEntropyLoss() +loss = loss_fct(logits.view(-1, self.config.num_labels), labels.view(-1))",
        "core_API": "cross_entropy"
    },
    {
        "commit_hash": "dea9d4023b578b4452c3861618e46466d4553658",
        "index": "76a8fee2..5f33534f 100644",
        "commit_message": "Work on Special Activations Functions (#690)\n\n* work on special activations\n\n* Special activation functions fixed\n\n* Changelog modified\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def leaky_twice_relu6(x, alpha_low=0.2, alpha_high=0.2, name=\"leaky_relu6\"):",
            "",
            "\"\"\"",
            "",
            "-    if not (0 < alpha_high <= 1):",
            "+    if not isinstance(alpha_high, tf.Tensor) and not (0 < alpha_high <= 1):",
            "raise ValueError(\"`alpha_high` value must be in [0, 1]`\")",
            "",
            "-    if not (0 < alpha_low <= 1):",
            "+    if not isinstance(alpha_low, tf.Tensor) and not (0 < alpha_low <= 1):",
            "raise ValueError(\"`alpha_low` value must be in [0, 1]`\")",
            "",
            "with tf.name_scope(name, \"leaky_twice_relu6\") as name_scope:"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 4910,
        "neg_line": [
            "-if not (0 < alpha_high <= 1):",
            "-if not (0 < alpha_low <= 1):"
        ],
        "pos_line": [
            "+if not isinstance(alpha_high, tf.Tensor) and not (0 < alpha_high <= 1):",
            "+if not isinstance(alpha_low, tf.Tensor) and not (0 < alpha_low <= 1):"
        ],
        "core_change": "-if not (0 < alpha_high <= 1): +if not isinstance(alpha_high, tf.Tensor) and not (0 < alpha_high <= 1): -if not (0 < alpha_low <= 1): +if not isinstance(alpha_low, tf.Tensor) and not (0 < alpha_low <= 1):",
        "core_API": "name_scope"
    },
    {
        "commit_hash": "616e89418407a6e8cf38f476582cee9302676ca1",
        "index": "38758c42..de31d178 100644",
        "commit_message": "Make BentoService.api decorator required for user model (#14)\n\n* lazily config service apis, avoid __init__ in BentoService base class\n\n* refactor - @property apis => get_service_api method\n\n* fix bentoml_config scope issue in model load\n\n* fix module save/load\n\n* remove 'predict' magic, enforce \"@api\" decorator\n\n* skip test_save_and_load_model_from_s3 for now\n\n* fix all linter errors\n\n* improve save/load from s3 location\n\n",
        "file": "BentoML.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "keras",
        "change": [
            "class TfKerasModelArtifact(Artifact):",
            "def model_file_path(self, base_path):",
            "return os.path.join(base_path, self.name + self._model_extension)",
            "",
            "-    def pack(self, model):",
            "+    def pack(self, model):  # pylint:disable=arguments-differ",
            "self.model = model",
            "",
            "def get(self):",
            "return self.model",
            "",
            "-    def load(self, base_path):  # pylint:disable=arguments-differ",
            "-        from tensorflow.keras.models import load_model",
            "+    def load(self, base_path):",
            "+        try:",
            "+            from tensorflow.keras.models import load_model",
            "+        except ImportError:",
            "+            raise ImportError(\"tensorflow package is required to use TfKerasModelArtifact\")",
            "self.model = load_model(self.model_file_path(base_path))",
            "",
            "def save(self, base_path):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=block), node=('try_statement', None), position=0, insert_id=2479409)",
            "Insert(target_node=IN(type=try_statement), node=('try', 'try'), position=0, insert_id=2479410)",
            "Insert(target_node=IN(type=try_statement), node=(':', ':'), position=1, insert_id=2479411)",
            "Move(target_node=IN(type=try_statement), node=ASTNode(type=block), position=2)",
            "Insert(target_node=IN(type=try_statement), node=('except_clause', None), position=3, insert_id=2479412)",
            "Insert(target_node=IN(type=except_clause), node=('except', 'except'), position=0, insert_id=2479413)",
            "Insert(target_node=IN(type=except_clause), node=('identifier', 'ImportError'), position=1, insert_id=2479414)",
            "Insert(target_node=IN(type=except_clause), node=(':', ':'), position=2, insert_id=2479415)",
            "Insert(target_node=IN(type=except_clause), node=('block', None), position=3, insert_id=2479416)",
            "Insert(target_node=IN(type=block), node=('raise_statement', None), position=0, insert_id=2479417)",
            "Insert(target_node=IN(type=raise_statement), node=('raise', 'raise'), position=0, insert_id=2479418)",
            "Insert(target_node=IN(type=raise_statement), node=('call', None), position=1, insert_id=2479419)",
            "Insert(target_node=IN(type=call), node=('identifier', 'ImportError'), position=0, insert_id=2479420)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2479421)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2479422)",
            "Insert(target_node=IN(type=argument_list), node=('string', '\"tensorflow package is required to use TfKerasModelArtifact\"'), position=1, insert_id=2479423)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2479424)"
        ],
        "plus_line": 6,
        "minus_line": 3,
        "AST_diff_line": 17,
        "number": 4911,
        "neg_line": [
            "-def pack(self, model):",
            "-def load(self, base_path):  # pylint:disable=arguments-differ",
            "-from tensorflow.keras.models import load_model"
        ],
        "pos_line": [
            "+def pack(self, model):  # pylint:disable=arguments-differ",
            "+def load(self, base_path):",
            "+try:",
            "+from tensorflow.keras.models import load_model",
            "+except ImportError:",
            "+raise ImportError(\"tensorflow package is required to use TfKerasModelArtifact\")"
        ],
        "core_change": "-def pack(self, model): +def pack(self, model):  # pylint:disable=arguments-differ -def load(self, base_path):  # pylint:disable=arguments-differ -from tensorflow.keras.models import load_model +def load(self, base_path): +try: +from tensorflow.keras.models import load_model +except ImportError: +raise ImportError(\"tensorflow package is required to use TfKerasModelArtifact\")",
        "core_API": "join"
    },
    {
        "commit_hash": "7b3a0a833b836c1b7b1e981091b514baec819c40",
        "index": "5a15a3b7..c8cc7349 100644",
        "commit_message": "Adding simple HMC Kernel  (#690)\n\n* Adding HMC kernel\n\n* support cuda\n\n* add more tests\n\n* fix lint\n\n* relax tolerance on test_mcmc\n\n* add whitespace\n\n* correct test\n\n* adjust tolerance\n\n* address comments\n\n* remove clone kwarg\n\n* fix rng seed\n\n* adjust eps\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_mcmc_interface():",
            "samples.append(marginal.sample(data))",
            "sample_mean = torch.mean(torch.stack(samples), 0)",
            "sample_std = torch.std(torch.stack(samples), 0)",
            "-    assert_equal(sample_mean.data, torch.Tensor([0.0]), prec=5e-2)",
            "-    assert_equal(sample_std.data, torch.Tensor([1.0]), prec=5e-2)",
            "+    assert_equal(sample_mean.data, torch.Tensor([0.0]), prec=0.08)",
            "+    assert_equal(sample_std.data, torch.Tensor([1.0]), prec=0.08)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=float, text=5e-2), value='0.08')",
            "Update(target_node=ASTNode(type=float, text=5e-2), value='0.08')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 4913,
        "neg_line": [
            "-assert_equal(sample_mean.data, torch.Tensor([0.0]), prec=5e-2)",
            "-assert_equal(sample_std.data, torch.Tensor([1.0]), prec=5e-2)"
        ],
        "pos_line": [
            "+assert_equal(sample_mean.data, torch.Tensor([0.0]), prec=0.08)",
            "+assert_equal(sample_std.data, torch.Tensor([1.0]), prec=0.08)"
        ],
        "core_change": "-assert_equal(sample_mean.data, torch.Tensor([0.0]), prec=5e-2) -assert_equal(sample_std.data, torch.Tensor([1.0]), prec=5e-2) +assert_equal(sample_mean.data, torch.Tensor([0.0]), prec=0.08) +assert_equal(sample_std.data, torch.Tensor([1.0]), prec=0.08)",
        "core_API": "append"
    },
    {
        "commit_hash": "de67c1546d065c86d5bd5cecf022925bb01d0ec7",
        "index": "58d0e332..4fc8dbef 100644",
        "commit_message": "Add Cuda tests for pyro.distributions; fix errors (#297)\n\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Multinomial(Distribution):",
            "if var.data.dim() == 1:",
            "return var.data",
            "# nested tensor arrays because of batches\"",
            "-        return var.data.numpy()[0]",
            "+        return var.data.cpu().numpy()[0]",
            "",
            "def analytic_mean(self, ps=None, n=None):",
            "ps, n = self._sanitize_input(ps, n)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('call', None), position=0, insert_id=763136)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=763137)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=763138)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=763139)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cpu'), position=2, insert_id=763140)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=763141)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=763142)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 4917,
        "neg_line": [
            "-return var.data.numpy()[0]"
        ],
        "pos_line": [
            "+return var.data.cpu().numpy()[0]"
        ],
        "core_change": "-return var.data.numpy()[0] +return var.data.cpu().numpy()[0]",
        "core_API": "dim"
    },
    {
        "commit_hash": "ec2c10309bb3676f5a77f817c1d8a38037b6ae6d",
        "index": "aace3e8b0..45f15acd4 100644",
        "commit_message": "[RLlib] CQL for HalfCheetah-Random-v0 + Hopper-Random-v0 + CQL Bug Fixes (#14243)\n\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def cql_loss(policy: Policy, model: ModelV2,",
            "# CQL Loss (We are using Entropy version of CQL (the best version))",
            "rand_actions = convert_to_torch_tensor(",
            "torch.FloatTensor(actions.shape[0] * num_actions,",
            "-                          actions.shape[-1]).uniform_(action_low, action_high))",
            "+                          actions.shape[-1]).uniform_(action_low, action_high),",
            "+        policy.device)",
            "curr_actions, curr_logp = policy_actions_repeat(model, action_dist_class,",
            "obs, num_actions)",
            "next_actions, next_logp = policy_actions_repeat(model, action_dist_class,",
            "next_obs, num_actions)",
            "+",
            "curr_logp = curr_logp.view(actions.shape[0], num_actions, 1)",
            "next_logp = next_logp.view(actions.shape[0], num_actions, 1)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1117846)",
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=3, insert_id=1117847)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'policy'), position=0, insert_id=1117848)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1117849)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=1117850)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 4927,
        "neg_line": [
            "-actions.shape[-1]).uniform_(action_low, action_high))"
        ],
        "pos_line": [
            "+actions.shape[-1]).uniform_(action_low, action_high),",
            "+policy.device)",
            "+"
        ],
        "core_change": "-actions.shape[-1]).uniform_(action_low, action_high)) +actions.shape[-1]).uniform_(action_low, action_high), +policy.device) +",
        "core_API": "FloatTensor"
    },
    {
        "commit_hash": "ea02caab3a2b0a97939df50e60009fa737799dc3",
        "index": "44d8afd8..2e94d6b0 100644",
        "commit_message": "fixed a bunch of linting errors...\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SimplePoutineTests(TestCase):",
            "",
            "def guide():",
            "latent = pyro.sample(\"latent\",",
            "-                                DiagNormal(Variable(torch.zeros(1)),",
            "+                                 DiagNormal(Variable(torch.zeros(1)),",
            "5 * Variable(torch.ones(1))))",
            "-            #x_dist = DiagNormal(latent, Variable(torch.ones(1)))",
            "+            # x_dist = DiagNormal(latent, Variable(torch.ones(1)))",
            "return latent",
            "",
            "self.guide = guide",
            "",
            "-",
            "def test_trace_replay(self):",
            "\"\"\"",
            "some simple invariants on a single example, but woefully incomplete"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 4929,
        "neg_line": [
            "-DiagNormal(Variable(torch.zeros(1)),",
            "-#x_dist = DiagNormal(latent, Variable(torch.ones(1)))",
            "-"
        ],
        "pos_line": [
            "+DiagNormal(Variable(torch.zeros(1)),",
            "+# x_dist = DiagNormal(latent, Variable(torch.ones(1)))"
        ],
        "core_change": "-DiagNormal(Variable(torch.zeros(1)), +DiagNormal(Variable(torch.zeros(1)), -#x_dist = DiagNormal(latent, Variable(torch.ones(1))) +# x_dist = DiagNormal(latent, Variable(torch.ones(1))) -",
        "core_API": "sample"
    },
    {
        "commit_hash": "258f4c64386d0e2caf73f98e45a46a7482ff4784",
        "index": "6ed528a..c2d9345 100644",
        "commit_message": "`attempt_load()` deserialize fix (#8051)\n\n\n",
        "file": "yolov5.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def attempt_load(weights, device=None, inplace=True, fuse=True):",
            "# Loads an ensemble of models weights=[a,b,c] or a single model weights=[a] or weights=a",
            "model = Ensemble()",
            "for w in weights if isinstance(weights, list) else [weights]:",
            "-        ckpt = torch.load(attempt_download(w))",
            "-        ckpt = (ckpt.get('ema') or ckpt['model']).to(device).float()  # FP32 model",
            "+        ckpt = torch.load(attempt_download(w), map_location=device)",
            "+        ckpt = (ckpt.get('ema') or ckpt['model']).float()  # FP32 model",
            "model.append(ckpt.fuse().eval() if fuse else ckpt.eval())  # fused or un-fused model in eval mode",
            "",
            "# Compatibility updates"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=call), position=2)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=argument_list), position=1)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1294065)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1294066)",
            "Update(target_node=ASTNode(type=identifier, text=to), value='float')",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'map_location'), position=0, insert_id=1294067)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1294068)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=2, insert_id=1294069)",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=device))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=float))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 16,
        "number": 4930,
        "neg_line": [
            "-ckpt = torch.load(attempt_download(w))",
            "-ckpt = (ckpt.get('ema') or ckpt['model']).to(device).float()  # FP32 model"
        ],
        "pos_line": [
            "+ckpt = torch.load(attempt_download(w), map_location=device)",
            "+ckpt = (ckpt.get('ema') or ckpt['model']).float()  # FP32 model"
        ],
        "core_change": "-ckpt = torch.load(attempt_download(w)) -ckpt = (ckpt.get('ema') or ckpt['model']).to(device).float()  # FP32 model +ckpt = torch.load(attempt_download(w), map_location=device) +ckpt = (ckpt.get('ema') or ckpt['model']).float()  # FP32 model",
        "core_API": "load"
    },
    {
        "commit_hash": "7abf4ace4040487c03df2d049350986536f023eb",
        "index": "713e6397..a0552519 100644",
        "commit_message": "Add AutoResumeTrainConfig to really do auto resuming from log dir (fix #660)\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Trainer(object):",
            "session_creator (tf.train.SessionCreator):",
            "session_init (sessinit.SessionInit):",
            "\"\"\"",
            "+        assert isinstance(session_creator, tf.train.SessionCreator), session_creator",
            "+        assert isinstance(session_init, SessionInit), session_init",
            "session_init._setup_graph()",
            "",
            "logger.info(\"Creating the session ...\")"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 0,
        "AST_diff_line": 17,
        "number": 4931,
        "neg_line": [],
        "pos_line": [
            "+assert isinstance(session_creator, tf.train.SessionCreator), session_creator",
            "+assert isinstance(session_init, SessionInit), session_init"
        ],
        "core_change": "+assert isinstance(session_creator, tf.train.SessionCreator), session_creator +assert isinstance(session_init, SessionInit), session_init",
        "core_API": "_setup_graph"
    },
    {
        "commit_hash": "640acf9d4bc2094553613c79bc551b24db2ce4a8",
        "index": "c82b8d9f7e..1303375e41 100644",
        "commit_message": "lintfixbot: Auto-commit fixed lint errors in codebase\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def count_nonzero(",
            "def _dtype_count_nonzero(a, axis, dtype):",
            "if dtype is None:",
            "return torch.count_nonzero(a, dim=axis)",
            "-        return torch.tensor(torch.count_nonzero(a, dim=axis),",
            "-                            dtype=ivy.as_native_dtype(dtype))",
            "+        return torch.tensor(",
            "+            torch.count_nonzero(a, dim=axis), dtype=ivy.as_native_dtype(dtype)",
            "+        )",
            "",
            "x = _dtype_count_nonzero(a, axis, dtype)",
            "if not keepdims:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 3,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 4936,
        "neg_line": [
            "-return torch.tensor(torch.count_nonzero(a, dim=axis),",
            "-dtype=ivy.as_native_dtype(dtype))"
        ],
        "pos_line": [
            "+return torch.tensor(",
            "+torch.count_nonzero(a, dim=axis), dtype=ivy.as_native_dtype(dtype)",
            "+)"
        ],
        "core_change": "-return torch.tensor(torch.count_nonzero(a, dim=axis), -dtype=ivy.as_native_dtype(dtype)) +return torch.tensor( +torch.count_nonzero(a, dim=axis), dtype=ivy.as_native_dtype(dtype) +)",
        "core_API": "count_nonzero"
    },
    {
        "commit_hash": "4bac6ab2b5de6948096b899cabb77c1eb7f21a4d",
        "index": "7e3500ccd7..f42292876c 100644",
        "commit_message": "skipping remainder and fixing torch and tf backends for remainder - deviates from standard\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def remainder(",
            ") -> torch.Tensor:",
            "x1, x2 = _cast_for_binary_op(x1, x2)",
            "ret = torch.remainder(x1, x2, out=out)",
            "-    ret[torch.isnan(ret)] = 0",
            "if ivy.exists(out):",
            "return ivy.inplace_update(out, ret)",
            "return ret"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=ret))",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=isnan))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=ret))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=integer, text=0))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 4938,
        "neg_line": [
            "-ret[torch.isnan(ret)] = 0"
        ],
        "pos_line": [],
        "core_change": "-ret[torch.isnan(ret)] = 0",
        "core_API": "remainder"
    },
    {
        "commit_hash": "9df89d93eb6d1eed74b2546690529c3d2733795c",
        "index": "6130a91437..a48fb00912 100644",
        "commit_message": "Fixed failing test for one_hot (#4192)\n\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def one_hot(",
            "device: torch.device,",
            "out: Optional[torch.Tensor] = None,",
            ") -> torch.Tensor:",
            "-    return torch.nn.functional.one_hot(indices.type(torch.int64), depth).to(device)",
            "+    return torch.nn.functional.one_hot(indices.to(torch.int64), depth).to(",
            "+        device, indices.dtype",
            "+    )",
            "",
            "",
            "def shape(x: torch.Tensor, as_array: bool = False) -> Union[ivy.Shape, ivy.Array]:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=329917)",
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=3, insert_id=329918)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'indices'), position=0, insert_id=329919)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=329920)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dtype'), position=2, insert_id=329921)",
            "Update(target_node=ASTNode(type=identifier, text=type), value='to')"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 4941,
        "neg_line": [
            "-return torch.nn.functional.one_hot(indices.type(torch.int64), depth).to(device)"
        ],
        "pos_line": [
            "+return torch.nn.functional.one_hot(indices.to(torch.int64), depth).to(",
            "+device, indices.dtype",
            "+)"
        ],
        "core_change": "-return torch.nn.functional.one_hot(indices.type(torch.int64), depth).to(device) +return torch.nn.functional.one_hot(indices.to(torch.int64), depth).to( +device, indices.dtype +)",
        "core_API": "one_hot"
    },
    {
        "commit_hash": "da2f1325014a158be36d62bdd20293ef97e574f4",
        "index": "d50ca472..eb69b1ff 100644",
        "commit_message": "enable black in the precommit (#1777)\n\n* enable black in the precommit\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add some fixes\n\n* update libface detection url\n\n* added url from kornia checkpoint\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def linear_transform(",
            "new_order: List[int] = perm.tolist()",
            "inv_order: List[int] = perm_inv.tolist()",
            "",
            "-    feature_sizes = torch.tensor(inp_size[0:dim] + inp_size[dim + 1::])",
            "+    feature_sizes = torch.tensor(inp_size[0:dim] + inp_size[dim + 1 : :])",
            "num_features: int = int(torch.prod(feature_sizes).item())",
            "",
            "inp_permute = inp.permute(new_order)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 4942,
        "neg_line": [
            "-feature_sizes = torch.tensor(inp_size[0:dim] + inp_size[dim + 1::])"
        ],
        "pos_line": [
            "+feature_sizes = torch.tensor(inp_size[0:dim] + inp_size[dim + 1 : :])"
        ],
        "core_change": "-feature_sizes = torch.tensor(inp_size[0:dim] + inp_size[dim + 1::]) +feature_sizes = torch.tensor(inp_size[0:dim] + inp_size[dim + 1 : :])",
        "core_API": "tolist"
    },
    {
        "commit_hash": "de359c459397f0944587982dfcbaf2fa469343e2",
        "index": "ac1cc13e9..b7270abb6 100644",
        "commit_message": "Fix doctest for `TFDeiTForImageClassification` (#19173)\n\n* Fix doctest for TFDeiTForImageClassification\n\n* Remove unnecessary tf.random.set_seed\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "keras",
        "change": [
            "class TFDeiTForImageClassification(TFDeiTPreTrainedModel, TFSequenceClassificati",
            ">>> # model predicts one of the 1000 ImageNet classes",
            ">>> predicted_class_idx = tf.math.argmax(logits, axis=-1)[0]",
            ">>> print(\"Predicted class:\", model.config.id2label[int(predicted_class_idx)])",
            "-        Predicted class: ptarmigan",
            "+        Predicted class: little blue heron, Egretta caerulea",
            "```\"\"\"",
            "return_dict = return_dict if return_dict is not None else self.config.use_return_dict"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=binary_operator), position=6)",
            "Insert(target_node=ASTNode(type=ERROR), node=(',', ','), position=7, insert_id=2552067)",
            "Insert(target_node=ASTNode(type=ERROR), node=('ERROR', None), position=8, insert_id=2552068)",
            "Insert(target_node=ASTNode(type=ERROR), node=('keyword_argument', None), position=9, insert_id=2552069)",
            "Update(target_node=ASTNode(type=identifier, text=return_dict), value='heron')",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'Egretta'), position=0, insert_id=2552070)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'caerulea'), position=1, insert_id=2552071)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=string, text=``), position=2)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=ERROR), position=3)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=\", text=\"\"\"), position=4)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=identifier, text=return_dict), position=0)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type==, text==), position=1)",
            "Insert(target_node=IN(type=keyword_argument), node=('conditional_expression', None), position=2, insert_id=2552072)",
            "Update(target_node=ASTNode(type=identifier, text=ptarmigan), value='little')",
            "Insert(target_node=ASTNode(type=ERROR), node=('identifier', 'blue'), position=6, insert_id=2552073)",
            "Insert(target_node=IN(type=conditional_expression), node=('identifier', 'return_dict'), position=0, insert_id=2552074)",
            "Move(target_node=IN(type=conditional_expression), node=ASTNode(type=if, text=if), position=1)",
            "Move(target_node=IN(type=conditional_expression), node=ASTNode(type=comparison_operator), position=2)",
            "Move(target_node=IN(type=conditional_expression), node=ASTNode(type=else, text=else), position=3)",
            "Move(target_node=IN(type=conditional_expression), node=ASTNode(type=attribute), position=4)",
            "Delete(target_node=ASTNode(type=conditional_expression))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 21,
        "number": 4944,
        "neg_line": [
            "-Predicted class: ptarmigan"
        ],
        "pos_line": [
            "+Predicted class: little blue heron, Egretta caerulea"
        ],
        "core_change": "-Predicted class: ptarmigan +Predicted class: little blue heron, Egretta caerulea",
        "core_API": "argmax"
    },
    {
        "commit_hash": "ab59f308b18622421edc67048d3b9fbfde96a9f4",
        "index": "2f7ff7e45..6da53e7b5 100644",
        "commit_message": "Future 4/n: test & legacy in test/ folder (#13295)\n\n* move: legacy >> test/\n\n* move: tests >> test/\n\n* rename unittests\n\n* update CI\n\n* tests4pl\n\n* tests_pytorch\n\n* proxi\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* ci\n\n* link\n\n* cli\n\n* standalone\n\n* fixing\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* .\n\n* Apply suggestions from code review\n\nCo-authored-by: Akihiro Nitta <nitta@akihironitta.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* alone\n\n* test -> tests\n\n* Standalone fixes\n\n* ci\n\n* Update\n\n* More fixes\n\n* Fix coverage\n\n* Fix mypy\n\n* mypy\n\n* Empty-Commit\n\n* Fix\n\n* mypy just for pl\n\n* Fix standalone\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Akihiro Nitta <nitta@akihironitta.com>\nCo-authored-by: Carlos Mochol <carlossmocholi@gmail.com>\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def init_checkpoint_callback(logger):",
            "",
            "",
            "def pl_multi_process_test(func):",
            "-    \"\"\"Wrapper for running multi-processing tests.\"\"\"",
            "+    \"\"\"Wrapper for running multi-processing tests_pytorch.\"\"\"",
            "",
            "@functools.wraps(func)",
            "def wrapper(*args, **kwargs):"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"\"\"Wrapper for running multi-processing tests.\"\"\"), value='\"\"\"Wrapper for running multi-processing tests_pytorch.\"\"\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 4946,
        "neg_line": [
            "-\"\"\"Wrapper for running multi-processing tests.\"\"\""
        ],
        "pos_line": [
            "+\"\"\"Wrapper for running multi-processing tests_pytorch.\"\"\""
        ],
        "core_change": "-\"\"\"Wrapper for running multi-processing tests.\"\"\" +\"\"\"Wrapper for running multi-processing tests_pytorch.\"\"\"",
        "core_API": "wraps"
    },
    {
        "commit_hash": "88ff1953e94cfbbcd53f4d659a1099fce8a7344e",
        "index": "839acd7d..38480a1f 100644",
        "commit_message": "Input alias deprecation warning fixes - related to issue #479 (#480)\n\n* __init__ import error message repositioned at a more appropriate location\n\n* gitignore updated with venv environment\n\n* Typo fixed in tensorlayer.layers.convolution.py\n\n* Deprecation warning added for tl.layer.deconv2d with backward compatibility restored - Issue #479\n\n* Deprecation warning added for Layer API change: `layer` argument changed to `prev_layer` - Issue #479\nAdditional Modification using the syntax super for inheritance - more pythonic\n\n* test layers extend with argument names precised\n\n* tl.layers.core.py forgotten Classes with deprecation\n\n* Error fix in deprecation warning tl.layers.spatial_transformer.py\n\n* Test refactored and fix with arguments missing added\n\n* ReshapeLayer error fix\n\n* test_layers_normalization argument name missing fixed\n\n* error in tl.layers.ReshapeLayer test if shape is not empty fixed\n\n* test_layers_special_activation missing argument name fixed\n\n* test_layers_stack missing argument name fixed\n\n* test_layers_super_resolution missing argument name fixed\n\n* test_models missing argument name fixed\n\n* Formating error fixed\n\n* Decorator for deprecated argument added\n\n* Deprecation API Change - Use of newly implemented decorator. Docstring updated\n\n* Codacy Issues Fix\n\n* Unnecessary PR changes removed - PR Cleaned & Refactored\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "layers",
        "change": [
            "net.print_layers()",
            "net.print_params(False)",
            "",
            "if len(net.all_layers) != 6:",
            "-    raise Exception(\"layers dont match\")",
            "+    raise Exception(\"layers do not match\")",
            "",
            "if len(net.all_params) != 12:",
            "-    raise Exception(\"params dont match\")",
            "+    raise Exception(\"params do not match\")",
            "",
            "if net.count_params() != 60560:",
            "-    raise Exception(\"params dont match\")",
            "+    raise Exception(\"params do not match\")"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"layers dont match\"), value='\"layers do not match\"')",
            "Update(target_node=ASTNode(type=string, text=\"params dont match\"), value='\"params do not match\"')",
            "Update(target_node=ASTNode(type=string, text=\"params dont match\"), value='\"params do not match\"')"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 3,
        "number": 4948,
        "neg_line": [
            "-raise Exception(\"layers dont match\")",
            "-raise Exception(\"params dont match\")",
            "-raise Exception(\"params dont match\")"
        ],
        "pos_line": [
            "+raise Exception(\"layers do not match\")",
            "+raise Exception(\"params do not match\")",
            "+raise Exception(\"params do not match\")"
        ],
        "core_change": "-raise Exception(\"layers dont match\") +raise Exception(\"layers do not match\") -raise Exception(\"params dont match\") +raise Exception(\"params do not match\") -raise Exception(\"params dont match\") +raise Exception(\"params do not match\")",
        "core_API": "print_layers"
    },
    {
        "commit_hash": "4cdb7ee51db6ec544382b75cda3685ae39c4ec8f",
        "index": "f0bfea8c5..8dddcfa63 100644",
        "commit_message": "fix #11724 (#11897)\n\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFMarianSinusoidalPositionalEmbedding(tf.keras.layers.Layer):",
            "position_enc = np.array(",
            "[[pos / np.power(10000, 2 * (j // 2) / dim) for j in range(dim)] for pos in range(n_pos)]",
            ")",
            "+        table = np.zeros_like(position_enc)",
            "# index 0 is all zero",
            "-        position_enc[:, 0 : dim // 2] = np.sin(position_enc[:, 0::2])",
            "-        position_enc[:, dim // 2 :] = np.cos(position_enc[:, 1::2])",
            "+        table[:, 0 : dim // 2] = np.sin(position_enc[:, 0::2])",
            "+        table[:, dim // 2 :] = np.cos(position_enc[:, 1::2])",
            "# convert to tensor",
            "-        table = tf.convert_to_tensor(position_enc)",
            "+        table = tf.convert_to_tensor(table)",
            "tf.stop_gradient(table)",
            "return table"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=2, insert_id=2369753)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=2369754)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'table'), position=0, insert_id=2369755)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=2369756)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=2369757)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2369758)",
            "Move(target_node=IN(type=call), node=ASTNode(type=argument_list), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=position_enc), value='table')",
            "Insert(target_node=ASTNode(type=subscript), node=('slice', None), position=2, insert_id=2369759)",
            "Update(target_node=ASTNode(type=identifier, text=position_enc), value='table')",
            "Move(target_node=ASTNode(type=subscript), node=ASTNode(type=slice), position=2)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=2369760)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'np'), position=0, insert_id=2369761)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2369762)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'zeros_like'), position=2, insert_id=2369763)",
            "Insert(target_node=IN(type=slice), node=(':', ':'), position=0, insert_id=2369764)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2369765)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'table'), position=1, insert_id=2369766)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2369767)",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=slice))"
        ],
        "plus_line": 4,
        "minus_line": 3,
        "AST_diff_line": 21,
        "number": 4952,
        "neg_line": [
            "-position_enc[:, 0 : dim // 2] = np.sin(position_enc[:, 0::2])",
            "-position_enc[:, dim // 2 :] = np.cos(position_enc[:, 1::2])",
            "-table = tf.convert_to_tensor(position_enc)"
        ],
        "pos_line": [
            "+table = np.zeros_like(position_enc)",
            "+table[:, 0 : dim // 2] = np.sin(position_enc[:, 0::2])",
            "+table[:, dim // 2 :] = np.cos(position_enc[:, 1::2])",
            "+table = tf.convert_to_tensor(table)"
        ],
        "core_change": "+table = np.zeros_like(position_enc) -position_enc[:, 0 : dim // 2] = np.sin(position_enc[:, 0::2]) -position_enc[:, dim // 2 :] = np.cos(position_enc[:, 1::2]) +table[:, 0 : dim // 2] = np.sin(position_enc[:, 0::2]) +table[:, dim // 2 :] = np.cos(position_enc[:, 1::2]) -table = tf.convert_to_tensor(position_enc) +table = tf.convert_to_tensor(table)",
        "core_API": "array"
    },
    {
        "commit_hash": "c3037a092d4ea03504dcc6b9f65762f21247b0cf",
        "index": "b6dc80b7..7092ed44 100644",
        "commit_message": "Add docs for MobileViT (#1430)\n\n* Fix typing in MobileViT\n\n* Fix bibtex citation\n\n* Add docs for MobileViT\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class PreNorm(nn.Module):",
            "self.norm = nn.LayerNorm(dim)",
            "self.fn = fn",
            "",
            "-    def forward(self, x: torch.Tensor, **kwargs) -> nn.Module:",
            "+    def forward(self, x: torch.Tensor, **kwargs) -> torch.Tensor:",
            "return self.fn(self.norm(x), **kwargs)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=nn), value='torch')",
            "Update(target_node=ASTNode(type=identifier, text=Module), value='Tensor')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 4953,
        "neg_line": [
            "-def forward(self, x: torch.Tensor, **kwargs) -> nn.Module:"
        ],
        "pos_line": [
            "+def forward(self, x: torch.Tensor, **kwargs) -> torch.Tensor:"
        ],
        "core_change": "-def forward(self, x: torch.Tensor, **kwargs) -> nn.Module: +def forward(self, x: torch.Tensor, **kwargs) -> torch.Tensor:",
        "core_API": "LayerNorm"
    },
    {
        "commit_hash": "cec92098641d3f4c395cd51d84ba93b691d1cdf3",
        "index": "67aad524..a1c5980e 100644",
        "commit_message": "Several micro optimizations (#4833)\n\n* benchmark transfers\n\n* create tensors directl on device when possible\n\n* fix\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SmoothGradient(SaliencyInterpreter):",
            "def forward_hook(module, inputs, output):",
            "# Random noise = N(0, stdev * (max-min))",
            "scale = output.detach().max() - output.detach().min()",
            "-            noise = torch.randn(output.shape).to(output.device) * stdev * scale",
            "+            noise = torch.randn(output.shape, device=output.device) * stdev * scale",
            "",
            "# Add the random noise",
            "output.add_(noise)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=5326)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=5327)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=5328)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=5329)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=attribute), position=2)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=to))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 4957,
        "neg_line": [
            "-noise = torch.randn(output.shape).to(output.device) * stdev * scale"
        ],
        "pos_line": [
            "+noise = torch.randn(output.shape, device=output.device) * stdev * scale"
        ],
        "core_change": "-noise = torch.randn(output.shape).to(output.device) * stdev * scale +noise = torch.randn(output.shape, device=output.device) * stdev * scale",
        "core_API": "detach"
    },
    {
        "commit_hash": "a8fd14805392531f4c581fe07817fb1a9dd70cf6",
        "index": "566b8efa..7b82ad08 100644",
        "commit_message": "Changed dtype to depend on spots, textbook title fix\n\n",
        "file": "tf-quant-finance.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def barrier_price(*,",
            "raise ValueError('At most one of continuous_dividends and cost of carries '",
            "'may be supplied')",
            "with tf.name_scope(name or \"barrier_price\"):",
            "-    strikes = tf.convert_to_tensor(strikes, dtype=dtype, name=\"strikes\")",
            "-    dtype = strikes.dtype",
            "spots = tf.convert_to_tensor(spots, dtype=dtype, name=\"spots\")",
            "+    dtype = spots.dtype",
            "+    strikes = tf.convert_to_tensor(strikes, dtype=dtype, name=\"strikes\")",
            "volatilities = tf.convert_to_tensor(",
            "volatilities, dtype=dtype, name=\"volatilities\")",
            "expiries = tf.convert_to_tensor(expiries, dtype=dtype, name=\"expiries\")"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=IN(type=root), node=ASTNode(type=call), position=0)",
            "Insert(target_node=ASTNode(type=attribute), node=('call', None), position=0, insert_id=2344290)",
            "Update(target_node=ASTNode(type=identifier, text=spots), value='strikes')",
            "Move(target_node=IN(type=call), node=ASTNode(type=attribute), position=0)",
            "Move(target_node=IN(type=call), node=ASTNode(type=argument_list), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=strikes), value='spots')",
            "Update(target_node=ASTNode(type=identifier, text=strikes), value='spots')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 7,
        "number": 4958,
        "neg_line": [
            "-strikes = tf.convert_to_tensor(strikes, dtype=dtype, name=\"strikes\")",
            "-dtype = strikes.dtype"
        ],
        "pos_line": [
            "+dtype = spots.dtype",
            "+strikes = tf.convert_to_tensor(strikes, dtype=dtype, name=\"strikes\")"
        ],
        "core_change": "-strikes = tf.convert_to_tensor(strikes, dtype=dtype, name=\"strikes\") -dtype = strikes.dtype +dtype = spots.dtype +strikes = tf.convert_to_tensor(strikes, dtype=dtype, name=\"strikes\")",
        "core_API": "name_scope"
    },
    {
        "commit_hash": "c310ce661e54bf5a23431a26f57e3e4857b8e8ff",
        "index": "59f01184b..7a40b8ab1 100644",
        "commit_message": "Logger connector re-design `_Metadata.reduce_fx` fixes. (#7890)\n\n\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def _setup_ddp(rank, worldsize):",
            "def _ddp_test_fn(rank, worldsize):",
            "_setup_ddp(rank, worldsize)",
            "tensor = torch.tensor([1.0])",
            "-    sync = _Sync(sync_ddp_if_available, should=True, op=torch.distributed.ReduceOp.SUM)",
            "+    sync = _Sync(sync_ddp_if_available, should=True, op='SUM')",
            "actual = sync(tensor)",
            "assert actual.item() == dist.get_world_size(), \"Result-Log does not work properly with DDP and Tensors\""
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=keyword_argument), node=('string', \"'SUM'\"), position=2, insert_id=533057)",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=distributed))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=ReduceOp))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=SUM))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 4966,
        "neg_line": [
            "-sync = _Sync(sync_ddp_if_available, should=True, op=torch.distributed.ReduceOp.SUM)"
        ],
        "pos_line": [
            "+sync = _Sync(sync_ddp_if_available, should=True, op='SUM')"
        ],
        "core_change": "-sync = _Sync(sync_ddp_if_available, should=True, op=torch.distributed.ReduceOp.SUM) +sync = _Sync(sync_ddp_if_available, should=True, op='SUM')",
        "core_API": "tensor"
    },
    {
        "commit_hash": "2ee8d43dbe420be152fd5ce0d80b43b419a0e352",
        "index": "5c1ea36..ddce75f 100644",
        "commit_message": "Add set_epoch and fix args in DDP-series example (#1076)\n\nAdd set_epoch for shuffling inputs, fix arg order\n",
        "file": "examples.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "dataloader",
        "change": [
            "class Trainer:",
            "return loss.item()",
            "",
            "def _run_epoch(self, epoch: int, dataloader: DataLoader, train: bool = True):",
            "+        self.dataloader.sampler.set_epoch(epoch)",
            "for iter, (source, targets) in enumerate(dataloader):",
            "step_type = \"Train\" if train else \"Eval\"",
            "source = source.to(self.local_rank)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('block', None), position=0, insert_id=1762096)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=1762097)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=1762098)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1762099)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1762100)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1762101)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1762102)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'set_epoch'), position=2, insert_id=1762103)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1762104)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'epoch'), position=1, insert_id=1762105)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1762106)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1762107)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1762108)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'sampler'), position=2, insert_id=1762109)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=1762110)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1762111)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dataloader'), position=2, insert_id=1762112)",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 18,
        "number": 4969,
        "neg_line": [],
        "pos_line": [
            "+self.dataloader.sampler.set_epoch(epoch)"
        ],
        "core_change": "+self.dataloader.sampler.set_epoch(epoch)",
        "core_API": "item"
    },
    {
        "commit_hash": "3efac3b45a37040f8d67c5b228f273bef99d8430",
        "index": "2ce1d007..929885ba 100644",
        "commit_message": "fix qm9 dataset\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class QM9(InMemoryDataset):",
            "bond_idx += 2 * [self.bonds[bond.GetBondType()]]",
            "",
            "edge_index = torch.tensor([row, col], dtype=torch.long)",
            "-            edge_attr = F.one_hot(torch.tensor(bond_idx),",
            "-                                  num_classes=len(self.bonds)).to(torch.float)",
            "+            edge_attr = F.one_hot(",
            "+                torch.tensor(bond_idx),",
            "+                num_classes=len(self.bonds)).to(torch.float)",
            "edge_index, edge_attr = coalesce(edge_index, edge_attr, N, N)",
            "",
            "y = target[i].unsqueeze(0)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 3,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 4971,
        "neg_line": [
            "-edge_attr = F.one_hot(torch.tensor(bond_idx),",
            "-num_classes=len(self.bonds)).to(torch.float)"
        ],
        "pos_line": [
            "+edge_attr = F.one_hot(",
            "+torch.tensor(bond_idx),",
            "+num_classes=len(self.bonds)).to(torch.float)"
        ],
        "core_change": "-edge_attr = F.one_hot(torch.tensor(bond_idx), -num_classes=len(self.bonds)).to(torch.float) +edge_attr = F.one_hot( +torch.tensor(bond_idx), +num_classes=len(self.bonds)).to(torch.float)",
        "core_API": "GetBondType"
    },
    {
        "commit_hash": "ed63b7ee9e68ba120893aaaaca0d4525a56ed484",
        "index": "5ab843f8..519aeb64 100644",
        "commit_message": "upgrade to pytorch 0.4.0 (#1126)\n\n* bump pytorch to 0.4 + fix sanitize\n\n* remove check for Variable in block_orthogonal\n\n* rename parameter split_size=\n\n* remove checks for Variable\n\n* fix more tests\n\n* fixes\n\n* get tests to pass\n\n* fix warnings\n\n* get rid of some of the Variables\n\n* more tests passing\n\n* more elimination of variables\n\n* finish removing all Variables\n\n* pylint and such\n\n* a few fixes\n\n* move torch.no_grad into model.forward_on_instances\n\n* more pytorch 0.4 changes\n\n* detach() -> data\n\n* pylint\n\n* fix bad tensor creation\n\n* fix types\n\n* remove print statement\n\n* more 0.4 goodness\n\n* factor out is_tensor\n\n* add no_grad to elmo command\n\n* cleanup\n\n* remove TODO\n\n* address PR feedback\n\n* replace all() with item()\n\n* more cleanup\n\n* further cleanup\n\n* remove Variable\n\n* really fix merge conflict\n\n* fix pylint\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class NlvrCoverageSemanticParserTest(ModelTestCase):",
            "('e -> 6', True, None)]",
            "# Of the actions above, those at indices 0 and 4 are on the agenda, and there are padding",
            "# indices at the end.",
            "-        test_agenda = Variable(torch.Tensor([[0], [4], [-1], [-1]]))",
            "+        test_agenda = torch.Tensor([[0], [4], [-1], [-1]])",
            "checklist_info = self.model._get_checklist_info(test_agenda, all_actions)",
            "target_checklist, terminal_actions, checklist_mask = checklist_info",
            "assert_almost_equal(target_checklist.data.numpy(), [[1], [0], [1]])"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=Variable))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 4973,
        "neg_line": [
            "-test_agenda = Variable(torch.Tensor([[0], [4], [-1], [-1]]))"
        ],
        "pos_line": [
            "+test_agenda = torch.Tensor([[0], [4], [-1], [-1]])"
        ],
        "core_change": "-test_agenda = Variable(torch.Tensor([[0], [4], [-1], [-1]])) +test_agenda = torch.Tensor([[0], [4], [-1], [-1]])",
        "core_API": "Tensor"
    },
    {
        "commit_hash": "24bece4d27f15195d24aafcc8db37e412ec0d44d",
        "index": "809f101c..57f81202 100644",
        "commit_message": "small fixes\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class DistributedPGModel(object):",
            "# TODO write summaries",
            "# self.summary_writer = tf.summary.FileWriter('log' + \"_%d\" % self.task_index)",
            "if not self.optimizer:",
            "-                self.optimizer = tf.train.AdamOptimizer(self.alpha)",
            "+                self.optimizer = tf.train.AdamOptimizer(self.learning_rate)",
            "",
            "else:",
            "optimizer_cls = get_function(self.optimizer)",
            "-                self.optimizer = optimizer_cls(self.alpha, *self.optimizer_args, **self.optimizer_kwargs)",
            "+                self.optimizer = optimizer_cls(self.learning_rate, *self.optimizer_args, **self.optimizer_kwargs)",
            "",
            "self.optimize_op = tf.group(self.optimizer.apply_gradients(grad_var_list),",
            "global_step_inc)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=ASTNode(type=assignment), node=('attribute', None), position=0, insert_id=2245834)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=2245835)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2245836)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'optimizer'), position=2, insert_id=2245837)",
            "Update(target_node=ASTNode(type=identifier, text=alpha), value='learning_rate')",
            "Update(target_node=ASTNode(type=identifier, text=alpha), value='learning_rate')",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=optimizer))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 11,
        "number": 4977,
        "neg_line": [
            "-self.optimizer = tf.train.AdamOptimizer(self.alpha)",
            "-self.optimizer = optimizer_cls(self.alpha, *self.optimizer_args, **self.optimizer_kwargs)"
        ],
        "pos_line": [
            "+self.optimizer = tf.train.AdamOptimizer(self.learning_rate)",
            "+self.optimizer = optimizer_cls(self.learning_rate, *self.optimizer_args, **self.optimizer_kwargs)"
        ],
        "core_change": "-self.optimizer = tf.train.AdamOptimizer(self.alpha) +self.optimizer = tf.train.AdamOptimizer(self.learning_rate) -self.optimizer = optimizer_cls(self.alpha, *self.optimizer_args, **self.optimizer_kwargs) +self.optimizer = optimizer_cls(self.learning_rate, *self.optimizer_args, **self.optimizer_kwargs)",
        "core_API": "FileWriter"
    },
    {
        "commit_hash": "23efadd9a394d9d69e10e8539a7332665b118df0",
        "index": "0966d664..c39baf22 100644",
        "commit_message": "upgrade to pytorch 1.2 (#3182)\n\n* first attempt at pytorch 1.2\n\n* explicit is better than implicit\n\n* more explicit\n\n* attempt to fix flaky tests\n\n* pylint\n\n* no disable dropout\n\n* disable dropout by default\n\n* restore dropout, don't deepcopy\n\n* change batch size for biaffine_dependency_parser_multilang_test, maybe that will make it pass? :(\n\n* try batch size 10\n\n* ignore bad gradient parameter\n\n* cleanup\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class BiaffineDependencyParser(Model):",
            "attended_arcs = attended_arcs + torch.diag(attended_arcs.new(mask.size(1)).fill_(-numpy.inf))",
            "# Mask padded tokens, because we only want to consider actual words as heads.",
            "if mask is not None:",
            "-            minus_mask = (1 - mask).byte().unsqueeze(2)",
            "+            minus_mask = (1 - mask).to(dtype=torch.bool).unsqueeze(2)",
            "attended_arcs.masked_fill_(minus_mask, -numpy.inf)",
            "",
            "# Compute the heads greedily."
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=byte), value='to')",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=1, insert_id=24316)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=24317)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=24318)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=24319)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=24320)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=24321)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'bool'), position=2, insert_id=24322)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 4981,
        "neg_line": [
            "-minus_mask = (1 - mask).byte().unsqueeze(2)"
        ],
        "pos_line": [
            "+minus_mask = (1 - mask).to(dtype=torch.bool).unsqueeze(2)"
        ],
        "core_change": "-minus_mask = (1 - mask).byte().unsqueeze(2) +minus_mask = (1 - mask).to(dtype=torch.bool).unsqueeze(2)",
        "core_API": "diag"
    },
    {
        "commit_hash": "96b50fe30a195afa5a206de5be4650a3d74a097d",
        "index": "d4cdcf89..9f2bd583 100644",
        "commit_message": "Fixes to device in augmentations (#1546)\n\n* add RandomPosterize test and fix device\n\n* fix other device transfer issues\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class PosterizeGenerator(RandomGeneratorBase):",
            "def forward(self, batch_shape: torch.Size, same_on_batch: bool = False) -> Dict[str, torch.Tensor]:  # type:ignore",
            "batch_size = batch_shape[0]",
            "_common_param_check(batch_size, same_on_batch)",
            "-        _device, _dtype = _extract_device_dtype([self.bits if isinstance(self.bits, torch.Tensor) else None])",
            "+        _device, _ = _extract_device_dtype([self.bits if isinstance(self.bits, torch.Tensor) else None])",
            "bits_factor = _adapted_rsampling((batch_size,), self.bit_sampler, same_on_batch)",
            "return dict(bits_factor=bits_factor.to(device=_device, dtype=torch.int32))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=_dtype), value='_')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 4982,
        "neg_line": [
            "-_device, _dtype = _extract_device_dtype([self.bits if isinstance(self.bits, torch.Tensor) else None])"
        ],
        "pos_line": [
            "+_device, _ = _extract_device_dtype([self.bits if isinstance(self.bits, torch.Tensor) else None])"
        ],
        "core_change": "-_device, _dtype = _extract_device_dtype([self.bits if isinstance(self.bits, torch.Tensor) else None]) +_device, _ = _extract_device_dtype([self.bits if isinstance(self.bits, torch.Tensor) else None])",
        "core_API": "to"
    },
    {
        "commit_hash": "d9e3f74497655d6cda1e5017f1ffa10c00bd232f",
        "index": "39b1a504d..55d2807c2 100644",
        "commit_message": "fix: change load/save logic in ner\n\n",
        "file": "DeepPavlov.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class DstcSlotFillingNetwork(Inferable, Trainable):",
            "self._slot_vals = json.load(f)",
            "",
            "self._ner_network = ner_network",
            "-        self.load()",
            "-",
            "",
            "@overrides",
            "def load(self):",
            "# Check presence of the model files",
            "-        if tf.train.get_checkpoint_state(str(path)) is not None:",
            "+        if tf.train.get_checkpoint_state(str(self.ser_path)) is not None:",
            "print(\"\\n:: initializing `{}` from saved\"\\",
            ".format(self.__class__.__name__))",
            "self._ner_network.load()"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=1, insert_id=1923822)",
            "Update(target_node=ASTNode(type=identifier, text=path), value='self')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=path), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1923823)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'ser_path'), position=2, insert_id=1923824)",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=load))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 14,
        "number": 4988,
        "neg_line": [
            "-self.load()",
            "-",
            "-if tf.train.get_checkpoint_state(str(path)) is not None:"
        ],
        "pos_line": [
            "+if tf.train.get_checkpoint_state(str(self.ser_path)) is not None:"
        ],
        "core_change": "-self.load() - -if tf.train.get_checkpoint_state(str(path)) is not None: +if tf.train.get_checkpoint_state(str(self.ser_path)) is not None:",
        "core_API": "load"
    },
    {
        "commit_hash": "d7c8ce57d43499f0042e63d5be2592c641354bad",
        "index": "ec344341b..92abe1ed5 100644",
        "commit_message": "Avoid accessing .dataset of a DataLoader in Trainer (#16451)\n\n* Avoid accessing .dataset of a dataloader\n\n* style\n\n* fix\n\n* cleaning up, reverting some misunderstandings\n\n* black\n\n* add train_dataset argument to get_train_dataloader, and fix other instances of length checks\n\n* flake8\n\n* address comments\n\n* fix bug\n\n* cleanup\n\n* add test\n\n* Update tests/trainer/test_trainer.py\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n\n* under torch\n\n* merge\n\n* stylistic suggestion\n\nCo-authored-by: Sander Land <sander@chatdesk.com>\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "dataloader",
        "change": [
            "class ProgressCallback(TrainerCallback):",
            "self.current_step = state.global_step",
            "",
            "def on_prediction_step(self, args, state, control, eval_dataloader=None, **kwargs):",
            "-        if state.is_local_process_zero and has_length(eval_dataloader.dataset):",
            "+        if state.is_local_process_zero and has_length(eval_dataloader):",
            "if self.prediction_bar is None:",
            "self.prediction_bar = tqdm(total=len(eval_dataloader), leave=self.training_bar is None)",
            "self.prediction_bar.update(1)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=eval_dataloader), position=1)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=dataset))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 4992,
        "neg_line": [
            "-if state.is_local_process_zero and has_length(eval_dataloader.dataset):"
        ],
        "pos_line": [
            "+if state.is_local_process_zero and has_length(eval_dataloader):"
        ],
        "core_change": "-if state.is_local_process_zero and has_length(eval_dataloader.dataset): +if state.is_local_process_zero and has_length(eval_dataloader):",
        "core_API": "update"
    },
    {
        "commit_hash": "b893c03bb098af1bd151f7a1675214f698182aa7",
        "index": "f9873f6..6f74eef 100644",
        "commit_message": "Fix bugs in tensorrt and openvino\n\n",
        "file": "nebullvm.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def get_outputs_sizes_torch(",
            "",
            "",
            "def create_model_inputs_torch(",
            "-    batch_size: int, input_infos: List[InputInfo]",
            "+    input_infos: List[InputInfo],",
            ") -> List[torch.Tensor]:",
            "input_tensors = (",
            "-        torch.randn((batch_size, *input_info.size))",
            "+        torch.randn(*input_info.size)",
            "if input_info.dtype is DataType.FLOAT32",
            "else torch.randint(",
            "-            size=(batch_size, *input_info.size),",
            "+            size=input_info.size,",
            "low=input_info.min_value or 0,",
            "high=input_info.max_value or 100,",
            ")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=4, insert_id=650928)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=list_splat), position=1)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=), text=)), position=2)",
            "Move(target_node=ASTNode(type=keyword_argument), node=ASTNode(type=attribute), position=2)",
            "Delete(target_node=ASTNode(type=identifier, text=batch_size))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=identifier, text=int))",
            "Delete(target_node=ASTNode(type=type))",
            "Delete(target_node=ASTNode(type=typed_parameter))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=batch_size))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=tuple))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=batch_size))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=*, text=*))",
            "Delete(target_node=ASTNode(type=list_splat))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=tuple))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 22,
        "number": 5002,
        "neg_line": [
            "-batch_size: int, input_infos: List[InputInfo]",
            "-torch.randn((batch_size, *input_info.size))",
            "-size=(batch_size, *input_info.size),"
        ],
        "pos_line": [
            "+input_infos: List[InputInfo],",
            "+torch.randn(*input_info.size)",
            "+size=input_info.size,"
        ],
        "core_change": "-batch_size: int, input_infos: List[InputInfo] +input_infos: List[InputInfo], -torch.randn((batch_size, *input_info.size)) +torch.randn(*input_info.size) -size=(batch_size, *input_info.size), +size=input_info.size,",
        "core_API": "randn"
    },
    {
        "commit_hash": "b15ccd98907c92835ac83dbf0cc0d29af439ebb7",
        "index": "14c4d67f..238d8c3c 100644",
        "commit_message": "GH-316: minor fixes\n\n",
        "file": "flair.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "cuda",
        "change": [
            "class LanguageModel(nn.Module):",
            "hidden = hidden",
            "",
            "input = torch.tensor(self.dictionary.get_idx_for_item(prefix[-1])).unsqueeze(0).unsqueeze(0)",
            "-            if torch.cuda.is_available():",
            "-                input = input.cuda()",
            "",
            "for i in range(number_of_characters):",
            "+",
            "+                if torch.cuda.is_available():",
            "+                    input = input.cuda()",
            "+",
            "prediction, _, hidden = self.forward(input, hidden)",
            "prediction /= temperature",
            "word_weights = prediction.squeeze().data.div(1.0).exp().cpu()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('for_statement', None), position=3, insert_id=1833599)",
            "Insert(target_node=IN(type=for_statement), node=('for', 'for'), position=0, insert_id=1833600)",
            "Insert(target_node=IN(type=for_statement), node=('identifier', 'i'), position=1, insert_id=1833601)",
            "Insert(target_node=IN(type=for_statement), node=('in', 'in'), position=2, insert_id=1833602)",
            "Move(target_node=IN(type=for_statement), node=ASTNode(type=call), position=3)",
            "Insert(target_node=IN(type=for_statement), node=(':', ':'), position=4, insert_id=1833603)",
            "Insert(target_node=IN(type=for_statement), node=('block', None), position=5, insert_id=1833604)",
            "Move(target_node=IN(type=block), node=ASTNode(type=if_statement), position=0)",
            "Delete(target_node=ASTNode(type=for, text=for))",
            "Delete(target_node=ASTNode(type=identifier, text=i))",
            "Delete(target_node=ASTNode(type=in, text=in))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=block, text=))",
            "Delete(target_node=ASTNode(type=for_statement))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 14,
        "number": 5004,
        "neg_line": [
            "-if torch.cuda.is_available():",
            "-input = input.cuda()"
        ],
        "pos_line": [
            "+",
            "+if torch.cuda.is_available():",
            "+input = input.cuda()",
            "+"
        ],
        "core_change": "-if torch.cuda.is_available(): -input = input.cuda() + +if torch.cuda.is_available(): +input = input.cuda() +",
        "core_API": "tensor"
    },
    {
        "commit_hash": "a0ed4151c036d9d860e1ce8b9f5eee4f04092c4b",
        "index": "0818d90..ea9bd8c 100644",
        "commit_message": "[transformer] Format & Test Refactoring (#1325)\n\n* try PyTorch custom TestCase class\n\n* revert\n\n* initial working example\n\n* update\n\n* data utils\n\n* fix imports\n\n* hardcode backend to nccl\n\n* fix signature\n\n* fix typo\n\n* mapping\n\n* set device\n\n* init\n\n* refactor x entropy\n\n* remove unused import & destroy model parallel\n\n* refactor random\n\n* fix test\n\n* remove migrated tests\n\n* refactor\n\n* init\n\n* separate affine weight init\n\n* init model parallel\n\n* split more\n\n* weight init fix part 1\n\n* use cpu init for consistency btwn native and tensor parallel\n\n* black\n\n* add col parallel\n\n* use a 3D tensor of square matrix for column parallel linear\n\n* skip the failing cases\n\n* migrate layers test\n\n* pipeline parallel forward/backward\n\n* fix typo\n\n* fix typo\n\n* fix\n\n* fix pipeline world size\n\n* black\n\n* rm `run_pipeline_parallel_test` in favor of test_pipeline_parallel_fwd_bwd.py\n\n* stop logging\n\n* set log level\n\n* black\n\n* license and format\n\n* fix\n\n* skip tf32 as matrices are small\n\n* remove potentially inappropriate license\n\n* Apply suggestions from code review\n\n* remove `TODO` comment\n\n* `torch.testing.assert_allclose` -> `torch.testing.assert_close`\n\n* remove comment-outs\n\n* remote unused import\n\n* minor fix\n",
        "file": "apex.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestBatchSamplerBehavior(unittest.TestCase):",
            "_micro_batch_size=_micro_batch_size,",
            "_global_batch_size=global_batch_size,",
            "))",
            "-            # print(batch)",
            "-            # print(microbatches)",
            "self.assertEqual(len(microbatches), global_batch_size // _micro_batch_size)",
            "self.assertEqual(len(microbatches[0][0]), _micro_batch_size)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 0,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 5005,
        "neg_line": [
            "-# print(batch)",
            "-# print(microbatches)"
        ],
        "pos_line": [],
        "core_change": "-# print(batch) -# print(microbatches)",
        "core_API": "assertEqual"
    },
    {
        "commit_hash": "9c6a02f2645db0043166c286cf01480ce1213c25",
        "index": "5129c054..8bbccfb5 100644",
        "commit_message": "fix tests\n\n",
        "file": "flair.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TransformerEmbeddings(TransformerBaseEmbeddings):",
            "def _forward_tensors(self, tensors) -> Dict[str, torch.Tensor]:",
            "return self.forward(**tensors)",
            "",
            "-    def export_onnx(self, path: Union[str, Path], example_sentences: List[Sentence], **kwargs) -> TransformerOnnxEmbeddings:",
            "+    def export_onnx(",
            "+        self, path: Union[str, Path], example_sentences: List[Sentence], **kwargs",
            "+    ) -> TransformerOnnxEmbeddings:",
            "\"\"\"",
            "Export TransformerEmbeddings to OnnxFormat.",
            ":param example_sentences: a list of sentences that will be used for tracing. It is recommended to take 2-4"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 5006,
        "neg_line": [
            "-def export_onnx(self, path: Union[str, Path], example_sentences: List[Sentence], **kwargs) -> TransformerOnnxEmbeddings:"
        ],
        "pos_line": [
            "+def export_onnx(",
            "+self, path: Union[str, Path], example_sentences: List[Sentence], **kwargs",
            "+) -> TransformerOnnxEmbeddings:"
        ],
        "core_change": "-def export_onnx(self, path: Union[str, Path], example_sentences: List[Sentence], **kwargs) -> TransformerOnnxEmbeddings: +def export_onnx( +self, path: Union[str, Path], example_sentences: List[Sentence], **kwargs +) -> TransformerOnnxEmbeddings:",
        "core_API": "forward"
    },
    {
        "commit_hash": "dd1cb0562d46951cadc5c40d4b525ba9420c141e",
        "index": "20fa717d87..ad0ae75b1f 100644",
        "commit_message": "fixed failing core.container unit tests.\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def random_uniform(low: float = 0.0, high: float = 1.0, shape: Optional[List[int",
            "true_shape: List[int] = []",
            "else:",
            "true_shape: List[int] = shape",
            "-    return _torch.rand(true_shape, device=default_device(dev).replace('gpu', 'cuda')) * rand_range + low",
            "+    return _torch.rand(true_shape, device=default_device(dev)) * rand_range + low",
            "",
            "",
            "def random_normal(mean: float = 0.0, std: float = 1.0, shape: Optional[List[int]] = None, dev: ivy.Device = None):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=replace))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=string, text='gpu'))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=string, text='cuda'))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 5007,
        "neg_line": [
            "-return _torch.rand(true_shape, device=default_device(dev).replace('gpu', 'cuda')) * rand_range + low"
        ],
        "pos_line": [
            "+return _torch.rand(true_shape, device=default_device(dev)) * rand_range + low"
        ],
        "core_change": "-return _torch.rand(true_shape, device=default_device(dev).replace('gpu', 'cuda')) * rand_range + low +return _torch.rand(true_shape, device=default_device(dev)) * rand_range + low",
        "core_API": "rand"
    },
    {
        "commit_hash": "c0f44f74ab5b7505bc6a8ca18ce05e9afaf67699",
        "index": "114f4781..4c7706ae 100644",
        "commit_message": "Fixed minor error while calculating span accuracy (#2923)\n\n* Fixed minor error while calculating span accuracy\n\n* Added checks for input shape in BooleanAccuracy\n\n* Fixed label shape in QaNet\n\n* Use f-string\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class QaNet(Model):",
            "self._span_start_accuracy(span_start_logits, span_start.squeeze(-1))",
            "loss += nll_loss(util.masked_log_softmax(span_end_logits, passage_mask), span_end.squeeze(-1))",
            "self._span_end_accuracy(span_end_logits, span_end.squeeze(-1))",
            "-            self._span_accuracy(best_span, torch.stack([span_start, span_end], -1))",
            "+            self._span_accuracy(best_span, torch.cat([span_start, span_end], -1))",
            "output_dict[\"loss\"] = loss",
            "",
            "# Compute the EM and F1 on SQuAD and add the tokenized input to the output."
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=stack), value='cat')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 5013,
        "neg_line": [
            "-self._span_accuracy(best_span, torch.stack([span_start, span_end], -1))"
        ],
        "pos_line": [
            "+self._span_accuracy(best_span, torch.cat([span_start, span_end], -1))"
        ],
        "core_change": "-self._span_accuracy(best_span, torch.stack([span_start, span_end], -1)) +self._span_accuracy(best_span, torch.cat([span_start, span_end], -1))",
        "core_API": "_span_start_accuracy"
    },
    {
        "commit_hash": "54c48f5b694e08b5ceeac17ee84c2bc70bd90f04",
        "index": "f8e311f..c2393e8 100644",
        "commit_message": "fixed bugs chatbot\n\n",
        "file": "stanford-tensorflow-tutorials.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def chat():",
            "model = ChatBotModel(True, batch_size=1)",
            "model.build_graph()",
            "",
            "-    # saver = tf.train.import_meta_graph('checkpoints/chatbot-30.meta')",
            "-",
            "saver = tf.train.Saver()",
            "",
            "with tf.Session() as sess:"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 5014,
        "neg_line": [
            "-# saver = tf.train.import_meta_graph('checkpoints/chatbot-30.meta')",
            "-"
        ],
        "pos_line": [],
        "core_change": "-# saver = tf.train.import_meta_graph('checkpoints/chatbot-30.meta') -",
        "core_API": "build_graph"
    },
    {
        "commit_hash": "8e861e75ed9afee06a046c24539fad5535e5c578",
        "index": "5b96b66b..5e1cc66a 100644",
        "commit_message": "pytorch 1.0 (#2165)\n\n* fixes\n\n* allenlp\n\n* fix bug\n\n* reduce=False -> reduction='none'\n\n* fix _sparse_mask\n\n* remove print statement\n\n* fix more tests\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def get_dropout_mask(dropout_probability: float, tensor_for_masking: torch.Tenso",
            "This scaling ensures expected values and variances of the output of applying this mask",
            "and the original tensor are the same.",
            "\"\"\"",
            "-    binary_mask = tensor_for_masking.new_tensor(torch.rand(tensor_for_masking.size()) > dropout_probability)",
            "+    binary_mask = (torch.rand(tensor_for_masking.size()) > dropout_probability).to(tensor_for_masking.device)",
            "# Scale mask by 1/keep_prob to preserve output statistics.",
            "dropout_mask = binary_mask.float().div(1.0 - dropout_probability)",
            "return dropout_mask"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=32049)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=32050)",
            "Insert(target_node=IN(type=attribute), node=('parenthesized_expression', None), position=0, insert_id=32051)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=32052)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to'), position=2, insert_id=32053)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=32054)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=32055)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=32056)",
            "Move(target_node=IN(type=parenthesized_expression), node=ASTNode(type=(, text=(), position=0)",
            "Move(target_node=IN(type=parenthesized_expression), node=ASTNode(type=comparison_operator), position=1)",
            "Move(target_node=IN(type=parenthesized_expression), node=ASTNode(type=), text=)), position=2)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tensor_for_masking'), position=0, insert_id=32057)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=32058)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=32059)",
            "Delete(target_node=ASTNode(type=identifier, text=tensor_for_masking))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=new_tensor))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 19,
        "number": 5018,
        "neg_line": [
            "-binary_mask = tensor_for_masking.new_tensor(torch.rand(tensor_for_masking.size()) > dropout_probability)"
        ],
        "pos_line": [
            "+binary_mask = (torch.rand(tensor_for_masking.size()) > dropout_probability).to(tensor_for_masking.device)"
        ],
        "core_change": "-binary_mask = tensor_for_masking.new_tensor(torch.rand(tensor_for_masking.size()) > dropout_probability) +binary_mask = (torch.rand(tensor_for_masking.size()) > dropout_probability).to(tensor_for_masking.device)",
        "core_API": "new_tensor"
    },
    {
        "commit_hash": "7b83a847cc61a25b468f29063cb6425beef6fff6",
        "index": "0d62c1b..b372d90 100644",
        "commit_message": "Various fixes\n\n",
        "file": "cnn-text-classification-tf.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TextCNN(object):",
            "# Final (unnormalized) scores and predictions",
            "with tf.name_scope(\"output\"):",
            "W = tf.Variable(tf.truncated_normal([num_filters_total, num_classes], stddev=0.1), name=\"W\")",
            "-            b = tf.Variable(tf.constant(0.1, shape=[num_classes]))",
            "+            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")",
            "self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")",
            "self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1910898)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1910899)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'name'), position=0, insert_id=1910900)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1910901)",
            "Insert(target_node=IN(type=keyword_argument), node=('string', '\"b\"'), position=2, insert_id=1910902)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 5021,
        "neg_line": [
            "-b = tf.Variable(tf.constant(0.1, shape=[num_classes]))"
        ],
        "pos_line": [
            "+b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")"
        ],
        "core_change": "-b = tf.Variable(tf.constant(0.1, shape=[num_classes])) +b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")",
        "core_API": "name_scope"
    },
    {
        "commit_hash": "ba7426366366e266cd21f30b07bd611720b13044",
        "index": "07be354e0..b691def6d 100644",
        "commit_message": "fix rnn inference\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class NaiveRNNDP(AbsSVS):",
            "before_outs.transpose(1, 2)",
            ").transpose(1, 2)",
            "",
            "-        return after_outs, None, None  # outs, probs, att_ws",
            "+        return dict(",
            "+            feat_gen=after_outs[0], prob=None, att_w=None",
            "+        )  # outs, probs, att_ws",
            "",
            "def _integrate_with_spk_embed(",
            "self, hs: torch.Tensor, spembs: torch.Tensor"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=return_statement), node=('call', None), position=1, insert_id=119805)",
            "Insert(target_node=IN(type=call), node=('identifier', 'dict'), position=0, insert_id=119806)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=119807)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=119808)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=1, insert_id=119809)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=,, text=,), position=2)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=3, insert_id=119810)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=,, text=,), position=4)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=5, insert_id=119811)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=6, insert_id=119812)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'feat_gen'), position=0, insert_id=119813)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=119814)",
            "Insert(target_node=IN(type=keyword_argument), node=('subscript', None), position=2, insert_id=119815)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'prob'), position=0, insert_id=119816)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=119817)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=none, text=None), position=2)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'att_w'), position=0, insert_id=119818)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=119819)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=none, text=None), position=2)",
            "Move(target_node=IN(type=subscript), node=ASTNode(type=identifier, text=after_outs), position=0)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=119820)",
            "Insert(target_node=IN(type=subscript), node=('integer', '0'), position=2, insert_id=119821)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=119822)",
            "Delete(target_node=ASTNode(type=expression_list))"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 24,
        "number": 5023,
        "neg_line": [
            "-return after_outs, None, None  # outs, probs, att_ws"
        ],
        "pos_line": [
            "+return dict(",
            "+feat_gen=after_outs[0], prob=None, att_w=None",
            "+)  # outs, probs, att_ws"
        ],
        "core_change": "-return after_outs, None, None  # outs, probs, att_ws +return dict( +feat_gen=after_outs[0], prob=None, att_w=None +)  # outs, probs, att_ws",
        "core_API": "transpose"
    },
    {
        "commit_hash": "0c166dcfb8b9e9d79d013b7967a59dfb1ac4b804",
        "index": "d8bcbb43..57fbfb18 100644",
        "commit_message": "define and apply YAPF formatting  (#1039)\n\n* define YAPF\n\n* yapf kornia -rip\n\n* yapf examples -rip\n\n* yapf test -rip\n\n* Update CONTRIBUTING.rst\n\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\n\n* update config\n\n* fix 4lint\n\n* mypy\n\n* mypy\n\n* mypy\n\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class UnsharpMask(nn.Module):",
            "torch.Size([2, 4, 5, 5])",
            "\"\"\"",
            "",
            "-    def __init__(self, kernel_size: Tuple[int, int],",
            "-                 sigma: Tuple[float, float],",
            "-                 border_type: str = 'reflect') -> None:",
            "+    def __init__(self, kernel_size: Tuple[int, int], sigma: Tuple[float, float], border_type: str = 'reflect') -> None:",
            "super(UnsharpMask, self).__init__()",
            "self.kernel_size: Tuple[int, int] = kernel_size",
            "self.sigma: Tuple[float, float] = sigma"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 17,
        "number": 5024,
        "neg_line": [
            "-def __init__(self, kernel_size: Tuple[int, int],",
            "-sigma: Tuple[float, float],",
            "-border_type: str = 'reflect') -> None:"
        ],
        "pos_line": [
            "+def __init__(self, kernel_size: Tuple[int, int], sigma: Tuple[float, float], border_type: str = 'reflect') -> None:"
        ],
        "core_change": "-def __init__(self, kernel_size: Tuple[int, int], -sigma: Tuple[float, float], -border_type: str = 'reflect') -> None: +def __init__(self, kernel_size: Tuple[int, int], sigma: Tuple[float, float], border_type: str = 'reflect') -> None:",
        "core_API": "Size"
    },
    {
        "commit_hash": "b610c47f8953c359f2cff677907912779e0f478f",
        "index": "a95fafd98..3969819e8 100644",
        "commit_message": "[MaskFormer] Add support for ResNet backbone (#20483)\n\n* Add SwinBackbone\n\n* Add hidden_states_before_downsampling support\n\n* Fix Swin tests\n\n* Improve conversion script\n\n* Add id2label mappings\n\n* Add vistas mapping\n\n* Update comments\n\n* Fix backbone\n\n* Improve tests\n\n* Extend conversion script\n\n* Add Swin conversion script\n\n* Fix style\n\n* Revert config attribute\n\n* Remove SwinBackbone from main init\n\n* Remove unused attribute\n\n* Use encoder for ResNet backbone\n\n* Improve conversion script and add integration test\n\n* Apply suggestion\n\nCo-authored-by: Niels Rogge <nielsrogge@Nielss-MacBook-Pro.local>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "models",
        "change": [
            "class RegNetPreTrainedModel(PreTrainedModel):",
            "main_input_name = \"pixel_values\"",
            "supports_gradient_checkpointing = True",
            "",
            "+    # Copied from transformers.models.resnet.modeling_resnet.ResNetPreTrainedModel._init_weights",
            "def _init_weights(self, module):",
            "if isinstance(module, nn.Conv2d):",
            "nn.init.kaiming_normal_(module.weight, mode=\"fan_out\", nonlinearity=\"relu\")"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 17,
        "number": 5025,
        "neg_line": [],
        "pos_line": [
            "+# Copied from transformers.models.resnet.modeling_resnet.ResNetPreTrainedModel._init_weights"
        ],
        "core_change": "+# Copied from transformers.models.resnet.modeling_resnet.ResNetPreTrainedModel._init_weights",
        "core_API": "kaiming_normal_"
    },
    {
        "commit_hash": "d215aa258e77f43b6c36950bc9a484d5d849eb4f",
        "index": "8e2a6279..50ebad72 100644",
        "commit_message": "fix torch.nonzero usage in pytorch 1.5 (#2602)\n\n\n",
        "file": "mmdetection.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class BBoxHead(nn.Module):",
            "",
            "bboxes_list = []",
            "for i in range(len(img_metas)):",
            "-            inds = torch.nonzero(rois[:, 0] == i).squeeze(dim=1)",
            "+            inds = torch.nonzero(",
            "+                rois[:, 0] == i, as_tuple=False).squeeze(dim=1)",
            "num_rois = inds.numel()",
            "",
            "bboxes_ = rois[inds, 1:]"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=638804)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=638805)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'as_tuple'), position=0, insert_id=638806)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=638807)",
            "Insert(target_node=IN(type=keyword_argument), node=('false', 'False'), position=2, insert_id=638808)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 5029,
        "neg_line": [
            "-inds = torch.nonzero(rois[:, 0] == i).squeeze(dim=1)"
        ],
        "pos_line": [
            "+inds = torch.nonzero(",
            "+rois[:, 0] == i, as_tuple=False).squeeze(dim=1)"
        ],
        "core_change": "-inds = torch.nonzero(rois[:, 0] == i).squeeze(dim=1) +inds = torch.nonzero( +rois[:, 0] == i, as_tuple=False).squeeze(dim=1)",
        "core_API": "nonzero"
    },
    {
        "commit_hash": "49bac724c00ef5e2370b4a084e10fd1ed04b500f",
        "index": "4430d9ff..816813c8 100644",
        "commit_message": "Implement VitsAudioConfig (#1556)\n\n* Implement VitsAudioConfig\n\n* Update VITS LJSpeech recipe\n\n* Update VITS VCTK recipe\n\n* Make style\n\n* Add missing decorator\n\n* Add missing param\n\n* Make style\n\n* Update recipes\n\n* Fix test\n\n* Bug fix\n\n* Exclude tests folder\n\n* Make linter\n\n* Make style\n",
        "file": "TTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SSIMLoss(torch.nn.Module):",
            "",
            "if ssim_loss.item() < 0.0:",
            "print(f\" > SSIM loss is out-of-range {ssim_loss.item()}, setting it 0.0\")",
            "-            ssim_loss =  torch.tensor([0.0])",
            "+            ssim_loss = torch.tensor([0.0])",
            "",
            "return ssim_loss"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 5030,
        "neg_line": [
            "-ssim_loss =  torch.tensor([0.0])"
        ],
        "pos_line": [
            "+ssim_loss = torch.tensor([0.0])"
        ],
        "core_change": "-ssim_loss =  torch.tensor([0.0]) +ssim_loss = torch.tensor([0.0])",
        "core_API": "item"
    },
    {
        "commit_hash": "194bf736c6480cda52f711e5ea097d1cd5d3e498",
        "index": "884fcf44..9a711031 100644",
        "commit_message": "Fix RaggedTensorSpec loading when reviving model form SavedModel.\n\nA bunch of arguments were not passed to the __init__ making the spec incorrect.\n\nPiperOrigin-RevId: 382692661\n\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def infer_inputs_from_restored_call_function(fn):",
            "if isinstance(x, tf.SparseTensorSpec):",
            "return tf.SparseTensorSpec(common_shape, x.dtype)",
            "elif isinstance(x, tf.RaggedTensorSpec):",
            "-      return tf.RaggedTensorSpec(common_shape, x.dtype)",
            "+      return tf.RaggedTensorSpec(",
            "+          common_shape,",
            "+          x.dtype,",
            "+          ragged_rank=x.ragged_rank,",
            "+          row_splits_dtype=x.row_splits_dtype,",
            "+          flat_values_spec=x.flat_values_spec)",
            "return tf.TensorSpec(common_shape, x.dtype, x.name)",
            "",
            "spec = fn.concrete_functions[0].structured_input_signature"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=2084914)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=2084915)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=6, insert_id=2084916)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=7, insert_id=2084917)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=8, insert_id=2084918)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=9, insert_id=2084919)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'ragged_rank'), position=0, insert_id=2084920)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2084921)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=2084922)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'row_splits_dtype'), position=0, insert_id=2084923)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2084924)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=2084925)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'flat_values_spec'), position=0, insert_id=2084926)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2084927)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=2084928)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'x'), position=0, insert_id=2084929)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2084930)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'ragged_rank'), position=2, insert_id=2084931)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'x'), position=0, insert_id=2084932)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2084933)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'row_splits_dtype'), position=2, insert_id=2084934)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'x'), position=0, insert_id=2084935)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2084936)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'flat_values_spec'), position=2, insert_id=2084937)"
        ],
        "plus_line": 6,
        "minus_line": 1,
        "AST_diff_line": 24,
        "number": 5036,
        "neg_line": [
            "-return tf.RaggedTensorSpec(common_shape, x.dtype)"
        ],
        "pos_line": [
            "+return tf.RaggedTensorSpec(",
            "+common_shape,",
            "+x.dtype,",
            "+ragged_rank=x.ragged_rank,",
            "+row_splits_dtype=x.row_splits_dtype,",
            "+flat_values_spec=x.flat_values_spec)"
        ],
        "core_change": "-return tf.RaggedTensorSpec(common_shape, x.dtype) +return tf.RaggedTensorSpec( +common_shape, +x.dtype, +ragged_rank=x.ragged_rank, +row_splits_dtype=x.row_splits_dtype, +flat_values_spec=x.flat_values_spec)",
        "core_API": "SparseTensorSpec"
    },
    {
        "commit_hash": "2fd719d00ae7b26ed80b3345c5eb0264685e95aa",
        "index": "975daea8..2630326b 100644",
        "commit_message": "Fix masked losses.\n\nMasked losses with the default \"auto\" reduction were giving outputs that are\ninconsistent with what you would get from a ragged input. Masked and Ragged are two different representations of the same thing (when it can be represented as ragged).  These should match.\n\nThe (input_type='masked', reduction='auto') case fails (doesn't match the ragged case) before this change.\n\nThe existing tests, where I'm changing the expected values are because I believe the old values are incorrect.\n\nPiperOrigin-RevId: 493003876\n\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def apply_mask(y_p, sw, mask):",
            "if mask is not None:",
            "mask = tf.cast(mask, y_p.dtype)",
            "if sw is not None:",
            "+            sw = tf.cast(sw, mask.dtype)",
            "mask, _, sw = squeeze_or_expand_dimensions(mask, sample_weight=sw)",
            "sw *= mask",
            "else:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('block', None), position=0, insert_id=2045171)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=2045172)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=2045173)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'sw'), position=0, insert_id=2045174)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=2045175)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=2045176)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2045177)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2045178)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2045179)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2045180)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cast'), position=2, insert_id=2045181)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2045182)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'sw'), position=1, insert_id=2045183)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=2045184)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=3, insert_id=2045185)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=2045186)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'mask'), position=0, insert_id=2045187)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2045188)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dtype'), position=2, insert_id=2045189)",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 20,
        "number": 5037,
        "neg_line": [],
        "pos_line": [
            "+sw = tf.cast(sw, mask.dtype)"
        ],
        "core_change": "+sw = tf.cast(sw, mask.dtype)",
        "core_API": "cast"
    },
    {
        "commit_hash": "88ff1953e94cfbbcd53f4d659a1099fce8a7344e",
        "index": "e8bbe966..7800a918 100644",
        "commit_message": "Input alias deprecation warning fixes - related to issue #479 (#480)\n\n* __init__ import error message repositioned at a more appropriate location\n\n* gitignore updated with venv environment\n\n* Typo fixed in tensorlayer.layers.convolution.py\n\n* Deprecation warning added for tl.layer.deconv2d with backward compatibility restored - Issue #479\n\n* Deprecation warning added for Layer API change: `layer` argument changed to `prev_layer` - Issue #479\nAdditional Modification using the syntax super for inheritance - more pythonic\n\n* test layers extend with argument names precised\n\n* tl.layers.core.py forgotten Classes with deprecation\n\n* Error fix in deprecation warning tl.layers.spatial_transformer.py\n\n* Test refactored and fix with arguments missing added\n\n* ReshapeLayer error fix\n\n* test_layers_normalization argument name missing fixed\n\n* error in tl.layers.ReshapeLayer test if shape is not empty fixed\n\n* test_layers_special_activation missing argument name fixed\n\n* test_layers_stack missing argument name fixed\n\n* test_layers_super_resolution missing argument name fixed\n\n* test_models missing argument name fixed\n\n* Formating error fixed\n\n* Decorator for deprecated argument added\n\n* Deprecation API Change - Use of newly implemented decorator. Docstring updated\n\n* Codacy Issues Fix\n\n* Unnecessary PR changes removed - PR Cleaned & Refactored\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "layers",
        "change": [
            "with tf.Graph().as_default() as graph:",
            "# vgg1.restore_params(sess)",
            "",
            "if len(vgg1.all_layers) != 21:",
            "-        raise Exception(\"layers dont match\")",
            "+        raise Exception(\"layers do not match\")",
            "",
            "if len(vgg1.all_params) != 30:",
            "-        raise Exception(\"params dont match\")",
            "+        raise Exception(\"params do not match\")"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"layers dont match\"), value='\"layers do not match\"')",
            "Update(target_node=ASTNode(type=string, text=\"params dont match\"), value='\"params do not match\"')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 5039,
        "neg_line": [
            "-raise Exception(\"layers dont match\")",
            "-raise Exception(\"params dont match\")"
        ],
        "pos_line": [
            "+raise Exception(\"layers do not match\")",
            "+raise Exception(\"params do not match\")"
        ],
        "core_change": "-raise Exception(\"layers dont match\") +raise Exception(\"layers do not match\") -raise Exception(\"params dont match\") +raise Exception(\"params do not match\")",
        "core_API": "Graph"
    },
    {
        "commit_hash": "275dd70a1bc1f71867666e20fb28168ac8104832",
        "index": "19e1ee56f..3e46c8dc1 100644",
        "commit_message": "[CI/Data] Fix `test_huggingface` failing (#30965)\n\nThe 'emotion' dataset used before has been removed from its upload location. This PR replaces it with an alternative dataset.\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "from ray.tests.conftest import *  # noqa",
            "",
            "",
            "def test_huggingface(ray_start_regular_shared):",
            "-    data = datasets.load_dataset(\"emotion\")",
            "+    data = datasets.load_dataset(\"tweet_eval\", \"emotion\")",
            "",
            "assert isinstance(data, datasets.DatasetDict)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('string', '\"tweet_eval\"'), position=1, insert_id=1802055)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1802056)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 5042,
        "neg_line": [
            "-data = datasets.load_dataset(\"emotion\")"
        ],
        "pos_line": [
            "+data = datasets.load_dataset(\"tweet_eval\", \"emotion\")"
        ],
        "core_change": "-data = datasets.load_dataset(\"emotion\") +data = datasets.load_dataset(\"tweet_eval\", \"emotion\")",
        "core_API": "load_dataset"
    },
    {
        "commit_hash": "17e058b68f2e93c4bf183006b9269d93c7a38fcb",
        "index": "490ed07fb..7ea39ecb1 100755",
        "commit_message": "Fixes wrong imports\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def main():",
            "np.random.seed(args.seed)",
            "",
            "if args.backend == \"pytorch\":",
            "-        from tts.pytorch.tts_pytorch import train",
            "+        fromespnet.lmpytorch.tts_pytorch import train",
            "train(args)",
            "else:",
            "raise NotImplementedError(\"Only pytorch is supported.\")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=if_statement), node=('ERROR', None), position=3, insert_id=178924)",
            "Insert(target_node=ASTNode(type=if_statement), node=('block', None), position=4, insert_id=178925)",
            "Insert(target_node=IN(type=ERROR), node=('attribute', None), position=0, insert_id=178926)",
            "Insert(target_node=IN(type=block), node=('import_statement', None), position=0, insert_id=178927)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=178928)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=tts_pytorch), position=2)",
            "Move(target_node=IN(type=import_statement), node=ASTNode(type=import, text=import), position=0)",
            "Move(target_node=IN(type=import_statement), node=ASTNode(type=dotted_name), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=tts), value='fromespnet')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=tts), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=pytorch), value='lmpytorch')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=pytorch), position=2)",
            "Delete(target_node=ASTNode(type=from, text=from))",
            "Delete(target_node=ASTNode(type=dotted_name))",
            "Delete(target_node=ASTNode(type=import_from_statement))",
            "Delete(target_node=ASTNode(type=block))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 18,
        "number": 5043,
        "neg_line": [
            "-from tts.pytorch.tts_pytorch import train"
        ],
        "pos_line": [
            "+fromespnet.lmpytorch.tts_pytorch import train"
        ],
        "core_change": "-from tts.pytorch.tts_pytorch import train +fromespnet.lmpytorch.tts_pytorch import train",
        "core_API": "seed"
    },
    {
        "commit_hash": "edb78974b84b324df08ec198e6ff7f267ee23d33",
        "index": "4d2344f4..244d1335 100644",
        "commit_message": "upgrade detection / some v2 compat fix\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class SyncMultiGPUReplicatedBuilder(DataParallelBuilder):",
            "Copy values of variables on GPU 0 to other GPUs.",
            "\"\"\"",
            "# literally all variables, because it's better to sync optimizer-internal variables as well",
            "-        all_vars = tf.global_variables() + tf.local_variables()",
            "+        all_vars = tfv1.global_variables() + tfv1.local_variables()",
            "var_by_name = {v.name: v for v in all_vars}",
            "-        trainable_names = {x.name for x in tf.trainable_variables()}",
            "+        trainable_names = {x.name for x in tfv1.trainable_variables()}",
            "post_init_ops = []",
            "",
            "def log_failure(name, reason):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=2273118)",
            "Update(target_node=ASTNode(type=identifier, text=tf), value='tfv1')",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2273119)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=2273120)",
            "Update(target_node=ASTNode(type=identifier, text=tf), value='tfv1')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=argument_list), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=tf), value='tfv1')",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 10,
        "number": 5045,
        "neg_line": [
            "-all_vars = tf.global_variables() + tf.local_variables()",
            "-trainable_names = {x.name for x in tf.trainable_variables()}"
        ],
        "pos_line": [
            "+all_vars = tfv1.global_variables() + tfv1.local_variables()",
            "+trainable_names = {x.name for x in tfv1.trainable_variables()}"
        ],
        "core_change": "-all_vars = tf.global_variables() + tf.local_variables() +all_vars = tfv1.global_variables() + tfv1.local_variables() -trainable_names = {x.name for x in tf.trainable_variables()} +trainable_names = {x.name for x in tfv1.trainable_variables()}",
        "core_API": "global_variables"
    },
    {
        "commit_hash": "f4463f55a15bd836248e2d736ae9212943f4a843",
        "index": "ce691c79..89ec9ca5 100644",
        "commit_message": "Add doc (#3125)\n\n* add doc\n\n* fix some typos\n\n* fix some doc\n\n* fix property doc\n\n* update doc\n\n* complete free anchor doc\n",
        "file": "mmdetection.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "import torch",
            "",
            "",
            "def cast_tensor_type(inputs, src_type, dst_type):",
            "+    \"\"\"Recursively convert Tensor in inputs from src_type to dst_type.",
            "+",
            "+    Args:",
            "+        inputs: Inputs that to be casted.",
            "+        src_type (torch.dtype): Source type..",
            "+        dst_type (torch.dtype): Destination type.",
            "+",
            "+    Returns:",
            "+        The same type with inputs, but all contained Tensors have been cast.",
            "+    \"\"\"",
            "if isinstance(inputs, torch.Tensor):",
            "return inputs.to(dst_type)",
            "elif isinstance(inputs, str):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('block', None), position=0, insert_id=636568)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=636569)",
            "Insert(target_node=IN(type=expression_statement), node=('string', '\"\"\"Recursively convert Tensor in inputs from src_type to dst_type.\\n\\n    Args:\\n        inputs: Inputs that to be casted.\\n        src_type (torch.dtype): Source type..\\n        dst_type (torch.dtype): Destination type.\\n\\n    Returns:\\n        The same type with inputs, but all contained Tensors have been cast.\\n    \"\"\"'), position=0, insert_id=636570)",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 8,
        "minus_line": 0,
        "AST_diff_line": 4,
        "number": 5048,
        "neg_line": [],
        "pos_line": [
            "+\"\"\"Recursively convert Tensor in inputs from src_type to dst_type.",
            "+",
            "+Args:",
            "+inputs: Inputs that to be casted.",
            "+src_type (torch.dtype): Source type..",
            "+dst_type (torch.dtype): Destination type.",
            "+",
            "+Returns:",
            "+The same type with inputs, but all contained Tensors have been cast.",
            "+\"\"\""
        ],
        "core_change": "+\"\"\"Recursively convert Tensor in inputs from src_type to dst_type. + +Args: +inputs: Inputs that to be casted. +src_type (torch.dtype): Source type.. +dst_type (torch.dtype): Destination type. + +Returns: +The same type with inputs, but all contained Tensors have been cast. +\"\"\"",
        "core_API": "to"
    },
    {
        "commit_hash": "27c8143c671b25740a8a587b96e182bb7fa4ba6f",
        "index": "812a580a..51d50536 100644",
        "commit_message": "fix typing callable in load storage (#1768)\n\n* fix typing callable in load storage\n\n* missing import\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def _torch_solve_cast(input: torch.Tensor, A: torch.Tensor) -> Tuple[torch.Tenso",
            "if dtype not in (torch.float32, torch.float64):",
            "dtype = torch.float32",
            "",
            "-    out = solve(A.to(dtype), input.to(dtype))",
            "+    out = torch.linalg.solve(A.to(dtype), input.to(dtype))",
            "",
            "return (out.to(input.dtype), out)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=402700)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=402701)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=402702)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=solve), position=2)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=402703)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=402704)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'linalg'), position=2, insert_id=402705)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 5052,
        "neg_line": [
            "-out = solve(A.to(dtype), input.to(dtype))"
        ],
        "pos_line": [
            "+out = torch.linalg.solve(A.to(dtype), input.to(dtype))"
        ],
        "core_change": "-out = solve(A.to(dtype), input.to(dtype)) +out = torch.linalg.solve(A.to(dtype), input.to(dtype))",
        "core_API": "to"
    },
    {
        "commit_hash": "a24ec204ae930af4c4bc9d80c0545bdb5e4e1449",
        "index": "8810a0f7b..1c8c97be1 100644",
        "commit_message": "Fix test\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_calculate_plot_attention():",
            "",
            "train_args = make_train_args(report_cer=True)",
            "",
            "-    model, x, ilens, y, data = prepare(train_args)",
            "+    model, x, ilens, y, data, uttid_list = prepare(train_args)",
            "",
            "attn_dict = model.calculate_all_attentions(x[0:1], ilens[0:1], y[0:1])",
            "-    plot.plot_multi_head_attention(data, attn_dict, \"/tmp/espnet-test\")",
            "+    plot.plot_multi_head_attention(data, uttid_list, attn_dict, \"/tmp/espnet-test\")",
            "",
            "",
            "def test_invalid_input_layer_type():"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=pattern_list), node=(',', ','), position=9, insert_id=142879)",
            "Insert(target_node=ASTNode(type=pattern_list), node=('identifier', 'uttid_list'), position=10, insert_id=142880)",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'uttid_list'), position=3, insert_id=142881)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=142882)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 4,
        "number": 5056,
        "neg_line": [
            "-model, x, ilens, y, data = prepare(train_args)",
            "-plot.plot_multi_head_attention(data, attn_dict, \"/tmp/espnet-test\")"
        ],
        "pos_line": [
            "+model, x, ilens, y, data, uttid_list = prepare(train_args)",
            "+plot.plot_multi_head_attention(data, uttid_list, attn_dict, \"/tmp/espnet-test\")"
        ],
        "core_change": "-model, x, ilens, y, data = prepare(train_args) +model, x, ilens, y, data, uttid_list = prepare(train_args) -plot.plot_multi_head_attention(data, attn_dict, \"/tmp/espnet-test\") +plot.plot_multi_head_attention(data, uttid_list, attn_dict, \"/tmp/espnet-test\")",
        "core_API": "calculate_all_attentions"
    },
    {
        "commit_hash": "e6537ba0be0427a7f6f156f56fbf2373ac89b37a",
        "index": "f6d80c2465..0143572264 100644",
        "commit_message": "fix torch frontend test `swapdims` (#3026)\n\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_torch_permute(",
            "),",
            "native_array=st.booleans(),",
            ")",
            "-@handle_cmd_line_args",
            "def test_torch_swapdims(",
            "dtype_and_values,",
            "dim0,"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=parameters), node=ASTNode(type=ERROR), position=7)",
            "Insert(target_node=ASTNode(type=ERROR), node=('parameters', None), position=1, insert_id=343125)",
            "Insert(target_node=ASTNode(type=ERROR), node=('def', 'def'), position=12, insert_id=343126)",
            "Insert(target_node=IN(type=parameters), node=('(', '('), position=0, insert_id=343127)",
            "Insert(target_node=IN(type=parameters), node=(')', ')'), position=1, insert_id=343128)",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=parameters))",
            "Delete(target_node=ASTNode(type=@, text=@))",
            "Delete(target_node=ASTNode(type=identifier, text=handle_cmd_line_args))",
            "Delete(target_node=ASTNode(type=identifier, text=def))"
        ],
        "plus_line": 0,
        "minus_line": 0,
        "AST_diff_line": 11,
        "number": 5062,
        "neg_line": [
            "-@handle_cmd_line_args"
        ],
        "pos_line": [],
        "core_change": "-@handle_cmd_line_args",
        "core_API": "booleans"
    },
    {
        "commit_hash": "27b51f5c8db9930e83de2eecd6b6f958e0720d9f",
        "index": "4e8b209..1d3cbdb 100644",
        "commit_message": "Fix tensorflow bugs\n\n",
        "file": "nebullvm.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TensorflowBaseInferenceLearner(BaseInferenceLearner, ABC):",
            "return \".npy\"",
            "",
            "def free_gpu_memory(self):",
            "-        tf.keras.clear_session()",
            "+        tf.keras.backend.clear_session()",
            "self._is_gpu_ready = False",
            "",
            "def set_model_on_gpu(self):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=2123473)",
            "Insert(target_node=ASTNode(type=attribute), node=('identifier', 'backend'), position=2, insert_id=2123474)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 3,
        "number": 5064,
        "neg_line": [
            "-tf.keras.clear_session()"
        ],
        "pos_line": [
            "+tf.keras.backend.clear_session()"
        ],
        "core_change": "-tf.keras.clear_session() +tf.keras.backend.clear_session()",
        "core_API": "clear_session"
    },
    {
        "commit_hash": "43043ee4d5c672f7fbc22ac9559e8164731fd053",
        "index": "4d891fbfaf..82b904bd13 100644",
        "commit_message": "[RLlib] Tf2x preparation; part 2 (upgrading `try_import_tf()`). (#9136)\n\n* WIP.\n\n* Fixes.\n\n* LINT.\n\n* WIP.\n\n* WIP.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* WIP.\n\n* Fixes.\n\n* Test\n\n* Fix.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* LINT.\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def run_timeline(sess, ops, debug_name, feed_dict={}, timeline_dir=None):",
            "if timeline_dir:",
            "from tensorflow.python.client import timeline",
            "",
            "-        run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)",
            "-        run_metadata = tf.RunMetadata()",
            "+        run_options = tf1.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)",
            "+        run_metadata = tf1.RunMetadata()",
            "start = time.time()",
            "fetches = sess.run(",
            "ops,"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tf), value='tf1')",
            "Update(target_node=ASTNode(type=identifier, text=tf), value='tf1')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 5070,
        "neg_line": [
            "-run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)",
            "-run_metadata = tf.RunMetadata()"
        ],
        "pos_line": [
            "+run_options = tf1.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)",
            "+run_metadata = tf1.RunMetadata()"
        ],
        "core_change": "-run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE) -run_metadata = tf.RunMetadata() +run_options = tf1.RunOptions(trace_level=tf.RunOptions.FULL_TRACE) +run_metadata = tf1.RunMetadata()",
        "core_API": "RunOptions"
    },
    {
        "commit_hash": "699341477123c2afe1881ab53ac7f8061e1b62f9",
        "index": "22d727cd5..d536978fc 100644",
        "commit_message": "fix dynamic quantize test for torch < 1.5\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_dynamic_quantization(train_dic, recog_dic, quantize_dic):",
            "train_args = get_default_train_args(**train_dic)",
            "recog_args = get_default_recog_args(**recog_dic)",
            "",
            "+    if not is_torch_1_5_plus:",
            "+        q_dtype = torch.qint8",
            "+    else:",
            "+        q_dtype = quantize_dic[\"mod\"]",
            "+",
            "model = E2E(idim, odim, train_args)",
            "model = torch.quantization.quantize_dynamic(",
            "-        model, quantize_dic[\"mod\"], dtype=quantize_dic[\"dtype\"]",
            "+        model, q_dtype, dtype=quantize_dic[\"dtype\"]",
            ")",
            "",
            "beam_search = BeamSearchTransducer("
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=3, insert_id=132922)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=132923)",
            "Insert(target_node=IN(type=if_statement), node=('not_operator', None), position=1, insert_id=132924)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=132925)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=132926)",
            "Insert(target_node=IN(type=if_statement), node=('else_clause', None), position=4, insert_id=132927)",
            "Insert(target_node=IN(type=not_operator), node=('not', 'not'), position=0, insert_id=132928)",
            "Insert(target_node=IN(type=not_operator), node=('identifier', 'is_torch_1_5_plus'), position=1, insert_id=132929)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=132930)",
            "Insert(target_node=IN(type=else_clause), node=('else', 'else'), position=0, insert_id=132931)",
            "Insert(target_node=IN(type=else_clause), node=(':', ':'), position=1, insert_id=132932)",
            "Insert(target_node=IN(type=else_clause), node=('block', None), position=2, insert_id=132933)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=132934)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=132935)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'q_dtype'), position=0, insert_id=132936)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=132937)",
            "Insert(target_node=IN(type=assignment), node=('attribute', None), position=2, insert_id=132938)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=132939)",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'q_dtype'), position=3, insert_id=132940)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=132941)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=132942)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'qint8'), position=2, insert_id=132943)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'q_dtype'), position=0, insert_id=132944)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=132945)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=subscript), position=2)"
        ],
        "plus_line": 5,
        "minus_line": 1,
        "AST_diff_line": 25,
        "number": 5072,
        "neg_line": [
            "-model, quantize_dic[\"mod\"], dtype=quantize_dic[\"dtype\"]"
        ],
        "pos_line": [
            "+if not is_torch_1_5_plus:",
            "+q_dtype = torch.qint8",
            "+else:",
            "+q_dtype = quantize_dic[\"mod\"]",
            "+",
            "+model, q_dtype, dtype=quantize_dic[\"dtype\"]"
        ],
        "core_change": "+if not is_torch_1_5_plus: +q_dtype = torch.qint8 +else: +q_dtype = quantize_dic[\"mod\"] + -model, quantize_dic[\"mod\"], dtype=quantize_dic[\"dtype\"] +model, q_dtype, dtype=quantize_dic[\"dtype\"]",
        "core_API": "quantize_dynamic"
    },
    {
        "commit_hash": "c05ace221339102abb1c8b0933b9cbfd51ddd4a3",
        "index": "c8c876ba8..62186ddfc 100644",
        "commit_message": "Use f-strings in the dataset scripts (#3291)\n\n* Finishes #3257\n\nUsed f-strings to format the .py files in the dataset folder\n\n* Fix style\n\n* Fix hkcancor dataset\n\nCo-authored-by: Mario ako <mario@huggingface.co>\nCo-authored-by: mariosasko <mariosasko777@gmail.com>\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class KannadaNews(datasets.GeneratorBasedBuilder):",
            "",
            "if not os.path.exists(path_to_manual_file):",
            "raise FileNotFoundError(",
            "-                \"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('kannada_news', data_dir=...)` that includes a file name {}. Manual download instructions: {})\".format(",
            "-                    path_to_manual_file, _TRAIN_FILENAME, self.manual_download_instructions",
            "-                )",
            "+                f\"{path_to_manual_file} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('kannada_news', data_dir=...)` that includes a file name {_TRAIN_FILENAME}. Manual download instructions: {self.manual_download_instructions})\"",
            ")",
            "return [",
            "datasets.SplitGenerator("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('string', 'f\"{path_to_manual_file} does not exist. Make sure you insert a manual dir via `datasets.load_dataset(\\'kannada_news\\', data_dir=...)` that includes a file name {_TRAIN_FILENAME}. Manual download instructions: {self.manual_download_instructions})\"'), position=1, insert_id=1781607)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=), text=)), position=2)",
            "Delete(target_node=ASTNode(type=string, text=\"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('kannada_news', data_dir=...)` that includes a file name {}. Manual download instructions: {})\"))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=format))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=path_to_manual_file))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=_TRAIN_FILENAME))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=manual_download_instructions))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=), text=)))"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 18,
        "number": 5074,
        "neg_line": [
            "-\"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('kannada_news', data_dir=...)` that includes a file name {}. Manual download instructions: {})\".format(",
            "-path_to_manual_file, _TRAIN_FILENAME, self.manual_download_instructions",
            "-)"
        ],
        "pos_line": [
            "+f\"{path_to_manual_file} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('kannada_news', data_dir=...)` that includes a file name {_TRAIN_FILENAME}. Manual download instructions: {self.manual_download_instructions})\""
        ],
        "core_change": "-\"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('kannada_news', data_dir=...)` that includes a file name {}. Manual download instructions: {})\".format( -path_to_manual_file, _TRAIN_FILENAME, self.manual_download_instructions -) +f\"{path_to_manual_file} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('kannada_news', data_dir=...)` that includes a file name {_TRAIN_FILENAME}. Manual download instructions: {self.manual_download_instructions})\"",
        "core_API": "exists"
    },
    {
        "commit_hash": "3facd518628a7225a7d97131748996ea8e9c6e70",
        "index": "92b5a42d..f4eadfb5 100644",
        "commit_message": "some small fix\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def is_training_name(name):",
            "This is only used to improve logging.",
            ":returns: guess whether this tensor is something only used in training.",
            "\"\"\"",
            "-    # TODO: maybe simply check against TRAINABLE_VARIABLES and EXTRA_SAVE_VARS_KEY ?",
            "+    # TODO: maybe simply check against TRAINABLE_VARIABLES and MODEL_VARIABLES?",
            "# TODO or use get_slot_names()",
            "name = get_op_tensor_name(name)[0]",
            "if name.endswith('/Adam') or name.endswith('/Adam_1'):"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 5077,
        "neg_line": [
            "-# TODO: maybe simply check against TRAINABLE_VARIABLES and EXTRA_SAVE_VARS_KEY ?"
        ],
        "pos_line": [
            "+# TODO: maybe simply check against TRAINABLE_VARIABLES and MODEL_VARIABLES?"
        ],
        "core_change": "-# TODO: maybe simply check against TRAINABLE_VARIABLES and EXTRA_SAVE_VARS_KEY ? +# TODO: maybe simply check against TRAINABLE_VARIABLES and MODEL_VARIABLES?",
        "core_API": "endswith"
    },
    {
        "commit_hash": "3537b69b6a98ec95f1bd2c920f74f14f89dbe070",
        "index": "fcd2446..cbe011e 100644",
        "commit_message": "fix ner_model_dir not in args bug\n\n",
        "file": "BERT-BiLSTM-CRF-NER.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def optimize_ner_model(args, num_labels,  logger=None):",
            "",
            "with tf.Session() as sess:",
            "sess.run(tf.global_variables_initializer())",
            "-                saver.restore(sess, tf.train.latest_checkpoint(args.ner_model_dir))",
            "+                saver.restore(sess, tf.train.latest_checkpoint(args.model_dir))",
            "logger.info('freeze...')",
            "from tensorflow.python.framework import graph_util",
            "tmp_g = graph_util.convert_variables_to_constants(sess, graph.as_graph_def(), ['pred_ids'])"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=ner_model_dir), value='model_dir')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 5078,
        "neg_line": [
            "-saver.restore(sess, tf.train.latest_checkpoint(args.ner_model_dir))"
        ],
        "pos_line": [
            "+saver.restore(sess, tf.train.latest_checkpoint(args.model_dir))"
        ],
        "core_change": "-saver.restore(sess, tf.train.latest_checkpoint(args.ner_model_dir)) +saver.restore(sess, tf.train.latest_checkpoint(args.model_dir))",
        "core_API": "Session"
    },
    {
        "commit_hash": "795cbf9ec222433828aff4efe98391be6aec0cd2",
        "index": "8cc50a21..cdcfe9cb 100644",
        "commit_message": "fixed nonzero deprecation warnings\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class GDC(object):",
            "kwargs['eps'] = self.__calculate_eps__(edge_weight, num_nodes,",
            "kwargs['avg_degree'])",
            "",
            "-            remaining_edge_idx = torch.nonzero(",
            "-                edge_weight >= kwargs['eps']).flatten()",
            "+            remaining_edge_idx = (edge_weight >= kwargs['eps']).nonzero(",
            "+                as_tuple=False).flatten()",
            "edge_index = edge_index[:, remaining_edge_idx]",
            "edge_weight = edge_weight[remaining_edge_idx]",
            "elif method == 'topk':"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1013359)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=1013360)",
            "Insert(target_node=IN(type=attribute), node=('parenthesized_expression', None), position=0, insert_id=1013361)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1013362)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'nonzero'), position=2, insert_id=1013363)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1013364)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=1, insert_id=1013365)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1013366)",
            "Move(target_node=IN(type=parenthesized_expression), node=ASTNode(type=(, text=(), position=0)",
            "Move(target_node=IN(type=parenthesized_expression), node=ASTNode(type=comparison_operator), position=1)",
            "Move(target_node=IN(type=parenthesized_expression), node=ASTNode(type=), text=)), position=2)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'as_tuple'), position=0, insert_id=1013367)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1013368)",
            "Insert(target_node=IN(type=keyword_argument), node=('false', 'False'), position=2, insert_id=1013369)",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=nonzero))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 19,
        "number": 5079,
        "neg_line": [
            "-remaining_edge_idx = torch.nonzero(",
            "-edge_weight >= kwargs['eps']).flatten()"
        ],
        "pos_line": [
            "+remaining_edge_idx = (edge_weight >= kwargs['eps']).nonzero(",
            "+as_tuple=False).flatten()"
        ],
        "core_change": "-remaining_edge_idx = torch.nonzero( -edge_weight >= kwargs['eps']).flatten() +remaining_edge_idx = (edge_weight >= kwargs['eps']).nonzero( +as_tuple=False).flatten()",
        "core_API": "__calculate_eps__"
    },
    {
        "commit_hash": "ca7b282682e73b16f22a932efa3a9e4abf962bd8",
        "index": "903fbed0..f9225725 100644",
        "commit_message": "viusalize_subgraph fix for graph explanation\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class GNNExplainer(torch.nn.Module):",
            "if node_idx == -1:",
            "hard_edge_mask = torch.BoolTensor([True] * edge_index.size(1),",
            "device=edge_mask.device)",
            "-            subset = torch.arange(",
            "-                edge_index.max() + 1,",
            "-                device=edge_index.device if y is None else y.device)",
            "+            subset = torch.arange(edge_index.max().item() + 1,",
            "+                                  device=edge_index.device)",
            "+            y = None",
            "+",
            "else:",
            "# Only operate on a k-hop subgraph around `node_idx`.",
            "subset, edge_index, _, hard_edge_mask = k_hop_subgraph("
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=4, insert_id=1005284)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1005285)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'y'), position=0, insert_id=1005286)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1005287)",
            "Insert(target_node=IN(type=assignment), node=('none', 'None'), position=2, insert_id=1005288)",
            "Insert(target_node=ASTNode(type=binary_operator), node=('call', None), position=0, insert_id=1005289)",
            "Move(target_node=ASTNode(type=keyword_argument), node=ASTNode(type=attribute), position=2)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1005290)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1005291)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1005292)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'item'), position=2, insert_id=1005293)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1005294)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1005295)",
            "Delete(target_node=ASTNode(type=if, text=if))",
            "Delete(target_node=ASTNode(type=identifier, text=y))",
            "Delete(target_node=ASTNode(type=is, text=is))",
            "Delete(target_node=ASTNode(type=none, text=None))",
            "Delete(target_node=ASTNode(type=comparison_operator))",
            "Delete(target_node=ASTNode(type=else, text=else))",
            "Delete(target_node=ASTNode(type=identifier, text=y))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=device))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=conditional_expression))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 25,
        "number": 5080,
        "neg_line": [
            "-subset = torch.arange(",
            "-edge_index.max() + 1,",
            "-device=edge_index.device if y is None else y.device)"
        ],
        "pos_line": [
            "+subset = torch.arange(edge_index.max().item() + 1,",
            "+device=edge_index.device)",
            "+y = None",
            "+"
        ],
        "core_change": "-subset = torch.arange( -edge_index.max() + 1, -device=edge_index.device if y is None else y.device) +subset = torch.arange(edge_index.max().item() + 1, +device=edge_index.device) +y = None +",
        "core_API": "BoolTensor"
    },
    {
        "commit_hash": "d3615e682e1c65e56427bffbf7527d8c2a91609a",
        "index": "d385a5bd..cfc4bfb5 100644",
        "commit_message": "Small fixes.\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "keras",
        "change": [
            "def test_srelu():",
            "",
            "",
            "if __name__ == '__main__':",
            "-    # pytest.main([__file__])",
            "-    test_srelu()",
            "+    pytest.main([__file__])"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2542251)",
            "Update(target_node=ASTNode(type=identifier, text=test_srelu), value='pytest')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=test_srelu), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2542252)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'main'), position=2, insert_id=2542253)",
            "Insert(target_node=ASTNode(type=argument_list), node=('list', None), position=1, insert_id=2542254)",
            "Insert(target_node=IN(type=list), node=('[', '['), position=0, insert_id=2542255)",
            "Insert(target_node=IN(type=list), node=('identifier', '__file__'), position=1, insert_id=2542256)",
            "Insert(target_node=IN(type=list), node=(']', ']'), position=2, insert_id=2542257)"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 9,
        "number": 5082,
        "neg_line": [
            "-# pytest.main([__file__])",
            "-test_srelu()"
        ],
        "pos_line": [
            "+pytest.main([__file__])"
        ],
        "core_change": "-# pytest.main([__file__]) -test_srelu() +pytest.main([__file__])",
        "core_API": "main"
    },
    {
        "commit_hash": "d52d68ebb5defa54511ae7c99dc2d9ea259d54bb",
        "index": "d3fc4741..6c7b5cd6 100644",
        "commit_message": "bug fix in summary. fix TF comptability break. update PTB readme\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def huber_loss(x, delta=1, name='huber_loss'):",
            "\"\"\"",
            "sqrcost = tf.square(x)",
            "abscost = tf.abs(x)",
            "-    return tf.select(abscost < delta,",
            "-                     sqrcost * 0.5,",
            "-                     abscost * delta - 0.5 * delta ** 2,",
            "-                     name=name)",
            "+    return tf.where(abscost < delta,",
            "+                    sqrcost * 0.5,",
            "+                    abscost * delta - 0.5 * delta ** 2,",
            "+                    name=name)",
            "",
            "",
            "def get_scalar_var(name, init_value, summary=False, trainable=False):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=select), value='where')"
        ],
        "plus_line": 4,
        "minus_line": 4,
        "AST_diff_line": 1,
        "number": 5084,
        "neg_line": [
            "-return tf.select(abscost < delta,",
            "-sqrcost * 0.5,",
            "-abscost * delta - 0.5 * delta ** 2,",
            "-name=name)"
        ],
        "pos_line": [
            "+return tf.where(abscost < delta,",
            "+sqrcost * 0.5,",
            "+abscost * delta - 0.5 * delta ** 2,",
            "+name=name)"
        ],
        "core_change": "-return tf.select(abscost < delta, -sqrcost * 0.5, -abscost * delta - 0.5 * delta ** 2, -name=name) +return tf.where(abscost < delta, +sqrcost * 0.5, +abscost * delta - 0.5 * delta ** 2, +name=name)",
        "core_API": "square"
    },
    {
        "commit_hash": "574f95e7ceccea4aeb4b9c0159a296f35b162250",
        "index": "a9343afe..4edc1298 100644",
        "commit_message": "Use nearest neighbour interpolation for masks (#1630)\n\n* If mask, use always nearest neighbour interpolation\n\n* added parameter override\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fixed pickle\n\n* fixed errors\n\n* Update colorjiggle\n\n* fixed lint\n\n* bug fix\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* try again\n\n* Fixed augmentation container\n\n* Bug fixes\n\n* bug fixes\n\n* Fixed lint\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Make it work\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fixed mypy\n\n* Fixed mypy\n\nCo-authored-by: shijianjian <sj8716643@126.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Jian Shi <shij0c@kaust.edu.sa>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class RandomInvert(IntensityAugmentationBase2D):",
            "self.flags = dict(max_val=max_val)",
            "",
            "def apply_transform(",
            "-        self, input: Tensor, params: Dict[str, Tensor], transform: Optional[Tensor] = None",
            "+        self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None",
            ") -> Tensor:",
            "-        return invert(input, torch.as_tensor(self.flags[\"max_val\"], device=input.device, dtype=input.dtype))",
            "+        return invert(input, torch.as_tensor(flags[\"max_val\"], device=input.device, dtype=input.dtype))"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=parameters), node=('typed_parameter', None), position=7, insert_id=403964)",
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=8, insert_id=403965)",
            "Insert(target_node=IN(type=typed_parameter), node=('identifier', 'flags'), position=0, insert_id=403966)",
            "Insert(target_node=IN(type=typed_parameter), node=(':', ':'), position=1, insert_id=403967)",
            "Insert(target_node=IN(type=typed_parameter), node=('type', None), position=2, insert_id=403968)",
            "Insert(target_node=IN(type=type), node=('subscript', None), position=0, insert_id=403969)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'Dict'), position=0, insert_id=403970)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=403971)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'str'), position=2, insert_id=403972)",
            "Insert(target_node=IN(type=subscript), node=(',', ','), position=3, insert_id=403973)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'Any'), position=4, insert_id=403974)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=5, insert_id=403975)",
            "Move(target_node=ASTNode(type=subscript), node=ASTNode(type=identifier, text=flags), position=0)",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 16,
        "number": 5091,
        "neg_line": [
            "-self, input: Tensor, params: Dict[str, Tensor], transform: Optional[Tensor] = None",
            "-return invert(input, torch.as_tensor(self.flags[\"max_val\"], device=input.device, dtype=input.dtype))"
        ],
        "pos_line": [
            "+self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None",
            "+return invert(input, torch.as_tensor(flags[\"max_val\"], device=input.device, dtype=input.dtype))"
        ],
        "core_change": "-self, input: Tensor, params: Dict[str, Tensor], transform: Optional[Tensor] = None +self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None -return invert(input, torch.as_tensor(self.flags[\"max_val\"], device=input.device, dtype=input.dtype)) +return invert(input, torch.as_tensor(flags[\"max_val\"], device=input.device, dtype=input.dtype))",
        "core_API": "as_tensor"
    },
    {
        "commit_hash": "5c49f254f4311195c924380eacd6c33245c5a54e",
        "index": "74ba5b4bb2..d6cd7f6be3 100644",
        "commit_message": "fixes to docstrings,signatures\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "clip.support_native_out = True",
            "clip.unsupported_dtypes = (\"float16\",)",
            "",
            "",
            "-def unstack(x: torch.Tensor, axis: int, keepdims: bool = False) -> List[torch.Tensor]:",
            "+def unstack(",
            "+    x: torch.Tensor, /, *, axis: int = 0, keepdims: bool = False",
            "+) -> List[torch.Tensor]:",
            "if x.shape == ():",
            "return [x]",
            "ret = list(torch.unbind(x, axis))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=parameters), node=('positional_separator', None), position=3, insert_id=322967)",
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=4, insert_id=322968)",
            "Insert(target_node=ASTNode(type=parameters), node=('keyword_separator', None), position=5, insert_id=322969)",
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=6, insert_id=322970)",
            "Insert(target_node=ASTNode(type=parameters), node=('typed_default_parameter', None), position=7, insert_id=322971)",
            "Insert(target_node=IN(type=positional_separator), node=('/', '/'), position=0, insert_id=322972)",
            "Insert(target_node=IN(type=keyword_separator), node=('*', '*'), position=0, insert_id=322973)",
            "Move(target_node=IN(type=typed_default_parameter), node=ASTNode(type=identifier, text=axis), position=0)",
            "Move(target_node=IN(type=typed_default_parameter), node=ASTNode(type=:, text=:), position=1)",
            "Move(target_node=IN(type=typed_default_parameter), node=ASTNode(type=type), position=2)",
            "Insert(target_node=IN(type=typed_default_parameter), node=('=', '='), position=3, insert_id=322974)",
            "Insert(target_node=IN(type=typed_default_parameter), node=('integer', '0'), position=4, insert_id=322975)",
            "Delete(target_node=ASTNode(type=typed_parameter))"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 13,
        "number": 5095,
        "neg_line": [
            "-def unstack(x: torch.Tensor, axis: int, keepdims: bool = False) -> List[torch.Tensor]:"
        ],
        "pos_line": [
            "+def unstack(",
            "+x: torch.Tensor, /, *, axis: int = 0, keepdims: bool = False",
            "+) -> List[torch.Tensor]:"
        ],
        "core_change": "-def unstack(x: torch.Tensor, axis: int, keepdims: bool = False) -> List[torch.Tensor]: +def unstack( +x: torch.Tensor, /, *, axis: int = 0, keepdims: bool = False +) -> List[torch.Tensor]:",
        "core_API": "unbind"
    },
    {
        "commit_hash": "1ed2ebf60d87ef12bd063c7c58e484e19189c754",
        "index": "40b2f864c..4d9ebe017 100644",
        "commit_message": "[style] consistent nn. and nn.functional (#12124)\n\n* consistent nn. and nn.functional\n\n* fix glitch\n\n* fix glitch #2\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class LightningModel(pl.LightningModule):",
            "super().__init__()",
            "self.model = model",
            "self.num_labels = 2",
            "-        self.qa_outputs = torch.nn.Linear(self.model.config.hidden_size, self.num_labels)",
            "+        self.qa_outputs = nn.Linear(self.model.config.hidden_size, self.num_labels)",
            "",
            "# implement only because lightning requires to do so",
            "def forward(self):"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 5101,
        "neg_line": [
            "-self.qa_outputs = torch.nn.Linear(self.model.config.hidden_size, self.num_labels)"
        ],
        "pos_line": [
            "+self.qa_outputs = nn.Linear(self.model.config.hidden_size, self.num_labels)"
        ],
        "core_change": "-self.qa_outputs = torch.nn.Linear(self.model.config.hidden_size, self.num_labels) +self.qa_outputs = nn.Linear(self.model.config.hidden_size, self.num_labels)",
        "core_API": "Linear"
    },
    {
        "commit_hash": "d3cb28886ac68beba9a6646b422a4d727b056c0c",
        "index": "9e487b609..4cc4055a6 100644",
        "commit_message": "Not use -1e4 as attn mask (#17306)\n\n* Use torch.finfo(self.dtype).min\n\n* for GPTNeoX\n\n* for Albert\n\n* For Splinter\n\n* Update src/transformers/models/data2vec/modeling_data2vec_audio.py\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* fix -inf used in Bart-like models\n\n* Fix a few remaining -inf\n\n* more fix\n\n* clean up\n\n* For CLIP\n\n* For FSMT\n\n* clean up\n\n* fix test\n\n* Add dtype argument and use it for LayoutLMv3\n\n* update FlaxLongT5Attention\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class FSMTHeadTests(unittest.TestCase):",
            "config, *_ = self._get_config_and_data()",
            "input_ids = _long_tensor(([4, 4, 2]))",
            "decoder_input_ids = _long_tensor([[26388, 2, config.pad_token_id]])",
            "-        ignore = float(\"-inf\")",
            "+        causal_mask_dtype = torch.float32",
            "+        ignore = torch.finfo(causal_mask_dtype).min",
            "decoder_input_ids, decoder_attn_mask, causal_mask = _prepare_fsmt_decoder_inputs(",
            "-            config, input_ids, decoder_input_ids",
            "+            config, input_ids, decoder_input_ids, causal_mask_dtype=causal_mask_dtype",
            ")",
            "expected_causal_mask = torch.tensor(",
            "[[0, ignore, ignore], [0, 0, ignore], [0, 0, 0]]  # never attend to the final token, because its pad"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=4, insert_id=1197442)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1197443)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'causal_mask_dtype'), position=0, insert_id=1197444)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1197445)",
            "Insert(target_node=IN(type=assignment), node=('attribute', None), position=2, insert_id=1197446)",
            "Insert(target_node=ASTNode(type=assignment), node=('attribute', None), position=2, insert_id=1197447)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1197448)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1197449)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'float32'), position=2, insert_id=1197450)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1197451)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'min'), position=2, insert_id=1197452)",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1197453)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=6, insert_id=1197454)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=7, insert_id=1197455)",
            "Update(target_node=ASTNode(type=identifier, text=float), value='torch')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=float), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1197456)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'finfo'), position=2, insert_id=1197457)",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'causal_mask_dtype'), position=1, insert_id=1197458)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'causal_mask_dtype'), position=0, insert_id=1197459)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1197460)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'causal_mask_dtype'), position=2, insert_id=1197461)",
            "Delete(target_node=ASTNode(type=string, text=\"-inf\"))"
        ],
        "plus_line": 3,
        "minus_line": 2,
        "AST_diff_line": 24,
        "number": 5102,
        "neg_line": [
            "-ignore = float(\"-inf\")",
            "-config, input_ids, decoder_input_ids"
        ],
        "pos_line": [
            "+causal_mask_dtype = torch.float32",
            "+ignore = torch.finfo(causal_mask_dtype).min",
            "+config, input_ids, decoder_input_ids, causal_mask_dtype=causal_mask_dtype"
        ],
        "core_change": "-ignore = float(\"-inf\") +causal_mask_dtype = torch.float32 +ignore = torch.finfo(causal_mask_dtype).min -config, input_ids, decoder_input_ids +config, input_ids, decoder_input_ids, causal_mask_dtype=causal_mask_dtype",
        "core_API": "_get_config_and_data"
    },
    {
        "commit_hash": "aa90e0c36adc0034ece203c857d0d993c82ae65a",
        "index": "e2a3f9c92..0cb63edf0 100644",
        "commit_message": "fix prediction on run-squad.py example\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def main():",
            "if args.do_train:",
            "torch.save(model_to_save.state_dict(), output_model_file)",
            "",
            "-    # Load a trained model that you have fine-tuned",
            "-    model_state_dict = torch.load(output_model_file)",
            "-    model = BertForQuestionAnswering.from_pretrained(args.bert_model, state_dict=model_state_dict)",
            "+        # Load a trained model that you have fine-tuned",
            "+        model_state_dict = torch.load(output_model_file)",
            "+        model = BertForQuestionAnswering.from_pretrained(args.bert_model, state_dict=model_state_dict)",
            "+    else:",
            "+        model = BertForQuestionAnswering.from_pretrained(args.bert_model)",
            "+",
            "model.to(device)",
            "",
            "if args.do_predict and (args.local_rank == -1 or torch.distributed.get_rank() == 0):"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=5, insert_id=1250239)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1250240)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'else'), position=0, insert_id=1250241)",
            "Insert(target_node=IN(type=assignment), node=(':', ':'), position=1, insert_id=1250242)",
            "Insert(target_node=IN(type=assignment), node=('type', None), position=2, insert_id=1250243)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=3, insert_id=1250244)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=4, insert_id=1250245)",
            "Insert(target_node=IN(type=type), node=('identifier', 'model'), position=0, insert_id=1250246)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1250247)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1250248)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'BertForQuestionAnswering'), position=0, insert_id=1250249)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1250250)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'from_pretrained'), position=2, insert_id=1250251)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1250252)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=1250253)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1250254)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'args'), position=0, insert_id=1250255)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1250256)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'bert_model'), position=2, insert_id=1250257)"
        ],
        "plus_line": 5,
        "minus_line": 3,
        "AST_diff_line": 19,
        "number": 5106,
        "neg_line": [
            "-# Load a trained model that you have fine-tuned",
            "-model_state_dict = torch.load(output_model_file)",
            "-model = BertForQuestionAnswering.from_pretrained(args.bert_model, state_dict=model_state_dict)"
        ],
        "pos_line": [
            "+# Load a trained model that you have fine-tuned",
            "+model_state_dict = torch.load(output_model_file)",
            "+model = BertForQuestionAnswering.from_pretrained(args.bert_model, state_dict=model_state_dict)",
            "+else:",
            "+model = BertForQuestionAnswering.from_pretrained(args.bert_model)",
            "+"
        ],
        "core_change": "-# Load a trained model that you have fine-tuned -model_state_dict = torch.load(output_model_file) -model = BertForQuestionAnswering.from_pretrained(args.bert_model, state_dict=model_state_dict) +# Load a trained model that you have fine-tuned +model_state_dict = torch.load(output_model_file) +model = BertForQuestionAnswering.from_pretrained(args.bert_model, state_dict=model_state_dict) +else: +model = BertForQuestionAnswering.from_pretrained(args.bert_model) +",
        "core_API": "save"
    },
    {
        "commit_hash": "73b5690050dc179d86858c2c9f6486e381bba0b9",
        "index": "4a33ab5..ff3ca1a 100644",
        "commit_message": "Fix hyperlstm\n\n",
        "file": "seq2seq.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class HyperLSTMCell(tf.contrib.rnn.RNNCell):",
            "x_size = x.get_shape().as_list()[-1]",
            "embedding_size = self.hyper_embedding_size",
            "num_units = self.num_units",
            "-      batch_size = x.get_shape().as_list()[0]",
            "+      batch_size = tf.shape(x)[0]",
            "",
            "W_xh = tf.get_variable('W_xh',",
            "[x_size, 4*num_units], initializer=w_init)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2165173)",
            "Update(target_node=ASTNode(type=identifier, text=get_shape), value='shape')",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=x), position=1)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=as_list))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 5107,
        "neg_line": [
            "-batch_size = x.get_shape().as_list()[0]"
        ],
        "pos_line": [
            "+batch_size = tf.shape(x)[0]"
        ],
        "core_change": "-batch_size = x.get_shape().as_list()[0] +batch_size = tf.shape(x)[0]",
        "core_API": "get_shape"
    },
    {
        "commit_hash": "57140e8e951e8e149e998505d6b7ba6adeeaaede",
        "index": "35f8bbd3..303267f0 100644",
        "commit_message": "fix: fix BF16_Optimizer compatibility issue with optimizer state 0-dim tensor (#2152)\n\nCo-authored-by: Olatunji Ruwase <olruwase@microsoft.com>\n",
        "file": "DeepSpeed.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class BF16_Optimizer(ZeROOptimizer):",
            "hp_frag_address.numel)",
            "for key,",
            "value in self.optimizer.state[flat_hp_partition].items()",
            "-                if torch.is_tensor(value)",
            "+                if torch.is_tensor(value) and value.dim() > 0",
            "}",
            "",
            "lp_frag_address = fragment_address(start=fragment_start - lp_start,"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=ERROR), node=('boolean_operator', None), position=0, insert_id=77378)",
            "Move(target_node=IN(type=boolean_operator), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=boolean_operator), node=('and', 'and'), position=1, insert_id=77379)",
            "Insert(target_node=IN(type=boolean_operator), node=('comparison_operator', None), position=2, insert_id=77380)",
            "Insert(target_node=IN(type=comparison_operator), node=('call', None), position=0, insert_id=77381)",
            "Insert(target_node=IN(type=comparison_operator), node=('>', '>'), position=1, insert_id=77382)",
            "Insert(target_node=IN(type=comparison_operator), node=('ERROR', None), position=2, insert_id=77383)",
            "Move(target_node=IN(type=comparison_operator), node=ASTNode(type=identifier, text=lp_frag_address), position=3)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=77384)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=77385)",
            "Insert(target_node=IN(type=ERROR), node=('integer', '0'), position=0, insert_id=77386)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=}, text=}), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'value'), position=0, insert_id=77387)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=77388)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dim'), position=2, insert_id=77389)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=77390)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=77391)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 5108,
        "neg_line": [
            "-if torch.is_tensor(value)"
        ],
        "pos_line": [
            "+if torch.is_tensor(value) and value.dim() > 0"
        ],
        "core_change": "-if torch.is_tensor(value) +if torch.is_tensor(value) and value.dim() > 0",
        "core_API": "is_tensor"
    },
    {
        "commit_hash": "b893c03bb098af1bd151f7a1675214f698182aa7",
        "index": "7bdc87a..b65a17e 100644",
        "commit_message": "Fix bugs in tensorrt and openvino\n\n",
        "file": "nebullvm.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TensorflowONNXTensorRTInferenceLearner(",
            "else None",
            ")",
            "out_arrays = self._predict_array(cuda_input_arrays, input_shapes)",
            "-        return tuple(tf.convert_to_tensor(array[0]) for array in out_arrays)",
            "+        return tuple(tf.convert_to_tensor(array) for array in out_arrays)",
            "",
            "",
            "class NumpyONNXTensorRTInferenceLearner("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=array), position=1)",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=integer, text=0))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 5111,
        "neg_line": [
            "-return tuple(tf.convert_to_tensor(array[0]) for array in out_arrays)"
        ],
        "pos_line": [
            "+return tuple(tf.convert_to_tensor(array) for array in out_arrays)"
        ],
        "core_change": "-return tuple(tf.convert_to_tensor(array[0]) for array in out_arrays) +return tuple(tf.convert_to_tensor(array) for array in out_arrays)",
        "core_API": "_predict_array"
    },
    {
        "commit_hash": "61443cd7d917ef323a799ee27bb4abc4344f0d11",
        "index": "d99b76f2f..c0781a294 100644",
        "commit_message": "[GPT2] Correct gradient checkpointing (#9308)\n\n* correct gpt2\n\n* fix gpt2\n\n* fix use_cache ordering\n\n* correct past tolerance\n\n* fix for all cases\n\n* style\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFGPT2ModelTester:",
            "output_from_past_slice = output_from_past[:, :, random_slice_idx]",
            "",
            "# test that outputs are equal for slice",
            "-        tf.debugging.assert_near(output_from_past_slice, output_from_no_past_slice, rtol=1e-6)",
            "+        tf.debugging.assert_near(output_from_past_slice, output_from_no_past_slice, rtol=1e-3)",
            "",
            "def create_and_check_gpt2_lm_head(self, config, input_ids, input_mask, head_mask, token_type_ids, *args):",
            "model = TFGPT2LMHeadModel(config=config)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=float, text=1e-6), value='1e-3')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 5112,
        "neg_line": [
            "-tf.debugging.assert_near(output_from_past_slice, output_from_no_past_slice, rtol=1e-6)"
        ],
        "pos_line": [
            "+tf.debugging.assert_near(output_from_past_slice, output_from_no_past_slice, rtol=1e-3)"
        ],
        "core_change": "-tf.debugging.assert_near(output_from_past_slice, output_from_no_past_slice, rtol=1e-6) +tf.debugging.assert_near(output_from_past_slice, output_from_no_past_slice, rtol=1e-3)",
        "core_API": "assert_near"
    },
    {
        "commit_hash": "952038da30c2419ad7d9e67e8ed330096921f983",
        "index": "6a0af4f9..3e01f7b6 100755",
        "commit_message": "fixed trpo model distribution object creation\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TRPOModel(PolicyGradientModel):",
            "gradients = [grad for grad in gradients if grad is not None]",
            "self.policy_gradient = tf.concat(values=[tf.reshape(grad, (-1,)) for grad in gradients], axis=0)  # util.prod(util.shape(v))",
            "",
            "-            fixed_distribution = distribution.__class__([tf.stop_gradient(x) for x in distribution])",
            "+            fixed_distribution = distribution.__class__.from_tensors(parameters=[tf.stop_gradient(x) for x in distribution])",
            "fixed_kl_divergence = fixed_distribution.kl_divergence(distribution)",
            "",
            "self.tangent = tf.placeholder(tf.float32, shape=(None,))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2244220)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2244221)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'from_tensors'), position=2, insert_id=2244222)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=1, insert_id=2244223)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'parameters'), position=0, insert_id=2244224)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2244225)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=list_comprehension), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 5118,
        "neg_line": [
            "-fixed_distribution = distribution.__class__([tf.stop_gradient(x) for x in distribution])"
        ],
        "pos_line": [
            "+fixed_distribution = distribution.__class__.from_tensors(parameters=[tf.stop_gradient(x) for x in distribution])"
        ],
        "core_change": "-fixed_distribution = distribution.__class__([tf.stop_gradient(x) for x in distribution]) +fixed_distribution = distribution.__class__.from_tensors(parameters=[tf.stop_gradient(x) for x in distribution])",
        "core_API": "concat"
    },
    {
        "commit_hash": "5ae087cf8ec080b121c9cdc9bafdc2b35b6e110e",
        "index": "35f1d9088..452bd913c 100644",
        "commit_message": "Fix T5/mT5 tests (#18029)\n\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFT5ModelIntegrationTests(unittest.TestCase):",
            "labels = tokenizer(\"Hi I am\", return_tensors=\"tf\").input_ids",
            "",
            "loss = model(input_ids, labels=labels).loss",
            "-        mtf_score = -tf.math.reduce_sum(loss).numpy()",
            "+        mtf_score = -tf.math.reduce_mean(loss).numpy()",
            "",
            "-        EXPECTED_SCORE = -60.7397",
            "+        EXPECTED_SCORE = -7.594554",
            "self.assertTrue(abs(mtf_score - EXPECTED_SCORE) < 1e-4)",
            "",
            "@slow"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=float, text=60.7397), value='7.594554')",
            "Update(target_node=ASTNode(type=identifier, text=reduce_sum), value='reduce_mean')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 5124,
        "neg_line": [
            "-mtf_score = -tf.math.reduce_sum(loss).numpy()",
            "-EXPECTED_SCORE = -60.7397"
        ],
        "pos_line": [
            "+mtf_score = -tf.math.reduce_mean(loss).numpy()",
            "+EXPECTED_SCORE = -7.594554"
        ],
        "core_change": "-mtf_score = -tf.math.reduce_sum(loss).numpy() +mtf_score = -tf.math.reduce_mean(loss).numpy() -EXPECTED_SCORE = -60.7397 +EXPECTED_SCORE = -7.594554",
        "core_API": "reduce_sum"
    },
    {
        "commit_hash": "8218f19ffb3a0b7c3aab03cb4c1349a527400cdf",
        "index": "05dd21f7b0..878df3b5e8 100644",
        "commit_message": "lint fixes\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def log2(",
            "x: Union[tf.Tensor, tf.Variable], *, out: Union[tf.Tensor, tf.Variable] = None",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "if ivy.is_int_dtype(x):",
            "-        x=tf.cast(x,dtype=ivy.default_float_dtype())",
            "+        x = tf.cast(x, dtype=ivy.default_float_dtype())",
            "return tf.math.log(x) / tf.math.log(tf.constant(2.0, x.dtype))"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 5127,
        "neg_line": [
            "-x=tf.cast(x,dtype=ivy.default_float_dtype())"
        ],
        "pos_line": [
            "+x = tf.cast(x, dtype=ivy.default_float_dtype())"
        ],
        "core_change": "-x=tf.cast(x,dtype=ivy.default_float_dtype()) +x = tf.cast(x, dtype=ivy.default_float_dtype())",
        "core_API": "is_int_dtype"
    },
    {
        "commit_hash": "dce78003c8da1ce93e08e678a995a7e304fa7487",
        "index": "62c36381..d515b74a 100644",
        "commit_message": "two minor fixes for parallel mode\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class PolicyModel(Model):",
            "is_frequency = tf.math.equal(x=tf.mod(x=episode, y=frequency), y=zero)",
            "# Only update once per episode increment",
            "false = tf.constant(value=False, dtype=util.tf_dtype(dtype='bool'))",
            "-                terminal = tf.concat(values=(false, terminal), axis=0)",
            "+                terminal = tf.concat(values=((false,), terminal), axis=0)",
            "is_terminal = terminal[-1]",
            "is_frequency = tf.math.logical_and(x=is_frequency, y=is_terminal)",
            "at_least_start = tf.math.greater_equal(x=episode, y=start)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=tuple), node=('tuple', None), position=1, insert_id=2229894)",
            "Insert(target_node=ASTNode(type=tuple), node=(',', ','), position=2, insert_id=2229895)",
            "Insert(target_node=IN(type=tuple), node=('(', '('), position=0, insert_id=2229896)",
            "Move(target_node=IN(type=tuple), node=ASTNode(type=identifier, text=false), position=1)",
            "Move(target_node=IN(type=tuple), node=ASTNode(type=,, text=,), position=2)",
            "Insert(target_node=IN(type=tuple), node=(')', ')'), position=3, insert_id=2229897)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 5129,
        "neg_line": [
            "-terminal = tf.concat(values=(false, terminal), axis=0)"
        ],
        "pos_line": [
            "+terminal = tf.concat(values=((false,), terminal), axis=0)"
        ],
        "core_change": "-terminal = tf.concat(values=(false, terminal), axis=0) +terminal = tf.concat(values=((false,), terminal), axis=0)",
        "core_API": "equal"
    },
    {
        "commit_hash": "15692dab6465f16c14d8aa0992c0c06fa5345214",
        "index": "cd27104..0d300e9 100644",
        "commit_message": "Initial support for TensorFlow 2.0 (#1193)\n\n* Initial support for TensorFlow 2.0\n\n* Support for TensorFlow 2.0 in `horovod.tensorflow`\n* Replace RMSProp with Adam (RMSProp doesn't scale)\n* Minor example bugfixes\n\nSigned-off-by: Alex Sergeev <alsrgv@users.noreply.github.com>\n\n* Bugfixes, enable subgraph caching\n\nSigned-off-by: Alex Sergeev <alsrgv@users.noreply.github.com>\n\n* Bugfixes & performance improvements\n\nSigned-off-by: Alex Sergeev <alsrgv@users.noreply.github.com>\n\n* Revert RMSprop changes in Keras tests\n\nSigned-off-by: Alex Sergeev <alsrgv@users.noreply.github.com>\n\n* Renamed examples tensorflow_20_* -> tensorflow2_*\n\nSigned-off-by: Alex Sergeev <alsrgv@users.noreply.github.com>\n\n* Support for TensorFlow 2.0 tests\n\nSigned-off-by: Alex Sergeev <alsrgv@users.noreply.github.com>\n\n* Bugfix Keras test exclusion logic\n\nSigned-off-by: Alex Sergeev <alsrgv@users.noreply.github.com>\n\n* Address review comments\n\nSigned-off-by: Alex Sergeev <alsrgv@users.noreply.github.com>\n\n",
        "file": "horovod.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def main(_):",
            "predict, loss = conv_model(image, label, tf.estimator.ModeKeys.TRAIN)",
            "",
            "# Horovod: adjust learning rate based on number of GPUs.",
            "-    opt = tf.train.RMSPropOptimizer(0.001 * hvd.size())",
            "+    opt = tf.train.AdamOptimizer(0.001 * hvd.size())",
            "",
            "# Horovod: add Horovod Distributed Optimizer.",
            "opt = hvd.DistributedOptimizer(opt)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=RMSPropOptimizer), value='AdamOptimizer')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 5130,
        "neg_line": [
            "-opt = tf.train.RMSPropOptimizer(0.001 * hvd.size())"
        ],
        "pos_line": [
            "+opt = tf.train.AdamOptimizer(0.001 * hvd.size())"
        ],
        "core_change": "-opt = tf.train.RMSPropOptimizer(0.001 * hvd.size()) +opt = tf.train.AdamOptimizer(0.001 * hvd.size())",
        "core_API": "RMSPropOptimizer"
    },
    {
        "commit_hash": "c0ae913ed615c2bf12a9ff6976376c5f16b4376b",
        "index": "a330182a..95a1e84f 100755",
        "commit_message": "add a missing output in dorefa (fix #800)\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Model(ImageNetModel):",
            ".apply(nonlin)",
            ".FullyConnected('fct', 1000, use_bias=True)())",
            "add_param_summary(('.*/W', ['histogram', 'rms']))",
            "+        tf.nn.softmax(logits, name='output')  # for prediction",
            "return logits",
            "",
            "def optimizer(self):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=4, insert_id=2283477)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=2283478)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2283479)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2283480)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2283481)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2283482)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'softmax'), position=2, insert_id=2283483)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2283484)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'logits'), position=1, insert_id=2283485)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=2283486)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=3, insert_id=2283487)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=2283488)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2283489)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2283490)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'nn'), position=2, insert_id=2283491)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'name'), position=0, insert_id=2283492)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2283493)",
            "Insert(target_node=IN(type=keyword_argument), node=('string', \"'output'\"), position=2, insert_id=2283494)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 18,
        "number": 5132,
        "neg_line": [],
        "pos_line": [
            "+tf.nn.softmax(logits, name='output')  # for prediction"
        ],
        "core_change": "+tf.nn.softmax(logits, name='output')  # for prediction",
        "core_API": "softmax"
    },
    {
        "commit_hash": "0d3edb67994e5514b71e30d6cf2830c71490a5d4",
        "index": "8ee8d0b625..8452554d43 100644",
        "commit_message": "fix elementwise, fix bitwise backends and tests\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def bitwise_right_shift(",
            "out: Optional[Union[tf.Tensor, tf.Variable]] = None,",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "x1, x2 = ivy.promote_types_of_inputs(x1, x2)",
            "-    x1, x2 = _clamp_bits(x1, x2)",
            "+    ivy.assertions.check_all(x2 >= 0, message=\"shifts must be non-negative\")",
            "return tf.bitwise.right_shift(x1, x2)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=expression_statement), node=('call', None), position=0, insert_id=1993164)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1993165)",
            "Move(target_node=IN(type=call), node=ASTNode(type=argument_list), position=1)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1993166)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1993167)",
            "Update(target_node=ASTNode(type=identifier, text=_clamp_bits), value='check_all')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=_clamp_bits), position=2)",
            "Insert(target_node=ASTNode(type=argument_list), node=('comparison_operator', None), position=1, insert_id=1993168)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=4, insert_id=1993169)",
            "Update(target_node=ASTNode(type=identifier, text=x1), value='ivy')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=x1), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1993170)",
            "Update(target_node=ASTNode(type=identifier, text=x2), value='assertions')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=x2), position=2)",
            "Move(target_node=IN(type=comparison_operator), node=ASTNode(type=identifier, text=x2), position=0)",
            "Insert(target_node=IN(type=comparison_operator), node=('>=', '>='), position=1, insert_id=1993171)",
            "Insert(target_node=IN(type=comparison_operator), node=('integer', '0'), position=2, insert_id=1993172)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'message'), position=0, insert_id=1993173)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type==, text==), position=1)",
            "Insert(target_node=IN(type=keyword_argument), node=('string', '\"shifts must be non-negative\"'), position=2, insert_id=1993174)",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=pattern_list))",
            "Delete(target_node=ASTNode(type=identifier, text=x1))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=assignment))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 25,
        "number": 5133,
        "neg_line": [
            "-x1, x2 = _clamp_bits(x1, x2)"
        ],
        "pos_line": [
            "+ivy.assertions.check_all(x2 >= 0, message=\"shifts must be non-negative\")"
        ],
        "core_change": "-x1, x2 = _clamp_bits(x1, x2) +ivy.assertions.check_all(x2 >= 0, message=\"shifts must be non-negative\")",
        "core_API": "promote_types_of_inputs"
    },
    {
        "commit_hash": "c61e22a8e0e393b7d701611437f595656cf16003",
        "index": "2aee5cb7..d6f1c6bc 100644",
        "commit_message": "Fix unit test failures with torch==1.1.0 (#1840)\n\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class _OMTMVNSample(Function):",
            "loc_grad = sum_leftmost(grad_output, -1)",
            "",
            "identity = eye_like(g, dim)",
            "-        R_inv = torch.trtrs(identity, L.t(), transpose=False, upper=True)[0]",
            "+        R_inv = torch.triangular_solve(identity, L.t(), transpose=False, upper=True)[0]",
            "",
            "z_ja = z.unsqueeze(-1)",
            "g_R_inv = torch.matmul(g, R_inv).unsqueeze(-2)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=trtrs), value='triangular_solve')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 5135,
        "neg_line": [
            "-R_inv = torch.trtrs(identity, L.t(), transpose=False, upper=True)[0]"
        ],
        "pos_line": [
            "+R_inv = torch.triangular_solve(identity, L.t(), transpose=False, upper=True)[0]"
        ],
        "core_change": "-R_inv = torch.trtrs(identity, L.t(), transpose=False, upper=True)[0] +R_inv = torch.triangular_solve(identity, L.t(), transpose=False, upper=True)[0]",
        "core_API": "trtrs"
    },
    {
        "commit_hash": "31017120fd017b64e8ae283323205b40356cb623",
        "index": "957397f3b..aaf16a104 100644",
        "commit_message": "fix incomplete RunningMean (#1309)\n\n* fix RunningMean\n\n* changelog\n\n* fix none\n\n* Update supporters.py\n\njust needed to multiply by zero for init\n\n* Revert \"Update supporters.py\"\n\nThis reverts commit 7e0da6c6\n\n* fix NaN\n\n* formatting\n\nCo-authored-by: William Falcon <waf2107@columbia.edu>\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def info_system():",
            "",
            "def info_cuda():",
            "return {",
            "-        'GPU': set([torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())]),",
            "+        'GPU': [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())],",
            "# 'nvidia_driver': get_nvidia_driver_version(run_lambda),",
            "'available': torch.cuda.is_available(),",
            "'version': torch.version.cuda,"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=pair), node=ASTNode(type=list_comprehension), position=2)",
            "Delete(target_node=ASTNode(type=identifier, text=set))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 5141,
        "neg_line": [
            "-'GPU': set([torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())]),"
        ],
        "pos_line": [
            "+'GPU': [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())],"
        ],
        "core_change": "-'GPU': set([torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())]), +'GPU': [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())],",
        "core_API": "get_device_name"
    },
    {
        "commit_hash": "455c6390938a5c737fa63e78396cedae41e4e87e",
        "index": "1f83e4f48..1fb62833c 100644",
        "commit_message": "CDN urls (#4030)\n\n* [file_utils] use_cdn + documentation\n\n* Move to cdn. urls for weights\n\n* [urls] Hotfix for bert-base-japanese\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "models",
        "change": [
            "from .tokenization_utils import BatchEncoding",
            "logger = logging.getLogger(__name__)",
            "",
            "TF_XLNET_PRETRAINED_MODEL_ARCHIVE_MAP = {",
            "-    \"xlnet-base-cased\": \"https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-tf_model.h5\",",
            "-    \"xlnet-large-cased\": \"https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-large-cased-tf_model.h5\",",
            "+    \"xlnet-base-cased\": \"https://cdn.huggingface.co/xlnet-base-cased-tf_model.h5\",",
            "+    \"xlnet-large-cased\": \"https://cdn.huggingface.co/xlnet-large-cased-tf_model.h5\",",
            "}"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=pair), node=ASTNode(type=dictionary), position=2)",
            "Insert(target_node=ASTNode(type=dictionary), node=(',', ','), position=2, insert_id=2689801)",
            "Insert(target_node=ASTNode(type=pair), node=(':', ':'), position=1, insert_id=2689802)",
            "Update(target_node=ASTNode(type=string, text=\"https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-tf_model.h5\"), value='\"https://cdn.huggingface.co/xlnet-base-cased-tf_model.h5\"')",
            "Move(target_node=ASTNode(type=pair), node=ASTNode(type=:, text=:), position=1)",
            "Update(target_node=ASTNode(type=string, text=\"https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-large-cased-tf_model.h5\"), value='\"https://cdn.huggingface.co/xlnet-large-cased-tf_model.h5\"')",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 8,
        "number": 5145,
        "neg_line": [
            "-\"xlnet-base-cased\": \"https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-tf_model.h5\",",
            "-\"xlnet-large-cased\": \"https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-large-cased-tf_model.h5\","
        ],
        "pos_line": [
            "+\"xlnet-base-cased\": \"https://cdn.huggingface.co/xlnet-base-cased-tf_model.h5\",",
            "+\"xlnet-large-cased\": \"https://cdn.huggingface.co/xlnet-large-cased-tf_model.h5\","
        ],
        "core_change": "-\"xlnet-base-cased\": \"https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-tf_model.h5\", -\"xlnet-large-cased\": \"https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-large-cased-tf_model.h5\", +\"xlnet-base-cased\": \"https://cdn.huggingface.co/xlnet-base-cased-tf_model.h5\", +\"xlnet-large-cased\": \"https://cdn.huggingface.co/xlnet-large-cased-tf_model.h5\",",
        "core_API": "getLogger"
    },
    {
        "commit_hash": "59f796edf3c2be0bb959bfef1b0cf56a13f10a9b",
        "index": "4daae937b7..ba7582903c 100644",
        "commit_message": "[RLlib] Fix crash when using StochasticSampling exploration (most PG-style algos) w/ tf and numpy > 1.19.5 (#18366)\n\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class OrnsteinUhlenbeckNoise(GaussianNoise):",
            "true_fn=lambda: exploration_actions,",
            "false_fn=lambda: deterministic_actions)",
            "# Logp=always zero.",
            "-        logp = tf.zeros_like(deterministic_actions, dtype=tf.float32)[:, 0]",
            "+        logp = zero_logps_from_actions(deterministic_actions)",
            "",
            "# Increment `last_timestep` by 1 (or set to `timestep`).",
            "if self.framework in [\"tf2\", \"tfe\"]:"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=call), position=3)",
            "Update(target_node=ASTNode(type=identifier, text=tf), value='zero_logps_from_actions')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=tf), position=0)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=zeros_like))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=dtype))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=float32))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=slice))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=integer, text=0))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 21,
        "number": 5147,
        "neg_line": [
            "-logp = tf.zeros_like(deterministic_actions, dtype=tf.float32)[:, 0]"
        ],
        "pos_line": [
            "+logp = zero_logps_from_actions(deterministic_actions)"
        ],
        "core_change": "-logp = tf.zeros_like(deterministic_actions, dtype=tf.float32)[:, 0] +logp = zero_logps_from_actions(deterministic_actions)",
        "core_API": "zeros_like"
    },
    {
        "commit_hash": "e5814dfa67fad7fc8acd408989fa5561276cd21f",
        "index": "afdb727..8992aca 100644",
        "commit_message": "fix initializer :fire:\n\n",
        "file": "darkflow.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "layers",
        "change": [
            "class BaseOp(object):",
            "self.lay.w[var] = tf.get_variable(var,",
            "shape = self.lay.wshape[var],",
            "dtype = tf.float32,",
            "-                initializer = tf.contrib.layers.xavier_initializer())",
            "+                initializer = self.lay.w[var])",
            "",
            "def wrap_pholder(self, ph, feed):",
            "\"\"\"wrap layer.h into placeholders\"\"\""
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=keyword_argument), node=('subscript', None), position=2, insert_id=2566734)",
            "Move(target_node=IN(type=subscript), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=2566735)",
            "Update(target_node=ASTNode(type=identifier, text=xavier_initializer), value='var')",
            "Move(target_node=IN(type=subscript), node=ASTNode(type=identifier, text=xavier_initializer), position=2)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=2566736)",
            "Update(target_node=ASTNode(type=identifier, text=layers), value='w')",
            "Update(target_node=ASTNode(type=identifier, text=tf), value='self')",
            "Update(target_node=ASTNode(type=identifier, text=contrib), value='lay')",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 15,
        "number": 5149,
        "neg_line": [
            "-initializer = tf.contrib.layers.xavier_initializer())"
        ],
        "pos_line": [
            "+initializer = self.lay.w[var])"
        ],
        "core_change": "-initializer = tf.contrib.layers.xavier_initializer()) +initializer = self.lay.w[var])",
        "core_API": "get_variable"
    },
    {
        "commit_hash": "418c54a59b807a9afef43da86c1a810fe9b48856",
        "index": "042aa411..a476ebd2 100644",
        "commit_message": "Add deterministic argument for Agent.act, lots of other small changes and fixes\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Recent(Queue):",
            "# Check whether memory contains at least one valid timestep",
            "num_timesteps = tf.math.minimum(x=self.buffer_index, y=capacity)",
            "num_timesteps -= (past_horizon + future_horizon)",
            "+        num_timesteps = tf.maximum(x=num_timesteps, y=self.episode_count)",
            "",
            "# Check whether memory contains at least one timestep",
            "assertions = list()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=2222909)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=2222910)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'num_timesteps'), position=0, insert_id=2222911)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=2222912)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=2222913)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2222914)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2222915)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2222916)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2222917)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'maximum'), position=2, insert_id=2222918)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2222919)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=1, insert_id=2222920)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=2222921)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=3, insert_id=2222922)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=2222923)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'x'), position=0, insert_id=2222924)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2222925)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'num_timesteps'), position=2, insert_id=2222926)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'y'), position=0, insert_id=2222927)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2222928)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=2222929)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=2222930)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2222931)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'episode_count'), position=2, insert_id=2222932)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 24,
        "number": 5150,
        "neg_line": [],
        "pos_line": [
            "+num_timesteps = tf.maximum(x=num_timesteps, y=self.episode_count)"
        ],
        "core_change": "+num_timesteps = tf.maximum(x=num_timesteps, y=self.episode_count)",
        "core_API": "minimum"
    },
    {
        "commit_hash": "f4781a0b27ffb3ea61ecd25b0b87305e0960304e",
        "index": "82748cb5..599c88fe 100644",
        "commit_message": "update expected results of slow tests (#268)\n\n* update expected results of slow tests\n\n* relax sum and mean tests\n\n* Print shapes when reporting exception\n\n* formatting\n\n* fix sentence\n\n* relax test_stable_diffusion_fast_ddim for gpu fp16\n\n* relax flakey tests on GPU\n\n* added comment on large tolerences\n\n* black\n\n* format\n\n* set scheduler seed\n\n* added generator\n\n* use np.isclose\n\n* set num_inference_steps to 50\n\n* fix dep. warning\n\n* update expected_slice\n\n* preprocess if image\n\n* updated expected results\n\n* updated expected from CI\n\n* pass generator to VAE\n\n* undo change back to orig\n\n* use orignal\n\n* revert back the expected on cpu\n\n* revert back values for CPU\n\n* more undo\n\n* update result after using gen\n\n* update mean\n\n* set generator for mps\n\n* update expected on CI server\n\n* undo\n\n* use new seed every time\n\n* cpu manual seed\n\n* reduce num_inference_steps\n\n* style\n\n* use generator for randn\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class AutoencoderKL(ModelMixin, ConfigMixin):",
            "x = sample",
            "posterior = self.encode(x).latent_dist",
            "if sample_posterior:",
            "-            z = posterior.sample()",
            "+            z = posterior.sample(generator=generator)",
            "else:",
            "z = posterior.mode()",
            "dec = self.decode(z).sample"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=1, insert_id=104822)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'generator'), position=0, insert_id=104823)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=104824)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'generator'), position=2, insert_id=104825)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 5159,
        "neg_line": [
            "-z = posterior.sample()"
        ],
        "pos_line": [
            "+z = posterior.sample(generator=generator)"
        ],
        "core_change": "-z = posterior.sample() +z = posterior.sample(generator=generator)",
        "core_API": "encode"
    },
    {
        "commit_hash": "a346dd6c4ad1c5df8bbcc7af1afd13fc491f17da",
        "index": "aac46e05..b904c4ac 100644",
        "commit_message": "Sift test fix (#341)\n\n* fix filter2d docs\n\n* speed-up patch local feature tests\n\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestLAFAffineShapeEstimator:",
            "assert_allclose(new_laf, expected, atol=1e-4, rtol=1e-4)",
            "",
            "def test_gradcheck(self):",
            "-        batch_size, channels, height, width = 1, 1, 100, 100",
            "+        batch_size, channels, height, width = 1, 1, 40, 40",
            "patches = torch.rand(batch_size, channels, height, width)",
            "patches = utils.tensor_to_gradcheck_var(patches)  # to var",
            "-        laf = torch.tensor([[[[20., 0., 56.], [0., 20., 56.]]]])",
            "+        laf = torch.tensor([[[[5., 0., 26.], [0., 5., 26.]]]])",
            "laf = utils.tensor_to_gradcheck_var(laf)  # to var",
            "assert gradcheck(LAFAffineShapeEstimator(11), (laf, patches),",
            "raise_exception=True, rtol=1e-3, atol=1e-3)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=integer, text=100), value='40')",
            "Update(target_node=ASTNode(type=integer, text=100), value='40')",
            "Update(target_node=ASTNode(type=float, text=20.), value='5.')",
            "Update(target_node=ASTNode(type=float, text=56.), value='26.')",
            "Update(target_node=ASTNode(type=float, text=20.), value='5.')",
            "Update(target_node=ASTNode(type=float, text=56.), value='26.')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 6,
        "number": 5160,
        "neg_line": [
            "-batch_size, channels, height, width = 1, 1, 100, 100",
            "-laf = torch.tensor([[[[20., 0., 56.], [0., 20., 56.]]]])"
        ],
        "pos_line": [
            "+batch_size, channels, height, width = 1, 1, 40, 40",
            "+laf = torch.tensor([[[[5., 0., 26.], [0., 5., 26.]]]])"
        ],
        "core_change": "-batch_size, channels, height, width = 1, 1, 100, 100 +batch_size, channels, height, width = 1, 1, 40, 40 -laf = torch.tensor([[[[20., 0., 56.], [0., 20., 56.]]]]) +laf = torch.tensor([[[[5., 0., 26.], [0., 5., 26.]]]])",
        "core_API": "rand"
    },
    {
        "commit_hash": "955b353cd0140eb276b07f70ad2a13235286775b",
        "index": "7a6719be..5f7b12ed 100644",
        "commit_message": "num features to dataset, num_classes bugfix\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "train_dataset = MNISTSuperpixels(path, True, transform=transform)",
            "test_dataset = MNISTSuperpixels(path, False, transform=transform)",
            "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)",
            "test_loader = DataLoader(test_dataset, batch_size=64)",
            "+d = train_dataset",
            "",
            "",
            "class Net(torch.nn.Module):",
            "def __init__(self):",
            "super(Net, self).__init__()",
            "-        self.conv1 = SplineConv(1, 32, dim=2, kernel_size=5)",
            "+        self.conv1 = SplineConv(d.num_features, 32, dim=2, kernel_size=5)",
            "self.conv2 = SplineConv(32, 64, dim=2, kernel_size=5)",
            "self.conv3 = SplineConv(64, 64, dim=2, kernel_size=5)",
            "self.fc1 = torch.nn.Linear(4 * 64, 128)",
            "-        self.fc2 = torch.nn.Linear(128, 10)",
            "+        self.fc2 = torch.nn.Linear(128, d.num_classes)",
            "",
            "def forward(self, data):",
            "data.x = F.elu(self.conv1(data.x, data.edge_index, data.edge_attr))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=4, insert_id=1497001)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1497002)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'd'), position=0, insert_id=1497003)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1497004)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'train_dataset'), position=2, insert_id=1497005)",
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=1, insert_id=1497006)",
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=3, insert_id=1497007)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'd'), position=0, insert_id=1497008)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1497009)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'num_features'), position=2, insert_id=1497010)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'd'), position=0, insert_id=1497011)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1497012)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'num_classes'), position=2, insert_id=1497013)",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=integer, text=10))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 15,
        "number": 5162,
        "neg_line": [
            "-self.conv1 = SplineConv(1, 32, dim=2, kernel_size=5)",
            "-self.fc2 = torch.nn.Linear(128, 10)"
        ],
        "pos_line": [
            "+d = train_dataset",
            "+self.conv1 = SplineConv(d.num_features, 32, dim=2, kernel_size=5)",
            "+self.fc2 = torch.nn.Linear(128, d.num_classes)"
        ],
        "core_change": "+d = train_dataset -self.conv1 = SplineConv(1, 32, dim=2, kernel_size=5) +self.conv1 = SplineConv(d.num_features, 32, dim=2, kernel_size=5) -self.fc2 = torch.nn.Linear(128, 10) +self.fc2 = torch.nn.Linear(128, d.num_classes)",
        "core_API": "Linear"
    },
    {
        "commit_hash": "ccf9a5210366fcfedf2f1b87965bed2a2b1a5a43",
        "index": "e9f00953..e2f111c3 100755",
        "commit_message": "fix pep8 style in /scripts\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "with tf.Graph().as_default() as G:",
            "logger.info(\"Variables to dump:\")",
            "logger.info(\", \".join(var_dict.keys()))",
            "saver = tf.train.Saver(",
            "-                    var_list=var_dict,",
            "-                    write_version=tf.train.SaverDef.V2)",
            "+                var_list=var_dict,",
            "+                write_version=tf.train.SaverDef.V2)",
            "saver.save(sess, args.output, write_meta_graph=False)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 5164,
        "neg_line": [
            "-var_list=var_dict,",
            "-write_version=tf.train.SaverDef.V2)"
        ],
        "pos_line": [
            "+var_list=var_dict,",
            "+write_version=tf.train.SaverDef.V2)"
        ],
        "core_change": "-var_list=var_dict, -write_version=tf.train.SaverDef.V2) +var_list=var_dict, +write_version=tf.train.SaverDef.V2)",
        "core_API": "Graph"
    },
    {
        "commit_hash": "9f1260971f041f4dcabf063ca2964847c3e5fc2a",
        "index": "ec060c9da..b5436b7dc 100644",
        "commit_message": "Add DeiT (PyTorch) (#11056)\n\n* First draft of deit\n\n* More improvements\n\n* Remove DeiTTokenizerFast from init\n\n* Conversion script works\n\n* Add DeiT to ViT conversion script\n\n* Add tests, add head model, add support for deit in vit conversion script\n\n* Update model checkpoint names\n\n* Update image_mean and image_std, set resample to bicubic\n\n* Improve docs\n\n* Docs improvements\n\n* Add DeiTForImageClassificationWithTeacher to init\n\n* Address comments by @sgugger\n\n* Improve feature extractors\n\n* Make fix-copies\n\n* Minor fixes\n\n* Address comments by @patil-suraj\n\n* All models uploaded\n\n* Fix tests\n\n* Remove labels argument from DeiTForImageClassificationWithTeacher\n\n* Fix-copies, style and quality\n\n* Fix tests\n\n* Fix typo\n\n* Multiple docs improvements\n\n* More docs fixes\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ViTModelIntegrationTest(unittest.TestCase):",
            "inputs = feature_extractor(images=image, return_tensors=\"pt\").to(torch_device)",
            "",
            "# forward pass",
            "-        # currently failing",
            "-        # see https://discuss.pytorch.org/t/runtimeerror-expected-object-of-scalar-type-double-but-got-scalar-type-float-for-argument-2-weight/38961/2",
            "-        outputs = model(inputs[\"pixel_values\"])",
            "-        # outputs = model(**inputs)",
            "+        outputs = model(**inputs)",
            "",
            "# verify the logits",
            "expected_shape = torch.Size((1, 1000))"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('dictionary_splat', None), position=1, insert_id=1217789)",
            "Insert(target_node=IN(type=dictionary_splat), node=('**', '**'), position=0, insert_id=1217790)",
            "Move(target_node=IN(type=dictionary_splat), node=ASTNode(type=identifier, text=inputs), position=1)",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=string, text=\"pixel_values\"))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))"
        ],
        "plus_line": 1,
        "minus_line": 4,
        "AST_diff_line": 7,
        "number": 5167,
        "neg_line": [
            "-# currently failing",
            "-# see https://discuss.pytorch.org/t/runtimeerror-expected-object-of-scalar-type-double-but-got-scalar-type-float-for-argument-2-weight/38961/2",
            "-outputs = model(inputs[\"pixel_values\"])",
            "-# outputs = model(**inputs)"
        ],
        "pos_line": [
            "+outputs = model(**inputs)"
        ],
        "core_change": "-# currently failing -# see https://discuss.pytorch.org/t/runtimeerror-expected-object-of-scalar-type-double-but-got-scalar-type-float-for-argument-2-weight/38961/2 -outputs = model(inputs[\"pixel_values\"]) -# outputs = model(**inputs) +outputs = model(**inputs)",
        "core_API": "Size"
    },
    {
        "commit_hash": "f28f240828d7d262767a29be080502ebc5f984fb",
        "index": "7564d192a..e8f615ec8 100644",
        "commit_message": "fix owlvit tests, update docstring examples (#18586)\n\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class OwlViTModelIntegrationTest(unittest.TestCase):",
            "",
            "num_queries = int((model.config.vision_config.image_size / model.config.vision_config.patch_size) ** 2)",
            "self.assertEqual(outputs.pred_boxes.shape, torch.Size((1, num_queries, 4)))",
            "+",
            "expected_slice_boxes = torch.tensor(",
            "-            [[0.0948, 0.0471, 0.1915], [0.3194, 0.0583, 0.6498], [0.1441, 0.0452, 0.2197]]",
            "+            [[0.0691, 0.0445, 0.1373], [0.1592, 0.0456, 0.3192], [0.1632, 0.0423, 0.2478]]",
            ").to(torch_device)",
            "self.assertTrue(torch.allclose(outputs.pred_boxes[0, :3, :3], expected_slice_boxes, atol=1e-4))"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=list), node=(',', ','), position=2, insert_id=1193239)",
            "Insert(target_node=ASTNode(type=list), node=(']', ']'), position=7, insert_id=1193240)",
            "Update(target_node=ASTNode(type=float, text=0.0948), value='0.0691')",
            "Update(target_node=ASTNode(type=float, text=0.0471), value='0.0445')",
            "Update(target_node=ASTNode(type=float, text=0.1915), value='0.1373')",
            "Update(target_node=ASTNode(type=float, text=0.3194), value='0.1592')",
            "Update(target_node=ASTNode(type=float, text=0.0583), value='0.0456')",
            "Update(target_node=ASTNode(type=float, text=0.6498), value='0.3192')",
            "Update(target_node=ASTNode(type=float, text=0.1441), value='0.1632')",
            "Move(target_node=ASTNode(type=list), node=ASTNode(type=,, text=,), position=2)",
            "Update(target_node=ASTNode(type=float, text=0.0452), value='0.0423')",
            "Update(target_node=ASTNode(type=float, text=0.2197), value='0.2478')",
            "Move(target_node=ASTNode(type=list), node=ASTNode(type=], text=]), position=7)",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=], text=]))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 15,
        "number": 5171,
        "neg_line": [
            "-[[0.0948, 0.0471, 0.1915], [0.3194, 0.0583, 0.6498], [0.1441, 0.0452, 0.2197]]"
        ],
        "pos_line": [
            "+",
            "+[[0.0691, 0.0445, 0.1373], [0.1592, 0.0456, 0.3192], [0.1632, 0.0423, 0.2478]]"
        ],
        "core_change": "+ -[[0.0948, 0.0471, 0.1915], [0.3194, 0.0583, 0.6498], [0.1441, 0.0452, 0.2197]] +[[0.0691, 0.0445, 0.1373], [0.1592, 0.0456, 0.3192], [0.1632, 0.0423, 0.2478]]",
        "core_API": "assertEqual"
    },
    {
        "commit_hash": "709cc2e206f384bfacc6f2732203c3d37f09b228",
        "index": "35d1718d..de968049 100644",
        "commit_message": "[Fix] Missing device initializations (#672)\n\n* Fix device initialization in several variables\n\n* Fix device initialization in several variables\n\n* Skip CUDA tests for #666\n\n* Lint\n\n* Fix device initialization in several variables\n\n* Skip CUDA tests for #666\n\n* Lint\n\n* Remove half-precision tests (#649) and add a jit test\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def _scale_channel(im):",
            "# and then normalization by step.",
            "lut = (torch.cumsum(histo, 0) + (step // 2)) // step",
            "# Shift lut, prepending with 0.",
            "-        lut = torch.cat([torch.zeros(1), lut[:-1]])",
            "+        lut = torch.cat([torch.zeros(1, device=lut.device), lut[:-1]])",
            "# Clip the counts to be in range.  This is done",
            "# in the C code for image.point.",
            "return torch.clamp(lut, 0, 255)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=436827)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=436828)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=436829)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=436830)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=436831)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'lut'), position=0, insert_id=436832)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=436833)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=436834)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 5173,
        "neg_line": [
            "-lut = torch.cat([torch.zeros(1), lut[:-1]])"
        ],
        "pos_line": [
            "+lut = torch.cat([torch.zeros(1, device=lut.device), lut[:-1]])"
        ],
        "core_change": "-lut = torch.cat([torch.zeros(1), lut[:-1]]) +lut = torch.cat([torch.zeros(1, device=lut.device), lut[:-1]])",
        "core_API": "cumsum"
    },
    {
        "commit_hash": "6a649555c1199750f72b27c53434b74ad278e392",
        "index": "4aa207d..acbf257 100644",
        "commit_message": "Fix pylint errors\n\n",
        "file": "seq2seq.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class MetadataCaptureHook(TrainingHook):",
            "return tf.train.SessionRunArgs(self._global_step)",
            "else:",
            "tf.logging.info(\"Performing full trace on next step.\")",
            "-      run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)",
            "+      run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE) #pylint: disable=E1101",
            "return tf.train.SessionRunArgs(self._global_step, options=run_options)",
            "",
            "def after_run(self, _run_context, run_values):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 5174,
        "neg_line": [
            "-run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)"
        ],
        "pos_line": [
            "+run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE) #pylint: disable=E1101"
        ],
        "core_change": "-run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE) +run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE) #pylint: disable=E1101",
        "core_API": "SessionRunArgs"
    },
    {
        "commit_hash": "db76a5c5c64ee16f2f68e330da980999506faa90",
        "index": "1c9bd09f..c5122427 100644",
        "commit_message": "fix UserWarning from slow tensor conversion (#1948)\n\n* fix UserWarning from slow tensor conversion\n\n* Add latest docstring and tutorial changes\n\nCo-authored-by: github-actions[bot] <41898282+github-actions[bot]@users.noreply.github.com>\n",
        "file": "haystack.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def convert_features_to_dataset(features):",
            "\"Converting now to a tensor of default type long.\")",
            "",
            "# Convert all remaining python objects to torch long tensors",
            "-        cur_tensor = torch.tensor([sample[t_name] for sample in features], dtype=torch.long)",
            "+        cur_tensor = torch.as_tensor(np.array([sample[t_name] for sample in features]), dtype=torch.long)",
            "",
            "all_tensors.append(cur_tensor)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tensor), value='as_tensor')",
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=1, insert_id=245723)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=245724)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=245725)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'np'), position=0, insert_id=245726)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=245727)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'array'), position=2, insert_id=245728)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=245729)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=list_comprehension), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=245730)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 5176,
        "neg_line": [
            "-cur_tensor = torch.tensor([sample[t_name] for sample in features], dtype=torch.long)"
        ],
        "pos_line": [
            "+cur_tensor = torch.as_tensor(np.array([sample[t_name] for sample in features]), dtype=torch.long)"
        ],
        "core_change": "-cur_tensor = torch.tensor([sample[t_name] for sample in features], dtype=torch.long) +cur_tensor = torch.as_tensor(np.array([sample[t_name] for sample in features]), dtype=torch.long)",
        "core_API": "tensor"
    },
    {
        "commit_hash": "84ad6af49a92b9e9bcb87fb873146cb3b7886d42",
        "index": "ef5af712d..dc3c49524 100644",
        "commit_message": "minor fixes (#14026)\n\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class CLIPModelTester:",
            "",
            "def create_and_check_model(self, config, input_ids, attention_mask, pixel_values):",
            "model = CLIPModel(config).to(torch_device).eval()",
            "-        result = model(input_ids, pixel_values, attention_mask)",
            "+        with torch.no_grad():",
            "+            result = model(input_ids, pixel_values, attention_mask)",
            "self.parent.assertEqual(",
            "result.logits_per_image.shape, (self.vision_model_tester.batch_size, self.text_model_tester.batch_size)",
            ")"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('with_statement', None), position=3, insert_id=1210521)",
            "Insert(target_node=IN(type=with_statement), node=('with', 'with'), position=0, insert_id=1210522)",
            "Insert(target_node=IN(type=with_statement), node=('with_clause', None), position=1, insert_id=1210523)",
            "Insert(target_node=IN(type=with_statement), node=(':', ':'), position=2, insert_id=1210524)",
            "Insert(target_node=IN(type=with_statement), node=('block', None), position=3, insert_id=1210525)",
            "Insert(target_node=IN(type=with_clause), node=('with_item', None), position=0, insert_id=1210526)",
            "Move(target_node=IN(type=block), node=ASTNode(type=expression_statement), position=0)",
            "Insert(target_node=IN(type=with_item), node=('call', None), position=0, insert_id=1210527)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1210528)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1210529)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1210530)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1210531)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'no_grad'), position=2, insert_id=1210532)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1210533)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1210534)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 15,
        "number": 5177,
        "neg_line": [
            "-result = model(input_ids, pixel_values, attention_mask)"
        ],
        "pos_line": [
            "+with torch.no_grad():",
            "+result = model(input_ids, pixel_values, attention_mask)"
        ],
        "core_change": "-result = model(input_ids, pixel_values, attention_mask) +with torch.no_grad(): +result = model(input_ids, pixel_values, attention_mask)",
        "core_API": "no_grad"
    },
    {
        "commit_hash": "3ba8f6e3cdc662534bb387858cfe4fc11c5d5d82",
        "index": "7f93ad41..b699951f 100644",
        "commit_message": "fix indices in inverse_pose\n\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def inverse_pose(pose):",
            "",
            "pose_inv = pose.clone()",
            "pose_inv[..., :3, 0:3] = torch.transpose(pose[..., :3, :3], 1, 2)",
            "-    pose_inv[..., :3, 2:3] = torch.matmul(",
            "-        -1.0 * pose_inv[..., :3, :3], pose[..., :3, 2:3])",
            "+    pose_inv[..., :3, 3:4] = torch.matmul(",
            "+        -1.0 * pose_inv[..., :3, :3], pose[..., :3, 3:4])",
            "",
            "if len(pose_shape) == 2:",
            "pose_inv = torch.squeeze(pose_inv, dim=0)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=subscript), node=('slice', None), position=4, insert_id=481512)",
            "Insert(target_node=IN(type=slice), node=(':', ':'), position=0, insert_id=481513)",
            "Insert(target_node=IN(type=slice), node=('integer', '3'), position=1, insert_id=481514)",
            "Update(target_node=ASTNode(type=integer, text=2), value='3')",
            "Update(target_node=ASTNode(type=integer, text=3), value='4')",
            "Move(target_node=ASTNode(type=subscript), node=ASTNode(type=slice), position=4)",
            "Update(target_node=ASTNode(type=integer, text=2), value='3')",
            "Update(target_node=ASTNode(type=integer, text=3), value='4')",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=integer, text=3))",
            "Delete(target_node=ASTNode(type=slice))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 11,
        "number": 5178,
        "neg_line": [
            "-pose_inv[..., :3, 2:3] = torch.matmul(",
            "--1.0 * pose_inv[..., :3, :3], pose[..., :3, 2:3])"
        ],
        "pos_line": [
            "+pose_inv[..., :3, 3:4] = torch.matmul(",
            "+-1.0 * pose_inv[..., :3, :3], pose[..., :3, 3:4])"
        ],
        "core_change": "-pose_inv[..., :3, 2:3] = torch.matmul( --1.0 * pose_inv[..., :3, :3], pose[..., :3, 2:3]) +pose_inv[..., :3, 3:4] = torch.matmul( +-1.0 * pose_inv[..., :3, :3], pose[..., :3, 3:4])",
        "core_API": "clone"
    },
    {
        "commit_hash": "35ec3508f20b8a78f52d48ffd1d897be3c1f0485",
        "index": "989c2298..f4c33380 100644",
        "commit_message": "Rename analytic_mean, analytic_var, enumerable (#932)\n\n* Rename analytic_mean, analytic_var, enumerable\n\n* Fix typo in Histogram\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Parameterized(nn.Module):",
            "p = pyro.sample(param_name, prior)",
            "else:  # prior != None and mode = \"guide\"",
            "MAP_param_name = param_name + \"_MAP\"",
            "-            MAP_param_0 = torch.tensor(prior.analytic_mean().data.clone(), requires_grad=True)",
            "+            MAP_param_0 = torch.tensor(prior.mean.data.clone(), requires_grad=True)",
            "MAP_param = pyro.param(MAP_param_name, MAP_param_0)",
            "p = pyro.sample(param_name, dist.Delta(MAP_param))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=analytic_mean), value='mean')",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 5179,
        "neg_line": [
            "-MAP_param_0 = torch.tensor(prior.analytic_mean().data.clone(), requires_grad=True)"
        ],
        "pos_line": [
            "+MAP_param_0 = torch.tensor(prior.mean.data.clone(), requires_grad=True)"
        ],
        "core_change": "-MAP_param_0 = torch.tensor(prior.analytic_mean().data.clone(), requires_grad=True) +MAP_param_0 = torch.tensor(prior.mean.data.clone(), requires_grad=True)",
        "core_API": "sample"
    },
    {
        "commit_hash": "1a90fa80a761fe15e69111a625d82874ed783f7b",
        "index": "916a9fc3..9c6efb46 100644",
        "commit_message": "[Fix] Fix a lot of typos (#6190)\n\n* pre-commit: Add codespell to look for typos\n\n* fixup! Indentation\n\n* Update lint\n\n* Fix lint\n\n* Fix typo\n\n* Fix comments\n\nCo-authored-by: Christian Clauss <cclauss@me.com>\n",
        "file": "mmdetection.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "def test_patch_merging():",
            "assert out_size == (2, 1)",
            "assert x_out.size(1) == out_size[0] * out_size[1]",
            "",
            "-        # test different kernel_size with diffrent stride",
            "+        # test different kernel_size with different stride",
            "input_size = (6, 5)",
            "kernel_size = (6, 2)",
            "stride = (6, 2)"
        ],
        "hunk_index": 4,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 5180,
        "neg_line": [
            "-# test different kernel_size with diffrent stride"
        ],
        "pos_line": [
            "+# test different kernel_size with different stride"
        ],
        "core_change": "-# test different kernel_size with diffrent stride +# test different kernel_size with different stride",
        "core_API": "size"
    },
    {
        "commit_hash": "15c72be444fab0d1ba6879d097399b12f6a2a8b0",
        "index": "47a901f..f4effbd 100644",
        "commit_message": "Fix coordinate system conventions in renderer\n\nSummary:\n## Updates\n\n- Defined the world and camera coordinates according to this figure. The world coordinates are defined as having +Y up, +X left and +Z in.\n\n{F230888499}\n\n- Removed all flipping from blending functions.\n- Updated the rasterizer to return images with +Y up and +X left.\n- Updated all the mesh rasterizer tests\n    - The expected values are now defined in terms of the default +Y up, +X left\n    - Added tests where the triangles in the meshes are non symmetrical so that it is clear which direction +X and +Y are\n\n## Questions:\n- Should we have **scene settings** instead of raster settings?\n    - To be more correct we should be [z clipping in the rasterizer based on the far/near clipping planes](https://github.com/ShichenLiu/SoftRas/blob/master/soft_renderer/cuda/soft_rasterize_cuda_kernel.cu#L400) - these values are also required in the blending functions so should we make these scene level parameters and have a scene settings tuple which is available to the rasterizer and shader?\n\nReviewed By: gkioxari\n\nDifferential Revision: D20208604\n\nfbshipit-source-id: 55787301b1bffa0afa9618f0a0886cc681da51f3\n\n",
        "file": "pytorch3d.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def softmax_blend_naive(colors, fragments, blend_params):",
            "pixel_colors[n, h, w, :3] += (delta / denom) * bk_color",
            "pixel_colors[n, h, w, 3] = 1.0 - alpha",
            "",
            "-    return torch.flip(pixel_colors, [1])",
            "+    return pixel_colors",
            "",
            "",
            "class TestBlending(unittest.TestCase):"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=ASTNode(type=return_statement), node=ASTNode(type=identifier, text=pixel_colors), position=1)",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=flip))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=list))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 14,
        "number": 5183,
        "neg_line": [
            "-return torch.flip(pixel_colors, [1])"
        ],
        "pos_line": [
            "+return pixel_colors"
        ],
        "core_change": "-return torch.flip(pixel_colors, [1]) +return pixel_colors",
        "core_API": "flip"
    },
    {
        "commit_hash": "44267fff892d26e55f5c88a20b84d1c820d2d82a",
        "index": "0231c63a..1d2e7338 100644",
        "commit_message": "Ensure compatibility with torch>=1.13 torchvision>=0.14 (#3155)\n\n* Bump dependency versions to torch>1.13 torchvision>=0.14\n\n* Avoid setuptools>=60\n\n* Replace Tensor.lu() -> torch.linalg.lu_factor()\n\n* Fix test\n\n* Revert \"Bump dependency versions to torch>1.13 torchvision>=0.14\"\n\nThis reverts commit 90ec2066e783a6d1a0485394fe58320ce8065a46.\n\n* Add comment on lap\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class GeneralizedChannelPermute(ConditionedGeneralizedChannelPermute, TransformM",
            "W, _ = torch.linalg.qr(torch.randn(channels, channels))",
            "",
            "# Construct the partially pivoted LU-form and the pivots",
            "-        LU, pivots = W.lu()",
            "+        LU, pivots = torch.linalg.lu_factor(W)",
            "",
            "# Convert the pivots into the permutation matrix",
            "if permutation is None:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=670117)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=670118)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'lu_factor'), position=2, insert_id=670119)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=W), position=1)",
            "Insert(target_node=ASTNode(type=attribute), node=('identifier', 'torch'), position=0, insert_id=670120)",
            "Update(target_node=ASTNode(type=identifier, text=lu), value='linalg')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 5187,
        "neg_line": [
            "-LU, pivots = W.lu()"
        ],
        "pos_line": [
            "+LU, pivots = torch.linalg.lu_factor(W)"
        ],
        "core_change": "-LU, pivots = W.lu() +LU, pivots = torch.linalg.lu_factor(W)",
        "core_API": "qr"
    },
    {
        "commit_hash": "bafbfaa5718e1de72b805237d2c350aec11de9fe",
        "index": "c6379f09e..22386f3a4 100644",
        "commit_message": "Integration testing v1 (#71)\n\n* new version with bug fix for gpu\n\n* hotfix\n\n* current data type default to TEXT in order to avoid null data types (#67)\n\n* Cleanup (#68)\n\n* removed a duplicated helper funtion and moved it to helpers with a better name\n\n* removed unused code and files, fixed a small isue in the encoder that would have resulted in a crash\n\n* fixed signature of tryCastToNumber\n\n* added scraps file and two functions we want to keep in it\n\n* remoed test files\n\n* added comment about funtion psotioning\n\n* added integration testing and better csv sniffing\n\n* added test data to gitignore\n\n* finalized the predict part of testing\n\n* commented out problematic line, added logging\n\n* added some test files to gitignore\n\n* rasigin exceptions where appropriate\n\n",
        "file": "mindsdb.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class EncoderRNN(nn.Module):",
            "return output, hidden",
            "",
            "def initHidden(self):",
            "-        return torch.zeros(1, 1, self.hidden_size), device=device)",
            "+        return torch.zeros(1, 1, self.hidden_size, device=device)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=function_definition), node=('block', None), position=4, insert_id=611594)",
            "Insert(target_node=IN(type=block), node=('return_statement', None), position=0, insert_id=611595)",
            "Move(target_node=IN(type=return_statement), node=ASTNode(type=return, text=return), position=0)",
            "Insert(target_node=IN(type=return_statement), node=('call', None), position=1, insert_id=611596)",
            "Move(target_node=IN(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=611597)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=(, text=(), position=0)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=integer, text=1), position=1)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=,, text=,), position=2)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=integer, text=1), position=3)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=,, text=,), position=4)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=attribute), position=5)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=,, text=,), position=6)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=7, insert_id=611598)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=), text=)), position=8)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=identifier, text=device), position=0)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type==, text==), position=1)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=identifier, text=device), position=2)",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=expression_list))",
            "Delete(target_node=ASTNode(type=return_statement))",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 25,
        "number": 5190,
        "neg_line": [
            "-return torch.zeros(1, 1, self.hidden_size), device=device)"
        ],
        "pos_line": [
            "+return torch.zeros(1, 1, self.hidden_size, device=device)"
        ],
        "core_change": "-return torch.zeros(1, 1, self.hidden_size), device=device) +return torch.zeros(1, 1, self.hidden_size, device=device)",
        "core_API": "zeros"
    },
    {
        "commit_hash": "f17c111d9b33c07a3a5d0fc00651d0ccfe33aa2a",
        "index": "d372cfbb..0f7f39da 100644",
        "commit_message": "fix a bug in tutorial 1\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Input(Layer):",
            "logging.info(\"Input  %s: %s\" % (self.name, str(shape)))",
            "",
            "shape_without_none = [_ if _ is not None else 1 for _ in shape]",
            "-        self.outputs = self.forward(tf.initializers.constant(value=0.0)(shape_without_none), is_train=False)",
            "+        self.outputs = self.forward(tf.initializers.random_normal()(shape_without_none), is_train=False)",
            "",
            "def __call__(self, prev_layer):",
            "# FIXME: better exception raising"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=constant), value='random_normal')",
            "Delete(target_node=ASTNode(type=identifier, text=value))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=float, text=0.0))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 5191,
        "neg_line": [
            "-self.outputs = self.forward(tf.initializers.constant(value=0.0)(shape_without_none), is_train=False)"
        ],
        "pos_line": [
            "+self.outputs = self.forward(tf.initializers.random_normal()(shape_without_none), is_train=False)"
        ],
        "core_change": "-self.outputs = self.forward(tf.initializers.constant(value=0.0)(shape_without_none), is_train=False) +self.outputs = self.forward(tf.initializers.random_normal()(shape_without_none), is_train=False)",
        "core_API": "info"
    },
    {
        "commit_hash": "dbb654232d64a40242a979e0b4f76b5dedf1a525",
        "index": "64b86cd..5eea71e 100644",
        "commit_message": "Fixes for 2.7\n\n",
        "file": "examples.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def read_image_file(path):",
            "row = []",
            "img.append(row)",
            "for c in range(num_cols):",
            "-                    row.append(data[idx])",
            "+                    row.append(parse_byte(data[idx]))",
            "idx += 1",
            "assert len(images) == length",
            "-        out = torch.FloatTensor(images)",
            "-        return out",
            "+        return torch.FloatTensor(images)",
            "",
            "print(\"Loading training set\")",
            "training_set = ("
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('return_statement', None), position=6, insert_id=192124)",
            "Insert(target_node=IN(type=return_statement), node=('return', 'return'), position=0, insert_id=192125)",
            "Move(target_node=IN(type=return_statement), node=ASTNode(type=call), position=1)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=192126)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=192127)",
            "Insert(target_node=IN(type=argument_list), node=('call', None), position=1, insert_id=192128)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=192129)",
            "Insert(target_node=IN(type=call), node=('identifier', 'parse_byte'), position=0, insert_id=192130)",
            "Move(target_node=IN(type=call), node=ASTNode(type=argument_list), position=1)",
            "Delete(target_node=ASTNode(type=identifier, text=out))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=return, text=return))",
            "Delete(target_node=ASTNode(type=identifier, text=out))",
            "Delete(target_node=ASTNode(type=return_statement))"
        ],
        "plus_line": 2,
        "minus_line": 3,
        "AST_diff_line": 16,
        "number": 5192,
        "neg_line": [
            "-row.append(data[idx])",
            "-out = torch.FloatTensor(images)",
            "-return out"
        ],
        "pos_line": [
            "+row.append(parse_byte(data[idx]))",
            "+return torch.FloatTensor(images)"
        ],
        "core_change": "-row.append(data[idx]) +row.append(parse_byte(data[idx])) -out = torch.FloatTensor(images) -return out +return torch.FloatTensor(images)",
        "core_API": "append"
    },
    {
        "commit_hash": "c8f250d1b761b9d14c362bb6830a1be8db4cf100",
        "index": "1249607f..495b5fd2 100644",
        "commit_message": "Fixed bugs about cascade_roi_head. (#3244)\n\n* Fixed bugs about cascade_roi_head.\n\n* Fixed the bug of refining bg rois at training when reg_class_agnostic is False.\n\n* Fixed the bug of potentially infering roi label to be bg at inference.\n\n* Reformatted code.\n\n* Ensured foreground roi_labels when refining bboxes at training.\n\n* Fixed typo.\n\n* Fixed incompatibility between mmdet2.2(ops/conv_ws.py) and mmcv1.0.0\n\n* Fixed typo.\n",
        "file": "mmdetection.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class CascadeRoIHead(BaseRoIHead, BBoxTestMixin, MaskTestMixin):",
            "ms_scores.append(bbox_results['cls_score'])",
            "",
            "if i < self.num_stages - 1:",
            "-                    bbox_label = bbox_results['cls_score'].argmax(dim=1)",
            "+                    bbox_label = bbox_results['cls_score'][:, :-1].argmax(",
            "+                        dim=1)",
            "rois = self.bbox_head[i].regress_by_class(",
            "rois, bbox_label, bbox_results['bbox_pred'],",
            "img_meta[0])"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=ASTNode(type=subscript), node=ASTNode(type=subscript), position=0)",
            "Insert(target_node=ASTNode(type=subscript), node=('[', '['), position=1, insert_id=636202)",
            "Insert(target_node=ASTNode(type=subscript), node=('slice', None), position=2, insert_id=636203)",
            "Insert(target_node=ASTNode(type=subscript), node=(',', ','), position=3, insert_id=636204)",
            "Insert(target_node=ASTNode(type=subscript), node=('slice', None), position=4, insert_id=636205)",
            "Insert(target_node=ASTNode(type=subscript), node=(']', ']'), position=5, insert_id=636206)",
            "Insert(target_node=IN(type=slice), node=(':', ':'), position=0, insert_id=636207)",
            "Insert(target_node=IN(type=slice), node=(':', ':'), position=0, insert_id=636208)",
            "Insert(target_node=IN(type=slice), node=('unary_operator', '-1'), position=1, insert_id=636209)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 5194,
        "neg_line": [
            "-bbox_label = bbox_results['cls_score'].argmax(dim=1)"
        ],
        "pos_line": [
            "+bbox_label = bbox_results['cls_score'][:, :-1].argmax(",
            "+dim=1)"
        ],
        "core_change": "-bbox_label = bbox_results['cls_score'].argmax(dim=1) +bbox_label = bbox_results['cls_score'][:, :-1].argmax( +dim=1)",
        "core_API": "append"
    },
    {
        "commit_hash": "a6ae97ac0922382a1e2ed7c593bb064f141001a3",
        "index": "1a3156250..bdd31a5c7 100644",
        "commit_message": "fixed hpc save, load. cleaned apu\n\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_model_saving_loading():",
            "# make prediction",
            "# assert that both predictions are the same",
            "new_pred = model_2(x)",
            "-    assert torch.eq(pred_before_saving, new_pred)",
            "+    assert torch.all(torch.eq(pred_before_saving, new_pred)).item() == 1",
            "",
            "clear_save_dir()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assert_statement), node=('comparison_operator', None), position=1, insert_id=590778)",
            "Insert(target_node=IN(type=comparison_operator), node=('call', None), position=0, insert_id=590779)",
            "Insert(target_node=IN(type=comparison_operator), node=('==', '=='), position=1, insert_id=590780)",
            "Insert(target_node=IN(type=comparison_operator), node=('integer', '1'), position=2, insert_id=590781)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=590782)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=590783)",
            "Insert(target_node=IN(type=attribute), node=('call', None), position=0, insert_id=590784)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=590785)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'item'), position=2, insert_id=590786)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=590787)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=590788)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=590789)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=590790)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=590791)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=590792)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'all'), position=2, insert_id=590793)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=590794)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=call), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=590795)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 19,
        "number": 5198,
        "neg_line": [
            "-assert torch.eq(pred_before_saving, new_pred)"
        ],
        "pos_line": [
            "+assert torch.all(torch.eq(pred_before_saving, new_pred)).item() == 1"
        ],
        "core_change": "-assert torch.eq(pred_before_saving, new_pred) +assert torch.all(torch.eq(pred_before_saving, new_pred)).item() == 1",
        "core_API": "eq"
    },
    {
        "commit_hash": "afa11399172a787ed6fb510e53eaf504aa4b84ab",
        "index": "94dd966f..3067892b 100644",
        "commit_message": "Fix distributed trainer (fix #671)\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def override_to_local_variable(enable=True):",
            "ns = orig_vs.original_name_scope",
            "with tf.variable_scope(",
            "orig_vs, custom_getter=custom_getter):",
            "-                with tf.name_scope(ns + '/'):",
            "+                with tf.name_scope(ns + '/' if ns else ''):",
            "yield",
            "else:",
            "yield"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('conditional_expression', None), position=1, insert_id=2289342)",
            "Move(target_node=IN(type=conditional_expression), node=ASTNode(type=binary_operator), position=0)",
            "Insert(target_node=IN(type=conditional_expression), node=('if', 'if'), position=1, insert_id=2289343)",
            "Insert(target_node=IN(type=conditional_expression), node=('identifier', 'ns'), position=2, insert_id=2289344)",
            "Insert(target_node=IN(type=conditional_expression), node=('else', 'else'), position=3, insert_id=2289345)",
            "Insert(target_node=IN(type=conditional_expression), node=('string', \"''\"), position=4, insert_id=2289346)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 5201,
        "neg_line": [
            "-with tf.name_scope(ns + '/'):"
        ],
        "pos_line": [
            "+with tf.name_scope(ns + '/' if ns else ''):"
        ],
        "core_change": "-with tf.name_scope(ns + '/'): +with tf.name_scope(ns + '/' if ns else ''):",
        "core_API": "variable_scope"
    },
    {
        "commit_hash": "129350f37c96be5717d9c195b701da57692fd24e",
        "index": "5d2a4e86..d6457bf7 100755",
        "commit_message": "fixed problem with multi-state/action networks and scoping\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Gaussian(Distribution):",
            "self.shape = shape",
            "action_size = util.prod(self.shape)",
            "",
            "-        with tf.name_scope(name=scope):",
            "-            self.mean = Linear(size=action_size, bias=mean, scope='mean')",
            "-            self.log_stddev = Linear(size=action_size, bias=log_stddev, scope='log-stddev')",
            "+        self.mean = Linear(size=action_size, bias=mean, scope='mean')",
            "+        self.log_stddev = Linear(size=action_size, bias=log_stddev, scope='log-stddev')",
            "",
            "super(Gaussian, self).__init__(scope, summary_labels)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=module), node=ASTNode(type=expression_statement), position=3)",
            "Move(target_node=ASTNode(type=module), node=ASTNode(type=expression_statement), position=4)",
            "Delete(target_node=ASTNode(type=with, text=with))",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=name_scope))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=name))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=scope))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=with_item))",
            "Delete(target_node=ASTNode(type=with_clause))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=block))",
            "Delete(target_node=ASTNode(type=with_statement))"
        ],
        "plus_line": 2,
        "minus_line": 3,
        "AST_diff_line": 20,
        "number": 5202,
        "neg_line": [
            "-with tf.name_scope(name=scope):",
            "-self.mean = Linear(size=action_size, bias=mean, scope='mean')",
            "-self.log_stddev = Linear(size=action_size, bias=log_stddev, scope='log-stddev')"
        ],
        "pos_line": [
            "+self.mean = Linear(size=action_size, bias=mean, scope='mean')",
            "+self.log_stddev = Linear(size=action_size, bias=log_stddev, scope='log-stddev')"
        ],
        "core_change": "-with tf.name_scope(name=scope): -self.mean = Linear(size=action_size, bias=mean, scope='mean') -self.log_stddev = Linear(size=action_size, bias=log_stddev, scope='log-stddev') +self.mean = Linear(size=action_size, bias=mean, scope='mean') +self.log_stddev = Linear(size=action_size, bias=log_stddev, scope='log-stddev')",
        "core_API": "prod"
    },
    {
        "commit_hash": "4f3bfa55e35f4803dce8ddccc3baa92daf6332b0",
        "index": "b5c9bfa..3abc1d0 100644",
        "commit_message": "Fix PReLU alphas reuse_variables error\n\n",
        "file": "tflearn.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def prelu(x, channel_shared=False, weights_init='zeros', restore=True, name=\"PRe",
            "i_scope = \"\"",
            "if hasattr(x, 'scope'):",
            "if x.scope: i_scope = x.scope",
            "-    with tf.name_scope(i_scope + name) as scope:",
            "+    with tf.variable_scope(i_scope + name) as scope:",
            "W_init = initializations.get(weights_init)()",
            "alphas = va.variable(shape=w_shape, initializer=W_init,",
            "-                             restore=restore, name=scope + \"alphas\")",
            "+                             restore=restore, name=\"alphas\")",
            "",
            "x = tf.nn.relu(x) + tf.mul(alphas, (x - tf.abs(x))) * 0.5"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=name_scope), value='variable_scope')",
            "Move(target_node=ASTNode(type=keyword_argument), node=ASTNode(type=string, text=\"alphas\"), position=2)",
            "Delete(target_node=ASTNode(type=identifier, text=scope))",
            "Delete(target_node=ASTNode(type=+, text=+))",
            "Delete(target_node=ASTNode(type=binary_operator))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 5,
        "number": 5204,
        "neg_line": [
            "-with tf.name_scope(i_scope + name) as scope:",
            "-restore=restore, name=scope + \"alphas\")"
        ],
        "pos_line": [
            "+with tf.variable_scope(i_scope + name) as scope:",
            "+restore=restore, name=\"alphas\")"
        ],
        "core_change": "-with tf.name_scope(i_scope + name) as scope: +with tf.variable_scope(i_scope + name) as scope: -restore=restore, name=scope + \"alphas\") +restore=restore, name=\"alphas\")",
        "core_API": "name_scope"
    },
    {
        "commit_hash": "7f353e1c0f8621bbd8919cfc5e70a1387770824b",
        "index": "d0da814..6a318c0 100644",
        "commit_message": "Fix symbol which no longer exists in tf2 root.\n\nThis was leading to a weird error if the filesystem threw an OpError\nwhen finding out if a path exists.\n\nPiperOrigin-RevId: 303131046\n\n",
        "file": "hub.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class PathResolver(Resolver):",
            "def is_supported(self, handle):",
            "try:",
            "return tf_v1.gfile.Exists(handle)",
            "-    except tf.OpError:",
            "+    except tf.errors.OpError:",
            "return False",
            "",
            "def __call__(self, handle):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=1949073)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=1949074)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=tf), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'errors'), position=2, insert_id=1949075)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 5205,
        "neg_line": [
            "-except tf.OpError:"
        ],
        "pos_line": [
            "+except tf.errors.OpError:"
        ],
        "core_change": "-except tf.OpError: +except tf.errors.OpError:",
        "core_API": "Exists"
    },
    {
        "commit_hash": "e15bc157d85d4744c1293898128e3dc3b4f0a729",
        "index": "e515ad04..afe51e04 100644",
        "commit_message": "Fix #873\n\n",
        "file": "TTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class Trainer:",
            "# main model optimizer step",
            "loss_dict[\"loss\"].backward()",
            "if grad_clip > 0:",
            "-                grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip, error_if_nonfinite=False)",
            "+                grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)",
            "optimizer.step()",
            "",
            "step_time = time.time() - step_start_time"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=error_if_nonfinite))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=false, text=False))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 5208,
        "neg_line": [
            "-grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip, error_if_nonfinite=False)"
        ],
        "pos_line": [
            "+grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)"
        ],
        "core_change": "-grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip, error_if_nonfinite=False) +grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)",
        "core_API": "clip_grad_norm_"
    },
    {
        "commit_hash": "af957044fbcb9e86a9a30e383ab79da22252b409",
        "index": "c2bace62..5172bdbd 100644",
        "commit_message": "[Refactor]: support batch input in bbox coder (#4721)\n\n* bbox coder support batch infer\n\n* Add unit test\n\n* get img_shape as tensor in single_stage\n\n* Fix max_shape\n\n* Update docs and unittest\n\nCo-authored-by: maningsheng <maningsheng@sensetime.com>\n",
        "file": "mmdetection.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SingleStageDetector(BaseDetector):",
            "outs = self.bbox_head(x)",
            "# get origin input shape to support onnx dynamic shape",
            "if torch.onnx.is_in_onnx_export():",
            "-            img_metas[0]['img_shape_for_onnx'] = img.shape[2:]",
            "+            # get shape as tensor",
            "+            img_shape = torch._shape_as_tensor(img)[2:]",
            "+            img_metas[0]['img_shape_for_onnx'] = img_shape",
            "bbox_list = self.bbox_head.get_bboxes(",
            "*outs, img_metas, rescale=rescale)",
            "# skip post-processing when exporting to ONNX"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=block), node=('expression_statement', None), position=0, insert_id=630334)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=630335)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'img_shape'), position=0, insert_id=630336)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=630337)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=subscript), position=2)",
            "Update(target_node=ASTNode(type=identifier, text=img), value='img_shape')",
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=identifier, text=img), position=2)",
            "Insert(target_node=ASTNode(type=subscript), node=('call', None), position=0, insert_id=630338)",
            "Move(target_node=IN(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=630339)",
            "Insert(target_node=ASTNode(type=attribute), node=('identifier', 'torch'), position=0, insert_id=630340)",
            "Update(target_node=ASTNode(type=identifier, text=shape), value='_shape_as_tensor')",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=630341)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'img'), position=1, insert_id=630342)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=630343)"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 15,
        "number": 5211,
        "neg_line": [
            "-img_metas[0]['img_shape_for_onnx'] = img.shape[2:]"
        ],
        "pos_line": [
            "+# get shape as tensor",
            "+img_shape = torch._shape_as_tensor(img)[2:]",
            "+img_metas[0]['img_shape_for_onnx'] = img_shape"
        ],
        "core_change": "-img_metas[0]['img_shape_for_onnx'] = img.shape[2:] +# get shape as tensor +img_shape = torch._shape_as_tensor(img)[2:] +img_metas[0]['img_shape_for_onnx'] = img_shape",
        "core_API": "bbox_head"
    },
    {
        "commit_hash": "66f65ca0c75a5f43c61977d20613cea97228a988",
        "index": "4a2b20d3..9fb74b1f 100644",
        "commit_message": "release switch norm layer (#737)\n\n* release switch norm layer\n\n* change log\n\n* fix john suggestion\n\n* refactoring\n\n* codacy fix\n\n* typo fix\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "layers",
        "change": [
            "class Layer_Normalization_Test(unittest.TestCase):",
            "tf.reset_default_graph()",
            "",
            "def test_all_layers(self):",
            "-        self.assertEqual(len(self.data[\"train_network\"][\"layers\"]), 7)",
            "-        self.assertEqual(len(self.data[\"eval_network\"][\"layers\"]), 7)",
            "+        self.assertEqual(len(self.data[\"train_network\"][\"layers\"]), 8)",
            "+        self.assertEqual(len(self.data[\"eval_network\"][\"layers\"]), 8)",
            "",
            "def test_all_params(self):",
            "-        self.assertEqual(len(self.data[\"train_network\"][\"params\"]), 12)",
            "+        self.assertEqual(len(self.data[\"train_network\"][\"params\"]), 16)",
            "",
            "def test_n_params(self):",
            "-        self.assertEqual(self.data[\"train_network\"][\"n_params\"], 60560)",
            "+        self.assertEqual(self.data[\"train_network\"][\"n_params\"], 60726)",
            "",
            "",
            "if __name__ == '__main__':"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=function_definition), node=('parameters', None), position=2, insert_id=2633996)",
            "Move(target_node=ASTNode(type=function_definition), node=ASTNode(type=parameters), position=2)",
            "Insert(target_node=IN(type=parameters), node=('(', '('), position=0, insert_id=2633997)",
            "Insert(target_node=IN(type=parameters), node=('identifier', 'self'), position=1, insert_id=2633998)",
            "Insert(target_node=IN(type=parameters), node=(')', ')'), position=2, insert_id=2633999)",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2634000)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=2634001)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2634002)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'assertEqual'), position=2, insert_id=2634003)",
            "Update(target_node=ASTNode(type=integer, text=7), value='8')",
            "Update(target_node=ASTNode(type=integer, text=7), value='8')",
            "Update(target_node=ASTNode(type=integer, text=12), value='16')",
            "Update(target_node=ASTNode(type=integer, text=60560), value='60726')",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=parameters))",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=assertEqual))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 4,
        "minus_line": 4,
        "AST_diff_line": 22,
        "number": 5214,
        "neg_line": [
            "-self.assertEqual(len(self.data[\"train_network\"][\"layers\"]), 7)",
            "-self.assertEqual(len(self.data[\"eval_network\"][\"layers\"]), 7)",
            "-self.assertEqual(len(self.data[\"train_network\"][\"params\"]), 12)",
            "-self.assertEqual(self.data[\"train_network\"][\"n_params\"], 60560)"
        ],
        "pos_line": [
            "+self.assertEqual(len(self.data[\"train_network\"][\"layers\"]), 8)",
            "+self.assertEqual(len(self.data[\"eval_network\"][\"layers\"]), 8)",
            "+self.assertEqual(len(self.data[\"train_network\"][\"params\"]), 16)",
            "+self.assertEqual(self.data[\"train_network\"][\"n_params\"], 60726)"
        ],
        "core_change": "-self.assertEqual(len(self.data[\"train_network\"][\"layers\"]), 7) -self.assertEqual(len(self.data[\"eval_network\"][\"layers\"]), 7) +self.assertEqual(len(self.data[\"train_network\"][\"layers\"]), 8) +self.assertEqual(len(self.data[\"eval_network\"][\"layers\"]), 8) -self.assertEqual(len(self.data[\"train_network\"][\"params\"]), 12) +self.assertEqual(len(self.data[\"train_network\"][\"params\"]), 16) -self.assertEqual(self.data[\"train_network\"][\"n_params\"], 60560) +self.assertEqual(self.data[\"train_network\"][\"n_params\"], 60726)",
        "core_API": "reset_default_graph"
    },
    {
        "commit_hash": "261cbbcf0e3d87a2e5fc4ff44546b1ac11d0d4d1",
        "index": "835e46547..aa5bfc641 100644",
        "commit_message": "fix normalization\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_decoder_cache():",
            "attention_dim=adim,",
            "linear_units=3,",
            "num_blocks=2,",
            "+        normalize_before=normalize_before,",
            "dropout_rate=0.0)",
            "dlayer = decoder.decoders[0]",
            "memory = torch.randn(2, 5, adim)",
            "",
            "-    x = torch.randn(2, 5, adim)",
            "+    x = torch.randn(2, 5, adim) * 100",
            "mask = subsequent_mask(x.shape[1]).unsqueeze(0)",
            "prev_mask = mask[:, :-1, :-1]",
            "decoder.eval()"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=4, insert_id=165597)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=165598)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'normalize_before'), position=0, insert_id=165599)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=165600)",
            "Insert(target_node=IN(type=assignment), node=('expression_list', None), position=2, insert_id=165601)",
            "Insert(target_node=ASTNode(type=assignment), node=('binary_operator', None), position=2, insert_id=165602)",
            "Insert(target_node=IN(type=expression_list), node=('identifier', 'normalize_before'), position=0, insert_id=165603)",
            "Insert(target_node=IN(type=expression_list), node=(',', ','), position=1, insert_id=165604)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=binary_operator), node=('*', '*'), position=1, insert_id=165605)",
            "Insert(target_node=IN(type=binary_operator), node=('integer', '100'), position=2, insert_id=165606)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 5217,
        "neg_line": [
            "-x = torch.randn(2, 5, adim)"
        ],
        "pos_line": [
            "+normalize_before=normalize_before,",
            "+x = torch.randn(2, 5, adim) * 100"
        ],
        "core_change": "+normalize_before=normalize_before, -x = torch.randn(2, 5, adim) +x = torch.randn(2, 5, adim) * 100",
        "core_API": "randn"
    },
    {
        "commit_hash": "24bece4d27f15195d24aafcc8db37e412ec0d44d",
        "index": "3382f764..213fc2a7 100644",
        "commit_message": "small fixes\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class MLPValueFunction(ValueFunction):",
            "with tf.variable_scope(\"mlp_value_function\"):",
            "self.input = tf.placeholder(tf.float32, shape=[None, self.get_features_size(state_size)], name=\"input\")",
            "",
            "-            define_network = NeuralNetwork.layered_network((",
            "+            network_builder = NeuralNetwork.layered_network((",
            "{'type': 'dense', 'num_outputs': layer_size},",
            "{'type': 'dense', 'num_outputs': 1}))",
            "-            network = NeuralNetwork(define_network=define_network, inputs=[self.input])",
            "+            network = NeuralNetwork(network_builder=network_builder, inputs=[self.input])",
            "",
            "# hidden_1 = dense(layer_input=self.input, {'num_outputs': input_shape}, scope='hidden_1')",
            "# hidden_2 = dense(hidden_1, {'num_outputs': self.layer_size}, scope='hidden_2')",
            "# out = dense(hidden_2, {'num_outputs': 1}, scope='out')",
            "-            self.mlp = tf.reshape(network.output, (-1,))",
            "+            self.mlp = tf.reshape(network.output, (-1, 1))",
            "",
            "l2 = tf.nn.l2_loss(self.mlp - self.labels)",
            "self.update = tf.train.AdamOptimizer().minimize(l2)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=define_network), value='network_builder')",
            "Update(target_node=ASTNode(type=identifier, text=define_network), value='network_builder')",
            "Update(target_node=ASTNode(type=identifier, text=define_network), value='network_builder')",
            "Insert(target_node=ASTNode(type=tuple), node=('integer', '1'), position=3, insert_id=2245831)"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 4,
        "number": 5218,
        "neg_line": [
            "-define_network = NeuralNetwork.layered_network((",
            "-network = NeuralNetwork(define_network=define_network, inputs=[self.input])",
            "-self.mlp = tf.reshape(network.output, (-1,))"
        ],
        "pos_line": [
            "+network_builder = NeuralNetwork.layered_network((",
            "+network = NeuralNetwork(network_builder=network_builder, inputs=[self.input])",
            "+self.mlp = tf.reshape(network.output, (-1, 1))"
        ],
        "core_change": "-define_network = NeuralNetwork.layered_network(( +network_builder = NeuralNetwork.layered_network(( -network = NeuralNetwork(define_network=define_network, inputs=[self.input]) +network = NeuralNetwork(network_builder=network_builder, inputs=[self.input]) -self.mlp = tf.reshape(network.output, (-1,)) +self.mlp = tf.reshape(network.output, (-1, 1))",
        "core_API": "variable_scope"
    },
    {
        "commit_hash": "c17169dc110ed0699eefab04ca17537eb68ce713",
        "index": "3cd65a80f..e7c19cb9e 100644",
        "commit_message": "[RLlib] Fix all example scripts to run on GPUs. (#11105)\n\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TorchCustomLossModel(TorchModelV2, nn.Module):",
            "",
            "# Compute the IL loss.",
            "action_dist = TorchCategorical(logits, self.model_config)",
            "-        imitation_loss = torch.mean(",
            "-            -action_dist.logp(torch.from_numpy(batch[\"actions\"])))",
            "+        imitation_loss = torch.mean(-action_dist.logp(",
            "+            torch.from_numpy(batch[\"actions\"]).to(policy_loss[0].device)))",
            "self.imitation_loss_metric = imitation_loss.item()",
            "self.policy_loss_metric = np.mean([l.item() for l in policy_loss])"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1121564)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=1121565)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1121566)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to'), position=2, insert_id=1121567)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1121568)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=1121569)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1121570)",
            "Insert(target_node=IN(type=attribute), node=('subscript', None), position=0, insert_id=1121571)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1121572)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=1121573)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'policy_loss'), position=0, insert_id=1121574)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=1121575)",
            "Insert(target_node=IN(type=subscript), node=('integer', '0'), position=2, insert_id=1121576)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=1121577)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 15,
        "number": 5219,
        "neg_line": [
            "-imitation_loss = torch.mean(",
            "--action_dist.logp(torch.from_numpy(batch[\"actions\"])))"
        ],
        "pos_line": [
            "+imitation_loss = torch.mean(-action_dist.logp(",
            "+torch.from_numpy(batch[\"actions\"]).to(policy_loss[0].device)))"
        ],
        "core_change": "-imitation_loss = torch.mean( --action_dist.logp(torch.from_numpy(batch[\"actions\"]))) +imitation_loss = torch.mean(-action_dist.logp( +torch.from_numpy(batch[\"actions\"]).to(policy_loss[0].device)))",
        "core_API": "mean"
    },
    {
        "commit_hash": "87e6e4fe5c7e65cb69e70306f22de6daf16b6e14",
        "index": "117848994..c7e140732 100644",
        "commit_message": "Doc styler v2 (#14950)\n\n* New doc styler\n\n* Fix issue with args at the start\n\n* Code sample fixes\n\n* Style code examples in MDX\n\n* Fix more patterns\n\n* Typo\n\n* Typo\n\n* More patterns\n\n* Do without black for now\n\n* Get more info in error\n\n* Docstring style\n\n* Re-enable check\n\n* Quality\n\n* Fix add_end_docstring decorator\n\n* Fix docstring\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "PT_SPEECH_SEQ_CLASS_SAMPLE = r\"\"\"",
            "",
            ">>> # audio file is decoded on the fly",
            ">>> inputs = feature_extractor(dataset[0][\"audio\"][\"array\"], return_tensors=\"pt\")",
            "-    >>> logits = model(**inputs).logits >>> predicted_class_ids = torch.argmax(logits, dim=-1)",
            "+    >>> logits = model(**inputs).logits",
            "+    >>> predicted_class_ids = torch.argmax(logits, dim=-1)",
            ">>> predicted_label = model.config.id2label[predicted_class_ids]",
            "",
            ">>> # compute loss - target_label is e.g. \"down\""
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 5220,
        "neg_line": [
            "->>> logits = model(**inputs).logits >>> predicted_class_ids = torch.argmax(logits, dim=-1)"
        ],
        "pos_line": [
            "+>>> logits = model(**inputs).logits",
            "+>>> predicted_class_ids = torch.argmax(logits, dim=-1)"
        ],
        "core_change": "->>> logits = model(**inputs).logits >>> predicted_class_ids = torch.argmax(logits, dim=-1) +>>> logits = model(**inputs).logits +>>> predicted_class_ids = torch.argmax(logits, dim=-1)",
        "core_API": "argmax"
    },
    {
        "commit_hash": "cec92098641d3f4c395cd51d84ba93b691d1cdf3",
        "index": "180810d6..8877f4f2 100644",
        "commit_message": "Several micro optimizations (#4833)\n\n* benchmark transfers\n\n* create tensors directl on device when possible\n\n* fix\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Covariance(Metric):",
            "",
            "#     # Note: this gives an approximate aggregation of the covariance.",
            "#     device = gold_labels.device",
            "-        #     delta_mean_prediction = torch.tensor(delta_mean_prediction).to(device)",
            "-        #     delta_mean_label = torch.tensor(delta_mean_label).to(device)",
            "-        #     delta_co_moment = torch.tensor(delta_co_moment).to(device)",
            "-        #     _total_count = torch.tensor(updated_count).to(device)",
            "+        #     delta_mean_prediction = torch.tensor(delta_mean_prediction, device=device)",
            "+        #     delta_mean_label = torch.tensor(delta_mean_label, device=device)",
            "+        #     delta_co_moment = torch.tensor(delta_co_moment, device=device)",
            "+        #     _total_count = torch.tensor(updated_count, device=device)",
            "#     dist.all_reduce(delta_mean_prediction, op=dist.ReduceOp.SUM)",
            "#     dist.all_reduce(delta_mean_label, op=dist.ReduceOp.SUM)",
            "#     dist.all_reduce(delta_co_moment, op=dist.ReduceOp.SUM)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 4,
        "minus_line": 4,
        "AST_diff_line": 17,
        "number": 5221,
        "neg_line": [
            "-#     delta_mean_prediction = torch.tensor(delta_mean_prediction).to(device)",
            "-#     delta_mean_label = torch.tensor(delta_mean_label).to(device)",
            "-#     delta_co_moment = torch.tensor(delta_co_moment).to(device)",
            "-#     _total_count = torch.tensor(updated_count).to(device)"
        ],
        "pos_line": [
            "+#     delta_mean_prediction = torch.tensor(delta_mean_prediction, device=device)",
            "+#     delta_mean_label = torch.tensor(delta_mean_label, device=device)",
            "+#     delta_co_moment = torch.tensor(delta_co_moment, device=device)",
            "+#     _total_count = torch.tensor(updated_count, device=device)"
        ],
        "core_change": "-#     delta_mean_prediction = torch.tensor(delta_mean_prediction).to(device) -#     delta_mean_label = torch.tensor(delta_mean_label).to(device) -#     delta_co_moment = torch.tensor(delta_co_moment).to(device) -#     _total_count = torch.tensor(updated_count).to(device) +#     delta_mean_prediction = torch.tensor(delta_mean_prediction, device=device) +#     delta_mean_label = torch.tensor(delta_mean_label, device=device) +#     delta_co_moment = torch.tensor(delta_co_moment, device=device) +#     _total_count = torch.tensor(updated_count, device=device)",
        "core_API": "tensor"
    },
    {
        "commit_hash": "709b251070cf82c367579953b5320152938cb2dc",
        "index": "5057d74..f14259d 100644",
        "commit_message": "fix test for cuda conversion\n\n",
        "file": "tensorboardX.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "autograd",
        "change": [
            "def test_pytorch_np():",
            "",
            "# CUDA variable",
            "if torch.cuda.device_count()>0:",
            "-            assert isinstance(x2num.makenp(torch.autograd.variable.Variable(tensor)).cuda(), np.ndarray)",
            "+            assert isinstance(x2num.makenp(torch.autograd.variable.Variable(tensor).cuda()), np.ndarray)",
            "",
            "# python primitive type",
            "assert(isinstance(x2num.makenp(0), np.ndarray))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=IN(type=root), node=ASTNode(type=call), position=0)",
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=1, insert_id=1581384)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=2, insert_id=1581385)",
            "Move(target_node=IN(type=call), node=ASTNode(type=attribute), position=0)",
            "Move(target_node=IN(type=call), node=ASTNode(type=argument_list), position=1)",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=call), position=0)",
            "Delete(target_node=ASTNode(type=), text=)))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 5223,
        "neg_line": [
            "-assert isinstance(x2num.makenp(torch.autograd.variable.Variable(tensor)).cuda(), np.ndarray)"
        ],
        "pos_line": [
            "+assert isinstance(x2num.makenp(torch.autograd.variable.Variable(tensor).cuda()), np.ndarray)"
        ],
        "core_change": "-assert isinstance(x2num.makenp(torch.autograd.variable.Variable(tensor)).cuda(), np.ndarray) +assert isinstance(x2num.makenp(torch.autograd.variable.Variable(tensor).cuda()), np.ndarray)",
        "core_API": "device_count"
    },
    {
        "commit_hash": "c84edfab0b97cd13d4b2b1eea0ef62c477f5207f",
        "index": "31df9a779..aeee8eaa2 100644",
        "commit_message": "bug fix\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class LookAheadWordLM(nn.Module):",
            "elif xi == self.space:",
            "y[:, self.space] = self.zero",
            "y[:, self.eos] = self.zero",
            "-            return (wlm_state, cumsum_probs, new_node), torch.log(log_y)",
            "+            return (wlm_state, cumsum_probs, new_node), torch.log(y)",
            "else:  # if no path in the tree, transition probability is one",
            "log_y = torch.zeros(1, self.subword_dict_size)",
            "return (wlm_state, cumsum_probs, new_node), log_y"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=log_y), value='y')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 5225,
        "neg_line": [
            "-return (wlm_state, cumsum_probs, new_node), torch.log(log_y)"
        ],
        "pos_line": [
            "+return (wlm_state, cumsum_probs, new_node), torch.log(y)"
        ],
        "core_change": "-return (wlm_state, cumsum_probs, new_node), torch.log(log_y) +return (wlm_state, cumsum_probs, new_node), torch.log(y)",
        "core_API": "log"
    },
    {
        "commit_hash": "e48d53f9dee1d8fa06616633f7e02634c7985989",
        "index": "3032c69c6e..5e7344b761 100644",
        "commit_message": "lintfixbot: Auto-commit fixed lint errors in codebase\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def quantile(",
            "temp = a.detach()",
            "dimension = len(a.size())",
            "for x in axis:",
            "-            axis1 = x",
            "-            for axis2 in range(x+1,dimension):",
            "+            axis1 = x",
            "+            for axis2 in range(x + 1, dimension):",
            "temp = torch.transpose(temp, axis1, axis2)",
            "axis1 = axis2",
            "-        temp = torch.flatten(temp, start_dim=dimension-len(axis))",
            "+        temp = torch.flatten(temp, start_dim=dimension - len(axis))",
            "return torch.quantile(",
            "temp, q, dim=-1, keepdim=keepdims, interpolation=interpolation, out=out",
            ")",
            "-    return torch.quantile(a, q, dim=axis, keepdim=keepdims, interpolation=interpolation, out=out)",
            "+    return torch.quantile(",
            "+        a, q, dim=axis, keepdim=keepdims, interpolation=interpolation, out=out",
            "+    )",
            "",
            "",
            "quantile.support_native_out = True"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 6,
        "minus_line": 4,
        "AST_diff_line": 17,
        "number": 5226,
        "neg_line": [
            "-axis1 = x",
            "-for axis2 in range(x+1,dimension):",
            "-temp = torch.flatten(temp, start_dim=dimension-len(axis))",
            "-return torch.quantile(a, q, dim=axis, keepdim=keepdims, interpolation=interpolation, out=out)"
        ],
        "pos_line": [
            "+axis1 = x",
            "+for axis2 in range(x + 1, dimension):",
            "+temp = torch.flatten(temp, start_dim=dimension - len(axis))",
            "+return torch.quantile(",
            "+a, q, dim=axis, keepdim=keepdims, interpolation=interpolation, out=out",
            "+)"
        ],
        "core_change": "-axis1 = x -for axis2 in range(x+1,dimension): +axis1 = x +for axis2 in range(x + 1, dimension): -temp = torch.flatten(temp, start_dim=dimension-len(axis)) +temp = torch.flatten(temp, start_dim=dimension - len(axis)) -return torch.quantile(a, q, dim=axis, keepdim=keepdims, interpolation=interpolation, out=out) +return torch.quantile( +a, q, dim=axis, keepdim=keepdims, interpolation=interpolation, out=out +)",
        "core_API": "detach"
    },
    {
        "commit_hash": "869c65ba2c7a224db68971d137e52a776cbe2fdd",
        "index": "ff6c95e4b..604c752bf 100644",
        "commit_message": "Consistent ner features (#636)\n\n* update lince features\n\n* update xtreme pan-x features\n\n* update lince dataset_info\n\n* fix lince\n\n* update lince dataset_info\n\n* update xtreme dataset_info\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class Xtreme(datasets.GeneratorBasedBuilder):",
            "for line in f:",
            "if line.startswith(\"-DOCSTART-\") or line == \"\" or line == \"\\n\":",
            "if words:",
            "-                            yield guid_index, {\"words\": words, \"ner_tags\": ner_tags, \"langs\": langs}",
            "+                            yield guid_index, {\"words\": words, \"ner\": ner_tags, \"langs\": langs}",
            "guid_index += 1",
            "words = []",
            "ner_tags = []"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"ner_tags\"), value='\"ner\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 5227,
        "neg_line": [
            "-yield guid_index, {\"words\": words, \"ner_tags\": ner_tags, \"langs\": langs}"
        ],
        "pos_line": [
            "+yield guid_index, {\"words\": words, \"ner\": ner_tags, \"langs\": langs}"
        ],
        "core_change": "-yield guid_index, {\"words\": words, \"ner_tags\": ner_tags, \"langs\": langs} +yield guid_index, {\"words\": words, \"ner\": ner_tags, \"langs\": langs}",
        "core_API": "startswith"
    },
    {
        "commit_hash": "c05ace221339102abb1c8b0933b9cbfd51ddd4a3",
        "index": "8b9cd20a6..1036f8b46 100644",
        "commit_message": "Use f-strings in the dataset scripts (#3291)\n\n* Finishes #3257\n\nUsed f-strings to format the .py files in the dataset folder\n\n* Fix style\n\n* Fix hkcancor dataset\n\nCo-authored-by: Mario ako <mario@huggingface.co>\nCo-authored-by: mariosasko <mariosasko777@gmail.com>\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class TurkishMovieSentiment(datasets.GeneratorBasedBuilder):",
            "path_to_manual_file = os.path.abspath(os.path.expanduser(dl_manager.manual_dir))",
            "if not os.path.exists(path_to_manual_file):",
            "raise FileNotFoundError(",
            "-                \"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('turkishmoviesentiment', data_dir=...)` that includes a file name {}. Manual download instructions: {})\".format(",
            "-                    path_to_manual_file, _FILENAME, self.manual_download_instructions",
            "-                )",
            "+                f\"{path_to_manual_file} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('turkishmoviesentiment', data_dir=...)` that includes a file name {_FILENAME}. Manual download instructions: {self.manual_download_instructions})\"",
            ")",
            "return [",
            "datasets.SplitGenerator("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('string', 'f\"{path_to_manual_file} does not exist. Make sure you insert a manual dir via `datasets.load_dataset(\\'turkishmoviesentiment\\', data_dir=...)` that includes a file name {_FILENAME}. Manual download instructions: {self.manual_download_instructions})\"'), position=1, insert_id=1781658)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=), text=)), position=2)",
            "Delete(target_node=ASTNode(type=string, text=\"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('turkishmoviesentiment', data_dir=...)` that includes a file name {}. Manual download instructions: {})\"))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=format))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=path_to_manual_file))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=_FILENAME))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=manual_download_instructions))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=), text=)))"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 18,
        "number": 5230,
        "neg_line": [
            "-\"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('turkishmoviesentiment', data_dir=...)` that includes a file name {}. Manual download instructions: {})\".format(",
            "-path_to_manual_file, _FILENAME, self.manual_download_instructions",
            "-)"
        ],
        "pos_line": [
            "+f\"{path_to_manual_file} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('turkishmoviesentiment', data_dir=...)` that includes a file name {_FILENAME}. Manual download instructions: {self.manual_download_instructions})\""
        ],
        "core_change": "-\"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('turkishmoviesentiment', data_dir=...)` that includes a file name {}. Manual download instructions: {})\".format( -path_to_manual_file, _FILENAME, self.manual_download_instructions -) +f\"{path_to_manual_file} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('turkishmoviesentiment', data_dir=...)` that includes a file name {_FILENAME}. Manual download instructions: {self.manual_download_instructions})\"",
        "core_API": "abspath"
    },
    {
        "commit_hash": "6e124e7207f6459cb43f540cfb5a1c6cc9b00f7a",
        "index": "a45e70adf..f4ae97098 100644",
        "commit_message": "CI: precommit - docformatter (#8584)\n\n* CI: precommit - docformatter\n* fix deprecated\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DDPSpawnPlugin(ParallelPlugin):",
            "self.model.to(self.root_device)",
            "",
            "def pre_backward(self, closure_loss: torch.Tensor) -> None:",
            "-        \"\"\"Run before precision plugin executes backward\"\"\"",
            "+        \"\"\"Run before precision plugin executes backward.\"\"\"",
            "if not self.lightning_module.automatic_optimization:",
            "prepare_for_backward(self.model, closure_loss)",
            "",
            "def reduce(self, tensor, group: Optional[Any] = None, reduce_op: Union[ReduceOp, str] = \"mean\") -> torch.Tensor:",
            "-        \"\"\"",
            "-        Reduces a tensor from several distributed processes to one aggregated tensor.",
            "+        \"\"\"Reduces a tensor from several distributed processes to one aggregated tensor.",
            "",
            "Args:",
            "tensor: the tensor to sync and reduce"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"\"\"Run before precision plugin executes backward\"\"\"), value='\"\"\"Run before precision plugin executes backward.\"\"\"')"
        ],
        "plus_line": 2,
        "minus_line": 3,
        "AST_diff_line": 1,
        "number": 5232,
        "neg_line": [
            "-\"\"\"Run before precision plugin executes backward\"\"\"",
            "-\"\"\"",
            "-Reduces a tensor from several distributed processes to one aggregated tensor."
        ],
        "pos_line": [
            "+\"\"\"Run before precision plugin executes backward.\"\"\"",
            "+\"\"\"Reduces a tensor from several distributed processes to one aggregated tensor."
        ],
        "core_change": "-\"\"\"Run before precision plugin executes backward\"\"\" +\"\"\"Run before precision plugin executes backward.\"\"\" -\"\"\" -Reduces a tensor from several distributed processes to one aggregated tensor. +\"\"\"Reduces a tensor from several distributed processes to one aggregated tensor.",
        "core_API": "to"
    },
    {
        "commit_hash": "47e22b6f2c64d2934a23643e74276ffd276934a5",
        "index": "619e566b..c2d92a01 100644",
        "commit_message": "bugfix\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SHREC2016(InMemoryDataset):",
            "'{}_{}_shape_{}'.format(self.part, self.cat, i))",
            "data = read_off('{}.off'.format(path))",
            "y = read_txt_array('{}.baryc_gt'.format(path))",
            "-            data.y_idx = y[:, 0].to(torch.long)",
            "+            data.y = y[:, 0].to(torch.long) - 1",
            "data.y_baryc = y[:, 1:]",
            "train_list.append(data)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('binary_operator', None), position=2, insert_id=1055278)",
            "Update(target_node=ASTNode(type=identifier, text=y_idx), value='y')",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=binary_operator), node=('-', '-'), position=1, insert_id=1055279)",
            "Insert(target_node=IN(type=binary_operator), node=('integer', '1'), position=2, insert_id=1055280)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 5234,
        "neg_line": [
            "-data.y_idx = y[:, 0].to(torch.long)"
        ],
        "pos_line": [
            "+data.y = y[:, 0].to(torch.long) - 1"
        ],
        "core_change": "-data.y_idx = y[:, 0].to(torch.long) +data.y = y[:, 0].to(torch.long) - 1",
        "core_API": "append"
    },
    {
        "commit_hash": "e94abf66783f5e24ab37adc754ac902e827a4b4d",
        "index": "37c274c1..92cb3d5c 100644",
        "commit_message": "fix TF deprecation of concat/split/pack/unpack\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def Deconv2D(x, out_shape, kernel_shape,",
            "if use_bias:",
            "b = tf.get_variable('b', [out_channel], initializer=b_init)",
            "",
            "-    out_shape_dyn = tf.pack([tf.shape(x)[0]] + shp3_dyn)",
            "+    out_shape_dyn = tf.stack([tf.shape(x)[0]] + shp3_dyn)",
            "conv = tf.nn.conv2d_transpose(x, W, out_shape_dyn, stride4d, padding=padding)",
            "conv.set_shape(tf.TensorShape([None] + shp3_static))",
            "return nl(tf.nn.bias_add(conv, b) if use_bias else conv, name='output')"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=pack), value='stack')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 5235,
        "neg_line": [
            "-out_shape_dyn = tf.pack([tf.shape(x)[0]] + shp3_dyn)"
        ],
        "pos_line": [
            "+out_shape_dyn = tf.stack([tf.shape(x)[0]] + shp3_dyn)"
        ],
        "core_change": "-out_shape_dyn = tf.pack([tf.shape(x)[0]] + shp3_dyn) +out_shape_dyn = tf.stack([tf.shape(x)[0]] + shp3_dyn)",
        "core_API": "get_variable"
    },
    {
        "commit_hash": "d9c0d08f9a27077ab65ee7eecac0e289e22e1dbe",
        "index": "67b9bd182..8e11594cb 100755",
        "commit_message": "Flax Big Bird (#11967)\n\n* add flax bert\n\n* bert -> bigbird\n\n* original_full ported\n\n* add debugger\n\n* init block sparse\n\n* fix copies ; gelu_fast -> gelu_new\n\n* block sparse port\n\n* fix block sparse\n\n* block sparse working\n\n* all ckpts working\n\n* fix-copies\n\n* make quality\n\n* init tests\n\n* temporary fix for FlaxBigBirdForMultipleChoice\n\n* skip test_attention_outputs\n\n* fix\n\n* gelu_fast -> gelu_new ; fix multiple choice model\n\n* remove nsp\n\n* fix sequence classifier\n\n* fix\n\n* make quality\n\n* make fix-copies\n\n* finish\n\n* Delete debugger.ipynb\n\n* Update src/transformers/models/big_bird/modeling_flax_big_bird.py\n\n* make style\n\n* finish\n\n* bye bye jit flax tests\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class BigBirdForMultipleChoice(BigBirdPreTrainedModel):",
            "return_dict=return_dict,",
            ")",
            "",
            "-        sequence_output = outputs[0]",
            "+        pooled_output = outputs[1]",
            "",
            "-        pooled_output = self.sequence_summary(sequence_output)",
            "+        pooled_output = self.dropout(pooled_output)",
            "logits = self.classifier(pooled_output)",
            "reshaped_logits = logits.view(-1, num_choices)"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=sequence_output), value='pooled_output')",
            "Update(target_node=ASTNode(type=integer, text=0), value='1')",
            "Update(target_node=ASTNode(type=identifier, text=sequence_summary), value='dropout')",
            "Update(target_node=ASTNode(type=identifier, text=sequence_output), value='pooled_output')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 4,
        "number": 5237,
        "neg_line": [
            "-sequence_output = outputs[0]",
            "-pooled_output = self.sequence_summary(sequence_output)"
        ],
        "pos_line": [
            "+pooled_output = outputs[1]",
            "+pooled_output = self.dropout(pooled_output)"
        ],
        "core_change": "-sequence_output = outputs[0] +pooled_output = outputs[1] -pooled_output = self.sequence_summary(sequence_output) +pooled_output = self.dropout(pooled_output)",
        "core_API": "sequence_summary"
    },
    {
        "commit_hash": "dfb61caf77a02a735af0bf430f6d6082b7d01cfd",
        "index": "8a25be78c..e83674e21 100644",
        "commit_message": "fix #1692\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFXLNetMainLayer(tf.keras.layers.Layer):",
            "assert input_mask is None or attention_mask is None, \"You can only use one of input_mask (uses 1 for padding) \" \\",
            "\"or attention_mask (uses 0 for padding, added for compatbility with BERT). Please choose one.\"",
            "if input_mask is None and attention_mask is not None:",
            "-            input_mask = 1.0 - attention_mask",
            "+            input_mask = 1.0 - tf.cast(attention_mask, dtype=dtype_float)",
            "if input_mask is not None and perm_mask is not None:",
            "data_mask = input_mask[None] + perm_mask",
            "elif input_mask is not None and perm_mask is None:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=binary_operator), node=('call', None), position=2, insert_id=2385600)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2385601)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2385602)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2385603)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2385604)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cast'), position=2, insert_id=2385605)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2385606)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=attention_mask), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=2385607)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=3, insert_id=2385608)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=2385609)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=2385610)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2385611)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype_float'), position=2, insert_id=2385612)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 14,
        "number": 5242,
        "neg_line": [
            "-input_mask = 1.0 - attention_mask"
        ],
        "pos_line": [
            "+input_mask = 1.0 - tf.cast(attention_mask, dtype=dtype_float)"
        ],
        "core_change": "-input_mask = 1.0 - attention_mask +input_mask = 1.0 - tf.cast(attention_mask, dtype=dtype_float)",
        "core_API": "cast"
    },
    {
        "commit_hash": "a5f7c6368ea506bde4c5e64f7e8eb048c13a4dd9",
        "index": "4578851cf..74d8cf3d6 100644",
        "commit_message": "fixing docs (#2227)\n\n\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def roc(",
            "",
            ">>> x = torch.tensor([0, 1, 2, 3])",
            ">>> y = torch.tensor([0, 1, 2, 2])",
            "-        >>> fpr, tpr, thresholds = roc(x,y)",
            "+        >>> fpr, tpr, thresholds = roc(x, y)",
            ">>> fpr",
            "tensor([0.0000, 0.3333, 0.6667, 0.6667, 1.0000])",
            ">>> tpr"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 5243,
        "neg_line": [
            "->>> fpr, tpr, thresholds = roc(x,y)"
        ],
        "pos_line": [
            "+>>> fpr, tpr, thresholds = roc(x, y)"
        ],
        "core_change": "->>> fpr, tpr, thresholds = roc(x,y) +>>> fpr, tpr, thresholds = roc(x, y)",
        "core_API": "tensor"
    },
    {
        "commit_hash": "a72f2715eeb329eab0bdeca28d928e21d232dc2e",
        "index": "9a77e41b8..9e70f8dec 100644",
        "commit_message": "Improvements on syft Hook & Plans (#3080)\n\n* Fix hook by removing unused and confusing functions\n\n* Fix hook of PlaceHolder and add tracer to th.tensor\n\n* Update list of ambiguous and hook-excluded functions\n\n* Improve instantiation of placeholder in Operation response\n\n* Fix extract_key bug, add checks and explicative failures\n\n* Add tests for plans with comparison and funcs with no args\n\n* Add dtype serialization and small fix in ptr plan\n\n* Add checks to AdditiveSharedTensor\n\n* Add clarification error to Plan\n\n* Add support of multi pointer tensors on plan ptr with>1 locations\n\n* Import placeholder instantiation\n\n* Add a tag index for worker and a find_by_tag method\n\n* Clean object storge rm_obj and use tag index\n\n* Fix search behavior, optimize search using tag index\n\n* Update plans & pointerplans to support tag serialization\n\n* Fix tests on plans\n\n* Fix Protocol to pass tests\n\n* Fix baseworker search to handle empty response and empty query\n\n* Basic FL training plan example\n\n* Improve plan representation\n\n* Run black code formatter\n\n* Update training plan example notebooks\n\n* update order of params in FL training plan example\n\n* Fix serde test issues\n\n* Fix stale tests using worker.search + fix issues related to tags\n\n* Fix module for AST conv2d\n\n* Update tutorial using worker.search\n\n* Small fix on tests and autograd tensor\n\n* Fix test serde\n\n* pip dep fix\n\n* Address PR comments and clean code\n\n* Add a check and fix a typo\n\n* Add comment note\n\nCo-authored-by: Vova Manannikov <vmanannikov@broadsoft.com>\nCo-authored-by: Karl Higley <kmhigley@gmail.com>\n",
        "file": "PySyft.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def make_autogradtensor(**kwargs):",
            "\"simplified\": (",
            "CODE[syft.frameworks.torch.tensors.interpreters.autograd.AutogradTensor],",
            "(",
            "-                    None,  # owner",
            "agt.id,  # (int)",
            "msgpack.serde._simplify(",
            "syft.hook.local_worker, agt.child"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=none, text=None))",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 5244,
        "neg_line": [
            "-None,  # owner"
        ],
        "pos_line": [],
        "core_change": "-None,  # owner",
        "core_API": "_simplify"
    },
    {
        "commit_hash": "a4fe3a3fea83a260ad22f45e928d0eec6862fadc",
        "index": "51137200..beb106af 100644",
        "commit_message": "[Feat] Enabled Torch1.5.1 cpu support (#796)\n\n* Added py151 support\n\n* Enabled 1.5.1 CI\n\n* Typo fix\n\n* Fixed Equalize\n\n* Update setup.py\n\n* Bug fix\n\n* typo fix\n\n* Fixed mypy check\n\n* Update tests_cpu_versions.yml\n\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def rgb_to_hsv(image: torch.Tensor) -> torch.Tensor:",
            "",
            "# avoid division by zero",
            "deltac = torch.where(",
            "-        deltac == 0, torch.ones_like(deltac), deltac)",
            "+        deltac == 0, torch.ones_like(deltac, device=deltac.device, dtype=deltac.dtype), deltac)",
            "",
            "maxc_tmp = maxc.unsqueeze(-3) - image",
            "rc: torch.Tensor = maxc_tmp[..., 0, :, :]"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=433532)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=433533)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=433534)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=433535)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=433536)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=433537)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=433538)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=433539)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=433540)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=433541)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'deltac'), position=0, insert_id=433542)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=433543)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=433544)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'deltac'), position=0, insert_id=433545)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=433546)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dtype'), position=2, insert_id=433547)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 16,
        "number": 5245,
        "neg_line": [
            "-deltac == 0, torch.ones_like(deltac), deltac)"
        ],
        "pos_line": [
            "+deltac == 0, torch.ones_like(deltac, device=deltac.device, dtype=deltac.dtype), deltac)"
        ],
        "core_change": "-deltac == 0, torch.ones_like(deltac), deltac) +deltac == 0, torch.ones_like(deltac, device=deltac.device, dtype=deltac.dtype), deltac)",
        "core_API": "where"
    },
    {
        "commit_hash": "2a4e336b927f4423173b17eb641a599342425c09",
        "index": "05a0cc0a..2693e9fc 100644",
        "commit_message": "bug fixes, under construction\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class NormalizedAdvantageFunctions(ValueFunction):",
            ":param batch:=",
            ":return:",
            "\"\"\"",
            "-        float_terminals = tf.to_float(batch['terminals'])",
            "+        float_terminals = batch['terminals'].astype(float)",
            "+",
            "q_targets = batch['rewards'] + (1. - float_terminals) \\",
            "* self.gamma * self.get_target_value_estimate(batch['next_states'])"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"\"\"\n        float_terminals = tf.to_float(batch['terminals'])\nq_targets = batch['rewards'] + (1. - float_terminals) \\\n), value='\"\"\"\\n        float_terminals = batch[\\'terminals\\'].astype(float)\\n\\nq_targets = batch[\\'rewards\\'] + (1. - float_terminals) \\\\\\n')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 5246,
        "neg_line": [
            "-float_terminals = tf.to_float(batch['terminals'])"
        ],
        "pos_line": [
            "+float_terminals = batch['terminals'].astype(float)",
            "+"
        ],
        "core_change": "-float_terminals = tf.to_float(batch['terminals']) +float_terminals = batch['terminals'].astype(float) +",
        "core_API": "to_float"
    },
    {
        "commit_hash": "514486739cc732ad05549d81bd48c0aa9e03a0f3",
        "index": "294a489a8..efe5ff0b9 100644",
        "commit_message": "Fix CI with change of name of nlp (#7054)\n\n* nlp -> datasets\n\n* More nlp -> datasets\n\n* Woopsie\n\n* More nlp -> datasets\n\n* One last\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "from tqdm import tqdm",
            "",
            "",
            "def download_wmt_dataset(src_lang=\"ro\", tgt_lang=\"en\", dataset=\"wmt16\", save_dir=None) -> None:",
            "-    \"\"\"Download a dataset using the nlp package and save it to the format expected by finetune.py",
            "+    \"\"\"Download a dataset using the datasets package and save it to the format expected by finetune.py",
            "Format of save_dir: train.source, train.target, val.source, val.target, test.source, test.target.",
            "",
            "Args:",
            "src_lang: <str> source language",
            "tgt_lang: <str> target language",
            "-        dataset: <str> wmt16, wmt17, etc. wmt16 is a good start as it's small. To get the full list run `import nlp; print([d.id for d in nlp.list_datasets() if \"wmt\" in d.id])`",
            "+        dataset: <str> wmt16, wmt17, etc. wmt16 is a good start as it's small. To get the full list run `import datasets; print([d.id for d in datasets.list_datasets() if \"wmt\" in d.id])`",
            "save_dir: <str>, where to save the datasets, defaults to f'{dataset}-{src_lang}-{tgt_lang}'",
            "",
            "Usage:",
            ">>> download_wmt_dataset('ro', 'en', dataset='wmt16') # saves to wmt16-ro-en",
            "\"\"\"",
            "try:",
            "-        import nlp",
            "+        import datasets",
            "except (ModuleNotFoundError, ImportError):",
            "-        raise ImportError(\"run pip install nlp\")",
            "+        raise ImportError(\"run pip install datasets\")",
            "pair = f\"{src_lang}-{tgt_lang}\"",
            "print(f\"Converting {dataset}-{pair}\")",
            "-    ds = nlp.load_dataset(dataset, pair)",
            "+    ds = datasets.load_dataset(dataset, pair)",
            "if save_dir is None:",
            "save_dir = f\"{dataset}-{pair}\"",
            "save_dir = Path(save_dir)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"\"\"Download a dataset using the nlp package and save it to the format expected by finetune.py\nFormat of save_dir: train.source, train.target, val.source, val.target, test.source, test.target.\n\nArgs:\nsrc_lang: <str> source language\ntgt_lang: <str> target language\n        dataset: <str> wmt16, wmt17, etc. wmt16 is a good start as it's small. To get the full list run `import nlp; print([d.id for d in nlp.list_datasets() if \"wmt\" in d.id])`\nsave_dir: <str>, where to save the datasets, defaults to f'{dataset}-{src_lang}-{tgt_lang}'\n\nUsage:\n>>> download_wmt_dataset('ro', 'en', dataset='wmt16') # saves to wmt16-ro-en\n\"\"\"), value='\"\"\"Download a dataset using the datasets package and save it to the format expected by finetune.py\\nFormat of save_dir: train.source, train.target, val.source, val.target, test.source, test.target.\\n\\nArgs:\\nsrc_lang: <str> source language\\ntgt_lang: <str> target language\\n        dataset: <str> wmt16, wmt17, etc. wmt16 is a good start as it\\'s small. To get the full list run `import datasets; print([d.id for d in datasets.list_datasets() if \"wmt\" in d.id])`\\nsave_dir: <str>, where to save the datasets, defaults to f\\'{dataset}-{src_lang}-{tgt_lang}\\'\\n\\nUsage:\\n>>> download_wmt_dataset(\\'ro\\', \\'en\\', dataset=\\'wmt16\\') # saves to wmt16-ro-en\\n\"\"\"')",
            "Update(target_node=ASTNode(type=identifier, text=nlp), value='datasets')",
            "Update(target_node=ASTNode(type=identifier, text=nlp), value='datasets')",
            "Update(target_node=ASTNode(type=string, text=\"run pip install nlp\"), value='\"run pip install datasets\"')"
        ],
        "plus_line": 5,
        "minus_line": 5,
        "AST_diff_line": 4,
        "number": 5247,
        "neg_line": [
            "-\"\"\"Download a dataset using the nlp package and save it to the format expected by finetune.py",
            "-dataset: <str> wmt16, wmt17, etc. wmt16 is a good start as it's small. To get the full list run `import nlp; print([d.id for d in nlp.list_datasets() if \"wmt\" in d.id])`",
            "-import nlp",
            "-raise ImportError(\"run pip install nlp\")",
            "-ds = nlp.load_dataset(dataset, pair)"
        ],
        "pos_line": [
            "+\"\"\"Download a dataset using the datasets package and save it to the format expected by finetune.py",
            "+dataset: <str> wmt16, wmt17, etc. wmt16 is a good start as it's small. To get the full list run `import datasets; print([d.id for d in datasets.list_datasets() if \"wmt\" in d.id])`",
            "+import datasets",
            "+raise ImportError(\"run pip install datasets\")",
            "+ds = datasets.load_dataset(dataset, pair)"
        ],
        "core_change": "-\"\"\"Download a dataset using the nlp package and save it to the format expected by finetune.py +\"\"\"Download a dataset using the datasets package and save it to the format expected by finetune.py -dataset: <str> wmt16, wmt17, etc. wmt16 is a good start as it's small. To get the full list run `import nlp; print([d.id for d in nlp.list_datasets() if \"wmt\" in d.id])` +dataset: <str> wmt16, wmt17, etc. wmt16 is a good start as it's small. To get the full list run `import datasets; print([d.id for d in datasets.list_datasets() if \"wmt\" in d.id])` -import nlp +import datasets -raise ImportError(\"run pip install nlp\") +raise ImportError(\"run pip install datasets\") -ds = nlp.load_dataset(dataset, pair) +ds = datasets.load_dataset(dataset, pair)",
        "core_API": "list_datasets"
    },
    {
        "commit_hash": "280e24fdeea43768f558aa3af4d267563d951911",
        "index": "cdc2e60d..d7499bc5 100644",
        "commit_message": "fixed log std shape in Gaussian policy and KL-divergence axis\"\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class PGModel(Model):",
            "size = 1",
            "for dims in self.state_shape:",
            "size *= dims",
            "-        self.baseline_value_function = MLPValueFunction(self.session, state_size=size, layer_size=100)  # LinearValueFunction()",
            "+        self.baseline_value_function = LinearValueFunction()",
            "# self.saver = tf.train.Saver()",
            "",
            "def get_action(self, state, episode=1):"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=MLPValueFunction), value='LinearValueFunction')",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=session))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=state_size))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=size))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=layer_size))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=integer, text=100))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 15,
        "number": 5249,
        "neg_line": [
            "-self.baseline_value_function = MLPValueFunction(self.session, state_size=size, layer_size=100)  # LinearValueFunction()"
        ],
        "pos_line": [
            "+self.baseline_value_function = LinearValueFunction()"
        ],
        "core_change": "-self.baseline_value_function = MLPValueFunction(self.session, state_size=size, layer_size=100)  # LinearValueFunction() +self.baseline_value_function = LinearValueFunction()",
        "core_API": "Saver"
    },
    {
        "commit_hash": "92bf5cce4248a6c9bd4955f712bf37b95ae450c5",
        "index": "4a88962807..52d73a8e1f 100644",
        "commit_message": "Fix linting issues\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def nonzero(x: torch.Tensor) -> Tuple[torch.Tensor]:",
            "return torch.nonzero(x, as_tuple=True)",
            "",
            "",
            "-def where(condition: torch.Tensor, x1: torch.Tensor, x2: torch.Tensor, *, out: Optional[torch.Tensor] = None) -> torch.Tensor:",
            "+def where(",
            "+    condition: torch.Tensor,",
            "+    x1: torch.Tensor,",
            "+    x2: torch.Tensor,",
            "+    *,",
            "+    out: Optional[torch.Tensor] = None",
            "+) -> torch.Tensor:",
            "promoted_type = torch.promote_types(x1.dtype, x2.dtype)",
            "x1 = x1.to(promoted_type)",
            "x2 = x2.to(promoted_type)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 5,
        "minus_line": 0,
        "AST_diff_line": 17,
        "number": 5251,
        "neg_line": [
            "-def where(condition: torch.Tensor, x1: torch.Tensor, x2: torch.Tensor, *, out: Optional[torch.Tensor] = None) -> torch.Tensor:"
        ],
        "pos_line": [
            "+def where(",
            "+condition: torch.Tensor,",
            "+x1: torch.Tensor,",
            "+x2: torch.Tensor,",
            "+*,",
            "+out: Optional[torch.Tensor] = None",
            "+) -> torch.Tensor:"
        ],
        "core_change": "-def where(condition: torch.Tensor, x1: torch.Tensor, x2: torch.Tensor, *, out: Optional[torch.Tensor] = None) -> torch.Tensor: +def where( +condition: torch.Tensor, +x1: torch.Tensor, +x2: torch.Tensor, +*, +out: Optional[torch.Tensor] = None +) -> torch.Tensor:",
        "core_API": "nonzero"
    },
    {
        "commit_hash": "1daab8714463e63c8f4b4ba4caed3cc1ef97a195",
        "index": "4f1492ce..9beb91e6 100644",
        "commit_message": "[torch] Enable GPU training (#1508)\n\n* Added more descriptive description_summary, fixed bug in TabNet combiner\n\n* Updated modules to do proper GPU conversion\n\n* Functional GPU training\n\n* Tabnet working, removed RayRemoteTrainer\n\n* Temporary changes to unblock GPU training\n\n* Functional test_seq_encoder\n\n* Cleaned up PR\n\n* Fixed failing CPU tests\n\n* Responded to comments\n\n* Fix failing GPU tests in test_api.py\n\n* [torch] Fix GPU tests for test_visualizations (#1539)\n\nUpdated torch.aranges to registered buffers in embedding_modules.py\n\n* [torch] Fix test_encoder.py for GPU (#1544)\n\n* Accept and move model to GPU as part of Predictor init (#1559)\n\n* [torch] Fixed tests in test_experiment.py (#1562)\n\n* Fixed tests in tests/integration_tests/test_experiment.py\n\n* Moved set embed and inputs to correct device in tests/ludwig/encoders/test_set_encoders.py\n\n* Moved set date and inputs to correct device in tests/ludwig/encoders/test_date_encoders.py\n\n* Moved bag embed and inputs to correct device in tests/ludwig/encoders/test_bag_encoders.py\n\n* Moved sequence embed and inputs to correct device in tests/ludwig/features/test_sequence_features.py\n\n* [torch] Fixed GPU issues in combiners.py (#1568)\n\n* Fix GPU tests in test_collect.py (#1569)\n\n* Added model to Predictor initialization\n\n* Fixed embedding modules tests (#1554)\n\n* Fixed embedding modules tests\n* Moved embedding modules to device in tests, removed duplicate argument\n\nCo-authored-by: Shreya Rajpal <shreya.rajpal@gmail.com>\n\n* Fixed encoders tests except test_h3_rnn_embed (#1556)\n\n* Fixed encoders tests except test_h3_rnn_embed\n\n* Addressed Shreya's comments\n\n* Moved embedding modules to device, used registered buffers\n\nCo-authored-by: Shreya Rajpal <shreya.rajpal@gmail.com>\n\n* Pass ECD object to Predictor\n\n* Removed model arg from predict function\n\nCo-authored-by: Jeffrey Tang <jeff@predibase.com>\nCo-authored-by: Jeffrey Tang <810895+jeffreyftang@users.noreply.github.com>\nCo-authored-by: Animesh Kumar <anmshkmr@users.noreply.github.com>\n",
        "file": "ludwig.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class MSEMetric(MeanSquaredError, LudwigMetric):",
            "super().__init__()",
            "",
            "def update(self, preds: Tensor, target: Tensor) -> None:",
            "-        super().update(preds.detach(), target)",
            "+        super().update(preds, target)",
            "",
            "@classmethod",
            "def get_objective(cls):"
        ],
        "hunk_index": 4,
        "AST_diff": [
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=preds), position=1)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=), text=)), position=5)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=detach))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=), text=)))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 5254,
        "neg_line": [
            "-super().update(preds.detach(), target)"
        ],
        "pos_line": [
            "+super().update(preds, target)"
        ],
        "core_change": "-super().update(preds.detach(), target) +super().update(preds, target)",
        "core_API": "detach"
    },
    {
        "commit_hash": "4fd8977eafd658056c92ffa6294027e77cdc46c0",
        "index": "7838fe617c..c5e13bf5e7 100644",
        "commit_message": "[RLlib] Minor cleanup in preparation to tf2.x support. (#9130)\n\n* WIP.\n\n* Fixes.\n\n* LINT.\n\n* Fixes.\n\n* Fixes and LINT.\n\n* WIP.\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class QLoss:",
            "r_tau = tf.clip_by_value(r_tau, v_min, v_max)",
            "b = (r_tau - v_min) / ((v_max - v_min) / float(num_atoms - 1))",
            "lb = tf.floor(b)",
            "-            ub = tf.ceil(b)",
            "+            ub = tf.math.ceil(b)",
            "# indispensable judgement which is missed in most implementations",
            "# when b happens to be an integer, lb == ub, so pr_j(s', a*) will",
            "# be discarded because (ub-b) == (b-lb) == 0",
            "-            floor_equal_ceil = tf.to_float(tf.less(ub - lb, 0.5))",
            "+            floor_equal_ceil = tf.cast(tf.less(ub - lb, 0.5), tf.float32)",
            "",
            "l_project = tf.one_hot(",
            "tf.cast(lb, dtype=tf.int32),"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=2145846)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=2145847)",
            "Update(target_node=ASTNode(type=identifier, text=to_float), value='cast')",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=2145848)",
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=3, insert_id=2145849)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=tf), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'math'), position=2, insert_id=2145850)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2145851)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2145852)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'float32'), position=2, insert_id=2145853)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 11,
        "number": 5255,
        "neg_line": [
            "-ub = tf.ceil(b)",
            "-floor_equal_ceil = tf.to_float(tf.less(ub - lb, 0.5))"
        ],
        "pos_line": [
            "+ub = tf.math.ceil(b)",
            "+floor_equal_ceil = tf.cast(tf.less(ub - lb, 0.5), tf.float32)"
        ],
        "core_change": "-ub = tf.ceil(b) +ub = tf.math.ceil(b) -floor_equal_ceil = tf.to_float(tf.less(ub - lb, 0.5)) +floor_equal_ceil = tf.cast(tf.less(ub - lb, 0.5), tf.float32)",
        "core_API": "clip_by_value"
    },
    {
        "commit_hash": "9a7d1a18760030653a296b230c69b941761cde8b",
        "index": "71a37bfee..9afdf84fa 100644",
        "commit_message": "[metrics] Accuracy num_classes error fix (#3764)\n\n* change accuracy error to warning\n\n* changelog\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_multilabel_accuracy():",
            "assert torch.allclose(accuracy(y2, torch.logical_not(y2), class_reduction='none'), torch.tensor([0., 0.]))",
            "assert torch.allclose(accuracy(y1, torch.logical_not(y1), class_reduction='none'), torch.tensor([0., 0.]))",
            "",
            "-    with pytest.raises(RuntimeError):",
            "-        accuracy(y2, torch.zeros_like(y2), class_reduction='none')",
            "+    # num_classes does not match extracted number from input we expect a warning",
            "+    with pytest.warns(RuntimeWarning,",
            "+                      match=r'You have set .* number of classes which is'",
            "+                            r' different from predicted (.*) and'",
            "+                            r' target (.*) number of classes'):",
            "+        _ = accuracy(y2, torch.zeros_like(y2), num_classes=3)",
            "",
            "",
            "def test_accuracy():"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=expression_statement), node=('assignment', None), position=0, insert_id=564671)",
            "Insert(target_node=IN(type=assignment), node=('identifier', '_'), position=0, insert_id=564672)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=564673)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=call), position=2)",
            "Update(target_node=ASTNode(type=identifier, text=raises), value='warns')",
            "Update(target_node=ASTNode(type=identifier, text=RuntimeError), value='RuntimeWarning')",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=564674)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=564675)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'match'), position=0, insert_id=564676)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=564677)",
            "Insert(target_node=IN(type=keyword_argument), node=('concatenated_string', None), position=2, insert_id=564678)",
            "Insert(target_node=IN(type=concatenated_string), node=('string', \"r'You have set .* number of classes which is'\"), position=0, insert_id=564679)",
            "Insert(target_node=IN(type=concatenated_string), node=('string', \"r' different from predicted (.*) and'\"), position=1, insert_id=564680)",
            "Insert(target_node=IN(type=concatenated_string), node=('string', \"r' target (.*) number of classes'\"), position=2, insert_id=564681)",
            "Update(target_node=ASTNode(type=identifier, text=class_reduction), value='num_classes')",
            "Insert(target_node=ASTNode(type=keyword_argument), node=('integer', '3'), position=2, insert_id=564682)",
            "Delete(target_node=ASTNode(type=string, text='none'))"
        ],
        "plus_line": 6,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 5257,
        "neg_line": [
            "-with pytest.raises(RuntimeError):",
            "-accuracy(y2, torch.zeros_like(y2), class_reduction='none')"
        ],
        "pos_line": [
            "+# num_classes does not match extracted number from input we expect a warning",
            "+with pytest.warns(RuntimeWarning,",
            "+match=r'You have set .* number of classes which is'",
            "+r' different from predicted (.*) and'",
            "+r' target (.*) number of classes'):",
            "+_ = accuracy(y2, torch.zeros_like(y2), num_classes=3)"
        ],
        "core_change": "-with pytest.raises(RuntimeError): -accuracy(y2, torch.zeros_like(y2), class_reduction='none') +# num_classes does not match extracted number from input we expect a warning +with pytest.warns(RuntimeWarning, +match=r'You have set .* number of classes which is' +r' different from predicted (.*) and' +r' target (.*) number of classes'): +_ = accuracy(y2, torch.zeros_like(y2), num_classes=3)",
        "core_API": "allclose"
    },
    {
        "commit_hash": "43043ee4d5c672f7fbc22ac9559e8164731fd053",
        "index": "6e2d44c046..a20236711b 100644",
        "commit_message": "[RLlib] Tf2x preparation; part 2 (upgrading `try_import_tf()`). (#9136)\n\n* WIP.\n\n* Fixes.\n\n* LINT.\n\n* WIP.\n\n* WIP.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* WIP.\n\n* Fixes.\n\n* Test\n\n* Fix.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* LINT.\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class EagerModel(TFModelV2):",
            "",
            "def lambda_(x):",
            "eager_out = tf.py_function(self.forward_eager, [x], tf.float32)",
            "-            with tf.control_dependencies([eager_out]):",
            "+            with tf1.control_dependencies([eager_out]):",
            "eager_out.set_shape(x.shape)",
            "return eager_out"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tf), value='tf1')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 5263,
        "neg_line": [
            "-with tf.control_dependencies([eager_out]):"
        ],
        "pos_line": [
            "+with tf1.control_dependencies([eager_out]):"
        ],
        "core_change": "-with tf.control_dependencies([eager_out]): +with tf1.control_dependencies([eager_out]):",
        "core_API": "py_function"
    },
    {
        "commit_hash": "1ed2b12b7320e1bacd79bc2d5a5691d641c66397",
        "index": "1bc9ac588..046607052 100644",
        "commit_message": "fixed style errors\n\n",
        "file": "PySyft.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestTorchVariable(TestCase):",
            "\"\"\"Linear transformation of x by w\"\"\"",
            "return x.mm(w)",
            "",
            "-        x = Var(torch.FloatTensor([[1,1],[2,2]]), requires_grad=True)",
            "-        y = Var(torch.FloatTensor([[1,1],[2,2]]), requires_grad=True)",
            "+        x = Var(torch.FloatTensor([[1, 1], [2, 2]]), requires_grad=True)",
            "+        y = Var(torch.FloatTensor([[1, 1], [2, 2]]), requires_grad=True)",
            "",
            "z = linear(x, y)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 5264,
        "neg_line": [
            "-x = Var(torch.FloatTensor([[1,1],[2,2]]), requires_grad=True)",
            "-y = Var(torch.FloatTensor([[1,1],[2,2]]), requires_grad=True)"
        ],
        "pos_line": [
            "+x = Var(torch.FloatTensor([[1, 1], [2, 2]]), requires_grad=True)",
            "+y = Var(torch.FloatTensor([[1, 1], [2, 2]]), requires_grad=True)"
        ],
        "core_change": "-x = Var(torch.FloatTensor([[1,1],[2,2]]), requires_grad=True) -y = Var(torch.FloatTensor([[1,1],[2,2]]), requires_grad=True) +x = Var(torch.FloatTensor([[1, 1], [2, 2]]), requires_grad=True) +y = Var(torch.FloatTensor([[1, 1], [2, 2]]), requires_grad=True)",
        "core_API": "mm"
    },
    {
        "commit_hash": "fb2f82332adf9266bf7a835e6669b76d7a69dc8f",
        "index": "f15c483d..bf9c8d19 100644",
        "commit_message": "fix linting and tests\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_base_storage():",
            "assert len(storage) == 1",
            "assert storage.x is not None",
            "",
            "-    storage = BaseStorage(key='key', parent={}, x=torch.zeros(1))",
            "-",
            "+    storage = BaseStorage(x=torch.zeros(1))",
            "copied_storage = copy.copy(storage)",
            "assert storage == copied_storage",
            "assert id(storage) != id(copied_storage)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=key))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=string, text='key'))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=parent))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type={, text={))",
            "Delete(target_node=ASTNode(type=}, text=}))",
            "Delete(target_node=ASTNode(type=dictionary))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 12,
        "number": 5265,
        "neg_line": [
            "-storage = BaseStorage(key='key', parent={}, x=torch.zeros(1))",
            "-"
        ],
        "pos_line": [
            "+storage = BaseStorage(x=torch.zeros(1))"
        ],
        "core_change": "-storage = BaseStorage(key='key', parent={}, x=torch.zeros(1)) - +storage = BaseStorage(x=torch.zeros(1))",
        "core_API": "zeros"
    },
    {
        "commit_hash": "ed63b7ee9e68ba120893aaaaca0d4525a56ed484",
        "index": "c73ea80e..218e96af 100644",
        "commit_message": "upgrade to pytorch 0.4.0 (#1126)\n\n* bump pytorch to 0.4 + fix sanitize\n\n* remove check for Variable in block_orthogonal\n\n* rename parameter split_size=\n\n* remove checks for Variable\n\n* fix more tests\n\n* fixes\n\n* get tests to pass\n\n* fix warnings\n\n* get rid of some of the Variables\n\n* more tests passing\n\n* more elimination of variables\n\n* finish removing all Variables\n\n* pylint and such\n\n* a few fixes\n\n* move torch.no_grad into model.forward_on_instances\n\n* more pytorch 0.4 changes\n\n* detach() -> data\n\n* pylint\n\n* fix bad tensor creation\n\n* fix types\n\n* remove print statement\n\n* more 0.4 goodness\n\n* factor out is_tensor\n\n* add no_grad to elmo command\n\n* cleanup\n\n* remove TODO\n\n* address PR feedback\n\n* replace all() with item()\n\n* more cleanup\n\n* further cleanup\n\n* remove Variable\n\n* really fix merge conflict\n\n* fix pylint\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class IndexField(Field[torch.Tensor]):",
            "@overrides",
            "def as_tensor(self,",
            "padding_lengths: Dict[str, int],",
            "-                  cuda_device: int = -1,",
            "-                  for_training: bool = True) -> torch.Tensor:",
            "+                  cuda_device: int = -1) -> torch.Tensor:",
            "# pylint: disable=unused-argument",
            "-        tensor = Variable(torch.LongTensor([self.sequence_index]), volatile=not for_training)",
            "+        tensor = torch.LongTensor([self.sequence_index])",
            "return tensor if cuda_device == -1 else tensor.cuda(cuda_device)",
            "",
            "@overrides"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=call), position=2)",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=for_training))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=identifier, text=bool))",
            "Delete(target_node=ASTNode(type=type))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=true, text=True))",
            "Delete(target_node=ASTNode(type=typed_default_parameter))",
            "Delete(target_node=ASTNode(type=identifier, text=Variable))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=volatile))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=not, text=not))",
            "Delete(target_node=ASTNode(type=identifier, text=for_training))",
            "Delete(target_node=ASTNode(type=not_operator))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 2,
        "minus_line": 3,
        "AST_diff_line": 21,
        "number": 5270,
        "neg_line": [
            "-cuda_device: int = -1,",
            "-for_training: bool = True) -> torch.Tensor:",
            "-tensor = Variable(torch.LongTensor([self.sequence_index]), volatile=not for_training)"
        ],
        "pos_line": [
            "+cuda_device: int = -1) -> torch.Tensor:",
            "+tensor = torch.LongTensor([self.sequence_index])"
        ],
        "core_change": "-cuda_device: int = -1, -for_training: bool = True) -> torch.Tensor: +cuda_device: int = -1) -> torch.Tensor: -tensor = Variable(torch.LongTensor([self.sequence_index]), volatile=not for_training) +tensor = torch.LongTensor([self.sequence_index])",
        "core_API": "LongTensor"
    },
    {
        "commit_hash": "5a9dfecf41728295dff0eaf420a8c205baa9ca4f",
        "index": "80a437c21..1d9820781 100644",
        "commit_message": "fixed lm backward compatibility\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def recog(args):",
            "rnnlm_args = get_model_conf(args.word_rnnlm, args.word_rnnlm_conf)",
            "word_dict = rnnlm_args.char_list_dict",
            "char_dict = {x: i for i, x in enumerate(train_args.char_list)}",
            "-        word_rnnlm = lm_pytorch.ClassifierWithState(lm_pytorch.RNNLM(",
            "-            len(word_dict), rnnlm_args.layer, rnnlm_args.unit, rnnlm_args.embed_unit))",
            "+        word_rnnlm = lm_pytorch.ClassifierWithState(",
            "+            lm_pytorch.RNNLM(",
            "+                len(word_dict), rnnlm_args.layer, rnnlm_args.unit,",
            "+                getattr(rnnlm_args, \"embed_unit\", None),  # for backward compatibility",
            "+            )",
            "+        )",
            "torch_load(args.word_rnnlm, word_rnnlm)",
            "word_rnnlm.eval()"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=7, insert_id=158328)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=8, insert_id=158329)",
            "Insert(target_node=IN(type=call), node=('identifier', 'getattr'), position=0, insert_id=158330)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=158331)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=158332)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=rnnlm_args), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=158333)",
            "Insert(target_node=IN(type=argument_list), node=('string', '\"embed_unit\"'), position=3, insert_id=158334)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=4, insert_id=158335)",
            "Insert(target_node=IN(type=argument_list), node=('none', 'None'), position=5, insert_id=158336)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=6, insert_id=158337)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=embed_unit))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 6,
        "minus_line": 2,
        "AST_diff_line": 14,
        "number": 5271,
        "neg_line": [
            "-word_rnnlm = lm_pytorch.ClassifierWithState(lm_pytorch.RNNLM(",
            "-len(word_dict), rnnlm_args.layer, rnnlm_args.unit, rnnlm_args.embed_unit))"
        ],
        "pos_line": [
            "+word_rnnlm = lm_pytorch.ClassifierWithState(",
            "+lm_pytorch.RNNLM(",
            "+len(word_dict), rnnlm_args.layer, rnnlm_args.unit,",
            "+getattr(rnnlm_args, \"embed_unit\", None),  # for backward compatibility",
            "+)",
            "+)"
        ],
        "core_change": "-word_rnnlm = lm_pytorch.ClassifierWithState(lm_pytorch.RNNLM( -len(word_dict), rnnlm_args.layer, rnnlm_args.unit, rnnlm_args.embed_unit)) +word_rnnlm = lm_pytorch.ClassifierWithState( +lm_pytorch.RNNLM( +len(word_dict), rnnlm_args.layer, rnnlm_args.unit, +getattr(rnnlm_args, \"embed_unit\", None),  # for backward compatibility +) +)",
        "core_API": "ClassifierWithState"
    },
    {
        "commit_hash": "dfc00b919a1f8e91bb97e18a121cc34aa251b6c0",
        "index": "f58d269..3cc1a47 100644",
        "commit_message": "bugfix & optional tensorflow implementation (#96)\n\n* bugfix & optional tensorflow implementation\n\n* removed torch max version\n\n* bugfix compressor\n\n* bugfix metric\n\nCo-authored-by: Valerio Sofi <v.sofi@nebuly.ai>\n",
        "file": "nebullvm.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def run_tf_model(",
            "model: tf.Module, input_tensors: Tuple[tf.Tensor]",
            ") -> Tuple[tf.Tensor]:",
            "pred = model.predict(*input_tensors)",
            "-    if isinstance(pred, tf.Module):",
            "+    if isinstance(pred, tf.Module) and pred is not None:",
            "pred = (pred,)",
            "return pred"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=if_statement), node=('boolean_operator', None), position=1, insert_id=2123951)",
            "Move(target_node=IN(type=boolean_operator), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=boolean_operator), node=('and', 'and'), position=1, insert_id=2123952)",
            "Insert(target_node=IN(type=boolean_operator), node=('comparison_operator', None), position=2, insert_id=2123953)",
            "Insert(target_node=IN(type=comparison_operator), node=('identifier', 'pred'), position=0, insert_id=2123954)",
            "Insert(target_node=IN(type=comparison_operator), node=('is not', 'is'), position=1, insert_id=2123955)",
            "Insert(target_node=IN(type=comparison_operator), node=('is not', 'not'), position=2, insert_id=2123956)",
            "Insert(target_node=IN(type=comparison_operator), node=('none', 'None'), position=3, insert_id=2123957)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 5274,
        "neg_line": [
            "-if isinstance(pred, tf.Module):"
        ],
        "pos_line": [
            "+if isinstance(pred, tf.Module) and pred is not None:"
        ],
        "core_change": "-if isinstance(pred, tf.Module): +if isinstance(pred, tf.Module) and pred is not None:",
        "core_API": "predict"
    },
    {
        "commit_hash": "56524975c41b9f8cbb75e7d86df520cd02bce2cf",
        "index": "41e1200b5..7a521e1dd 100644",
        "commit_message": "Fix module reloading during tests\n\n",
        "file": "wandb.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tensorflow",
        "change": [
            "def patch(save=True, tensorboardX=tensorboardX_loaded):",
            "writer.EventFileWriter.add_event = add_event(",
            "writer.EventFileWriter.add_event)",
            "wandb.patched[\"tensorboard\"].append(tensorboard_py_module)",
            "-        wandb.patched[\"tensorboard\"].append(\"tensorflow.summary\")",
            "",
            "# This configures TensorFlow 2 style Tensorboard logging",
            "c_writer = wandb.util.get_module(tensorboard_c_module)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=wandb))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=patched))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=string, text=\"tensorboard\"))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=append))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=string, text=\"tensorflow.summary\"))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 5276,
        "neg_line": [
            "-wandb.patched[\"tensorboard\"].append(\"tensorflow.summary\")"
        ],
        "pos_line": [],
        "core_change": "-wandb.patched[\"tensorboard\"].append(\"tensorflow.summary\")",
        "core_API": "get_module"
    },
    {
        "commit_hash": "fcc306a605c8220e27cfb218156a160f04de811d",
        "index": "3e30eb3..9f3d9cc 100644",
        "commit_message": "fix type\n\n",
        "file": "face-alignment.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class FaceAlignment:",
            "out += flip(self.face_alignment_net(flip(inp)).detach(), is_label=True)",
            "out = out.cpu().numpy()",
            "",
            "-            pts, pts_img = get_preds_fromhm(out, center, scale)",
            "-            pts, pts_img = pts.view(68, 2) * 4, pts_img.view(68, 2)",
            "+            pts, pts_img = get_preds_fromhm(out, center.numpy(), scale)",
            "pts, pts_img = torch.from_numpy(pts), torch.from_numpy(pts_img)",
            "+            pts, pts_img = pts.view(68, 2) * 4, pts_img.view(68, 2)",
            "",
            "if self.landmarks_type == LandmarksType._3D:",
            "heatmaps = np.zeros((68, 256, 256), dtype=np.float32)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=expression_statement), node=ASTNode(type=module), position=4)",
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=3, insert_id=199660)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=199661)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=199662)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=center), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=199663)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'numpy'), position=2, insert_id=199664)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=199665)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=199666)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 9,
        "number": 5277,
        "neg_line": [
            "-pts, pts_img = get_preds_fromhm(out, center, scale)",
            "-pts, pts_img = pts.view(68, 2) * 4, pts_img.view(68, 2)"
        ],
        "pos_line": [
            "+pts, pts_img = get_preds_fromhm(out, center.numpy(), scale)",
            "+pts, pts_img = pts.view(68, 2) * 4, pts_img.view(68, 2)"
        ],
        "core_change": "-pts, pts_img = get_preds_fromhm(out, center, scale) -pts, pts_img = pts.view(68, 2) * 4, pts_img.view(68, 2) +pts, pts_img = get_preds_fromhm(out, center.numpy(), scale) +pts, pts_img = pts.view(68, 2) * 4, pts_img.view(68, 2)",
        "core_API": "face_alignment_net"
    },
    {
        "commit_hash": "93a30aae1125fa15234e2dba7f4850701b1e7316",
        "index": "b34996c..afb7ae6 100644",
        "commit_message": "fix minor bug\n\n",
        "file": "faster-rcnn.pytorch.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class _AnchorTargetLayer(nn.Module):",
            "disable_inds = bg_inds[rand_num[:bg_inds.size(0)-num_bg]]",
            "labels[i][disable_inds] = -1",
            "",
            "-        offset = torch.arange(0, batch_size)*20",
            "+        offset = torch.arange(0, batch_size)*gt_boxes.size(1)",
            "+",
            "argmax_overlaps = argmax_overlaps + offset.view(batch_size, 1).type_as(argmax_overlaps)",
            "bbox_targets = _compute_targets_batch(anchors, gt_boxes.view(-1,5)[argmax_overlaps.view(-1), :].view(batch_size, -1, 5))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=binary_operator), node=('call', None), position=2, insert_id=228202)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=228203)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=228204)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'gt_boxes'), position=0, insert_id=228205)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=228206)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'size'), position=2, insert_id=228207)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=228208)",
            "Insert(target_node=IN(type=argument_list), node=('integer', '1'), position=1, insert_id=228209)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=228210)",
            "Delete(target_node=ASTNode(type=integer, text=20))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 5285,
        "neg_line": [
            "-offset = torch.arange(0, batch_size)*20"
        ],
        "pos_line": [
            "+offset = torch.arange(0, batch_size)*gt_boxes.size(1)",
            "+"
        ],
        "core_change": "-offset = torch.arange(0, batch_size)*20 +offset = torch.arange(0, batch_size)*gt_boxes.size(1) +",
        "core_API": "size"
    },
    {
        "commit_hash": "3dda5ac9fcf390d8b83eb855249360711707c11c",
        "index": "73f78fc7..7bdf353f 100644",
        "commit_message": "for discussion: incorporate black code formatter (#3308)\n\n* setup files\n\n* run black\n\n* undo\n\n* update CONTRIBUTING.md\n\n* fix quotes in test_other_modules\n\n* make flake8 happy\n\n* set black to 100 characters per line\n\n* move type: ignore to where mypy wants them\n\n* more flake8\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "from allennlp.state_machines import util",
            "",
            "class TestStateMachinesUtil(AllenNlpTestCase):",
            "def test_create_allowed_transitions(self):",
            "-        targets = torch.Tensor([[[2, 3, 4], [1, 3, 4], [1, 2, 4]], [[3, 4, 0], [2, 3, 4], [0, 0, 0]]])",
            "-        target_mask = torch.Tensor([[[1, 1, 1], [1, 1, 1], [1, 1, 1]], [[1, 1, 0], [1, 1, 1], [0, 0, 0]]])",
            "+        targets = torch.Tensor(",
            "+            [[[2, 3, 4], [1, 3, 4], [1, 2, 4]], [[3, 4, 0], [2, 3, 4], [0, 0, 0]]]",
            "+        )",
            "+        target_mask = torch.Tensor(",
            "+            [[[1, 1, 1], [1, 1, 1], [1, 1, 1]], [[1, 1, 0], [1, 1, 1], [0, 0, 0]]]",
            "+        )",
            "prefix_tree = util.construct_prefix_tree(targets, target_mask)",
            "",
            "# There were two instances in this batch."
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 6,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 5287,
        "neg_line": [
            "-targets = torch.Tensor([[[2, 3, 4], [1, 3, 4], [1, 2, 4]], [[3, 4, 0], [2, 3, 4], [0, 0, 0]]])",
            "-target_mask = torch.Tensor([[[1, 1, 1], [1, 1, 1], [1, 1, 1]], [[1, 1, 0], [1, 1, 1], [0, 0, 0]]])"
        ],
        "pos_line": [
            "+targets = torch.Tensor(",
            "+[[[2, 3, 4], [1, 3, 4], [1, 2, 4]], [[3, 4, 0], [2, 3, 4], [0, 0, 0]]]",
            "+)",
            "+target_mask = torch.Tensor(",
            "+[[[1, 1, 1], [1, 1, 1], [1, 1, 1]], [[1, 1, 0], [1, 1, 1], [0, 0, 0]]]",
            "+)"
        ],
        "core_change": "-targets = torch.Tensor([[[2, 3, 4], [1, 3, 4], [1, 2, 4]], [[3, 4, 0], [2, 3, 4], [0, 0, 0]]]) -target_mask = torch.Tensor([[[1, 1, 1], [1, 1, 1], [1, 1, 1]], [[1, 1, 0], [1, 1, 1], [0, 0, 0]]]) +targets = torch.Tensor( +[[[2, 3, 4], [1, 3, 4], [1, 2, 4]], [[3, 4, 0], [2, 3, 4], [0, 0, 0]]] +) +target_mask = torch.Tensor( +[[[1, 1, 1], [1, 1, 1], [1, 1, 1]], [[1, 1, 0], [1, 1, 1], [0, 0, 0]]] +)",
        "core_API": "Tensor"
    },
    {
        "commit_hash": "da2f1325014a158be36d62bdd20293ef97e574f4",
        "index": "9cbbb6a7..5a715dcb 100644",
        "commit_message": "enable black in the precommit (#1777)\n\n* enable black in the precommit\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add some fixes\n\n* update libface detection url\n\n* added url from kornia checkpoint\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DeFMO(nn.Module):",
            "# use torch.hub to load pretrained model",
            "if pretrained:",
            "storage_fcn: Callable = lambda storage, loc: storage",
            "-            pretrained_dict = torch.hub.load_state_dict_from_url(",
            "-                urls['defmo_encoder'], map_location=storage_fcn",
            "-            )",
            "+            pretrained_dict = torch.hub.load_state_dict_from_url(urls['defmo_encoder'], map_location=storage_fcn)",
            "self.encoder.load_state_dict(pretrained_dict, strict=True)",
            "-            pretrained_dict_ren = torch.hub.load_state_dict_from_url(",
            "-                urls['defmo_rendering'], map_location=storage_fcn",
            "-            )",
            "+            pretrained_dict_ren = torch.hub.load_state_dict_from_url(urls['defmo_rendering'], map_location=storage_fcn)",
            "self.rendering.load_state_dict(pretrained_dict_ren, strict=True)",
            "self.eval()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 6,
        "AST_diff_line": 17,
        "number": 5289,
        "neg_line": [
            "-pretrained_dict = torch.hub.load_state_dict_from_url(",
            "-urls['defmo_encoder'], map_location=storage_fcn",
            "-)",
            "-pretrained_dict_ren = torch.hub.load_state_dict_from_url(",
            "-urls['defmo_rendering'], map_location=storage_fcn",
            "-)"
        ],
        "pos_line": [
            "+pretrained_dict = torch.hub.load_state_dict_from_url(urls['defmo_encoder'], map_location=storage_fcn)",
            "+pretrained_dict_ren = torch.hub.load_state_dict_from_url(urls['defmo_rendering'], map_location=storage_fcn)"
        ],
        "core_change": "-pretrained_dict = torch.hub.load_state_dict_from_url( -urls['defmo_encoder'], map_location=storage_fcn -) +pretrained_dict = torch.hub.load_state_dict_from_url(urls['defmo_encoder'], map_location=storage_fcn) -pretrained_dict_ren = torch.hub.load_state_dict_from_url( -urls['defmo_rendering'], map_location=storage_fcn -) +pretrained_dict_ren = torch.hub.load_state_dict_from_url(urls['defmo_rendering'], map_location=storage_fcn)",
        "core_API": "load_state_dict_from_url"
    },
    {
        "commit_hash": "d81885c1f1c451e76749890105722130ef9bb4fa",
        "index": "9fad2fc002..75563663c3 100644",
        "commit_message": "[RLlib] Fix all the CI tests that were broken by is_training and replay buffer changes; re-comment-in the failing RLlib tests (#19809)\n\n* Fix DDPG, since it is based on GenericOffPolicyTrainer.\n\n* Fix QMix, SAC, and MADDPA too.\n\n* Undo QMix change.\n\n* Fix DQN input batch type. Always use SampleBatch.\n\n* apex ddpg should not use replay_buffer_config yet.\n\n* Make eager tf policy to use SampleBatch.\n\n* lint\n\n* LINT.\n\n* Re-enable RLlib broken tests to make sure things work ok now.\n\n* fixes.\n\nCo-authored-by: sven1977 <svenmika1977@gmail.com>\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def build_eager_tf_policy(",
            "dist_inputs, dist_class, _ = action_distribution_fn(",
            "self,",
            "self.model,",
            "-                    input_dict[SampleBatch.CUR_OBS],",
            "+                    input_batch,",
            "explore=False,",
            "is_training=False)",
            "# Default log-likelihood calculation.",
            "else:",
            "-                dist_inputs, _ = self.model(input_dict, state_batches,",
            "+                dist_inputs, _ = self.model(input_batch, state_batches,",
            "seq_lens)",
            "dist_class = self.dist_class"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=3, insert_id=2139064)",
            "Update(target_node=ASTNode(type=identifier, text=input_dict), value='input_batch')",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=input_dict), position=6)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=2139065)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2139066)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'model'), position=2, insert_id=2139067)",
            "Update(target_node=ASTNode(type=identifier, text=input_dict), value='input_batch')",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=identifier, text=SampleBatch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=CUR_OBS))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=model))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 19,
        "number": 5295,
        "neg_line": [
            "-input_dict[SampleBatch.CUR_OBS],",
            "-dist_inputs, _ = self.model(input_dict, state_batches,"
        ],
        "pos_line": [
            "+input_batch,",
            "+dist_inputs, _ = self.model(input_batch, state_batches,"
        ],
        "core_change": "-input_dict[SampleBatch.CUR_OBS], +input_batch, -dist_inputs, _ = self.model(input_dict, state_batches, +dist_inputs, _ = self.model(input_batch, state_batches,",
        "core_API": "model"
    },
    {
        "commit_hash": "6892dc4f48fe143aaf1c883bc0f4e5fd988e3278",
        "index": "aaa9359..bed885d 100644",
        "commit_message": "Minor fix to image retraining example code in retrieving training files\n\nPiperOrigin-RevId: 240981917\n\n",
        "file": "hub.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def create_image_lists(image_dir, testing_percentage, validation_percentage):",
            "extensions = sorted(set(os.path.normcase(ext)  # Smash case on Windows.",
            "for ext in ['JPEG', 'JPG', 'jpeg', 'jpg', 'png']))",
            "file_list = []",
            "-    dir_name = os.path.basename(sub_dir)",
            "+    dir_name = os.path.basename(",
            "+        # tf.gfile.Walk() returns sub-directory with trailing '/' when it is in",
            "+        # Google Cloud Storage, which confuses os.path.basename().",
            "+        sub_dir[:-1] if sub_dir.endswith('/') else sub_dir)",
            "+",
            "if dir_name == image_dir:",
            "continue",
            "tf.logging.info(\"Looking for images in '\" + dir_name + \"'\")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('(', '('), position=0, insert_id=1951400)",
            "Insert(target_node=ASTNode(type=argument_list), node=('conditional_expression', None), position=1, insert_id=1951401)",
            "Insert(target_node=IN(type=conditional_expression), node=('subscript', None), position=0, insert_id=1951402)",
            "Insert(target_node=IN(type=conditional_expression), node=('if', 'if'), position=1, insert_id=1951403)",
            "Insert(target_node=IN(type=conditional_expression), node=('call', None), position=2, insert_id=1951404)",
            "Insert(target_node=IN(type=conditional_expression), node=('else', 'else'), position=3, insert_id=1951405)",
            "Move(target_node=IN(type=conditional_expression), node=ASTNode(type=identifier, text=sub_dir), position=4)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'sub_dir'), position=0, insert_id=1951406)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=1951407)",
            "Insert(target_node=IN(type=subscript), node=('slice', None), position=2, insert_id=1951408)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=1951409)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1951410)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1951411)",
            "Insert(target_node=IN(type=slice), node=(':', ':'), position=0, insert_id=1951412)",
            "Insert(target_node=IN(type=slice), node=('unary_operator', '-1'), position=1, insert_id=1951413)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'sub_dir'), position=0, insert_id=1951414)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1951415)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'endswith'), position=2, insert_id=1951416)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=(, text=(), position=0)",
            "Insert(target_node=IN(type=argument_list), node=('string', \"'/'\"), position=1, insert_id=1951417)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1951418)"
        ],
        "plus_line": 4,
        "minus_line": 1,
        "AST_diff_line": 21,
        "number": 5296,
        "neg_line": [
            "-dir_name = os.path.basename(sub_dir)"
        ],
        "pos_line": [
            "+dir_name = os.path.basename(",
            "+# tf.gfile.Walk() returns sub-directory with trailing '/' when it is in",
            "+# Google Cloud Storage, which confuses os.path.basename().",
            "+sub_dir[:-1] if sub_dir.endswith('/') else sub_dir)",
            "+"
        ],
        "core_change": "-dir_name = os.path.basename(sub_dir) +dir_name = os.path.basename( +# tf.gfile.Walk() returns sub-directory with trailing '/' when it is in +# Google Cloud Storage, which confuses os.path.basename(). +sub_dir[:-1] if sub_dir.endswith('/') else sub_dir) +",
        "core_API": "normcase"
    },
    {
        "commit_hash": "157795d55000bd30d3cece3de9b5cf1f282c27be",
        "index": "a0342714..3e01492c 100755",
        "commit_message": "fixed ornstein-uhlenbeck exploration variable; pass action_spec instead of action_shape to tf_exploration\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class GaussianNoise(Exploration):",
            "",
            "super(GaussianNoise, self).__init__(scope=scope, summary_labels=summary_labels)",
            "",
            "-    def tf_explore(self, episode, timestep, action_shape):",
            "-        return tf.random_normal(shape=action_shape[1:], mean=self.mu, stddev=self.sigma)",
            "+    def tf_explore(self, episode, timestep, action_spec):",
            "+        return tf.random_normal(shape=action_spec['shape'], mean=self.mu, stddev=self.sigma)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=action_shape), value='action_spec')",
            "Update(target_node=ASTNode(type=identifier, text=action_shape), value='action_spec')",
            "Insert(target_node=ASTNode(type=subscript), node=('string', \"'shape'\"), position=2, insert_id=2235543)",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=slice))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 6,
        "number": 5298,
        "neg_line": [
            "-def tf_explore(self, episode, timestep, action_shape):",
            "-return tf.random_normal(shape=action_shape[1:], mean=self.mu, stddev=self.sigma)"
        ],
        "pos_line": [
            "+def tf_explore(self, episode, timestep, action_spec):",
            "+return tf.random_normal(shape=action_spec['shape'], mean=self.mu, stddev=self.sigma)"
        ],
        "core_change": "-def tf_explore(self, episode, timestep, action_shape): -return tf.random_normal(shape=action_shape[1:], mean=self.mu, stddev=self.sigma) +def tf_explore(self, episode, timestep, action_spec): +return tf.random_normal(shape=action_spec['shape'], mean=self.mu, stddev=self.sigma)",
        "core_API": "random_normal"
    },
    {
        "commit_hash": "660e0b97bd652bd3a0dfd5f847e5cf62502d0469",
        "index": "d43bfa45b..a5bf778c4 100644",
        "commit_message": "Fix train_step, test_step and tests for CLIP (#18684)\n\n* Fix train_step and test_step, correctly enable CLIP fit test\n\n* Stop using get_args on older Python versions\n\n* Don't use get_origin either\n\n* UnionType is actually even newer, don't use that either\n\n* Apply the same fix to test_loss_computation\n\n* Just realized I was accidentally skipping a bunch of tests!\n\n* Fix test_loss_computation for models without separable labels\n\n* Fix scalar losses in test_step and train_step\n\n* Stop committing your breakpoints\n\n* Fix Swin loss shape\n\n* Fix Tapas loss shape\n\n* Shape fixes for TAPAS, DeIT, HuBERT and ViTMAE\n\n* Add loss computation to TFMobileBertForPreTraining\n\n* make fixup and move copied from statement\n\n* make fixup and move copied from statement\n\n* Correct copied from\n\n* Add labels and next_sentence_label inputs to TFMobileBERT\n\n* Make sure total_loss is always defined\n\n* Update tests/test_modeling_tf_common.py\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* Fix copied from\n\n* Ensure CTC models get labels in tests\n\n* Ensure CTC models get labels in tests\n\n* Fix tests for vit_mae\n\n* Fix tests for vit_mae\n\n* Fix tests for vit_mae\n\n* Reduce batch size for wav2vec2 testing because it was causing OOM\n\n* Skip some TAPAS tests that are failing\n\n* Skip a failing HuBERT test\n\n* make style\n\n* Fix mobilebertforpretraining test\n\n* Skip Wav2Vec2 tests that use huge amounts of mem\n\n* Skip keras_fit for Wav2Vec2 as well\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFViTMAEForPreTraining(TFViTMAEPreTrainedModel):",
            "loss = tf.reduce_mean(loss, axis=-1)  # [batch_size, num_patches], mean loss per patch",
            "",
            "loss = tf.reduce_sum(loss * mask) / tf.reduce_sum(mask)  # mean loss on removed patches",
            "+        loss = tf.reshape(loss, (1,))",
            "return loss",
            "",
            "@unpack_inputs"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=2361923)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=2361924)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'loss'), position=0, insert_id=2361925)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=2361926)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=2361927)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2361928)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2361929)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2361930)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2361931)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'reshape'), position=2, insert_id=2361932)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2361933)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'loss'), position=1, insert_id=2361934)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=2361935)",
            "Insert(target_node=IN(type=argument_list), node=('tuple', None), position=3, insert_id=2361936)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=2361937)",
            "Insert(target_node=IN(type=tuple), node=('(', '('), position=0, insert_id=2361938)",
            "Insert(target_node=IN(type=tuple), node=('integer', '1'), position=1, insert_id=2361939)",
            "Insert(target_node=IN(type=tuple), node=(',', ','), position=2, insert_id=2361940)",
            "Insert(target_node=IN(type=tuple), node=(')', ')'), position=3, insert_id=2361941)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 19,
        "number": 5299,
        "neg_line": [],
        "pos_line": [
            "+loss = tf.reshape(loss, (1,))"
        ],
        "core_change": "+loss = tf.reshape(loss, (1,))",
        "core_API": "reduce_mean"
    },
    {
        "commit_hash": "077478637d9e45c02a64f9cd1877bcc5b708b70d",
        "index": "f71e86475..ff45e87d6 100644",
        "commit_message": "Fix label name in DataCollatorForNextSentencePrediction test (#8048)\n\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DataCollatorIntegrationTest(unittest.TestCase):",
            "total_samples = batch[\"input_ids\"].shape[0]",
            "self.assertEqual(batch[\"input_ids\"].shape, torch.Size((total_samples, 512)))",
            "self.assertEqual(batch[\"token_type_ids\"].shape, torch.Size((total_samples, 512)))",
            "-        self.assertEqual(batch[\"masked_lm_labels\"].shape, torch.Size((total_samples, 512)))",
            "+        self.assertEqual(batch[\"labels\"].shape, torch.Size((total_samples, 512)))",
            "self.assertEqual(batch[\"next_sentence_label\"].shape, torch.Size((total_samples,)))",
            "",
            "@slow"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"masked_lm_labels\"), value='\"labels\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 5301,
        "neg_line": [
            "-self.assertEqual(batch[\"masked_lm_labels\"].shape, torch.Size((total_samples, 512)))"
        ],
        "pos_line": [
            "+self.assertEqual(batch[\"labels\"].shape, torch.Size((total_samples, 512)))"
        ],
        "core_change": "-self.assertEqual(batch[\"masked_lm_labels\"].shape, torch.Size((total_samples, 512))) +self.assertEqual(batch[\"labels\"].shape, torch.Size((total_samples, 512)))",
        "core_API": "assertEqual"
    },
    {
        "commit_hash": "36b6577e3458c0e068f4523d60ce8c6e7c19febf",
        "index": "7eaab120..503818ef 100644",
        "commit_message": "Code for \"Double-Head RCNN: Rethinking Classification and Localization for Object Detection\" (#809)\n\n* add major components for training double head\n\ndouble head only with two losses and no attention\n\nremove double_head detector for now, merge upchannel to double_head bbox head\n\nchange the stype using yapl\n\nremove uncessary comment#\n\nto pass check\n\nto pass v2\n\nto pass v3\n\nline too long and style again\n\nreuse bottlenet\n\n* refactoring\n\n* bug fix\n\n* bug fix for weight initialization\n\n* add reg roi scale factor and modify loss weights\n\n* rescale the roi after mapping to fpn levels\n\n",
        "file": "mmdetection.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SingleRoIExtractor(nn.Module):",
            "target_lvls = self.map_roi_levels(rois, num_levels)",
            "roi_feats = feats[0].new_zeros(rois.size()[0], self.out_channels,",
            "out_size, out_size)",
            "+        if roi_scale_factor is not None:",
            "+            rois = self.roi_rescale(rois, roi_scale_factor)",
            "for i in range(num_levels):",
            "inds = target_lvls == i",
            "if inds.any():"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=3, insert_id=644332)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=644333)",
            "Insert(target_node=IN(type=if_statement), node=('comparison_operator', None), position=1, insert_id=644334)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=644335)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=644336)",
            "Insert(target_node=IN(type=comparison_operator), node=('identifier', 'roi_scale_factor'), position=0, insert_id=644337)",
            "Insert(target_node=IN(type=comparison_operator), node=('is not', 'is'), position=1, insert_id=644338)",
            "Insert(target_node=IN(type=comparison_operator), node=('is not', 'not'), position=2, insert_id=644339)",
            "Insert(target_node=IN(type=comparison_operator), node=('none', 'None'), position=3, insert_id=644340)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=644341)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=644342)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'rois'), position=0, insert_id=644343)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=644344)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=644345)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=644346)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=644347)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=644348)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=644349)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'roi_rescale'), position=2, insert_id=644350)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=644351)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'rois'), position=1, insert_id=644352)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=644353)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'roi_scale_factor'), position=3, insert_id=644354)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=644355)"
        ],
        "plus_line": 2,
        "minus_line": 0,
        "AST_diff_line": 24,
        "number": 5302,
        "neg_line": [],
        "pos_line": [
            "+if roi_scale_factor is not None:",
            "+rois = self.roi_rescale(rois, roi_scale_factor)"
        ],
        "core_change": "+if roi_scale_factor is not None: +rois = self.roi_rescale(rois, roi_scale_factor)",
        "core_API": "map_roi_levels"
    },
    {
        "commit_hash": "c98ea36b96f6036b1dec569e5e496aee06a44b29",
        "index": "daa13548..09766322 100644",
        "commit_message": "Fix various LossScaleOptimizer issues.\n\nIn particular, fix crash when saving LossScaleOptimizer with h5, and when passing LossScaleOptimizer to convert_to_legacy_optimizer(). Also change mixed_precision/model_test.py to use the new optimizer instead of the legacy optimizer when TF2 is used.\n\nFixes https://github.com/keras-team/keras/issues/17275\n\nPiperOrigin-RevId: 494221589\n\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Model(training_lib.Model):",
            "if not isinstance(self.optimizer, optimizer_v2.OptimizerV2):",
            "raise ValueError(",
            "'\"optimizer\" must be an instance of '",
            "-                    \"tf.keras.optimizers.Optimizer when a dype policy \"",
            "-                    \"with a loss scale  used, but got: %s. Using policy: \"",
            "+                    \"tf.keras.optimizers.legacy.Optimizer when a dype policy \"",
            "+                    \"with a loss scale is used, but got: %s. Using policy: \"",
            "\"%s\" % (self.optimizer, self._dtype_policy)",
            ")",
            "self.optimizer = loss_scale_optimizer.LossScaleOptimizer("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"tf.keras.optimizers.Optimizer when a dype policy \"), value='\"tf.keras.optimizers.legacy.Optimizer when a dype policy \"')",
            "Update(target_node=ASTNode(type=string, text=\"with a loss scale  used, but got: %s. Using policy: \"), value='\"with a loss scale is used, but got: %s. Using policy: \"')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 5306,
        "neg_line": [
            "-\"tf.keras.optimizers.Optimizer when a dype policy \"",
            "-\"with a loss scale  used, but got: %s. Using policy: \""
        ],
        "pos_line": [
            "+\"tf.keras.optimizers.legacy.Optimizer when a dype policy \"",
            "+\"with a loss scale is used, but got: %s. Using policy: \""
        ],
        "core_change": "-\"tf.keras.optimizers.Optimizer when a dype policy \" -\"with a loss scale  used, but got: %s. Using policy: \" +\"tf.keras.optimizers.legacy.Optimizer when a dype policy \" +\"with a loss scale is used, but got: %s. Using policy: \"",
        "core_API": "LossScaleOptimizer"
    },
    {
        "commit_hash": "b93ce6e23042da153d6ee04b067e1f3d827d8189",
        "index": "1071b2ac8..8eba1a0a7 100644",
        "commit_message": "fix crossentropy size in chatbot to align mask\n",
        "file": "tutorials.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class LuongAttnDecoderRNN(nn.Module):",
            "",
            "def maskNLLLoss(inp, target, mask):",
            "nTotal = mask.sum()",
            "-    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)))",
            "+    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))",
            "loss = crossEntropy.masked_select(mask).mean()",
            "loss = loss.to(device)",
            "return loss, nTotal.item()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1276951)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=1276952)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1276953)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'squeeze'), position=2, insert_id=1276954)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1276955)",
            "Insert(target_node=IN(type=argument_list), node=('integer', '1'), position=1, insert_id=1276956)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1276957)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 5309,
        "neg_line": [
            "-crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)))"
        ],
        "pos_line": [
            "+crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))"
        ],
        "core_change": "-crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1))) +crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))",
        "core_API": "sum"
    },
    {
        "commit_hash": "e750fe7819ce00cb3585aea628a038ed67ad364f",
        "index": "c865bcccd..ae5061245 100644",
        "commit_message": "fix test for dtype\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class CTCPrefixScorer(PartialScorerInterface):",
            "def score_partial(self, y, ids, state, x):",
            "prev_score, state = state",
            "presub_score, new_st = self.impl(y.cpu(), ids.cpu(), state)",
            "-        tscore = torch.as_tensor(presub_score - prev_score, device=y.device)",
            "+        tscore = torch.as_tensor(presub_score - prev_score, device=x.device, dtype=x.dtype)",
            "return tscore, (presub_score, new_st)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=168802)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=168803)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=168804)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=168805)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=168806)",
            "Update(target_node=ASTNode(type=identifier, text=y), value='x')",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'x'), position=0, insert_id=168807)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=168808)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dtype'), position=2, insert_id=168809)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 5310,
        "neg_line": [
            "-tscore = torch.as_tensor(presub_score - prev_score, device=y.device)"
        ],
        "pos_line": [
            "+tscore = torch.as_tensor(presub_score - prev_score, device=x.device, dtype=x.dtype)"
        ],
        "core_change": "-tscore = torch.as_tensor(presub_score - prev_score, device=y.device) +tscore = torch.as_tensor(presub_score - prev_score, device=x.device, dtype=x.dtype)",
        "core_API": "impl"
    },
    {
        "commit_hash": "5da3628de50f597f08cd265e921b06d4951b8607",
        "index": "6e29cf7a..5e03a56a 100644",
        "commit_message": "PointConv fix\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class PointConv(MessagePassing):",
            "N, M = edge_index[0].max().item() + 1, edge_index[1].max().item() + 1",
            "return self.propagate(edge_index, size=(N, M), x=x, pos=pos)",
            "",
            "-    def message(self, x_i, pos_i, pos_j):",
            "-        msg = pos_i - pos_j",
            "-        if x_i is not None:",
            "-            msg = torch.cat([x_i, msg], dim=1)",
            "+    def message(self, x_j, pos_j, pos_i):",
            "+        msg = pos_j - pos_i",
            "+        if x_j is not None:",
            "+            msg = torch.cat([x_j, msg], dim=1)",
            "if self.local_nn is not None:",
            "msg = self.local_nn(msg)",
            "return msg"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=x_i), value='x_j')",
            "Update(target_node=ASTNode(type=identifier, text=pos_i), value='pos_j')",
            "Update(target_node=ASTNode(type=identifier, text=pos_j), value='pos_i')",
            "Update(target_node=ASTNode(type=identifier, text=x_i), value='x_j')",
            "Update(target_node=ASTNode(type=identifier, text=pos_i), value='pos_j')",
            "Update(target_node=ASTNode(type=identifier, text=pos_j), value='pos_i')",
            "Update(target_node=ASTNode(type=identifier, text=x_i), value='x_j')"
        ],
        "plus_line": 4,
        "minus_line": 4,
        "AST_diff_line": 7,
        "number": 5311,
        "neg_line": [
            "-def message(self, x_i, pos_i, pos_j):",
            "-msg = pos_i - pos_j",
            "-if x_i is not None:",
            "-msg = torch.cat([x_i, msg], dim=1)"
        ],
        "pos_line": [
            "+def message(self, x_j, pos_j, pos_i):",
            "+msg = pos_j - pos_i",
            "+if x_j is not None:",
            "+msg = torch.cat([x_j, msg], dim=1)"
        ],
        "core_change": "-def message(self, x_i, pos_i, pos_j): -msg = pos_i - pos_j -if x_i is not None: -msg = torch.cat([x_i, msg], dim=1) +def message(self, x_j, pos_j, pos_i): +msg = pos_j - pos_i +if x_j is not None: +msg = torch.cat([x_j, msg], dim=1)",
        "core_API": "propagate"
    },
    {
        "commit_hash": "fab5085674f7748dc16d7ca25afb225fa441bc9d",
        "index": "5fe5685..d790523 100644",
        "commit_message": "EMA bug fix 2 (#2330)\n\n* EMA bug fix 2\n\n* update\n",
        "file": "yolov5.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def attempt_load(weights, map_location=None):",
            "model = Ensemble()",
            "for w in weights if isinstance(weights, list) else [weights]:",
            "attempt_download(w)",
            "-        model.append(torch.load(w, map_location=map_location)['model'].float().fuse().eval())  # load FP32 model",
            "+        ckpt = torch.load(w, map_location=map_location)  # load",
            "+        model.append(ckpt['ema' if ckpt.get('ema') else 'model'].float().fuse().eval())  # FP32 model",
            "",
            "# Compatibility updates",
            "for m in model.modules():"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=4, insert_id=1299269)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1299270)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'ckpt'), position=0, insert_id=1299271)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1299272)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=call), position=2)",
            "Insert(target_node=ASTNode(type=subscript), node=('identifier', 'ckpt'), position=0, insert_id=1299273)",
            "Insert(target_node=ASTNode(type=subscript), node=('[', '['), position=1, insert_id=1299274)",
            "Insert(target_node=ASTNode(type=subscript), node=('conditional_expression', None), position=2, insert_id=1299275)",
            "Insert(target_node=IN(type=conditional_expression), node=('string', \"'ema'\"), position=0, insert_id=1299276)",
            "Insert(target_node=IN(type=conditional_expression), node=('if', 'if'), position=1, insert_id=1299277)",
            "Insert(target_node=IN(type=conditional_expression), node=('call', None), position=2, insert_id=1299278)",
            "Insert(target_node=IN(type=conditional_expression), node=('else', 'else'), position=3, insert_id=1299279)",
            "Move(target_node=IN(type=conditional_expression), node=ASTNode(type=string, text='model'), position=4)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1299280)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1299281)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'ckpt'), position=0, insert_id=1299282)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1299283)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'get'), position=2, insert_id=1299284)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1299285)",
            "Insert(target_node=IN(type=argument_list), node=('string', \"'ema'\"), position=1, insert_id=1299286)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1299287)",
            "Delete(target_node=ASTNode(type=[, text=[))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 22,
        "number": 5312,
        "neg_line": [
            "-model.append(torch.load(w, map_location=map_location)['model'].float().fuse().eval())  # load FP32 model"
        ],
        "pos_line": [
            "+ckpt = torch.load(w, map_location=map_location)  # load",
            "+model.append(ckpt['ema' if ckpt.get('ema') else 'model'].float().fuse().eval())  # FP32 model"
        ],
        "core_change": "-model.append(torch.load(w, map_location=map_location)['model'].float().fuse().eval())  # load FP32 model +ckpt = torch.load(w, map_location=map_location)  # load +model.append(ckpt['ema' if ckpt.get('ema') else 'model'].float().fuse().eval())  # FP32 model",
        "core_API": "append"
    },
    {
        "commit_hash": "3260fe768cd1d387e86afee61b9297076ee5b164",
        "index": "57901bf08e..3eb2b669f1 100644",
        "commit_message": "small device handling fixes.\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "from ivy.core.container import Container",
            "",
            "",
            "def variable(x):",
            "-    with _tf.device('/' + ivy.dev(x).upper()):",
            "+    with _tf.device('/' + ivy.dev(x, as_str=True).upper()):",
            "return _tf.Variable(x, trainable=True)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=2034918)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=2034919)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'as_str'), position=0, insert_id=2034920)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2034921)",
            "Insert(target_node=IN(type=keyword_argument), node=('true', 'True'), position=2, insert_id=2034922)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 5313,
        "neg_line": [
            "-with _tf.device('/' + ivy.dev(x).upper()):"
        ],
        "pos_line": [
            "+with _tf.device('/' + ivy.dev(x, as_str=True).upper()):"
        ],
        "core_change": "-with _tf.device('/' + ivy.dev(x).upper()): +with _tf.device('/' + ivy.dev(x, as_str=True).upper()):",
        "core_API": "device"
    },
    {
        "commit_hash": "c6bc7fde370e54312d9fabdeec66818240b6e783",
        "index": "64ca84a1..18b04c49 100644",
        "commit_message": "SRL eval script (#220)\n\n* script for SRL evaluation\n\n* rename script\n\n* script tweaks\n\n* fix for sentences with no verbal predicates\n\n* change name and remove perl script bit\n\n* script for SRL evaluation\n\n* rename script\n\n* script tweaks\n\n* fix for sentences with no verbal predicates\n\n* change name and remove perl script bit\n\n* make evaluation faster by doing batch prediction\n\n* spacing\n\n* fix viterbi decoding in SRL model, edit script to use model.decode\n\n* make forward_on_instance call decode\n\n* pylint\n\n* fix out of date docstring\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Model(torch.nn.Module, Registrable):",
            "add_batch_dimension=True,",
            "cuda_device=cuda_device,",
            "for_training=False)",
            "-        outputs = self.forward(**model_input)",
            "+        outputs = self.decode(self.forward(**model_input))",
            "",
            "for name, output in list(outputs.items()):",
            "output = output[0]"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=44641)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=44642)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=44643)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=44644)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'decode'), position=2, insert_id=44645)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=44646)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=call), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=44647)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 5314,
        "neg_line": [
            "-outputs = self.forward(**model_input)"
        ],
        "pos_line": [
            "+outputs = self.decode(self.forward(**model_input))"
        ],
        "core_change": "-outputs = self.forward(**model_input) +outputs = self.decode(self.forward(**model_input))",
        "core_API": "forward"
    },
    {
        "commit_hash": "c05ace221339102abb1c8b0933b9cbfd51ddd4a3",
        "index": "8b47960be..d90833283 100644",
        "commit_message": "Use f-strings in the dataset scripts (#3291)\n\n* Finishes #3257\n\nUsed f-strings to format the .py files in the dataset folder\n\n* Fix style\n\n* Fix hkcancor dataset\n\nCo-authored-by: Mario ako <mario@huggingface.co>\nCo-authored-by: mariosasko <mariosasko777@gmail.com>\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class Reclor(datasets.GeneratorBasedBuilder):",
            "",
            "if not os.path.exists(data_dir):",
            "raise FileNotFoundError(",
            "-                \"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('wikihow', data_dir=...)` that includes files unzipped from the reclor zip. Manual download instructions: {}\".format(",
            "-                    data_dir, self.manual_download_instructions",
            "-                )",
            "+                f\"{data_dir} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('wikihow', data_dir=...)` that includes files unzipped from the reclor zip. Manual download instructions: {self.manual_download_instructions}\"",
            ")",
            "return [",
            "datasets.SplitGenerator("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('string', 'f\"{data_dir} does not exist. Make sure you insert a manual dir via `datasets.load_dataset(\\'wikihow\\', data_dir=...)` that includes files unzipped from the reclor zip. Manual download instructions: {self.manual_download_instructions}\"'), position=1, insert_id=1781640)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=), text=)), position=2)",
            "Delete(target_node=ASTNode(type=string, text=\"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('wikihow', data_dir=...)` that includes files unzipped from the reclor zip. Manual download instructions: {}\"))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=format))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=data_dir))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=manual_download_instructions))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=), text=)))"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 16,
        "number": 5315,
        "neg_line": [
            "-\"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('wikihow', data_dir=...)` that includes files unzipped from the reclor zip. Manual download instructions: {}\".format(",
            "-data_dir, self.manual_download_instructions",
            "-)"
        ],
        "pos_line": [
            "+f\"{data_dir} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('wikihow', data_dir=...)` that includes files unzipped from the reclor zip. Manual download instructions: {self.manual_download_instructions}\""
        ],
        "core_change": "-\"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('wikihow', data_dir=...)` that includes files unzipped from the reclor zip. Manual download instructions: {}\".format( -data_dir, self.manual_download_instructions -) +f\"{data_dir} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('wikihow', data_dir=...)` that includes files unzipped from the reclor zip. Manual download instructions: {self.manual_download_instructions}\"",
        "core_API": "exists"
    },
    {
        "commit_hash": "d31ba4595bb60b0537b8aae227c4e82f8f21775f",
        "index": "0d5fcc97..79b1efec 100644",
        "commit_message": "misc fix\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def build_GAN_losses(vecpos, vecneg):",
            "tf.histogram_summary('sigmoid-neg', sigmneg)",
            "",
            "d_loss_pos = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(",
            "-        vecpos, tf.ones_like(vecpos)), name='d_loss_pos')",
            "+        vecpos, tf.ones_like(vecpos)), name='d_CE_loss_pos')",
            "d_loss_neg = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(",
            "-        vecneg, tf.zeros_like(vecneg)), name='d_loss_neg')",
            "+        vecneg, tf.zeros_like(vecneg)), name='d_CE_loss_neg')",
            "",
            "d_pos_acc = tf.reduce_mean(tf.cast(sigmpos > 0.5, tf.float32), name='pos_acc')",
            "d_neg_acc = tf.reduce_mean(tf.cast(sigmneg < 0.5, tf.float32), name='neg_acc')",
            "",
            "g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(",
            "-        vecneg, tf.ones_like(vecneg)), name='g_loss')",
            "-    d_loss = tf.add(d_loss_pos, d_loss_neg, name='d_loss')",
            "+        vecneg, tf.ones_like(vecneg)), name='g_CE_loss')",
            "+    d_loss = tf.add(d_loss_pos, d_loss_neg, name='d_CE_loss')",
            "add_moving_summary(d_loss_pos, d_loss_neg,",
            "g_loss, d_loss,",
            "d_pos_acc, d_neg_acc)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2309879)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2309880)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2309881)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'reduce_mean'), position=2, insert_id=2309882)",
            "Update(target_node=ASTNode(type=string, text='d_loss_pos'), value=\"'d_CE_loss_pos'\")",
            "Update(target_node=ASTNode(type=string, text='d_loss_neg'), value=\"'d_CE_loss_neg'\")",
            "Update(target_node=ASTNode(type=string, text='g_loss'), value=\"'g_CE_loss'\")",
            "Update(target_node=ASTNode(type=string, text='d_loss'), value=\"'d_CE_loss'\")",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=reduce_mean))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 4,
        "minus_line": 4,
        "AST_diff_line": 13,
        "number": 5316,
        "neg_line": [
            "-vecpos, tf.ones_like(vecpos)), name='d_loss_pos')",
            "-vecneg, tf.zeros_like(vecneg)), name='d_loss_neg')",
            "-vecneg, tf.ones_like(vecneg)), name='g_loss')",
            "-d_loss = tf.add(d_loss_pos, d_loss_neg, name='d_loss')"
        ],
        "pos_line": [
            "+vecpos, tf.ones_like(vecpos)), name='d_CE_loss_pos')",
            "+vecneg, tf.zeros_like(vecneg)), name='d_CE_loss_neg')",
            "+vecneg, tf.ones_like(vecneg)), name='g_CE_loss')",
            "+d_loss = tf.add(d_loss_pos, d_loss_neg, name='d_CE_loss')"
        ],
        "core_change": "-vecpos, tf.ones_like(vecpos)), name='d_loss_pos') +vecpos, tf.ones_like(vecpos)), name='d_CE_loss_pos') -vecneg, tf.zeros_like(vecneg)), name='d_loss_neg') +vecneg, tf.zeros_like(vecneg)), name='d_CE_loss_neg') -vecneg, tf.ones_like(vecneg)), name='g_loss') -d_loss = tf.add(d_loss_pos, d_loss_neg, name='d_loss') +vecneg, tf.ones_like(vecneg)), name='g_CE_loss') +d_loss = tf.add(d_loss_pos, d_loss_neg, name='d_CE_loss')",
        "core_API": "histogram_summary"
    },
    {
        "commit_hash": "777ed83e4f85f0e24af7ee3d14fb1b1eb098b28d",
        "index": "5649e2d9..79eed2f6 100644",
        "commit_message": "fix: typo spelling, fixed indentation\n\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class DenseFeaturesTest(keras_parameterized.TestCase):",
            "with tf.Graph().as_default():",
            "# Dynamic rank 0 should fail",
            "features = {",
            "-          'price': tf.compat.v1.placeholder(tf.float32),",
            "+        'price': tf.compat.v1.placeholder(tf.float32),",
            "}",
            "net = df.DenseFeatures([price])(features)",
            "self.assertEqual(1, net.shape[1])"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 5320,
        "neg_line": [
            "-'price': tf.compat.v1.placeholder(tf.float32),"
        ],
        "pos_line": [
            "+'price': tf.compat.v1.placeholder(tf.float32),"
        ],
        "core_change": "-'price': tf.compat.v1.placeholder(tf.float32), +'price': tf.compat.v1.placeholder(tf.float32),",
        "core_API": "Graph"
    },
    {
        "commit_hash": "47ec57b52e41492f2747997c39bf2951797ed7c8",
        "index": "362497acb1..4d9c10b8cd 100644",
        "commit_message": "lintfixbot: Auto-commit fixed lint errors in codebase\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def real(",
            "",
            "",
            "def isposinf(",
            "-        x: Union[tf.Tensor, tf.Variable],",
            "-        /,",
            "-        *,",
            "-        out: Optional[Union[tf.Tensor, tf.Variable]] = None,",
            "+    x: Union[tf.Tensor, tf.Variable],",
            "+    /,",
            "+    *,",
            "+    out: Optional[Union[tf.Tensor, tf.Variable]] = None,",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "return tf.experimental.numpy.isposinf(x)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 4,
        "minus_line": 4,
        "AST_diff_line": 17,
        "number": 5321,
        "neg_line": [
            "-x: Union[tf.Tensor, tf.Variable],",
            "-/,",
            "-*,",
            "-out: Optional[Union[tf.Tensor, tf.Variable]] = None,"
        ],
        "pos_line": [
            "+x: Union[tf.Tensor, tf.Variable],",
            "+/,",
            "+*,",
            "+out: Optional[Union[tf.Tensor, tf.Variable]] = None,"
        ],
        "core_change": "-x: Union[tf.Tensor, tf.Variable], -/, -*, -out: Optional[Union[tf.Tensor, tf.Variable]] = None, +x: Union[tf.Tensor, tf.Variable], +/, +*, +out: Optional[Union[tf.Tensor, tf.Variable]] = None,",
        "core_API": "isposinf"
    },
    {
        "commit_hash": "3febd85f8e1ba45e391fcb86fb3f68168e924501",
        "index": "6b1d6d91cc..af98abb861 100644",
        "commit_message": "small fix inplace_update to pass tf 'inv' in array api tests (#6288)\n\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def inplace_update(",
            "elif ivy.is_ivy_array(x):",
            "x.data = val_native",
            "else:",
            "-            raise ivy.exceptions.IvyException(",
            "-                \"TensorFlow does not support inplace updates of the tf.Tensor\"",
            "-            )",
            "+            x = ivy.to_ivy(x_native)",
            "return x",
            "else:",
            "return val"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('type', None), position=2, insert_id=1981793)",
            "Insert(target_node=ASTNode(type=assignment), node=('=', '='), position=3, insert_id=1981794)",
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=call), position=4)",
            "Insert(target_node=IN(type=type), node=('identifier', 'x'), position=0, insert_id=1981795)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=ivy), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=exceptions), value='to_ivy')",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'x_native'), position=1, insert_id=1981796)",
            "Delete(target_node=ASTNode(type=identifier, text=raise))",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=IvyException))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=string, text=\"TensorFlow does not support inplace updates of the tf.Tensor\"))",
            "Delete(target_node=ASTNode(type=type))"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 15,
        "number": 5322,
        "neg_line": [
            "-raise ivy.exceptions.IvyException(",
            "-\"TensorFlow does not support inplace updates of the tf.Tensor\"",
            "-)"
        ],
        "pos_line": [
            "+x = ivy.to_ivy(x_native)"
        ],
        "core_change": "-raise ivy.exceptions.IvyException( -\"TensorFlow does not support inplace updates of the tf.Tensor\" -) +x = ivy.to_ivy(x_native)",
        "core_API": "is_ivy_array"
    },
    {
        "commit_hash": "cd14f8995bf4c659a5f2b58b87d615da849cbbaf",
        "index": "eb7ebb50fa..19a40b852f 100644",
        "commit_message": "Fixed small issue with reshape\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def reshape(",
            "shape: Union[ivy.NativeShape, Sequence[int]],",
            "*,",
            "copy: Optional[bool] = None,",
            "-    out: Optional[Union[tf.Tensor, tf.Variable]] = None,",
            "order: Optional[str] = \"C\",",
            "+    out: Optional[Union[tf.Tensor, tf.Variable]] = None,",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "ivy.assertions.check_elem_in_list(order, [\"C\", \"F\"])",
            "if copy:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=typed_default_parameter), node=ASTNode(type=parameters), position=7)",
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=10, insert_id=1973250)",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 3,
        "number": 5325,
        "neg_line": [
            "-out: Optional[Union[tf.Tensor, tf.Variable]] = None,"
        ],
        "pos_line": [
            "+out: Optional[Union[tf.Tensor, tf.Variable]] = None,"
        ],
        "core_change": "-out: Optional[Union[tf.Tensor, tf.Variable]] = None, +out: Optional[Union[tf.Tensor, tf.Variable]] = None,",
        "core_API": "check_elem_in_list"
    },
    {
        "commit_hash": "c81821b8562daecc9796d205c25d4ceeb72d1a31",
        "index": "8bda9106fc..1e0b39fe55 100644",
        "commit_message": "[rllib] Make Pong-v0 + EvolutionStrategies work by sharing preprocessors with PPO (#848)\n\n* fix by sharing preprocessors\n\n* revert param changeg\n\n* Update evolution_strategies.py\n\n* Update catalog.py\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Agent(object):",
            "self.common_policy = self.par_opt.get_common_loss()",
            "self.variables = ray.experimental.TensorFlowVariables(",
            "self.common_policy.loss, self.sess)",
            "-        self.observation_filter = MeanStdFilter(preprocessor.shape, clip=None)",
            "+        self.observation_filter = MeanStdFilter(",
            "+            self.preprocessor_shape, clip=None)",
            "self.reward_filter = MeanStdFilter((), clip=5.0)",
            "self.sess.run(tf.global_variables_initializer())"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=preprocessor), value='self')",
            "Update(target_node=ASTNode(type=identifier, text=shape), value='preprocessor_shape')"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 5327,
        "neg_line": [
            "-self.observation_filter = MeanStdFilter(preprocessor.shape, clip=None)"
        ],
        "pos_line": [
            "+self.observation_filter = MeanStdFilter(",
            "+self.preprocessor_shape, clip=None)"
        ],
        "core_change": "-self.observation_filter = MeanStdFilter(preprocessor.shape, clip=None) +self.observation_filter = MeanStdFilter( +self.preprocessor_shape, clip=None)",
        "core_API": "get_common_loss"
    },
    {
        "commit_hash": "98291f0d33b704d1e4d14197278f2996ac8f4b4a",
        "index": "189b2416..eb210d9d 100755",
        "commit_message": "[WIP] - Documentation Cleaning and Coding Style (#516)\n\n* Activation Cleaning Docstring Test\n\n* Requirements Pinned with range to insure tested versions are used. Range are used to prevent updating requirements all the time.\n\n* setup.cfg file added with PEP8 configuration\n\n* activation.py refactored\n\n* docstring fixed - ready for documentation unittest\n\n* Yapf correction for max_line_length: 120\n\n* test yapf refactored\n\n* Requirements conflict solved\n\n* Yapf Style modified and merged in file \"setup.cfg\"\n\n* Yapf Confiuguration Updated\n\n* Code Refactored with new YAPF formating style\n\n* Code Refactored with new YAPF formating style\n\n* Code Refactored with new YAPF formating style\n\n* tl.layers.pooling YAPF reformat\n\n* yapf updated\n\n* gitignore updated\n\n* YAPF Style Fixing Attempt\n\n* Space Error Fix\n\n* Style Correction\n\n* Assertion Codacy Errors Corrected\n\n* Error Fix\n\n* Assertion Refactored\n\n* YAPF Style Applied to Master\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "img, label = read_and_decode(\"train.cifar10\")",
            "",
            "## Use shuffle_batch or batch",
            "# see https://www.tensorflow.org/versions/master/api_docs/python/io_ops.html#shuffle_batch",
            "-img_batch, label_batch = tf.train.shuffle_batch([img, label], batch_size=4, capacity=50000, min_after_dequeue=10000, num_threads=1)",
            "+img_batch, label_batch = tf.train.shuffle_batch(",
            "+    [img, label], batch_size=4, capacity=50000, min_after_dequeue=10000, num_threads=1",
            "+)",
            "",
            "print(\"img_batch   : %s\" % img_batch._shape)",
            "print(\"label_batch : %s\" % label_batch._shape)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 17,
        "number": 5331,
        "neg_line": [
            "-img_batch, label_batch = tf.train.shuffle_batch([img, label], batch_size=4, capacity=50000, min_after_dequeue=10000, num_threads=1)"
        ],
        "pos_line": [
            "+img_batch, label_batch = tf.train.shuffle_batch(",
            "+[img, label], batch_size=4, capacity=50000, min_after_dequeue=10000, num_threads=1",
            "+)"
        ],
        "core_change": "-img_batch, label_batch = tf.train.shuffle_batch([img, label], batch_size=4, capacity=50000, min_after_dequeue=10000, num_threads=1) +img_batch, label_batch = tf.train.shuffle_batch( +[img, label], batch_size=4, capacity=50000, min_after_dequeue=10000, num_threads=1 +)",
        "core_API": "shuffle_batch"
    },
    {
        "commit_hash": "4093ead11984c0b5c904869e3fd1ac211b36714a",
        "index": "a8141c1..ffc2e0b 100644",
        "commit_message": "Remove AttributeError trap (#241)\n\n* Remove AttributeError trap\n\n* Trap AttributeError in run_pass\n\n* One more fix\n\n",
        "file": "tensorboardX.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "autograd",
        "change": [
            "class SummaryWriter(object):",
            "print('add_graph() only supports PyTorch v0.2.')",
            "return",
            "self.file_writer.add_graph(graph(model, input_to_model, verbose))",
            "-        except AttributeError:",
            "+        else:",
            "# Caffe2 models do not have the 'forward' method",
            "if not self.caffe2_enabled:",
            "# TODO (ml7): Remove when PyTorch 1.0 merges PyTorch and Caffe2"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=AttributeError), value='else')",
            "Delete(target_node=ASTNode(type=identifier, text=except))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 5333,
        "neg_line": [
            "-except AttributeError:"
        ],
        "pos_line": [
            "+else:"
        ],
        "core_change": "-except AttributeError: +else:",
        "core_API": "add_graph"
    },
    {
        "commit_hash": "88b317739fe56888528c857fc8e90967148a0051",
        "index": "ee122205b..b2bf66f75 100644",
        "commit_message": "Fix issue: #1962, input's shape seem to cause error in 2.2.0 version tf_albert_model\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFAlbertModel(TFAlbertPreTrainedModel):",
            "if input_ids is not None and inputs_embeds is not None:",
            "raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")",
            "elif input_ids is not None:",
            "-            input_shape = input_ids.shape",
            "+            input_shape = tf.shape(input_ids)",
            "elif inputs_embeds is not None:",
            "input_shape = inputs_embeds.shape[:-1]",
            "else:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=expression_statement), node=('call', None), position=0, insert_id=2385042)",
            "Move(target_node=IN(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2385043)",
            "Insert(target_node=ASTNode(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2385044)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2385045)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=input_ids), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2385046)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 5334,
        "neg_line": [
            "-input_shape = input_ids.shape"
        ],
        "pos_line": [
            "+input_shape = tf.shape(input_ids)"
        ],
        "core_change": "-input_shape = input_ids.shape +input_shape = tf.shape(input_ids)",
        "core_API": "shape"
    },
    {
        "commit_hash": "30d4da02da09f3704cb0a70d60317fecde913cda",
        "index": "2674d79e..de7394f1 100644",
        "commit_message": "Fix dropout\n\n",
        "file": "flair.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class RelationExtractor(flair.nn.DefaultClassifier):",
            "",
            "relation_embeddings.append(embedding)",
            "",
            "-            # stack and drop out",
            "-            all_relations = torch.stack(relation_embeddings)",
            "+            # stack and drop out (squeeze and unsqueeze)",
            "+            all_relations = torch.stack(relation_embeddings).unsqueeze(1)",
            "",
            "all_relations = self.dropout(all_relations)",
            "all_relations = self.locked_dropout(all_relations)",
            "all_relations = self.word_dropout(all_relations)",
            "",
            "+            all_relations = all_relations.squeeze(1)",
            "+",
            "# send through decoder",
            "if self.non_linear_decoder:",
            "sentence_relation_scores = self.decoder_2(self.nonlinearity(self.decoder_1(all_relations)))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=6, insert_id=235535)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=235536)",
            "Insert(target_node=ASTNode(type=assignment), node=('call', None), position=2, insert_id=235537)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'all_relations'), position=0, insert_id=235538)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=235539)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=235540)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=235541)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=235542)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=235543)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=235544)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=235545)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'unsqueeze'), position=2, insert_id=235546)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=235547)",
            "Insert(target_node=IN(type=argument_list), node=('integer', '1'), position=1, insert_id=235548)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=235549)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'all_relations'), position=0, insert_id=235550)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=235551)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'squeeze'), position=2, insert_id=235552)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=235553)",
            "Insert(target_node=IN(type=argument_list), node=('integer', '1'), position=1, insert_id=235554)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=235555)"
        ],
        "plus_line": 3,
        "minus_line": 2,
        "AST_diff_line": 22,
        "number": 5337,
        "neg_line": [
            "-# stack and drop out",
            "-all_relations = torch.stack(relation_embeddings)"
        ],
        "pos_line": [
            "+# stack and drop out (squeeze and unsqueeze)",
            "+all_relations = torch.stack(relation_embeddings).unsqueeze(1)",
            "+all_relations = all_relations.squeeze(1)",
            "+"
        ],
        "core_change": "-# stack and drop out -all_relations = torch.stack(relation_embeddings) +# stack and drop out (squeeze and unsqueeze) +all_relations = torch.stack(relation_embeddings).unsqueeze(1) +all_relations = all_relations.squeeze(1) +",
        "core_API": "append"
    },
    {
        "commit_hash": "133cd97161ef58f1cf486c498e36b76eb6e53725",
        "index": "c31e48ca..343a7621 100644",
        "commit_message": "* Clean up documentation of Hull-White and HJM.\n* Fix treatment of initial_discount_rate_fn by Hull-White model.\n* Reduce test size for hjm/swaption_pricing_test\n\nPiperOrigin-RevId: 379671870\n\n",
        "file": "tf-quant-finance.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class HullWhiteBermudanSwaptionTest(parameterized.TestCase, tf.test.TestCase):",
            "self.float_leg_end_times) - np.array(self.float_leg_start_times)",
            "self.fixed_leg_daycount_fractions = self.float_leg_daycount_fractions",
            "self.fixed_leg_coupon = 0.011 * np.ones_like(self.fixed_leg_payment_times)",
            "-    self.zero_rate_fn = lambda x: 0.01 * tf.ones_like(x)",
            "+    zero_rate_fn = lambda x: 0.01 * tf.expand_dims(tf.ones_like(x), axis=-1)",
            "+    self.zero_rate_fn = zero_rate_fn",
            "",
            "super(HullWhiteBermudanSwaptionTest, self).setUp()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=5, insert_id=2337504)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=2337505)",
            "Insert(target_node=ASTNode(type=assignment), node=('identifier', 'zero_rate_fn'), position=0, insert_id=2337506)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=2337507)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'zero_rate_fn'), position=2, insert_id=2337508)",
            "Insert(target_node=ASTNode(type=binary_operator), node=('call', None), position=2, insert_id=2337509)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2337510)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2337511)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2337512)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2337513)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'expand_dims'), position=2, insert_id=2337514)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2337515)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=call), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=2337516)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=3, insert_id=2337517)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=2337518)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'axis'), position=0, insert_id=2337519)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2337520)",
            "Insert(target_node=IN(type=keyword_argument), node=('unary_operator', '-1'), position=2, insert_id=2337521)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 20,
        "number": 5340,
        "neg_line": [
            "-self.zero_rate_fn = lambda x: 0.01 * tf.ones_like(x)"
        ],
        "pos_line": [
            "+zero_rate_fn = lambda x: 0.01 * tf.expand_dims(tf.ones_like(x), axis=-1)",
            "+self.zero_rate_fn = zero_rate_fn"
        ],
        "core_change": "-self.zero_rate_fn = lambda x: 0.01 * tf.ones_like(x) +zero_rate_fn = lambda x: 0.01 * tf.expand_dims(tf.ones_like(x), axis=-1) +self.zero_rate_fn = zero_rate_fn",
        "core_API": "array"
    },
    {
        "commit_hash": "2488a34f8a95d71796bbf71418271670d6e02f81",
        "index": "4a5e6c7b..b709ddaf 100755",
        "commit_message": "fix build\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class CenterModel(EmbeddingModel):",
            "",
            "# tag the embedding of 'input' with name 'emb', just for inference later on",
            "with tf.variable_scope(tf.get_variable_scope(), reuse=True):",
            "-            tf.identity(self.embed(inputs[0]), name=\"emb\")",
            "+            tf.identity(self.embed(x), name=\"emb\")",
            "",
            "# compute the embedding loss",
            "emb_cost = center_loss(x, label, 10, 0.01)"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=inputs), value='x')",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=inputs), position=1)",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=integer, text=0))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 5341,
        "neg_line": [
            "-tf.identity(self.embed(inputs[0]), name=\"emb\")"
        ],
        "pos_line": [
            "+tf.identity(self.embed(x), name=\"emb\")"
        ],
        "core_change": "-tf.identity(self.embed(inputs[0]), name=\"emb\") +tf.identity(self.embed(x), name=\"emb\")",
        "core_API": "variable_scope"
    },
    {
        "commit_hash": "6a4fa73a38935a18779ce1809892730fd1572bee",
        "index": "3372aae2..3bc71ee5 100644",
        "commit_message": "small fix\n\n",
        "file": "stable-diffusion-webui.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class HypernetworkModule(torch.nn.Module):",
            "if add_layer_norm:",
            "linears.append(torch.nn.LayerNorm(int(dim * layer_structure[i+1])))",
            "",
            "-            # Add dropout",
            "-            if use_dropout:",
            "-                p = 0.5 if 0 <= i <= len(layer_structure) - 3 else 0.2",
            "-                linears.append(torch.nn.Dropout(p=p))",
            "+            # Add dropout expect last layer",
            "+            if use_dropout and i < len(layer_structure) - 3:",
            "+                linears.append(torch.nn.Dropout(p=0.3))",
            "",
            "self.linear = torch.nn.Sequential(*linears)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=if_statement), node=('boolean_operator', None), position=1, insert_id=1138964)",
            "Insert(target_node=ASTNode(type=if_statement), node=(':', ':'), position=2, insert_id=1138965)",
            "Insert(target_node=ASTNode(type=if_statement), node=('block', None), position=3, insert_id=1138966)",
            "Move(target_node=IN(type=boolean_operator), node=ASTNode(type=identifier, text=use_dropout), position=0)",
            "Insert(target_node=IN(type=boolean_operator), node=('and', 'and'), position=1, insert_id=1138967)",
            "Move(target_node=IN(type=boolean_operator), node=ASTNode(type=comparison_operator), position=2)",
            "Move(target_node=IN(type=block), node=ASTNode(type=expression_statement), position=0)",
            "Insert(target_node=ASTNode(type=comparison_operator), node=('<', '<'), position=2, insert_id=1138968)",
            "Insert(target_node=ASTNode(type=keyword_argument), node=('float', '0.3'), position=2, insert_id=1138969)",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=identifier, text=p))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=float, text=0.5))",
            "Delete(target_node=ASTNode(type=if, text=if))",
            "Delete(target_node=ASTNode(type=integer, text=0))",
            "Delete(target_node=ASTNode(type=<=, text=<=))",
            "Delete(target_node=ASTNode(type=<=, text=<=))",
            "Delete(target_node=ASTNode(type=else, text=else))",
            "Delete(target_node=ASTNode(type=float, text=0.2))",
            "Delete(target_node=ASTNode(type=conditional_expression))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=identifier, text=p))",
            "Delete(target_node=ASTNode(type=block))"
        ],
        "plus_line": 3,
        "minus_line": 4,
        "AST_diff_line": 24,
        "number": 5344,
        "neg_line": [
            "-# Add dropout",
            "-if use_dropout:",
            "-p = 0.5 if 0 <= i <= len(layer_structure) - 3 else 0.2",
            "-linears.append(torch.nn.Dropout(p=p))"
        ],
        "pos_line": [
            "+# Add dropout expect last layer",
            "+if use_dropout and i < len(layer_structure) - 3:",
            "+linears.append(torch.nn.Dropout(p=0.3))"
        ],
        "core_change": "-# Add dropout -if use_dropout: -p = 0.5 if 0 <= i <= len(layer_structure) - 3 else 0.2 -linears.append(torch.nn.Dropout(p=p)) +# Add dropout expect last layer +if use_dropout and i < len(layer_structure) - 3: +linears.append(torch.nn.Dropout(p=0.3))",
        "core_API": "append"
    },
    {
        "commit_hash": "cce13ee245fab03717d2afa1860e35986ebd9de9",
        "index": "8b7ed125..78fa8b1c 100644",
        "commit_message": "Fix bug in Graves Attn\n\nOn my machine at Graves attention the variable self.J ( self.J = torch.arange(0, inputs.shape[1]+2).to(inputs.device) + 0.5) is a LongTensor, but it must be a float tensor. So I get the following error:\n\nTraceback (most recent call last):\n  File \"train.py\", line 704, in <module>\n    main(args)\n  File \"train.py\", line 619, in main\n    global_step, epoch)\n  File \"train.py\", line 170, in train\n    text_input, text_lengths, mel_input, speaker_embeddings=speaker_embeddings)\n  File \"/home/edresson/anaconda3/envs/TTS2/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 489, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/mnt/edresson/DD/TTS/voice-clonning/TTS/tts_namespace/TTS/models/tacotron.py\", line 121, in forward\n    self.speaker_embeddings_projected)\n  File \"/home/edresson/anaconda3/envs/TTS2/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 489, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/mnt/edresson/DD/TTS/voice-clonning/TTS/tts_namespace/TTS/layers/tacotron.py\", line 435, in forward\n    output, stop_token, attention = self.decode(inputs, mask)\n  File \"/mnt/edresson/DD/TTS/voice-clonning/TTS/tts_namespace/TTS/layers/tacotron.py\", line 367, in decode\n    self.attention_rnn_hidden, inputs, self.processed_inputs, mask)\n  File \"/home/edresson/anaconda3/envs/TTS2/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 489, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/mnt/edresson/DD/TTS/voice-clonning/TTS/tts_namespace/TTS/layers/common_layers.py\", line 180, in forward\n    phi_t = g_t.unsqueeze(-1) * (1.0 / (1.0 + torch.sigmoid((mu_t.unsqueeze(-1) - j) / sig_t.unsqueeze(-1))))\nRuntimeError: expected type torch.cuda.FloatTensor but got torch.cuda.LongTensor\n\n\nIn addition the + 0.5 operation is canceled if it is a LongTensor.\nTest: \n>>> torch.arange(0, 10) \ntensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n>>> torch.arange(0, 10) + 0.5\ntensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n>>> torch.arange(0, 10.0) + 0.5\ntensor([0.5000, 1.5000, 2.5000, 3.5000, 4.5000, 5.5000, 6.5000, 7.5000, 8.5000,\n        9.5000])\n\nTo resolve this I forced the arrange range to float:\nself.J = torch.arange(0, inputs.shape[1]+2.0).to(inputs.device) + 0.5\n",
        "file": "TTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class GravesAttention(nn.Module):",
            "",
            "def init_states(self, inputs):",
            "if self.J is None or inputs.shape[1]+1 > self.J.shape[-1]:",
            "-            self.J = torch.arange(0, inputs.shape[1]+2).to(inputs.device) + 0.5",
            "+            self.J = torch.arange(0, inputs.shape[1]+2.0).to(inputs.device) + 0.5",
            "self.attention_weights = torch.zeros(inputs.shape[0], inputs.shape[1]).to(inputs.device)",
            "self.mu_prev = torch.zeros(inputs.shape[0], self.K).to(inputs.device)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=binary_operator), node=('float', '2.0'), position=2, insert_id=1270581)",
            "Delete(target_node=ASTNode(type=integer, text=2))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 5345,
        "neg_line": [
            "-self.J = torch.arange(0, inputs.shape[1]+2).to(inputs.device) + 0.5"
        ],
        "pos_line": [
            "+self.J = torch.arange(0, inputs.shape[1]+2.0).to(inputs.device) + 0.5"
        ],
        "core_change": "-self.J = torch.arange(0, inputs.shape[1]+2).to(inputs.device) + 0.5 +self.J = torch.arange(0, inputs.shape[1]+2.0).to(inputs.device) + 0.5",
        "core_API": "arange"
    },
    {
        "commit_hash": "ed63b7ee9e68ba120893aaaaca0d4525a56ed484",
        "index": "10293127..9222087d 100644",
        "commit_message": "upgrade to pytorch 0.4.0 (#1126)\n\n* bump pytorch to 0.4 + fix sanitize\n\n* remove check for Variable in block_orthogonal\n\n* rename parameter split_size=\n\n* remove checks for Variable\n\n* fix more tests\n\n* fixes\n\n* get tests to pass\n\n* fix warnings\n\n* get rid of some of the Variables\n\n* more tests passing\n\n* more elimination of variables\n\n* finish removing all Variables\n\n* pylint and such\n\n* a few fixes\n\n* move torch.no_grad into model.forward_on_instances\n\n* more pytorch 0.4 changes\n\n* detach() -> data\n\n* pylint\n\n* fix bad tensor creation\n\n* fix types\n\n* remove print statement\n\n* more 0.4 goodness\n\n* factor out is_tensor\n\n* add no_grad to elmo command\n\n* cleanup\n\n* remove TODO\n\n* address PR feedback\n\n* replace all() with item()\n\n* more cleanup\n\n* further cleanup\n\n* remove Variable\n\n* really fix merge conflict\n\n* fix pylint\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestSparseClipGrad(AllenNlpTestCase):",
            "# Now try to clip the gradients.",
            "_ = sparse_clip_norm([embedding.weight], 1.5)",
            "# Final norm should be 1.5",
            "-        grad = embedding.weight.grad.data.coalesce()",
            "-        self.assertAlmostEqual(grad._values().norm(2.0), 1.5, places=5) # pylint: disable=protected-access",
            "+        grad = embedding.weight.grad.coalesce()  # pylint: disable=no-member",
            "+        self.assertAlmostEqual(grad._values().norm(2.0).item(), 1.5, places=5) # pylint: disable=protected-access"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=37308)",
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=1, insert_id=37309)",
            "Update(target_node=ASTNode(type=identifier, text=data), value='coalesce')",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=37310)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=37311)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=37312)",
            "Move(target_node=IN(type=call), node=ASTNode(type=argument_list), position=1)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=37313)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'item'), position=2, insert_id=37314)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=coalesce))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 14,
        "number": 5347,
        "neg_line": [
            "-grad = embedding.weight.grad.data.coalesce()",
            "-self.assertAlmostEqual(grad._values().norm(2.0), 1.5, places=5) # pylint: disable=protected-access"
        ],
        "pos_line": [
            "+grad = embedding.weight.grad.coalesce()  # pylint: disable=no-member",
            "+self.assertAlmostEqual(grad._values().norm(2.0).item(), 1.5, places=5) # pylint: disable=protected-access"
        ],
        "core_change": "-grad = embedding.weight.grad.data.coalesce() -self.assertAlmostEqual(grad._values().norm(2.0), 1.5, places=5) # pylint: disable=protected-access +grad = embedding.weight.grad.coalesce()  # pylint: disable=no-member +self.assertAlmostEqual(grad._values().norm(2.0).item(), 1.5, places=5) # pylint: disable=protected-access",
        "core_API": "coalesce"
    },
    {
        "commit_hash": "0ba7472da92a89af9aca84de5b01a228d18340a2",
        "index": "f753b5fc7..2b7e9ca9e 100644",
        "commit_message": "[Testing] Fix LINT/sphinx errors. (#8874)\n\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class DQNTorchModel(TorchModelV2):",
            "sigma0 (float): initial value of noisy nets",
            "add_layer_norm (bool): Enable layer norm (for param noise).",
            "\"\"\"",
            "-",
            "+        nn.Module.__init__(self)",
            "super(DQNTorchModel, self).__init__(obs_space, action_space,",
            "num_outputs, model_config, name)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=4, insert_id=1506288)",
            "Insert(target_node=ASTNode(type=module), node=('ERROR', None), position=5, insert_id=1506289)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=1506290)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'super'), position=0, insert_id=1506291)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1506292)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1506293)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1506294)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1506295)",
            "Insert(target_node=IN(type=attribute), node=('identifier', '__init__'), position=2, insert_id=1506296)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1506297)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'self'), position=1, insert_id=1506298)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1506299)",
            "Insert(target_node=ASTNode(type=attribute), node=('tuple', None), position=0, insert_id=1506300)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'nn'), position=0, insert_id=1506301)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1506302)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Module'), position=2, insert_id=1506303)",
            "Move(target_node=IN(type=tuple), node=ASTNode(type=(, text=(), position=0)",
            "Move(target_node=IN(type=tuple), node=ASTNode(type=identifier, text=DQNTorchModel), position=1)",
            "Move(target_node=IN(type=tuple), node=ASTNode(type=,, text=,), position=2)",
            "Move(target_node=IN(type=tuple), node=ASTNode(type=identifier, text=self), position=3)",
            "Move(target_node=IN(type=tuple), node=ASTNode(type=), text=)), position=4)",
            "Delete(target_node=ASTNode(type=identifier, text=super))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 24,
        "number": 5348,
        "neg_line": [
            "-"
        ],
        "pos_line": [
            "+nn.Module.__init__(self)"
        ],
        "core_change": "- +nn.Module.__init__(self)",
        "core_API": "__init__"
    },
    {
        "commit_hash": "42458d10cc749dc9c33826211d6da532a2a57624",
        "index": "92548cb6..443a7084 100644",
        "commit_message": "Fix code form\n\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestHlsToRgb:",
            "[8., 8.]]])  # 3x2x2",
            "",
            "data = torch.tensor([[[0.0641, 0.07138],",
            "-                                  [0.07138, 0.07138]],",
            "+                                [0.07138, 0.07138]],",
            "",
            "-                                 [[0.0569, 0.0588],",
            "-                                  [0.0588, 0.0588]],",
            "+                                [[0.0569, 0.0588],",
            "+                                 [0.0588, 0.0588]],",
            "",
            "-                                 [[0.4483, 0.4667],",
            "-                                  [0.4667, 0.4667]]])  # 3x2x2",
            "+                                [[0.4483, 0.4667],",
            "+                                 [0.4667, 0.4667]]])  # 3x2x2",
            "",
            "f = kornia.color.HlsToRgb()",
            "data = data.repeat(2, 1, 1, 1)  # 2x3x2x2"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 5,
        "minus_line": 5,
        "AST_diff_line": 17,
        "number": 5349,
        "neg_line": [
            "-[0.07138, 0.07138]],",
            "-[[0.0569, 0.0588],",
            "-[0.0588, 0.0588]],",
            "-[[0.4483, 0.4667],",
            "-[0.4667, 0.4667]]])  # 3x2x2"
        ],
        "pos_line": [
            "+[0.07138, 0.07138]],",
            "+[[0.0569, 0.0588],",
            "+[0.0588, 0.0588]],",
            "+[[0.4483, 0.4667],",
            "+[0.4667, 0.4667]]])  # 3x2x2"
        ],
        "core_change": "-[0.07138, 0.07138]], +[0.07138, 0.07138]], -[[0.0569, 0.0588], -[0.0588, 0.0588]], +[[0.0569, 0.0588], +[0.0588, 0.0588]], -[[0.4483, 0.4667], -[0.4667, 0.4667]]])  # 3x2x2 +[[0.4483, 0.4667], +[0.4667, 0.4667]]])  # 3x2x2",
        "core_API": "tensor"
    },
    {
        "commit_hash": "cfdc3159ee495c0dec07ddf83cd33dcf3c5f9aef",
        "index": "30926cae25..21cfe887cc 100644",
        "commit_message": "fixing failing tests of miscellaneous_ops\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def diagonal(",
            "axis2: int = -1,",
            "out: Optional[Union[tf.Tensor, tf.Variable]] = None,",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "-    return tf.experimental.numpy.diagonal(x, offset, axis1=axis1, axis2=axis2)",
            "+    return tf.experimental.numpy.diagonal(x, offset=offset, axis1=axis1, axis2=axis2)",
            "",
            "",
            "@with_unsupported_dtypes({\"2.9.1 and below\": (\"float16\", \"bfloat16\")}, backend_version)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1971959)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=identifier, text=offset), position=0)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1971960)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'offset'), position=2, insert_id=1971961)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 5352,
        "neg_line": [
            "-return tf.experimental.numpy.diagonal(x, offset, axis1=axis1, axis2=axis2)"
        ],
        "pos_line": [
            "+return tf.experimental.numpy.diagonal(x, offset=offset, axis1=axis1, axis2=axis2)"
        ],
        "core_change": "-return tf.experimental.numpy.diagonal(x, offset, axis1=axis1, axis2=axis2) +return tf.experimental.numpy.diagonal(x, offset=offset, axis1=axis1, axis2=axis2)",
        "core_API": "diagonal"
    },
    {
        "commit_hash": "74171efadf5ea94416d26f1e2c27c1586a3a84b3",
        "index": "012e1486e..20b38c58a 100644",
        "commit_message": "drop duplicate metrics (#5014)\n\n* drop duplicate metrics\n\n* keep\n\n* fix\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def _explained_variance_compute(preds: torch.Tensor,",
            "return torch.sum(denominator / denom_sum * output_scores)",
            "",
            "",
            "-def explained_variance(preds: torch.Tensor,",
            "-                       target: torch.Tensor,",
            "-                       multioutput: str = 'uniform_average',",
            "-                       ) -> Union[torch.Tensor, Sequence[torch.Tensor]]:",
            "+def explained_variance(",
            "+        preds: torch.Tensor,",
            "+        target: torch.Tensor,",
            "+        multioutput: str = 'uniform_average',",
            "+) -> Union[torch.Tensor, Sequence[torch.Tensor]]:",
            "\"\"\"",
            "Computes explained variance."
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 17,
        "number": 5353,
        "neg_line": [
            "-def explained_variance(preds: torch.Tensor,",
            "-target: torch.Tensor,",
            "-multioutput: str = 'uniform_average',",
            "-) -> Union[torch.Tensor, Sequence[torch.Tensor]]:"
        ],
        "pos_line": [
            "+def explained_variance(",
            "+preds: torch.Tensor,",
            "+target: torch.Tensor,",
            "+multioutput: str = 'uniform_average',",
            "+) -> Union[torch.Tensor, Sequence[torch.Tensor]]:"
        ],
        "core_change": "-def explained_variance(preds: torch.Tensor, -target: torch.Tensor, -multioutput: str = 'uniform_average', -) -> Union[torch.Tensor, Sequence[torch.Tensor]]: +def explained_variance( +preds: torch.Tensor, +target: torch.Tensor, +multioutput: str = 'uniform_average', +) -> Union[torch.Tensor, Sequence[torch.Tensor]]:",
        "core_API": "sum"
    },
    {
        "commit_hash": "66d4b129903058365f6cf1d4fb4ac73814ca3bfc",
        "index": "5c55253a..8f52cc7d 100644",
        "commit_message": "Fix examples and tutorials to use the updated tensor API (#886)\n\n* Fix examples and tutorials to use the updated tensor API\n\n* small fixes\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "autograd",
        "change": [
            "class ConcatModule(nn.Module):",
            "input_args = input_args[0]",
            "",
            "# don't concat things that are just single objects",
            "-        if torch.is_tensor(input_args) or isinstance(input_args, torch.autograd.Variable):",
            "+        if torch.is_tensor(input_args):",
            "return input_args",
            "else:",
            "return torch.cat(input_args, dim=-1)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=if_statement), node=ASTNode(type=call), position=1)",
            "Delete(target_node=ASTNode(type=or, text=or))",
            "Delete(target_node=ASTNode(type=identifier, text=isinstance))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=input_args))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=autograd))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=Variable))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=boolean_operator))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 5355,
        "neg_line": [
            "-if torch.is_tensor(input_args) or isinstance(input_args, torch.autograd.Variable):"
        ],
        "pos_line": [
            "+if torch.is_tensor(input_args):"
        ],
        "core_change": "-if torch.is_tensor(input_args) or isinstance(input_args, torch.autograd.Variable): +if torch.is_tensor(input_args):",
        "core_API": "is_tensor"
    },
    {
        "commit_hash": "75e856be03827fcaa16d367aff42fe73c68d51bd",
        "index": "ae244f20..961e586a 100644",
        "commit_message": "py2.7 fix\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SplineGCN(Module):",
            "self.kernel_size = kernel_size",
            "self.spline_degree = spline_degree",
            "",
            "-        self.weight = Parameter(",
            "-            torch.Tensor(*kernel_size, in_features, out_features))",
            "+        weight_size = kernel_size + (in_features, out_features)",
            "+        self.weight = Parameter(torch.Tensor(*weight_size))",
            "",
            "if bias:",
            "self.bias = Parameter(torch.Tensor(out_features))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=1100309)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1100310)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'weight_size'), position=0, insert_id=1100311)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1100312)",
            "Insert(target_node=IN(type=assignment), node=('binary_operator', None), position=2, insert_id=1100313)",
            "Insert(target_node=IN(type=binary_operator), node=('identifier', 'kernel_size'), position=0, insert_id=1100314)",
            "Insert(target_node=IN(type=binary_operator), node=('+', '+'), position=1, insert_id=1100315)",
            "Insert(target_node=IN(type=binary_operator), node=('tuple', None), position=2, insert_id=1100316)",
            "Insert(target_node=IN(type=tuple), node=('(', '('), position=0, insert_id=1100317)",
            "Insert(target_node=IN(type=tuple), node=('identifier', 'in_features'), position=1, insert_id=1100318)",
            "Insert(target_node=IN(type=tuple), node=(',', ','), position=2, insert_id=1100319)",
            "Insert(target_node=IN(type=tuple), node=('identifier', 'out_features'), position=3, insert_id=1100320)",
            "Insert(target_node=IN(type=tuple), node=(')', ')'), position=4, insert_id=1100321)",
            "Update(target_node=ASTNode(type=identifier, text=kernel_size), value='weight_size')",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=in_features))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=out_features))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 18,
        "number": 5357,
        "neg_line": [
            "-self.weight = Parameter(",
            "-torch.Tensor(*kernel_size, in_features, out_features))"
        ],
        "pos_line": [
            "+weight_size = kernel_size + (in_features, out_features)",
            "+self.weight = Parameter(torch.Tensor(*weight_size))"
        ],
        "core_change": "-self.weight = Parameter( -torch.Tensor(*kernel_size, in_features, out_features)) +weight_size = kernel_size + (in_features, out_features) +self.weight = Parameter(torch.Tensor(*weight_size))",
        "core_API": "Tensor"
    },
    {
        "commit_hash": "0df55a139c1c7d11760995ee8ea3c3f95742e30e",
        "index": "7b20a34ac..f294b510d 100644",
        "commit_message": "[RLlib] Attention Net prep PR #1: Smaller cleanups. (#12447)\n\n* WIP.\n\n* Fix.\n\n* Fix.\n\n* Fix.\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TorchPolicy(Policy):",
            "",
            "all_grads = []",
            "for i, opt in enumerate(self._optimizers):",
            "+            # Erase gradients in all vars of this optimizer.",
            "opt.zero_grad()",
            "# Recompute gradients of loss over all variables.",
            "loss_out[i].backward(retain_graph=(i < len(self._optimizers) - 1))"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 17,
        "number": 5359,
        "neg_line": [],
        "pos_line": [
            "+# Erase gradients in all vars of this optimizer."
        ],
        "core_change": "+# Erase gradients in all vars of this optimizer.",
        "core_API": "zero_grad"
    },
    {
        "commit_hash": "813328682e2a6822ff8d0fde30a7ed9012449daf",
        "index": "945cd4eb6..e3058c4ca 100755",
        "commit_message": "[Flax] Example scripts - correct weight decay  (#12409)\n\n* fix_torch_device_generate_test\n\n* remove @\n\n* finish\n\n* finish\n\n* correct style\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "models",
        "change": [
            "if __name__ == \"__main__\":",
            "# to bias and LayerNorm scale parameters. decay_mask_fn returns a",
            "# mask boolean with the same structure as the parameters.",
            "# The mask is True for parameters that should be decayed.",
            "+    # Note that this mask is specifically adapted for FlaxBERT-like models.",
            "+    # For other models, one should correct the layer norm parameter naming",
            "+    # accordingly.",
            "def decay_mask_fn(params):",
            "flat_params = traverse_util.flatten_dict(params)",
            "flat_mask = {path: (path[-1] != \"bias\" and path[-2:] != (\"LayerNorm\", \"scale\")) for path in flat_params}"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 3,
        "minus_line": 0,
        "AST_diff_line": 17,
        "number": 5363,
        "neg_line": [],
        "pos_line": [
            "+# Note that this mask is specifically adapted for FlaxBERT-like models.",
            "+# For other models, one should correct the layer norm parameter naming",
            "+# accordingly."
        ],
        "core_change": "+# Note that this mask is specifically adapted for FlaxBERT-like models. +# For other models, one should correct the layer norm parameter naming +# accordingly.",
        "core_API": "flatten_dict"
    },
    {
        "commit_hash": "62e3d1dbdc88af9ed8afb37f7e8ecc011f5486a3",
        "index": "7870cce8..761cca08 100644",
        "commit_message": "Scope TokenIndexer output by indexer name (#3597)\n\n* Indexer tests are now passing, at least\n\n* Fixed some masking issues, and padding keys for one config file\n\n* TextFieldEmbedder tests pass\n\n* Fixing fixtures, some more tests passing\n\n* Fixed weird ordering bug\n\n* All TokenEmbedder tests passing?\n\n* Update fixtures\n\n* Fix more hard-coded references\n\n* fix field tests\n\n* fix dataset reader tests\n\n* Fix iterator tests\n\n* More tests passing\n\n* fix hotflip and some other tests\n\n* more tests\n\n* more test fixes\n\n* more tests\n\n* most tests passing; I think the remaining ones are spacy model changes\n\n* hard-code POS tag test\n\n* last test, I think\n\n* black\n\n* updated black\n\n* flake8\n\n* mypy\n\n* black again\n\n* fix training configs\n\n* remove reference to embedder_to_indexer_map\n\n* Other fixes from PR comments\n\n* fix breakage from incorrect merge during rebase\n\n* flake, some docstring formatting\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class BertForClassification(Model):",
            "loss : torch.FloatTensor, optional",
            "A scalar loss to be optimised.",
            "\"\"\"",
            "-        input_ids = tokens[self._index]",
            "-        token_type_ids = tokens[f\"{self._index}-type-ids\"]",
            "+        inputs = tokens[self._index]",
            "+        input_ids = inputs[\"input_ids\"]",
            "+        token_type_ids = inputs[\"token_type_ids\"]",
            "input_mask = (input_ids != 0).long()",
            "",
            "_, pooled = self.bert_model("
        ],
        "hunk_index": 2,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 3,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 5367,
        "neg_line": [
            "-input_ids = tokens[self._index]",
            "-token_type_ids = tokens[f\"{self._index}-type-ids\"]"
        ],
        "pos_line": [
            "+inputs = tokens[self._index]",
            "+input_ids = inputs[\"input_ids\"]",
            "+token_type_ids = inputs[\"token_type_ids\"]"
        ],
        "core_change": "-input_ids = tokens[self._index] -token_type_ids = tokens[f\"{self._index}-type-ids\"] +inputs = tokens[self._index] +input_ids = inputs[\"input_ids\"] +token_type_ids = inputs[\"token_type_ids\"]",
        "core_API": "bert_model"
    },
    {
        "commit_hash": "c470a34591c82906761dc79f9da773d5104e448a",
        "index": "f1e5949f..171c1e57 100644",
        "commit_message": "vpg/trpo/dqn tests, cat policy fix\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class CategoricalOneHotPolicy(StochasticPolicy):",
            "",
            "def __init__(self, network, session, state, random, action_count=1, scope='policy'):",
            "with tf.variable_scope(scope):",
            "-            action_layer = linear(layer_input=network.output, config={'num_outputs': action_count}, scope='outputs')",
            "-",
            "-            distribution = tf.nn.softmax(action_layer)",
            "-            sample = tf.map_fn(lambda t: tf.multinomial(logits=t, num_samples=1), elems=distribution, dtype=tf.int64)",
            "+            logits = linear(layer_input=network.output, config={'num_outputs': action_count}, scope='outputs')",
            "+            distribution = tf.nn.softmax(logits)",
            "+            sample = tf.map_fn(lambda t: tf.multinomial(logits=t, num_samples=1), elems=logits, dtype=tf.int64)",
            "",
            "super(CategoricalOneHotPolicy, self).__init__(network, [distribution, sample], session, state, random, action_count)",
            "self.dist = Categorical(random)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=action_layer), value='logits')",
            "Update(target_node=ASTNode(type=identifier, text=action_layer), value='logits')",
            "Update(target_node=ASTNode(type=identifier, text=distribution), value='logits')"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 3,
        "number": 5370,
        "neg_line": [
            "-action_layer = linear(layer_input=network.output, config={'num_outputs': action_count}, scope='outputs')",
            "-",
            "-distribution = tf.nn.softmax(action_layer)",
            "-sample = tf.map_fn(lambda t: tf.multinomial(logits=t, num_samples=1), elems=distribution, dtype=tf.int64)"
        ],
        "pos_line": [
            "+logits = linear(layer_input=network.output, config={'num_outputs': action_count}, scope='outputs')",
            "+distribution = tf.nn.softmax(logits)",
            "+sample = tf.map_fn(lambda t: tf.multinomial(logits=t, num_samples=1), elems=logits, dtype=tf.int64)"
        ],
        "core_change": "-action_layer = linear(layer_input=network.output, config={'num_outputs': action_count}, scope='outputs') - -distribution = tf.nn.softmax(action_layer) -sample = tf.map_fn(lambda t: tf.multinomial(logits=t, num_samples=1), elems=distribution, dtype=tf.int64) +logits = linear(layer_input=network.output, config={'num_outputs': action_count}, scope='outputs') +distribution = tf.nn.softmax(logits) +sample = tf.map_fn(lambda t: tf.multinomial(logits=t, num_samples=1), elems=logits, dtype=tf.int64)",
        "core_API": "variable_scope"
    },
    {
        "commit_hash": "d08bc09495a289519a774e881f399cf864178f9b",
        "index": "029f1d0f..f9987e92 100644",
        "commit_message": "Fix unused variable found (#1250)\n\n* Fix Unused variable found\n\n* more fixes\n\n* Update kornia/enhance/histogram.py\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def conv_quad_interp3d(",
            "raise ValueError(f\"Invalid input shape, we expect BxCxDxHxW. Got: {input.shape}\")",
            "",
            "B, CH, D, H, W = input.shape",
            "-    dev: torch.device = input.device",
            "grid_global: torch.Tensor = create_meshgrid3d(D, H, W, False, device=input.device).permute(0, 4, 1, 2, 3)",
            "grid_global = grid_global.to(input.dtype)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=dev))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=device))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=input))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=device))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 5371,
        "neg_line": [
            "-dev: torch.device = input.device"
        ],
        "pos_line": [],
        "core_change": "-dev: torch.device = input.device",
        "core_API": "to"
    },
    {
        "commit_hash": "e94abf66783f5e24ab37adc754ac902e827a4b4d",
        "index": "4e5d5614..8d52179f 100755",
        "commit_message": "fix TF deprecation of concat/split/pack/unpack\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Model(ModelDesc):",
            "b5 = branch('branch5', l, 16)",
            "",
            "final_map = Conv2D('convfcweight',",
            "-                           tf.concat(3, [b1, b2, b3, b4, b5]), 1, 1,",
            "+                           tf.concat_v2([b1, b2, b3, b4, b5], 3), 1, 1,",
            "W_init=tf.constant_initializer(0.2),",
            "use_bias=False, nl=tf.identity)",
            "costs = []"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=concat), value='concat_v2')",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=3, insert_id=2308268)",
            "Insert(target_node=ASTNode(type=argument_list), node=('integer', '3'), position=4, insert_id=2308269)",
            "Delete(target_node=ASTNode(type=integer, text=3))",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 5377,
        "neg_line": [
            "-tf.concat(3, [b1, b2, b3, b4, b5]), 1, 1,"
        ],
        "pos_line": [
            "+tf.concat_v2([b1, b2, b3, b4, b5], 3), 1, 1,"
        ],
        "core_change": "-tf.concat(3, [b1, b2, b3, b4, b5]), 1, 1, +tf.concat_v2([b1, b2, b3, b4, b5], 3), 1, 1,",
        "core_API": "concat"
    },
    {
        "commit_hash": "85149cdcd06e57eb2a6b001c9ba1d125383a926f",
        "index": "41591ac9d..844207784 100644",
        "commit_message": "fixed a bug in the case of multi-gpu\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Decoder(torch.nn.Module):",
            "if torch_is_old:",
            "vy = Variable(h.data.new(1).zero_().long(), volatile=True)",
            "else:",
            "-            torch.set_grad_enabled(False)",
            "vy = h.new_zeros(1).long()",
            "",
            "if recog_args.maxlenratio == 0:"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=identifier, text=else), position=0)",
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=:, text=:), position=1)",
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=type), position=2)",
            "Move(target_node=ASTNode(type=type), node=ASTNode(type=identifier, text=vy), position=0)",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=set_grad_enabled))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=false, text=False))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 15,
        "number": 5380,
        "neg_line": [
            "-torch.set_grad_enabled(False)"
        ],
        "pos_line": [],
        "core_change": "-torch.set_grad_enabled(False)",
        "core_API": "new"
    },
    {
        "commit_hash": "ed4e510628994a571ef7d7148287b74d236d227f",
        "index": "885d95a2..bf6b0e72 100644",
        "commit_message": "fix some bugs\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def start_train(config):",
            "raise",
            "finally:",
            "coord.request_stop()",
            "-            queue.close(cancel_pending_enqueues=True)",
            "+            input_queue.close(cancel_pending_enqueues=True)",
            "callbacks.after_train()",
            "sess.close()"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=queue), value='input_queue')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 5384,
        "neg_line": [
            "-queue.close(cancel_pending_enqueues=True)"
        ],
        "pos_line": [
            "+input_queue.close(cancel_pending_enqueues=True)"
        ],
        "core_change": "-queue.close(cancel_pending_enqueues=True) +input_queue.close(cancel_pending_enqueues=True)",
        "core_API": "request_stop"
    },
    {
        "commit_hash": "db33edfffc3a211dbb01fd96465a34e7407e255d",
        "index": "081b74a7c..f46ef21a0 100644",
        "commit_message": "Fixes tests still using old layout\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_recognition_results_with_lm(etype, m_str, text_idx1):",
            "",
            "",
            "@pytest.mark.parametrize((\"etype\", \"m_str\"), [",
            "-    (\"blstmp\", \"espnet.nets.e2e_asr\"),",
            "-    (\"blstmp\", \"espnet.nets.e2e_asr_th\"),",
            "-    (\"vggblstmp\", \"espnet.nets.e2e_asr\"),",
            "-    (\"vggblstmp\", \"espnet.nets.e2e_asr_th\"),",
            "+    (\"blstmp\", \"espnet.nets.chainer.e2e_asr\"),",
            "+    (\"blstmp\", \"espnet.nets.pytorch.e2e_asr_th\"),",
            "+    (\"vggblstmp\", \"espnet.nets.chainer.e2e_asr\"),",
            "+    (\"vggblstmp\", \"espnet.nets.pytorch.e2e_asr_th\"),",
            "])",
            "def test_batch_beam_search(etype, m_str):",
            "const = 1e-4"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=list), node=(',', ','), position=2, insert_id=178765)",
            "Insert(target_node=ASTNode(type=tuple), node=('(', '('), position=0, insert_id=178766)",
            "Insert(target_node=ASTNode(type=tuple), node=('string', '\"blstmp\"'), position=1, insert_id=178767)",
            "Update(target_node=ASTNode(type=string, text=\"espnet.nets.e2e_asr\"), value='\"espnet.nets.chainer.e2e_asr\"')",
            "Insert(target_node=ASTNode(type=tuple), node=(')', ')'), position=5, insert_id=178768)",
            "Move(target_node=ASTNode(type=tuple), node=ASTNode(type=string, text=\"blstmp\"), position=1)",
            "Update(target_node=ASTNode(type=string, text=\"espnet.nets.e2e_asr_th\"), value='\"espnet.nets.pytorch.e2e_asr_th\"')",
            "Insert(target_node=ASTNode(type=tuple), node=('string', '\"vggblstmp\"'), position=1, insert_id=178769)",
            "Update(target_node=ASTNode(type=string, text=\"espnet.nets.e2e_asr\"), value='\"espnet.nets.chainer.e2e_asr\"')",
            "Move(target_node=ASTNode(type=tuple), node=ASTNode(type=(, text=(), position=0)",
            "Move(target_node=ASTNode(type=tuple), node=ASTNode(type=string, text=\"vggblstmp\"), position=1)",
            "Move(target_node=ASTNode(type=tuple), node=ASTNode(type=,, text=,), position=2)",
            "Update(target_node=ASTNode(type=string, text=\"espnet.nets.e2e_asr_th\"), value='\"espnet.nets.pytorch.e2e_asr_th\"')",
            "Move(target_node=ASTNode(type=tuple), node=ASTNode(type=), text=)), position=5)",
            "Delete(target_node=ASTNode(type=string, text=\"blstmp\"))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=string, text=\"vggblstmp\"))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=), text=)))"
        ],
        "plus_line": 4,
        "minus_line": 4,
        "AST_diff_line": 19,
        "number": 5388,
        "neg_line": [
            "-(\"blstmp\", \"espnet.nets.e2e_asr\"),",
            "-(\"blstmp\", \"espnet.nets.e2e_asr_th\"),",
            "-(\"vggblstmp\", \"espnet.nets.e2e_asr\"),",
            "-(\"vggblstmp\", \"espnet.nets.e2e_asr_th\"),"
        ],
        "pos_line": [
            "+(\"blstmp\", \"espnet.nets.chainer.e2e_asr\"),",
            "+(\"blstmp\", \"espnet.nets.pytorch.e2e_asr_th\"),",
            "+(\"vggblstmp\", \"espnet.nets.chainer.e2e_asr\"),",
            "+(\"vggblstmp\", \"espnet.nets.pytorch.e2e_asr_th\"),"
        ],
        "core_change": "-(\"blstmp\", \"espnet.nets.e2e_asr\"), -(\"blstmp\", \"espnet.nets.e2e_asr_th\"), -(\"vggblstmp\", \"espnet.nets.e2e_asr\"), -(\"vggblstmp\", \"espnet.nets.e2e_asr_th\"), +(\"blstmp\", \"espnet.nets.chainer.e2e_asr\"), +(\"blstmp\", \"espnet.nets.pytorch.e2e_asr_th\"), +(\"vggblstmp\", \"espnet.nets.chainer.e2e_asr\"), +(\"vggblstmp\", \"espnet.nets.pytorch.e2e_asr_th\"),",
        "core_API": "parametrize"
    },
    {
        "commit_hash": "95b6d985ffb231d9a2047c039c95660913063b9d",
        "index": "1b075ee8..c9094e56 100644",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestBiMPMMatching(AllenNlpTestCase):",
            "mask1 = []",
            "for w in seq_len1:",
            "mask1.append([1] * w.item() + [0] * (len1 - w.item()))",
            "-        mask1 = torch.FloatTensor(mask1)",
            "+        mask1 = torch.BoolTensor(mask1)",
            "mask2 = []",
            "for w in seq_len2:",
            "mask2.append([1] * w.item() + [0] * (len2 - w.item()))",
            "-        mask2 = torch.FloatTensor(mask2)",
            "+        mask2 = torch.BoolTensor(mask2)",
            "",
            "d = 200  # hidden dimension",
            "n = 20  # number of perspective"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=FloatTensor), value='BoolTensor')",
            "Update(target_node=ASTNode(type=identifier, text=FloatTensor), value='BoolTensor')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 5390,
        "neg_line": [
            "-mask1 = torch.FloatTensor(mask1)",
            "-mask2 = torch.FloatTensor(mask2)"
        ],
        "pos_line": [
            "+mask1 = torch.BoolTensor(mask1)",
            "+mask2 = torch.BoolTensor(mask2)"
        ],
        "core_change": "-mask1 = torch.FloatTensor(mask1) +mask1 = torch.BoolTensor(mask1) -mask2 = torch.FloatTensor(mask2) +mask2 = torch.BoolTensor(mask2)",
        "core_API": "append"
    },
    {
        "commit_hash": "31331e6178d7541773a1262b72e86c715d089d6e",
        "index": "12e095a4..10572b22 100644",
        "commit_message": "Fix broken readthedocs (#1990)\n\n* override rtd settings\n\n* change torch version\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def setup(app):",
            "",
            "# @jpchen's hack to get rtd builder to install latest pytorch",
            "if 'READTHEDOCS' in os.environ:",
            "-    os.system('pip install https://download.pytorch.org/whl/cpu/torch-1.1.0-cp27-cp27mu-linux_x86_64.whl')",
            "+    os.system('pip install https://download.pytorch.org/whl/cpu/torch-1.1.0-cp36-cp36m-linux_x86_64.whl')"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text='pip install https://download.pytorch.org/whl/cpu/torch-1.1.0-cp27-cp27mu-linux_x86_64.whl'), value=\"'pip install https://download.pytorch.org/whl/cpu/torch-1.1.0-cp36-cp36m-linux_x86_64.whl'\")"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 5392,
        "neg_line": [
            "-os.system('pip install https://download.pytorch.org/whl/cpu/torch-1.1.0-cp27-cp27mu-linux_x86_64.whl')"
        ],
        "pos_line": [
            "+os.system('pip install https://download.pytorch.org/whl/cpu/torch-1.1.0-cp36-cp36m-linux_x86_64.whl')"
        ],
        "core_change": "-os.system('pip install https://download.pytorch.org/whl/cpu/torch-1.1.0-cp27-cp27mu-linux_x86_64.whl') +os.system('pip install https://download.pytorch.org/whl/cpu/torch-1.1.0-cp36-cp36m-linux_x86_64.whl')",
        "core_API": "system"
    },
    {
        "commit_hash": "3699d78a19102c1fbe6a6d3d6d4a936e4e0793ec",
        "index": "95569ed39..1c08019ef 100644",
        "commit_message": "added Winogrande debiased subset (#655)\n\n* added Winogrande debiased subset\n\n* fixed dymmy data\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class Winogrande(datasets.GeneratorBasedBuilder):",
            "# TODO(winogrande): Set up version.",
            "VERSION = datasets.Version(\"1.1.0\")",
            "BUILDER_CONFIGS = [",
            "-        WinograndeConfig(name=\"winogrande_\" + size, description=\"AI2 dataset\", data_size=size) for size in _SIZES",
            "+        WinograndeConfig(name=\"winogrande_\" + data_size, description=\"AI2 dataset\", data_size=data_size)",
            "+        for data_size in _FORMATS",
            "]",
            "",
            "def _info(self):"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=size), value='data_size')",
            "Update(target_node=ASTNode(type=identifier, text=_SIZES), value='_FORMATS')",
            "Update(target_node=ASTNode(type=identifier, text=size), value='data_size')",
            "Update(target_node=ASTNode(type=identifier, text=size), value='data_size')"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 5394,
        "neg_line": [
            "-WinograndeConfig(name=\"winogrande_\" + size, description=\"AI2 dataset\", data_size=size) for size in _SIZES"
        ],
        "pos_line": [
            "+WinograndeConfig(name=\"winogrande_\" + data_size, description=\"AI2 dataset\", data_size=data_size)",
            "+for data_size in _FORMATS"
        ],
        "core_change": "-WinograndeConfig(name=\"winogrande_\" + size, description=\"AI2 dataset\", data_size=size) for size in _SIZES +WinograndeConfig(name=\"winogrande_\" + data_size, description=\"AI2 dataset\", data_size=data_size) +for data_size in _FORMATS",
        "core_API": "Version"
    },
    {
        "commit_hash": "296fe9b4efe3674a6e1f5fe1b0fb629828c56a63",
        "index": "65497fd..71566cd 100644",
        "commit_message": "Fixed some of the model definitions\n",
        "file": "facenet.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def main(args):",
            "",
            "# Get input and output tensors",
            "images_placeholder = tf.get_default_graph().get_tensor_by_name(\"input:0\")",
            "+            phase_train_placeholder = tf.get_default_graph().get_tensor_by_name(\"phase_train:0\")",
            "embeddings = tf.get_default_graph().get_tensor_by_name(\"embeddings:0\")",
            "tpr, fpr, accuracy, val, val_std, far = lfw.validate(sess, paths,",
            "actual_issame, args.seed, 60,",
            "-                images_placeholder, embeddings, nrof_folds=args.lfw_nrof_folds)",
            "+                images_placeholder, phase_train_placeholder, embeddings, nrof_folds=args.lfw_nrof_folds)",
            "print('Accuracy: %1.3f+-%1.3f' % (np.mean(accuracy), np.std(accuracy)))",
            "print('Validation rate: %2.5f+-%2.5f @ FAR=%2.5f' % (val, val_std, far))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=2, insert_id=1929640)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1929641)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'phase_train_placeholder'), position=0, insert_id=1929642)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1929643)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=1929644)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1929645)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1929646)",
            "Insert(target_node=IN(type=attribute), node=('call', None), position=0, insert_id=1929647)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1929648)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'get_tensor_by_name'), position=2, insert_id=1929649)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1929650)",
            "Insert(target_node=IN(type=argument_list), node=('string', '\"phase_train:0\"'), position=1, insert_id=1929651)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1929652)",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'phase_train_placeholder'), position=13, insert_id=1929653)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=14, insert_id=1929654)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1929655)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1929656)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=1929657)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1929658)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'get_default_graph'), position=2, insert_id=1929659)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1929660)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1929661)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 22,
        "number": 5398,
        "neg_line": [
            "-images_placeholder, embeddings, nrof_folds=args.lfw_nrof_folds)"
        ],
        "pos_line": [
            "+phase_train_placeholder = tf.get_default_graph().get_tensor_by_name(\"phase_train:0\")",
            "+images_placeholder, phase_train_placeholder, embeddings, nrof_folds=args.lfw_nrof_folds)"
        ],
        "core_change": "+phase_train_placeholder = tf.get_default_graph().get_tensor_by_name(\"phase_train:0\") -images_placeholder, embeddings, nrof_folds=args.lfw_nrof_folds) +images_placeholder, phase_train_placeholder, embeddings, nrof_folds=args.lfw_nrof_folds)",
        "core_API": "get_default_graph"
    },
    {
        "commit_hash": "ab5d06a094ad32722018a069c2fb9707d1c6a8b1",
        "index": "cf26ae889..340ea49d8 100644",
        "commit_message": "[T5, examples] replace heavy t5 models with tiny random models (#3556)\n\n* replace heavy t5 models with tiny random models as was done by sshleifer\n\n* fix isort\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class TestT5Examples(unittest.TestCase):",
            "output_file_name = Path(tempfile.gettempdir()) / \"utest_output_t5_sum.hypo\"",
            "score_file_name = Path(tempfile.gettempdir()) / \"utest_score_t5_sum.hypo\"",
            "",
            "-        testargs = [\"evaluate_cnn.py\", \"t5-small\", str(tmp), str(output_file_name), str(tmp), str(score_file_name)]",
            "+        testargs = [",
            "+            \"evaluate_cnn.py\",",
            "+            \"patrickvonplaten/t5-tiny-random\",",
            "+            str(tmp),",
            "+            str(output_file_name),",
            "+            str(tmp),",
            "+            str(score_file_name),",
            "+        ]",
            "",
            "with patch.object(sys, \"argv\", testargs):",
            "run_generate()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=list), position=8)",
            "Update(target_node=ASTNode(type=string, text=\"t5-small\"), value='\"patrickvonplaten/t5-tiny-random\"')",
            "Insert(target_node=ASTNode(type=list), node=('call', None), position=5, insert_id=1545599)",
            "Insert(target_node=ASTNode(type=list), node=(',', ','), position=13, insert_id=1545600)",
            "Insert(target_node=IN(type=call), node=('identifier', 'str'), position=0, insert_id=1545601)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1545602)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1545603)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'tmp'), position=1, insert_id=1545604)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1545605)",
            "Delete(target_node=ASTNode(type=identifier, text=str))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=tmp))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 8,
        "minus_line": 1,
        "AST_diff_line": 15,
        "number": 5399,
        "neg_line": [
            "-testargs = [\"evaluate_cnn.py\", \"t5-small\", str(tmp), str(output_file_name), str(tmp), str(score_file_name)]"
        ],
        "pos_line": [
            "+testargs = [",
            "+\"evaluate_cnn.py\",",
            "+\"patrickvonplaten/t5-tiny-random\",",
            "+str(tmp),",
            "+str(output_file_name),",
            "+str(tmp),",
            "+str(score_file_name),",
            "+]"
        ],
        "core_change": "-testargs = [\"evaluate_cnn.py\", \"t5-small\", str(tmp), str(output_file_name), str(tmp), str(score_file_name)] +testargs = [ +\"evaluate_cnn.py\", +\"patrickvonplaten/t5-tiny-random\", +str(tmp), +str(output_file_name), +str(tmp), +str(score_file_name), +]",
        "core_API": "gettempdir"
    },
    {
        "commit_hash": "832c3b46f7e3383757dc505c1c406728e87e550f",
        "index": "f8223c9a1..5a64aca45 100644",
        "commit_message": "Bugfix test_parameter (missing clone()) and test_pointer (test previously commented on torch_1)\n\n",
        "file": "PySyft.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "def test_local_param_in_nn_module_linear():",
            "",
            "",
            "def test_remote_param_in_nn_module_linear(workers):",
            "-    model = nn.Linear(2, 1)",
            "+    model = nn.Linear(2, 1, bias=False)",
            "tensor = torch.tensor([1.0, -1.0])",
            "model_ptr = model.send(workers[\"bob\"])",
            "tensor_ptr = tensor.send(workers[\"bob\"])"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=1463604)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=1463605)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'bias'), position=0, insert_id=1463606)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1463607)",
            "Insert(target_node=IN(type=keyword_argument), node=('false', 'False'), position=2, insert_id=1463608)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 5401,
        "neg_line": [
            "-model = nn.Linear(2, 1)"
        ],
        "pos_line": [
            "+model = nn.Linear(2, 1, bias=False)"
        ],
        "core_change": "-model = nn.Linear(2, 1) +model = nn.Linear(2, 1, bias=False)",
        "core_API": "Linear"
    },
    {
        "commit_hash": "92ab7e56ec7f4a5f917cd2e28a2bdc9e2162f48a",
        "index": "c2f24367ca..f1a80d7499 100644",
        "commit_message": "[rllib] Fix PPO regression\n\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class LocalMultiGPUOptimizer(PolicyOptimizer):",
            "# all of the device copies are created.",
            "with self.local_evaluator.tf_sess.graph.as_default():",
            "with self.local_evaluator.tf_sess.as_default():",
            "-                main_scope = tf.get_variable_scope()",
            "-                with tf.variable_scope(main_scope, reuse=tf.AUTO_REUSE):",
            "+                with tf.variable_scope(\"default\", reuse=tf.AUTO_REUSE):",
            "self.par_opt = LocalSyncParallelOptimizer(",
            "tf.train.AdamOptimizer(self.sgd_stepsize),",
            "self.devices,"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('string', '\"default\"'), position=1, insert_id=2154521)",
            "Delete(target_node=ASTNode(type=identifier, text=main_scope))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=get_variable_scope))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=identifier, text=main_scope))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 14,
        "number": 5404,
        "neg_line": [
            "-main_scope = tf.get_variable_scope()",
            "-with tf.variable_scope(main_scope, reuse=tf.AUTO_REUSE):"
        ],
        "pos_line": [
            "+with tf.variable_scope(\"default\", reuse=tf.AUTO_REUSE):"
        ],
        "core_change": "-main_scope = tf.get_variable_scope() -with tf.variable_scope(main_scope, reuse=tf.AUTO_REUSE): +with tf.variable_scope(\"default\", reuse=tf.AUTO_REUSE):",
        "core_API": "as_default"
    },
    {
        "commit_hash": "dcd6487ea8612c56f44613fa6060f75eb20ab7d3",
        "index": "37e05a72..f7933c9f 100644",
        "commit_message": "More parser improvements (#934)\n\n* catch singleton words for labels, refactor span extractor internals\n\n* add ability to use embedded pos tags\n\n* use a text field embedder for pos tags\n\n* use pos tags in fixtures to test more complex case\n\n* update to use original Embedding\n\n* remove postag indexers\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SpanConstituencyParserTest(ModelTestCase):",
            "output_dict = self.model(**training_tensors)",
            "decode_output_dict = self.model.decode(output_dict)",
            "assert set(decode_output_dict.keys()) == {'spans', 'class_probabilities', 'trees',",
            "-                                                  'tokens', 'num_spans', 'loss'}",
            "+                                                  'tokens', 'pos_tags', 'num_spans', 'loss'}",
            "metrics = self.model.get_metrics(reset=True)",
            "metric_keys = set(metrics.keys())",
            "assert \"evalb_precision\" in metric_keys"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=,, text=,), node=ASTNode(type=set), position=7)",
            "Move(target_node=ASTNode(type=string, text='num_spans'), node=ASTNode(type=set), position=10)",
            "Insert(target_node=ASTNode(type=set), node=(',', ','), position=2, insert_id=38485)",
            "Insert(target_node=ASTNode(type=set), node=('string', \"'pos_tags'\"), position=4, insert_id=38486)",
            "Insert(target_node=ASTNode(type=set), node=(',', ','), position=12, insert_id=38487)",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 5405,
        "neg_line": [
            "-'tokens', 'num_spans', 'loss'}"
        ],
        "pos_line": [
            "+'tokens', 'pos_tags', 'num_spans', 'loss'}"
        ],
        "core_change": "-'tokens', 'num_spans', 'loss'} +'tokens', 'pos_tags', 'num_spans', 'loss'}",
        "core_API": "model"
    },
    {
        "commit_hash": "e38aa2e483bf167d0c8f00e17401d634a42b036b",
        "index": "f0eaf65670..1e055b51da 100644",
        "commit_message": "fix conv1d in backends\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def conv1d(",
            "dilations: int = 1,",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "if data_format == \"NCW\":",
            "-        x = tf.transpose(x, (0, 1, 2))",
            "+        x = tf.transpose(x, (0, 2, 1))",
            "res = tf.nn.conv1d(x, filters, strides, padding, \"NWC\", dilations)",
            "if data_format == \"NCW\":",
            "-        res = tf.transpose(res, (0, 1, 2))",
            "+        res = tf.transpose(res, (0, 2, 1))",
            "return res"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=if_statement), node=('comparison_operator', None), position=1, insert_id=2006618)",
            "Move(target_node=ASTNode(type=if_statement), node=ASTNode(type=comparison_operator), position=1)",
            "Insert(target_node=IN(type=comparison_operator), node=('identifier', 'data_format'), position=0, insert_id=2006619)",
            "Insert(target_node=IN(type=comparison_operator), node=('==', '=='), position=1, insert_id=2006620)",
            "Insert(target_node=IN(type=comparison_operator), node=('string', '\"NCW\"'), position=2, insert_id=2006621)",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2006622)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2006623)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2006624)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'transpose'), position=2, insert_id=2006625)",
            "Update(target_node=ASTNode(type=integer, text=1), value='2')",
            "Update(target_node=ASTNode(type=integer, text=2), value='1')",
            "Update(target_node=ASTNode(type=integer, text=1), value='2')",
            "Update(target_node=ASTNode(type=integer, text=2), value='1')",
            "Delete(target_node=ASTNode(type=identifier, text=data_format))",
            "Delete(target_node=ASTNode(type===, text===))",
            "Delete(target_node=ASTNode(type=string, text=\"NCW\"))",
            "Delete(target_node=ASTNode(type=comparison_operator))",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=transpose))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 22,
        "number": 5408,
        "neg_line": [
            "-x = tf.transpose(x, (0, 1, 2))",
            "-res = tf.transpose(res, (0, 1, 2))"
        ],
        "pos_line": [
            "+x = tf.transpose(x, (0, 2, 1))",
            "+res = tf.transpose(res, (0, 2, 1))"
        ],
        "core_change": "-x = tf.transpose(x, (0, 1, 2)) +x = tf.transpose(x, (0, 2, 1)) -res = tf.transpose(res, (0, 1, 2)) +res = tf.transpose(res, (0, 2, 1))",
        "core_API": "transpose"
    },
    {
        "commit_hash": "a7a5c7b42cb485ab3817afce3c5400bbe22ee4d0",
        "index": "507d998810..73896a3aa2 100644",
        "commit_message": "Type promotion fixes (#2620)\n\n* casting fixes\n\n* lint fixes\n\n* changes\n\n* more changes\n\n* lint fixes\n\n* Fixed matrix_Rank\n\n* update matrix_rank return dtype\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def matrix_rank(",
            ") -> torch.Tensor:",
            "# ToDo: add support for default rtol value here, for the case where None is provided",
            "ret = torch.linalg.matrix_rank(x, rtol=rtol, out=out)",
            "-    ret = torch.tensor(ret, dtype=ivy.default_float_dtype(as_native=True))",
            "+    ret = torch.tensor(ret, dtype=ivy.default_int_dtype(as_native=True))",
            "return ret"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=default_float_dtype), value='default_int_dtype')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 5409,
        "neg_line": [
            "-ret = torch.tensor(ret, dtype=ivy.default_float_dtype(as_native=True))"
        ],
        "pos_line": [
            "+ret = torch.tensor(ret, dtype=ivy.default_int_dtype(as_native=True))"
        ],
        "core_change": "-ret = torch.tensor(ret, dtype=ivy.default_float_dtype(as_native=True)) +ret = torch.tensor(ret, dtype=ivy.default_int_dtype(as_native=True))",
        "core_API": "matrix_rank"
    },
    {
        "commit_hash": "854f604719b6a571e47ad3ca8b758cec4133d2f7",
        "index": "95824013..e579605e 100644",
        "commit_message": "[Enhance] Added better augmentation support for backpropagation (#826)\n\n* Avoid breaking gradients with a new tensor\n\n* Moved param init into generators\n\n* Removed deprecation warnings.\n\n* Fixed 3D ones\n\n* Fixed device / dtype error\n\n* Fixed mypy\n\n* Fixed sharpness bug\n\n* Added backward tests\n\n* Added more backward tests & Make motion blur angle/direction differentiable\n\n* Fixed RandomResizedCrop\n\n* Bug fixed & Added sharpness tests.\n\n* Updated 3D augmentations\n\n* Added RandomAffine tests\n\n* Added 3D augmentation backpropagation tsets\n\n* Updated 3D tests\n\n* Fixed broken tests\n\n* Fixed some bugs\n\n* Fixed typo\n\n* Fixed broken 3D tests\n\n* cuda fix\n\n* Skipped nearest mode under cuda devices.\n\n* Fixed linting\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def bbox_to_mask(boxes: torch.Tensor, width: int, height: int) -> torch.Tensor:",
            "m = m.index_fill(0, torch.arange(box[1, 1].item(), box[2, 1].item() + 1, dtype=torch.long), torch.tensor(1))",
            "m = m.unsqueeze(dim=0)",
            "m_out = (m == 1).all(dim=1) * (m == 1).all(dim=2).T",
            "+        m_out = m_out[1:-1, 1:-1]",
            "mask_out.append(m_out)",
            "",
            "return torch.stack(mask_out, dim=0).float()"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=4, insert_id=431923)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=431924)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'm_out'), position=0, insert_id=431925)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=431926)",
            "Insert(target_node=IN(type=assignment), node=('subscript', None), position=2, insert_id=431927)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'm_out'), position=0, insert_id=431928)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=431929)",
            "Insert(target_node=IN(type=subscript), node=('slice', None), position=2, insert_id=431930)",
            "Insert(target_node=IN(type=subscript), node=(',', ','), position=3, insert_id=431931)",
            "Insert(target_node=IN(type=subscript), node=('slice', None), position=4, insert_id=431932)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=5, insert_id=431933)",
            "Insert(target_node=IN(type=slice), node=('integer', '1'), position=0, insert_id=431934)",
            "Insert(target_node=IN(type=slice), node=(':', ':'), position=1, insert_id=431935)",
            "Insert(target_node=IN(type=slice), node=('unary_operator', '-1'), position=2, insert_id=431936)",
            "Insert(target_node=IN(type=slice), node=('integer', '1'), position=0, insert_id=431937)",
            "Insert(target_node=IN(type=slice), node=(':', ':'), position=1, insert_id=431938)",
            "Insert(target_node=IN(type=slice), node=('unary_operator', '-1'), position=2, insert_id=431939)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 17,
        "number": 5417,
        "neg_line": [],
        "pos_line": [
            "+m_out = m_out[1:-1, 1:-1]"
        ],
        "core_change": "+m_out = m_out[1:-1, 1:-1]",
        "core_API": "index_fill"
    },
    {
        "commit_hash": "b4513f63107bb54bef694de2612260f72e8b0eaf",
        "index": "7dcf4144..399f512b 100644",
        "commit_message": "fix softmax dim of Residual MoE in moe/layer.py (#2110)\n\nThanks a lot for finding this issue and fixed it :)\nCo-authored-by: Zhewei Yao <zheweiyao@gmail.com>\n",
        "file": "DeepSpeed.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "functional",
        "change": [
            "class MoE(torch.nn.Module):",
            "if type(output_mlp) is tuple:",
            "output_mlp = output_mlp[0]  # Ignore the bias term for now",
            "coef = self.coefficient(hidden_states)",
            "-            coef = torch.nn.functional.softmax(coef, dim=1)",
            "+            coef = torch.nn.functional.softmax(coef, dim=-1)",
            "output = output * coef[..., 0:1] + output_mlp * coef[..., 1:]",
            "return output, self.deepspeed_moe.l_aux, self.deepspeed_moe.exp_counts"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=keyword_argument), node=('unary_operator', '-1'), position=2, insert_id=1624340)",
            "Delete(target_node=ASTNode(type=integer, text=1))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 5418,
        "neg_line": [
            "-coef = torch.nn.functional.softmax(coef, dim=1)"
        ],
        "pos_line": [
            "+coef = torch.nn.functional.softmax(coef, dim=-1)"
        ],
        "core_change": "-coef = torch.nn.functional.softmax(coef, dim=1) +coef = torch.nn.functional.softmax(coef, dim=-1)",
        "core_API": "coefficient"
    },
    {
        "commit_hash": "32e85eda58c560e5d5a596f71fce3682ac2ef80a",
        "index": "c792d2a6..55674b54 100755",
        "commit_message": "[see_memory_usage] fix deprecation (#1234)\n\nCo-authored-by: Olatunji Ruwase <olruwase@microsoft.com>\n",
        "file": "DeepSpeed.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def see_memory_usage(message, force=False):",
            "logger.info(",
            "f\"MA {round(torch.cuda.memory_allocated() / (1024 * 1024 * 1024),2 )} GB \\",
            "Max_MA {round(torch.cuda.max_memory_allocated() / (1024 * 1024 * 1024),2)} GB \\",
            "-        CA {round(torch.cuda.memory_cached() / (1024 * 1024 * 1024),2)} GB \\",
            "-        Max_CA {round(torch.cuda.max_memory_cached() / (1024 * 1024 * 1024))} GB \")",
            "+        CA {round(torch_memory_reserved() / (1024 * 1024 * 1024),2)} GB \\",
            "+        Max_CA {round(torch_max_memory_reserved() / (1024 * 1024 * 1024))} GB \")",
            "",
            "vm_stats = psutil.virtual_memory()",
            "used_GB = round(((vm_stats.total - vm_stats.available) / (1024**3)), 2)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=f\"MA {round(torch.cuda.memory_allocated() / (1024 * 1024 * 1024),2 )} GB \\\nMax_MA {round(torch.cuda.max_memory_allocated() / (1024 * 1024 * 1024),2)} GB \\\n        CA {round(torch.cuda.memory_cached() / (1024 * 1024 * 1024),2)} GB \\\n        Max_CA {round(torch.cuda.max_memory_cached() / (1024 * 1024 * 1024))} GB \"), value='f\"MA {round(torch.cuda.memory_allocated() / (1024 * 1024 * 1024),2 )} GB \\\\\\nMax_MA {round(torch.cuda.max_memory_allocated() / (1024 * 1024 * 1024),2)} GB \\\\\\n        CA {round(torch_memory_reserved() / (1024 * 1024 * 1024),2)} GB \\\\\\n        Max_CA {round(torch_max_memory_reserved() / (1024 * 1024 * 1024))} GB \"')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 1,
        "number": 5420,
        "neg_line": [
            "-CA {round(torch.cuda.memory_cached() / (1024 * 1024 * 1024),2)} GB \\",
            "-Max_CA {round(torch.cuda.max_memory_cached() / (1024 * 1024 * 1024))} GB \")"
        ],
        "pos_line": [
            "+CA {round(torch_memory_reserved() / (1024 * 1024 * 1024),2)} GB \\",
            "+Max_CA {round(torch_max_memory_reserved() / (1024 * 1024 * 1024))} GB \")"
        ],
        "core_change": "-CA {round(torch.cuda.memory_cached() / (1024 * 1024 * 1024),2)} GB \\ -Max_CA {round(torch.cuda.max_memory_cached() / (1024 * 1024 * 1024))} GB \") +CA {round(torch_memory_reserved() / (1024 * 1024 * 1024),2)} GB \\ +Max_CA {round(torch_max_memory_reserved() / (1024 * 1024 * 1024))} GB \")",
        "core_API": "info"
    },
    {
        "commit_hash": "0d823530ccaf87f7933855e610c8d1c6ea046490",
        "index": "99926220..5e9a6e7f 100644",
        "commit_message": "Fix support for dynamic schedules.\n\nPiperOrigin-RevId: 330394792\n\n",
        "file": "tf-quant-finance.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class DateTensor(tensor_wrapper.TensorWrapper):",
            "",
            "if period_type == constants.PeriodType.YEAR:",
            "y = self._years + period_tensor.quantity()",
            "-      m = tf.broadcast_to(self._months, y.shape)",
            "+      # Use tf.shape to handle the case of dynamically shaped `y`",
            "+      m = tf.broadcast_to(self._months, tf.shape(y))",
            "d = adjust_day(y, m, self._days)",
            "return from_year_month_day(y, m, d, validate=False)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=3, insert_id=2342305)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=4, insert_id=2342306)",
            "Move(target_node=IN(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2342307)",
            "Update(target_node=ASTNode(type=identifier, text=y), value='tf')",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2342308)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'y'), position=1, insert_id=2342309)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=), text=)), position=2)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 5422,
        "neg_line": [
            "-m = tf.broadcast_to(self._months, y.shape)"
        ],
        "pos_line": [
            "+# Use tf.shape to handle the case of dynamically shaped `y`",
            "+m = tf.broadcast_to(self._months, tf.shape(y))"
        ],
        "core_change": "-m = tf.broadcast_to(self._months, y.shape) +# Use tf.shape to handle the case of dynamically shaped `y` +m = tf.broadcast_to(self._months, tf.shape(y))",
        "core_API": "quantity"
    },
    {
        "commit_hash": "35b52001fffbe09033f42f74acdc91cb2c489333",
        "index": "99ed4196..5c83ec50 100644",
        "commit_message": "Gh-109: Fix bug in single label method.\n\n",
        "file": "flair.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TextClassifier(nn.Module):",
            "return labels",
            "",
            "def _get_single_label(self, label_scores) -> List[Label]:",
            "-        conf, idx = torch.max(label_scores[0], 0)",
            "+        conf, idx = torch.max(label_scores, 0)",
            "label = self.label_dictionary.get_item_for_index(idx.item())",
            "",
            "return [Label(label, conf.item())]"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=label_scores), position=1)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=integer, text=0), position=4)",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))",
            "Delete(target_node=ASTNode(type=integer, text=0))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 5424,
        "neg_line": [
            "-conf, idx = torch.max(label_scores[0], 0)"
        ],
        "pos_line": [
            "+conf, idx = torch.max(label_scores, 0)"
        ],
        "core_change": "-conf, idx = torch.max(label_scores[0], 0) +conf, idx = torch.max(label_scores, 0)",
        "core_API": "max"
    },
    {
        "commit_hash": "3306db01ff9efd80c71842ff2bd12d641d5ae531",
        "index": "ef618acf8..f6b5533db 100644",
        "commit_message": "Fix deprecated warning message and docstring (#2100)\n\n* Use deprecated Sphinx directive in docstrings\n\n* Fix deprecation message format\n\n* Use warnings.warn instead of logger.warning for deprecation\n\n* Remove trailing blank in deprecated warning message\n\n* Fix deprecated in dictionary_encode_column_\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "def deprecated(help_message: Optional[str] = None):",
            "def wrapper(*args, **kwargs):",
            "func_hash = hash(deprecated_function)",
            "if func_hash not in _emitted_deprecation_warnings:",
            "-                logger.warning(warning_msg)",
            "+                warnings.warn(warning_msg, category=FutureWarning, stacklevel=2)",
            "_emitted_deprecation_warnings.add(func_hash)",
            "return deprecated_function(*args, **kwargs)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=logger), value='warnings')",
            "Update(target_node=ASTNode(type=identifier, text=warning), value='warn')",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1786101)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1786102)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=1786103)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=1786104)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'category'), position=0, insert_id=1786105)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1786106)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'FutureWarning'), position=2, insert_id=1786107)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'stacklevel'), position=0, insert_id=1786108)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1786109)",
            "Insert(target_node=IN(type=keyword_argument), node=('integer', '2'), position=2, insert_id=1786110)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 12,
        "number": 5425,
        "neg_line": [
            "-logger.warning(warning_msg)"
        ],
        "pos_line": [
            "+warnings.warn(warning_msg, category=FutureWarning, stacklevel=2)"
        ],
        "core_change": "-logger.warning(warning_msg) +warnings.warn(warning_msg, category=FutureWarning, stacklevel=2)",
        "core_API": "warning"
    },
    {
        "commit_hash": "d079b37c3ec18bfb82128969958c9133c5d12f44",
        "index": "6cd0453a..28bd834d 100644",
        "commit_message": "fix #806\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def generate_rpn_proposals(boxes, scores, img_shape,",
            "(-1, 4), name='nms_input_boxes')",
            "nms_indices = tf.image.non_max_suppression(",
            "topk_valid_boxes_y1x1y2x2,",
            "-        # TODO use exp to work around a bug in TF1.9: https://github.com/tensorflow/tensorflow/issues/19578",
            "-        tf.exp(topk_valid_scores),",
            "+        topk_valid_scores,",
            "max_output_size=post_nms_topk,",
            "iou_threshold=cfg.RPN.PROPOSAL_NMS_THRESH)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=parameters), node=ASTNode(type=identifier, text=topk_valid_scores), position=3)",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=exp))",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=tuple_pattern))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 8,
        "number": 5426,
        "neg_line": [
            "-# TODO use exp to work around a bug in TF1.9: https://github.com/tensorflow/tensorflow/issues/19578",
            "-tf.exp(topk_valid_scores),"
        ],
        "pos_line": [
            "+topk_valid_scores,"
        ],
        "core_change": "-# TODO use exp to work around a bug in TF1.9: https://github.com/tensorflow/tensorflow/issues/19578 -tf.exp(topk_valid_scores), +topk_valid_scores,",
        "core_API": "non_max_suppression"
    },
    {
        "commit_hash": "c4c532d25e012dbe6ab1ac14bca75e53e0acc621",
        "index": "01c19744..603c386d 100644",
        "commit_message": "pylint -> flake8 (#3288)\n\n* pylint\n\n* update pylint\n\n* undo a lot of the raise / else\n\n* add bound on typed-ast\n\n* first mypy fixes\n\n* new flag\n\n* fix mypy errors\n\n* requirements.txt\n\n* pylint -> flake8\n\n* mypy 0.720 -> mypy 0.730\n\n* add back erroneously removed initial newline\n\n* remove .pylintrc\n\n* remove pylintrc from Dockerfile\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestExpectedRiskMinimization(AllenNlpTestCase):",
            "self.initial_state = SimpleState([0], [[0]], [torch.Tensor([0.0])])",
            "self.decoder_step = SimpleTransitionFunction()",
            "# Cost is the number of odd elements in the action history.",
            "-        self.supervision = lambda state: torch.Tensor([sum([x%2 != 0 for x in",
            "+        self.supervision = lambda state: torch.Tensor([sum([x % 2 != 0 for x in",
            "state.action_history[0]])])",
            "# High beam size ensures exhaustive search.",
            "self.trainer = ExpectedRiskMinimization(beam_size=100,"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 5427,
        "neg_line": [
            "-self.supervision = lambda state: torch.Tensor([sum([x%2 != 0 for x in"
        ],
        "pos_line": [
            "+self.supervision = lambda state: torch.Tensor([sum([x % 2 != 0 for x in"
        ],
        "core_change": "-self.supervision = lambda state: torch.Tensor([sum([x%2 != 0 for x in +self.supervision = lambda state: torch.Tensor([sum([x % 2 != 0 for x in",
        "core_API": "Tensor"
    },
    {
        "commit_hash": "d523a6dda13dce9bb84a45afb5105f96a1340220",
        "index": "4c467371..d7afac45 100644",
        "commit_message": "fix is tensor\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Model(object):",
            "\"It should be either Tensor or a list of Tensor.\"",
            ")",
            "for idx in range(len(check_argu)):",
            "-                        if not isinstance(check_argu[idx], [tf.Tensor, tf.SparseTensor, tf.Variable]) or not tf_ops.is_dense_tensor_like(",
            "+                        if not isinstance(check_argu[idx], (tf.Tensor, tf.SparseTensor, tf.Variable)) or not tf_ops.is_dense_tensor_like(",
            "check_argu[idx]):",
            "raise TypeError(",
            "\"The argument `%s` should be either Tensor or a list of Tensor \" % (check_order[co]) +"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('tuple', None), position=3, insert_id=2249311)",
            "Insert(target_node=IN(type=tuple), node=('(', '('), position=0, insert_id=2249312)",
            "Move(target_node=IN(type=tuple), node=ASTNode(type=attribute), position=1)",
            "Move(target_node=IN(type=tuple), node=ASTNode(type=,, text=,), position=2)",
            "Move(target_node=IN(type=tuple), node=ASTNode(type=attribute), position=3)",
            "Move(target_node=IN(type=tuple), node=ASTNode(type=,, text=,), position=4)",
            "Move(target_node=IN(type=tuple), node=ASTNode(type=attribute), position=5)",
            "Insert(target_node=IN(type=tuple), node=(')', ')'), position=6, insert_id=2249313)",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=list))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 5428,
        "neg_line": [
            "-if not isinstance(check_argu[idx], [tf.Tensor, tf.SparseTensor, tf.Variable]) or not tf_ops.is_dense_tensor_like("
        ],
        "pos_line": [
            "+if not isinstance(check_argu[idx], (tf.Tensor, tf.SparseTensor, tf.Variable)) or not tf_ops.is_dense_tensor_like("
        ],
        "core_change": "-if not isinstance(check_argu[idx], [tf.Tensor, tf.SparseTensor, tf.Variable]) or not tf_ops.is_dense_tensor_like( +if not isinstance(check_argu[idx], (tf.Tensor, tf.SparseTensor, tf.Variable)) or not tf_ops.is_dense_tensor_like(",
        "core_API": "is_dense_tensor_like"
    },
    {
        "commit_hash": "149a703884ee126d20a9df187abe9176bb4a581d",
        "index": "73f4c27..e188c6e 100755",
        "commit_message": "add compat.is_gpu_available(); fix test failure\n\n",
        "file": "TensorFlowOnSpark.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def run(fn, tf_args, cluster_meta, tensorboard, log_dir, queues, background):",
            "os.environ['TF_CONFIG'] = tf_config",
            "",
            "# reserve GPU(s) again, just before launching TF process (in case situation has changed)",
            "-    if tf.test.is_built_with_cuda():",
            "+    if compat.is_gpu_available():",
            "# compute my index relative to other nodes on the same host (for GPU allocation)",
            "my_addr = cluster_spec[job_name][task_index]",
            "my_host = my_addr.split(':')[0]"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tf), value='compat')",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=tf), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=test), value='is_gpu_available')",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=test), position=3)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=identifier, text=is_built_with_cuda))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 5434,
        "neg_line": [
            "-if tf.test.is_built_with_cuda():"
        ],
        "pos_line": [
            "+if compat.is_gpu_available():"
        ],
        "core_change": "-if tf.test.is_built_with_cuda(): +if compat.is_gpu_available():",
        "core_API": "is_built_with_cuda"
    },
    {
        "commit_hash": "dd7accb9ccf2eee25d1a8d2285f83b7e5e8d04c2",
        "index": "1fab185..731eb16 100644",
        "commit_message": "decay LR, little bugfix\n\n",
        "file": "tinygrad.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "optim",
        "change": [
            "def train_cifar():",
            "optimizer = optim.Adam(get_parameters(model), lr=3e-4)",
            "else:",
            "#optimizer = optim.SGD(get_parameters(model), lr=0.001)",
            "-    optimizer = optim.SGD(get_parameters(model), lr=0.003, momentum=0.85, nesterov=True)",
            "+    optimizer = optim.SGD(get_parameters(model), lr=Tensor([0.003]).realize(), momentum=0.85, nesterov=True)",
            "",
            "# 97 steps in 2 seconds = 20ms / step",
            "# step is 1163.42 GOPS = 56 TFLOPS!!!, 41% of max 136"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=keyword_argument), node=('call', None), position=2, insert_id=1617417)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1617418)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1617419)",
            "Insert(target_node=IN(type=attribute), node=('call', None), position=0, insert_id=1617420)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1617421)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'realize'), position=2, insert_id=1617422)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1617423)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1617424)",
            "Insert(target_node=IN(type=call), node=('identifier', 'Tensor'), position=0, insert_id=1617425)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1617426)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1617427)",
            "Insert(target_node=IN(type=argument_list), node=('list', None), position=1, insert_id=1617428)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1617429)",
            "Insert(target_node=IN(type=list), node=('[', '['), position=0, insert_id=1617430)",
            "Move(target_node=IN(type=list), node=ASTNode(type=float, text=0.003), position=1)",
            "Insert(target_node=IN(type=list), node=(']', ']'), position=2, insert_id=1617431)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 16,
        "number": 5438,
        "neg_line": [
            "-optimizer = optim.SGD(get_parameters(model), lr=0.003, momentum=0.85, nesterov=True)"
        ],
        "pos_line": [
            "+optimizer = optim.SGD(get_parameters(model), lr=Tensor([0.003]).realize(), momentum=0.85, nesterov=True)"
        ],
        "core_change": "-optimizer = optim.SGD(get_parameters(model), lr=0.003, momentum=0.85, nesterov=True) +optimizer = optim.SGD(get_parameters(model), lr=Tensor([0.003]).realize(), momentum=0.85, nesterov=True)",
        "core_API": "Adam"
    },
    {
        "commit_hash": "6c8e2bab4d45f2386929c83bb4480c18d2b660fd",
        "index": "4467e32..96d1feb 100644",
        "commit_message": "fix warnings and failures\n\n",
        "file": "examples.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "for epoch in range(opt.niter):",
            "netD.zero_grad()",
            "real_cpu = data[0].to(device)",
            "batch_size = real_cpu.size(0)",
            "-        label = torch.full((batch_size,), real_label, device=device)",
            "+        label = torch.full((batch_size,), real_label,",
            "+                           dtype=real_cpu.dtype, device=device)",
            "",
            "output = netD(real_cpu)",
            "errD_real = criterion(output, label)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=189855)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=6, insert_id=189856)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=189857)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=189858)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=189859)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'real_cpu'), position=0, insert_id=189860)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=189861)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dtype'), position=2, insert_id=189862)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 5440,
        "neg_line": [
            "-label = torch.full((batch_size,), real_label, device=device)"
        ],
        "pos_line": [
            "+label = torch.full((batch_size,), real_label,",
            "+dtype=real_cpu.dtype, device=device)"
        ],
        "core_change": "-label = torch.full((batch_size,), real_label, device=device) +label = torch.full((batch_size,), real_label, +dtype=real_cpu.dtype, device=device)",
        "core_API": "zero_grad"
    },
    {
        "commit_hash": "b41cffaa93e8205bd8bd309f82c33c07c420eefd",
        "index": "a73047c..3b6f90a 100644",
        "commit_message": "Fix a few issues loading pretrained vit/bit npz weights w/ num_classes=0 __init__ arg. Missed a few other small classifier handling detail on Mlp, GhostNet, Levit. Should fix #713\n\n",
        "file": "pytorch-image-models.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class GhostNet(nn.Module):",
            "self.conv_head = nn.Conv2d(prev_chs, out_chs, 1, 1, 0, bias=True)",
            "self.act2 = nn.ReLU(inplace=True)",
            "self.flatten = nn.Flatten(1) if global_pool else nn.Identity()  # don't flatten if pooling disabled",
            "-        self.classifier = Linear(out_chs, num_classes)",
            "+        self.classifier = Linear(out_chs, num_classes) if num_classes > 0 else nn.Identity()",
            "",
            "def get_classifier(self):",
            "return self.classifier"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('conditional_expression', None), position=2, insert_id=1477783)",
            "Move(target_node=IN(type=conditional_expression), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=conditional_expression), node=('if', 'if'), position=1, insert_id=1477784)",
            "Insert(target_node=IN(type=conditional_expression), node=('comparison_operator', None), position=2, insert_id=1477785)",
            "Insert(target_node=IN(type=conditional_expression), node=('else', 'else'), position=3, insert_id=1477786)",
            "Insert(target_node=IN(type=conditional_expression), node=('call', None), position=4, insert_id=1477787)",
            "Insert(target_node=IN(type=comparison_operator), node=('identifier', 'num_classes'), position=0, insert_id=1477788)",
            "Insert(target_node=IN(type=comparison_operator), node=('>', '>'), position=1, insert_id=1477789)",
            "Insert(target_node=IN(type=comparison_operator), node=('integer', '0'), position=2, insert_id=1477790)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1477791)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1477792)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'nn'), position=0, insert_id=1477793)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1477794)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Identity'), position=2, insert_id=1477795)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1477796)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1477797)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 16,
        "number": 5443,
        "neg_line": [
            "-self.classifier = Linear(out_chs, num_classes)"
        ],
        "pos_line": [
            "+self.classifier = Linear(out_chs, num_classes) if num_classes > 0 else nn.Identity()"
        ],
        "core_change": "-self.classifier = Linear(out_chs, num_classes) +self.classifier = Linear(out_chs, num_classes) if num_classes > 0 else nn.Identity()",
        "core_API": "Conv2d"
    },
    {
        "commit_hash": "d08bc09495a289519a774e881f399cf864178f9b",
        "index": "9f4694d2..556f8dfe 100644",
        "commit_message": "Fix unused variable found (#1250)\n\n* Fix Unused variable found\n\n* more fixes\n\n* Update kornia/enhance/histogram.py\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def DepthWarperApp():",
            "# load the data",
            "root_dir = os.path.join(root_path, 'training')",
            "img_ref, depth_ref, cam_ref = load_data(root_dir, args.sequence_name, args.frame_ref_id)",
            "-    img_i, depth_i, cam_i = load_data(root_dir, args.sequence_name, args.frame_i_id)",
            "+    img_i, _, cam_i = load_data(root_dir, args.sequence_name, args.frame_i_id)",
            "",
            "# instantiate the homography warper from `kornia`",
            "warper = dgm.DepthWarper(cam_i)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=depth_i), value='_')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 5445,
        "neg_line": [
            "-img_i, depth_i, cam_i = load_data(root_dir, args.sequence_name, args.frame_i_id)"
        ],
        "pos_line": [
            "+img_i, _, cam_i = load_data(root_dir, args.sequence_name, args.frame_i_id)"
        ],
        "core_change": "-img_i, depth_i, cam_i = load_data(root_dir, args.sequence_name, args.frame_i_id) +img_i, _, cam_i = load_data(root_dir, args.sequence_name, args.frame_i_id)",
        "core_API": "join"
    },
    {
        "commit_hash": "fec3112cbacf19cd211109b1bc0b33a502595aca",
        "index": "eea0fc57..7675ac8f 100644",
        "commit_message": "Update to PyTorch 1.9.0 (#2887)\n\n* Update to PyTorch 1.9.0\n\n* Update torch.cholesky, torch.symeig\n\n* Fix torch.tensordot, torch.qr, funsor dependency\n\n* Fix torch import, AutoGuideList usage\n\n* Ignore bug in PyTorch jit\n\n* Ignore tracer warnings\n\n* Replace torch.solve with torch.linalg.solve\n\n* Xfail vectorized markov funsor tests\n\n* Update funsor version\n\n* Switch MNIST mirrors\n\n* Pin pillow version\n\n* Bump torchvision version\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_lower_cholesky_transform(batch_shape, dim):",
            "x = torch.randn(batch_shape + (dim, dim))",
            "y = t(x)",
            "assert y.shape == x.shape",
            "-    actual = y.matmul(y.transpose(-1, -2)).cholesky()",
            "+    actual = torch.linalg.cholesky(y.matmul(y.transpose(-1, -2)))",
            "assert_close(actual, y)",
            "x2 = t.inv(y)",
            "assert x2.shape == x.shape"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=677141)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=677142)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=677143)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=677144)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cholesky'), position=2, insert_id=677145)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=677146)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=call), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=677147)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=677148)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=677149)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'linalg'), position=2, insert_id=677150)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=cholesky))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 5447,
        "neg_line": [
            "-actual = y.matmul(y.transpose(-1, -2)).cholesky()"
        ],
        "pos_line": [
            "+actual = torch.linalg.cholesky(y.matmul(y.transpose(-1, -2)))"
        ],
        "core_change": "-actual = y.matmul(y.transpose(-1, -2)).cholesky() +actual = torch.linalg.cholesky(y.matmul(y.transpose(-1, -2)))",
        "core_API": "randn"
    },
    {
        "commit_hash": "ef27d10aeed00bce5d32cc0eeb7ee45dcbe438bf",
        "index": "70b1a430..9764caa7 100644",
        "commit_message": "Implement ProjectedNormal distribution and reparametrizer (#2736)\n\n* Implement ProjectedNormal distribution and reparametrizer\n\n* Attempt to fix import error\n\n* Attempt to implement .log_prob() for dims 2,3\n\n* Add test fixture; fix bugs\n\n* Fix some bugs in log_prob implementations\n\n* Get _log_prob_2 to pass gof tests\n\n* Get _log_prob_2 to pass gof tests\n\n* Get remaining gof tests to pass\n\n* Add test for constraints.sphere\n\n* Add test for ProjectedNormalReparam\n\n* Add helpful errors to autoguides\n\n* Support PyTorch 1.6\n\n* Fix docs\n\n* Clarify error message\n\n* Use non-centered reparameterizer\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class InitMessenger(Messenger):",
            "def _pyro_sample(self, msg):",
            "if msg[\"done\"] or msg[\"is_observed\"] or type(msg[\"fn\"]).__name__ == \"_Subsample\":",
            "return",
            "-        with torch.no_grad():",
            "+        with torch.no_grad(), helpful_support_errors(msg):",
            "value = self.init_fn(msg)",
            "if is_validation_enabled() and msg[\"value\"] is not None:",
            "if not isinstance(value, type(msg[\"value\"])):"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=with_clause), node=(',', ','), position=1, insert_id=687267)",
            "Insert(target_node=ASTNode(type=with_clause), node=('with_item', None), position=2, insert_id=687268)",
            "Insert(target_node=IN(type=with_item), node=('call', None), position=0, insert_id=687269)",
            "Insert(target_node=IN(type=call), node=('identifier', 'helpful_support_errors'), position=0, insert_id=687270)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=687271)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=687272)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'msg'), position=1, insert_id=687273)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=687274)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 5448,
        "neg_line": [
            "-with torch.no_grad():"
        ],
        "pos_line": [
            "+with torch.no_grad(), helpful_support_errors(msg):"
        ],
        "core_change": "-with torch.no_grad(): +with torch.no_grad(), helpful_support_errors(msg):",
        "core_API": "no_grad"
    },
    {
        "commit_hash": "014781835ff64cd85bbf7999f10ead1d7b064f8a",
        "index": "b18da3af0..fee4b1c55 100644",
        "commit_message": "bug fix: transfer lengths to cpu\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Encoder(torch.nn.Module):",
            "",
            "\"\"\"",
            "xs = x.unsqueeze(0)",
            "-        ilens = [x.size(0)]",
            "+        ilens = torch.tensor([x.size(0)])",
            "",
            "return self.forward(xs, ilens)[0][0]"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('call', None), position=2, insert_id=145971)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=145972)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=145973)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=145974)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=145975)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tensor'), position=2, insert_id=145976)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=145977)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=list), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=145978)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 5449,
        "neg_line": [
            "-ilens = [x.size(0)]"
        ],
        "pos_line": [
            "+ilens = torch.tensor([x.size(0)])"
        ],
        "core_change": "-ilens = [x.size(0)] +ilens = torch.tensor([x.size(0)])",
        "core_API": "unsqueeze"
    },
    {
        "commit_hash": "0eac7bd682a163b4b47d75f7814eb6e8de019cf7",
        "index": "9345d5ba..a4cdc8bd 100644",
        "commit_message": "small fix\n\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class NCSNpp(ModelMixin, ConfigMixin):",
            "for i_level in reversed(range(self.num_resolutions)):",
            "for i_block in range(num_res_blocks + 1):",
            "out_ch = nf * ch_mult[i_level]",
            "+                in_ch = in_ch + hs_c.pop()",
            "modules.append(",
            "ResnetBlock(",
            "-                        in_channels=in_ch + hs_c.pop(),",
            "+                        in_channels=in_ch,",
            "out_channels=out_ch,",
            "temb_channels=4 * nf,",
            "output_scale_factor=np.sqrt(2.0),"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=4, insert_id=1330622)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1330623)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'in_ch'), position=0, insert_id=1330624)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1330625)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=binary_operator), position=2)",
            "Insert(target_node=ASTNode(type=keyword_argument), node=('identifier', 'in_ch'), position=2, insert_id=1330626)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 5451,
        "neg_line": [
            "-in_channels=in_ch + hs_c.pop(),"
        ],
        "pos_line": [
            "+in_ch = in_ch + hs_c.pop()",
            "+in_channels=in_ch,"
        ],
        "core_change": "+in_ch = in_ch + hs_c.pop() -in_channels=in_ch + hs_c.pop(), +in_channels=in_ch,",
        "core_API": "pop"
    },
    {
        "commit_hash": "0dbcd3c9611e1f6b239a3aa52b75370b907e844d",
        "index": "d97f4c9d1..337128d57 100644",
        "commit_message": "Add comment on reason for torch fix\n\n",
        "file": "PySyft.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TorchHook:",
            "",
            "if torch.__version__ < \"1.0.2\":",
            "# Hard fix for PyTorch versions < 1.0.2",
            "+            # usage of torch.jit requires a torch version < torch 1.1, so we still need to support this torch version",
            "syft.torch.apply_fix16922(self.torch)",
            "",
            "torch_modules = syft.torch.torch_modules"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 17,
        "number": 5452,
        "neg_line": [],
        "pos_line": [
            "+# usage of torch.jit requires a torch version < torch 1.1, so we still need to support this torch version"
        ],
        "core_change": "+# usage of torch.jit requires a torch version < torch 1.1, so we still need to support this torch version",
        "core_API": "apply_fix16922"
    },
    {
        "commit_hash": "1a19939ea3ea02fb977f08975c2b344ed0c333c1",
        "index": "8753a66f..c5cdfe52 100644",
        "commit_message": "Replace some fs operations by tf.gfile for other fs support. (fix #416)\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def TryResumeTraining():",
            "if not logger.LOG_DIR:",
            "return JustCurrentSession()",
            "path = os.path.join(logger.LOG_DIR, 'checkpoint')",
            "-    if not os.path.isfile(path):",
            "+    if not tf.gfile.Exists(path):",
            "return JustCurrentSession()",
            "return SaverRestore(path)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=isfile), value='Exists')",
            "Update(target_node=ASTNode(type=identifier, text=os), value='tf')",
            "Update(target_node=ASTNode(type=identifier, text=path), value='gfile')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 3,
        "number": 5453,
        "neg_line": [
            "-if not os.path.isfile(path):"
        ],
        "pos_line": [
            "+if not tf.gfile.Exists(path):"
        ],
        "core_change": "-if not os.path.isfile(path): +if not tf.gfile.Exists(path):",
        "core_API": "join"
    },
    {
        "commit_hash": "6e124e7207f6459cb43f540cfb5a1c6cc9b00f7a",
        "index": "f209a310e..75884d8cf 100644",
        "commit_message": "CI: precommit - docformatter (#8584)\n\n* CI: precommit - docformatter\n* fix deprecated\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class DeviceAssertCallback(Callback):",
            "@pytest.mark.parametrize([\"dst_device\"], [pytest.param(torch.device(\"cpu\")), pytest.param(torch.device(\"cuda\", 0))])",
            "@RunIf(min_gpus=1)",
            "def test_submodules_device_and_dtype(dst_device, dst_dtype):",
            "-    \"\"\"",
            "-    Test that the device and dtype property updates propagate through mixed nesting of regular",
            "-    nn.Modules and the special modules of type DeviceDtypeModuleMixin (e.g. Metric or LightningModule).",
            "-    \"\"\"",
            "+    \"\"\"Test that the device and dtype property updates propagate through mixed nesting of regular nn.Modules and",
            "+    the special modules of type DeviceDtypeModuleMixin (e.g. Metric or LightningModule).\"\"\"",
            "",
            "model = TopModule()",
            "assert model.device == torch.device(\"cpu\")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"\"\"\n    Test that the device and dtype property updates propagate through mixed nesting of regular\n    nn.Modules and the special modules of type DeviceDtypeModuleMixin (e.g. Metric or LightningModule).\n    \"\"\"), value='\"\"\"Test that the device and dtype property updates propagate through mixed nesting of regular nn.Modules and\\n    the special modules of type DeviceDtypeModuleMixin (e.g. Metric or LightningModule).\"\"\"')"
        ],
        "plus_line": 2,
        "minus_line": 4,
        "AST_diff_line": 1,
        "number": 5455,
        "neg_line": [
            "-\"\"\"",
            "-Test that the device and dtype property updates propagate through mixed nesting of regular",
            "-nn.Modules and the special modules of type DeviceDtypeModuleMixin (e.g. Metric or LightningModule).",
            "-\"\"\""
        ],
        "pos_line": [
            "+\"\"\"Test that the device and dtype property updates propagate through mixed nesting of regular nn.Modules and",
            "+the special modules of type DeviceDtypeModuleMixin (e.g. Metric or LightningModule).\"\"\""
        ],
        "core_change": "-\"\"\" -Test that the device and dtype property updates propagate through mixed nesting of regular -nn.Modules and the special modules of type DeviceDtypeModuleMixin (e.g. Metric or LightningModule). -\"\"\" +\"\"\"Test that the device and dtype property updates propagate through mixed nesting of regular nn.Modules and +the special modules of type DeviceDtypeModuleMixin (e.g. Metric or LightningModule).\"\"\"",
        "core_API": "parametrize"
    },
    {
        "commit_hash": "d93a32ecdadeff89f13a67d0a8fdb7d08cef9035",
        "index": "dd66011..7e36b69 100644",
        "commit_message": "L2 nofm fix for tensorflow 1.0 (#810)\n\n\n",
        "file": "tflearn.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def l2_normalize(incoming, dim, epsilon=1e-12, name=\"l2_normalize\"):",
            "A `Tensor` with the same shape as `x`.",
            "\"\"\"",
            "with tf.name_scope(name) as name:",
            "-        x = tf.ops.convert_to_tensor(incoming, name=\"x\")",
            "+        x = tf.convert_to_tensor(incoming, name=\"x\")",
            "square_sum = tf.reduce_sum(tf.square(x), [dim], keep_dims=True)",
            "x_inv_norm = tf.rsqrt(tf.maximum(square_sum, epsilon))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 5456,
        "neg_line": [
            "-x = tf.ops.convert_to_tensor(incoming, name=\"x\")"
        ],
        "pos_line": [
            "+x = tf.convert_to_tensor(incoming, name=\"x\")"
        ],
        "core_change": "-x = tf.ops.convert_to_tensor(incoming, name=\"x\") +x = tf.convert_to_tensor(incoming, name=\"x\")",
        "core_API": "name_scope"
    },
    {
        "commit_hash": "a2900a39bd7dd5682268582c9cba39e4f2a43fbc",
        "index": "c389655d..c692697b 100644",
        "commit_message": "Fix dropout by temporarily replacing with nn.Dropout\n\n",
        "file": "stanza.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class HighwayLSTM(nn.Module):",
            "self.lstm = nn.ModuleList()",
            "self.highway = nn.ModuleList()",
            "self.gate = nn.ModuleList()",
            "-        self.drop = Dropout(dropout, dims=[1] if batch_first else [0])",
            "+        self.drop = nn.Dropout(dropout)",
            "",
            "in_size = input_size",
            "for l in range(num_layers):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1517121)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'nn'), position=0, insert_id=1517122)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1517123)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=Dropout), position=2)",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=dims))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=list))",
            "Delete(target_node=ASTNode(type=if, text=if))",
            "Delete(target_node=ASTNode(type=identifier, text=batch_first))",
            "Delete(target_node=ASTNode(type=else, text=else))",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=integer, text=0))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=list))",
            "Delete(target_node=ASTNode(type=conditional_expression))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 20,
        "number": 5461,
        "neg_line": [
            "-self.drop = Dropout(dropout, dims=[1] if batch_first else [0])"
        ],
        "pos_line": [
            "+self.drop = nn.Dropout(dropout)"
        ],
        "core_change": "-self.drop = Dropout(dropout, dims=[1] if batch_first else [0]) +self.drop = nn.Dropout(dropout)",
        "core_API": "ModuleList"
    },
    {
        "commit_hash": "798384467bdbef1478c322b6ccd3e718e344fb91",
        "index": "364bc67c8..b3a0beea3 100755",
        "commit_message": "bugfix: div-->dim (#18135)\n\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class PerceiverBasicDecoder(PerceiverAbstractDecoder):",
            "if self.concat_preprocessed_input:",
            "if inputs_without_pos is None:",
            "raise ValueError(\"Value is required for inputs_without_pos if concat_preprocessed_input is True\")",
            "-            pos_emb = torch.cat([inputs_without_pos, pos_emb], div=-1)",
            "+            pos_emb = torch.cat([inputs_without_pos, pos_emb], dim=-1)",
            "",
            "return pos_emb"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=div), value='dim')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 5462,
        "neg_line": [
            "-pos_emb = torch.cat([inputs_without_pos, pos_emb], div=-1)"
        ],
        "pos_line": [
            "+pos_emb = torch.cat([inputs_without_pos, pos_emb], dim=-1)"
        ],
        "core_change": "-pos_emb = torch.cat([inputs_without_pos, pos_emb], div=-1) +pos_emb = torch.cat([inputs_without_pos, pos_emb], dim=-1)",
        "core_API": "cat"
    },
    {
        "commit_hash": "035b98ed55c56cd06aac09d39de54a2403757ee2",
        "index": "21a76b2..13de262 100644",
        "commit_message": "keras: fix MetricAverageCallbackImpl\n\n",
        "file": "byteps.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class MetricAverageCallbackImpl(object):",
            "self.device = device",
            "",
            "def _make_variable(self, metric, value):",
            "-        with tf.name_scope('MetricAverageCallback'):",
            "+        with tf.name_scope('MetricAverageCallback') as scope:",
            "var = tf.Variable(value, name=metric)",
            "self.backend.get_session().run(var.initializer)",
            "-            push_pull_op = bps.push_pull(var, device_dense=self.device)",
            "+            push_pull_op = bps.push_pull(var, scope, device_dense=self.device)",
            "return var, push_pull_op",
            "",
            "def _average_metrics_in_place(self, logs):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'scope'), position=3, insert_id=1908517)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=1908518)",
            "Insert(target_node=ASTNode(type=with_item), node=('as_pattern', None), position=0, insert_id=1908519)",
            "Move(target_node=IN(type=as_pattern), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=as_pattern), node=('as', 'as'), position=1, insert_id=1908520)",
            "Insert(target_node=IN(type=as_pattern), node=('as_pattern_target', None), position=2, insert_id=1908521)",
            "Insert(target_node=IN(type=as_pattern_target), node=('identifier', 'scope'), position=0, insert_id=1908522)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 7,
        "number": 5464,
        "neg_line": [
            "-with tf.name_scope('MetricAverageCallback'):",
            "-push_pull_op = bps.push_pull(var, device_dense=self.device)"
        ],
        "pos_line": [
            "+with tf.name_scope('MetricAverageCallback') as scope:",
            "+push_pull_op = bps.push_pull(var, scope, device_dense=self.device)"
        ],
        "core_change": "-with tf.name_scope('MetricAverageCallback'): +with tf.name_scope('MetricAverageCallback') as scope: -push_pull_op = bps.push_pull(var, device_dense=self.device) +push_pull_op = bps.push_pull(var, scope, device_dense=self.device)",
        "core_API": "name_scope"
    },
    {
        "commit_hash": "f3db4de472cfad06a34c986e972c3b3134ec79eb",
        "index": "0d37ba8a1..04822f7f1 100644",
        "commit_message": "fix #771 and flake8\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Conv2dSubsampling(torch.nn.Module):",
            "torch.nn.ReLU()",
            ")",
            "self.out = torch.nn.Sequential(",
            "-            torch.nn.Linear(odim * ((idim - 1)// 4), odim),",
            "+            torch.nn.Linear(odim * ((idim - 1) // 4), odim),",
            "PositionalEncoding(odim, dropout_rate)",
            ")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 5465,
        "neg_line": [
            "-torch.nn.Linear(odim * ((idim - 1)// 4), odim),"
        ],
        "pos_line": [
            "+torch.nn.Linear(odim * ((idim - 1) // 4), odim),"
        ],
        "core_change": "-torch.nn.Linear(odim * ((idim - 1)// 4), odim), +torch.nn.Linear(odim * ((idim - 1) // 4), odim),",
        "core_API": "ReLU"
    },
    {
        "commit_hash": "4fd8977eafd658056c92ffa6294027e77cdc46c0",
        "index": "c464cb5549..f0c62935de 100644",
        "commit_message": "[RLlib] Minor cleanup in preparation to tf2.x support. (#9130)\n\n* WIP.\n\n* Fixes.\n\n* LINT.\n\n* Fixes.\n\n* Fixes and LINT.\n\n* WIP.\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class ParametricActionsModel(DistributionalQTFModel):",
            "action_logits = tf.reduce_sum(avail_actions * intent_vector, axis=2)",
            "",
            "# Mask out invalid actions (use tf.float32.min for stability)",
            "-        inf_mask = tf.maximum(tf.log(action_mask), tf.float32.min)",
            "+        inf_mask = tf.maximum(tf.math.log(action_mask), tf.float32.min)",
            "return action_logits + inf_mask, state",
            "",
            "def value_function(self):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=2145932)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=2145933)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=tf), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'math'), position=2, insert_id=2145934)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 5467,
        "neg_line": [
            "-inf_mask = tf.maximum(tf.log(action_mask), tf.float32.min)"
        ],
        "pos_line": [
            "+inf_mask = tf.maximum(tf.math.log(action_mask), tf.float32.min)"
        ],
        "core_change": "-inf_mask = tf.maximum(tf.log(action_mask), tf.float32.min) +inf_mask = tf.maximum(tf.math.log(action_mask), tf.float32.min)",
        "core_API": "reduce_sum"
    },
    {
        "commit_hash": "4d938bca820a8a0d6ebf48af2a025322e64042e4",
        "index": "86913a1c..ae445103 100644",
        "commit_message": "Rename .reshape(s,n) -> .expand_by(s).independent(n) (#1016)\n\n* Start to rename .reshape()\n\n* Remove the .reshape() method entirely\n\n* Add .reshape() with informative error message\n\n* Fix test\n\n* Fix failing test\n\n* Fix failing test\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class NormalNormalTests(TestCase):",
            "requires_grad=True))",
            "sig_q = torch.exp(log_sig_q)",
            "Normal = dist.Normal if reparameterized else fakes.NonreparameterizedNormal",
            "-            pyro.sample(\"loc_latent\", Normal(loc_q, sig_q).reshape(extra_event_dims=1))",
            "+            pyro.sample(\"loc_latent\", Normal(loc_q, sig_q).independent(1))",
            "",
            "adam = optim.Adam({\"lr\": .001})",
            "svi = SVI(model, guide, adam, loss=\"ELBO\", trace_graph=False)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=reshape), value='independent')",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=integer, text=1), position=1)",
            "Delete(target_node=ASTNode(type=identifier, text=extra_event_dims))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 5469,
        "neg_line": [
            "-pyro.sample(\"loc_latent\", Normal(loc_q, sig_q).reshape(extra_event_dims=1))"
        ],
        "pos_line": [
            "+pyro.sample(\"loc_latent\", Normal(loc_q, sig_q).independent(1))"
        ],
        "core_change": "-pyro.sample(\"loc_latent\", Normal(loc_q, sig_q).reshape(extra_event_dims=1)) +pyro.sample(\"loc_latent\", Normal(loc_q, sig_q).independent(1))",
        "core_API": "exp"
    },
    {
        "commit_hash": "2c83b3c38d00377c0bb92a7f438758e10417bfd1",
        "index": "9e4a5a28f..79dc6b0d1 100755",
        "commit_message": "Support various BERT relative position embeddings (2nd) (#8276)\n\n* Support BERT relative position embeddings\n\n* Fix typo in README.md\n\n* Address review comment\n\n* Fix failing tests\n\n* [tiny] Fix style_doc.py check by adding an empty line to configuration_bert.py\n\n* make fix copies\n\n* fix configs of electra and albert and fix longformer\n\n* remove copy statement from longformer\n\n* fix albert\n\n* fix electra\n\n* Add bert variants forward tests for various position embeddings\n\n* [tiny] Fix style for test_modeling_bert.py\n\n* improve docstring\n\n* [tiny] improve docstring and remove unnecessary dependency\n\n* [tiny] Remove unused import\n\n* re-add to ALBERT\n\n* make embeddings work for ALBERT\n\n* add test for albert\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "models",
        "change": [
            "class LongformerEmbeddings(nn.Module):",
            "else:",
            "position_ids = self.create_position_ids_from_inputs_embeds(inputs_embeds)",
            "",
            "-        # Copied from transformers.models.bert.modeling_bert.BertEmbeddings.forward",
            "if input_ids is not None:",
            "input_shape = input_ids.size()",
            "else:"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 5470,
        "neg_line": [
            "-# Copied from transformers.models.bert.modeling_bert.BertEmbeddings.forward"
        ],
        "pos_line": [],
        "core_change": "-# Copied from transformers.models.bert.modeling_bert.BertEmbeddings.forward",
        "core_API": "create_position_ids_from_inputs_embeds"
    },
    {
        "commit_hash": "2625fb59dc7a3f03516f0c6c5c0cdda18ef0ef5b",
        "index": "6d28f07b..5b337f48 100644",
        "commit_message": "[Versatile Diffusion] Add versatile diffusion model (#1283)\n\n* up\n\n* convert dual unet\n\n* revert dual attn\n\n* adapt for vd-official\n\n* test the full pipeline\n\n* mixed inference\n\n* mixed inference for text2img\n\n* add image prompting\n\n* fix clip norm\n\n* split text2img and img2img\n\n* fix format\n\n* refactor text2img\n\n* mega pipeline\n\n* add optimus\n\n* refactor image var\n\n* wip text_unet\n\n* text unet end to end\n\n* update tests\n\n* reshape\n\n* fix image to text\n\n* add some first docs\n\n* dual guided pipeline\n\n* fix token ratio\n\n* propose change\n\n* dual transformer as a native module\n\n* DualTransformer(nn.Module)\n\n* DualTransformer(nn.Module)\n\n* correct unconditional image\n\n* save-load with mega pipeline\n\n* remove image to text\n\n* up\n\n* uP\n\n* fix\n\n* up\n\n* final fix\n\n* remove_unused_weights\n\n* test updates\n\n* save progress\n\n* uP\n\n* fix dual prompts\n\n* some fixes\n\n* finish\n\n* style\n\n* finish renaming\n\n* up\n\n* fix\n\n* fix\n\n* fix\n\n* finish\n\nCo-authored-by: anton-l <anton@huggingface.co>\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class UNet2DModel(ModelMixin, ConfigMixin):",
            "num_groups_out = norm_num_groups if norm_num_groups is not None else min(block_out_channels[0] // 4, 32)",
            "self.conv_norm_out = nn.GroupNorm(num_channels=block_out_channels[0], num_groups=num_groups_out, eps=norm_eps)",
            "self.conv_act = nn.SiLU()",
            "-        self.conv_out = nn.Conv2d(block_out_channels[0], out_channels, 3, padding=1)",
            "+        self.conv_out = nn.Conv2d(block_out_channels[0], out_channels, kernel_size=3, padding=1)",
            "",
            "def forward(",
            "self,"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=1328783)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'kernel_size'), position=0, insert_id=1328784)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1328785)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=integer, text=3), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 5473,
        "neg_line": [
            "-self.conv_out = nn.Conv2d(block_out_channels[0], out_channels, 3, padding=1)"
        ],
        "pos_line": [
            "+self.conv_out = nn.Conv2d(block_out_channels[0], out_channels, kernel_size=3, padding=1)"
        ],
        "core_change": "-self.conv_out = nn.Conv2d(block_out_channels[0], out_channels, 3, padding=1) +self.conv_out = nn.Conv2d(block_out_channels[0], out_channels, kernel_size=3, padding=1)",
        "core_API": "GroupNorm"
    },
    {
        "commit_hash": "04a6ad1981abd959f12585f42212e947cfa96318",
        "index": "353beca1..f44f2f5c 100644",
        "commit_message": "adopt torch.testing.assert_close (#1031)\n\n* adopt torch.testing.assert_close\n\n* use torch.testing.assert_close\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add TODO\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestSolveCast:",
            "error = torch.dist(B, A.matmul(X))",
            "",
            "tol_val: float = 1e-1 if dtype == torch.float16 else 1e-4",
            "-        assert_allclose(error, torch.zeros_like(error), atol=tol_val, rtol=tol_val)",
            "+        assert_close(error, torch.zeros_like(error), atol=tol_val, rtol=tol_val)"
        ],
        "hunk_index": 4,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=assert_allclose), value='assert_close')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 5474,
        "neg_line": [
            "-assert_allclose(error, torch.zeros_like(error), atol=tol_val, rtol=tol_val)"
        ],
        "pos_line": [
            "+assert_close(error, torch.zeros_like(error), atol=tol_val, rtol=tol_val)"
        ],
        "core_change": "-assert_allclose(error, torch.zeros_like(error), atol=tol_val, rtol=tol_val) +assert_close(error, torch.zeros_like(error), atol=tol_val, rtol=tol_val)",
        "core_API": "dist"
    },
    {
        "commit_hash": "7ed8d51edaf373a7ace08750685377c75ee9578e",
        "index": "21162eb9..45d0177b 100644",
        "commit_message": "Change V2.0 coors (#2380)\n\n* Refactor (all): change coordinate system\n\n* Fix (mask_head): fix cat -1 bug in mask_paste\n\n* Fix (unittest)\n: modify unittest and pass CI\n\n* reformat to pass CI\n\n* Fix round coordinates bugs\n\n* clean file\n\n* Fix (test): use cpu version of aligned roi_align in tests\n\n* Refactor (mask): clean np.stack\n\n* Refactor (head): reformat code and fix missing -1\n\n* Reformat: reformat and add doc strings\n\n* Refactor (mask_head): more clea docstring\n",
        "file": "mmdetection.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def ga_loc_target(gt_bboxes_list,",
            "all_ignore_map.append(ignore_map)",
            "for img_id in range(img_per_gpu):",
            "gt_bboxes = gt_bboxes_list[img_id]",
            "-        scale = torch.sqrt((gt_bboxes[:, 2] - gt_bboxes[:, 0] + 1) *",
            "-                           (gt_bboxes[:, 3] - gt_bboxes[:, 1] + 1))",
            "+        scale = torch.sqrt((gt_bboxes[:, 2] - gt_bboxes[:, 0]) *",
            "+                           (gt_bboxes[:, 3] - gt_bboxes[:, 1]))",
            "min_anchor_size = scale.new_full(",
            "(1, ), float(anchor_scale * anchor_strides[0]))",
            "# assign gt bboxes to different feature levels w.r.t. their scales"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=IN(type=root), node=ASTNode(type=binary_operator), position=0)",
            "Move(target_node=ASTNode(type=parenthesized_expression), node=ASTNode(type=binary_operator), position=1)",
            "Move(target_node=ASTNode(type=parenthesized_expression), node=ASTNode(type=binary_operator), position=1)",
            "Delete(target_node=ASTNode(type=+, text=+))",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=+, text=+))",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=binary_operator))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 8,
        "number": 5476,
        "neg_line": [
            "-scale = torch.sqrt((gt_bboxes[:, 2] - gt_bboxes[:, 0] + 1) *",
            "-(gt_bboxes[:, 3] - gt_bboxes[:, 1] + 1))"
        ],
        "pos_line": [
            "+scale = torch.sqrt((gt_bboxes[:, 2] - gt_bboxes[:, 0]) *",
            "+(gt_bboxes[:, 3] - gt_bboxes[:, 1]))"
        ],
        "core_change": "-scale = torch.sqrt((gt_bboxes[:, 2] - gt_bboxes[:, 0] + 1) * -(gt_bboxes[:, 3] - gt_bboxes[:, 1] + 1)) +scale = torch.sqrt((gt_bboxes[:, 2] - gt_bboxes[:, 0]) * +(gt_bboxes[:, 3] - gt_bboxes[:, 1]))",
        "core_API": "append"
    },
    {
        "commit_hash": "6a2b04741a0d511e8599bd648291e659179da5d9",
        "index": "b1f12f19..6e9d370c 100644",
        "commit_message": "Implement Lightning module for GraphGym (#4511)\n\n* add LitModule\n\n* format\n\n* add pl dep\n\n* add pl to min deps\n\n* add type hint\n\n* apply suggestions\n\n* apply suggestions\n\n* Update setup.py\n\n* fix tests\n\n* graphgym_requires\n\n* graphgym install\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def create_model(to_device=True, dim_in=None, dim_out=None):",
            "if 'classification' in cfg.dataset.task_type and dim_out == 2:",
            "dim_out = 1",
            "",
            "-    model = network_dict[cfg.model.type](dim_in=dim_in, dim_out=dim_out)",
            "+    model = GraphGymModule(dim_in, dim_out, cfg)",
            "if to_device:",
            "model.to(torch.device(cfg.device))",
            "return model"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=network_dict), value='GraphGymModule')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=network_dict), position=0)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=dim_in), position=1)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=981437)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=dim_out), position=3)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=cfg), position=6)",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=model))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=type))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=dim_in))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=dim_out))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 21,
        "number": 5478,
        "neg_line": [
            "-model = network_dict[cfg.model.type](dim_in=dim_in, dim_out=dim_out)"
        ],
        "pos_line": [
            "+model = GraphGymModule(dim_in, dim_out, cfg)"
        ],
        "core_change": "-model = network_dict[cfg.model.type](dim_in=dim_in, dim_out=dim_out) +model = GraphGymModule(dim_in, dim_out, cfg)",
        "core_API": "to"
    },
    {
        "commit_hash": "c394607b4729e03dfa7290496e325511d5ead766",
        "index": "bfcf2b20e..ab19dbda8 100644",
        "commit_message": "fix multi-gpu bug for cer/wer report\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def nsc_beam_search(decoder, h, recog_args, rnnlm=None):",
            "for i, hyp in enumerate(hyps):",
            "i_topk = (",
            "torch.cat((beam_topk[0][i], beam_logp[i, 0:1])),",
            "-                    torch.cat((beam_topk[1][i], torch.LongTensor([0]))),",
            "+                    torch.cat((beam_topk[1][i], blank_tensor)),",
            ")",
            "",
            "for logp, k in zip(*i_topk):"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=torch), value='blank_tensor')",
            "Move(target_node=ASTNode(type=tuple), node=ASTNode(type=identifier, text=torch), position=3)",
            "Move(target_node=ASTNode(type=tuple), node=ASTNode(type=), text=)), position=4)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=LongTensor))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=integer, text=0))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=list))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=), text=)))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 14,
        "number": 5479,
        "neg_line": [
            "-torch.cat((beam_topk[1][i], torch.LongTensor([0]))),"
        ],
        "pos_line": [
            "+torch.cat((beam_topk[1][i], blank_tensor)),"
        ],
        "core_change": "-torch.cat((beam_topk[1][i], torch.LongTensor([0]))), +torch.cat((beam_topk[1][i], blank_tensor)),",
        "core_API": "cat"
    },
    {
        "commit_hash": "df124d0ad58ea7189e88f9fe42c1ee377ade9c8d",
        "index": "103b5149bb..d72d2f6b98 100644",
        "commit_message": "[AIR - Datasets] Hide tensor extension from UDFs. (#27019)\n\nWe previously added automatic tensor extension casting on Datasets transformation outputs to allow the user to not have to worry about tensor column casting; however, this current state creates several issues:\n\n1. Not all tensors are supported, which means that well need to have an opaque object dtype (i.e. ndarray of ndarray pointers) fallback for the Pandas-only case. Known unsupported tensor use cases:\na. Heterogeneous-shaped (i.e. ragged) tensors\nb. Struct arrays\n2. UDFs will expect a NumPy column and wont know what to do with our TensorArray type. E.g., torchvision transforms dont respect the array protocol (which they should), and instead only support Torch tensors and NumPy ndarrays; passing a TensorArray column or a TensorArrayElement (a single item in the TensorArray column) fails.\nImplicit casting with object dtype fallback on UDF outputs can make the input type to downstream UDFs nondeterministic, where the user wont know if theyll get a TensorArray column or an object dtype column.\n3. The tensor extension cast fallback warning spams the logs.\n\nThis PR:\n\n1. Adds automatic casting of tensor extension columns to NumPy ndarray columns for Datasets UDF inputs, meaning the UDFs will never have to see tensor extensions and that the UDF input column types will be consistent and deterministic; this fixes both (2) and (3).\n2. No longer implicitly falls back to an opaque object dtype when TensorArray casting fails (e.g. for ragged tensors), and instead raises an error; this fixes (4) but removes our support for (1).\n3. Adds a global enable_tensor_extension_casting config flag, which is True by default, that controls whether we perform this automatic casting. Turning off the implicit casting provides a path for (1), where the tensor extension can be avoided if working with ragged tensors in Pandas land. Turning off this flag also allows the user to explicitly control their tensor extension casting, if they want to work with it in their UDFs in order to reap the benefits of less data copies, more efficient slicing, stronger column typing, etc.\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def convert_ndarray_batch_to_tf_tensor_batch(",
            "f\"should be given, instead got: {dtypes}\"",
            ")",
            "dtypes = next(iter(dtypes.values()))",
            "-        batch = tf.convert_to_tensor(ndarrays, dtype=dtypes)",
            "+        batch = convert_ndarray_to_tf_tensor(ndarrays, dtypes)",
            "else:",
            "# Multi-tensor case.",
            "batch = {",
            "-            col_name: tf.convert_to_tensor(",
            "+            col_name: convert_ndarray_to_tf_tensor(",
            "col_ndarray,",
            "dtype=dtypes[col_name] if isinstance(dtypes, dict) else dtypes,",
            ")"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tf), value='convert_ndarray_to_tf_tensor')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=tf), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=tf), value='convert_ndarray_to_tf_tensor')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=tf), position=0)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=dtypes), position=3)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=convert_to_tensor))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=identifier, text=dtype))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=convert_to_tensor))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 14,
        "number": 5484,
        "neg_line": [
            "-batch = tf.convert_to_tensor(ndarrays, dtype=dtypes)",
            "-col_name: tf.convert_to_tensor("
        ],
        "pos_line": [
            "+batch = convert_ndarray_to_tf_tensor(ndarrays, dtypes)",
            "+col_name: convert_ndarray_to_tf_tensor("
        ],
        "core_change": "-batch = tf.convert_to_tensor(ndarrays, dtype=dtypes) +batch = convert_ndarray_to_tf_tensor(ndarrays, dtypes) -col_name: tf.convert_to_tensor( +col_name: convert_ndarray_to_tf_tensor(",
        "core_API": "values"
    },
    {
        "commit_hash": "5fa0b17c3d10cdb6411a173a7dce42b0de56a8f2",
        "index": "7f835f800..da4eaf0f1 100644",
        "commit_message": "[Past CI]  Leave Past CI failures in the past   (#20861)\n\n* torch.jit._state\n\n* Fix past CI\n\n* Fix for perceiver\n\n* Fix REALM\n\n* Fix for Bloom\n\n* Fix for SwinMode\n\n* Fix for TrajectoryTransformerModel\n\n* Fix for test_wav2vec2_with_lm\n\n* make style\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class RealmScorer(RealmPreTrainedModel):",
            "# [batch_size, num_candidates, retriever_proj_size]",
            "candidate_score = candidate_score.view(-1, self.config.num_candidates, self.config.retriever_proj_size)",
            "# [batch_size, num_candidates]",
            "-        relevance_score = torch.einsum(\"BD,BND->BN\", query_score, candidate_score)",
            "+        relevance_score = torch.einsum(\"bd,bnd->bn\", query_score, candidate_score)",
            "",
            "if not return_dict:",
            "return relevance_score, query_score, candidate_score"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"BD,BND->BN\"), value='\"bd,bnd->bn\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 5492,
        "neg_line": [
            "-relevance_score = torch.einsum(\"BD,BND->BN\", query_score, candidate_score)"
        ],
        "pos_line": [
            "+relevance_score = torch.einsum(\"bd,bnd->bn\", query_score, candidate_score)"
        ],
        "core_change": "-relevance_score = torch.einsum(\"BD,BND->BN\", query_score, candidate_score) +relevance_score = torch.einsum(\"bd,bnd->bn\", query_score, candidate_score)",
        "core_API": "view"
    },
    {
        "commit_hash": "123eee9d8cada4470a78ccdab4ac396b1f245926",
        "index": "4a683296..3ac9b53a 100644",
        "commit_message": "fix linter errors\n\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "functional",
        "change": [
            "class HomographyWarper(nn.Module):",
            "raise TypeError(\"Patch and homography must be on the same device. \\",
            "Got patch.device: {} dst_H_src.device: {}.\"",
            ".format(patch.device, dst_homo_src.device))",
            "-        return torch.nn.functional.grid_sample(patch,",
            "-            self.warp_grid(dst_homo_src), mode='bilinear',",
            "+        return torch.nn.functional.grid_sample(",
            "+            patch, self.warp_grid(dst_homo_src), mode='bilinear',",
            "padding_mode=padding_mode)",
            "",
            "# functional api"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 5496,
        "neg_line": [
            "-return torch.nn.functional.grid_sample(patch,",
            "-self.warp_grid(dst_homo_src), mode='bilinear',"
        ],
        "pos_line": [
            "+return torch.nn.functional.grid_sample(",
            "+patch, self.warp_grid(dst_homo_src), mode='bilinear',"
        ],
        "core_change": "-return torch.nn.functional.grid_sample(patch, -self.warp_grid(dst_homo_src), mode='bilinear', +return torch.nn.functional.grid_sample( +patch, self.warp_grid(dst_homo_src), mode='bilinear',",
        "core_API": "grid_sample"
    },
    {
        "commit_hash": "43043ee4d5c672f7fbc22ac9559e8164731fd053",
        "index": "963d4d816d..f5af4281fb 100644",
        "commit_message": "[RLlib] Tf2x preparation; part 2 (upgrading `try_import_tf()`). (#9136)\n\n* WIP.\n\n* Fixes.\n\n* LINT.\n\n* WIP.\n\n* WIP.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* WIP.\n\n* Fixes.\n\n* Test\n\n* Fix.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* LINT.\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class KLCoeffMixin:",
            "# KL Coefficient",
            "self.kl_coeff_val = config[\"kl_coeff\"]",
            "self.kl_target = config[\"kl_target\"]",
            "-        self.kl_coeff = tf.get_variable(",
            "+        self.kl_coeff = tf1.get_variable(",
            "initializer=tf.constant_initializer(self.kl_coeff_val),",
            "name=\"kl_coeff\",",
            "shape=(),"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tf), value='tf1')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 5507,
        "neg_line": [
            "-self.kl_coeff = tf.get_variable("
        ],
        "pos_line": [
            "+self.kl_coeff = tf1.get_variable("
        ],
        "core_change": "-self.kl_coeff = tf.get_variable( +self.kl_coeff = tf1.get_variable(",
        "core_API": "get_variable"
    },
    {
        "commit_hash": "fdc907675050cb9e3aa887bf89d6c92519178349",
        "index": "edf5c945..59a9d3cf 100755",
        "commit_message": "fix use of deprecated TF functions.\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Model(ModelDesc):",
            "log_qc = tf.reduce_sum(prior_prob * zc, 1, name='logQc')",
            "Elog_qc_given_x = tf.reduce_mean(log_qc_given_x, name='ElogQc_x')",
            "Hc = tf.reduce_mean(-log_qc, name='Hc')",
            "-        MIloss = tf.mul(Hc + Elog_qc_given_x, -1.0, name='neg_MI')",
            "+        MIloss = tf.multiply(Hc + Elog_qc_given_x, -1.0, name='neg_MI')",
            "",
            "self.g_loss, self.d_loss = build_GAN_losses(vecpos, vecneg)",
            "self.g_loss = tf.add(self.g_loss, MIloss, name='total_g_loss')"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=mul), value='multiply')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 5511,
        "neg_line": [
            "-MIloss = tf.mul(Hc + Elog_qc_given_x, -1.0, name='neg_MI')"
        ],
        "pos_line": [
            "+MIloss = tf.multiply(Hc + Elog_qc_given_x, -1.0, name='neg_MI')"
        ],
        "core_change": "-MIloss = tf.mul(Hc + Elog_qc_given_x, -1.0, name='neg_MI') +MIloss = tf.multiply(Hc + Elog_qc_given_x, -1.0, name='neg_MI')",
        "core_API": "reduce_sum"
    },
    {
        "commit_hash": "ad2cc0fb8c467f90d343458f95baceef51a58077",
        "index": "5699f817f6..46213e146d 100644",
        "commit_message": "small fix for `to_numpy`.\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def array_equal(",
            "",
            "def to_numpy(x: Union[tf.Tensor, tf.Variable], copy: bool = True) -> np.ndarray:",
            "# TensorFlow fails to convert bfloat16 tensor when it has 0 dimensions",
            "-    if get_num_dims(x) == 0 and ivy.as_native_dtype(x.dtype) is tf.bfloat16:",
            "+    if (",
            "+        ivy.is_array(x)",
            "+        and get_num_dims(x) == 0",
            "+        and ivy.as_native_dtype(x.dtype) is tf.bfloat16",
            "+    ):",
            "x = tf.expand_dims(x, 0)",
            "if copy:",
            "return np.squeeze(np.array(tf.convert_to_tensor(x)), 0)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=if_statement), node=('parenthesized_expression', None), position=1, insert_id=1996702)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('(', '('), position=0, insert_id=1996703)",
            "Move(target_node=IN(type=parenthesized_expression), node=ASTNode(type=boolean_operator), position=1)",
            "Insert(target_node=IN(type=parenthesized_expression), node=(')', ')'), position=2, insert_id=1996704)",
            "Insert(target_node=ASTNode(type=boolean_operator), node=('boolean_operator', None), position=0, insert_id=1996705)",
            "Insert(target_node=IN(type=boolean_operator), node=('call', None), position=0, insert_id=1996706)",
            "Insert(target_node=IN(type=boolean_operator), node=('and', 'and'), position=1, insert_id=1996707)",
            "Move(target_node=IN(type=boolean_operator), node=ASTNode(type=comparison_operator), position=2)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1996708)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1996709)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'ivy'), position=0, insert_id=1996710)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1996711)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'is_array'), position=2, insert_id=1996712)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1996713)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'x'), position=1, insert_id=1996714)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1996715)"
        ],
        "plus_line": 5,
        "minus_line": 1,
        "AST_diff_line": 16,
        "number": 5512,
        "neg_line": [
            "-if get_num_dims(x) == 0 and ivy.as_native_dtype(x.dtype) is tf.bfloat16:"
        ],
        "pos_line": [
            "+if (",
            "+ivy.is_array(x)",
            "+and get_num_dims(x) == 0",
            "+and ivy.as_native_dtype(x.dtype) is tf.bfloat16",
            "+):"
        ],
        "core_change": "-if get_num_dims(x) == 0 and ivy.as_native_dtype(x.dtype) is tf.bfloat16: +if ( +ivy.is_array(x) +and get_num_dims(x) == 0 +and ivy.as_native_dtype(x.dtype) is tf.bfloat16 +):",
        "core_API": "as_native_dtype"
    },
    {
        "commit_hash": "7caa9dbc933796b1b228c44d54d044bb17270959",
        "index": "c3ceaf8c64..5525625ece 100644",
        "commit_message": "Fix for `tensordot()` linalg.py (#7350)\n\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def tensordot(",
            "# type casting to float32 which is acceptable for tf.tensordot",
            "x1, x2 = tf.cast(x1, tf.float32), tf.cast(x2, tf.float32)",
            "",
            "-    ret = tf.cast(tf.tensordot(x1, x2, axes), dtype)",
            "+    ret = tf.cast(tf.tensordot(x1, x2, axes=axes), dtype)",
            "return ret"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=1973245)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=identifier, text=axes), position=0)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1973246)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'axes'), position=2, insert_id=1973247)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 5513,
        "neg_line": [
            "-ret = tf.cast(tf.tensordot(x1, x2, axes), dtype)"
        ],
        "pos_line": [
            "+ret = tf.cast(tf.tensordot(x1, x2, axes=axes), dtype)"
        ],
        "core_change": "-ret = tf.cast(tf.tensordot(x1, x2, axes), dtype) +ret = tf.cast(tf.tensordot(x1, x2, axes=axes), dtype)",
        "core_API": "cast"
    },
    {
        "commit_hash": "0afa5755b2212a783f861f03006a73f883100e81",
        "index": "6770761e..73d34355 100644",
        "commit_message": "Fixes warnings and add compatibility stub in torch solve (#1235)\n\n* add a compatibiliy stub for torch solve\n\n* change missing _torch_solve_cast calls\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* bump pytorch version to 1.7.1\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def conv_quad_interp3d(",
            "",
            "nms_mask: torch.Tensor = kornia.feature.nms3d(input, (3, 3, 3), True)",
            "x_solved: torch.Tensor = torch.zeros_like(b)",
            "-    x_solved_masked, _ = torch.solve(b[nms_mask.view(-1)], Hes[nms_mask.view(-1)])",
            "+    x_solved_masked, _ = _torch_solve_cast(b[nms_mask.view(-1)], Hes[nms_mask.view(-1)])",
            "x_solved.masked_scatter_(nms_mask.view(-1, 1, 1), x_solved_masked)",
            "dx: torch.Tensor = -x_solved"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=torch), value='_torch_solve_cast')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=torch), position=0)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=solve))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 5514,
        "neg_line": [
            "-x_solved_masked, _ = torch.solve(b[nms_mask.view(-1)], Hes[nms_mask.view(-1)])"
        ],
        "pos_line": [
            "+x_solved_masked, _ = _torch_solve_cast(b[nms_mask.view(-1)], Hes[nms_mask.view(-1)])"
        ],
        "core_change": "-x_solved_masked, _ = torch.solve(b[nms_mask.view(-1)], Hes[nms_mask.view(-1)]) +x_solved_masked, _ = _torch_solve_cast(b[nms_mask.view(-1)], Hes[nms_mask.view(-1)])",
        "core_API": "nms3d"
    },
    {
        "commit_hash": "94a7f55c4e5cca3dfe4de0bd0793173d5b152ec5",
        "index": "ba6b854..24f5a30 100644",
        "commit_message": "FReLU bias=False bug fix (#1666)\n\n\n",
        "file": "yolov5.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class MemoryEfficientMish(nn.Module):",
            "class FReLU(nn.Module):",
            "def __init__(self, c1, k=3):  # ch_in, kernel",
            "super().__init__()",
            "-        self.conv = nn.Conv2d(c1, c1, k, 1, 1, groups=c1)",
            "+        self.conv = nn.Conv2d(c1, c1, k, 1, 1, groups=c1, bias=False)",
            "self.bn = nn.BatchNorm2d(c1)",
            "",
            "def forward(self, x):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=12, insert_id=1570628)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=13, insert_id=1570629)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'bias'), position=0, insert_id=1570630)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1570631)",
            "Insert(target_node=IN(type=keyword_argument), node=('false', 'False'), position=2, insert_id=1570632)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 5516,
        "neg_line": [
            "-self.conv = nn.Conv2d(c1, c1, k, 1, 1, groups=c1)"
        ],
        "pos_line": [
            "+self.conv = nn.Conv2d(c1, c1, k, 1, 1, groups=c1, bias=False)"
        ],
        "core_change": "-self.conv = nn.Conv2d(c1, c1, k, 1, 1, groups=c1) +self.conv = nn.Conv2d(c1, c1, k, 1, 1, groups=c1, bias=False)",
        "core_API": "Conv2d"
    },
    {
        "commit_hash": "c5e05d7adcd337f8386f7dea381e03b79a9ce1d4",
        "index": "946fba09..d42cef04 100644",
        "commit_message": "fix colocation problems\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class MapGradient(GradientProcessor):",
            "for grad, var in grads:",
            "if re.match(self.regex, var.op.name):",
            "matched = True",
            "-                with tf.device(grad.device):",
            "-                    grad = self.func(grad, var)",
            "+                grad = self.func(grad, var)",
            "if grad is not None:",
            "ret.append((grad, var))",
            "else:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=module), node=ASTNode(type=expression_statement), position=4)",
            "Delete(target_node=ASTNode(type=with, text=with))",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=device))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=grad))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=device))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=with_item))",
            "Delete(target_node=ASTNode(type=with_clause))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=block))",
            "Delete(target_node=ASTNode(type=with_statement))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 19,
        "number": 5523,
        "neg_line": [
            "-with tf.device(grad.device):",
            "-grad = self.func(grad, var)"
        ],
        "pos_line": [
            "+grad = self.func(grad, var)"
        ],
        "core_change": "-with tf.device(grad.device): -grad = self.func(grad, var) +grad = self.func(grad, var)",
        "core_API": "match"
    },
    {
        "commit_hash": "e143e8bb2083992529e43a15af3aeffcde20304e",
        "index": "6f846605..9cf3a335 100644",
        "commit_message": "layer tests and model tests bug fixed\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "models",
        "change": [
            "class Layer_Embed_Test(CustomTestCase):",
            "except AttributeError as e:",
            "print(e)",
            "self.assertEqual(embed_tensor.get_shape().as_list(), [batch_size, embedding_size])",
            "-        model = tl.models.Model(inputs=inputs, outputs=embed_tensor, name=\"word2vec_model\")",
            "+        model = tl.models.Model(inputs=inputs, outputs=embed_tensor)",
            "",
            "",
            "if __name__ == '__main__':"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=name))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=string, text=\"word2vec_model\"))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 5525,
        "neg_line": [
            "-model = tl.models.Model(inputs=inputs, outputs=embed_tensor, name=\"word2vec_model\")"
        ],
        "pos_line": [
            "+model = tl.models.Model(inputs=inputs, outputs=embed_tensor)"
        ],
        "core_change": "-model = tl.models.Model(inputs=inputs, outputs=embed_tensor, name=\"word2vec_model\") +model = tl.models.Model(inputs=inputs, outputs=embed_tensor)",
        "core_API": "assertEqual"
    },
    {
        "commit_hash": "737b1f467e3610691f6f7ca4debb94a9d6219ad1",
        "index": "9dfb69ec..7f310d7d 100644",
        "commit_message": "Full GraphGym + PyG integration (#3076)\n\n* Full GraphGym + PyG integration\n\n* fix torch.allclose\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "sns.set_context('poster')",
            "",
            "",
            "def view_emb(emb, dir):",
            "+    '''",
            "+    Visualize a embedding matrix.",
            "+",
            "+    Args:",
            "+        emb (torch.tensor): Embedding matrix with shape (N, D). D is the",
            "+        feature dimension.",
            "+        dir (str): Output directory for the embedding figure.",
            "+",
            "+    '''",
            "if emb.shape[1] > 2:",
            "pca = PCA(n_components=2)",
            "emb = pca.fit_transform(emb)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('block', None), position=0, insert_id=1000081)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=1000082)",
            "Insert(target_node=IN(type=expression_statement), node=('string', \"'''\\n    Visualize a embedding matrix.\\n\\n    Args:\\n        emb (torch.tensor): Embedding matrix with shape (N, D). D is the\\n        feature dimension.\\n        dir (str): Output directory for the embedding figure.\\n\\n    '''\"), position=0, insert_id=1000083)",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 7,
        "minus_line": 0,
        "AST_diff_line": 4,
        "number": 5526,
        "neg_line": [],
        "pos_line": [
            "+'''",
            "+Visualize a embedding matrix.",
            "+",
            "+Args:",
            "+emb (torch.tensor): Embedding matrix with shape (N, D). D is the",
            "+feature dimension.",
            "+dir (str): Output directory for the embedding figure.",
            "+",
            "+'''"
        ],
        "core_change": "+''' +Visualize a embedding matrix. + +Args: +emb (torch.tensor): Embedding matrix with shape (N, D). D is the +feature dimension. +dir (str): Output directory for the embedding figure. + +'''",
        "core_API": "set_context"
    },
    {
        "commit_hash": "5a1d74ee4d7ead315a3f782bc730fd2d4ae49cbf",
        "index": "3163e97f..fc357dc5 100644",
        "commit_message": "Add a new intro tutorial (#2991)\n\n* Add a new language introduction tutorial derived from the Bayesian regression tutorials\n\n* Address coments\n\n* Address comments 2\n\n* Address more comments, add intro and conclusion\n\n* Fix some conclusion text and links\n\n* smoke test\n\n* Address comments, add table of contents\n\n* Fix seaborn warnings\n\n* tweak text and fix ylim\n\n* nits\n\n* rename\n\n* remove all references to old intro tutorials\n\n* update index\n\n* Add notes about deprecation to old intros\n\n* fix code links\n\n* Fix more sphinx errors\n\n* fix yet another sphinx error...\n\n* add a couple sentences about elbo estimators\n\n* fix weird commit??\n\n* Speed up inference\n\n* Remove obsolete comment\n\n* regenerate plots\n\n* address comments\n\nCo-authored-by: Fritz Obermeyer <fritz.obermeyer@gmail.com>\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "\"# Modules in Pyro\\n\",",
            "\"\\n\",",
            "\"\\n\",",
            "-    \"This tutorial introduces [PyroModule](http://docs.pyro.ai/en/stable/nn.html#pyro.nn.module.PyroModule), Pyro's Bayesian extension of PyTorch's [nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) class. Before starting you should understand the basics of Pyro [models](http://pyro.ai/examples/intro_part_i.html) and [inference](http://pyro.ai/examples/intro_part_ii.html), understand the two primitives [pyro.sample()](http://docs.pyro.ai/en/stable/primitives.html#pyro.primitives.sample) and [pyro.param()](http://docs.pyro.ai/en/stable/primitives.html#pyro.primitives.param), and understand the basics of Pyro's [effect handlers](http://pyro.ai/examples/effect_handlers.html) (e.g. by browsing [minipyro.py](https://github.com/pyro-ppl/pyro/blob/dev/pyro/contrib/minipyro.py)).\\n\",",
            "+    \"This tutorial introduces [PyroModule](http://docs.pyro.ai/en/stable/nn.html#pyro.nn.module.PyroModule), Pyro's Bayesian extension of PyTorch's [nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) class. Before starting you should understand the basics of Pyro [models and inference](http://pyro.ai/examples/intro_long.html), understand the two primitives [pyro.sample()](http://docs.pyro.ai/en/stable/primitives.html#pyro.primitives.sample) and [pyro.param()](http://docs.pyro.ai/en/stable/primitives.html#pyro.primitives.param), and understand the basics of Pyro's [effect handlers](http://pyro.ai/examples/effect_handlers.html) (e.g. by browsing [minipyro.py](https://github.com/pyro-ppl/pyro/blob/dev/pyro/contrib/minipyro.py)).\\n\",",
            "\"\\n\",",
            "\"#### Summary:\\n\",",
            "\"\\n\","
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"This tutorial introduces [PyroModule](http://docs.pyro.ai/en/stable/nn.html#pyro.nn.module.PyroModule), Pyro's Bayesian extension of PyTorch's [nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) class. Before starting you should understand the basics of Pyro [models](http://pyro.ai/examples/intro_part_i.html) and [inference](http://pyro.ai/examples/intro_part_ii.html), understand the two primitives [pyro.sample()](http://docs.pyro.ai/en/stable/primitives.html#pyro.primitives.sample) and [pyro.param()](http://docs.pyro.ai/en/stable/primitives.html#pyro.primitives.param), and understand the basics of Pyro's [effect handlers](http://pyro.ai/examples/effect_handlers.html) (e.g. by browsing [minipyro.py](https://github.com/pyro-ppl/pyro/blob/dev/pyro/contrib/minipyro.py)).\\n\"), value='\"This tutorial introduces [PyroModule](http://docs.pyro.ai/en/stable/nn.html#pyro.nn.module.PyroModule), Pyro\\'s Bayesian extension of PyTorch\\'s [nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) class. Before starting you should understand the basics of Pyro [models and inference](http://pyro.ai/examples/intro_long.html), understand the two primitives [pyro.sample()](http://docs.pyro.ai/en/stable/primitives.html#pyro.primitives.sample) and [pyro.param()](http://docs.pyro.ai/en/stable/primitives.html#pyro.primitives.param), and understand the basics of Pyro\\'s [effect handlers](http://pyro.ai/examples/effect_handlers.html) (e.g. by browsing [minipyro.py](https://github.com/pyro-ppl/pyro/blob/dev/pyro/contrib/minipyro.py)).\\\\n\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 5527,
        "neg_line": [
            "-\"This tutorial introduces [PyroModule](http://docs.pyro.ai/en/stable/nn.html#pyro.nn.module.PyroModule), Pyro's Bayesian extension of PyTorch's [nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) class. Before starting you should understand the basics of Pyro [models](http://pyro.ai/examples/intro_part_i.html) and [inference](http://pyro.ai/examples/intro_part_ii.html), understand the two primitives [pyro.sample()](http://docs.pyro.ai/en/stable/primitives.html#pyro.primitives.sample) and [pyro.param()](http://docs.pyro.ai/en/stable/primitives.html#pyro.primitives.param), and understand the basics of Pyro's [effect handlers](http://pyro.ai/examples/effect_handlers.html) (e.g. by browsing [minipyro.py](https://github.com/pyro-ppl/pyro/blob/dev/pyro/contrib/minipyro.py)).\\n\","
        ],
        "pos_line": [
            "+\"This tutorial introduces [PyroModule](http://docs.pyro.ai/en/stable/nn.html#pyro.nn.module.PyroModule), Pyro's Bayesian extension of PyTorch's [nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) class. Before starting you should understand the basics of Pyro [models and inference](http://pyro.ai/examples/intro_long.html), understand the two primitives [pyro.sample()](http://docs.pyro.ai/en/stable/primitives.html#pyro.primitives.sample) and [pyro.param()](http://docs.pyro.ai/en/stable/primitives.html#pyro.primitives.param), and understand the basics of Pyro's [effect handlers](http://pyro.ai/examples/effect_handlers.html) (e.g. by browsing [minipyro.py](https://github.com/pyro-ppl/pyro/blob/dev/pyro/contrib/minipyro.py)).\\n\","
        ],
        "core_change": "-\"This tutorial introduces [PyroModule](http://docs.pyro.ai/en/stable/nn.html#pyro.nn.module.PyroModule), Pyro's Bayesian extension of PyTorch's [nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) class. Before starting you should understand the basics of Pyro [models](http://pyro.ai/examples/intro_part_i.html) and [inference](http://pyro.ai/examples/intro_part_ii.html), understand the two primitives [pyro.sample()](http://docs.pyro.ai/en/stable/primitives.html#pyro.primitives.sample) and [pyro.param()](http://docs.pyro.ai/en/stable/primitives.html#pyro.primitives.param), and understand the basics of Pyro's [effect handlers](http://pyro.ai/examples/effect_handlers.html) (e.g. by browsing [minipyro.py](https://github.com/pyro-ppl/pyro/blob/dev/pyro/contrib/minipyro.py)).\\n\", +\"This tutorial introduces [PyroModule](http://docs.pyro.ai/en/stable/nn.html#pyro.nn.module.PyroModule), Pyro's Bayesian extension of PyTorch's [nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) class. Before starting you should understand the basics of Pyro [models and inference](http://pyro.ai/examples/intro_long.html), understand the two primitives [pyro.sample()](http://docs.pyro.ai/en/stable/primitives.html#pyro.primitives.sample) and [pyro.param()](http://docs.pyro.ai/en/stable/primitives.html#pyro.primitives.param), and understand the basics of Pyro's [effect handlers](http://pyro.ai/examples/effect_handlers.html) (e.g. by browsing [minipyro.py](https://github.com/pyro-ppl/pyro/blob/dev/pyro/contrib/minipyro.py)).\\n\",",
        "core_API": "sample"
    },
    {
        "commit_hash": "78f5fe1416e74a2225e162e349cb8a53f1d39212",
        "index": "487e178b2..dcb3aa6d0 100644",
        "commit_message": "[Deepspeed] adapt multiple models, add zero_to_fp32 tests (#12477)\n\n* zero_to_fp32 tests\n\n* args change\n\n* remove unnecessary work\n\n* use transformers.trainer_utils.get_last_checkpoint\n\n* document the new features\n\n* cleanup\n\n* wip\n\n* fix fsmt\n\n* add bert\n\n* cleanup\n\n* add xlm-roberta\n\n* electra works\n\n* cleanup\n\n* sync\n\n* split off the model zoo tests\n\n* cleanup\n\n* cleanup\n\n* cleanup\n\n* cleanup\n\n* reformat\n\n* cleanup\n\n* casing\n\n* deepspeed>=0.4.3\n\n* adjust distilbert\n\n* Update docs/source/main_classes/deepspeed.rst\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n\n* style\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TrainingArguments:",
            "device = torch.device(\"cuda\", self.local_rank)",
            "self._n_gpu = 1",
            "elif self.deepspeed:",
            "-            # deepspeed performs its own DDP internally, and requires the program to be started with:",
            "-            # deepspeed  ./program.py",
            "-            # rather than:",
            "-            # python -m torch.distributed.launch --nproc_per_node=2 ./program.py",
            "+            # deepspeed inits torch.distributed internally",
            "from .deepspeed import is_deepspeed_available",
            "",
            "if not is_deepspeed_available():"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 4,
        "AST_diff_line": 17,
        "number": 5530,
        "neg_line": [
            "-# deepspeed performs its own DDP internally, and requires the program to be started with:",
            "-# deepspeed  ./program.py",
            "-# rather than:",
            "-# python -m torch.distributed.launch --nproc_per_node=2 ./program.py"
        ],
        "pos_line": [
            "+# deepspeed inits torch.distributed internally"
        ],
        "core_change": "-# deepspeed performs its own DDP internally, and requires the program to be started with: -# deepspeed  ./program.py -# rather than: -# python -m torch.distributed.launch --nproc_per_node=2 ./program.py +# deepspeed inits torch.distributed internally",
        "core_API": "device"
    },
    {
        "commit_hash": "da3aceb876e0f92b65405a20198836c08ee80d10",
        "index": "ff6c2431..30c6fb49 100644",
        "commit_message": "Add fp16 support (#963)\n\n* Update imgwarp.py\n\n* update fix\n\n* move _torch_inverse_cast under utils\n\n* fix some tests in pyramid\n\n* fix issues\n\n* implement other autocasting functions\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def _compute_luts(tiles_x_im: torch.Tensor, num_bins: int = 256, clip: float = 4",
            "histos: torch.Tensor = torch.empty((tiles.shape[0], num_bins), device=tiles.device)",
            "if not diff:",
            "for i in range(tiles.shape[0]):",
            "-            histos[i] = torch.histc(tiles[i], bins=num_bins, min=0, max=1)",
            "+            histos[i] = _torch_histc_cast(tiles[i], bins=num_bins, min=0, max=1)",
            "else:",
            "bins: torch.Tensor = torch.linspace(0, 1, num_bins, device=tiles.device)",
            "histos = histogram(tiles, bins, torch.tensor(0.001)).squeeze()"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=torch), value='_torch_histc_cast')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=torch), position=0)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=histc))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 5532,
        "neg_line": [
            "-histos[i] = torch.histc(tiles[i], bins=num_bins, min=0, max=1)"
        ],
        "pos_line": [
            "+histos[i] = _torch_histc_cast(tiles[i], bins=num_bins, min=0, max=1)"
        ],
        "core_change": "-histos[i] = torch.histc(tiles[i], bins=num_bins, min=0, max=1) +histos[i] = _torch_histc_cast(tiles[i], bins=num_bins, min=0, max=1)",
        "core_API": "empty"
    },
    {
        "commit_hash": "a74cf7ff1c71b32b84f11bc094853eeb135218f9",
        "index": "1b70225c2..8c0cdb2eb 100644",
        "commit_message": "[Train] Torch Prepare utilities (#20254)\n\n* update\n\n* formatting\n\n* fix failures\n\n* fix session tests\n\n* address comments\n\n* add to api docs\n\n* package refactor\n\n* wip\n\n* wip\n\n* wip\n\n* finish\n\n* finish\n\n* fix\n\n* comment\n\n* fix\n\n* install horovod for docs\n\n* address comment\n\n* Update python/ray/train/session.py\n\nCo-authored-by: matthewdeng <matthew.j.deng@gmail.com>\n\n* Update python/ray/train/torch.py\n\nCo-authored-by: matthewdeng <matthew.j.deng@gmail.com>\n\n* address comments\n\n* try fix docs\n\n* fix doc build failure\n\n* fix\n\n* fix\n\n* fix\n\n* try fix doc highlighting\n\n* fix docs\n\nCo-authored-by: matthewdeng <matthew.j.deng@gmail.com>\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def train_func():",
            "",
            "# __torch_distributed_begin__",
            "",
            "-from torch.nn.parallel import DistributedDataParallel",
            "+from ray import train",
            "",
            "def train_func_distributed():",
            "num_epochs = 3",
            "model = NeuralNetwork()",
            "-    model = DistributedDataParallel(model)",
            "+    model = train.torch.prepare_model(model)",
            "loss_fn = nn.MSELoss()",
            "optimizer = optim.SGD(model.parameters(), lr=0.1)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=torch), value='ray')",
            "Update(target_node=ASTNode(type=identifier, text=DistributedDataParallel), value='train')",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1114529)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1114530)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1114531)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'prepare_model'), position=2, insert_id=1114532)",
            "Update(target_node=ASTNode(type=identifier, text=DistributedDataParallel), value='train')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=DistributedDataParallel), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1114533)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=2, insert_id=1114534)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=nn))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=parallel))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 14,
        "number": 5533,
        "neg_line": [
            "-from torch.nn.parallel import DistributedDataParallel",
            "-model = DistributedDataParallel(model)"
        ],
        "pos_line": [
            "+from ray import train",
            "+model = train.torch.prepare_model(model)"
        ],
        "core_change": "-from torch.nn.parallel import DistributedDataParallel +from ray import train -model = DistributedDataParallel(model) +model = train.torch.prepare_model(model)",
        "core_API": "prepare_model"
    },
    {
        "commit_hash": "68427c9bebd1e4ff43d25b18bb9c7eb786303712",
        "index": "af650e75e..5e95cc3f3 100644",
        "commit_message": "Fixing slow pipeline tests (#14260)\n\n* Fiixng slow pipeline tests\n\n* Remove the image-segmentaiton override.\n\n* Fixing clamping only in training.\n\n* Wav2vec2.\n\n* Remove last mention of `no_grad`.\n\n* Fixing copies.\n\n* Rename.\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DetrEncoderLayer(nn.Module):",
            "hidden_states = residual + hidden_states",
            "hidden_states = self.final_layer_norm(hidden_states)",
            "",
            "-        if torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any():",
            "-            clamp_value = torch.finfo(hidden_states.dtype).max - 1000",
            "-            hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)",
            "+        if self.training:",
            "+            if torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any():",
            "+                clamp_value = torch.finfo(hidden_states.dtype).max - 1000",
            "+                hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)",
            "",
            "outputs = (hidden_states,)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=if_statement), node=('if', 'if'), position=0, insert_id=1209805)",
            "Insert(target_node=ASTNode(type=if_statement), node=('attribute', None), position=1, insert_id=1209806)",
            "Insert(target_node=ASTNode(type=if_statement), node=(':', ':'), position=2, insert_id=1209807)",
            "Insert(target_node=ASTNode(type=if_statement), node=('block', None), position=3, insert_id=1209808)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=1209809)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1209810)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'training'), position=2, insert_id=1209811)",
            "Move(target_node=IN(type=block), node=ASTNode(type=if_statement), position=0)"
        ],
        "plus_line": 4,
        "minus_line": 3,
        "AST_diff_line": 8,
        "number": 5535,
        "neg_line": [
            "-if torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any():",
            "-clamp_value = torch.finfo(hidden_states.dtype).max - 1000",
            "-hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)"
        ],
        "pos_line": [
            "+if self.training:",
            "+if torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any():",
            "+clamp_value = torch.finfo(hidden_states.dtype).max - 1000",
            "+hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)"
        ],
        "core_change": "-if torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any(): -clamp_value = torch.finfo(hidden_states.dtype).max - 1000 -hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value) +if self.training: +if torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any(): +clamp_value = torch.finfo(hidden_states.dtype).max - 1000 +hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)",
        "core_API": "final_layer_norm"
    },
    {
        "commit_hash": "e28d15b33aa4434b04d8dd16427b802d6035ea22",
        "index": "54d245c5..fbba0d89 100644",
        "commit_message": "Fix some criterion code after recent API change (fixes #1866) (#1874)\n\nSummary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/1874\n\nReviewed By: ngoyal2707\n\nDifferential Revision: D20565473\n\nPulled By: myleott\n\nfbshipit-source-id: 25edef9f41f28a41f1c573dcc3da48b674b678e4\n\n",
        "file": "fairseq.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class WSCTask(FairseqTask):",
            "prefix + leading_space + txt + trailing_space + suffix,",
            "append_eos=True,",
            ")",
            "-        mask = torch.zeros_like(toks, dtype=torch.uint8)",
            "+        mask = torch.zeros_like(toks, dtype=torch.bool)",
            "mask_start = len(self.binarize(prefix))",
            "mask_size = len(self.binarize(leading_space + txt))",
            "mask[mask_start:mask_start + mask_size] = 1"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=uint8), value='bool')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 5536,
        "neg_line": [
            "-mask = torch.zeros_like(toks, dtype=torch.uint8)"
        ],
        "pos_line": [
            "+mask = torch.zeros_like(toks, dtype=torch.bool)"
        ],
        "core_change": "-mask = torch.zeros_like(toks, dtype=torch.uint8) +mask = torch.zeros_like(toks, dtype=torch.bool)",
        "core_API": "zeros_like"
    },
    {
        "commit_hash": "38343366cc2825f13cab04d1402b3d87f7df9eff",
        "index": "559988f..1eca59f 100644",
        "commit_message": "Fix grid index to prvent deprication warning\n\n",
        "file": "PyTorch-YOLOv3.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class YOLOLayer(nn.Module):",
            "",
            "@staticmethod",
            "def _make_grid(nx=20, ny=20):",
            "-        yv, xv = torch.meshgrid([torch.arange(ny), torch.arange(nx)])",
            "+        yv, xv = torch.meshgrid([torch.arange(ny), torch.arange(nx)], indexing='ij')",
            "return torch.stack((xv, yv), 2).view((1, 1, ny, nx, 2)).float()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=904998)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=904999)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'indexing'), position=0, insert_id=905000)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=905001)",
            "Insert(target_node=IN(type=keyword_argument), node=('string', \"'ij'\"), position=2, insert_id=905002)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 5537,
        "neg_line": [
            "-yv, xv = torch.meshgrid([torch.arange(ny), torch.arange(nx)])"
        ],
        "pos_line": [
            "+yv, xv = torch.meshgrid([torch.arange(ny), torch.arange(nx)], indexing='ij')"
        ],
        "core_change": "-yv, xv = torch.meshgrid([torch.arange(ny), torch.arange(nx)]) +yv, xv = torch.meshgrid([torch.arange(ny), torch.arange(nx)], indexing='ij')",
        "core_API": "meshgrid"
    },
    {
        "commit_hash": "842845cd2356e0a263b3ab309078fdbbc7834c25",
        "index": "f03306e1f..1ca7a9a08 100644",
        "commit_message": "embed sized setting in default lm bug fix\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class RNNLM(nn.Module):",
            "self.embed = nn.Embedding(n_vocab, n_embed)",
            "if typ == \"lstm\":",
            "self.rnn = nn.ModuleList(",
            "-                [nn.LSTMCell(n_embed, n_units)] + [nn.LSTMCell(n_units, n_units) for _ in range(n_layers - 1)]",
            "-                )",
            "+                [nn.LSTMCell(n_embed, n_units)] + [nn.LSTMCell(n_units, n_units) for _ in range(n_layers - 1)])",
            "else:",
            "self.rnn = nn.ModuleList(",
            "-                [nn.GRUCell(n_embed, n_units)] + [nn.GRUCell(n_units, n_units) for _ in range(n_layers - 1)]",
            "-                )",
            "+                [nn.GRUCell(n_embed, n_units)] + [nn.GRUCell(n_units, n_units) for _ in range(n_layers - 1)])",
            "",
            "self.dropout = nn.ModuleList(",
            "[nn.Dropout(dropout_rate) for _ in range(n_layers + 1)])"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 4,
        "AST_diff_line": 17,
        "number": 5541,
        "neg_line": [
            "-[nn.LSTMCell(n_embed, n_units)] + [nn.LSTMCell(n_units, n_units) for _ in range(n_layers - 1)]",
            "-)",
            "-[nn.GRUCell(n_embed, n_units)] + [nn.GRUCell(n_units, n_units) for _ in range(n_layers - 1)]",
            "-)"
        ],
        "pos_line": [
            "+[nn.LSTMCell(n_embed, n_units)] + [nn.LSTMCell(n_units, n_units) for _ in range(n_layers - 1)])",
            "+[nn.GRUCell(n_embed, n_units)] + [nn.GRUCell(n_units, n_units) for _ in range(n_layers - 1)])"
        ],
        "core_change": "-[nn.LSTMCell(n_embed, n_units)] + [nn.LSTMCell(n_units, n_units) for _ in range(n_layers - 1)] -) +[nn.LSTMCell(n_embed, n_units)] + [nn.LSTMCell(n_units, n_units) for _ in range(n_layers - 1)]) -[nn.GRUCell(n_embed, n_units)] + [nn.GRUCell(n_units, n_units) for _ in range(n_layers - 1)] -) +[nn.GRUCell(n_embed, n_units)] + [nn.GRUCell(n_units, n_units) for _ in range(n_layers - 1)])",
        "core_API": "Embedding"
    },
    {
        "commit_hash": "073f6eac22357dbc8ed3f2c55f292a82c7ab25d9",
        "index": "ccbaa9ad..7b2030d4 100644",
        "commit_message": "potential fix for embeddings no loading on AMD cards\n\n",
        "file": "stable-diffusion-webui.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class StableDiffusionModelHijack:",
            "if len(emb.shape) == 1:",
            "emb = emb.unsqueeze(0)",
            "",
            "-            self.word_embeddings[name] = emb.detach()",
            "+            self.word_embeddings[name] = emb.detach().to(device)",
            "self.word_embeddings_checksums[name] = f'{const_hash(emb.reshape(-1)*100)&0xffff:04x}'",
            "",
            "ids = tokenizer([name], add_special_tokens=False)['input_ids'][0]"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1142015)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=1142016)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1142017)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to'), position=2, insert_id=1142018)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1142019)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'device'), position=1, insert_id=1142020)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1142021)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 5542,
        "neg_line": [
            "-self.word_embeddings[name] = emb.detach()"
        ],
        "pos_line": [
            "+self.word_embeddings[name] = emb.detach().to(device)"
        ],
        "core_change": "-self.word_embeddings[name] = emb.detach() +self.word_embeddings[name] = emb.detach().to(device)",
        "core_API": "unsqueeze"
    },
    {
        "commit_hash": "7ec2223c843e923bc002d11e72dcff72ab3149d3",
        "index": "ab7cbeb46d..519365f0e3 100644",
        "commit_message": "[RLlib] DDPG PyTorch actor-model was missing sigmoid layer (#8188)\n\nFix DDPG PyTorch (missing sigmoid layer (to squash action outputs) after deterministic action outputs).\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "keras",
        "change": [
            "def ddpg_actor_critic_loss(policy, model, _, train_batch):",
            "def make_ddpg_optimizers(policy, config):",
            "# Create separate optimizers for actor & critic losses.",
            "policy._actor_optimizer = torch.optim.Adam(",
            "-        params=policy.model.policy_variables(), lr=config[\"actor_lr\"])",
            "+        params=policy.model.policy_variables(),",
            "+        lr=config[\"actor_lr\"],",
            "+        eps=1e-7)  # to match tf.keras.optimizers.Adam's epsilon default",
            "policy._critic_optimizer = torch.optim.Adam(",
            "-        params=policy.model.q_variables(), lr=config[\"critic_lr\"])",
            "+        params=policy.model.q_variables(), lr=config[\"critic_lr\"],",
            "+        eps=1e-7)  # to match tf.keras.optimizers.Adam's epsilon default",
            "return policy._actor_optimizer, policy._critic_optimizer"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2547371)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2547372)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2547373)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Adam'), position=2, insert_id=2547374)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=2547375)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=2547376)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=2547377)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=2547378)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=2547379)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2547380)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'optim'), position=2, insert_id=2547381)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'eps'), position=0, insert_id=2547382)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2547383)",
            "Insert(target_node=IN(type=keyword_argument), node=('float', '1e-7'), position=2, insert_id=2547384)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'eps'), position=0, insert_id=2547385)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2547386)",
            "Insert(target_node=IN(type=keyword_argument), node=('float', '1e-7'), position=2, insert_id=2547387)",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=optim))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=Adam))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 5,
        "minus_line": 2,
        "AST_diff_line": 25,
        "number": 5545,
        "neg_line": [
            "-params=policy.model.policy_variables(), lr=config[\"actor_lr\"])",
            "-params=policy.model.q_variables(), lr=config[\"critic_lr\"])"
        ],
        "pos_line": [
            "+params=policy.model.policy_variables(),",
            "+lr=config[\"actor_lr\"],",
            "+eps=1e-7)  # to match tf.keras.optimizers.Adam's epsilon default",
            "+params=policy.model.q_variables(), lr=config[\"critic_lr\"],",
            "+eps=1e-7)  # to match tf.keras.optimizers.Adam's epsilon default"
        ],
        "core_change": "-params=policy.model.policy_variables(), lr=config[\"actor_lr\"]) +params=policy.model.policy_variables(), +lr=config[\"actor_lr\"], +eps=1e-7)  # to match tf.keras.optimizers.Adam's epsilon default -params=policy.model.q_variables(), lr=config[\"critic_lr\"]) +params=policy.model.q_variables(), lr=config[\"critic_lr\"], +eps=1e-7)  # to match tf.keras.optimizers.Adam's epsilon default",
        "core_API": "Adam"
    },
    {
        "commit_hash": "8d051d7bf2742e758f53cfb83d50ccc47f926d14",
        "index": "96c42188..7e2904ee 100755",
        "commit_message": "Saver/summaries/distributed handling updated to using MonitoredSession, hooks, etc; custom save problems fixed; added saver/summary/distributed_spec config entries; added batched_observe config entry to address performance problem; modified episode/timestep counter handling in runner; various other and related fixes\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class LayerBasedNetwork(Network):",
            "return network_variables + layer_variables",
            "",
            "def get_summaries(self):",
            "-        return super(LayerBasedNetwork, self).get_summaries() + \\",
            "-            [summary for layer in self.layers for summary in layer.get_summaries()]",
            "+        network_summaries = super(LayerBasedNetwork, self).get_summaries()",
            "+        layer_summaries = [summary for layer in self.layers for summary in layer.get_summaries()]",
            "+",
            "+        return network_summaries + layer_summaries",
            "",
            "",
            "class LayeredNetwork(LayerBasedNetwork):"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=block), node=('expression_statement', None), position=0, insert_id=2240053)",
            "Insert(target_node=ASTNode(type=block), node=('expression_statement', None), position=1, insert_id=2240054)",
            "Insert(target_node=ASTNode(type=block), node=('return_statement', None), position=2, insert_id=2240055)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=2240056)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=2240057)",
            "Insert(target_node=IN(type=return_statement), node=('return', 'return'), position=0, insert_id=2240058)",
            "Insert(target_node=IN(type=return_statement), node=('binary_operator', None), position=1, insert_id=2240059)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'network_summaries'), position=0, insert_id=2240060)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=2240061)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=call), position=2)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'layer_summaries'), position=0, insert_id=2240062)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=2240063)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=list_comprehension), position=2)",
            "Insert(target_node=IN(type=binary_operator), node=('identifier', 'network_summaries'), position=0, insert_id=2240064)",
            "Insert(target_node=IN(type=binary_operator), node=('+', '+'), position=1, insert_id=2240065)",
            "Insert(target_node=IN(type=binary_operator), node=('identifier', 'layer_summaries'), position=2, insert_id=2240066)",
            "Delete(target_node=ASTNode(type=return, text=return))",
            "Delete(target_node=ASTNode(type=+, text=+))",
            "Delete(target_node=ASTNode(type=binary_operator))",
            "Delete(target_node=ASTNode(type=return_statement))"
        ],
        "plus_line": 3,
        "minus_line": 2,
        "AST_diff_line": 20,
        "number": 5546,
        "neg_line": [
            "-return super(LayerBasedNetwork, self).get_summaries() + \\",
            "-[summary for layer in self.layers for summary in layer.get_summaries()]"
        ],
        "pos_line": [
            "+network_summaries = super(LayerBasedNetwork, self).get_summaries()",
            "+layer_summaries = [summary for layer in self.layers for summary in layer.get_summaries()]",
            "+",
            "+return network_summaries + layer_summaries"
        ],
        "core_change": "-return super(LayerBasedNetwork, self).get_summaries() + \\ -[summary for layer in self.layers for summary in layer.get_summaries()] +network_summaries = super(LayerBasedNetwork, self).get_summaries() +layer_summaries = [summary for layer in self.layers for summary in layer.get_summaries()] + +return network_summaries + layer_summaries",
        "core_API": "get_summaries"
    },
    {
        "commit_hash": "ef2a6088ff5bc515be27e24f5f6c8b16b96d3bb8",
        "index": "d96578b59..64fa483d3 100644",
        "commit_message": "Drop support for PyTorch 1.10 (#16492)\n\n* Drop support for PyTorch 1.10\n\n* CHANGELOG\n\n* READMEs\n\n* mypy\n\n* ls\n\n* New poplar version\n\n* Fixed tests\n\n* links\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* skip azure badges\n\n* Table\n\n* Matching dockerfiles\n\n* Drop unnecessary channels and packages\n\n* Push nightly\n\n* Undo unrelated changes\n\n* Revert \"Push nightly\"\n\nThis reverts commit 9618f737c4dc65331fef4bb11fe46a61513d220a.\n\n---------\n\nCo-authored-by: Jirka <jirka.borovec@seznam.cz>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_sharded_tensor_state_dict(single_process_pg):",
            "",
            "m_0 = BoringModelWithShardedTensor(spec)",
            "m_0.sharded_tensor.local_shards()[0].tensor.fill_(1)",
            "-    name_st = \".sharded_tensor\" if _TORCH_GREATER_EQUAL_1_11 and not _TORCH_GREATER_EQUAL_1_13 else \"sharded_tensor\"",
            "+    name_st = \".sharded_tensor\" if not _TORCH_GREATER_EQUAL_1_13 else \"sharded_tensor\"",
            "assert name_st in m_0.state_dict(), 'Expect \"sharded_tensor\" to appear in the state dict'",
            "",
            "m_1 = BoringModelWithShardedTensor(spec)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=ASTNode(type=conditional_expression), node=ASTNode(type=not_operator), position=2)",
            "Delete(target_node=ASTNode(type=identifier, text=_TORCH_GREATER_EQUAL_1_11))",
            "Delete(target_node=ASTNode(type=and, text=and))",
            "Delete(target_node=ASTNode(type=boolean_operator))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 5548,
        "neg_line": [
            "-name_st = \".sharded_tensor\" if _TORCH_GREATER_EQUAL_1_11 and not _TORCH_GREATER_EQUAL_1_13 else \"sharded_tensor\""
        ],
        "pos_line": [
            "+name_st = \".sharded_tensor\" if not _TORCH_GREATER_EQUAL_1_13 else \"sharded_tensor\""
        ],
        "core_change": "-name_st = \".sharded_tensor\" if _TORCH_GREATER_EQUAL_1_11 and not _TORCH_GREATER_EQUAL_1_13 else \"sharded_tensor\" +name_st = \".sharded_tensor\" if not _TORCH_GREATER_EQUAL_1_13 else \"sharded_tensor\"",
        "core_API": "local_shards"
    },
    {
        "commit_hash": "c7fd1d9fb18862318b133c9474d37c5085850070",
        "index": "00166e69..51912215 100644",
        "commit_message": "fix deprecations about casting & initializers in tf1.13\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class CascadeRCNNHead(object):",
            "labels_per_box = tf.gather(self.gt_labels, best_iou_ind)",
            "fg_mask = max_iou_per_box >= iou_threshold",
            "fg_inds_wrt_gt = tf.boolean_mask(best_iou_ind, fg_mask)",
            "-                labels_per_box = tf.stop_gradient(labels_per_box * tf.to_int64(fg_mask))",
            "+                labels_per_box = tf.stop_gradient(labels_per_box * tf.cast(fg_mask, tf.int64))",
            "return BoxProposals(boxes, labels_per_box, fg_inds_wrt_gt)",
            "else:",
            "return BoxProposals(boxes)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=to_int64), value='cast')",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=2278872)",
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=3, insert_id=2278873)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2278874)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2278875)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'int64'), position=2, insert_id=2278876)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 5549,
        "neg_line": [
            "-labels_per_box = tf.stop_gradient(labels_per_box * tf.to_int64(fg_mask))"
        ],
        "pos_line": [
            "+labels_per_box = tf.stop_gradient(labels_per_box * tf.cast(fg_mask, tf.int64))"
        ],
        "core_change": "-labels_per_box = tf.stop_gradient(labels_per_box * tf.to_int64(fg_mask)) +labels_per_box = tf.stop_gradient(labels_per_box * tf.cast(fg_mask, tf.int64))",
        "core_API": "gather"
    },
    {
        "commit_hash": "95b6d985ffb231d9a2047c039c95660913063b9d",
        "index": "eb90fe21..47eeda0d 100644",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class MaskedLayerNorm(torch.nn.Module):",
            "self.size = size",
            "self.eps = eps",
            "",
            "-    def forward(self, tensor: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:",
            "+    def forward(self, tensor: torch.Tensor, mask: torch.BoolTensor) -> torch.Tensor:",
            "",
            "-        broadcast_mask = mask.unsqueeze(-1).float()",
            "+        broadcast_mask = mask.unsqueeze(-1)",
            "num_elements = broadcast_mask.sum() * self.size",
            "mean = (tensor * broadcast_mask).sum() / num_elements",
            "masked_centered = (tensor - mean) * broadcast_mask"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=call), position=2)",
            "Update(target_node=ASTNode(type=identifier, text=Tensor), value='BoolTensor')",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=float))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 9,
        "number": 5552,
        "neg_line": [
            "-def forward(self, tensor: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:",
            "-broadcast_mask = mask.unsqueeze(-1).float()"
        ],
        "pos_line": [
            "+def forward(self, tensor: torch.Tensor, mask: torch.BoolTensor) -> torch.Tensor:",
            "+broadcast_mask = mask.unsqueeze(-1)"
        ],
        "core_change": "-def forward(self, tensor: torch.Tensor, mask: torch.Tensor) -> torch.Tensor: +def forward(self, tensor: torch.Tensor, mask: torch.BoolTensor) -> torch.Tensor: -broadcast_mask = mask.unsqueeze(-1).float() +broadcast_mask = mask.unsqueeze(-1)",
        "core_API": "unsqueeze"
    },
    {
        "commit_hash": "368630afaba985ee9c26cc0aadd1c56c42bb38a6",
        "index": "5bb5a3adb..5e69516da 100644",
        "commit_message": "Make an `ObjectStore` something a `Worker` has (instead of is) (#3484)\n\n* [WIP] Make an `ObjectStore` something a `Worker` has (instead of is)\n\n* Update tests to look for objects in `worker.object_store._objects`\n\n* Fix references to `_tensors` and `rm_obj()`\n\n* Remove `owner` attribute from `ObjectStorage`\n\n* Revert \"Remove `owner` attribute from `ObjectStorage`\"\n\nThis reverts commit 7d687fbaebb402b889c6ed90a837e514d305e258.\n\n* Fix loose ends with setting `owner` attribute\n\n* Fix `ObjectStorage` tests\n\n* Fix `test_share_get`\n\n* Fix the Udacity backwards compatibility tests\n\n* Clean up `AbstractObject.tag()`\n\n* Fix `get_obj` references in `FederatedClient`\n\n* Replace direct access to `_objects` with `get_obj()`\n\n* Fix tag registration\n\n* Remove stray comment\n\n* Use `clear_objects()` method instead of setting to `{}`\n\n* Add a missing `NodeClient` import to `AbstractGrid`\n\n* Rename `ObjectStorage` to `ObjectStore`\n\n* Tone down triple exclamation points in error messages\n\n* Remove unnecessary guard clause and exception from `Object.tag()`\n\n* Clean up and expand the Udacity course compatibility comments\n\n* Add `get_obj` to `FederatedClient`\n",
        "file": "PySyft.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_set_obj_takes_ownership(workers):",
            "",
            "me.set_obj(x)",
            "",
            "-    objs = me._objects",
            "+    objs = me.object_store._objects",
            "",
            "assert objs[x.id] == x",
            "assert objs[x.id].owner == workers[\"me\"]"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=793243)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=793244)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=me), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'object_store'), position=2, insert_id=793245)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 5554,
        "neg_line": [
            "-objs = me._objects"
        ],
        "pos_line": [
            "+objs = me.object_store._objects"
        ],
        "core_change": "-objs = me._objects +objs = me.object_store._objects",
        "core_API": "set_obj"
    },
    {
        "commit_hash": "5fe263b9287864bd6f658fab0dfcd00346ac85ab",
        "index": "350645e..f327c11 100644",
        "commit_message": "Improve snt.Embed performance in distributed training.\n\nAdds a fix to avoid excess computation on parameter servers.\n\nPiperOrigin-RevId: 180674688\n\n",
        "file": "sonnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Embed(base.AbstractModule):",
            "regularizer=self._regularizers.get(self.EMBEDDINGS, None),",
            "trainable=self._trainable)",
            "",
            "-    # On the backwards pass, we want to convert the gradient from",
            "-    # indexed-slices to a regular tensor before sending it back to the",
            "-    # parameter server. This avoids excess computation on the parameter server.",
            "-",
            "-    embeddings = util.convert_gradient_to_tensor(self._embeddings)",
            "-",
            "# Lookup embeddings",
            "-    return tf.nn.embedding_lookup(embeddings, ids, name=\"embedding_lookup\")",
            "+    return tf.nn.embedding_lookup(",
            "+        self._embeddings, ids, name=\"embedding_lookup\")",
            "",
            "@property",
            "def vocab_size(self):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=module), node=ASTNode(type=ERROR), position=2)",
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=identifier, text=trainable), position=0)",
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type==, text==), position=1)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=attribute), position=1)",
            "Delete(target_node=ASTNode(type=identifier, text=embeddings))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=util))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=convert_gradient_to_tensor))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=identifier, text=embeddings))"
        ],
        "plus_line": 2,
        "minus_line": 5,
        "AST_diff_line": 18,
        "number": 5556,
        "neg_line": [
            "-# On the backwards pass, we want to convert the gradient from",
            "-# indexed-slices to a regular tensor before sending it back to the",
            "-# parameter server. This avoids excess computation on the parameter server.",
            "-",
            "-embeddings = util.convert_gradient_to_tensor(self._embeddings)",
            "-",
            "-return tf.nn.embedding_lookup(embeddings, ids, name=\"embedding_lookup\")"
        ],
        "pos_line": [
            "+return tf.nn.embedding_lookup(",
            "+self._embeddings, ids, name=\"embedding_lookup\")"
        ],
        "core_change": "-# On the backwards pass, we want to convert the gradient from -# indexed-slices to a regular tensor before sending it back to the -# parameter server. This avoids excess computation on the parameter server. - -embeddings = util.convert_gradient_to_tensor(self._embeddings) - -return tf.nn.embedding_lookup(embeddings, ids, name=\"embedding_lookup\") +return tf.nn.embedding_lookup( +self._embeddings, ids, name=\"embedding_lookup\")",
        "core_API": "get"
    },
    {
        "commit_hash": "809043f5abb19109ef63775770dd615e57b2cce8",
        "index": "28329c0..cef255b 100644",
        "commit_message": "[contrib] Fix the reference implementation of multihead_attn (#1423)\n\n* follow the current signature\n\nSigned-off-by: Masaki Kozuki <mkozuki@nvidia.com>\n\n* call .backward on outputs\n\nSigned-off-by: Masaki Kozuki <mkozuki@nvidia.com>\n\n* update the other caller of _softmax_backward_data\n\nSigned-off-by: Masaki Kozuki <mkozuki@nvidia.com>\n",
        "file": "apex.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class EncdecAttnFunc(torch.autograd.Function):",
            "dropout_grads = torch._masked_scale(matmul2_dgrad1, dropout_mask, 1.0 / (1.0 - dropout_prob_t[0]))",
            "",
            "# Softmax Grad (not a publically documented op)",
            "-        softmax_grads = torch._softmax_backward_data(dropout_grads, softmax_results, -1, softmax_results)",
            "+        softmax_grads = torch._softmax_backward_data(dropout_grads, softmax_results, -1, softmax_results.dtype)",
            "",
            "# Matmul1 - DGRAD1",
            "# Input1: (data grads)  [seqs*heads, seql_q, seql_k]"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=7, insert_id=49596)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=softmax_results), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=49597)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dtype'), position=2, insert_id=49598)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 5557,
        "neg_line": [
            "-softmax_grads = torch._softmax_backward_data(dropout_grads, softmax_results, -1, softmax_results)"
        ],
        "pos_line": [
            "+softmax_grads = torch._softmax_backward_data(dropout_grads, softmax_results, -1, softmax_results.dtype)"
        ],
        "core_change": "-softmax_grads = torch._softmax_backward_data(dropout_grads, softmax_results, -1, softmax_results) +softmax_grads = torch._softmax_backward_data(dropout_grads, softmax_results, -1, softmax_results.dtype)",
        "core_API": "_masked_scale"
    },
    {
        "commit_hash": "e257bd7278138a89d1a7287edbc2e0e498654689",
        "index": "9e467ef8..f2b6fcb0 100644",
        "commit_message": "bug fix loss\n\n",
        "file": "TTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "functional",
        "change": [
            "class L1LossMasked(nn.Module):",
            "# target_flat: (batch * max_len, dim)",
            "target_flat = target.view(-1, target.shape[-1])",
            "# losses_flat: (batch * max_len, dim)",
            "-        losses_flat = functional.l1_loss(input, target, size_average=False,",
            "+        losses_flat = functional.l1_loss(input, target_flat, size_average=False,",
            "reduce=False)",
            "# losses: (batch, max_len, dim)",
            "losses = losses_flat.view(*target.size())",
            "+",
            "# mask: (batch, max_len, 1)",
            "mask = _sequence_mask(sequence_length=length,",
            "max_len=target.size(1)).unsqueeze(2)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=target), value='target_flat')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 5558,
        "neg_line": [
            "-losses_flat = functional.l1_loss(input, target, size_average=False,"
        ],
        "pos_line": [
            "+losses_flat = functional.l1_loss(input, target_flat, size_average=False,",
            "+"
        ],
        "core_change": "-losses_flat = functional.l1_loss(input, target, size_average=False, +losses_flat = functional.l1_loss(input, target_flat, size_average=False, +",
        "core_API": "view"
    },
    {
        "commit_hash": "a0d60a648b7ad20c95d9f34a7ce5ee4925308d58",
        "index": "7a8cf392..e5209f2a 100644",
        "commit_message": "Fix build and tests of zmq op (#362)\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def compile():",
            "# https://github.com/uber/horovod/blob/10835d25eccf4b198a23a0795edddf0896f6563d/horovod/tensorflow/mpi_ops.py#L30-L40",
            "def get_ext_suffix():",
            "\"\"\"Determine library extension for various versions of Python.\"\"\"",
            "+    return '.so'    # TODO",
            "ext_suffix = sysconfig.get_config_var('EXT_SUFFIX')",
            "if ext_suffix:",
            "return ext_suffix"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('return_statement', None), position=3, insert_id=2292363)",
            "Insert(target_node=IN(type=return_statement), node=('return', 'return'), position=0, insert_id=2292364)",
            "Insert(target_node=IN(type=return_statement), node=('string', \"'.so'\"), position=1, insert_id=2292365)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 3,
        "number": 5564,
        "neg_line": [],
        "pos_line": [
            "+return '.so'    # TODO"
        ],
        "core_change": "+return '.so'    # TODO",
        "core_API": "get_config_var"
    },
    {
        "commit_hash": "7ae5a2cb5a1f7c81043eda2a1d9faae4efbf51c8",
        "index": "aba40a2f0..b22b65036 100644",
        "commit_message": "fix more cuda bugs in RL tutorial\n\n",
        "file": "tutorials.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "for i_episode in range(num_episodes):",
            "break",
            "",
            "print('Complete')",
            "+env.render(close=True)",
            "env.close()",
            "plt.ioff()",
            "plt.show()"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=1277185)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=1277186)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1277187)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1277188)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'env'), position=0, insert_id=1277189)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1277190)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'render'), position=2, insert_id=1277191)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1277192)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=1, insert_id=1277193)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1277194)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'close'), position=0, insert_id=1277195)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1277196)",
            "Insert(target_node=IN(type=keyword_argument), node=('true', 'True'), position=2, insert_id=1277197)"
        ],
        "plus_line": 0,
        "minus_line": 0,
        "AST_diff_line": 13,
        "number": 5566,
        "neg_line": [],
        "pos_line": [
            "+env.render(close=True)"
        ],
        "core_change": "+env.render(close=True)",
        "core_API": "render"
    },
    {
        "commit_hash": "9578680a53dff5dc4fb8afaff4eb0e4b379a759f",
        "index": "164700b..b8cdb22 100644",
        "commit_message": "Expose VQ VAE modules to as snt.nets.VectorQuantizer{EMA}\n\nAdded goldens.\n\nAlso fixed checkpoint_test & saved_model_test to support nested output.\n\nPiperOrigin-RevId: 256566131\nChange-Id: Ic26fe1785741ebac9b4a6a790c3585a64fd2f882\n\n",
        "file": "sonnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class SavedModelTest(test_utils.TestCase, parameterized.TestCase):",
            "if golden.deterministic:",
            "# The output from both the saved and restored model should be close.",
            "y1 = saved_model.inference(x)",
            "-      self.assertAllEqual(y1, y2)",
            "+      tf.nest.map_structure(self.assertAllEqual, y1, y2)",
            "",
            "for a, b in zip(v1, v2):",
            "self.assertEqual(a.name, b.name)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2183216)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2183217)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2183218)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'map_structure'), position=2, insert_id=2183219)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=attribute), position=1)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=2183220)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2183221)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2183222)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'nest'), position=2, insert_id=2183223)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 5568,
        "neg_line": [
            "-self.assertAllEqual(y1, y2)"
        ],
        "pos_line": [
            "+tf.nest.map_structure(self.assertAllEqual, y1, y2)"
        ],
        "core_change": "-self.assertAllEqual(y1, y2) +tf.nest.map_structure(self.assertAllEqual, y1, y2)",
        "core_API": "inference"
    },
    {
        "commit_hash": "edb78974b84b324df08ec198e6ff7f267ee23d33",
        "index": "3627e21d..1fd9c6be 100644",
        "commit_message": "upgrade detection / some v2 compat fix\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def maskrcnn_upXconv_head(feature, num_category, num_convs, norm=None):",
            "with argscope([Conv2D, Conv2DTranspose], data_format='channels_first',",
            "kernel_initializer=tf.variance_scaling_initializer(",
            "scale=2.0, mode='fan_out',",
            "-                      distribution='untruncated_normal' if get_tf_version_tuple() >= (1, 12) else 'normal')):",
            "+                      distribution='untruncated_normal')):",
            "# c2's MSRAFill is fan_out",
            "for k in range(num_convs):",
            "l = Conv2D('fcn{}'.format(k), l, cfg.MRCNN.HEAD_DIM, 3, activation=tf.nn.relu)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=ASTNode(type=keyword_argument), node=ASTNode(type=string, text='untruncated_normal'), position=2)",
            "Delete(target_node=ASTNode(type=if, text=if))",
            "Delete(target_node=ASTNode(type=identifier, text=get_tf_version_tuple))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=>=, text=>=))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=integer, text=12))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=tuple))",
            "Delete(target_node=ASTNode(type=comparison_operator))",
            "Delete(target_node=ASTNode(type=else, text=else))",
            "Delete(target_node=ASTNode(type=string, text='normal'))",
            "Delete(target_node=ASTNode(type=conditional_expression))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 18,
        "number": 5570,
        "neg_line": [
            "-distribution='untruncated_normal' if get_tf_version_tuple() >= (1, 12) else 'normal')):"
        ],
        "pos_line": [
            "+distribution='untruncated_normal')):"
        ],
        "core_change": "-distribution='untruncated_normal' if get_tf_version_tuple() >= (1, 12) else 'normal')): +distribution='untruncated_normal')):",
        "core_API": "variance_scaling_initializer"
    },
    {
        "commit_hash": "c1aaa439350051acdcd585946e91525502a6b063",
        "index": "9b3b3a153..9696db6a8 100644",
        "commit_message": "[Doctests] Move doctests to new GPU & Fix bugs (#15969)\n\n* test\n\n* up\n\n* up\n\n* Empty test commit\n\n* up\n\n* update tests\n\n* up\n\n* fix some vision models\n\n* correct\n\n* correct docs\n\n* Trigger notification\n\n* finalize\n\n* check\n\n* correct quicktour\n\n* Apply suggestions from code review\n\n* improve doctests\n\n* Trigger Build\n\n* next try\n\n* next try\n\n* and again\n\n* Output current clone information\n\n* Output current clone information\n\n* Correct path\n\n* add tf round again\n\n* revert to daily job\n\nCo-authored-by: Lysandre <lysandre.debut@reseau.eseo.fr>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DeiTForImageClassification(DeiTPreTrainedModel):",
            ">>> # model predicts one of the 1000 ImageNet classes",
            ">>> predicted_class_idx = logits.argmax(-1).item()",
            ">>> print(\"Predicted class:\", model.config.id2label[predicted_class_idx])",
            "+        Predicted class: maillot",
            "```\"\"\"",
            "return_dict = return_dict if return_dict is not None else self.config.use_return_dict"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=5, insert_id=1204973)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1204974)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'Predicted'), position=0, insert_id=1204975)",
            "Insert(target_node=IN(type=assignment), node=('ERROR', None), position=1, insert_id=1204976)",
            "Insert(target_node=IN(type=assignment), node=(':', ':'), position=2, insert_id=1204977)",
            "Insert(target_node=IN(type=assignment), node=('type', None), position=3, insert_id=1204978)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'class'), position=0, insert_id=1204979)",
            "Insert(target_node=IN(type=type), node=('identifier', 'maillot'), position=0, insert_id=1204980)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 8,
        "number": 5572,
        "neg_line": [],
        "pos_line": [
            "+Predicted class: maillot"
        ],
        "core_change": "+Predicted class: maillot",
        "core_API": "argmax"
    },
    {
        "commit_hash": "1b1901e4ef49cd6d338a5059dafedf2defa2f103",
        "index": "e68b8438..e6088af3 100644",
        "commit_message": "fix shift_rgb stack dimension (#1930)\n\n* fix shift_rgb stack dimension\n\n* update RandomRGBShift tests\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def shift_rgb(image: torch.Tensor, r_shift: torch.Tensor, g_shift: torch.Tensor,",
            "",
            "shifts = [r_shift, g_shift, b_shift]",
            "",
            "-    shifted = (image + torch.stack(shifts).view(-1, 3, 1, 1).to(image)).clamp_(min=0, max=1)",
            "+    shifted = (image + torch.stack(shifts, dim=1).view(-1, 3, 1, 1).to(image)).clamp_(min=0, max=1)",
            "",
            "return shifted"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=398791)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=398792)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dim'), position=0, insert_id=398793)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=398794)",
            "Insert(target_node=IN(type=keyword_argument), node=('integer', '1'), position=2, insert_id=398795)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 5574,
        "neg_line": [
            "-shifted = (image + torch.stack(shifts).view(-1, 3, 1, 1).to(image)).clamp_(min=0, max=1)"
        ],
        "pos_line": [
            "+shifted = (image + torch.stack(shifts, dim=1).view(-1, 3, 1, 1).to(image)).clamp_(min=0, max=1)"
        ],
        "core_change": "-shifted = (image + torch.stack(shifts).view(-1, 3, 1, 1).to(image)).clamp_(min=0, max=1) +shifted = (image + torch.stack(shifts, dim=1).view(-1, 3, 1, 1).to(image)).clamp_(min=0, max=1)",
        "core_API": "stack"
    },
    {
        "commit_hash": "65e93458d4630ab467872508304c115b670f9000",
        "index": "fc5e5e21..6fd61831 100644",
        "commit_message": "fix AdaLAM crash (#1881)\n\n* fix AdaLAM crash\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class AdalamFilter:",
            "\"Please either provide orientations or set 'orientation_difference_threshold' to None to disable orientations filtering\"  # noqa: E501",
            ")",
            "k1, k2, d1, d2, o1, o2, s1, s2 = self.__to_torch(k1, k2, d1, d2, o1, o2, s1, s2)",
            "+        if len(d2) <= 1:",
            "+            return _no_match(d1)",
            "distmat = dist_matrix(d1, d2, is_normalized=False)",
            "dd12, nn12 = torch.topk(distmat, k=2, dim=1, largest=False)  # (n1, 2)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=4, insert_id=401092)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=401093)",
            "Insert(target_node=IN(type=if_statement), node=('comparison_operator', None), position=1, insert_id=401094)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=401095)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=401096)",
            "Insert(target_node=IN(type=comparison_operator), node=('call', None), position=0, insert_id=401097)",
            "Insert(target_node=IN(type=comparison_operator), node=('<=', '<='), position=1, insert_id=401098)",
            "Insert(target_node=IN(type=comparison_operator), node=('integer', '1'), position=2, insert_id=401099)",
            "Insert(target_node=IN(type=block), node=('return_statement', None), position=0, insert_id=401100)",
            "Insert(target_node=IN(type=call), node=('identifier', 'len'), position=0, insert_id=401101)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=401102)",
            "Insert(target_node=IN(type=return_statement), node=('return', 'return'), position=0, insert_id=401103)",
            "Insert(target_node=IN(type=return_statement), node=('call', None), position=1, insert_id=401104)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=401105)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'd2'), position=1, insert_id=401106)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=401107)",
            "Insert(target_node=IN(type=call), node=('identifier', '_no_match'), position=0, insert_id=401108)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=401109)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=401110)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'd1'), position=1, insert_id=401111)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=401112)"
        ],
        "plus_line": 2,
        "minus_line": 0,
        "AST_diff_line": 21,
        "number": 5579,
        "neg_line": [],
        "pos_line": [
            "+if len(d2) <= 1:",
            "+return _no_match(d1)"
        ],
        "core_change": "+if len(d2) <= 1: +return _no_match(d1)",
        "core_API": "__to_torch"
    },
    {
        "commit_hash": "d01dc9e22d5e8625ae6ac49e2e689eebf472b5f8",
        "index": "27206adaf2..00d6575e62 100644",
        "commit_message": "[rllib] format with yapf (#2427)\n\n* initial yapf\n\n* manual fix yapf bugs\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Model(object):",
            "self.outputs, self.last_layer = self._build_layers(",
            "inputs, num_outputs, options)",
            "if options.get(\"free_log_std\", False):",
            "-            log_std = tf.get_variable(name=\"log_std\", shape=[num_outputs],",
            "-                                      initializer=tf.zeros_initializer)",
            "+            log_std = tf.get_variable(",
            "+                name=\"log_std\",",
            "+                shape=[num_outputs],",
            "+                initializer=tf.zeros_initializer)",
            "self.outputs = tf.concat(",
            "[self.outputs, 0.0 * self.outputs + log_std], 1)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 4,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 5582,
        "neg_line": [
            "-log_std = tf.get_variable(name=\"log_std\", shape=[num_outputs],",
            "-initializer=tf.zeros_initializer)"
        ],
        "pos_line": [
            "+log_std = tf.get_variable(",
            "+name=\"log_std\",",
            "+shape=[num_outputs],",
            "+initializer=tf.zeros_initializer)"
        ],
        "core_change": "-log_std = tf.get_variable(name=\"log_std\", shape=[num_outputs], -initializer=tf.zeros_initializer) +log_std = tf.get_variable( +name=\"log_std\", +shape=[num_outputs], +initializer=tf.zeros_initializer)",
        "core_API": "_build_layers"
    },
    {
        "commit_hash": "993a187c6ff03cd971c71ad234d1ddfb3beb020a",
        "index": "065d0fb8c..137e99a67 100755",
        "commit_message": "fix device in longformer onnx path (#20419)\n\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class LongformerSelfAttention(nn.Module):",
            "hidden_states.size(2),",
            "]",
            "",
            "-        overlapping_chunks = torch.empty(chunk_size)",
            "+        overlapping_chunks = torch.empty(chunk_size, device=hidden_states.device)",
            "for chunk in range(chunk_size[1]):",
            "overlapping_chunks[:, chunk, :, :] = hidden_states[",
            ":, chunk * window_overlap : chunk * window_overlap + 2 * window_overlap, :"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1183526)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1183527)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=1183528)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1183529)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=1183530)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'hidden_states'), position=0, insert_id=1183531)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1183532)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=1183533)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 5583,
        "neg_line": [
            "-overlapping_chunks = torch.empty(chunk_size)"
        ],
        "pos_line": [
            "+overlapping_chunks = torch.empty(chunk_size, device=hidden_states.device)"
        ],
        "core_change": "-overlapping_chunks = torch.empty(chunk_size) +overlapping_chunks = torch.empty(chunk_size, device=hidden_states.device)",
        "core_API": "size"
    },
    {
        "commit_hash": "2d458b2c7d6fb1dd5b2361938d1b5bd4c2106479",
        "index": "e5413be45..b3441e293 100644",
        "commit_message": "ConvBERT fix torch <> tf weights conversion (#10314)\n\n* convbert conversion test\n\n* fin\n\n* fin\n\n* fin\n\n* clean up tf<->pt conversion\n\n* remove from_pt\n\nCo-authored-by: patrickvonplaten <patrick.v.platen@gmail.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class GroupedLinearLayer(tf.keras.layers.Layer):",
            "def call(self, hidden_states):",
            "batch_size = shape_list(hidden_states)[0]",
            "x = tf.transpose(tf.reshape(hidden_states, [-1, self.num_groups, self.group_in_dim]), [1, 0, 2])",
            "-        x = tf.matmul(x, self.kernel)",
            "+        x = tf.matmul(x, tf.transpose(self.kernel, [2, 1, 0]))",
            "x = tf.transpose(x, [1, 0, 2])",
            "x = tf.reshape(x, [batch_size, -1, self.output_size])",
            "x = tf.nn.bias_add(value=x, bias=self.bias)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=3, insert_id=2371425)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=4, insert_id=2371426)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2371427)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2371428)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2371429)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2371430)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'transpose'), position=2, insert_id=2371431)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2371432)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=attribute), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=2371433)",
            "Insert(target_node=IN(type=argument_list), node=('list', None), position=3, insert_id=2371434)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=), text=)), position=4)",
            "Insert(target_node=IN(type=list), node=('[', '['), position=0, insert_id=2371435)",
            "Insert(target_node=IN(type=list), node=('integer', '2'), position=1, insert_id=2371436)",
            "Insert(target_node=IN(type=list), node=(',', ','), position=2, insert_id=2371437)",
            "Insert(target_node=IN(type=list), node=('integer', '1'), position=3, insert_id=2371438)",
            "Insert(target_node=IN(type=list), node=(',', ','), position=4, insert_id=2371439)",
            "Insert(target_node=IN(type=list), node=('integer', '0'), position=5, insert_id=2371440)",
            "Insert(target_node=IN(type=list), node=(']', ']'), position=6, insert_id=2371441)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 19,
        "number": 5590,
        "neg_line": [
            "-x = tf.matmul(x, self.kernel)"
        ],
        "pos_line": [
            "+x = tf.matmul(x, tf.transpose(self.kernel, [2, 1, 0]))"
        ],
        "core_change": "-x = tf.matmul(x, self.kernel) +x = tf.matmul(x, tf.transpose(self.kernel, [2, 1, 0]))",
        "core_API": "transpose"
    },
    {
        "commit_hash": "df4637b3d26dcab13f7237b9958458bf952c2cfd",
        "index": "660d99f221..bb6c04d22a 100644",
        "commit_message": "`vecdot()` fix for torch backend.\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def vecdot(",
            "out: Optional[torch.Tensor] = None,",
            ") -> torch.Tensor:",
            "dtype = torch.promote_types(x1.dtype, x2.dtype)",
            "-    x1, x2 = x1.type(torch.float32), x2.type(torch.float32)",
            "-    return torch.tensordot(x1, x2, dims=([axis], [axis]), out=out).type(dtype)",
            "+    x1, x2 = x1.to(torch.float32), x2.to(torch.float32)",
            "+    return torch.tensordot(x1, x2, dims=([axis], [axis]), out=out)",
            "",
            "",
            "vecdot.unsupported_dtypes = ("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=return_statement), node=ASTNode(type=call), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=type), value='to')",
            "Update(target_node=ASTNode(type=identifier, text=type), value='to')",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=type))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=dtype))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 11,
        "number": 5593,
        "neg_line": [
            "-x1, x2 = x1.type(torch.float32), x2.type(torch.float32)",
            "-return torch.tensordot(x1, x2, dims=([axis], [axis]), out=out).type(dtype)"
        ],
        "pos_line": [
            "+x1, x2 = x1.to(torch.float32), x2.to(torch.float32)",
            "+return torch.tensordot(x1, x2, dims=([axis], [axis]), out=out)"
        ],
        "core_change": "-x1, x2 = x1.type(torch.float32), x2.type(torch.float32) -return torch.tensordot(x1, x2, dims=([axis], [axis]), out=out).type(dtype) +x1, x2 = x1.to(torch.float32), x2.to(torch.float32) +return torch.tensordot(x1, x2, dims=([axis], [axis]), out=out)",
        "core_API": "promote_types"
    },
    {
        "commit_hash": "709cc2e206f384bfacc6f2732203c3d37f09b228",
        "index": "51b6951d..51578a67 100644",
        "commit_message": "[Fix] Missing device initializations (#672)\n\n* Fix device initialization in several variables\n\n* Fix device initialization in several variables\n\n* Skip CUDA tests for #666\n\n* Lint\n\n* Fix device initialization in several variables\n\n* Skip CUDA tests for #666\n\n* Lint\n\n* Remove half-precision tests (#649) and add a jit test\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestRandomCutMix:",
            "",
            "assert_allclose(out_image, expected, rtol=1e-4, atol=1e-4)",
            "assert (out_label[:, :, 0] == label).all()",
            "-        assert (out_label[:, :, 1] == torch.tensor([[1, 0], [1, 0], [1, 0], [1, 0], [0, 1]])).all()",
            "+        assert (out_label[:, :, 1] == torch.tensor([[1, 0], [1, 0], [1, 0], [1, 0], [0, 1]], device=device)).all()",
            "assert_allclose(out_label[:, :, 2], torch.tensor([[0., 0.], [0., 0.], [0., 0.0833], [0., 0.], [0.5, 0.3333]],",
            "device=device, dtype=dtype), rtol=1e-4, atol=1e-4)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=436878)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=436879)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=436880)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=436881)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=2, insert_id=436882)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 5595,
        "neg_line": [
            "-assert (out_label[:, :, 1] == torch.tensor([[1, 0], [1, 0], [1, 0], [1, 0], [0, 1]])).all()"
        ],
        "pos_line": [
            "+assert (out_label[:, :, 1] == torch.tensor([[1, 0], [1, 0], [1, 0], [1, 0], [0, 1]], device=device)).all()"
        ],
        "core_change": "-assert (out_label[:, :, 1] == torch.tensor([[1, 0], [1, 0], [1, 0], [1, 0], [0, 1]])).all() +assert (out_label[:, :, 1] == torch.tensor([[1, 0], [1, 0], [1, 0], [1, 0], [0, 1]], device=device)).all()",
        "core_API": "tensor"
    },
    {
        "commit_hash": "f33a6f34461fea61b579a7ec732fcd174b2b41cd",
        "index": "c0781a294..9f58fabe5 100644",
        "commit_message": "[TFGPT2] - Fix flaky past_key_values test (#9460)\n\n* fix tf flakey\n\n* remove test files\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFGPT2ModelTester:",
            "",
            "# create hypothetical next token and extent to next_input_ids",
            "next_tokens = ids_tensor((self.batch_size, 3), config.vocab_size)",
            "-        next_token_types = ids_tensor((self.batch_size, 3), self.type_vocab_size)",
            "next_attn_mask = ids_tensor((self.batch_size, 3), 2)",
            "+        next_token_types = ids_tensor((self.batch_size, 3), self.type_vocab_size)",
            "",
            "# append to next input_ids and token_type_ids",
            "next_input_ids = tf.concat([input_ids, next_tokens], axis=-1)",
            "-        next_token_type_ids = tf.concat([token_type_ids, next_token_types], axis=-1)",
            "next_attention_mask = tf.concat([input_mask, next_attn_mask], axis=-1)",
            "+        next_token_type_ids = tf.concat([token_type_ids, next_token_types], axis=-1)",
            "",
            "output_from_no_past = model(",
            "next_input_ids, token_type_ids=next_token_type_ids, attention_mask=next_attention_mask"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=expression_statement), node=ASTNode(type=module), position=2)",
            "Move(target_node=ASTNode(type=expression_statement), node=ASTNode(type=module), position=6)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 5597,
        "neg_line": [
            "-next_token_types = ids_tensor((self.batch_size, 3), self.type_vocab_size)",
            "-next_token_type_ids = tf.concat([token_type_ids, next_token_types], axis=-1)"
        ],
        "pos_line": [
            "+next_token_types = ids_tensor((self.batch_size, 3), self.type_vocab_size)",
            "+next_token_type_ids = tf.concat([token_type_ids, next_token_types], axis=-1)"
        ],
        "core_change": "-next_token_types = ids_tensor((self.batch_size, 3), self.type_vocab_size) +next_token_types = ids_tensor((self.batch_size, 3), self.type_vocab_size) -next_token_type_ids = tf.concat([token_type_ids, next_token_types], axis=-1) +next_token_type_ids = tf.concat([token_type_ids, next_token_types], axis=-1)",
        "core_API": "concat"
    },
    {
        "commit_hash": "c7a6039bbff6fe0aa06b3985e6ffa858bce9a7ee",
        "index": "975a88e..29d8759 100644",
        "commit_message": "minor fix\n\n",
        "file": "deep-voice-conversion.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Model:",
            "return ppgs, preds_ppg, logits_ppg, pred_spec, pred_mel",
            "",
            "def loss_net2(self):",
            "-        loss_spec = tf.reduce_mean(tf.abs(self.pred_spec - self.y_spec))",
            "-        loss_mel = tf.reduce_mean(tf.abs(self.pred_mel - self.y_mel))",
            "+        loss_spec = tf.reduce_mean(tf.squared_difference(self.pred_spec, self.y_spec))",
            "+        loss_mel = tf.reduce_mean(tf.squared_difference(self.pred_mel, self.y_mel))",
            "loss = loss_spec + loss_mel",
            "return loss"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1920462)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=1920463)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1920464)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'reduce_mean'), position=2, insert_id=1920465)",
            "Update(target_node=ASTNode(type=identifier, text=abs), value='squared_difference')",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=attribute), position=1)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1920466)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=attribute), position=3)",
            "Update(target_node=ASTNode(type=identifier, text=abs), value='squared_difference')",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=attribute), position=1)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1920467)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=attribute), position=3)",
            "Delete(target_node=ASTNode(type=-, text=-))",
            "Delete(target_node=ASTNode(type=binary_operator))",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=reduce_mean))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=-, text=-))",
            "Delete(target_node=ASTNode(type=binary_operator))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 21,
        "number": 5598,
        "neg_line": [
            "-loss_spec = tf.reduce_mean(tf.abs(self.pred_spec - self.y_spec))",
            "-loss_mel = tf.reduce_mean(tf.abs(self.pred_mel - self.y_mel))"
        ],
        "pos_line": [
            "+loss_spec = tf.reduce_mean(tf.squared_difference(self.pred_spec, self.y_spec))",
            "+loss_mel = tf.reduce_mean(tf.squared_difference(self.pred_mel, self.y_mel))"
        ],
        "core_change": "-loss_spec = tf.reduce_mean(tf.abs(self.pred_spec - self.y_spec)) -loss_mel = tf.reduce_mean(tf.abs(self.pred_mel - self.y_mel)) +loss_spec = tf.reduce_mean(tf.squared_difference(self.pred_spec, self.y_spec)) +loss_mel = tf.reduce_mean(tf.squared_difference(self.pred_mel, self.y_mel))",
        "core_API": "reduce_mean"
    },
    {
        "commit_hash": "cdf037e2d6d71816be9bd90749a612507a4f2fbd",
        "index": "79ceeea8..00aa55f8 100644",
        "commit_message": "version initiatlizer fix\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class NormalizedAdvantageFunctions(ValueFunction):",
            "'outputs_target')",
            "self.create_training_operations()",
            "self.saver = tf.train.Saver()",
            "-        self.session.run(tf.initialize_all_variables())",
            "+        self.session.run(tf.tf.global_variables_initializer())",
            "",
            "def get_action(self, state, episode=1, total_states=0):",
            "\"\"\""
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=2248596)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=2248597)",
            "Insert(target_node=ASTNode(type=attribute), node=('identifier', 'global_variables_initializer'), position=2, insert_id=2248598)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=tf), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=initialize_all_variables), value='tf')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=initialize_all_variables), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 5600,
        "neg_line": [
            "-self.session.run(tf.initialize_all_variables())"
        ],
        "pos_line": [
            "+self.session.run(tf.tf.global_variables_initializer())"
        ],
        "core_change": "-self.session.run(tf.initialize_all_variables()) +self.session.run(tf.tf.global_variables_initializer())",
        "core_API": "create_training_operations"
    },
    {
        "commit_hash": "867a5efa3523fd2bd2fb7705e293f80f4bb15299",
        "index": "eca11fdf..2e2f7d8d 100644",
        "commit_message": "Release 1.8.6rc0 (#660)\n\n* Release 1.8.6rc0\n\n* Update CHANGELOG.md\n\n* Typo Fix\n\n* Changelog Updated\n\n* tl_logging more readable\n\n* Typo Fixed\n\n* https enforced for badges\n\n* RTD badges pointing to Latest Commit\n\n* Decorators API Refactored\n\n* extra_requires `all`, `all_cpu` and `all_gpu` added\n\n* Error fix\n\n* YAPF Formating Correction\n\n* Test for private method decorator added\n\n* Test Logging Verbosity Fixed to Debug when runned individually\n\n* YAPF corrections applied\n\n* Changelog Added\n\n* Changelog updated\n\n* PR number changed\n\n* Changelog updated\n\n* Update .travis.yml\n\n* Update CONTRIBUTING.md\n\n* Update CONTRIBUTING.md\n\n* Update CHANGELOG.md\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Layer_Core_Test(unittest.TestCase):",
            "",
            "if __name__ == '__main__':",
            "",
            "-    # tl.logging.set_verbosity(tl.logging.INFO)",
            "+    tf.logging.set_verbosity(tf.logging.DEBUG)",
            "tl.logging.set_verbosity(tl.logging.DEBUG)",
            "",
            "-    unittest.main()",
            "+    unittest.main()",
            "\\ No newline at end of file"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('block', None), position=0, insert_id=2262394)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=2262395)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=2262396)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2262397)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2262398)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2262399)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2262400)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'set_verbosity'), position=2, insert_id=2262401)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2262402)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=2262403)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2262404)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2262405)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2262406)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'logging'), position=2, insert_id=2262407)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2262408)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2262409)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'DEBUG'), position=2, insert_id=2262410)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2262411)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2262412)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'logging'), position=2, insert_id=2262413)",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 21,
        "number": 5602,
        "neg_line": [
            "-# tl.logging.set_verbosity(tl.logging.INFO)",
            "-unittest.main()"
        ],
        "pos_line": [
            "+tf.logging.set_verbosity(tf.logging.DEBUG)",
            "+unittest.main()"
        ],
        "core_change": "-# tl.logging.set_verbosity(tl.logging.INFO) +tf.logging.set_verbosity(tf.logging.DEBUG) -unittest.main() +unittest.main()",
        "core_API": "set_verbosity"
    },
    {
        "commit_hash": "f6c76230938f8c5cd09ef85913a414c37862b206",
        "index": "ac3c724c..0fca0f71 100644",
        "commit_message": "Add Pre-commit bot fixes (#3722)\n\n* flake8\n\n* flake8\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_polar():",
            "assert len(data) == 3",
            "assert data.pos.tolist() == pos.tolist()",
            "assert data.edge_index.tolist() == edge_index.tolist()",
            "-    assert torch.allclose(",
            "-        data.edge_attr, torch.Tensor([[1, 0], [1, PI]]), atol=1e-04)",
            "+    assert torch.allclose(data.edge_attr, torch.Tensor([[1, 0], [1, PI]]),",
            "+                          atol=1e-04)",
            "",
            "data = Data(edge_index=edge_index, pos=pos, edge_attr=edge_attr)",
            "data = Polar(norm=True)(data)",
            "assert len(data) == 3",
            "assert data.pos.tolist() == pos.tolist()",
            "assert data.edge_index.tolist() == edge_index.tolist()",
            "-    assert torch.allclose(",
            "-        data.edge_attr, torch.Tensor([[1, 1, 0], [1, 1, 0.5]]), atol=1e-04)",
            "+    assert torch.allclose(data.edge_attr,",
            "+                          torch.Tensor([[1, 1, 0], [1, 1, 0.5]]), atol=1e-04)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 4,
        "minus_line": 4,
        "AST_diff_line": 17,
        "number": 5603,
        "neg_line": [
            "-assert torch.allclose(",
            "-data.edge_attr, torch.Tensor([[1, 0], [1, PI]]), atol=1e-04)",
            "-assert torch.allclose(",
            "-data.edge_attr, torch.Tensor([[1, 1, 0], [1, 1, 0.5]]), atol=1e-04)"
        ],
        "pos_line": [
            "+assert torch.allclose(data.edge_attr, torch.Tensor([[1, 0], [1, PI]]),",
            "+atol=1e-04)",
            "+assert torch.allclose(data.edge_attr,",
            "+torch.Tensor([[1, 1, 0], [1, 1, 0.5]]), atol=1e-04)"
        ],
        "core_change": "-assert torch.allclose( -data.edge_attr, torch.Tensor([[1, 0], [1, PI]]), atol=1e-04) +assert torch.allclose(data.edge_attr, torch.Tensor([[1, 0], [1, PI]]), +atol=1e-04) -assert torch.allclose( -data.edge_attr, torch.Tensor([[1, 1, 0], [1, 1, 0.5]]), atol=1e-04) +assert torch.allclose(data.edge_attr, +torch.Tensor([[1, 1, 0], [1, 1, 0.5]]), atol=1e-04)",
        "core_API": "tolist"
    },
    {
        "commit_hash": "3e050bdb4ca4b65ea39c8f70293b3b8ce1c40ed7",
        "index": "5ec6eaa..9f87716 100644",
        "commit_message": "Fix pylint errors\n\n",
        "file": "seq2seq.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "from __future__ import print_function",
            "import tensorflow as tf",
            "",
            "def templatemethod(name_):",
            "+  \"\"\"This decorator wraps a method with `tf.make_template`. For example,",
            "+",
            "+  @templatemethod",
            "+  def my_method():",
            "+    # Create variables",
            "+  \"\"\"",
            "def template_decorator(func):",
            "+    \"\"\"Inner decorator function\"\"\"",
            "def func_wrapper(*args, **kwargs):",
            "-      templated_func =  tf.make_template(name_, func)",
            "+      \"\"\"Inner wrapper function\"\"\"",
            "+      templated_func = tf.make_template(name_, func)",
            "return templated_func(*args, **kwargs)",
            "return func_wrapper",
            "return template_decorator"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=function_definition), node=('block', None), position=4, insert_id=2159341)",
            "Insert(target_node=ASTNode(type=function_definition), node=('block', None), position=4, insert_id=2159342)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=2159343)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=2159344)",
            "Insert(target_node=ASTNode(type=block), node=('expression_statement', None), position=0, insert_id=2159345)",
            "Insert(target_node=IN(type=expression_statement), node=('string', '\"\"\"This decorator wraps a method with `tf.make_template`. For example,\\n\\n  @templatemethod\\n  def my_method():\\n    # Create variables\\n  \"\"\"'), position=0, insert_id=2159346)",
            "Insert(target_node=IN(type=expression_statement), node=('string', '\"\"\"Inner decorator function\"\"\"'), position=0, insert_id=2159347)",
            "Insert(target_node=IN(type=expression_statement), node=('string', '\"\"\"Inner wrapper function\"\"\"'), position=0, insert_id=2159348)",
            "Delete(target_node=ASTNode(type=block, text=))",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 8,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 5614,
        "neg_line": [
            "-templated_func =  tf.make_template(name_, func)"
        ],
        "pos_line": [
            "+\"\"\"This decorator wraps a method with `tf.make_template`. For example,",
            "+",
            "+@templatemethod",
            "+def my_method():",
            "+# Create variables",
            "+\"\"\"",
            "+\"\"\"Inner decorator function\"\"\"",
            "+\"\"\"Inner wrapper function\"\"\"",
            "+templated_func = tf.make_template(name_, func)"
        ],
        "core_change": "+\"\"\"This decorator wraps a method with `tf.make_template`. For example, + +@templatemethod +def my_method(): +# Create variables +\"\"\" +\"\"\"Inner decorator function\"\"\" -templated_func =  tf.make_template(name_, func) +\"\"\"Inner wrapper function\"\"\" +templated_func = tf.make_template(name_, func)",
        "core_API": "make_template"
    },
    {
        "commit_hash": "05cda7e842ebe2ff9b5f45a087a161266357ac86",
        "index": "a83745fc0e..4c903ef0f0 100644",
        "commit_message": "lint fixes\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def pow(",
            "return ret",
            "",
            "",
            "-def round(",
            "-    x: torch.Tensor,",
            "-    *,",
            "-    out: Optional[torch.Tensor] = None",
            "-) -> torch.Tensor:",
            "+def round(x: torch.Tensor, *, out: Optional[torch.Tensor] = None) -> torch.Tensor:",
            "if \"int\" in str(x.dtype):",
            "if ivy.exists(out):",
            "return ivy.inplace_update(out, x)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 0,
        "minus_line": 3,
        "AST_diff_line": 17,
        "number": 5615,
        "neg_line": [
            "-def round(",
            "-x: torch.Tensor,",
            "-*,",
            "-out: Optional[torch.Tensor] = None",
            "-) -> torch.Tensor:"
        ],
        "pos_line": [
            "+def round(x: torch.Tensor, *, out: Optional[torch.Tensor] = None) -> torch.Tensor:"
        ],
        "core_change": "-def round( -x: torch.Tensor, -*, -out: Optional[torch.Tensor] = None -) -> torch.Tensor: +def round(x: torch.Tensor, *, out: Optional[torch.Tensor] = None) -> torch.Tensor:",
        "core_API": "exists"
    },
    {
        "commit_hash": "35ea517d9682d063d3319219029c13c422aa49f1",
        "index": "dc9e73ff32..b31211cdce 100644",
        "commit_message": "fixed wrong fn_tree of test experimental gcd\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "functional",
        "change": [
            "def test_nansum(",
            "",
            "# gcd",
            "@handle_test(",
            "-    fn_tree=\"functional.experimental.nansum\",",
            "+    fn_tree=\"functional.experimental.gcd\",",
            "dtype_and_x=helpers.dtype_and_values(",
            "available_dtypes=helpers.get_dtypes(\"integer\"),",
            "num_arrays=2,"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"functional.experimental.nansum\"), value='\"functional.experimental.gcd\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 5616,
        "neg_line": [
            "-fn_tree=\"functional.experimental.nansum\","
        ],
        "pos_line": [
            "+fn_tree=\"functional.experimental.gcd\","
        ],
        "core_change": "-fn_tree=\"functional.experimental.nansum\", +fn_tree=\"functional.experimental.gcd\",",
        "core_API": "dtype_and_values"
    },
    {
        "commit_hash": "983163494663e24b611f1ba8d5d47a3edc00e2e5",
        "index": "898edb6d..36d558eb 100644",
        "commit_message": "Misc fixes (#2448)\n\nSummary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/2448\n\nReviewed By: ngoyal2707\n\nDifferential Revision: D23011193\n\nPulled By: myleott\n\nfbshipit-source-id: 1a29481707108e4465aca78ec1581fb79f05efba\n\n",
        "file": "fairseq.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Trainer(object):",
            "",
            "def is_consistent(tensor):",
            "max_abs_diff = torch.max(torch.abs(tensor - tensor[0]))",
            "-                return (max_abs_diff / (tensor[0] + 1e-6) < 1e-6).all()",
            "+                return (",
            "+                    not torch.isfinite(tensor).any()",
            "+                    or (max_abs_diff / (tensor[0] + 1e-6) < 1e-6).all()",
            "+                )",
            "",
            "if not is_consistent(self._grad_norm_buf):",
            "pretty_detail = \"\\n\".join("
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=return_statement), node=('parenthesized_expression', None), position=1, insert_id=211711)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('(', '('), position=0, insert_id=211712)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('boolean_operator', None), position=1, insert_id=211713)",
            "Insert(target_node=IN(type=parenthesized_expression), node=(')', ')'), position=2, insert_id=211714)",
            "Insert(target_node=IN(type=boolean_operator), node=('not_operator', None), position=0, insert_id=211715)",
            "Insert(target_node=IN(type=boolean_operator), node=('or', 'or'), position=1, insert_id=211716)",
            "Move(target_node=IN(type=boolean_operator), node=ASTNode(type=call), position=2)",
            "Insert(target_node=IN(type=not_operator), node=('not', 'not'), position=0, insert_id=211717)",
            "Insert(target_node=IN(type=not_operator), node=('call', None), position=1, insert_id=211718)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=211719)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=211720)",
            "Insert(target_node=IN(type=attribute), node=('call', None), position=0, insert_id=211721)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=211722)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'any'), position=2, insert_id=211723)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=211724)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=211725)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=211726)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=211727)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=211728)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=211729)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'isfinite'), position=2, insert_id=211730)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=211731)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'tensor'), position=1, insert_id=211732)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=211733)"
        ],
        "plus_line": 4,
        "minus_line": 1,
        "AST_diff_line": 24,
        "number": 5618,
        "neg_line": [
            "-return (max_abs_diff / (tensor[0] + 1e-6) < 1e-6).all()"
        ],
        "pos_line": [
            "+return (",
            "+not torch.isfinite(tensor).any()",
            "+or (max_abs_diff / (tensor[0] + 1e-6) < 1e-6).all()",
            "+)"
        ],
        "core_change": "-return (max_abs_diff / (tensor[0] + 1e-6) < 1e-6).all() +return ( +not torch.isfinite(tensor).any() +or (max_abs_diff / (tensor[0] + 1e-6) < 1e-6).all() +)",
        "core_API": "max"
    },
    {
        "commit_hash": "f601a3a0da458b6ff451d65b1b92296a3b1735b0",
        "index": "3507ed4..19a7ed1 100644",
        "commit_message": "Add Buildkite CI support (#984)\n\n* Buildkite support\n\nSigned-off-by: Alex Sergeev <alsrgv@users.noreply.github.com>\n\n* Incorporate recent changes\n\nSigned-off-by: Alex Sergeev <alsrgv@users.noreply.github.com>\n\n* Bugfix env variable\n\nSigned-off-by: Alex Sergeev <alsrgv@users.noreply.github.com>\n\n* Improved update-alternatives\n\nSigned-off-by: Alex Sergeev <alsrgv@users.noreply.github.com>\n\n* Move cache cutover to a point right before framework installation\n\nSigned-off-by: Alex Sergeev <alsrgv@users.noreply.github.com>\n\n* Bump build timeout to 20 minutes\n\nSigned-off-by: Alex Sergeev <alsrgv@users.noreply.github.com>\n\n* Increase # of push retries to 5\n\nSigned-off-by: Alex Sergeev <alsrgv@users.noreply.github.com>\n\n* Fix cpu-gpu error tests\n\nSigned-off-by: Alex Sergeev <alsrgv@users.noreply.github.com>\n\n* Bump timeout further\n\nSigned-off-by: Alex Sergeev <alsrgv@users.noreply.github.com>\n\n* Downgrade NCCL to 2.3.7\n\nSigned-off-by: Alex Sergeev <alsrgv@users.noreply.github.com>\n\n* Add more exclusions to TensorFlow mixed mode tests\n\nSigned-off-by: Alex Sergeev <alsrgv@users.noreply.github.com>\n\n* Pin tf-nightly to a version before the breaking change\n\nSigned-off-by: Alex Sergeev <alsrgv@users.noreply.github.com>\n\n* Update cache condition\n\nSigned-off-by: Alex Sergeev <alsrgv@users.noreply.github.com>\n\n* Replace torchvision_nightly with torchvision\n\nSigned-off-by: Alex Sergeev <alsrgv@users.noreply.github.com>\n\n* Add Pillow for torchvision\n\nSigned-off-by: Alex Sergeev <alsrgv@users.noreply.github.com>\n\n* Fix test_horovod_allreduce_multi_gpu\n\nSigned-off-by: Alex Sergeev <alsrgv@users.noreply.github.com>\n\n",
        "file": "horovod.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TfKerasTests(tf.test.TestCase):",
            "initial_epoch=1)",
            "",
            "def test_sparse_as_dense(self):",
            "-        hvd.init()",
            "-",
            "-        with self.test_session() as sess:",
            "+        with self.test_session(config=self.config) as sess:",
            "K.set_session(sess)",
            "",
            "opt = keras.optimizers.RMSprop(lr=0.0001)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=1, insert_id=1946909)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'config'), position=0, insert_id=1946910)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1946911)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=1946912)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=1946913)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1946914)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'config'), position=2, insert_id=1946915)",
            "Delete(target_node=ASTNode(type=identifier, text=hvd))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=init))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 16,
        "number": 5622,
        "neg_line": [
            "-hvd.init()",
            "-",
            "-with self.test_session() as sess:"
        ],
        "pos_line": [
            "+with self.test_session(config=self.config) as sess:"
        ],
        "core_change": "-hvd.init() - -with self.test_session() as sess: +with self.test_session(config=self.config) as sess:",
        "core_API": "init"
    },
    {
        "commit_hash": "6f16e46bdb397bf1898e4d7d9050492924177abd",
        "index": "767de7541..19c377008 100644",
        "commit_message": "Various test fixes (#15068)\n\n\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def sync_ddp(result: Tensor, group: Optional[Any] = None, reduce_op: Optional[Un",
            "",
            "# WA for HPU. HPU doesn't support Long types, forcefully set it to float",
            "if module_available(\"habana_frameworks.torch.utils.library_loader\"):",
            "-        from habana_frameworks.torch.utils.library_loader import is_habana_available",
            "+        from habana_frameworks.torch.utils.library_loader import is_habana_avaialble",
            "",
            "if (",
            "-            is_habana_available()",
            "+            is_habana_avaialble()",
            "and os.environ.get(\"HCCL_DISTRIBUTED_BACKEND\") == \"1\"",
            "and result.type() in (\"torch.LongTensor\", \"torch.hpu.LongTensor\")",
            "):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=is_habana_available), value='is_habana_avaialble')",
            "Update(target_node=ASTNode(type=identifier, text=is_habana_available), value='is_habana_avaialble')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 5626,
        "neg_line": [
            "-from habana_frameworks.torch.utils.library_loader import is_habana_available",
            "-is_habana_available()"
        ],
        "pos_line": [
            "+from habana_frameworks.torch.utils.library_loader import is_habana_avaialble",
            "+is_habana_avaialble()"
        ],
        "core_change": "-from habana_frameworks.torch.utils.library_loader import is_habana_available +from habana_frameworks.torch.utils.library_loader import is_habana_avaialble -is_habana_available() +is_habana_avaialble()",
        "core_API": "get"
    },
    {
        "commit_hash": "d38ad6a17164b9bf07477ceb17ca5c7f09657720",
        "index": "e70f00b4..c9722af2 100644",
        "commit_message": "change dist to torch.distributed to fix bug in assert. (#638)\n\n\n",
        "file": "DeepSpeed.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def mpi_discovery(distributed_port=TORCH_DISTRIBUTED_DEFAULT_PORT, verbose=True)",
            "os.environ['MASTER_PORT']))",
            "",
            "if torch.distributed.is_initialized():",
            "-        assert dist.get_rank() == rank, \"MPI rank {} does not match torch rank {}\".format(rank, dist.get_rank())",
            "-        assert dist.get_world_size() == world_size, \"MPI world size {} does not match torch world size {}\".format(",
            "-            world_size, dist.get_world_size())",
            "+        assert torch.distributed.get_rank() == rank, \"MPI rank {} does not match torch rank {}\".format(rank, dist.get_rank())",
            "+        assert torch.distributed.get_world_size() == world_size, \"MPI world size {} does not match torch world size {}\".format(",
            "+            world_size, torch.distributed.get_world_size())",
            "",
            "",
            "def in_aml():"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=84322)",
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=84323)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=84324)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=84325)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=84326)",
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=84327)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=84328)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=argument_list), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=dist), value='torch')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=dist), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'distributed'), position=2, insert_id=84329)",
            "Update(target_node=ASTNode(type=identifier, text=dist), value='torch')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=dist), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'distributed'), position=2, insert_id=84330)",
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=84331)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=84332)",
            "Update(target_node=ASTNode(type=identifier, text=dist), value='torch')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=dist), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'distributed'), position=2, insert_id=84333)",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 25,
        "number": 5627,
        "neg_line": [
            "-assert dist.get_rank() == rank, \"MPI rank {} does not match torch rank {}\".format(rank, dist.get_rank())",
            "-assert dist.get_world_size() == world_size, \"MPI world size {} does not match torch world size {}\".format(",
            "-world_size, dist.get_world_size())"
        ],
        "pos_line": [
            "+assert torch.distributed.get_rank() == rank, \"MPI rank {} does not match torch rank {}\".format(rank, dist.get_rank())",
            "+assert torch.distributed.get_world_size() == world_size, \"MPI world size {} does not match torch world size {}\".format(",
            "+world_size, torch.distributed.get_world_size())"
        ],
        "core_change": "-assert dist.get_rank() == rank, \"MPI rank {} does not match torch rank {}\".format(rank, dist.get_rank()) -assert dist.get_world_size() == world_size, \"MPI world size {} does not match torch world size {}\".format( -world_size, dist.get_world_size()) +assert torch.distributed.get_rank() == rank, \"MPI rank {} does not match torch rank {}\".format(rank, dist.get_rank()) +assert torch.distributed.get_world_size() == world_size, \"MPI world size {} does not match torch world size {}\".format( +world_size, torch.distributed.get_world_size())",
        "core_API": "is_initialized"
    },
    {
        "commit_hash": "3f19c1b8d4d77577a9b65bc21aa5bff9d0f9299a",
        "index": "d6cd0526..ab309227 100644",
        "commit_message": "fix global_step scope problem\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def get_global_step_var():",
            "scope = tf.get_variable_scope()",
            "assert scope.name == '', \\",
            "\"Creating global_step_var under a variable scope would cause problems!\"",
            "-        var = tf.Variable(",
            "-            0, trainable=False, name=GLOBAL_STEP_OP_NAME)",
            "+        var = tf.get_variable(GLOBAL_STEP_OP_NAME, shape=[],",
            "+                initializer=tf.constant_initializer(), trainable=False)",
            "return var",
            "",
            "def get_global_step():"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=keyword_argument), node=ASTNode(type=argument_list), position=4)",
            "Update(target_node=ASTNode(type=identifier, text=Variable), value='get_variable')",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'GLOBAL_STEP_OP_NAME'), position=1, insert_id=2311739)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=4, insert_id=2311740)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=8, insert_id=2311741)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'shape'), position=0, insert_id=2311742)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2311743)",
            "Insert(target_node=IN(type=keyword_argument), node=('list', None), position=2, insert_id=2311744)",
            "Update(target_node=ASTNode(type=identifier, text=name), value='initializer')",
            "Insert(target_node=ASTNode(type=keyword_argument), node=('call', None), position=2, insert_id=2311745)",
            "Insert(target_node=IN(type=list), node=('[', '['), position=0, insert_id=2311746)",
            "Insert(target_node=IN(type=list), node=(']', ']'), position=1, insert_id=2311747)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2311748)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2311749)",
            "Update(target_node=ASTNode(type=identifier, text=GLOBAL_STEP_OP_NAME), value='tf')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=GLOBAL_STEP_OP_NAME), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2311750)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'constant_initializer'), position=2, insert_id=2311751)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2311752)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=2311753)",
            "Delete(target_node=ASTNode(type=integer, text=0))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 21,
        "number": 5628,
        "neg_line": [
            "-var = tf.Variable(",
            "-0, trainable=False, name=GLOBAL_STEP_OP_NAME)"
        ],
        "pos_line": [
            "+var = tf.get_variable(GLOBAL_STEP_OP_NAME, shape=[],",
            "+initializer=tf.constant_initializer(), trainable=False)"
        ],
        "core_change": "-var = tf.Variable( -0, trainable=False, name=GLOBAL_STEP_OP_NAME) +var = tf.get_variable(GLOBAL_STEP_OP_NAME, shape=[], +initializer=tf.constant_initializer(), trainable=False)",
        "core_API": "get_variable_scope"
    },
    {
        "commit_hash": "95b6d985ffb231d9a2047c039c95660913063b9d",
        "index": "edb957f9..6e1568af 100644",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestStackedSelfAttentionDecoderNet(AllenNlpTestCase):",
            "batch_size = 5",
            "time_steps = 10",
            "encoded_state = torch.rand(batch_size, time_steps, decoder_inout_dim)",
            "-        source_mask = torch.ones(batch_size, time_steps)",
            "+        source_mask = torch.ones(batch_size, time_steps).bool()",
            "source_mask[0, 7:] = 0",
            "source_mask[1, 5:] = 0",
            "prev_timesteps = 3"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=19774)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=19775)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=19776)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'bool'), position=2, insert_id=19777)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=19778)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=19779)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 5632,
        "neg_line": [
            "-source_mask = torch.ones(batch_size, time_steps)"
        ],
        "pos_line": [
            "+source_mask = torch.ones(batch_size, time_steps).bool()"
        ],
        "core_change": "-source_mask = torch.ones(batch_size, time_steps) +source_mask = torch.ones(batch_size, time_steps).bool()",
        "core_API": "rand"
    },
    {
        "commit_hash": "da3aceb876e0f92b65405a20198836c08ee80d10",
        "index": "27971b0e..852ee348 100644",
        "commit_message": "Add fp16 support (#963)\n\n* Update imgwarp.py\n\n* update fix\n\n* move _torch_inverse_cast under utils\n\n* fix some tests in pyramid\n\n* fix issues\n\n* implement other autocasting functions\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def normalize_homography3d(dst_pix_trans_src_pix: torch.Tensor,",
            "# compute the transformation pixel/norm for src/dst",
            "src_norm_trans_src_pix: torch.Tensor = normal_transform_pixel3d(",
            "src_d, src_h, src_w).to(dst_pix_trans_src_pix)",
            "-    src_pix_trans_src_norm = torch.inverse(src_norm_trans_src_pix)",
            "+",
            "+    src_pix_trans_src_norm = _torch_inverse_cast(src_norm_trans_src_pix)",
            "dst_norm_trans_dst_pix: torch.Tensor = normal_transform_pixel3d(",
            "dst_d, dst_h, dst_w).to(dst_pix_trans_src_pix)",
            "# compute chain transformations"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=call), position=0)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=ERROR), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=torch), value='_torch_inverse_cast')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 3,
        "number": 5637,
        "neg_line": [
            "-src_pix_trans_src_norm = torch.inverse(src_norm_trans_src_pix)"
        ],
        "pos_line": [
            "+",
            "+src_pix_trans_src_norm = _torch_inverse_cast(src_norm_trans_src_pix)"
        ],
        "core_change": "-src_pix_trans_src_norm = torch.inverse(src_norm_trans_src_pix) + +src_pix_trans_src_norm = _torch_inverse_cast(src_norm_trans_src_pix)",
        "core_API": "inverse"
    },
    {
        "commit_hash": "8358d3c4ab3f31cdb9eab442ac41b95b96b9e6ce",
        "index": "c7c8e142..9c87ed57 100644",
        "commit_message": "fix conv1d flatten bug; update reccurent layers implementation from 0(deprecated) to 1\n\n",
        "file": "coremltools.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "keras",
        "change": [
            "class NetGraph(object):",
            "self.remove_skip_layers(_KERAS_SKIP_LAYERS) # done 1 pass",
            "self.insert_1d_permute_layers()",
            "self.insert_permute_for_spatial_bn()",
            "-            self.insert_permute_for_embed_flatten()",
            "self.defuse_activation()",
            "self.remove_internal_input_layers()"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=insert_permute_for_embed_flatten))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 5643,
        "neg_line": [
            "-self.insert_permute_for_embed_flatten()"
        ],
        "pos_line": [],
        "core_change": "-self.insert_permute_for_embed_flatten()",
        "core_API": "remove_skip_layers"
    },
    {
        "commit_hash": "2511e66d7e00736a7ed1e824d7bb3c6f6fe1812b",
        "index": "a9e1751633..cc0a37b9fd 100644",
        "commit_message": "[Datasets] [AIR] Fixes label tensor squeezing in to_tf() (#25553)\n\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Dataset(Generic[T]):",
            "):",
            "if label_column:",
            "targets = convert_pandas_to_tf_tensor(batch[[label_column]])",
            "-                    assert targets.ndim == 2",
            "-                    targets = tf.squeeze(targets, axis=1)",
            "+                    if targets.ndim == 2 and targets.shape[1] == 1:",
            "+                        targets = tf.squeeze(targets, axis=1)",
            "batch.pop(label_column)",
            "",
            "features = None"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=4, insert_id=2136307)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=2136308)",
            "Insert(target_node=IN(type=if_statement), node=('boolean_operator', None), position=1, insert_id=2136309)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=2136310)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=2136311)",
            "Move(target_node=IN(type=boolean_operator), node=ASTNode(type=comparison_operator), position=0)",
            "Insert(target_node=IN(type=boolean_operator), node=('and', 'and'), position=1, insert_id=2136312)",
            "Insert(target_node=IN(type=boolean_operator), node=('comparison_operator', None), position=2, insert_id=2136313)",
            "Move(target_node=IN(type=block), node=ASTNode(type=expression_statement), position=0)",
            "Insert(target_node=IN(type=comparison_operator), node=('subscript', None), position=0, insert_id=2136314)",
            "Insert(target_node=IN(type=comparison_operator), node=('==', '=='), position=1, insert_id=2136315)",
            "Insert(target_node=IN(type=comparison_operator), node=('integer', '1'), position=2, insert_id=2136316)",
            "Insert(target_node=IN(type=subscript), node=('attribute', None), position=0, insert_id=2136317)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=2136318)",
            "Insert(target_node=IN(type=subscript), node=('integer', '1'), position=2, insert_id=2136319)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=2136320)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'targets'), position=0, insert_id=2136321)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2136322)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'shape'), position=2, insert_id=2136323)",
            "Delete(target_node=ASTNode(type=assert, text=assert))",
            "Delete(target_node=ASTNode(type=assert_statement))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 21,
        "number": 5647,
        "neg_line": [
            "-assert targets.ndim == 2",
            "-targets = tf.squeeze(targets, axis=1)"
        ],
        "pos_line": [
            "+if targets.ndim == 2 and targets.shape[1] == 1:",
            "+targets = tf.squeeze(targets, axis=1)"
        ],
        "core_change": "-assert targets.ndim == 2 -targets = tf.squeeze(targets, axis=1) +if targets.ndim == 2 and targets.shape[1] == 1: +targets = tf.squeeze(targets, axis=1)",
        "core_API": "squeeze"
    },
    {
        "commit_hash": "008c0edb3b7b96543930c47bcaed03429503971a",
        "index": "a357073..dd997fd 100644",
        "commit_message": "Data Input Refactoring\n\n- Data input pipeline can now deal properly with missing target data.\nFixes #101\n- Moved some of the featurizer functionality into the data reader and\nmade featurizer implicit part of the model.\n- Misc: Replace concat with concat_v2\n\n",
        "file": "seq2seq.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class BidirectionalRNNEncoder(GraphModule):",
            "**kwargs)",
            "",
            "# Concatenate outputs and states of the forward and backward RNNs",
            "-    outputs_concat = tf.concat(2, outputs)",
            "+    outputs_concat = tf.concat_v2(outputs, 2)",
            "",
            "return RNNEncoderOutput(outputs=outputs_concat, final_state=states)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=integer, text=2), node=ASTNode(type=argument_list), position=3)",
            "Move(target_node=ASTNode(type=,, text=,), node=ASTNode(type=argument_list), position=4)",
            "Update(target_node=ASTNode(type=identifier, text=concat), value='concat_v2')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 3,
        "number": 5648,
        "neg_line": [
            "-outputs_concat = tf.concat(2, outputs)"
        ],
        "pos_line": [
            "+outputs_concat = tf.concat_v2(outputs, 2)"
        ],
        "core_change": "-outputs_concat = tf.concat(2, outputs) +outputs_concat = tf.concat_v2(outputs, 2)",
        "core_API": "concat"
    },
    {
        "commit_hash": "7517807a2c601bbe36fb3394e6af83feaddfe235",
        "index": "5e80f2d..6872d83 100644",
        "commit_message": "fix actor update problem\n\n",
        "file": "Reinforcement-learning-with-tensorflow.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Critic(object):",
            "",
            "with tf.variable_scope('Critic'):",
            "# Input (s, a), output q",
            "-            self.a = a",
            "+            self.a = tf.stop_gradient(a)    # stop critic update flows to actor",
            "self.q = self._build_net(S, self.a, 'eval_net', trainable=True)",
            "",
            "# Input (s_, a_), output q_ for q_target"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('call', None), position=2, insert_id=2157200)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2157201)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2157202)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2157203)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2157204)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'stop_gradient'), position=2, insert_id=2157205)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2157206)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=a), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2157207)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 5650,
        "neg_line": [
            "-self.a = a"
        ],
        "pos_line": [
            "+self.a = tf.stop_gradient(a)    # stop critic update flows to actor"
        ],
        "core_change": "-self.a = a +self.a = tf.stop_gradient(a)    # stop critic update flows to actor",
        "core_API": "variable_scope"
    },
    {
        "commit_hash": "ee45c6c2da78ae944d6bb0043d6414122590299e",
        "index": "0e3b66f5..d0f9f8e4 100644",
        "commit_message": "[Fix] MotionBlur bug fix and doctest update (#782)\n\n* Fixed #779\n\n* Added tests for _extract_device_dtype\n\n* Fixed broken tests\n\n* Added tests against functional\n\n* Fixed doctests\n\n* bug fix\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def motion_blur3d(",
            ">>> torch.allclose(out_1[0], out_1[1])",
            "True",
            ">>> # perform element-wise motion blur accross the batch",
            "-        >>> out_1 = motion_blur3d(input, 5, torch.tensor([[0., 90., 90.], [90., 180., 0.]]), torch.tensor([1, -1]))",
            "+        >>> out_1 = motion_blur3d(input, 5, torch.tensor([[0., 90., 90.], [90., 180., 0.]]), torch.tensor([1., -1.]))",
            ">>> torch.allclose(out_1[0], out_1[1])",
            "False",
            "\"\"\""
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=list), node=('float', '1.'), position=1, insert_id=433282)",
            "Insert(target_node=ASTNode(type=list), node=('unary_operator', None), position=4, insert_id=433283)",
            "Insert(target_node=IN(type=unary_operator), node=('-', '-'), position=0, insert_id=433284)",
            "Insert(target_node=IN(type=unary_operator), node=('float', '1.'), position=1, insert_id=433285)",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=unary_operator, text=-1))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 5652,
        "neg_line": [
            "->>> out_1 = motion_blur3d(input, 5, torch.tensor([[0., 90., 90.], [90., 180., 0.]]), torch.tensor([1, -1]))"
        ],
        "pos_line": [
            "+>>> out_1 = motion_blur3d(input, 5, torch.tensor([[0., 90., 90.], [90., 180., 0.]]), torch.tensor([1., -1.]))"
        ],
        "core_change": "->>> out_1 = motion_blur3d(input, 5, torch.tensor([[0., 90., 90.], [90., 180., 0.]]), torch.tensor([1, -1])) +>>> out_1 = motion_blur3d(input, 5, torch.tensor([[0., 90., 90.], [90., 180., 0.]]), torch.tensor([1., -1.]))",
        "core_API": "allclose"
    },
    {
        "commit_hash": "f8ea5906c0ae5ef6fb7800e3f0a05ebf56cdd927",
        "index": "3fbdd5c..4bc4822 100644",
        "commit_message": "Fix softmax_rgb_blend() when mesh is outside zfar\n\nSummary:\nThis fixes two small issues with blending.py:softmax_rgb_blend():\n  1) zfar and znear attributes are propagated from the camera settings instead of just using default settings of znear=1.0 and zfar=100.0\n  2) A check is added to prevent arithmetic overflow in softmax_rgb_blend()\n\nThis is a fix in response to https://github.com/facebookresearch/pytorch3d/issues/334\nwhere meshes rendererd using a SoftPhongShader with faces_per_pixel=1 appear black.  This only occurs when the scale of the mesh is large (vertex values > 100, where 100 is the default value of zfar).  This fix allows the caller to increase the value of cameras.zfar to match the scale of her/his mesh.\n\nReviewed By: nikhilaravi\n\nDifferential Revision: D23517541\n\nfbshipit-source-id: ab8631ce9e5f2149f140b67b13eff857771b8807\n\n",
        "file": "pytorch3d.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def softmax_rgb_blend(",
            "z_inv = (zfar - fragments.zbuf) / (zfar - znear) * mask",
            "# pyre-fixme[16]: `Tuple` has no attribute `values`.",
            "# pyre-fixme[6]: Expected `Tensor` for 1st param but got `float`.",
            "-    z_inv_max = torch.max(z_inv, dim=-1).values[..., None]",
            "+    z_inv_max = torch.max(z_inv, dim=-1).values[..., None].clamp(min=eps)",
            "# pyre-fixme[6]: Expected `Tensor` for 1st param but got `float`.",
            "weights_num = prob_map * torch.exp((z_inv - z_inv_max) / blend_params.gamma)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=binary_operator), node=('call', None), position=2, insert_id=922850)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=922851)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=922852)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=subscript), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=922853)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'clamp'), position=2, insert_id=922854)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=922855)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=1, insert_id=922856)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=922857)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'min'), position=0, insert_id=922858)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=922859)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'eps'), position=2, insert_id=922860)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 12,
        "number": 5654,
        "neg_line": [
            "-z_inv_max = torch.max(z_inv, dim=-1).values[..., None]"
        ],
        "pos_line": [
            "+z_inv_max = torch.max(z_inv, dim=-1).values[..., None].clamp(min=eps)"
        ],
        "core_change": "-z_inv_max = torch.max(z_inv, dim=-1).values[..., None] +z_inv_max = torch.max(z_inv, dim=-1).values[..., None].clamp(min=eps)",
        "core_API": "max"
    },
    {
        "commit_hash": "75f29cc0f8ff9419dbd7f7d67839ba7c65e4be8f",
        "index": "5b162a0e..fb276a3a 100644",
        "commit_message": "Replicate optimizer hyperparams; Fix tagger scorer; Add word dropout\n\n",
        "file": "stanza.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "optim",
        "change": [
            "def harmonic_mean(a, weights=None):",
            "return sum(weights) / sum(w/x for x, w in zip(a, weights))",
            "",
            "# torch utils",
            "-def get_optimizer(name, parameters, lr, betas=(0.9, 0.999)):",
            "+def get_optimizer(name, parameters, lr, betas=(0.9, 0.999), eps=1e-8):",
            "if name == 'sgd':",
            "return torch.optim.SGD(parameters, lr=lr)",
            "elif name == 'adagrad':",
            "return torch.optim.Adagrad(parameters, lr=lr)",
            "elif name == 'adam':",
            "-        return torch.optim.Adam(parameters, lr=lr, betas=betas) # use default lr",
            "+        return torch.optim.Adam(parameters, lr=lr, betas=betas, eps=eps)",
            "elif name == 'adamax':",
            "return torch.optim.Adamax(parameters) # use default lr",
            "else:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=8, insert_id=1617088)",
            "Insert(target_node=ASTNode(type=parameters), node=('default_parameter', None), position=9, insert_id=1617089)",
            "Insert(target_node=IN(type=default_parameter), node=('identifier', 'eps'), position=0, insert_id=1617090)",
            "Insert(target_node=IN(type=default_parameter), node=('=', '='), position=1, insert_id=1617091)",
            "Insert(target_node=IN(type=default_parameter), node=('float', '1e-8'), position=2, insert_id=1617092)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=6, insert_id=1617093)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=7, insert_id=1617094)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'eps'), position=0, insert_id=1617095)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1617096)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'eps'), position=2, insert_id=1617097)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 5655,
        "neg_line": [
            "-def get_optimizer(name, parameters, lr, betas=(0.9, 0.999)):",
            "-return torch.optim.Adam(parameters, lr=lr, betas=betas) # use default lr"
        ],
        "pos_line": [
            "+def get_optimizer(name, parameters, lr, betas=(0.9, 0.999), eps=1e-8):",
            "+return torch.optim.Adam(parameters, lr=lr, betas=betas, eps=eps)"
        ],
        "core_change": "-def get_optimizer(name, parameters, lr, betas=(0.9, 0.999)): +def get_optimizer(name, parameters, lr, betas=(0.9, 0.999), eps=1e-8): -return torch.optim.Adam(parameters, lr=lr, betas=betas) # use default lr +return torch.optim.Adam(parameters, lr=lr, betas=betas, eps=eps)",
        "core_API": "SGD"
    },
    {
        "commit_hash": "466154fcbd075c0372f06d6072617f60fe43548b",
        "index": "7a01fc28..3a2a810d 100644",
        "commit_message": "fix baseline dim\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class FlatVarHelper(object):",
            "self.session = session",
            "shapes = map(get_shape, variables)",
            "total_size = sum(np.prod(shape) for shape in shapes)",
            "-        self.theta = theta = tf.placeholder(tf.float32, [total_size])",
            "+        self.theta = tf.placeholder(tf.float32, [total_size])",
            "start = 0",
            "assigns = []",
            "",
            "for (shape, variable) in zip(shapes, variables):",
            "size = np.prod(shape)",
            "-            assigns.append(tf.assign(variable, tf.reshape(theta[start:start + size], shape)))",
            "+            assigns.append(tf.assign(variable, tf.reshape(self.theta[start:start + size], shape)))",
            "start += size",
            "",
            "self.set_op = tf.group(*assigns)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=expression_statement), node=ASTNode(type=assignment), position=0)",
            "Insert(target_node=ASTNode(type=assignment), node=('attribute', None), position=0, insert_id=2248018)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=2248019)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2248020)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=theta), position=2)",
            "Move(target_node=ASTNode(type=subscript), node=ASTNode(type=attribute), position=0)",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=identifier, text=theta))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 9,
        "number": 5662,
        "neg_line": [
            "-self.theta = theta = tf.placeholder(tf.float32, [total_size])",
            "-assigns.append(tf.assign(variable, tf.reshape(theta[start:start + size], shape)))"
        ],
        "pos_line": [
            "+self.theta = tf.placeholder(tf.float32, [total_size])",
            "+assigns.append(tf.assign(variable, tf.reshape(self.theta[start:start + size], shape)))"
        ],
        "core_change": "-self.theta = theta = tf.placeholder(tf.float32, [total_size]) +self.theta = tf.placeholder(tf.float32, [total_size]) -assigns.append(tf.assign(variable, tf.reshape(theta[start:start + size], shape))) +assigns.append(tf.assign(variable, tf.reshape(self.theta[start:start + size], shape)))",
        "core_API": "prod"
    },
    {
        "commit_hash": "70d4a0ba0e9bc740cd9d1982d73c159ed4d76e6c",
        "index": "170cce4..81d8682 100644",
        "commit_message": "Fix typo in contrib FusedLamb. (#1172)\n\n\n",
        "file": "apex.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class FusedLAMB(torch.optim.Optimizer):",
            "continue",
            "if p.dtype == torch.float32:",
            "g_all_32.append(p.grad.data)",
            "-                elif p.dytpe == torch.float16:",
            "+                elif p.dtype == torch.float16:",
            "g_all_16.append(p.grad.data)",
            "else:",
            "raise RuntimeError('FusedLAMB only support fp16 and fp32.')"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=dytpe), value='dtype')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 5663,
        "neg_line": [
            "-elif p.dytpe == torch.float16:"
        ],
        "pos_line": [
            "+elif p.dtype == torch.float16:"
        ],
        "core_change": "-elif p.dytpe == torch.float16: +elif p.dtype == torch.float16:",
        "core_API": "append"
    },
    {
        "commit_hash": "5b8e23ec09a0a5c92dd0e5d02e292fa986555d73",
        "index": "c75537ac4d..681d163674 100644",
        "commit_message": "Fixed problem with ivy.matmul where the transpose operation wasn't a matrix transpose in the backends.\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def matmul(",
            "dtype_from = tf.as_dtype(x1.dtype)",
            "",
            "if transpose_a:",
            "-        x1 = tf.transpose(x1)",
            "+        x1 = tf.linalg.matrix_transpose(x1)",
            "if transpose_b:",
            "-        x2 = tf.transpose(x2)",
            "+        x2 = tf.linalg.matrix_transpose(x2)",
            "",
            "if adjoint_a:",
            "x1 = tf.linalg.adjoint(x1)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1956468)",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1956469)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1956470)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'matrix_transpose'), position=2, insert_id=1956471)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1956472)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'matrix_transpose'), position=2, insert_id=1956473)",
            "Update(target_node=ASTNode(type=identifier, text=transpose), value='linalg')",
            "Update(target_node=ASTNode(type=identifier, text=transpose), value='linalg')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 10,
        "number": 5664,
        "neg_line": [
            "-x1 = tf.transpose(x1)",
            "-x2 = tf.transpose(x2)"
        ],
        "pos_line": [
            "+x1 = tf.linalg.matrix_transpose(x1)",
            "+x2 = tf.linalg.matrix_transpose(x2)"
        ],
        "core_change": "-x1 = tf.transpose(x1) +x1 = tf.linalg.matrix_transpose(x1) -x2 = tf.transpose(x2) +x2 = tf.linalg.matrix_transpose(x2)",
        "core_API": "as_dtype"
    },
    {
        "commit_hash": "d87148c56b0bb86669a11697b7d94c24a8d4ff86",
        "index": "c6dddcba..5cbd510f 100644",
        "commit_message": "fix deconv2d None error (#5093)\n\n\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def _preprocess_deconv_output_shape(x, shape, dim_ordering):",
            "",
            "if shape[0] is None:",
            "shape = (tf.shape(x)[0], ) + tuple(shape[1:])",
            "+        shape = tf.stack(list(shape))",
            "return shape"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=2114683)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=2114684)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'shape'), position=0, insert_id=2114685)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=2114686)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=2114687)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2114688)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2114689)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2114690)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2114691)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'stack'), position=2, insert_id=2114692)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2114693)",
            "Insert(target_node=IN(type=argument_list), node=('call', None), position=1, insert_id=2114694)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2114695)",
            "Insert(target_node=IN(type=call), node=('identifier', 'list'), position=0, insert_id=2114696)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2114697)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2114698)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'shape'), position=1, insert_id=2114699)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2114700)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 18,
        "number": 5667,
        "neg_line": [],
        "pos_line": [
            "+shape = tf.stack(list(shape))"
        ],
        "core_change": "+shape = tf.stack(list(shape))",
        "core_API": "shape"
    },
    {
        "commit_hash": "3d0c0ae43748812348f8bb8153fa9db5c464a0f7",
        "index": "f9d9834c1..9a265bcb4 100644",
        "commit_message": "Fix longformer onnx broken export (#20292)\n\n* fix controlflow for onnx export\n\n* fix warning\n\n* fix the case padding_len = 0, explicit the recorded control flows\n\n* style\n\n* style\n\n* fix bug\n\n* fix copy\n\n* nits\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class LongformerOnnxConfig(OnnxConfig):",
            ")",
            "import torch",
            "",
            "+        # for some reason, replacing this code by inputs[\"global_attention_mask\"] = torch.randint(2, inputs[\"input_ids\"].shape, dtype=torch.int64)",
            "+        # makes the export fail randomly",
            "inputs[\"global_attention_mask\"] = torch.zeros_like(inputs[\"input_ids\"])",
            "# make every second token global",
            "inputs[\"global_attention_mask\"][:, ::2] = 1",
            "+",
            "return inputs"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 0,
        "AST_diff_line": 17,
        "number": 5669,
        "neg_line": [],
        "pos_line": [
            "+# for some reason, replacing this code by inputs[\"global_attention_mask\"] = torch.randint(2, inputs[\"input_ids\"].shape, dtype=torch.int64)",
            "+# makes the export fail randomly",
            "+"
        ],
        "core_change": "+# for some reason, replacing this code by inputs[\"global_attention_mask\"] = torch.randint(2, inputs[\"input_ids\"].shape, dtype=torch.int64) +# makes the export fail randomly +",
        "core_API": "randint"
    },
    {
        "commit_hash": "b166304e41268a4c49ad24393326f4559b61f5f9",
        "index": "21b21a0c..f0cd833a 100644",
        "commit_message": "update to pytorch 1.3 and fix small test issues\n\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestHomographyWarper:",
            "batch_size, channels, height, width = batch_shape",
            "patch_src = torch.rand(batch_size, channels, height, width)",
            "# rotation of 90deg",
            "-        dst_homo_src = utils.create_eye_batch(batch_size, 3)",
            "+        dst_homo_src = torch.eye(3)",
            "dst_homo_src[..., 0, 0] = 0.0",
            "dst_homo_src[..., 0, 1] = 1.0",
            "dst_homo_src[..., 1, 0] = -1.0",
            "dst_homo_src[..., 1, 1] = 0.0",
            "+        dst_homo_src = dst_homo_src.expand(batch_size, -1, -1)",
            "",
            "# instantiate warper and warp from source to destination",
            "warper = kornia.HomographyWarper(height, width)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=8, insert_id=457105)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=457106)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'dst_homo_src'), position=0, insert_id=457107)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=457108)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=457109)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=457110)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=457111)",
            "Update(target_node=ASTNode(type=identifier, text=utils), value='torch')",
            "Update(target_node=ASTNode(type=identifier, text=create_eye_batch), value='eye')",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dst_homo_src'), position=0, insert_id=457112)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=457113)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'expand'), position=2, insert_id=457114)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=457115)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'batch_size'), position=1, insert_id=457116)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=457117)",
            "Insert(target_node=IN(type=argument_list), node=('unary_operator', '-1'), position=3, insert_id=457118)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=4, insert_id=457119)",
            "Insert(target_node=IN(type=argument_list), node=('unary_operator', '-1'), position=5, insert_id=457120)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=6, insert_id=457121)",
            "Delete(target_node=ASTNode(type=identifier, text=batch_size))",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 21,
        "number": 5671,
        "neg_line": [
            "-dst_homo_src = utils.create_eye_batch(batch_size, 3)"
        ],
        "pos_line": [
            "+dst_homo_src = torch.eye(3)",
            "+dst_homo_src = dst_homo_src.expand(batch_size, -1, -1)"
        ],
        "core_change": "-dst_homo_src = utils.create_eye_batch(batch_size, 3) +dst_homo_src = torch.eye(3) +dst_homo_src = dst_homo_src.expand(batch_size, -1, -1)",
        "core_API": "rand"
    },
    {
        "commit_hash": "0d90e35f3b14bd242a64a7a2af744a258e7d0298",
        "index": "54ddd404..7c254d1e 100644",
        "commit_message": "More unit test fixes\n\n",
        "file": "fairseq.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "cuda",
        "change": [
            "def main(args):",
            "if args.distributed_port > 0 \\",
            "or args.distributed_init_method is not None:",
            "distributed_main(args)",
            "-    elif torch.cuda.device_count() > 1:",
            "+    elif args.distributed_world_size > 1:",
            "multiprocessing_main(args)",
            "else:",
            "singleprocess_main(args)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=comparison_operator), node=ASTNode(type=attribute), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=torch), value='args')",
            "Update(target_node=ASTNode(type=identifier, text=cuda), value='distributed_world_size')",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=device_count))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 5672,
        "neg_line": [
            "-elif torch.cuda.device_count() > 1:"
        ],
        "pos_line": [
            "+elif args.distributed_world_size > 1:"
        ],
        "core_change": "-elif torch.cuda.device_count() > 1: +elif args.distributed_world_size > 1:",
        "core_API": "device_count"
    },
    {
        "commit_hash": "ab5b9174940f29a62374bddaf38cd5d2eeb68e25",
        "index": "a4bc3ca..cc37ad5 100755",
        "commit_message": "`check_fonts()` download to `CONFIG_DIR` fix (#7489)\n\nFollows https://github.com/ultralytics/yolov5/pull/7488. Correct bug where fonts were downloading to current working directory rather than global CONFIG_DIR\n",
        "file": "yolov5.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def check_file(file, suffix=''):",
            "def check_font(font=FONT, progress=False):",
            "# Download font to CONFIG_DIR if necessary",
            "font = Path(font)",
            "-    if not font.exists() and not (CONFIG_DIR / font.name).exists():",
            "+    file = CONFIG_DIR / font.name",
            "+    if not font.exists() and not file.exists():",
            "url = \"https://ultralytics.com/assets/\" + font.name",
            "-        LOGGER.info(f'Downloading {url} to {CONFIG_DIR / font.name}...')",
            "-        torch.hub.download_url_to_file(url, str(font), progress=progress)",
            "+        LOGGER.info(f'Downloading {url} to {file}...')",
            "+        torch.hub.download_url_to_file(url, str(file), progress=progress)",
            "",
            "",
            "def check_dataset(data, autodownload=True):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=1294802)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1294803)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'file'), position=0, insert_id=1294804)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1294805)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=binary_operator), position=2)",
            "Update(target_node=ASTNode(type=string, text=f'Downloading {url} to {CONFIG_DIR / font.name}...'), value=\"f'Downloading {url} to {file}...'\")",
            "Insert(target_node=ASTNode(type=attribute), node=('identifier', 'file'), position=0, insert_id=1294806)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=1294807)",
            "Update(target_node=ASTNode(type=identifier, text=font), value='file')",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=parenthesized_expression))",
            "Delete(target_node=ASTNode(type=., text=.))"
        ],
        "plus_line": 4,
        "minus_line": 3,
        "AST_diff_line": 13,
        "number": 5673,
        "neg_line": [
            "-if not font.exists() and not (CONFIG_DIR / font.name).exists():",
            "-LOGGER.info(f'Downloading {url} to {CONFIG_DIR / font.name}...')",
            "-torch.hub.download_url_to_file(url, str(font), progress=progress)"
        ],
        "pos_line": [
            "+file = CONFIG_DIR / font.name",
            "+if not font.exists() and not file.exists():",
            "+LOGGER.info(f'Downloading {url} to {file}...')",
            "+torch.hub.download_url_to_file(url, str(file), progress=progress)"
        ],
        "core_change": "-if not font.exists() and not (CONFIG_DIR / font.name).exists(): +file = CONFIG_DIR / font.name +if not font.exists() and not file.exists(): -LOGGER.info(f'Downloading {url} to {CONFIG_DIR / font.name}...') -torch.hub.download_url_to_file(url, str(font), progress=progress) +LOGGER.info(f'Downloading {url} to {file}...') +torch.hub.download_url_to_file(url, str(file), progress=progress)",
        "core_API": "exists"
    },
    {
        "commit_hash": "55d3ed3d088ac8a149ff1a2e27ff9d60c5389c51",
        "index": "9a35871e..9da0c41b 100644",
        "commit_message": "use rmsprop, fix typo in asyncrunner\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class DeepQNetwork(Model):",
            "self.target_output = self.target_model.get_output()",
            "",
            "# Create training operations",
            "-        self.optimizer = tf.train.AdamOptimizer(self.alpha)",
            "+        self.optimizer = tf.train.RMSPropOptimizer(self.alpha, momentum=0.95, epsilon=0.01)",
            "self.create_training_operations()",
            "self.saver = tf.train.Saver()",
            "writer = tf.train.SummaryWriter('logs', graph=tf.get_default_graph())"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=AdamOptimizer), value='RMSPropOptimizer')",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=2248303)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=2248304)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=2248305)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=2248306)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'momentum'), position=0, insert_id=2248307)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2248308)",
            "Insert(target_node=IN(type=keyword_argument), node=('float', '0.95'), position=2, insert_id=2248309)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'epsilon'), position=0, insert_id=2248310)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2248311)",
            "Insert(target_node=IN(type=keyword_argument), node=('float', '0.01'), position=2, insert_id=2248312)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 5674,
        "neg_line": [
            "-self.optimizer = tf.train.AdamOptimizer(self.alpha)"
        ],
        "pos_line": [
            "+self.optimizer = tf.train.RMSPropOptimizer(self.alpha, momentum=0.95, epsilon=0.01)"
        ],
        "core_change": "-self.optimizer = tf.train.AdamOptimizer(self.alpha) +self.optimizer = tf.train.RMSPropOptimizer(self.alpha, momentum=0.95, epsilon=0.01)",
        "core_API": "get_output"
    },
    {
        "commit_hash": "c7fd1d9fb18862318b133c9474d37c5085850070",
        "index": "3d399782..0093e425 100755",
        "commit_message": "fix deprecations about casting & initializers in tf1.13\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Model(ModelDesc):",
            ".FullyConnected('linear', units=10)())",
            "tf.nn.softmax(logits, name='output')",
            "",
            "-        accuracy = tf.to_float(tf.nn.in_top_k(logits, label, 1))",
            "+        accuracy = tf.cast(tf.nn.in_top_k(logits, label, 1), tf.float32)",
            "add_moving_summary(tf.reduce_mean(accuracy, name='accuracy'))",
            "",
            "cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=to_float), value='cast')",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=2278980)",
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=3, insert_id=2278981)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2278982)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2278983)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'float32'), position=2, insert_id=2278984)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 5675,
        "neg_line": [
            "-accuracy = tf.to_float(tf.nn.in_top_k(logits, label, 1))"
        ],
        "pos_line": [
            "+accuracy = tf.cast(tf.nn.in_top_k(logits, label, 1), tf.float32)"
        ],
        "core_change": "-accuracy = tf.to_float(tf.nn.in_top_k(logits, label, 1)) +accuracy = tf.cast(tf.nn.in_top_k(logits, label, 1), tf.float32)",
        "core_API": "softmax"
    },
    {
        "commit_hash": "47d5c244706ab01859a3931f66066407a9d5efbb",
        "index": "728eb3aa..2fe8ac37 100755",
        "commit_message": "Summaries completely changed, distributed mode incompletely changed, various fixes\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Gaussian(Distribution):",
            "definite = mean",
            "",
            "# Non-deterministic: sample action using default normal distribution",
            "-        normal = tf.random_normal(shape=tf.shape(input=mean))",
            "-        sampled = mean + stddev * normal",
            "+        normal_distribution = tf.random_normal(shape=tf.shape(input=mean))",
            "+        sampled = mean + stddev * normal_distribution",
            "",
            "return tf.where(condition=deterministic, x=definite, y=sampled)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=normal), value='normal_distribution')",
            "Update(target_node=ASTNode(type=identifier, text=normal), value='normal_distribution')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 5676,
        "neg_line": [
            "-normal = tf.random_normal(shape=tf.shape(input=mean))",
            "-sampled = mean + stddev * normal"
        ],
        "pos_line": [
            "+normal_distribution = tf.random_normal(shape=tf.shape(input=mean))",
            "+sampled = mean + stddev * normal_distribution"
        ],
        "core_change": "-normal = tf.random_normal(shape=tf.shape(input=mean)) -sampled = mean + stddev * normal +normal_distribution = tf.random_normal(shape=tf.shape(input=mean)) +sampled = mean + stddev * normal_distribution",
        "core_API": "random_normal"
    },
    {
        "commit_hash": "bbd03fb5d1c2620acebb812f14648398cf348460",
        "index": "94fdd479..e12127f5 100644",
        "commit_message": "Supports for exporting DETR to onnx with dynamic shapes and batch inference (#5168)\n\n* support exporting `DETR` to ONNX with dynamic shapes and competitive performance\n\n* support exporting `DETR` to ONNX with dynamic shapes and competitive performance\n\n* add onnx performance docs for `DETR`\n\n* fix lint error\n\n* fix\n\n* refactor the onnx export for detr\n\n* fix doc\n\n* supports batch inference for detr\n\n* inherit config for batch inference\n\n* fix type\n\n* support batch inference for ONNX\n\n* fix dynamically clip bboxes\n\n* remove batch inference config\n",
        "file": "mmdetection.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_sine_positional_encoding(num_feats=16, batch_size=2):",
            "",
            "module = SinePositionalEncoding(num_feats)",
            "h, w = 10, 6",
            "-    mask = torch.rand(batch_size, h, w) > 0.5",
            "+    mask = (torch.rand(batch_size, h, w) > 0.5).to(torch.int)",
            "assert not module.normalize",
            "out = module(mask)",
            "assert out.shape == (batch_size, num_feats * 2, h, w)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('call', None), position=2, insert_id=626366)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=626367)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=626368)",
            "Insert(target_node=IN(type=attribute), node=('parenthesized_expression', None), position=0, insert_id=626369)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=626370)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to'), position=2, insert_id=626371)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=626372)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=626373)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=626374)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('(', '('), position=0, insert_id=626375)",
            "Move(target_node=IN(type=parenthesized_expression), node=ASTNode(type=comparison_operator), position=1)",
            "Insert(target_node=IN(type=parenthesized_expression), node=(')', ')'), position=2, insert_id=626376)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=626377)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=626378)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'int'), position=2, insert_id=626379)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 15,
        "number": 5678,
        "neg_line": [
            "-mask = torch.rand(batch_size, h, w) > 0.5"
        ],
        "pos_line": [
            "+mask = (torch.rand(batch_size, h, w) > 0.5).to(torch.int)"
        ],
        "core_change": "-mask = torch.rand(batch_size, h, w) > 0.5 +mask = (torch.rand(batch_size, h, w) > 0.5).to(torch.int)",
        "core_API": "rand"
    },
    {
        "commit_hash": "236f4359669813572a869cb07d83f9e180bb8552",
        "index": "af5a74c25..1cb644ec3 100644",
        "commit_message": "bugfix VirtualWorker\n\n",
        "file": "PySyft.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_ellipsis_simplify():",
            "def test_pointer_tensor_simplify():",
            "\"\"\"Test the simplification of PointerTensor\"\"\"",
            "",
            "-    alice = syft.VirtualWorker(id=\"alice\")",
            "+    alice = syft.VirtualWorker(syft.torch.hook, id=\"alice\")",
            "input_tensor = PointerTensor(id=1000, location=alice, owner=alice)",
            "",
            "output = _simplify(input_tensor)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=1, insert_id=841251)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=841252)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=841253)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=841254)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'hook'), position=2, insert_id=841255)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'syft'), position=0, insert_id=841256)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=841257)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=2, insert_id=841258)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 5680,
        "neg_line": [
            "-alice = syft.VirtualWorker(id=\"alice\")"
        ],
        "pos_line": [
            "+alice = syft.VirtualWorker(syft.torch.hook, id=\"alice\")"
        ],
        "core_change": "-alice = syft.VirtualWorker(id=\"alice\") +alice = syft.VirtualWorker(syft.torch.hook, id=\"alice\")",
        "core_API": "VirtualWorker"
    },
    {
        "commit_hash": "6295e3117f842a723b731ab7a1932766dcf30b47",
        "index": "a0545c1..81dcff9 100644",
        "commit_message": "fix TFLogSTFTMagnitude.\n\n",
        "file": "TensorFlowTTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFLogSTFTMagnitude(tf.keras.layers.Layer):",
            "Returns:",
            "Tensor: Spectral convergence loss value.",
            "\"\"\"",
            "-        return tf.math.log(tf.abs(y_mag) + 1e-9) - tf.math.log(tf.abs(x_mag) + 1e-9)",
            "+        return tf.abs(tf.math.log(y_mag + 1e-9) - tf.math.log(x_mag + 1e-9))",
            "",
            "",
            "class TFSTFT(tf.keras.layers.Layer):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=type), node=('call', None), position=0, insert_id=2216335)",
            "Move(target_node=IN(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2216336)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2216337)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=binary_operator), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2216338)",
            "Move(target_node=ASTNode(type=binary_operator), node=ASTNode(type=identifier, text=y_mag), position=0)",
            "Move(target_node=ASTNode(type=binary_operator), node=ASTNode(type=identifier, text=x_mag), position=0)",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=abs))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 20,
        "number": 5685,
        "neg_line": [
            "-return tf.math.log(tf.abs(y_mag) + 1e-9) - tf.math.log(tf.abs(x_mag) + 1e-9)"
        ],
        "pos_line": [
            "+return tf.abs(tf.math.log(y_mag + 1e-9) - tf.math.log(x_mag + 1e-9))"
        ],
        "core_change": "-return tf.math.log(tf.abs(y_mag) + 1e-9) - tf.math.log(tf.abs(x_mag) + 1e-9) +return tf.abs(tf.math.log(y_mag + 1e-9) - tf.math.log(x_mag + 1e-9))",
        "core_API": "log"
    },
    {
        "commit_hash": "ead868f74585552c96bfe104a258af8ec210a948",
        "index": "9abdfd55..ad47ffb6 100644",
        "commit_message": "addressd CR comments, fixed merge conflict\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Uniform(Distribution):",
            "else:",
            "# x is 2-d",
            "if x.le(_a).data[0, 0] or x.ge(_b).data[0, 0]:",
            "-                return Variable(torch.Tensor([[-float(\"inf\")]]))",
            "+                return Variable(torch.Tensor([[-np.inf]]))",
            "return torch.sum(-torch.log(_b - _a))",
            "",
            "def batch_log_pdf(self, x, a=None, b=None, batch_size=1, *args, **kwargs):"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=unary_operator), node=('attribute', None), position=1, insert_id=770020)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'np'), position=0, insert_id=770021)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=770022)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'inf'), position=2, insert_id=770023)",
            "Delete(target_node=ASTNode(type=identifier, text=float))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=string, text=\"inf\"))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 5687,
        "neg_line": [
            "-return Variable(torch.Tensor([[-float(\"inf\")]]))"
        ],
        "pos_line": [
            "+return Variable(torch.Tensor([[-np.inf]]))"
        ],
        "core_change": "-return Variable(torch.Tensor([[-float(\"inf\")]])) +return Variable(torch.Tensor([[-np.inf]]))",
        "core_API": "le"
    },
    {
        "commit_hash": "9cd62723fa2967ca4ee9c5d855de91f4df4f46a3",
        "index": "b81977b9..f601f3be 100644",
        "commit_message": "Fix output types of augmentations on autocast regions (#2168)\n\n* update `.type` casts on base augmentation class\n\n- remove all casts around the bases augmentations classes\n- add cast on `transform_inputs` of `_AugmentationBase` when autocast region is enabled\n\n* add autocast test to `GeometricAugmentationBase2D`\n\n* add `is_autocast_enabled` with old torch compat\n\n- add to docs the map_location_to_cpu\n\n* add aug 2d and 3d base tests\n\n* Fix autocast for geometric 2d\n\n* update `eye_like` to cast type after construction\n\n* add test sequential container 2D\n\n* fix autocast for AugmentationSequential container\n\n* fix AugmentationSequential container\n\n- add `.type` method to Keypoints and Boxes\n\n* add test for `VideoSequential`, `PatchSequential`\n\n* Update `is_autocast_enabled`\n\n* revert typechecking statement on is_autocast_enabled\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def eye_like(n: int, input: torch.Tensor, shared_memory: bool = False) -> torch.",
            "if len(input.shape) < 1:",
            "raise AssertionError(input.shape)",
            "",
            "-    identity = torch.eye(n, device=input.device, dtype=input.dtype)",
            "+    identity = torch.eye(n, device=input.device).type(input.dtype)",
            "return identity[None].expand(input.shape[0], n, n) if shared_memory else identity[None].repeat(input.shape[0], 1, 1)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=390310)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=390311)",
            "Insert(target_node=IN(type=attribute), node=('call', None), position=0, insert_id=390312)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=390313)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'type'), position=2, insert_id=390314)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=390315)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=attribute), position=1)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=), text=)), position=2)",
            "Move(target_node=IN(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=390316)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=(, text=(), position=0)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=n), position=1)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=,, text=,), position=2)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=keyword_argument), position=3)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=390317)",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=dtype))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 20,
        "number": 5689,
        "neg_line": [
            "-identity = torch.eye(n, device=input.device, dtype=input.dtype)"
        ],
        "pos_line": [
            "+identity = torch.eye(n, device=input.device).type(input.dtype)"
        ],
        "core_change": "-identity = torch.eye(n, device=input.device, dtype=input.dtype) +identity = torch.eye(n, device=input.device).type(input.dtype)",
        "core_API": "eye"
    },
    {
        "commit_hash": "661c7312c8bef04fad50c36dc8252b991a7f1b49",
        "index": "= edge_type + torch.arange(len(edge_list)) * 2 * len(relations)",
        "commit_message": "fix no printing\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Entities(InMemoryDataset):",
            "edge = torch.tensor(edge_list, dtype=torch.long).t().contiguous()",
            "edge_index, edge_type = edge[:2], edge[2]",
            "",
            "-        oh = F.one_hot(",
            "-            edge_type, num_classes=2 * len(relations)).to(torch.float)",
            "+        oh = F.one_hot(edge_type,",
            "+                       num_classes=2 * len(relations)).to(torch.float)",
            "deg = scatter_add(oh, edge_index[0], dim=0, dim_size=len(nodes))",
            "index = edge_type + torch.arange(len(edge_list)) * 2 * len(relations)",
            "edge_norm = 1 / deg[edge_index[0]].view(-1)[index]"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 5690,
        "neg_line": [
            "-oh = F.one_hot(",
            "-edge_type, num_classes=2 * len(relations)).to(torch.float)"
        ],
        "pos_line": [
            "+oh = F.one_hot(edge_type,",
            "+num_classes=2 * len(relations)).to(torch.float)"
        ],
        "core_change": "-oh = F.one_hot( -edge_type, num_classes=2 * len(relations)).to(torch.float) +oh = F.one_hot(edge_type, +num_classes=2 * len(relations)).to(torch.float)",
        "core_API": "tensor"
    },
    {
        "commit_hash": "c6a485fa5ab32752e00870eb3d9ba3b866e94952",
        "index": "f3a29f175..cf506b591 100644",
        "commit_message": "Fix unit test errors\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_forward_with_beamformer_net(",
            "# `mask_type` has no effect when `loss_type` is not \"mask...\"",
            "return",
            "if not is_torch_1_9_plus and use_builtin_complex:",
            "-        # builtin complex support is only available in PyTorch 1.8+",
            "+        # builtin complex support is only well supported in PyTorch 1.9+",
            "return",
            "",
            "ch = 3",
            "inputs = random_speech[..., :ch].float()",
            "ilens = torch.LongTensor([16, 12])",
            "-    speech_refs = [torch.randn(2, 16, ch).float() for spk in range(num_spk)]",
            "+    speech_refs = [torch.randn(2, 16, ch, dtype=torch.float) for spk in range(num_spk)]",
            "noise_ref1 = torch.randn(2, 16, ch, dtype=torch.float)",
            "dereverb_ref1 = torch.randn(2, 16, ch, dtype=torch.float)",
            "encoder = STFTEncoder("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=127012)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=(, text=(), position=0)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=integer, text=2), position=1)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=,, text=,), position=2)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=integer, text=16), position=3)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=,, text=,), position=4)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=ch), position=5)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=6, insert_id=127013)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=7, insert_id=127014)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=), text=)), position=8)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=127015)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=127016)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=127017)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=127018)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=float), position=2)",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 23,
        "number": 5700,
        "neg_line": [
            "-# builtin complex support is only available in PyTorch 1.8+",
            "-speech_refs = [torch.randn(2, 16, ch).float() for spk in range(num_spk)]"
        ],
        "pos_line": [
            "+# builtin complex support is only well supported in PyTorch 1.9+",
            "+speech_refs = [torch.randn(2, 16, ch, dtype=torch.float) for spk in range(num_spk)]"
        ],
        "core_change": "-# builtin complex support is only available in PyTorch 1.8+ +# builtin complex support is only well supported in PyTorch 1.9+ -speech_refs = [torch.randn(2, 16, ch).float() for spk in range(num_spk)] +speech_refs = [torch.randn(2, 16, ch, dtype=torch.float) for spk in range(num_spk)]",
        "core_API": "LongTensor"
    },
    {
        "commit_hash": "5a0dac53bfd6e69ae64fb3119d607445e1a308d8",
        "index": "cf721be25..988500458 100644",
        "commit_message": "Empty assert hunt (#6056)\n\n* Fixed empty asserts\n\n* black-reformatted stragglers in templates\n\n* More code quality checks\n\n* Update src/transformers/convert_marian_to_pytorch.py\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n\n* Update src/transformers/convert_marian_to_pytorch.py\n\nCo-authored-by: Sam Shleifer <sshleifer@gmail.com>\n\n* removed unused line as per @sshleifer\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\nCo-authored-by: Sam Shleifer <sshleifer@gmail.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFFlaubertMainLayer(TFXLMMainLayer):",
            "position_ids = tf.expand_dims(tf.range(slen), axis=0)",
            "else:",
            "# assert shape_list(position_ids) == [bs, slen]  # (slen, bs)",
            "-            tf.debugging.assert_equal(shape_list(position_ids), [bs, slen])",
            "+            tf.debugging.assert_equal(",
            "+                shape_list(position_ids), [bs, slen]",
            "+            ), f\"Position id shape {shape_list(position_ids)} and input shape {[bs, slen]} mismatched\"",
            "# position_ids = position_ids.transpose(0, 1)",
            "",
            "# langs",
            "if langs is not None:",
            "# assert shape_list(langs) == [bs, slen]  # (slen, bs)",
            "-            tf.debugging.assert_equal(shape_list(langs), [bs, slen])",
            "+            tf.debugging.assert_equal(",
            "+                shape_list(langs), [bs, slen]",
            "+            ), f\"Lang shape {shape_list(langs)} and input shape {[bs, slen]} mismatched\"",
            "# langs = langs.transpose(0, 1)",
            "",
            "# Prepare head mask if needed"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('ERROR', None), position=2, insert_id=2380805)",
            "Insert(target_node=ASTNode(type=assignment), node=('type', None), position=3, insert_id=2380806)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=ERROR), node=(',', ','), position=1, insert_id=2380807)",
            "Insert(target_node=IN(type=type), node=('string', 'f\"Position id shape {shape_list(position_ids)} and input shape {[bs, slen]} mismatched\"'), position=0, insert_id=2380808)",
            "Insert(target_node=ASTNode(type=expression_statement), node=(',', ','), position=1, insert_id=2380809)",
            "Insert(target_node=ASTNode(type=expression_statement), node=('string', 'f\"Lang shape {shape_list(langs)} and input shape {[bs, slen]} mismatched\"'), position=2, insert_id=2380810)",
            "Delete(target_node=ASTNode(type=type))"
        ],
        "plus_line": 6,
        "minus_line": 2,
        "AST_diff_line": 8,
        "number": 5702,
        "neg_line": [
            "-tf.debugging.assert_equal(shape_list(position_ids), [bs, slen])",
            "-tf.debugging.assert_equal(shape_list(langs), [bs, slen])"
        ],
        "pos_line": [
            "+tf.debugging.assert_equal(",
            "+shape_list(position_ids), [bs, slen]",
            "+), f\"Position id shape {shape_list(position_ids)} and input shape {[bs, slen]} mismatched\"",
            "+tf.debugging.assert_equal(",
            "+shape_list(langs), [bs, slen]",
            "+), f\"Lang shape {shape_list(langs)} and input shape {[bs, slen]} mismatched\""
        ],
        "core_change": "-tf.debugging.assert_equal(shape_list(position_ids), [bs, slen]) +tf.debugging.assert_equal( +shape_list(position_ids), [bs, slen] +), f\"Position id shape {shape_list(position_ids)} and input shape {[bs, slen]} mismatched\" -tf.debugging.assert_equal(shape_list(langs), [bs, slen]) +tf.debugging.assert_equal( +shape_list(langs), [bs, slen] +), f\"Lang shape {shape_list(langs)} and input shape {[bs, slen]} mismatched\"",
        "core_API": "expand_dims"
    },
    {
        "commit_hash": "155c2a8380da87e07d54165c5b637b9c2b69ab4a",
        "index": "10b19ad..c392820 100644",
        "commit_message": "Fix missing to(device) for DataParallel in detection.py\n\n",
        "file": "EasyOCR.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def get_detector(trained_model, device='cpu'):",
            "net.load_state_dict(copyStateDict(torch.load(trained_model, map_location=device)))",
            "else:",
            "net.load_state_dict(copyStateDict(torch.load(trained_model, map_location=device)))",
            "-        net = torch.nn.DataParallel(net)",
            "+        net = torch.nn.DataParallel(net).to(device)",
            "cudnn.benchmark = False",
            "",
            "net.eval()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=115438)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=115439)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=115440)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to'), position=2, insert_id=115441)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=115442)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'device'), position=1, insert_id=115443)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=115444)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 5704,
        "neg_line": [
            "-net = torch.nn.DataParallel(net)"
        ],
        "pos_line": [
            "+net = torch.nn.DataParallel(net).to(device)"
        ],
        "core_change": "-net = torch.nn.DataParallel(net) +net = torch.nn.DataParallel(net).to(device)",
        "core_API": "load_state_dict"
    },
    {
        "commit_hash": "9b6385488617ee99188b60c0525cbe4ec7e8f2a0",
        "index": "41a73b3a..b60b4a71 100644",
        "commit_message": "Improve reproduceability 2/3 (#1906)\n\n* [Repro] Correct reproducability\n\n* up\n\n* up\n\n* uP\n\n* up\n\n* need better image\n\n* allow conversion from no state dict checkpoints\n\n* up\n\n* up\n\n* up\n\n* up\n\n* check tensors\n\n* check tensors\n\n* check tensors\n\n* check tensors\n\n* next try\n\n* up\n\n* up\n\n* better name\n\n* up\n\n* up\n\n* Apply suggestions from code review\n\n* correct more\n\n* up\n\n* replace all torch randn\n\n* fix\n\n* correct\n\n* correct\n\n* finish\n\n* fix more\n\n* up\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class KarrasVeScheduler(SchedulerMixin, ConfigMixin):",
            "gamma = 0",
            "",
            "# sample eps ~ N(0, S_noise^2 * I)",
            "-        eps = self.config.s_noise * torch.randn(sample.shape, generator=generator).to(sample.device)",
            "+        eps = self.config.s_noise * randn_tensor(sample.shape, generator=generator).to(sample.device)",
            "sigma_hat = sigma + gamma * sigma",
            "sample_hat = sample + ((sigma_hat**2 - sigma**2) ** 0.5 * eps)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=torch), value='randn_tensor')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=torch), position=0)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=randn))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 5706,
        "neg_line": [
            "-eps = self.config.s_noise * torch.randn(sample.shape, generator=generator).to(sample.device)"
        ],
        "pos_line": [
            "+eps = self.config.s_noise * randn_tensor(sample.shape, generator=generator).to(sample.device)"
        ],
        "core_change": "-eps = self.config.s_noise * torch.randn(sample.shape, generator=generator).to(sample.device) +eps = self.config.s_noise * randn_tensor(sample.shape, generator=generator).to(sample.device)",
        "core_API": "randn"
    },
    {
        "commit_hash": "dec36a9ae0e757c88a2196ec03b6ca86ac71b293",
        "index": "c5fe5f0..1b4082f 100644",
        "commit_message": "fix get_attr and new tensorflow saver\n\n",
        "file": "MMdnn.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class tensorflow_extractor(base_extractor):",
            "",
            "init = tf.global_variables_initializer()",
            "with tf.Session() as sess:",
            "-            # tf.train.export_meta_graph(\"kit.meta\", as_text=True)",
            "-            # writer = tf.summary.FileWriter('./graphs', sess.graph)",
            "-            # writer.close()",
            "sess.run(init)",
            "saver = tf.train.Saver()",
            "saver.restore(sess, path + cls.architecture_map[architecture]['filename'])"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 0,
        "minus_line": 3,
        "AST_diff_line": 17,
        "number": 5710,
        "neg_line": [
            "-# tf.train.export_meta_graph(\"kit.meta\", as_text=True)",
            "-# writer = tf.summary.FileWriter('./graphs', sess.graph)",
            "-# writer.close()"
        ],
        "pos_line": [],
        "core_change": "-# tf.train.export_meta_graph(\"kit.meta\", as_text=True) -# writer = tf.summary.FileWriter('./graphs', sess.graph) -# writer.close()",
        "core_API": "global_variables_initializer"
    },
    {
        "commit_hash": "de4159a318f9a4f2f474733306c00ac326e643a7",
        "index": "e70785c91..c25b93097 100644",
        "commit_message": "More TF int dtype fixes (#20384)\n\n* Add a test to ensure int dummy inputs are int64\n\n* Move the test into the existing int64 test and update a lot of existing dummies\n\n* Fix remaining dummies\n\n* Fix remaining dummies\n\n* Test for int64 serving sigs as well\n\n* Update core tests to use tf.int64\n\n* Add better messages to the assertions\n\n* Update all serving sigs to int64\n\n* More sneaky hiding tf.int32s\n\n* Add an optional int32 signature in save_pretrained\n\n* make fixup\n\n* Add Amy's suggestions\n\n* Switch all serving sigs back to tf.int32\n\n* Switch all dummies to tf.int32\n\n* Adjust tests to check for tf.int32 instead of tf.int64\n\n* Fix base dummy_inputs dtype\n\n* Start casting to tf.int32 in input_processing\n\n* Change dtype for unpack_inputs test\n\n* Add proper tf.int32 test\n\n* Make the alternate serving signature int64\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFGPTJPreTrainedModel(TFPreTrainedModel):",
            "Returns:",
            "`Dict[str, tf.Tensor]`: The dummy inputs.",
            "\"\"\"",
            "-        dummy = {\"input_ids\": tf.constant(DUMMY_INPUTS)}",
            "+        dummy = {\"input_ids\": tf.constant(DUMMY_INPUTS, dtype=tf.int32)}",
            "return dummy",
            "",
            "@tf.function(",
            "input_signature=[",
            "{",
            "-                \"input_ids\": tf.TensorSpec((None, None), tf.int64, name=\"input_ids\"),",
            "-                \"attention_mask\": tf.TensorSpec((None, None), tf.int64, name=\"attention_mask\"),",
            "+                \"input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"input_ids\"),",
            "+                \"attention_mask\": tf.TensorSpec((None, None), tf.int32, name=\"attention_mask\"),",
            "}",
            "]",
            ")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 17,
        "number": 5718,
        "neg_line": [
            "-dummy = {\"input_ids\": tf.constant(DUMMY_INPUTS)}",
            "-\"input_ids\": tf.TensorSpec((None, None), tf.int64, name=\"input_ids\"),",
            "-\"attention_mask\": tf.TensorSpec((None, None), tf.int64, name=\"attention_mask\"),"
        ],
        "pos_line": [
            "+dummy = {\"input_ids\": tf.constant(DUMMY_INPUTS, dtype=tf.int32)}",
            "+\"input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"input_ids\"),",
            "+\"attention_mask\": tf.TensorSpec((None, None), tf.int32, name=\"attention_mask\"),"
        ],
        "core_change": "-dummy = {\"input_ids\": tf.constant(DUMMY_INPUTS)} +dummy = {\"input_ids\": tf.constant(DUMMY_INPUTS, dtype=tf.int32)} -\"input_ids\": tf.TensorSpec((None, None), tf.int64, name=\"input_ids\"), -\"attention_mask\": tf.TensorSpec((None, None), tf.int64, name=\"attention_mask\"), +\"input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"input_ids\"), +\"attention_mask\": tf.TensorSpec((None, None), tf.int32, name=\"attention_mask\"),",
        "core_API": "constant"
    },
    {
        "commit_hash": "475565ac9387228d898ef4851efbe846c12e1e14",
        "index": "f9c9488b..7cd08602 100644",
        "commit_message": "fix layer norm test\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_layer_norm(affine):",
            "torch.jit.script(norm)",
            "out1 = norm(x)",
            "assert out1.size() == (100, 16)",
            "-    assert torch.allclose(norm(x, batch), out1)",
            "+    assert torch.allclose(norm(x, batch), out1, atol=1e-6)",
            "",
            "out2 = norm(torch.cat([x, x], dim=0), torch.cat([batch, batch + 1], dim=0))",
            "-    assert torch.allclose(out1, out2[:100])",
            "-    assert torch.allclose(out1, out2[100:])",
            "+    assert torch.allclose(out1, out2[:100], atol=1e-6)",
            "+    assert torch.allclose(out1, out2[100:], atol=1e-6)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=994690)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=994691)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=994692)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'allclose'), position=2, insert_id=994693)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=994694)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=994695)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=994696)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=994697)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=994698)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=994699)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'atol'), position=0, insert_id=994700)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=994701)",
            "Insert(target_node=IN(type=keyword_argument), node=('float', '1e-6'), position=2, insert_id=994702)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'atol'), position=0, insert_id=994703)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=994704)",
            "Insert(target_node=IN(type=keyword_argument), node=('float', '1e-6'), position=2, insert_id=994705)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'atol'), position=0, insert_id=994706)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=994707)",
            "Insert(target_node=IN(type=keyword_argument), node=('float', '1e-6'), position=2, insert_id=994708)",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=allclose))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 24,
        "number": 5720,
        "neg_line": [
            "-assert torch.allclose(norm(x, batch), out1)",
            "-assert torch.allclose(out1, out2[:100])",
            "-assert torch.allclose(out1, out2[100:])"
        ],
        "pos_line": [
            "+assert torch.allclose(norm(x, batch), out1, atol=1e-6)",
            "+assert torch.allclose(out1, out2[:100], atol=1e-6)",
            "+assert torch.allclose(out1, out2[100:], atol=1e-6)"
        ],
        "core_change": "-assert torch.allclose(norm(x, batch), out1) +assert torch.allclose(norm(x, batch), out1, atol=1e-6) -assert torch.allclose(out1, out2[:100]) -assert torch.allclose(out1, out2[100:]) +assert torch.allclose(out1, out2[:100], atol=1e-6) +assert torch.allclose(out1, out2[100:], atol=1e-6)",
        "core_API": "script"
    },
    {
        "commit_hash": "192e5c52b0da0bbd5f464adc0a0ef35bc59bf653",
        "index": "a42f3c4d2..0b519ed77 100644",
        "commit_message": "fix legacy creation (#16282)\n\n\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def main_train(dir_path, max_epochs: int = 20):",
            "seed_everything(42)",
            "stopping = EarlyStopping(monitor=\"val_acc\", mode=\"max\", min_delta=0.005)",
            "trainer = pl.Trainer(",
            "+        accelerator=\"auto\",",
            "default_root_dir=dir_path,",
            "-        devices=int(torch.cuda.is_available()),",
            "precision=(16 if torch.cuda.is_available() else 32),",
            "callbacks=[stopping],",
            "min_epochs=3,"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=keyword_argument), node=ASTNode(type=ERROR), position=5)",
            "Insert(target_node=ASTNode(type=ERROR), node=('keyword_argument', None), position=4, insert_id=491632)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'accelerator'), position=0, insert_id=491633)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=491634)",
            "Insert(target_node=IN(type=keyword_argument), node=('string', '\"auto\"'), position=2, insert_id=491635)",
            "Delete(target_node=ASTNode(type=identifier, text=devices))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=int))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=cuda))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=is_available))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 24,
        "number": 5722,
        "neg_line": [
            "-devices=int(torch.cuda.is_available()),"
        ],
        "pos_line": [
            "+accelerator=\"auto\","
        ],
        "core_change": "+accelerator=\"auto\", -devices=int(torch.cuda.is_available()),",
        "core_API": "Trainer"
    },
    {
        "commit_hash": "f6a5c359cc4d6d34df18a6f6ab90b1462395eef9",
        "index": "764129e8..115af6f9 100644",
        "commit_message": "[Community] Fix merger (#2006)\n\n* [Community] Fix merger\n\n* finish\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class CheckpointMergerPipeline(DiffusionPipeline):",
            "theta_0 = theta_0()",
            "",
            "update_theta_0 = getattr(module, \"load_state_dict\")",
            "-                    theta_1 = torch.load(checkpoint_path_1)",
            "+                    theta_1 = torch.load(checkpoint_path_1, map_location=\"cpu\")",
            "",
            "-                    theta_2 = torch.load(checkpoint_path_2) if checkpoint_path_2 else None",
            "+                    theta_2 = torch.load(checkpoint_path_2, map_location=\"cpu\") if checkpoint_path_2 else None",
            "",
            "if not theta_0.keys() == theta_1.keys():",
            "print(\"SKIPPING ATTR \", attr, \" DUE TO MISMATCH\")"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=90517)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=90518)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=90519)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'load'), position=2, insert_id=90520)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=90521)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=90522)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'map_location'), position=0, insert_id=90523)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=90524)",
            "Insert(target_node=IN(type=keyword_argument), node=('string', '\"cpu\"'), position=2, insert_id=90525)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=90526)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=90527)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'map_location'), position=0, insert_id=90528)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=90529)",
            "Insert(target_node=IN(type=keyword_argument), node=('string', '\"cpu\"'), position=2, insert_id=90530)",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=load))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 19,
        "number": 5723,
        "neg_line": [
            "-theta_1 = torch.load(checkpoint_path_1)",
            "-theta_2 = torch.load(checkpoint_path_2) if checkpoint_path_2 else None"
        ],
        "pos_line": [
            "+theta_1 = torch.load(checkpoint_path_1, map_location=\"cpu\")",
            "+theta_2 = torch.load(checkpoint_path_2, map_location=\"cpu\") if checkpoint_path_2 else None"
        ],
        "core_change": "-theta_1 = torch.load(checkpoint_path_1) +theta_1 = torch.load(checkpoint_path_1, map_location=\"cpu\") -theta_2 = torch.load(checkpoint_path_2) if checkpoint_path_2 else None +theta_2 = torch.load(checkpoint_path_2, map_location=\"cpu\") if checkpoint_path_2 else None",
        "core_API": "load"
    },
    {
        "commit_hash": "a6bcfb80156fac34c40a3b8dcd973c4a990d75ca",
        "index": "2c761ef51..90bdb231f 100644",
        "commit_message": "fix tests\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "TF2_WEIGHTS_NAME = 'tf_model.h5'",
            "TF_WEIGHTS_NAME = 'model.ckpt'",
            "CONFIG_NAME = \"config.json\"",
            "",
            "-logger = logging.getLogger(__name__)  # pylint: disable=invalid-name",
            "-",
            "def is_torch_available():",
            "return _torch_available"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=logger))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=logging))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=getLogger))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=__name__))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 0,
        "minus_line": 0,
        "AST_diff_line": 13,
        "number": 5730,
        "neg_line": [
            "-logger = logging.getLogger(__name__)  # pylint: disable=invalid-name",
            "-"
        ],
        "pos_line": [],
        "core_change": "-logger = logging.getLogger(__name__)  # pylint: disable=invalid-name -",
        "core_API": "getLogger"
    },
    {
        "commit_hash": "185a0df5991220e9cb5b7e6f639e22332f786e2d",
        "index": "419badf5..4c23b115 100644",
        "commit_message": "Fix warning about deprecated `volatile` kwarg for Variables\n\n",
        "file": "fairseq.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "autograd",
        "change": [
            "class LinearizedConvolution(ConvTBC):",
            "self.input_buffer[:, :-1, :] = self.input_buffer[:, 1:, :].clone()",
            "# append next input",
            "self.input_buffer[:, -1, :] = input[:, -1, :]",
            "-            input = torch.autograd.Variable(self.input_buffer, volatile=True)",
            "+            input = utils.volatile_variable(self.input_buffer)",
            "output = F.linear(input.view(bsz, -1), weight, self.bias)",
            "return output.view(bsz, 1, -1)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=torch), value='utils')",
            "Update(target_node=ASTNode(type=identifier, text=autograd), value='volatile_variable')",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=Variable))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=volatile))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=true, text=True))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 5734,
        "neg_line": [
            "-input = torch.autograd.Variable(self.input_buffer, volatile=True)"
        ],
        "pos_line": [
            "+input = utils.volatile_variable(self.input_buffer)"
        ],
        "core_change": "-input = torch.autograd.Variable(self.input_buffer, volatile=True) +input = utils.volatile_variable(self.input_buffer)",
        "core_API": "Variable"
    },
    {
        "commit_hash": "3db43cc5d2678e1bfadefce3b3303516de8d69fc",
        "index": "29fbed04..d7ff3cf8 100644",
        "commit_message": "GH-2534: fix language model training\n\n",
        "file": "flair.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class LanguageModelTrainer:",
            "# not really sure what this does",
            "ntokens = len(self.corpus.dictionary)",
            "",
            "-                    total_loss = torch.zeros(1)",
            "+                    total_loss = torch.zeros(1, device=flair.device)",
            "start_time = time.time()",
            "",
            "for batch, i in enumerate("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=235248)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=235249)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=235250)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=235251)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=235252)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'flair'), position=0, insert_id=235253)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=235254)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=235255)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 5735,
        "neg_line": [
            "-total_loss = torch.zeros(1)"
        ],
        "pos_line": [
            "+total_loss = torch.zeros(1, device=flair.device)"
        ],
        "core_change": "-total_loss = torch.zeros(1) +total_loss = torch.zeros(1, device=flair.device)",
        "core_API": "zeros"
    },
    {
        "commit_hash": "da2f1325014a158be36d62bdd20293ef97e574f4",
        "index": "050878ba..2cc97285 100644",
        "commit_message": "enable black in the precommit (#1777)\n\n* enable black in the precommit\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add some fixes\n\n* update libface detection url\n\n* added url from kornia checkpoint\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "from kornia.augmentation.random_generator import DistributionWithMapper",
            "",
            "",
            "class TestDistMapper:",
            "-",
            "-    def test_mapper(self,):",
            "+    def test_mapper(self):",
            "_ = torch.manual_seed(0)",
            "-        dist = DistributionWithMapper(Normal(0., 1.,), map_fn=nn.Sigmoid())",
            "+        dist = DistributionWithMapper(Normal(0.0, 1.0), map_fn=nn.Sigmoid())",
            "out = dist.rsample((8,))",
            "-        exp = torch.tensor([",
            "-            0.8236, 0.4272, 0.1017, 0.6384, 0.2527, 0.1980, 0.5995, 0.6980",
            "-        ])",
            "+        exp = torch.tensor([0.8236, 0.4272, 0.1017, 0.6384, 0.2527, 0.1980, 0.5995, 0.6980])",
            "assert_allclose(out, exp, rtol=1e-4, atol=1e-4)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=float, text=0.), value='0.0')",
            "Update(target_node=ASTNode(type=float, text=1.), value='1.0')",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 3,
        "minus_line": 5,
        "AST_diff_line": 4,
        "number": 5736,
        "neg_line": [
            "-",
            "-def test_mapper(self,):",
            "-dist = DistributionWithMapper(Normal(0., 1.,), map_fn=nn.Sigmoid())",
            "-exp = torch.tensor([",
            "-0.8236, 0.4272, 0.1017, 0.6384, 0.2527, 0.1980, 0.5995, 0.6980",
            "-])"
        ],
        "pos_line": [
            "+def test_mapper(self):",
            "+dist = DistributionWithMapper(Normal(0.0, 1.0), map_fn=nn.Sigmoid())",
            "+exp = torch.tensor([0.8236, 0.4272, 0.1017, 0.6384, 0.2527, 0.1980, 0.5995, 0.6980])"
        ],
        "core_change": "- -def test_mapper(self,): +def test_mapper(self): -dist = DistributionWithMapper(Normal(0., 1.,), map_fn=nn.Sigmoid()) +dist = DistributionWithMapper(Normal(0.0, 1.0), map_fn=nn.Sigmoid()) -exp = torch.tensor([ -0.8236, 0.4272, 0.1017, 0.6384, 0.2527, 0.1980, 0.5995, 0.6980 -]) +exp = torch.tensor([0.8236, 0.4272, 0.1017, 0.6384, 0.2527, 0.1980, 0.5995, 0.6980])",
        "core_API": "manual_seed"
    },
    {
        "commit_hash": "d45fc7da3d43ff29ca597f5ffa8cf3151d705013",
        "index": "6faed43ed..38e47103c 100644",
        "commit_message": "[Speech Examples] Add pytorch speech pretraining (#13877)\n\n* adapt wav2vec2\n\n* add example\n\n* add files\n\n* adapt\n\n* remove bogus file\n\n* Apply suggestions from code review\n\n* adapt files more\n\n* upload changes\n\n* del old files\n\n* up\n\n* up\n\n* up\n\n* up\n\n* up\n\n* correct gradient checkpoitning\n\n* add readme\n\n* finish\n\n* finish\n\n* up\n\n* more fixes\n\n* up\n\n* up\n\n* add demo run to readme\n\n* up\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class HubertUtilsTest(unittest.TestCase):",
            "mask_prob = 0.5",
            "mask_length = 4",
            "",
            "-        mask = _compute_mask_indices((batch_size, sequence_length), mask_prob, mask_length, torch_device)",
            "+        mask = _compute_mask_indices((batch_size, sequence_length), mask_prob, mask_length)",
            "+        mask = torch.from_numpy(mask).to(torch_device)",
            "",
            "# because of overlap mask don't have to add up exactly to `mask_prob * sequence_length`, but have to be smaller or equal",
            "for batch_sum in mask.sum(axis=-1):"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=4, insert_id=1210628)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1210629)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'mask'), position=0, insert_id=1210630)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1210631)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=1210632)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1210633)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1210634)",
            "Insert(target_node=IN(type=attribute), node=('call', None), position=0, insert_id=1210635)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1210636)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to'), position=2, insert_id=1210637)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1210638)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'torch_device'), position=1, insert_id=1210639)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1210640)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1210641)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1210642)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1210643)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1210644)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'from_numpy'), position=2, insert_id=1210645)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1210646)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'mask'), position=1, insert_id=1210647)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1210648)",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=torch_device))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 23,
        "number": 5738,
        "neg_line": [
            "-mask = _compute_mask_indices((batch_size, sequence_length), mask_prob, mask_length, torch_device)"
        ],
        "pos_line": [
            "+mask = _compute_mask_indices((batch_size, sequence_length), mask_prob, mask_length)",
            "+mask = torch.from_numpy(mask).to(torch_device)"
        ],
        "core_change": "-mask = _compute_mask_indices((batch_size, sequence_length), mask_prob, mask_length, torch_device) +mask = _compute_mask_indices((batch_size, sequence_length), mask_prob, mask_length) +mask = torch.from_numpy(mask).to(torch_device)",
        "core_API": "from_numpy"
    },
    {
        "commit_hash": "4a88dfc35630c54f27673106694cc124dfa0e337",
        "index": "d5e7268f..dd8c66d8 100644",
        "commit_message": "always reuse when not training. otherwise reuse can be set back to False in TF<1.1 (fix #277)\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TowerContext(object):",
            "self._ctxs.append(tf.variable_scope(self._name))",
            "else:",
            "# use existing variable scope",
            "+                reuse = self.index > 0 or (not self.is_training)",
            "self._ctxs.append(tf.variable_scope(",
            "-                    tf.get_variable_scope(), reuse=self.index > 0))",
            "+                    tf.get_variable_scope(), reuse=reuse))",
            "self._ctxs.append(tf.name_scope(self._name))",
            "self._ctxs.append(tf.device(self._device))",
            "for c in self._ctxs:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=2, insert_id=2301678)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=2301679)",
            "Move(target_node=ASTNode(type=expression_statement), node=ASTNode(type=call), position=0)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=identifier, text=else), position=0)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=:, text=:), position=1)",
            "Insert(target_node=IN(type=assignment), node=('type', None), position=2, insert_id=2301680)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=3, insert_id=2301681)",
            "Insert(target_node=IN(type=assignment), node=('boolean_operator', None), position=4, insert_id=2301682)",
            "Insert(target_node=IN(type=type), node=('identifier', 'reuse'), position=0, insert_id=2301683)",
            "Move(target_node=IN(type=boolean_operator), node=ASTNode(type=comparison_operator), position=0)",
            "Insert(target_node=IN(type=boolean_operator), node=('or', 'or'), position=1, insert_id=2301684)",
            "Insert(target_node=IN(type=boolean_operator), node=('parenthesized_expression', None), position=2, insert_id=2301685)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('(', '('), position=0, insert_id=2301686)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('not_operator', None), position=1, insert_id=2301687)",
            "Insert(target_node=IN(type=parenthesized_expression), node=(')', ')'), position=2, insert_id=2301688)",
            "Insert(target_node=IN(type=not_operator), node=('not', 'not'), position=0, insert_id=2301689)",
            "Insert(target_node=IN(type=not_operator), node=('attribute', None), position=1, insert_id=2301690)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=2301691)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2301692)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'is_training'), position=2, insert_id=2301693)",
            "Insert(target_node=ASTNode(type=keyword_argument), node=('identifier', 'reuse'), position=2, insert_id=2301694)",
            "Delete(target_node=ASTNode(type=type))",
            "Delete(target_node=ASTNode(type=assignment))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 23,
        "number": 5744,
        "neg_line": [
            "-tf.get_variable_scope(), reuse=self.index > 0))"
        ],
        "pos_line": [
            "+reuse = self.index > 0 or (not self.is_training)",
            "+tf.get_variable_scope(), reuse=reuse))"
        ],
        "core_change": "+reuse = self.index > 0 or (not self.is_training) -tf.get_variable_scope(), reuse=self.index > 0)) +tf.get_variable_scope(), reuse=reuse))",
        "core_API": "append"
    },
    {
        "commit_hash": "7c76e76330b5cbc3400b4a3967d68a2ff1844abb",
        "index": "b8192a64..aeec7826 100644",
        "commit_message": "fix serving export (fix #1449)\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class ModelExporter(object):",
            "\"\"\"",
            "if tags is None:",
            "tags = (tf.saved_model.SERVING if get_tf_version_tuple() >= (1, 12)",
            "-                    else tf.saved_model.tag_constants.SERVING)",
            "+                    else tf.saved_model.tag_constants.SERVING, )",
            "",
            "self.graph = self.config._maybe_create_graph()",
            "with self.graph.as_default():"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('tuple', None), position=2, insert_id=2273131)",
            "Move(target_node=IN(type=tuple), node=ASTNode(type=(, text=(), position=0)",
            "Move(target_node=IN(type=tuple), node=ASTNode(type=conditional_expression), position=1)",
            "Insert(target_node=IN(type=tuple), node=(',', ','), position=2, insert_id=2273132)",
            "Move(target_node=IN(type=tuple), node=ASTNode(type=), text=)), position=3)",
            "Delete(target_node=ASTNode(type=parenthesized_expression))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 5745,
        "neg_line": [
            "-else tf.saved_model.tag_constants.SERVING)"
        ],
        "pos_line": [
            "+else tf.saved_model.tag_constants.SERVING, )"
        ],
        "core_change": "-else tf.saved_model.tag_constants.SERVING) +else tf.saved_model.tag_constants.SERVING, )",
        "core_API": "_maybe_create_graph"
    },
    {
        "commit_hash": "da2f1325014a158be36d62bdd20293ef97e574f4",
        "index": "8b5d70a7..d6aed9ba 100644",
        "commit_message": "enable black in the precommit (#1777)\n\n* enable black in the precommit\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add some fixes\n\n* update libface detection url\n\n* added url from kornia checkpoint\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TFeat(nn.Module):",
            "# use torch.hub to load pretrained model",
            "if pretrained:",
            "storage_fcn: Callable = lambda storage, loc: storage",
            "-            pretrained_dict = torch.hub.load_state_dict_from_url(",
            "-                urls['liberty'], map_location=storage_fcn",
            "-            )",
            "+            pretrained_dict = torch.hub.load_state_dict_from_url(urls['liberty'], map_location=storage_fcn)",
            "self.load_state_dict(pretrained_dict, strict=True)",
            "self.eval()"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 17,
        "number": 5749,
        "neg_line": [
            "-pretrained_dict = torch.hub.load_state_dict_from_url(",
            "-urls['liberty'], map_location=storage_fcn",
            "-)"
        ],
        "pos_line": [
            "+pretrained_dict = torch.hub.load_state_dict_from_url(urls['liberty'], map_location=storage_fcn)"
        ],
        "core_change": "-pretrained_dict = torch.hub.load_state_dict_from_url( -urls['liberty'], map_location=storage_fcn -) +pretrained_dict = torch.hub.load_state_dict_from_url(urls['liberty'], map_location=storage_fcn)",
        "core_API": "load_state_dict_from_url"
    },
    {
        "commit_hash": "8b9631fc5a06149fb6fc5465305217bf5ae31c3e",
        "index": "f736841..7eeef94 100644",
        "commit_message": "Fix mtf broadcast error when lowering\n\nWe don't need to add the temporary dimensions to tensors before broadcasting them\n\nThis fixes:\nValueError: No new dimensions allowed in output input_shapes = [[Dimension(name='batch', size=256), Dimension(name='sequence', size=128)]] output_shape= [Dimension(name='dummy_batch', size=1), Dimension(name='sequence', size=128)]\n\n",
        "file": "gpt-neo.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def expand_tile(value, newdim):",
            "print(value)",
            "print('############')",
            "",
            "-    return mtf.broadcast(mtf_expand_dims(value, 'dummy_batch', 0),",
            "+    return mtf.broadcast(mtf_expand_dims(value, newdim, 0),",
            "[newdim] + value.shape.dims)  # shape.dims gets us a list which we need in order to concat"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'newdim'), position=3, insert_id=1942768)",
            "Delete(target_node=ASTNode(type=string, text='dummy_batch'))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 5755,
        "neg_line": [
            "-return mtf.broadcast(mtf_expand_dims(value, 'dummy_batch', 0),"
        ],
        "pos_line": [
            "+return mtf.broadcast(mtf_expand_dims(value, newdim, 0),"
        ],
        "core_change": "-return mtf.broadcast(mtf_expand_dims(value, 'dummy_batch', 0), +return mtf.broadcast(mtf_expand_dims(value, newdim, 0),",
        "core_API": "broadcast"
    },
    {
        "commit_hash": "ce8f42b12ecd522e3ef7251e0d5f5175075a3fb4",
        "index": "4649b483..068290d3 100644",
        "commit_message": "Fixes to enable dev branch to move to pytorch-1.2 (#2001)\n\n* Fixes to enable dev branch to move to pytorch-1.2\n\n* change name for published cpu wheels\n\n* fix failing unit tests\n\n* change docs conf\n\n* change tracking example\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_unconstrained_to_corr_cholesky_transform(y_shape):",
            "log_det = transform.log_abs_det_jacobian(y, x)",
            "assert log_det.shape == y_shape[:-1]",
            "if len(y_shape) == 1:",
            "-        triu_index = x.new_ones(x.shape).triu(diagonal=1).to(torch.uint8)",
            "+        triu_index = x.new_ones(x.shape).triu(diagonal=1).to(torch.bool)",
            "x_tril_vector = x.t()[triu_index]",
            "assert_tensors_equal(_autograd_log_det(x_tril_vector, y), log_det, prec=1e-4)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=uint8), value='bool')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 5757,
        "neg_line": [
            "-triu_index = x.new_ones(x.shape).triu(diagonal=1).to(torch.uint8)"
        ],
        "pos_line": [
            "+triu_index = x.new_ones(x.shape).triu(diagonal=1).to(torch.bool)"
        ],
        "core_change": "-triu_index = x.new_ones(x.shape).triu(diagonal=1).to(torch.uint8) +triu_index = x.new_ones(x.shape).triu(diagonal=1).to(torch.bool)",
        "core_API": "log_abs_det_jacobian"
    },
    {
        "commit_hash": "b8bb14960a55e08a2c69f2d17b129b33e7ae3632",
        "index": "4462631c..58e28f94 100644",
        "commit_message": "Fix variable name typo in error message for launch.py\n\nCOMMANDINE_ARGS -> COMMANDLINE_ARGS\n",
        "file": "stable-diffusion-webui.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "cuda",
        "change": [
            "if not is_installed(\"torch\") or not is_installed(\"torchvision\"):",
            "run(f'\"{python}\" -m {torch_command}', \"Installing torch and torchvision\", \"Couldn't install torch\")",
            "",
            "if not skip_torch_cuda_test:",
            "-    run_python(\"import torch; assert torch.cuda.is_available(), 'Torch is not able to use GPU; add --skip-torch-cuda-test to COMMANDINE_ARGS variable to disable this check'\")",
            "+    run_python(\"import torch; assert torch.cuda.is_available(), 'Torch is not able to use GPU; add --skip-torch-cuda-test to COMMANDLINE_ARGS variable to disable this check'\")",
            "",
            "if not is_installed(\"k_diffusion.sampling\"):",
            "run_pip(f\"install {k_diffusion_package}\", \"k-diffusion\")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"import torch; assert torch.cuda.is_available(), 'Torch is not able to use GPU; add --skip-torch-cuda-test to COMMANDINE_ARGS variable to disable this check'\"), value='\"import torch; assert torch.cuda.is_available(), \\'Torch is not able to use GPU; add --skip-torch-cuda-test to COMMANDLINE_ARGS variable to disable this check\\'\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 5758,
        "neg_line": [
            "-run_python(\"import torch; assert torch.cuda.is_available(), 'Torch is not able to use GPU; add --skip-torch-cuda-test to COMMANDINE_ARGS variable to disable this check'\")"
        ],
        "pos_line": [
            "+run_python(\"import torch; assert torch.cuda.is_available(), 'Torch is not able to use GPU; add --skip-torch-cuda-test to COMMANDLINE_ARGS variable to disable this check'\")"
        ],
        "core_change": "-run_python(\"import torch; assert torch.cuda.is_available(), 'Torch is not able to use GPU; add --skip-torch-cuda-test to COMMANDINE_ARGS variable to disable this check'\") +run_python(\"import torch; assert torch.cuda.is_available(), 'Torch is not able to use GPU; add --skip-torch-cuda-test to COMMANDLINE_ARGS variable to disable this check'\")",
        "core_API": "is_available"
    },
    {
        "commit_hash": "06a1991d4153fda633af6228542f172c8cf8a9fd",
        "index": "536581a2..28e9c978 100644",
        "commit_message": ":elephant: Remove warnings during testing (#1401)\n\n* fix warnings during testing\n\n* Update test/geometry/transform/test_imgwarp.py\n\n* remove warning in clahe due to //\n\n* Apply suggestions from code review\n\nCo-authored-by: Luis Ferraz <luisferrazc@gmail.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestDivergenceLoss:",
            "target = torch.randn((2, 4, 10, 16), dtype=dtype, device=device)",
            "args = (input, target)",
            "op = kornia.losses.js_div_loss_2d",
            "-        op_jit = torch.jit.script(op, args)",
            "+        op_jit = torch.jit.script(op)",
            "assert_close(op(*args), op_jit(*args), rtol=0, atol=1e-5)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=args))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 5761,
        "neg_line": [
            "-op_jit = torch.jit.script(op, args)"
        ],
        "pos_line": [
            "+op_jit = torch.jit.script(op)"
        ],
        "core_change": "-op_jit = torch.jit.script(op, args) +op_jit = torch.jit.script(op)",
        "core_API": "randn"
    },
    {
        "commit_hash": "cd1e5ca34d124a9bfcb0b4981ed88dfe8b7cbf8b",
        "index": "beb84bfc..9d022410 100644",
        "commit_message": "fixed code formatting\n\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TrainingTest(test_combinations.TestCase):",
            "@test_combinations.run_all_keras_modes(always_skip_v1=True)",
            "def test_distribution_reduction_method_sum(self):",
            "",
            "-        strategy = tf.distribute.MirroredStrategy([\"/cpu:1\", \"/cpu:2\", \"/cpu:3\", \"/cpu:4\"])",
            "+        strategy = tf.distribute.MirroredStrategy(",
            "+            [\"/cpu:1\", \"/cpu:2\", \"/cpu:3\", \"/cpu:4\"]",
            "+        )",
            "BATCH_SIZE = 10",
            "",
            "class MyModel(training_module.Model):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 5762,
        "neg_line": [
            "-strategy = tf.distribute.MirroredStrategy([\"/cpu:1\", \"/cpu:2\", \"/cpu:3\", \"/cpu:4\"])"
        ],
        "pos_line": [
            "+strategy = tf.distribute.MirroredStrategy(",
            "+[\"/cpu:1\", \"/cpu:2\", \"/cpu:3\", \"/cpu:4\"]",
            "+)"
        ],
        "core_change": "-strategy = tf.distribute.MirroredStrategy([\"/cpu:1\", \"/cpu:2\", \"/cpu:3\", \"/cpu:4\"]) +strategy = tf.distribute.MirroredStrategy( +[\"/cpu:1\", \"/cpu:2\", \"/cpu:3\", \"/cpu:4\"] +)",
        "core_API": "run_all_keras_modes"
    },
    {
        "commit_hash": "48a3208c39d9d4e5fcd5dd2942a9e8751cfe2674",
        "index": "c7980e26..73bd1e56 100644",
        "commit_message": "Fixed nightly errors (#886)\n\n\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestCenterCropGen3D(RandomGeneratorBaseTests):",
            "[0, 0, 119],",
            "[99, 0, 119],",
            "[99, 149, 119],",
            "-                 [0, 149, 119]]], device=device, dtype=torch.long),",
            "+                 [0, 149, 119]]], device=device, dtype=torch.long).repeat(2, 1, 1),",
            ")",
            "assert res.keys() == expected.keys()",
            "assert_allclose(res['src'].to(device=device), expected['src'], atol=1e-4, rtol=1e-4)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=expression_statement), node=('call', None), position=7, insert_id=429624)",
            "Insert(target_node=ASTNode(type=expression_statement), node=(',', ','), position=8, insert_id=429625)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=429626)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=429627)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=ERROR), position=1)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=2, insert_id=429628)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'repeat'), position=3, insert_id=429629)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=429630)",
            "Insert(target_node=IN(type=argument_list), node=('integer', '2'), position=1, insert_id=429631)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=,, text=,), position=2)",
            "Insert(target_node=IN(type=argument_list), node=('integer', '1'), position=3, insert_id=429632)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=4, insert_id=429633)",
            "Insert(target_node=IN(type=argument_list), node=('integer', '1'), position=5, insert_id=429634)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=6, insert_id=429635)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 15,
        "number": 5763,
        "neg_line": [
            "-[0, 149, 119]]], device=device, dtype=torch.long),"
        ],
        "pos_line": [
            "+[0, 149, 119]]], device=device, dtype=torch.long).repeat(2, 1, 1),"
        ],
        "core_change": "-[0, 149, 119]]], device=device, dtype=torch.long), +[0, 149, 119]]], device=device, dtype=torch.long).repeat(2, 1, 1),",
        "core_API": "keys"
    },
    {
        "commit_hash": "8178687ace4e3458d4d516dedf1ebbd134654add",
        "index": "372fac88..b2b8d1ee 100644",
        "commit_message": "Fixes input layer naming bugs via explicit naming propagation, including unit tests for TypeSpec naming, saving, and exporting.\n\nPiperOrigin-RevId: 521005984\n\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class ReloadedLayer(base_layer.Layer):",
            "",
            "",
            "def _make_tensor_spec(x):",
            "-    return tf.TensorSpec(x.shape, dtype=x.dtype)",
            "+    return tf.TensorSpec(x.shape, dtype=x.dtype, name=x.name)",
            "",
            "",
            "def _print_signature(fn, name):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=2039920)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=2039921)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'name'), position=0, insert_id=2039922)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2039923)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=2039924)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'x'), position=0, insert_id=2039925)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2039926)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'name'), position=2, insert_id=2039927)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 5766,
        "neg_line": [
            "-return tf.TensorSpec(x.shape, dtype=x.dtype)"
        ],
        "pos_line": [
            "+return tf.TensorSpec(x.shape, dtype=x.dtype, name=x.name)"
        ],
        "core_change": "-return tf.TensorSpec(x.shape, dtype=x.dtype) +return tf.TensorSpec(x.shape, dtype=x.dtype, name=x.name)",
        "core_API": "TensorSpec"
    },
    {
        "commit_hash": "9dd56398e3f2dde97f3e2f2aad651549adb992e4",
        "index": "d0c015287..064e1773e 100644",
        "commit_message": "fixing some compatibility with PT 1.8 (#5864)\n\n* change default\n\n* .\n\n* p\n\n* 0.21.2\n\n* .\n\n* fix\n\n* .\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_metrics_mod(second_operand, expected_result):",
            "final_mod = first_metric % second_operand",
            "",
            "assert isinstance(final_mod, CompositionalMetric)",
            "-",
            "-    assert torch.allclose(expected_result, final_mod.compute())",
            "+    # prevent Runtime error for PT 1.8 - Long did not match Float",
            "+    assert torch.allclose(expected_result.to(float), final_mod.compute().to(float))",
            "",
            "",
            "@pytest.mark.parametrize("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=1, insert_id=548405)",
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=4, insert_id=548406)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=5, insert_id=548407)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=548408)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=548409)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=548410)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=548411)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=expected_result), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=548412)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to'), position=2, insert_id=548413)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=548414)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'float'), position=1, insert_id=548415)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=548416)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=548417)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to'), position=2, insert_id=548418)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=548419)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'float'), position=1, insert_id=548420)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=), text=)), position=2)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 19,
        "number": 5767,
        "neg_line": [
            "-",
            "-assert torch.allclose(expected_result, final_mod.compute())"
        ],
        "pos_line": [
            "+# prevent Runtime error for PT 1.8 - Long did not match Float",
            "+assert torch.allclose(expected_result.to(float), final_mod.compute().to(float))"
        ],
        "core_change": "- -assert torch.allclose(expected_result, final_mod.compute()) +# prevent Runtime error for PT 1.8 - Long did not match Float +assert torch.allclose(expected_result.to(float), final_mod.compute().to(float))",
        "core_API": "allclose"
    },
    {
        "commit_hash": "46779540edaaf9963660cb36c9998913195399dc",
        "index": "dfe3a899e..370c13791 100644",
        "commit_message": "Modify librispeech run.sh for improved LM training and fix some LM-related bugs\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def recog(args):",
            "if args.rnnlm:",
            "rnnlm_args = get_model_conf(args.rnnlm, args.rnnlm_conf)",
            "rnnlm = lm_pytorch.ClassifierWithState(",
            "-            lm_pytorch.RNNLM(len(train_args.char_list), rnnlm_args.layer, rnnlm_args.unit))",
            "+            lm_pytorch.RNNLM(",
            "+                len(train_args.char_list), rnnlm_args.layer, rnnlm_args.unit))",
            "torch_load(args.rnnlm, rnnlm)",
            "rnnlm.eval()",
            "else:",
            "rnnlm = None",
            "",
            "if args.word_rnnlm:",
            "-        rnnlm_args = get_model_conf(args.word_rnnlm, args.rnnlm_conf)",
            "+        rnnlm_args = get_model_conf(args.word_rnnlm, args.word_rnnlm_conf)",
            "word_dict = rnnlm_args.char_list_dict",
            "char_dict = {x: i for i, x in enumerate(train_args.char_list)}",
            "word_rnnlm = lm_pytorch.ClassifierWithState(lm_pytorch.RNNLM("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=rnnlm_conf), value='word_rnnlm_conf')"
        ],
        "plus_line": 3,
        "minus_line": 2,
        "AST_diff_line": 1,
        "number": 5769,
        "neg_line": [
            "-lm_pytorch.RNNLM(len(train_args.char_list), rnnlm_args.layer, rnnlm_args.unit))",
            "-rnnlm_args = get_model_conf(args.word_rnnlm, args.rnnlm_conf)"
        ],
        "pos_line": [
            "+lm_pytorch.RNNLM(",
            "+len(train_args.char_list), rnnlm_args.layer, rnnlm_args.unit))",
            "+rnnlm_args = get_model_conf(args.word_rnnlm, args.word_rnnlm_conf)"
        ],
        "core_change": "-lm_pytorch.RNNLM(len(train_args.char_list), rnnlm_args.layer, rnnlm_args.unit)) +lm_pytorch.RNNLM( +len(train_args.char_list), rnnlm_args.layer, rnnlm_args.unit)) -rnnlm_args = get_model_conf(args.word_rnnlm, args.rnnlm_conf) +rnnlm_args = get_model_conf(args.word_rnnlm, args.word_rnnlm_conf)",
        "core_API": "ClassifierWithState"
    },
    {
        "commit_hash": "936b3fdeaaf772a7858f118a960bb4e6710f90d2",
        "index": "d0cd8f612..e85e8c467 100644",
        "commit_message": "Update modeling_tf_deberta.py (#13654)\n\nFixed expand_dims axis\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFDebertaDisentangledSelfAttention(tf.keras.layers.Layer):",
            "if len(shape_list_pos) == 2:",
            "relative_pos = tf.expand_dims(tf.expand_dims(relative_pos, 0), 0)",
            "elif len(shape_list_pos) == 3:",
            "-            relative_pos = tf.expand_dims(relative_pos, 0)",
            "+            relative_pos = tf.expand_dims(relative_pos, 1)",
            "# bxhxqxk",
            "elif len(shape_list_pos) != 4:",
            "raise ValueError(f\"Relative position ids must be of dim 2 or 3 or 4. {len(shape_list_pos)}\")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=integer, text=0), value='1')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 5771,
        "neg_line": [
            "-relative_pos = tf.expand_dims(relative_pos, 0)"
        ],
        "pos_line": [
            "+relative_pos = tf.expand_dims(relative_pos, 1)"
        ],
        "core_change": "-relative_pos = tf.expand_dims(relative_pos, 0) +relative_pos = tf.expand_dims(relative_pos, 1)",
        "core_API": "expand_dims"
    }
]