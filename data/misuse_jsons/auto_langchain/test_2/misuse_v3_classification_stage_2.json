[
    {
        "number": 5775,
        "comments": "",
        "commit_message": "Time Series Forecaster task (#1050)\n\n* Initial Commit for final Time Series Forecaster task\n\n* Code for Time Series Forecaster task\n\n* Completed changes for Time Series forecaster task\n\n* Fix for failing tests\n\n* Fix for output shape test for TimeseriesForecaster\n\n* Fix for test of fit and predict timeseriesforecaster\n\n* Moved StructuredData Functionality to a Mixin\n\n* Fixed Typo\n\n* Fixed typo in SupervisedStructuredDatapipeline\n",
        "label": "",
        "answer": "no",
        "change": [
            "class AutoModel(object):",
            "if isinstance(input_node, input_module.StructuredDataInput):",
            "middle_nodes.append(hypermodels.StructuredDataBlock()(input_node))",
            "if isinstance(input_node, input_module.TimeseriesInput):",
            "-                middle_nodes.append(hypermodels.TimeSeriesBlock()(input_node))",
            "+                middle_nodes.append(hypermodels.TimeseriesBlock()(input_node))",
            "",
            "# Merge the middle nodes.",
            "if len(middle_nodes) > 1:"
        ]
    },
    {
        "number": 5781,
        "comments": "",
        "commit_message": "Numerically stabilize ProjectedNormal.log_prob() via erfc (#3071)\n\n* Numerically stabilize ProjectedNormal.log_prob() via logaddexp\n\n* Fix conceptual error, now this NANs\n\n* Switch to erfc\n\n* Strengthen tests\n\n* lint\n\n* Strengthen test, clamp harder\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_gof(continuous_dist):",
            "num_samples = 50000",
            "for i in range(continuous_dist.get_num_test_data()):",
            "d = Dist(**continuous_dist.get_dist_params(i))",
            "-        samples = d.sample(torch.Size([num_samples]))",
            "+        with torch.random.fork_rng():",
            "+            samples = d.sample(torch.Size([num_samples]))",
            "with xfail_if_not_implemented():",
            "probs = d.log_prob(samples).exp()"
        ]
    },
    {
        "number": 5782,
        "comments": "",
        "commit_message": "fixed test\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestChainTensor(TestCase):",
            "x.get()",
            "x.child = x.child.child",
            "",
            "-        target = sy._PlusIsMinusTensor().on(torch.FloatTensor([1, 1]))",
            "+        # target = sy._PlusIsMinusTensor().on(torch.FloatTensor([1, 1]))",
            "+        target = torch.FloatTensor([1, 1])",
            "assert torch.equal(x.grad.data, target)"
        ]
    },
    {
        "number": 5783,
        "comments": "",
        "commit_message": "fix syntax error\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_torch_leaky_relu(",
            "num_positional_args,",
            "as_variable,",
            "with_out,",
            "-    native_array",
            "+    native_array,",
            "fw,",
            "alpha,",
            "):",
            "input_dtype, x = dtype_and_x",
            "-",
            "helpers.test_frontend_function(",
            "input_dtypes=input_dtype,",
            "as_variable_flags=as_variable,"
        ]
    },
    {
        "number": 5785,
        "comments": "",
        "commit_message": "Fix TRPO natural-gradient/linesearch optimizer bug\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class NaturalGradient(Optimizer):",
            "#     tf.math.reduce_sum(input_tensor=(loss_grad * delta))",
            "#     for loss_grad, delta in zip(loss_gradients, estimated_deltas.values())",
            "# ])",
            "-                return estimated_deltas.fmap(function=tf_util.identity)",
            "+                return [tf_util.identity(input=delta) for delta in estimated_deltas.values()]",
            "",
            "if self.only_positive_updates:",
            "# Natural gradient step only works if constant > 0 (epsilon to avoid zero division)"
        ]
    },
    {
        "number": 5786,
        "comments": "",
        "commit_message": "TF 2.0 compatibility, fixed pretrain issue\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TrackableNestedDict(NestedDict, tf.python.training.tracking.tracking.AutoT",
            "super().__setattr__(name, value)",
            "",
            "def __setitem__(self, key, value):",
            "-        value = tf.python.training.tracking.data_structures.sticky_attribute_assignment(",
            "-            trackable=self, value=value, name=key",
            "-        )",
            "+        value = sticky_attribute_assignment(trackable=self, value=value, name=key)",
            "super().__setitem__(key, value)"
        ]
    },
    {
        "number": 5787,
        "comments": "",
        "commit_message": "try to fix nanmean by isolate the casting of input dtype out for nanmean of torch backend\n",
        "label": "",
        "answer": "no",
        "change": [
            "def nanmean(",
            "dtype: Optional[torch.dtype] = None,",
            "out: Optional[torch.Tensor] = None,",
            ") -> torch.Tensor:",
            "-    return torch.nanmean(a, dim=axis, keepdim=keepdims, dtype=dtype, out=out)",
            "+    input = a.to(dtype)",
            "+    return torch.nanmean(input, dim=axis, keepdim=keepdims, out=out)",
            "",
            "",
            "nanmean.support_native_out = True"
        ]
    },
    {
        "number": 5789,
        "comments": "",
        "commit_message": "fixed problem with multi-state/action networks and scoping\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Beta(Distribution):",
            "self.max_value = max_value",
            "action_size = util.prod(self.shape)",
            "",
            "-        with tf.name_scope(name=scope):",
            "-            self.alpha = Linear(size=action_size, bias=alpha, scope='alpha')",
            "-            self.beta = Linear(size=action_size, bias=beta, scope='beta')",
            "+        self.alpha = Linear(size=action_size, bias=alpha, scope='alpha')",
            "+        self.beta = Linear(size=action_size, bias=beta, scope='beta')",
            "",
            "super(Beta, self).__init__(scope, summary_labels)"
        ]
    },
    {
        "number": 5795,
        "comments": "",
        "commit_message": "fixed flake8\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def get_layer(l_name, library=torch.nn):",
            "",
            "Returns:",
            "layer_handler (object): handler for the requested layer e.g. (torch.nn.ELU)",
            "+",
            "\"\"\"",
            "+",
            "all_torch_layers = [x for x in dir(torch.nn)]",
            "match = [x for x in all_torch_layers if l_name.lower() == x.lower()]",
            "if len(match) == 0:"
        ]
    },
    {
        "number": 5799,
        "comments": "",
        "commit_message": "Traced models serialization and torchscripting fix (#17206)\n\n* Fix torch.jit.script and pickling issues\n\n* Fix get_attr issues\n\n* Fix import in function\n\n* Fix GPT-J and T5 tracing for torch=1.11\n\n* Gate graph surgery on torch version\n\n* Modeling minor changes to enable TorchScripting\n\n* Model serialization / deserialization test\n\n* Remove _assert_is_none users\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class DecisionTransformerGPT2Attention(nn.Module):",
            "if not self.is_cross_attention:",
            "# if only \"normal\" attention layer implements causal mask",
            "query_length, key_length = query.size(-2), key.size(-2)",
            "-            causal_mask = self.bias[:, :, key_length - query_length : key_length, :key_length].bool()",
            "+            causal_mask = self.bias[:, :, key_length - query_length : key_length, :key_length].to(torch.bool)",
            "attn_weights = torch.where(causal_mask, attn_weights, self.masked_bias.to(attn_weights.dtype))",
            "",
            "if attention_mask is not None:"
        ]
    },
    {
        "number": 5800,
        "comments": "",
        "commit_message": "bugfix on model name\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "from .modeling_tf_roberta import (",
            "logger = logging.getLogger(__name__)",
            "",
            "TF_CAMEMBERT_PRETRAINED_MODEL_ARCHIVE_MAP = {",
            "-    # \"camembert-base\": \"https://s3.amazonaws.com/models.huggingface.co/bert/camembert-base-tf_model.h5\"",
            "}"
        ]
    },
    {
        "number": 5802,
        "comments": "",
        "commit_message": "CI fix3\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class SISNRLoss(TimeDomainLoss):",
            "# s_target = <s', s>s / ||s||^2",
            "pair_wise_dot = torch.sum(s_estimate * s_target, dim=1, keepdim=True)  # [B, 1]",
            "s_target_energy = (",
            "-            torch.sum(s_target**2, dim=1, keepdim=True) + self.eps",
            "+            torch.sum(s_target ** 2, dim=1, keepdim=True) + self.eps",
            ")  # [B, 1]",
            "pair_wise_proj = pair_wise_dot * s_target / s_target_energy  # [B, T]",
            "# e_noise = s' - s_target",
            "e_noise = s_estimate - pair_wise_proj  # [B, T]",
            "",
            "# SI-SNR = 10 * log_10(||s_target||^2 / ||e_noise||^2)",
            "-        pair_wise_si_snr = torch.sum(pair_wise_proj**2, dim=1) / (",
            "-            torch.sum(e_noise**2, dim=1) + self.eps",
            "+        pair_wise_si_snr = torch.sum(pair_wise_proj ** 2, dim=1) / (",
            "+            torch.sum(e_noise ** 2, dim=1) + self.eps",
            ")",
            "pair_wise_si_snr = 10 * torch.log10(pair_wise_si_snr + self.eps)  # [B]"
        ]
    },
    {
        "number": 5803,
        "comments": "",
        "commit_message": "[SequenceFeatureExtractor] Rewrite padding logic from pure python to numpy (#13650)\n\n* Test np padding\n\n* Pass feature extraction tests\n\n* Update type hints\n\n* Fix flaky integration tests\n\n* Try a more stable waveform\n\n* Add to_numpy jax support\n\n* int32 attention masks\n\n* Refactor normalization tests\n",
        "label": "",
        "answer": "no",
        "change": [
            "class SequenceFeatureExtractionTestMixin(FeatureExtractionSavingTestMixin):",
            "input_np = feat_extract.pad(processed_features, padding=\"longest\", return_tensors=\"np\")[input_name]",
            "input_tf = feat_extract.pad(processed_features, padding=\"longest\", return_tensors=\"tf\")[input_name]",
            "",
            "-        self.assertTrue(abs(input_np.astype(np.float32).sum() - input_tf.numpy().sum()) < 1e-2)",
            "+        self.assertTrue(abs(input_np.astype(np.float32).sum() - input_tf.numpy().astype(np.float32).sum()) < 1e-2)",
            "",
            "def test_attention_mask(self):",
            "feat_dict = self.feat_extract_dict"
        ]
    },
    {
        "number": 5805,
        "comments": "",
        "commit_message": "fixed NumPy and PyTorch implementations of iinfo.\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def finfo(type: Union[torch.dtype, str, torch.Tensor]) -> Finfo:",
            "",
            "",
            "def iinfo(type: Union[torch.dtype, str, torch.Tensor]) -> torch.iinfo:",
            "+    if isinstance(type, torch.Tensor):",
            "+        type = type.dtype",
            "return torch.iinfo(ivy.as_native_dtype(type))"
        ]
    },
    {
        "number": 5807,
        "comments": "",
        "commit_message": "Fix tpu spawn plugin test (#11131)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class BoringModelTPU(BoringModel):",
            "@pl_multi_process_test",
            "def test_model_tpu_one_core():",
            "\"\"\"Tests if device/debug flag is set correctely when training and after teardown for TPUSpawnPlugin.\"\"\"",
            "-    trainer = Trainer(tpu_cores=1, fast_dev_run=True, plugin=TPUSpawnPlugin(debug=True))",
            "+    trainer = Trainer(tpu_cores=1, fast_dev_run=True, strategy=TPUSpawnPlugin(debug=True))",
            "# assert training type plugin attributes for device setting",
            "assert isinstance(trainer.training_type_plugin, TPUSpawnPlugin)",
            "assert not trainer.training_type_plugin.on_gpu",
            "assert trainer.training_type_plugin.on_tpu",
            "-    assert trainer.training_type_plugin.root_device == torch.device(\"xla\")",
            "+    assert trainer.training_type_plugin.root_device == torch.device(\"xla\", index=1)",
            "model = BoringModelTPU()",
            "trainer.fit(model)",
            "assert \"PT_XLA_DEBUG\" not in os.environ"
        ]
    },
    {
        "number": 5812,
        "comments": "",
        "commit_message": "Fix import from torch.distributed when distributed not available (#16658)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_import_pytorch_lightning_with_torch_dist_unavailable():",
            "code = dedent(",
            "\"\"\"",
            "import torch",
            "-        torch.distributed.is_available = lambda: False  # pretend torch.distributed not available",
            "+",
            "+        # pretend torch.distributed not available",
            "+        for name in list(torch.distributed.__dict__.keys()):",
            "+            if not name.startswith(\"__\"):",
            "+                delattr(torch.distributed, name)",
            "+",
            "+        torch.distributed.is_available = lambda: False",
            "+",
            "import lightning.pytorch",
            "\"\"\"",
            ")"
        ]
    },
    {
        "number": 5816,
        "comments": "",
        "commit_message": "Fix code based on pep8\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def recog(args):",
            "",
            "if rnnlm is not None:",
            "rnnlm = lm_pytorch.ClassifierWithState(",
            "-                            extlm_pytorch.MultiLevelLM(word_rnnlm.predictor,",
            "-                                    rnnlm.predictor, word_dict, char_dict))",
            "+                extlm_pytorch.MultiLevelLM(word_rnnlm.predictor,",
            "+                                           rnnlm.predictor, word_dict, char_dict))",
            "else:",
            "rnnlm = lm_pytorch.ClassifierWithState(",
            "-                            extlm_pytorch.LookAheadWordLM(word_rnnlm.predictor,",
            "-                                    word_dict, char_dict))",
            "+                extlm_pytorch.LookAheadWordLM(word_rnnlm.predictor,",
            "+                                              word_dict, char_dict))",
            "",
            "# read json data",
            "with open(args.recog_json, 'rb') as f:"
        ]
    },
    {
        "number": 5818,
        "comments": "",
        "commit_message": "lint fixes\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def svd(",
            "",
            "",
            "def outer(",
            "-    x1: torch.Tensor,",
            "-    x2: torch.Tensor,",
            "-    out: Optional[torch.Tensor] = None",
            "+    x1: torch.Tensor, x2: torch.Tensor, out: Optional[torch.Tensor] = None",
            ") -> torch.Tensor:",
            "ret = torch.outer(x1, x2, out=out)",
            "return ret"
        ]
    },
    {
        "number": 5819,
        "comments": "",
        "commit_message": "Fixed non-4-dim input error for sequential (#1146)\n\n* Fixed non-4-dim input error for sequential\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TestAugmentationSequential:",
            "data_keys=[\"input\"],",
            "random_apply=random_apply,",
            "return_transform=return_transform,",
            "+            same_on_batch=same_on_batch,",
            ")",
            "out = aug(inp)",
            "if aug.return_label:",
            "out, label = out",
            "if return_transform and isinstance(out, (tuple, list)):",
            "out = out[0]",
            "-        assert out.shape == inp.shape",
            "+        assert out.shape[-3:] == inp.shape[-3:]",
            "reproducibility_test(inp, aug)",
            "",
            "@pytest.mark.parametrize('random_apply', [1, (2, 2), (1, 2), (2,), 10, True, False])"
        ]
    },
    {
        "number": 5820,
        "comments": "",
        "commit_message": "fixes ssim_loss average (#850)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def ssim_loss(img1: torch.Tensor, img2: torch.Tensor, window_size: int,",
            "ssim_map: torch.Tensor = ssim(img1, img2, window_size, max_val, eps)",
            "",
            "# compute and reduce the loss",
            "-    loss = torch.clamp(1. - ssim_map, min=0, max=1) / 2.",
            "+    loss = torch.clamp((1. - ssim_map) / 2, min=0, max=1)",
            "",
            "if reduction == \"mean\":",
            "loss = torch.mean(loss)"
        ]
    },
    {
        "number": 5824,
        "comments": "",
        "commit_message": "Fix compute_q_noisy_max tensor of tensors\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def compute_q_noisy_max_torch(counts, noise_eps):",
            "",
            "if type(counts) != torch.tensor:",
            "",
            "-        counts = torch.tensor(counts, dtype=torch.float)",
            "+        counts = torch.tensor(tensors_to_literals(counts), dtype=torch.float)",
            "",
            "_, winner = counts.max(0)",
            "counts_normalized = noise_eps * ("
        ]
    },
    {
        "number": 5828,
        "comments": "",
        "commit_message": "Fix CI out of memory & add PyTorch1.9 Python3.9 unit tests (#5862)\n\n* fix ci\n\n* fix ci\n\n* fix ci\n\n* fix ci\n\n* fix ci\n\n* fix ci\n\n* fix ci\n\n* fix ci\n\n* fix ci\n\n* fix ci\n\n* try to fix CI\n\n* try to fix CI\n\n* try to fix CI\n\n* try to fix CI\n\n* fix ci\n\n* Use docker to skip CUDA installation in CI\n\n* fix mmcv install\n\n* delete pt1.9\n\n* fix ci\n\n* delete py3.9\n\n* fix ci\n\n* fix albu\n\n* fix albu\n\n* skip mmpycocotools\n\n* pycocotools\n\n* pycocotools\n\n* env\n\n* env\n\n* env\n\n* env\n\n* env\n\n* env\n\n* env\n\n* skip some large model\n\n* fix tracking\n\n* reduce resnet channels\n\n* reduce resnset channels\n\n* reduce resnet channels\n\n* reduce csp darknet channels\n\n* reduce tracking channels\n\n* fix CE loss\n\n* enable cpu test\n\n* add cuda10.2 and torch1.9 and python3.9\n\n* fix python3.9 env\n\n* fix python3.9 env\n\n* fix python3.9 env\n\n* fix python3.9 env\n\n* fix apt install\n\n* fix py3.9 ppa\n\n* fix py3.9 ppa\n\n* fix py3.9\n\n* fix py3.9\n\n* fix py3.9\n\n* fix python dev\n\n* fix python dev\n\n* add cmake\n\n* update onnxruntime\n\n* update codecov\n\n* update CI\n\n* fix mmcv cuda version\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_trident_resnet_backbone():",
            "TridentResNet(50, num_stages=4, **tridentresnet_config)",
            "",
            "model = TridentResNet(50, num_stages=3, **tridentresnet_config)",
            "-    model.init_weights()",
            "model.train()",
            "",
            "-    imgs = torch.randn(1, 3, 224, 224)",
            "+    imgs = torch.randn(1, 3, 32, 32)",
            "feat = model(imgs)",
            "assert len(feat) == 1",
            "-    assert feat[0].shape == torch.Size([3, 1024, 14, 14])",
            "+    assert feat[0].shape == torch.Size([3, 1024, 2, 2])"
        ]
    },
    {
        "number": 5829,
        "comments": "",
        "commit_message": "fix seed in `GraphNorm` tests (#6642)\n\nsolves https://github.com/pyg-team/pytorch_geometric/issues/6622\n\n---------\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n",
        "label": "",
        "answer": "no",
        "change": [
            "from torch_geometric.testing import is_full_test",
            "",
            "",
            "def test_graph_norm():",
            "+    torch.manual_seed(42)",
            "x = torch.randn(200, 16)",
            "batch = torch.arange(4).view(-1, 1).repeat(1, 50).view(-1)"
        ]
    },
    {
        "number": 5833,
        "comments": "",
        "commit_message": "Doc styler examples (#14953)\n\n* Fix bad examples\n\n* Add black formatting to style_doc\n\n* Use first nonempty line\n\n* Put it at the right place\n\n* Don't add spaces to empty lines\n\n* Better templates\n\n* Deal with triple quotes in docstrings\n\n* Result of style_doc\n\n* Enable mdx treatment and fix code examples in MDXs\n\n* Result of doc styler on doc source files\n\n* Last fixes\n\n* Break copy from\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TFHubertForCTC(TFHubertPreTrainedModel):",
            ">>> processor = Wav2Vec2Processor.from_pretrained(\"facebook/hubert-base-960h\")",
            ">>> model = TFHubertForCTC.from_pretrained(\"facebook/hubert-base-960h\")",
            "",
            "+",
            ">>> def map_to_array(batch):",
            "...     speech, _ = sf.read(batch[\"file\"])",
            "...     batch[\"speech\"] = speech",
            "...     return batch",
            "",
            "+",
            ">>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")",
            ">>> ds = ds.map(map_to_array)",
            "",
            "-        >>> input_values = processor(ds[\"speech\"][0], return_tensors=\"tf\").input_values # Batch size 1",
            "-        >>> logits = model(input_values).logits >>> predicted_ids = tf.argmax(logits, axis=-1)",
            "+        >>> input_values = processor(ds[\"speech\"][0], return_tensors=\"tf\").input_values  # Batch size 1",
            "+        >>> logits = model(input_values).logits",
            "+        >>> predicted_ids = tf.argmax(logits, axis=-1)",
            "",
            ">>> transcription = processor.decode(predicted_ids[0])"
        ]
    },
    {
        "number": 5837,
        "comments": "",
        "commit_message": "fix metric name to work with default earlystopping (#628)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "Minimal example",
            "",
            "def test_end(self, outputs):",
            "# OPTIONAL",
            "-            avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()",
            "-            return {'avg_test_loss': avg_loss}",
            "+            test_loss_mean = torch.stack([x['test_loss'] for x in outputs]).mean()",
            "+            return {'test_loss': test_loss_mean}",
            "",
            "def configure_optimizers(self):",
            "# REQUIRED"
        ]
    },
    {
        "number": 5838,
        "comments": "",
        "commit_message": "Update serving signatures and make sure we actually use them (#19034)\n\n* Override save() to use the serving signature as the default\n\n* Replace int32 with int64 in all our serving signatures\n\n* Remember one very important line so as not to break every test at once\n\n* Dtype fix for TFLED\n\n* dtype fix for shift_tokens_right in general\n\n* Dtype fixes in mBART and RAG\n\n* Fix dtypes for test_unpack_inputs\n\n* More dtype fixes\n\n* Yet more mBART + RAG dtype fixes\n\n* Yet more mBART + RAG dtype fixes\n\n* Add a check that the model actually has a serving method\n",
        "label": "",
        "answer": "no",
        "change": [
            "def shift_tokens_right(input_ids: tf.Tensor, pad_token_id: int):",
            "if pad_token_id is None:",
            "raise ValueError(\"self.model.config.pad_token_id has to be defined.\")",
            "# replace possible -100 values in labels by `pad_token_id`",
            "-    input_ids = tf.where(input_ids == -100, tf.fill(shape_list(input_ids), pad_token_id), input_ids)",
            "+    input_ids = tf.where(",
            "+        input_ids == -100, tf.fill(shape_list(input_ids), tf.cast(pad_token_id, input_ids.dtype)), input_ids",
            "+    )",
            "language_id_index = (",
            "tf.reduce_sum(tf.cast(tf.math.not_equal(input_ids, pad_token_id), dtype=input_ids.dtype), axis=-1) - 1",
            ")",
            "-    language_id_index = tf.stack([tf.range(shape_list(input_ids)[0]), language_id_index], axis=-1)",
            "+    language_id_index = tf.stack(",
            "+        [tf.range(shape_list(input_ids)[0], dtype=input_ids.dtype), language_id_index], axis=-1",
            "+    )",
            "languages_ids = tf.gather_nd(input_ids, language_id_index)",
            "",
            "shifted_input_ids = tf.concat([tf.expand_dims(languages_ids, axis=-1), input_ids[:, :-1]], axis=-1)"
        ]
    },
    {
        "number": 5840,
        "comments": "",
        "commit_message": "Add support for Torchscript export of IntentSlotOutputLayer and CRF (#1146)\n\nSummary:\nPull Request resolved: https://github.com/facebookresearch/pytext/pull/1146\n\nThis diff does the following:\n\n1. Modifies `IntentSlotOutputLayer`, `WordTaggingOutputLayer` and `CRFOutputLayer` for torchscript export.\n\n1. Makes CRF implementation torchscriptable\n1. Fixes `predict` method of `NewTask` to make sure it passes model context as well to `get_pred`\n1. Fixes return type of the `forward` method of the decoder to return tuples of tensors instead of lists of tensors.\n\nReviewed By: liaimi\n\nDifferential Revision: D18565235\n\nfbshipit-source-id: 80836351c96b53f0650fa05ba4b4ab78b866899a\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class IntentSlotModelDecoder(DecoderBase):",
            "dense = dense.unsqueeze(1).repeat(1, word_input_shape[1], 1)",
            "x_w = torch.cat((x_w, dense), 2)",
            "",
            "-        return [logit_d, self.word_decoder(x_w)]",
            "+        return logit_d, self.word_decoder(x_w)",
            "",
            "def get_decoder(self) -> List[nn.Module]:",
            "\"\"\"Returns the document and word decoder modules."
        ]
    },
    {
        "number": 5842,
        "comments": "",
        "commit_message": "fix scripts on new config\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "with tf.Graph().as_default() as G:",
            "",
            "if args.output_type == 'label':",
            "for r in res:",
            "-            print r.argsort()[-top:][::-1]",
            "+            print r[0].argsort(axis=1)[:,-args.top:][:,::-1]",
            "elif args.output_type == 'label_prob':",
            "raise NotImplementedError",
            "elif args.output_type == 'raw':"
        ]
    },
    {
        "number": 5845,
        "comments": "",
        "commit_message": "fix bug in prepare inputs for language generation for xlm for effective batch_size > 1\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class XLMWithLMHeadModel(XLMPreTrainedModel):",
            "mask_token_id = self.config.mask_token_id",
            "lang_id = self.config.lang_id",
            "",
            "-        mask_token = torch.full((1, 1), mask_token_id, dtype=torch.long, device=input_ids.device)",
            "+        effective_batch_size = input_ids.shape[0]",
            "+        mask_token = torch.full((effective_batch_size, 1), mask_token_id, dtype=torch.long, device=input_ids.device)",
            "input_ids = torch.cat([input_ids, mask_token], dim=1)",
            "if lang_id is not None:",
            "langs = torch.full_like(input_ids, lang_id)"
        ]
    },
    {
        "number": 5848,
        "comments": "",
        "commit_message": "Fix TensorFlow RNN backwards support. (#3662)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def rnn(step_function, inputs, initial_states,",
            "",
            "if mask is not None:",
            "if go_backwards:",
            "-                mask = tf.reverse(mask, [True] + [False] * (ndim - 1))",
            "+                mask = tf.reverse(mask, [True] + [False] * (ndim - 2))",
            "",
            "# Transpose not supported by bool tensor types, hence round-trip to uint8.",
            "mask = tf.cast(mask, tf.uint8)"
        ]
    },
    {
        "number": 5857,
        "comments": "",
        "commit_message": "Tracing from inside the placeholders (#3303)\n\n* draft tracing from placeholders\n\n* trace from placeholder hook\n\n* add role attribute to placeholder\n\n* use tracing from placeholders to build Plan\n\n* rename hook functions\n\n* add a toggle_tracing method on Plan\n\n* change State.read() so that it returns placeholders\n\n* add tracing of hooked functions\n\n* find first arg placeholder instead of using first arg when tracing\n\n* fix import issues\n\n* start experimenting with wrapper and contextmanager around frameworks\n\n* Add tracing for module-level functions (like `torch.rand`)\n\n* black\n\n* use framework kwarg only conditionally\n\n* wrap only module functions\n\n* add torch in arguments for test plan made with the decorator\n\n* fix state attributes not being placeholders on build\n\n* transform parameters to placeholders on the fly at build time\n\n* fix unwrap on Placeholder.instantiate()\n\n* Fix `Placeholder.expected_shape` after instantiation\n\nInstantiating with a new tensor should also set the `expected_shape`.\n\n* handle actions that don't return tensors in plans\n\n* implement function to create already instantiated placeholder\n\n* test create_from\n\n* add test for functions as torch.split\n\n* handle functions returning tuples\n\n* remove wrapper case when instantiate placeholders\n\n* clean State.tensors()\n\n* fix state bugs\n\n* fix bug __setattr__ torchscript\n\n* remove previous tracing implementation\n\n* noqa in test\n\n* use pragma: no cover\n\n* move tracing logic to separate file\n\n* remove comment\n\n* update protobuf serde for Role\n\nCo-authored-by: Karl Higley <kmhigley@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TorchHook(FrameworkHook):",
            "if \"native_tensor\" not in dir(hook_self.torch):",
            "hook_self.torch.native_tensor = hook_self.torch.tensor",
            "",
            "-        @tracer(func_name=\"torch.tensor\")",
            "def new_tensor(*args, owner=None, id=None, register=True, **kwargs):",
            "current_tensor = hook_self.torch.native_tensor(*args, **kwargs)",
            "_apply_args(hook_self, current_tensor, owner, id)"
        ]
    },
    {
        "number": 5860,
        "comments": "",
        "commit_message": "pytorch 1.0 (#2165)\n\n* fixes\n\n* allenlp\n\n* fix bug\n\n* reduce=False -> reduction='none'\n\n* fix _sparse_mask\n\n* remove print statement\n\n* fix more tests\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class GraphParser(Model):",
            "\"arc representation dim\", \"arc feedforward output dim\")",
            "",
            "self._unlabelled_f1 = F1Measure(positive_label=1)",
            "-        self._arc_loss = torch.nn.BCEWithLogitsLoss(reduce=False)",
            "-        self._tag_loss = torch.nn.CrossEntropyLoss(reduce=False)",
            "+        self._arc_loss = torch.nn.BCEWithLogitsLoss(reduction='none')",
            "+        self._tag_loss = torch.nn.CrossEntropyLoss(reduction='none')",
            "initializer(self)",
            "",
            "@overrides"
        ]
    },
    {
        "number": 5862,
        "comments": "",
        "commit_message": "fix tests\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestTransformerWordEmbeddings(BaseEmbeddingsTest):",
            "\"Berlin to Germany is like Vienna to Austria\",",
            "]",
            "",
            "-        normal_sentences = [",
            "-            Sentence(text) for text in texts",
            "-        ]",
            "-        onnx_sentences = [",
            "-            Sentence(text) for text in texts",
            "-        ]",
            "+        normal_sentences = [Sentence(text) for text in texts]",
            "+        onnx_sentences = [Sentence(text) for text in texts]",
            "",
            "embeddings = TransformerWordEmbeddings(\"distilbert-base-uncased\")",
            "results_base_path.mkdir(exist_ok=True, parents=True)"
        ]
    },
    {
        "number": 5866,
        "comments": "",
        "commit_message": "Fixed small issue with reshape\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def reshape(",
            "shape: Union[ivy.NativeShape, Sequence[int]],",
            "*,",
            "copy: Optional[bool] = None,",
            "-    out: Optional[torch.Tensor] = None,",
            "order: Optional[str] = \"C\",",
            "+    out: Optional[torch.Tensor] = None,",
            ") -> torch.Tensor:",
            "ivy.assertions.check_elem_in_list(order, [\"C\", \"F\"])",
            "if copy:"
        ]
    },
    {
        "number": 5867,
        "comments": "",
        "commit_message": "Display activation functions as modules. (#4045)\n\n* Display activations as modules.\n\n* Fix tests and make changes in doc.\n\n* Fix parameter type.\n\n* Fix lambda based activation name displaying.\n\n* Fix formatting.\n\nCo-authored-by: Dirk Groeneveld <dirkg@allenai.org>\nCo-authored-by: Evan Pete Walsh <epwalsh10@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class MockSeq2SeqEncoder(Seq2SeqEncoder):",
            "def _make_feedforward(input_dim, output_dim):",
            "return FeedForwardEncoder(",
            "FeedForward(",
            "-            input_dim=input_dim, num_layers=1, activations=torch.relu, hidden_dims=output_dim",
            "+            input_dim=input_dim, num_layers=1, activations=torch.nn.ReLU(), hidden_dims=output_dim",
            ")",
            ")"
        ]
    },
    {
        "number": 5870,
        "comments": "",
        "commit_message": "fix deprecations about casting & initializers in tf1.13\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Model(ModelDesc):",
            "isTrain = get_current_tower_context().is_training",
            "if isTrain:",
            "# beam search is too slow to run in training",
            "-            predictions = tf.to_int32(",
            "-                tf.nn.ctc_greedy_decoder(logits, seqlen)[0][0])",
            "+            predictions = tf.cast(",
            "+                tf.nn.ctc_greedy_decoder(logits, seqlen)[0][0], tf.int32)",
            "else:",
            "-            predictions = tf.to_int32(",
            "-                tf.nn.ctc_beam_search_decoder(logits, seqlen)[0][0])",
            "+            predictions = tf.cast(",
            "+                tf.nn.ctc_beam_search_decoder(logits, seqlen)[0][0], tf.int32)",
            "err = tf.edit_distance(predictions, label, normalize=True)",
            "err.set_shape([None])",
            "err = tf.reduce_mean(err, name='error')"
        ]
    },
    {
        "number": 5871,
        "comments": "",
        "commit_message": "[sgd] Add benchmarks (#7454)\n\n* Init fp16\n\n* fp16 and schedulers\n\n* scheduler linking and fp16\n\n* to fp16\n\n* loss scaling and documentation\n\n* more documentation\n\n* add tests, refactor config\n\n* moredocs\n\n* more docs\n\n* fix logo, add test mode, add fp16 flag\n\n* fix tests\n\n* fix scheduler\n\n* fix apex\n\n* improve safety\n\n* fix tests\n\n* fix tests\n\n* remove pin memory default\n\n* rm\n\n* fix\n\n* Update doc/examples/doc_code/raysgd_torch_signatures.py\n\n* fix\n\n* migrate changes from other PR\n\n* ok thanks\n\n* pass\n\n* signatures\n\n* lint'\n\n* Update python/ray/experimental/sgd/pytorch/utils.py\n\n* Apply suggestions from code review\n\nCo-Authored-By: Edward Oakes <ed.nmi.oakes@gmail.com>\n\n* should address most comments\n\n* comments\n\n* fix this ci\n\n* first_pass\n\n* add overrides\n\n* override\n\n* fixing up operators\n\n* format\n\n* sgd\n\n* constants\n\n* rm\n\n* revert\n\n* save\n\n* failures\n\n* fixes\n\n* trainer\n\n* run test\n\n* operator\n\n* code\n\n* op\n\n* ok done\n\n* operator\n\n* sgd test fixes\n\n* ok\n\n* trainer\n\n* format\n\n* Apply suggestions from code review\n\nCo-Authored-By: Edward Oakes <ed.nmi.oakes@gmail.com>\n\n* Update doc/source/raysgd/raysgd_pytorch.rst\n\n* docstring\n\n* dcgan\n\n* doc\n\n* commits\n\n* nit\n\n* testing\n\n* revert\n\n* Start renaming pytorch to torch\n\n* Rename PyTorchTrainer to TorchTrainer\n\n* Rename PyTorch runners to Torch runners\n\n* Finish renaming API\n\n* Rename to torch in tests\n\n* Finish renaming docs + tests\n\n* Run format + fix DeprecationWarning\n\n* fix\n\n* move tests up\n\n* benchmarks\n\n* rename\n\n* remove some args\n\n* better metrics output\n\n* fix up the benchmark\n\n* benchmark-yaml\n\n* horovod-benchmark\n\n* benchmarks\n\n* Remove benchmark code for cleanups\n\n* benchmark-code\n\n* nits\n\n* benchmark yamls\n\n* benchmark yaml\n\n* ok\n\n* ok\n\n* ok\n\n* benchmark\n\n* nit\n\n* finish_bench\n\n* makedatacreator\n\n* relax\n\n* metrics\n\n* autosetsampler\n\n* profile\n\n* movements\n\n* OK\n\n* smoothen\n\n* fix\n\n* nitdocs\n\n* loss\n\n* envflag\n\n* comments\n\n* nit\n\n* format\n\n* visible\n\n* images\n\n* move_images\n\n* fix\n\n* rernder\n\n* rrender\n\n* rest\n\n* multgpu\n\n* fix\n\n* nit\n\n* finish\n\n* extrra\n\n* setup\n\n* revert\n\nCo-authored-by: Edward Oakes <ed.nmi.oakes@gmail.com>\nCo-authored-by: Maksim Smolin <maximsmol@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TorchRunner:",
            "else:",
            "self.criterion = self.loss_creator(self.config)",
            "",
            "-        if torch.cuda.is_available() and hasattr(\"cuda\", self.criterion):",
            "+        if torch.cuda.is_available() and hasattr(self.criterion, \"cuda\"):",
            "self.criterion = self.criterion.cuda()",
            "",
            "def _create_schedulers_if_available(self):"
        ]
    },
    {
        "number": 5873,
        "comments": "",
        "commit_message": "Scope TokenIndexer output by indexer name (#3597)\n\n* Indexer tests are now passing, at least\n\n* Fixed some masking issues, and padding keys for one config file\n\n* TextFieldEmbedder tests pass\n\n* Fixing fixtures, some more tests passing\n\n* Fixed weird ordering bug\n\n* All TokenEmbedder tests passing?\n\n* Update fixtures\n\n* Fix more hard-coded references\n\n* fix field tests\n\n* fix dataset reader tests\n\n* Fix iterator tests\n\n* More tests passing\n\n* fix hotflip and some other tests\n\n* more tests\n\n* more test fixes\n\n* more tests\n\n* most tests passing; I think the remaining ones are spacy model changes\n\n* hard-code POS tag test\n\n* last test, I think\n\n* black\n\n* updated black\n\n* flake8\n\n* mypy\n\n* black again\n\n* fix training configs\n\n* remove reference to embedder_to_indexer_map\n\n* Other fixes from PR comments\n\n* fix breakage from incorrect merge during rebase\n\n* flake, some docstring formatting\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class SpanConstituencyParser(Model):",
            "@overrides",
            "def forward(",
            "self,  # type: ignore",
            "-        tokens: Dict[str, torch.LongTensor],",
            "+        tokens: TextFieldTensors,",
            "spans: torch.LongTensor,",
            "metadata: List[Dict[str, Any]],",
            "-        pos_tags: Dict[str, torch.LongTensor] = None,",
            "+        pos_tags: TextFieldTensors = None,",
            "span_labels: torch.LongTensor = None,",
            ") -> Dict[str, torch.Tensor]:",
            "",
            "\"\"\"",
            "# Parameters",
            "",
            "-        tokens : Dict[str, torch.LongTensor], required",
            "+        tokens : TextFieldTensors, required",
            "The output of ``TextField.as_array()``, which should typically be passed directly to a",
            "``TextFieldEmbedder``. This output is a dictionary mapping keys to ``TokenIndexer``",
            "tensors.  At its most basic, using a ``SingleIdTokenIndexer`` this is : ``{\"tokens\":"
        ]
    },
    {
        "number": 5876,
        "comments": "",
        "commit_message": "fixed docstring\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Conv1dLinear(torch.nn.Module):",
            "\"\"\"Calculate forward propagation.",
            "",
            "Args:",
            "-            x (torch.Tensor): Batch of input tensors (B, ..., in_chans).",
            "+            x (torch.Tensor): Batch of input tensors (B, T, in_chans).",
            "",
            "Returns:",
            "-            torch.Tensor: Batch of output tensors (B, `*`, hidden_chans).",
            "+            torch.Tensor: Batch of output tensors (B, T, hidden_chans).",
            "",
            "\"\"\"",
            "x = torch.relu(self.w_1(x.transpose(-1, 1))).transpose(-1, 1)"
        ]
    },
    {
        "number": 5878,
        "comments": "",
        "commit_message": "sample points device fix\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class SamplePoints(object):",
            "sample = torch.multinomial(prob, self.num, replacement=True)",
            "face = face[:, sample]",
            "",
            "-        frac = torch.rand(self.num, 2)",
            "+        frac = torch.rand(self.num, 2, device=pos.device)",
            "mask = frac.sum(dim=-1) > 1",
            "frac[mask] = 1 - frac[mask]"
        ]
    },
    {
        "number": 5882,
        "comments": "",
        "commit_message": "Fix incorrect comments about atten mask for pytorch backend (#18728)\n\n* fix incorrect comments about atten mask\n\n* typo\n\n* Update for CodeGen\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class CodeGenModel(CodeGenPreTrainedModel):",
            "",
            "# Since attention_mask is 1.0 for positions we want to attend and 0.0 for",
            "# masked positions, this operation will create a tensor which is 0.0 for",
            "-            # positions we want to attend and -10000.0 for masked positions.",
            "+            # positions we want to attend and the dtype's smallest value for masked positions.",
            "# Since we are adding it to the raw scores before the softmax, this is",
            "# effectively the same as removing these entirely.",
            "attention_mask = attention_mask.to(dtype=self.dtype)  # fp16 compatibility",
            "-            attention_mask = (1.0 - attention_mask) * -10000.0",
            "+            attention_mask = (1.0 - attention_mask) * torch.finfo(self.dtype).min",
            "",
            "# Prepare head mask if needed",
            "# 1.0 in head_mask indicate we keep the head"
        ]
    },
    {
        "number": 5883,
        "comments": "",
        "commit_message": "Add exeption for jit C++ lib runtime error (#396)\n\n* Add exption for \"tuple appears in op that does not forward tuples\" runtime error\n\n* Fix unit tests\n\n* Remove redundant comment\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class PytorchGraphTest(unittest.TestCase):",
            "class myLinear(torch.nn.Module):",
            "def __init__(self):",
            "super(myLinear, self).__init__()",
            "-                self.l = torch.nn.Linear(3, 5)",
            "+                self.linear = torch.nn.Linear(3, 5)",
            "",
            "def forward(self, x):",
            "-                return self.l(x)",
            "+                return self.linear(x)",
            "",
            "with SummaryWriter(comment='LinearModel') as w:",
            "w.add_graph(myLinear(), dummy_input, True)",
            "",
            "def test_wrong_input_size(self):",
            "print('expect error here:')",
            "-        with self.assertRaises(RuntimeError) as e_info:",
            "+        with self.assertRaises(TypeError):",
            "dummy_input = torch.rand(1, 9)",
            "model = torch.nn.Linear(3, 5)",
            "with SummaryWriter(comment='expect_error') as w:"
        ]
    },
    {
        "number": 5886,
        "comments": "",
        "commit_message": "Fix for multi-GPU training.\n\nAnd cleaner path split in nucleus sample.\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class MaskRCNN():",
            "# TODO: can this be optimized to avoid duplicating the anchors?",
            "anchors = np.broadcast_to(anchors, (config.BATCH_SIZE,) + anchors.shape)",
            "# A hack to get around Keras's bad support for constants",
            "-            anchors = KL.Lambda(lambda x: tf.constant(anchors), name=\"anchors\")(input_image)",
            "+            anchors = KL.Lambda(lambda x: tf.Variable(anchors), name=\"anchors\")(input_image)",
            "else:",
            "anchors = input_anchors"
        ]
    },
    {
        "number": 5893,
        "comments": "",
        "commit_message": "[torch] remove deprecated uint8 in favor of bool (#21384)\n\n* uint8 -> bool\n\n* fix copies\n\n* style\n\n* update test modeling commen when checking attention buffers\n\n* style\n\n* use logical not on random mask instead of subtraction with 1\n\n* remove torch uint8\n\n* quality\n\n* remove modified modeling utils\n\n* Update based on review\n\nCo-authored-by: sgugger <sylvain.gugger@gmail.com>\n\n---------\n\nCo-authored-by: sgugger <sylvain.gugger@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def convert_megatron_checkpoint(sd_megatron, config):",
            "",
            "pf = \"model.language_model.encoder.layers.\"",
            "for i in range(layers):",
            "-        causal_mask = torch.tril(torch.ones((n_positions, n_positions), dtype=torch.uint8))",
            "+        causal_mask = torch.tril(torch.ones((n_positions, n_positions), dtype=torch.bool))",
            "causal_mask = causal_mask.view(1, 1, n_positions, n_positions)",
            "sd_hf[f\"transformer.h.{i}.attn.bias\"] = causal_mask",
            "sd_hf[f\"transformer.h.{i}.attn.masked_bias\"] = torch.tensor(-1e4, dtype=torch.bfloat16)"
        ]
    },
    {
        "number": 5899,
        "comments": "",
        "commit_message": "prepare_seq2seq_batch makes labels/ decoder_input_ids made later. (#6654)\n\n* broken test\n\n* batch parity\n\n* tests pass\n\n* boom boom\n\n* boom boom\n\n* split out bart tokenizer tests\n\n* fix tests\n\n* boom boom\n\n* Fixed dataset bug\n\n* Fix marian\n\n* Undo extra\n\n* Get marian working\n\n* Fix t5 tok tests\n\n* Test passing\n\n* Cleanup\n\n* better assert msg\n\n* require torch\n\n* Fix mbart tests\n\n* undo extra decoder_attn_mask change\n\n* Fix import\n\n* pegasus tokenizer can ignore src_lang kwargs\n\n* unused kwarg test cov\n\n* boom boom\n\n* add todo for pegasus issue\n\n* cover one word translation edge case\n\n* Cleanup\n\n* doc\n",
        "label": "",
        "answer": "no",
        "change": [
            "class T5PreTrainedModel(PreTrainedModel):",
            "# replace possible -100 values in labels by `pad_token_id`",
            "shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)",
            "",
            "-        assert torch.all(shifted_input_ids >= 0).item(), \"Verify that `labels` has only positive values and -100\"",
            "+        assert torch.all(shifted_input_ids >= 0).item(), \"Verify that `shifted_input_ids` has only positive values\"",
            "",
            "return shifted_input_ids"
        ]
    },
    {
        "number": 5900,
        "comments": "",
        "commit_message": "fixes for half precision in imgwrp (#1723)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def invert_affine_transform(matrix: torch.Tensor) -> torch.Tensor:",
            "raise ValueError(f\"Input matrix must be a Bx2x3 tensor. Got {matrix.shape}\")",
            "",
            "matrix_tmp: torch.Tensor = convert_affinematrix_to_homography(matrix)",
            "-    matrix_inv: torch.Tensor = torch.inverse(matrix_tmp)",
            "+    matrix_inv: torch.Tensor = _torch_inverse_cast(matrix_tmp)",
            "",
            "return matrix_inv[..., :2, :3]"
        ]
    },
    {
        "number": 5907,
        "comments": "",
        "commit_message": "Fixed formatting of 1 file\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_lstm(workers):",
            "lstm = nn.LSTM(3, 3)",
            "lstm.send(bob)",
            "inputs = torch.randn(5, 1, 3).send(bob)",
            "-    hidden = (torch.randn(1, 1, 3).send(bob),",
            "-        torch.randn(1, 1, 3).send(bob), )  # clean out hidden state",
            "+    hidden = (",
            "+        torch.randn(1, 1, 3).send(bob),",
            "+        torch.randn(1, 1, 3).send(bob),",
            "+    )  # clean out hidden state",
            "out, hidden = lstm(inputs, hidden)",
            "assert out.shape == torch.Size([5, 1, 3])"
        ]
    },
    {
        "number": 5909,
        "comments": "",
        "commit_message": "Update ONNX docs (#14904)\n\n* Remove docs for deprecated ONNX export\n\n* Tidy up the CLI help messages\n\n* Revamp ONNX docs\n\n* Update auto-config table\n\n* Use DistilBERT as example for consistency\n\n* Wrap up first pass at ONNX docs\n\n* Fix table check\n\n* Add tweaks and introduction\n\n* Add cross-ref\n\n* Fix missing import\n\n* Fix style\n\n* Add permalinks to ONNX configs\n\n* Clarify role of OrderedDict\n\n* Update docs/source/serialization.mdx\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n\n* Add doctest syntax to code blocks\n\n* Remove permalinks\n\n* Revert \"Remove permalinks\"\n\nThis reverts commit 099701daf0db27823457867938efdb2d4f22a7c1.\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def get_onnx_model_list():",
            "",
            "",
            "def check_onnx_model_list(overwrite=False):",
            "-    \"\"\"Check the model list in the serialization.rst is consistent with the state of the lib and maybe `overwrite`.\"\"\"",
            "+    \"\"\"Check the model list in the serialization.mdx is consistent with the state of the lib and maybe `overwrite`.\"\"\"",
            "current_list, start_index, end_index, lines = _find_text_in_file(",
            "filename=os.path.join(PATH_TO_DOCS, \"serialization.mdx\"),",
            "start_prompt=\"<!--This table is automatically generated by make style, do not fill manually!-->\",",
            "-        end_prompt=\"This conversion is handled with the PyTorch version of models \",",
            "+        end_prompt=\"The ONNX conversion is supported for the PyTorch versions of the models.\",",
            ")",
            "new_list = get_onnx_model_list()"
        ]
    },
    {
        "number": 5915,
        "comments": "",
        "commit_message": "Deprecate LKJCorrCholesky distribution (#2771)\n\n* Deprecate LKJCorrCholesky distribution\n\n* fix tests; add FutureWarning\n\n* Fix failing examples/lkj.py\n\n* Fix deprecation warning in bart example\n\n* Address review comment\n\nCo-authored-by: Neeraj Pradhan <neerajprad@fb.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_empty_model_error():",
            "",
            "def test_unpack_latent():",
            "def model():",
            "-        return pyro.sample('x', dist.LKJCorrCholesky(2, torch.tensor(1.)))",
            "+        return pyro.sample('x', dist.LKJCholesky(2, torch.tensor(1.)))",
            "",
            "guide = AutoDiagonalNormal(model)",
            "assert guide()['x'].shape == model().shape"
        ]
    },
    {
        "number": 5917,
        "comments": "",
        "commit_message": "Fix gradient bug for `distance_transform` (#1584)\n\n* Fix gradient bug for distance_transform.\n\n* Move signal_ones outside the loop && Add another grad check for distance_transform.\n\n* Move signal_ones outside the loop && Add another grad check for distance_transform.\n\nMove signal_ones outside the loop && Add another grad check for distance_transform.\n\n* Removing try/catch block of distance_transform grad test.\n\nCo-authored-by: guangpan.cd <guangpan.cd@alibaba-inc.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def distance_transform(",
            "",
            "offset: int = i * kernel_size // 2",
            "out += (offset + cdt) * mask",
            "-        boundary[mask == 1] = 1",
            "+        boundary = torch.where(mask == 1, signal_ones, boundary)",
            "",
            "return out"
        ]
    },
    {
        "number": 5918,
        "comments": "",
        "commit_message": "[FasterRCNN] fix rgb/bgr in visualization (fix #519)\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Model(ModelDesc):",
            "fg_inds_wrt_sample = tf.reshape(tf.where(rcnn_labels > 0), [-1])   # fg inds w.r.t all samples",
            "fg_sampled_boxes = tf.gather(rcnn_sampled_boxes, fg_inds_wrt_sample)",
            "",
            "-            # TODO move to models",
            "with tf.name_scope('fg_sample_patch_viz'):",
            "fg_sampled_patches = crop_and_resize(",
            "image, fg_sampled_boxes,",
            "tf.zeros_like(fg_inds_wrt_sample, dtype=tf.int32), 300)",
            "fg_sampled_patches = tf.transpose(fg_sampled_patches, [0, 2, 3, 1])",
            "+                fg_sampled_patches = tf.reverse(fg_sampled_patches, axis=-1)  # BGR->RGB",
            "tf.summary.image('viz', fg_sampled_patches, max_outputs=30)",
            "",
            "matched_gt_boxes = tf.gather(gt_boxes, fg_inds_wrt_gt)"
        ]
    },
    {
        "number": 5920,
        "comments": "",
        "commit_message": "pylint -> flake8 (#3288)\n\n* pylint\n\n* update pylint\n\n* undo a lot of the raise / else\n\n* add bound on typed-ast\n\n* first mypy fixes\n\n* new flag\n\n* fix mypy errors\n\n* requirements.txt\n\n* pylint -> flake8\n\n* mypy 0.720 -> mypy 0.730\n\n* add back erroneously removed initial newline\n\n* remove .pylintrc\n\n* remove pylintrc from Dockerfile\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class IntraSentenceAttentionEncoder(Seq2SeqEncoder):",
            "return False",
            "",
            "@overrides",
            "-    def forward(self, tokens: torch.Tensor, mask: torch.Tensor):  # pylint: disable=arguments-differ",
            "+    def forward(self, tokens: torch.Tensor, mask: torch.Tensor):",
            "batch_size, sequence_length, _ = tokens.size()",
            "# Shape: (batch_size, sequence_length, sequence_length)",
            "similarity_matrix = self._matrix_attention(tokens, tokens)"
        ]
    },
    {
        "number": 5923,
        "comments": "",
        "commit_message": "Fix src and test code\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def recog(args):",
            "word_dict = rnnlm_args.char_list_dict",
            "char_dict = {x: i for i, x in enumerate(train_args.char_list)}",
            "word_rnnlm = lm_pytorch.ClassifierWithState(lm_pytorch.RNNLM(",
            "-            len(word_dict), rnnlm_args.unit))",
            "+            len(word_dict), rnnlm_args.layers, rnnlm_args.units))",
            "torch_load(args.word_rnnlm, word_rnnlm)",
            "word_rnnlm.eval()"
        ]
    },
    {
        "number": 5925,
        "comments": "",
        "commit_message": "summaries updated and fixed\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Activation(Layer):",
            "x = tf.nn.leaky_relu(features=x, alpha=0.2)  # alpha argument???",
            "",
            "elif self.nonlinearity == 'none':",
            "-            x = tf.identity(input=x)",
            "+            pass",
            "",
            "elif self.nonlinearity == 'relu':",
            "x = tf.nn.relu(features=x)"
        ]
    },
    {
        "number": 5926,
        "comments": "",
        "commit_message": "fix import_user_module (#3144)\n\nSummary:\n## What does this PR do?\nAvoid throwing ValueError when attempting to load a user defined module from common.user_dir that has the same module name and same module path as some loaded module. This occurs when a job is preempted and restarts using submitit_slurm\n\nX-link: https://github.com/fairinternal/fairseq-py/pull/3144\n\nReviewed By: Abdel-rahmanMohamed\n\nDifferential Revision: D34521450\n\nPulled By: wnhsu\n\nfbshipit-source-id: eed00d4238a66dc524eee400a55ad2c011e1543c\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class BeamSearch(Search):",
            "scores_buf = top_prediction[0]",
            "indices_buf = top_prediction[1]",
            "# Project back into relative indices and beams",
            "-        beams_buf = torch.div(indices_buf, vocab_size, rounding_mode='trunc')",
            "+        beams_buf = torch.div(indices_buf, vocab_size, rounding_mode=\"trunc\")",
            "indices_buf = indices_buf.fmod(vocab_size)",
            "",
            "# At this point, beams_buf and indices_buf are single-dim and contain relative indices"
        ]
    },
    {
        "number": 5927,
        "comments": "",
        "commit_message": "Standardize on `use_causal_mask` in `call()` and mark `causal` argument in constructor as deprecated.\n\nAlso remove testing of causal masks from `layer_correctness` since not needed and changing arguments in call doesn't fit easily into this test fixture.\n\nPiperOrigin-RevId: 459791640\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class AdditiveAttentionTest(tf.test.TestCase, parameterized.TestCase):",
            "q = tf.cast(tf.random.uniform((2, 3, 4), seed=1), \"float16\")",
            "v = tf.cast(tf.random.uniform((2, 3, 4), seed=2), \"float16\")",
            "k = tf.cast(tf.random.uniform((2, 3, 4), seed=3), \"float16\")",
            "-            layer = keras.layers.AdditiveAttention(causal=True)",
            "-            _ = layer([q, v, k])",
            "+            layer = keras.layers.AdditiveAttention()",
            "+            _ = layer([q, v, k], use_causal_mask=True)",
            "",
            "",
            "if __name__ == \"__main__\":"
        ]
    },
    {
        "number": 5928,
        "comments": "",
        "commit_message": "Fix tensorflow_backend  deprecation warning (#9488)\n\nWARNING:tensorflow:From .../lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3148: calling l2_normalize (from tensorflow.python.ops.nn_impl) with dim is deprecated and will be removed in a future version.\nInstructions for updating:\ndim is deprecated, use axis instead\n",
        "label": "",
        "answer": "no",
        "change": [
            "def l2_normalize(x, axis=None):",
            "# Returns",
            "A tensor.",
            "\"\"\"",
            "-    return tf.nn.l2_normalize(x, dim=axis)",
            "+    return tf.nn.l2_normalize(x, axis=axis)",
            "",
            "",
            "def in_top_k(predictions, targets, k):"
        ]
    },
    {
        "number": 5932,
        "comments": "",
        "commit_message": "glow-tts fix for saving inverse weight\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class InvConvNear(nn.Module):",
            "return z, logdet",
            "",
            "def store_inverse(self):",
            "-        self.weight_inv = torch.inverse(",
            "+        weight_inv = torch.inverse(",
            "self.weight.float()).to(dtype=self.weight.dtype)",
            "+        self.weight_inv = nn.Parameter(weight_inv, requires_grad=False)",
            "",
            "",
            "class CouplingBlock(nn.Module):"
        ]
    },
    {
        "number": 5935,
        "comments": "",
        "commit_message": "Fix PDE grid batching\n\nPiperOrigin-RevId: 468659748\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class ParabolicEquationStepperTest(tf.test.TestCase, parameterized.TestCase):",
            "boundary_conditions=[(None, upper_boundary_fn)])[0]",
            "",
            "true_values = tf.math.exp(final_t + grid[0])",
            "-    print('est_values: ', est_values)",
            "self.assertAllClose(",
            "est_values, true_values, atol=1e-2, rtol=1e-2)"
        ]
    },
    {
        "number": 5937,
        "comments": "",
        "commit_message": "Fix ALBERT exports with pretraining + sp classifier; Fix naming for ALBERT TF models\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "import logging",
            "logger = logging.getLogger(__name__)",
            "",
            "TF_ALBERT_PRETRAINED_MODEL_ARCHIVE_MAP = {",
            "-    'albert-base-v1': \"https://s3.amazonaws.com/models.huggingface.co/bert/albert-base-tf_model.h5\",",
            "-    'albert-large-v1': \"https://s3.amazonaws.com/models.huggingface.co/bert/albert-large-tf_model.h5\",",
            "-    'albert-xlarge-v1': \"https://s3.amazonaws.com/models.huggingface.co/bert/albert-xlarge-tf_model.h5\",",
            "-    'albert-xxlarge-v1': \"https://s3.amazonaws.com/models.huggingface.co/bert/albert-xxlarge-tf_model.h5\",",
            "+    'albert-base-v1': \"https://s3.amazonaws.com/models.huggingface.co/bert/albert-base-v1-tf_model.h5\",",
            "+    'albert-large-v1': \"https://s3.amazonaws.com/models.huggingface.co/bert/albert-large-v1-tf_model.h5\",",
            "+    'albert-xlarge-v1': \"https://s3.amazonaws.com/models.huggingface.co/bert/albert-xlarge-v1-tf_model.h5\",",
            "+    'albert-xxlarge-v1': \"https://s3.amazonaws.com/models.huggingface.co/bert/albert-xxlarge-v1-tf_model.h5\",",
            "'albert-base-v2': \"https://s3.amazonaws.com/models.huggingface.co/bert/albert-base-v2-tf_model.h5\",",
            "'albert-large-v2': \"https://s3.amazonaws.com/models.huggingface.co/bert/albert-large-v2-tf_model.h5\",",
            "'albert-xlarge-v2': \"https://s3.amazonaws.com/models.huggingface.co/bert/albert-xlarge-v2-tf_model.h5\","
        ]
    },
    {
        "number": 5945,
        "comments": "",
        "commit_message": "Fixed failing test for searching where (#4744)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def where(",
            "out: Optional[torch.Tensor] = None,",
            ") -> torch.Tensor:",
            "x1, x2 = ivy.promote_types_of_inputs(x1, x2)",
            "-    return torch.where(condition, x1, x2)",
            "+    return torch.where(condition, x1, x2).to(dtype=x1.dtype)",
            "",
            "",
            "# Extra #"
        ]
    },
    {
        "number": 5946,
        "comments": "",
        "commit_message": "[Compression v2] bugfix & improvement (#4307)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class SlimPruner(BasicPruner):",
            "def patched_criterion(input_tensor: Tensor, target: Tensor):",
            "sum_l1 = 0",
            "for _, wrapper in self.get_modules_wrapper().items():",
            "-                sum_l1 += torch.norm(wrapper.module.weight.data, p=1)",
            "+                sum_l1 += torch.norm(wrapper.module.weight, p=1)",
            "return criterion(input_tensor, target) + self._scale * sum_l1",
            "return patched_criterion"
        ]
    },
    {
        "number": 5949,
        "comments": "",
        "commit_message": "[TFT5, Cache] Add cache to TFT5 (#3772)\n\n* correct gpt2 test inputs\n\n* make style\n\n* delete modeling_gpt2 change in test file\n\n* translate from pytorch\n\n* correct tests\n\n* fix conflicts\n\n* fix conflicts\n\n* fix conflicts\n\n* fix conflicts\n\n* make tensorflow t5 caching work\n\n* make style\n\n* clean reorder cache\n\n* remove unnecessary spaces\n\n* fix test\n",
        "label": "",
        "answer": "no",
        "change": [
            "class T5ModelTest(ModelTesterMixin, unittest.TestCase):",
            "def create_t5_and_check_t5_generate_with_past_key_value_states(",
            "self, config, input_ids, decoder_input_ids, attention_mask, decoder_attention_mask, lm_labels,",
            "):",
            "-            config.num_layers = 1",
            "model = T5ForConditionalGeneration(config=config)",
            "model.to(torch_device)",
            "model.eval()"
        ]
    },
    {
        "number": 5954,
        "comments": "",
        "commit_message": "0.4.0 fixes\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_add_self_loops():",
            "",
            "",
            "def test_remove_self_loops():",
            "-    row = torch.LongTensor([1, 0, 1, 0, 2, 1])",
            "-    col = torch.LongTensor([0, 1, 1, 1, 2, 0])",
            "+    row = torch.tensor([1, 0, 1, 0, 2, 1])",
            "+    col = torch.tensor([0, 1, 1, 1, 2, 0])",
            "expected_output = [[1, 0, 0, 1], [0, 1, 1, 0]]",
            "",
            "output = remove_self_loops(torch.stack([row, col], dim=0))"
        ]
    },
    {
        "number": 5963,
        "comments": "",
        "commit_message": "fix use of deprecated TF functions.\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Model(ModelDesc):",
            "if not USE_SLIM:",
            "# Use a regex to find parameters to apply weight decay.",
            "# Here we apply a weight decay on all W (weight matrix) of all fc layers",
            "-            wd_cost = tf.mul(1e-5,",
            "-                             regularize_cost('fc.*/W', tf.nn.l2_loss),",
            "-                             name='regularize_loss')",
            "+            wd_cost = tf.multiply(1e-5,",
            "+                                  regularize_cost('fc.*/W', tf.nn.l2_loss),",
            "+                                  name='regularize_loss')",
            "self.cost = tf.add_n([wd_cost, cost], name='total_cost')",
            "summary.add_moving_summary(cost, wd_cost, self.cost)",
            "else:"
        ]
    },
    {
        "number": 5964,
        "comments": "",
        "commit_message": "cos fix\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class WarmupCosineSchedule(LRSchedule):",
            "return progress / self.warmup",
            "else:",
            "progress = (progress - self.warmup) / (1 - self.warmup)   # progress after warmup",
            "-            return 0.5 * (1. + torch.cos(math.pi * self.cycles * 2 * progress))",
            "+            return 0.5 * (1. + math.cos(math.pi * self.cycles * 2 * progress))",
            "",
            "",
            "class WarmupConstantSchedule(LRSchedule):"
        ]
    },
    {
        "number": 5968,
        "comments": "",
        "commit_message": "applied fix to matmul for tf backend for 1-D case\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def matmul(",
            "ret = tf.constant(0)",
            "else:",
            "",
            "-            ret = tf.math.multiply(x1, x2)[0]",
            "+            ret = tf.reduce_sum(tf.math.multiply(x1, x2))",
            "ret = tf.cast(ret, dtype=dtype_from)  # return ret",
            "",
            "else:"
        ]
    },
    {
        "number": 5969,
        "comments": "",
        "commit_message": "Merge pull request #7872 from PyTorchLightning/refactor/logger-poc-changes\n\nRandom fixes for logger connector PoC\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_early_stopping_patience_train(",
            "callbacks=[early_stop_callback],",
            "num_sanity_val_steps=0,",
            "max_epochs=10,",
            "+        progress_bar_refresh_rate=0,",
            ")",
            "trainer.fit(model)",
            "assert trainer.current_epoch == expected_stop_epoch"
        ]
    },
    {
        "number": 5971,
        "comments": "",
        "commit_message": "fix `fn_name` to `fn_tree` in `test_frontend_function` (#2991)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_permute(",
            "native_array_flags=native_array,",
            "fw=fw,",
            "frontend=\"torch\",",
            "-        fn_name=\"permute\",",
            "+        fn_tree=\"permute\",",
            "input=np.asarray(value, dtype=dtype),",
            "dims=axis,",
            ")"
        ]
    },
    {
        "number": 5974,
        "comments": "",
        "commit_message": "Fix colab tfds version\n\nPiperOrigin-RevId: 310566035\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "\"def get_example_data(dataset, num_examples, **data_kw):\\n\",",
            "\"  \\\"\\\"\\\"Show example data\\\"\\\"\\\"\\n\",",
            "\"  with tf.Session() as sess:\\n\",",
            "-        \"    it = dataset.get_data(**data_kw).take(num_examples).map(dataset.preprocess_fn).batch(num_examples).make_one_shot_iterator().get_next()\\n\",",
            "+        \"    batched_ds = dataset.get_data(**data_kw).take(num_examples).map(dataset.preprocess_fn).batch(num_examples)\\n\",",
            "+        \"    it = tf.data.make_one_shot_iterator(batched_ds).get_next()\\n\",",
            "\"    data = sess.run(it)\\n\",",
            "\"  return data\\n\",",
            "\"\\n\","
        ]
    },
    {
        "number": 5981,
        "comments": "",
        "commit_message": "rename logging -> loggers (#767)\n\n* move logging >> loggers\n\n* add warning\n\n* fix tests\n\n* logging alias\n\n* formatting\n\n* formatting\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TrainerDDPMixin(ABC):",
            "gpu_str = ','.join([str(x) for x in data_parallel_device_ids])",
            "os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu_str",
            "",
            "-        logging.info(f'VISIBLE GPUS: {os.environ[\"CUDA_VISIBLE_DEVICES\"]}')",
            "+        log.info(f'VISIBLE GPUS: {os.environ[\"CUDA_VISIBLE_DEVICES\"]}')",
            "",
            "def ddp_train(self, gpu_idx, model):",
            "\"\"\""
        ]
    },
    {
        "number": 5984,
        "comments": "",
        "commit_message": "Fix Pytext Documentation Build on CircleCI (#1596)\n\nSummary:\nPull Request resolved: https://github.com/facebookresearch/pytext/pull/1596\n\nBugfixes:\n- dependency errors: import/fix transformers installation, mock module for accelerators\n- rename bert_classification model inputs to differentiate config paths\n- fix adabelief docstring spacing so that Sphinx can parse it correctly\n- manually generate ExportConfig similarly to PytextConfig\n\nReviewed By: mikekgfb\n\nDifferential Revision: D26229331\n\nfbshipit-source-id: 2ecc72aa0b95b8a787cc475b40fe79fb4d0868a1\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "html_static_path = [\"_static\"]",
            "# autodoc_mock_imports = ['scipy', 'tensorboardX']",
            "",
            "# Manually mocking out the libraries to prevent requiring these modules",
            "-MOCK_MODULES = [\"scipy\", \"scipy.special\", \"torch.utils.tensorboard\"]",
            "+MOCK_MODULES = [",
            "+    \"scipy\",",
            "+    \"scipy.special\",",
            "+    \"torch.utils.tensorboard\",",
            "+    \"accelerators.pytorch.lib.glow_decorator\",",
            "+    \"pytext.PreprocessingMap.ttypes\",",
            "+]",
            "for mod_name in MOCK_MODULES:",
            "sys.modules[mod_name] = mock.Mock()"
        ]
    },
    {
        "number": 5989,
        "comments": "",
        "commit_message": "Full GraphGym + PyG integration (#3076)\n\n* Full GraphGym + PyG integration\n\n* fix torch.allclose\n",
        "label": "",
        "answer": "no",
        "change": [
            "import torch.nn as nn",
            "",
            "",
            "def init_weights(m):",
            "-    \"\"\"Performs weight initialization.\"\"\"",
            "+    r\"\"\"",
            "+    Performs weight initialization",
            "+",
            "+    Args:",
            "+        m (nn.Module): PyTorch module",
            "+",
            "+    \"\"\"",
            "if isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):",
            "m.weight.data.fill_(1.0)",
            "m.bias.data.zero_()"
        ]
    },
    {
        "number": 5991,
        "comments": "",
        "commit_message": "lintfixbot: Auto-commit fixed lint errors in codebase\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def where(",
            "# ----- #",
            "",
            "",
            "-def argwhere(",
            "-        x: torch.Tensor,",
            "-        /,",
            "-        *,",
            "-        out: Optional[torch.Tensor] = None",
            "-) -> torch.Tensor:",
            "+def argwhere(x: torch.Tensor, /, *, out: Optional[torch.Tensor] = None) -> torch.Tensor:",
            "return torch.argwhere(x)"
        ]
    },
    {
        "number": 5992,
        "comments": "",
        "commit_message": "Update to PyTorch 1.9.0 (#2887)\n\n* Update to PyTorch 1.9.0\n\n* Update torch.cholesky, torch.symeig\n\n* Fix torch.tensordot, torch.qr, funsor dependency\n\n* Fix torch import, AutoGuideList usage\n\n* Ignore bug in PyTorch jit\n\n* Ignore tracer warnings\n\n* Replace torch.solve with torch.linalg.solve\n\n* Xfail vectorized markov funsor tests\n\n* Update funsor version\n\n* Switch MNIST mirrors\n\n* Pin pillow version\n\n* Bump torchvision version\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_precision_to_scale_tril(batch_shape, event_shape):",
            "x = torch.randn(batch_shape + event_shape + event_shape)",
            "precision = x.matmul(x.transpose(-2, -1))",
            "actual = precision_to_scale_tril(precision)",
            "-    expected = precision.inverse().cholesky()",
            "+    expected = torch.linalg.cholesky(precision.inverse())",
            "assert_close(actual, expected)"
        ]
    },
    {
        "number": 5995,
        "comments": "",
        "commit_message": "Fix/find homography dlt iterated gradcheck fail (#1032)\n\n* Fixing #795: find_homography_dlt_iterated sometimes fails\n\n* Fixing #795: omitting gradcheck if torch.version < 1.7\n\n* Fixing #795: omitting gradcheck if torch.version < 1.7 with skipif\n\n* Fixing #795: omitting gradcheck if torch.version < 1.7 with skipif and flake8\n\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestFindHomographyDLTIter:",
            "kornia.transform_points(dst_homo_src, points_src), points_dst, rtol=1e-3, atol=1e-4)",
            "",
            "@pytest.mark.grad",
            "+    @pytest.mark.skipif(torch.__version__ < '1.7', reason=\"pytorch bug of incopatible types: #33546 fixed in v1.7\")",
            "def test_gradcheck(self, device):",
            "",
            "# Save initial seed"
        ]
    },
    {
        "number": 6000,
        "comments": "",
        "commit_message": "Test documentation (#511)\n\n* Activation Cleaning Docstring Test\n\n* Requirements Pinned with range to insure tested versions are used. Range are used to prevent updating requirements all the time.\n\n* setup.cfg file added with PEP8 configuration\n\n* activation.py refactored\n\n* docstring fixed - ready for documentation unittest\n\n* Yapf correction for max_line_length: 120\n\n* test yapf refactored\n\n* test documentation added\n\n* Missing requirement added: sphinx\n\n* Allow test on documentation to pass on warning\n\n* Fix travis dependencies install\n\n* Travis install script fixed\n\n* Travis install command fixed\n\n* Requirements conflict solved\n\n* Yapf Style modified and merged in file \"setup.cfg\"\n\n* Yapf Confiuguration Updated\n\n* Code Refactored with new YAPF formating style\n\n* Code Refactored with new YAPF formating style\n\n* Code Refactored with new YAPF formating style\n\n* shorten codes\n\n* Various Cleaning\n\n* Trailing Slashes removed\n\n* Test Recurrent Fixed\n\n* Line Width Fix\n\n* docs requirements updated\n\n* fix example docs style\n\n* Codacy Issue Fixed\n\n* Merge Errors fixed\n\n* YAPF Style Applied\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class ReconLayer(DenseLayer):",
            "# ce = cost.cross_entropy(y, x_recon)                                               # <haodong>: list , list , Error (only be used for softmax output)",
            "# ce = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y, x_recon))          # <haodong>: list , list , Error (only be used for softmax output)",
            "# ce = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(y, x_recon))   # <haodong>: list , index , Error (only be used for softmax output)",
            "-        L2_w = tf.contrib.layers.l2_regularizer(lambda_l2_w)(self.train_params[0]) \\",
            "-                + tf.contrib.layers.l2_regularizer(lambda_l2_w)(self.train_params[2])           # faster than the code below",
            "+        L2_w = tf.contrib.layers.l2_regularizer(lambda_l2_w)(",
            "+            self.train_params[0]",
            "+        ) + tf.contrib.layers.l2_regularizer(lambda_l2_w)(self.train_params[2])  # faster than the code below",
            "# L2_w = lambda_l2_w * tf.reduce_mean(tf.square(self.train_params[0])) + lambda_l2_w * tf.reduce_mean( tf.square(self.train_params[2]))",
            "",
            "# DropNeuro"
        ]
    },
    {
        "number": 6001,
        "comments": "",
        "commit_message": "[Testing] Fix LINT/sphinx errors. (#8874)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class RecurrentNetwork(TorchModelV2):",
            "raise NotImplementedError(\"You must implement this for an RNN model\")",
            "",
            "",
            "-class LSTMWrapper(RecurrentNetwork):",
            "+class LSTMWrapper(RecurrentNetwork, nn.Module):",
            "\"\"\"An LSTM wrapper serving as an interface for ModelV2s that set use_lstm.",
            "\"\"\"",
            "",
            "def __init__(self, obs_space, action_space, num_outputs, model_config,",
            "name):",
            "",
            "-        super(LSTMWrapper, self).__init__(obs_space, action_space, None,",
            "-                                          model_config, name)",
            "+        nn.Module.__init__(self)",
            "+        super().__init__(obs_space, action_space, None, model_config, name)",
            "",
            "self.cell_size = model_config[\"lstm_cell_size\"]",
            "self.lstm = nn.LSTM(self.num_outputs, self.cell_size, batch_first=True)"
        ]
    },
    {
        "number": 6003,
        "comments": "",
        "commit_message": "fixed self-loops in pointnet\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class PPFConv(MessagePassing):",
            "for use in message passing in bipartite graphs.",
            "edge_index (LongTensor): The edge indices.",
            "\"\"\"",
            "-        if torch.is_tensor(x):",
            "+        if torch.is_tensor(pos):  # Add self-loops for symmetric adjacencies.",
            "edge_index, _ = remove_self_loops(edge_index)",
            "edge_index = add_self_loops(edge_index, num_nodes=x.size(0))"
        ]
    },
    {
        "number": 6005,
        "comments": "",
        "commit_message": "fix image summary\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Model(ModelDesc):",
            "fake_output = tf.image.grayscale_to_rgb(fake_output)",
            "viz = (tf.concat(2, [input, output, fake_output]) + 1.0) * 128.0",
            "viz = tf.cast(tf.clip_by_value(viz, 0, 255), tf.uint8, name='viz')",
            "-        tf.image_summary('gen', viz, max_images=max(30, BATCH))",
            "+        tf.image_summary('gen', viz, max_outputs=max(30, BATCH))",
            "",
            "all_vars = tf.trainable_variables()",
            "self.g_vars = [v for v in all_vars if v.name.startswith('gen/')]"
        ]
    },
    {
        "number": 6006,
        "comments": "",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class GatedCnnEncoder(Seq2SeqEncoder):",
            "",
            "self._return_all_layers = return_all_layers",
            "",
            "-    def forward(self, token_embeddings: torch.Tensor, mask: torch.Tensor):",
            "+    def forward(self, token_embeddings: torch.Tensor, mask: torch.BoolTensor):",
            "",
            "# Convolutions need transposed input",
            "transposed_embeddings = torch.transpose(token_embeddings, 1, 2)",
            "",
            "# We need to broadcast the mask to feature dimension,",
            "# and to use masked_fill_ we need the inverse of the mask.",
            "-        mask_for_fill = (1 - mask).unsqueeze(1).to(dtype=torch.bool)",
            "+        mask_for_fill = ~mask.unsqueeze(1)",
            "",
            "if self._return_all_layers:",
            "# outputs will be [[all forward layers], [all backward layers]]"
        ]
    },
    {
        "number": 6008,
        "comments": "",
        "commit_message": "fix bug\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Imagen(BaseGaussianDiffusion):",
            "device = next(self.parameters()).device",
            "",
            "lowres_sample_noise_level = default(lowres_sample_noise_level, self.lowres_sample_noise_level)",
            "-        lowres_noise_times = torch.full((batch_size,), lowres_sample_noise_level, device = device, dtype = torch.long)",
            "+        lowres_noise_times = torch.full((batch_size,), int(lowres_sample_noise_level * self.num_timesteps), device = device, dtype = torch.long)",
            "",
            "for unet_number, unet, channel, image_size, learned_variance in tqdm(zip(range(1, len(self.unets) + 1), self.unets, self.sample_channels, self.image_sizes, self.learned_variance)):"
        ]
    },
    {
        "number": 6009,
        "comments": "",
        "commit_message": "[rllib] Initial RLLib documentation (#969)\n\n* initial documentation for RLLib\n\n* more RL documentation\n\n* fix linting\n\n* fix comments\n\n* update\n\n* fix\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "from ray.rllib.models.misc import (conv2d, linear, flatten,",
            "normc_initializer)",
            "from ray.rllib.models.model import Model",
            "",
            "-use_tf100_api = (distutils.version.LooseVersion(tf.VERSION) >=",
            "-                 distutils.version.LooseVersion(\"1.0.0\"))",
            "-",
            "",
            "class LSTM(Model):",
            "# TODO(rliaw): Add LSTM code for other algorithms",
            "def _init(self, inputs, num_outputs, options):",
            "+        use_tf100_api = (distutils.version.LooseVersion(tf.VERSION) >=",
            "+                         distutils.version.LooseVersion(\"1.0.0\"))",
            "+",
            "self.x = x = inputs",
            "for i in range(4):",
            "x = tf.nn.elu(conv2d(x, 32, \"l{}\".format(i + 1), [3, 3], [2, 2]))"
        ]
    },
    {
        "number": 6013,
        "comments": "",
        "commit_message": "Summaries completely changed, distributed mode incompletely changed, various fixes\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Categorical(Distribution):",
            "definite = tf.argmax(input=logits, axis=-1)",
            "",
            "# Non-deterministic: sample action using Gumbel distribution",
            "-        uniform = tf.random_uniform(shape=tf.shape(input=logits), minval=util.epsilon, maxval=(1.0 - util.epsilon))",
            "-        gumbel_distribution = -tf.log(x=-tf.log(x=uniform))",
            "+        uniform_distribution = tf.random_uniform(",
            "+            shape=tf.shape(input=logits),",
            "+            minval=util.epsilon,",
            "+            maxval=(1.0 - util.epsilon)",
            "+        )",
            "+        gumbel_distribution = -tf.log(x=-tf.log(x=uniform_distribution))",
            "sampled = tf.argmax(input=(logits + gumbel_distribution), axis=-1)",
            "",
            "return tf.where(condition=deterministic, x=definite, y=sampled)"
        ]
    },
    {
        "number": 6014,
        "comments": "",
        "commit_message": "Improvements in the Documentation of Backend (#5767)\n\n* Fix and improvements to the `backend` documentation\n\nImproved Preamble of the `backend.md` template:\n- fixed a typo\n- Added few notes that makes the documentation more self explanatory\n- Made all code examples running by Copy&Paste\n\nAligned the format of the  `backend()` function\n\nFixed docstring of `set_image_dim_ordering()` function\n\n* Fixed a Typo in %USERPROFILE% env name for Window Users\n\n* Added `_variable` so not to get a different value every time\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "else:",
            "def backend():",
            "\"\"\"Publicly accessible method",
            "for determining the current backend.",
            "+",
            "+    # Returns",
            "+        String, the name of the backend Keras is currently using.",
            "+",
            "+    # Example",
            "+    ```python",
            "+        >>> keras.backend.backend()",
            "+        'tensorflow'",
            "+    ```",
            "\"\"\"",
            "return _BACKEND"
        ]
    },
    {
        "number": 6019,
        "comments": "",
        "commit_message": "fix typos (#5)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class WaveNet(object):",
            "",
            "# We skip connections from the outputs of each layer, adding them",
            "# all up here.",
            "-            total = sum(out)",
            "+            total = sum(outputs)",
            "transformed1 = tf.nn.relu(total)",
            "conv1 = tf.nn.conv2d(transformed1, w1, [1] * 4, padding=\"SAME\")",
            "transformed2 = tf.nn.relu(conv1)"
        ]
    },
    {
        "number": 6024,
        "comments": "",
        "commit_message": "Refactor download (#4384)\n\n* Refactor download\n\n* Fix textvqa dataset\n\n* Move download module one level up\n\n* Fix test mock fsspec\n\n* Fix docs\n\n* Fix import of DownloadConfig\n\n* Refactor mock_download_manager\n\n* Refactor DownloadConfig\n\n* Fix docs\n\n* Add sections to docs of builder classes\n\n* Use relative imports\n\n* Fix path\n\n* Fix import\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Textvqa(datasets.GeneratorBasedBuilder):",
            "citation=_CITATION,",
            ")",
            "",
            "-    def _split_generators(self, dl_manager: datasets.utils.DownloadManager):",
            "+    def _split_generators(self, dl_manager):",
            "downloaded_files = dl_manager.download_and_extract(_URLS)",
            "return [",
            "datasets.SplitGenerator("
        ]
    },
    {
        "number": 6026,
        "comments": "",
        "commit_message": "fix `all` and `any` to be consistent across backends (#4539)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def any(",
            "keepdims: bool = False,",
            "out: Optional[torch.Tensor] = None,",
            ") -> torch.Tensor:",
            "-    x = ivy.asarray(x).type(torch.bool)",
            "+    x = torch.as_tensor(x).type(torch.bool)",
            "if axis is None:",
            "num_dims = len(x.shape)",
            "axis = list(range(num_dims))"
        ]
    },
    {
        "number": 6028,
        "comments": "",
        "commit_message": "NAF added, regularization loss handling improved, various fixes and formatting\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Optimizer(tf.train.Optimizer):",
            "raise NotImplementedError",
            "",
            "def minimize(self, time, variables, **kwargs):",
            "-        diffs = self.fn_step(time=time, variables=variables, **kwargs)",
            "+        diffs = self.step(time=time, variables=variables, **kwargs)",
            "# diffs[0] = tf.Print(diffs[0], (diffs[0],))",
            "with tf.control_dependencies(control_inputs=diffs):",
            "return tf.no_op()"
        ]
    },
    {
        "number": 6029,
        "comments": "",
        "commit_message": "Fix batch size extraction when set by the user in `LightningModule.log` (#10408)\n\nCo-authored-by: Carlos Mocholi <carlossmocholi@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_loops_state_dict_structure():",
            "\"is_last_batch\": False,",
            "},",
            "\"_results\": {",
            "+                \"batch\": None,",
            "+                \"batch_size\": None,",
            "\"training\": False,",
            "-                \"_batch_size\": torch.tensor(1),",
            "\"device\": None,",
            "\"items\": {},",
            "},"
        ]
    },
    {
        "number": 6032,
        "comments": "",
        "commit_message": "Fix error related to cpu only machines\n",
        "label": "",
        "answer": "no",
        "change": [
            "def load_model(model_path, weights_path=None):",
            "if weights_path:",
            "if weights_path.endswith(\".pth\"):",
            "# Load checkpoint weights",
            "-            model.load_state_dict(torch.load(weights_path))",
            "+            model.load_state_dict(torch.load(weights_path, map_location=device))",
            "else:",
            "# Load darknet weights",
            "model.load_darknet_weights(weights_path)"
        ]
    },
    {
        "number": 6033,
        "comments": "",
        "commit_message": "style(models): fix style error (#1566)\n\nstyle(models): fix style error\n",
        "label": "",
        "answer": "no",
        "change": [
            "class YOLOXHead(nn.Module):",
            "grids = torch.cat(grids, dim=1).type(dtype)",
            "strides = torch.cat(strides, dim=1).type(dtype)",
            "",
            "-        outputs = torch.cat([(outputs[..., 0:2] + grids) * strides, torch.exp(outputs[..., 2:4]) * strides, outputs[..., 4:]], dim=-1)",
            "+        outputs = torch.cat([",
            "+            (outputs[..., 0:2] + grids) * strides,",
            "+            torch.exp(outputs[..., 2:4]) * strides,",
            "+            outputs[..., 4:]",
            "+        ], dim=-1)",
            "return outputs",
            "",
            "def get_losses("
        ]
    },
    {
        "number": 6034,
        "comments": "",
        "commit_message": "fix DistributedTrainer (fix #505)\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class SimplePredictBuilder(GraphBuilder):",
            "self._ns_name, self._device))",
            "",
            "with tf.device(self._device), \\",
            "-                self._maybe_open_vs(), \\",
            "TowerContext(",
            "self._ns_name, is_training=False, vs_name=self._vs_name):",
            "inputs = input.get_input_tensors()"
        ]
    },
    {
        "number": 6035,
        "comments": "",
        "commit_message": "Fix mutable proj_out weight in the Attention layer (#73)\n\n* Catch unused params in DDP\n\n* Fix proj_out, add test\n",
        "label": "",
        "answer": "no",
        "change": [
            "class AttentionBlock(nn.Module):",
            "a = torch.einsum(\"bts,bcs->bct\", weight, v)",
            "h = a.reshape(bs, -1, length)",
            "",
            "-        h = self.proj_out(h)",
            "+        h = self.proj(h)",
            "h = h.reshape(b, c, *spatial)",
            "",
            "result = x + h"
        ]
    },
    {
        "number": 6037,
        "comments": "",
        "commit_message": "fixed failing core.general unit tests.\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def linspace(start, stop, num, axis=None, dev=None):",
            "if axis is None:",
            "axis = -1",
            "dev = default_device(dev)",
            "-    with _tf.device('/' + dev.upper()):",
            "+    with _tf.device(ivy.dev_from_str(dev)):",
            "return _tf.linspace(start, stop, num, axis=axis)"
        ]
    },
    {
        "number": 6038,
        "comments": "",
        "commit_message": "fix tensorflow asarray inplace updating its inputs\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def asarray(",
            "",
            "dtype = ivy.as_ivy_dtype(ivy.default_dtype(dtype=dtype, item=obj))",
            "return tf.convert_to_tensor(",
            "-                    ivy.nested_map(obj, lambda x: tf.cast(x, dtype)),",
            "+                    ivy.nested_map(obj, lambda x: tf.cast(x, dtype), shallow=False),",
            "dtype=dtype,",
            ")",
            "else:"
        ]
    },
    {
        "number": 6039,
        "comments": "",
        "commit_message": "Fix coordinate system conventions in renderer\n\nSummary:\n## Updates\n\n- Defined the world and camera coordinates according to this figure. The world coordinates are defined as having +Y up, +X left and +Z in.\n\n{F230888499}\n\n- Removed all flipping from blending functions.\n- Updated the rasterizer to return images with +Y up and +X left.\n- Updated all the mesh rasterizer tests\n    - The expected values are now defined in terms of the default +Y up, +X left\n    - Added tests where the triangles in the meshes are non symmetrical so that it is clear which direction +X and +Y are\n\n## Questions:\n- Should we have **scene settings** instead of raster settings?\n    - To be more correct we should be [z clipping in the rasterizer based on the far/near clipping planes](https://github.com/ShichenLiu/SoftRas/blob/master/soft_renderer/cuda/soft_rasterize_cuda_kernel.cu#L400) - these values are also required in the blending functions so should we make these scene level parameters and have a scene settings tuple which is available to the rasterizer and shader?\n\nReviewed By: gkioxari\n\nDifferential Revision: D20208604\n\nfbshipit-source-id: 55787301b1bffa0afa9618f0a0886cc681da51f3\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def camera_position_from_spherical_angles(",
            "azim = math.pi / 180.0 * azim",
            "x = dist * torch.cos(elev) * torch.sin(azim)",
            "y = dist * torch.sin(elev)",
            "-    z = -dist * torch.cos(elev) * torch.cos(azim)",
            "+    z = dist * torch.cos(elev) * torch.cos(azim)",
            "camera_position = torch.stack([x, y, z], dim=1)",
            "if camera_position.dim() == 0:",
            "camera_position = camera_position.view(1, -1)  # add batch dim."
        ]
    },
    {
        "number": 6040,
        "comments": "",
        "commit_message": "Bug fix: espnet/espnet2/torch_utils/recursive_op\n",
        "label": "",
        "answer": "no",
        "change": [
            "def recursive_average(obj, weight: torch.Tensor, distributed: bool = False):",
            "obj = recursive_sum(obj, weight, distributed)",
            "weight = weight.sum()",
            "if distributed:",
            "-        torch.distributed.all_reduce(weight, op=torch.distributed.ReduceOP.SUM)",
            "+        torch.distributed.all_reduce(weight, op=torch.distributed.reduce_op.SUM)",
            "# Normalize weight to be sum-to-1",
            "obj = recursive_divide(obj, weight)",
            "return obj, weight"
        ]
    },
    {
        "number": 6041,
        "comments": "",
        "commit_message": "Augmentation Base Refactor (#2117)\n\n* refactor\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Updated augmentation base.\n\n* Removed label & start apply/inverse\n\n* Finished augmentaion base refactor\n\n* container refactoring\n\n* Added missing files\n\n* Added ops\n\n* Update sequential ops\n\n* Almost there\n\n* Fixed computation matrix computation\n\n* Fixed randomcrop\n\n* Fixed erasing\n\n* almost almost\n\n* finished\n\n* Added missing file\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Bug and typing fix\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Added list typing\n\n* fixed test base\n\n* Fixed typing\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* bug fix\n\n* Update kornia/augmentation/_2d/geometric/crop.py\n\nCo-authored-by: Jo\u00e3o Gustavo A. Amorim <joaogustavoamorim@gmail.com>\n\n* Fixed build-docs\n\n* Fixed bfloat16 issue on torch1.13.1\n\n* Revert the last commit\n\n* Fixed typing\n\n* Fixed typos\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Typo fix\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Jo\u00e3o Gustavo A. Amorim <joaogustavoamorim@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def bbox_to_mask(boxes: torch.Tensor, width: int, height: int) -> torch.Tensor:",
            "\"\"\"",
            "validate_bbox(boxes)",
            "# zero padding the surroudings",
            "-    mask = torch.zeros((len(boxes), height + 2, width + 2), dtype=torch.float, device=boxes.device)",
            "+    mask = torch.zeros((len(boxes), height + 2, width + 2), dtype=boxes.dtype, device=boxes.device)",
            "# push all points one pixel off",
            "# in order to zero-out the fully filled rows or columns",
            "box_i = (boxes + 1).long()"
        ]
    },
    {
        "number": 6045,
        "comments": "",
        "commit_message": "CDN urls (#4030)\n\n* [file_utils] use_cdn + documentation\n\n* Move to cdn. urls for weights\n\n* [urls] Hotfix for bert-base-japanese\n",
        "label": "",
        "answer": "no",
        "change": [
            "from .tokenization_utils import BatchEncoding",
            "logger = logging.getLogger(__name__)",
            "",
            "TF_GPT2_PRETRAINED_MODEL_ARCHIVE_MAP = {",
            "-    \"gpt2\": \"https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-tf_model.h5\",",
            "-    \"gpt2-medium\": \"https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-medium-tf_model.h5\",",
            "-    \"gpt2-large\": \"https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-large-tf_model.h5\",",
            "-    \"gpt2-xl\": \"https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-xl-tf_model.h5\",",
            "-    \"distilgpt2\": \"https://s3.amazonaws.com/models.huggingface.co/bert/distilgpt2-tf_model.h5\",",
            "+    \"gpt2\": \"https://cdn.huggingface.co/gpt2-tf_model.h5\",",
            "+    \"gpt2-medium\": \"https://cdn.huggingface.co/gpt2-medium-tf_model.h5\",",
            "+    \"gpt2-large\": \"https://cdn.huggingface.co/gpt2-large-tf_model.h5\",",
            "+    \"gpt2-xl\": \"https://cdn.huggingface.co/gpt2-xl-tf_model.h5\",",
            "+    \"distilgpt2\": \"https://cdn.huggingface.co/distilgpt2-tf_model.h5\",",
            "}"
        ]
    },
    {
        "number": 6050,
        "comments": "",
        "commit_message": "[tune] Fix tutorial training on GPU (#12914)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "space = {",
            "hyperopt_search = HyperOptSearch(space, metric=\"mean_accuracy\", mode=\"max\")",
            "",
            "analysis = tune.run(train_mnist, num_samples=10, search_alg=hyperopt_search)",
            "+",
            "+# To enable GPUs, use this instead:",
            "+# analysis = tune.run(",
            "+#     train_mnist, config=search_space, resources_per_trial={'gpu': 1})",
            "+",
            "# __run_searchalg_end__",
            "",
            "# __run_analysis_begin__"
        ]
    },
    {
        "number": 6054,
        "comments": "",
        "commit_message": "Fix issues with tests\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_onnxruntime(",
            "res_orig = tuple(model(*inputs_example))",
            "assert all(",
            "[",
            "-                    torch.allclose(res_tensor, res_orig_tensor, rtol=1e-03)",
            "+                    torch.allclose(res_tensor, res_orig_tensor, rtol=2e-01)",
            "for (res_tensor, res_orig_tensor) in zip(res, res_orig)",
            "]",
            ")"
        ]
    },
    {
        "number": 6055,
        "comments": "",
        "commit_message": "fix nois in ldm\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class LatentDiffusion(DiffusionPipeline):",
            "# 3. optionally sample variance",
            "variance = 0",
            "if eta > 0:",
            "-                noise = self.noise_scheduler.sample_noise(image.shape, device=image.device, generator=generator)",
            "+                noise = torch.randn(image.shape, generator=generator, device=image.device)",
            "variance = self.noise_scheduler.get_variance(t, num_inference_steps).sqrt() * eta * noise",
            "",
            "# 4. set current image to prev_image: x_t -> x_t-1"
        ]
    },
    {
        "number": 6059,
        "comments": "",
        "commit_message": "fix bugs\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TGN(torch.nn.Module):",
            ")",
            "",
            "self.register_buffer('memory', torch.empty(num_nodes, memory_dim))",
            "-        self.register_buffer('last_update', torch.empty(num_nodes))",
            "+        self.register_buffer('last_update',",
            "+                             torch.empty(num_nodes, dtype=torch.long))",
            "self.previous_events = None",
            "",
            "self.reset_parameters()"
        ]
    },
    {
        "number": 6062,
        "comments": "",
        "commit_message": "Fix batch processing in enh_inference\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def config_file(tmp_path: Path):",
            "",
            "",
            "@pytest.mark.execution_timeout(5)",
            "+@pytest.mark.parametrize(\"batch_size\", [1, 2])",
            "@pytest.mark.parametrize(",
            "\"input_size, segment_size, hop_size\", [(16000, None, None), (35000, 2.4, 0.8)]",
            ")",
            "-def test_SeparateSpeech(config_file, input_size, segment_size, hop_size):",
            "+def test_SeparateSpeech(config_file, batch_size, input_size, segment_size, hop_size):",
            "if not is_torch_1_2_plus:",
            "pytest.skip(\"Pytorch Version Under 1.2 is not supported for Enh task\")",
            "",
            "separate_speech = SeparateSpeech(",
            "enh_train_config=config_file, segment_size=segment_size, hop_size=hop_size",
            ")",
            "-    wav = torch.rand(1, input_size)",
            "+    wav = torch.rand(batch_size, input_size)",
            "separate_speech(wav, fs=8000)"
        ]
    },
    {
        "number": 6071,
        "comments": "",
        "commit_message": "enable `disallow_incomplete_defs` on mypy (#2094)\n\n* enable `disallow_incomplete_defs` on mypy\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix `blur_pool2d` doc\n\n* finish v1: works on torch 1.13.1\n\n- Remove JIT support for Boxes3D\n\n* rip off the np typing\n\n* replace `Size` with `Tuple[int, ...]` on augs\n\n* add `Dtype` to kornia.filters.kernels\n\n* minor fix after rebase\n\n* Remove old torch from typing CI\n\n---------\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def batched_forward(",
            "if st >= end:",
            "continue",
            "out_list.append(model_dev(data[st:end].to(device), **kwargs))",
            "-        out = torch.cat(out_list, dim=0)",
            "+        out = concatenate(out_list, 0)",
            "return out.to(data.device)",
            "return model(data, **kwargs)"
        ]
    },
    {
        "number": 6077,
        "comments": "",
        "commit_message": "Zca inverse transform fix (#695)\n\n* fix inv transform\n\n* update zca test\n\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def zca_mean(inp: torch.Tensor, dim: int = 0,",
            "",
            "T_inv: Optional[torch.Tensor] = None",
            "if return_inverse:",
            "-        T_inv = (U).mm(torch.sqrt(S) * U.t())",
            "+        T_inv = (U).mm(torch.sqrt(S + eps) * U.t())",
            "",
            "return T, mean, T_inv"
        ]
    },
    {
        "number": 6078,
        "comments": "",
        "commit_message": "various improvements and fixes\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Model(Module):",
            "# at most one terminal",
            "assertions.append(",
            "tf.debugging.assert_less_equal(",
            "-                x=tf.count_nonzero(input_tensor=terminal, dtype=util.tf_dtype(dtype='int')),",
            "+                x=tf.math.count_nonzero(input_tensor=terminal, dtype=util.tf_dtype(dtype='int')),",
            "y=tf.constant(value=1, dtype=util.tf_dtype(dtype='int'))",
            ")",
            ")",
            "# if terminal, last timestep in batch",
            "assertions.append(",
            "-            tf.debugging.assert_equal(x=tf.reduce_any(input_tensor=terminal), y=terminal[-1])",
            "+            tf.debugging.assert_equal(x=tf.math.reduce_any(input_tensor=terminal), y=terminal[-1])",
            ")",
            "",
            "# Set global tensors"
        ]
    },
    {
        "number": 6080,
        "comments": "",
        "commit_message": "Fix some model support functions\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class EfficientFormer(nn.Module):",
            "def get_classifier(self):",
            "return self.head, self.head_dist",
            "",
            "-    def reset_classifier(self, num_classes, global_pool=None, distillation=None):",
            "+    def reset_classifier(self, num_classes, global_pool=None):",
            "self.num_classes = num_classes",
            "if global_pool is not None:",
            "self.global_pool = global_pool",
            "self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()",
            "-        if self.dist:",
            "-            self.head_dist = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()",
            "+        self.head_dist = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()",
            "",
            "@torch.jit.ignore",
            "def set_distilled_training(self, enable=True):"
        ]
    },
    {
        "number": 6081,
        "comments": "",
        "commit_message": "fix arange dtype\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def grid_index(height, width, device=None):",
            "",
            "",
            "def grid_pos(height, width, dtype=None, device=None):",
            "+    dtype = torch.float if dtype is None else dtype",
            "x = torch.arange(width, dtype=dtype, device=device)",
            "y = (height - 1) - torch.arange(height, dtype=dtype, device=device)"
        ]
    },
    {
        "number": 6083,
        "comments": "",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestEndpointSpanExtractor:",
            "# for both the forward and backward directions.",
            "extractor = EndpointSpanExtractor(8, \"x,y\", use_exclusive_start_indices=True)",
            "indices = torch.LongTensor([[[1, 3], [2, 4]], [[0, 2], [0, 1]]])",
            "-        sequence_mask = torch.LongTensor([[1, 1, 1, 1, 1], [1, 1, 1, 0, 0]])",
            "+        sequence_mask = torch.BoolTensor(",
            "+            [[True, True, True, True, True], [True, True, True, False, False]]",
            "+        )",
            "",
            "span_representations = extractor(sequence_tensor, indices, sequence_mask=sequence_mask)"
        ]
    },
    {
        "number": 6084,
        "comments": "",
        "commit_message": "Use attention_mask in pretrained transformers (#3560)\n\n* Add attention_mask to PretrainedTransformerIndexer\n\n* Add attention_mask to PretrainedTransformerEmbedder\n\n* delete mask_padding_with_zero\n\n* set model_type for xlnet\n\n* Rename attention_mask -> mask\n\n* Fix PretrainedTransformerEmbedder\n\n* Add end-to-end test\n\nCo-authored-by: Dirk Groeneveld <groeneveld@gmail.com>\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class PretrainedTransformerEmbedder(TokenEmbedder):",
            "def get_output_dim(self):",
            "return self.output_dim",
            "",
            "-    def forward(self, token_ids: torch.LongTensor) -> torch.Tensor:  # type: ignore",
            "+    def forward(",
            "+        self, token_ids: torch.LongTensor, attention_mask: torch.LongTensor",
            "+    ) -> torch.Tensor:  # type: ignore",
            "",
            "-        return self.transformer_model(token_ids)[0]",
            "+        return self.transformer_model(input_ids=token_ids, attention_mask=attention_mask)[0]"
        ]
    },
    {
        "number": 6085,
        "comments": "",
        "commit_message": "fix parallel devices return type & add copyright (#6215)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class AcceleratorConnector(object):",
            "return len(gpus)",
            "",
            "@property",
            "-    def parallel_devices(self) -> Union[List[torch.device], int]:",
            "+    def parallel_devices(self) -> List[Union[torch.device, int]]:",
            "if self.on_gpu:",
            "devices = [torch.device(\"cuda\", i) for i in self.parallel_device_ids]",
            "elif self.on_tpu:"
        ]
    },
    {
        "number": 6088,
        "comments": "",
        "commit_message": "\ud83d\udd27 Fix #70\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def main():",
            "mel_outputs, post_mel_outputs, stop_outputs, alignment_historys = tacotron2.inference(",
            "charactor,",
            "char_length,",
            "-            speaker_ids=tf.zeros(shape=[tf.shape(charactor)[0]]),",
            "+            speaker_ids=tf.zeros(shape=[tf.shape(charactor)[0]], dtype=tf.int32),",
            ")",
            "",
            "# convert to numpy"
        ]
    },
    {
        "number": 6089,
        "comments": "",
        "commit_message": "fix cuda test issues\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestTransformLAFs:",
            "# projected should be equal as initial",
            "assert_allclose(lafs_src, lafs_dst_to_src)",
            "",
            "-    def test_gradcheck(self):",
            "+    def test_gradcheck(self, device):",
            "# generate input data",
            "batch_size, num_points, num_dims = 2, 3, 2",
            "eye_size = 3",
            "-        points_src = torch.rand(batch_size, num_points, 2, 3)",
            "-        dst_homo_src = utils.create_random_homography(batch_size, eye_size)",
            "+        points_src = torch.rand(batch_size, num_points, 2, 3).to(device)",
            "+        dst_homo_src = utils.create_random_homography(batch_size, eye_size).to(device)",
            "# evaluate function gradient",
            "points_src = utils.tensor_to_gradcheck_var(points_src)  # to var",
            "dst_homo_src = utils.tensor_to_gradcheck_var(dst_homo_src)  # to var"
        ]
    },
    {
        "number": 6098,
        "comments": "",
        "commit_message": "Formatting fixes (#12090)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def predict_generator(model, generator,",
            "warnings.warn(",
            "UserWarning('Using a generator with `use_multiprocessing=True`'",
            "' and multiple workers may duplicate your data.'",
            "-                        ' Please consider using the`keras.utils.Sequence'",
            "+                        ' Please consider using the `keras.utils.Sequence'",
            "' class.'))",
            "if steps is None:",
            "if use_sequence_api:"
        ]
    },
    {
        "number": 6099,
        "comments": "",
        "commit_message": "small fix for `dev` in torch (#2232)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "torch_scatter = None",
            "def dev(x: torch.Tensor, as_native: bool = False) -> Union[ivy.Device, torch.device]:",
            "dv = x.device",
            "if as_native:",
            "-        return torch.device(dv.type.replace(\"gpu\", \"cuda\"))",
            "+        if isinstance(dv, torch.device):",
            "+            dv = dv.type",
            "+        return torch.device(dv.replace(\"gpu\", \"cuda\"))",
            "return as_ivy_dev(dv)"
        ]
    },
    {
        "number": 6101,
        "comments": "",
        "commit_message": "RANSAC improvements (#1435)\n\n- Added LU solver for RANSAC-homography, it speeds-up the minimal-solver part x10\n- Now find_fundamental weights is optional, as it was for homography\n- Fix test precision and made one xFail test to be compulsory\n- Fixed tests for fundamental matrix, as one have to have >= 8 points (also added to docstring)\n- updates deprecated torch.svd to torch.linalg.svd\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def _nullspace(A):",
            "",
            "Return the smallest singular value and the corresponding vector.",
            "\"\"\"",
            "-    _, s, vh = torch.svd(A)",
            "-    return s[..., -1], vh[..., -1]",
            "+    _, s, v = _torch_svd_cast(A)",
            "+    return s[..., -1], v[..., -1]",
            "",
            "",
            "def projections_from_fundamental(F_mat: torch.Tensor) -> torch.Tensor:"
        ]
    },
    {
        "number": 6103,
        "comments": "",
        "commit_message": "Use f-strings in the dataset scripts (#3291)\n\n* Finishes #3257\n\nUsed f-strings to format the .py files in the dataset folder\n\n* Fix style\n\n* Fix hkcancor dataset\n\nCo-authored-by: Mario \u0160a\u0161ko <mario@huggingface.co>\nCo-authored-by: mariosasko <mariosasko777@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class JigsawUnintendedBias(datasets.GeneratorBasedBuilder):",
            "",
            "if not os.path.exists(data_dir):",
            "raise FileNotFoundError(",
            "-                \"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('jigsaw_unintended_bias', data_dir=...)`. Manual download instructions: {}\".format(",
            "-                    data_dir, self.manual_download_instructions",
            "-                )",
            "+                f\"{data_dir} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('jigsaw_unintended_bias', data_dir=...)`. Manual download instructions: {self.manual_download_instructions}\"",
            ")",
            "",
            "return ["
        ]
    },
    {
        "number": 6107,
        "comments": "",
        "commit_message": "Improve reproduceability 2/3 (#1906)\n\n* [Repro] Correct reproducability\n\n* up\n\n* up\n\n* uP\n\n* up\n\n* need better image\n\n* allow conversion from no state dict checkpoints\n\n* up\n\n* up\n\n* up\n\n* up\n\n* check tensors\n\n* check tensors\n\n* check tensors\n\n* check tensors\n\n* next try\n\n* up\n\n* up\n\n* better name\n\n* up\n\n* up\n\n* Apply suggestions from code review\n\n* correct more\n\n* up\n\n* replace all torch randn\n\n* fix\n\n* correct\n\n* correct\n\n* finish\n\n* fix more\n\n* up\n",
        "label": "",
        "answer": "no",
        "change": [
            "class AudioDiffusionPipeline(DiffusionPipeline):",
            "input_dims = self.get_input_dims()",
            "self.mel.set_resolution(x_res=input_dims[1], y_res=input_dims[0])",
            "if noise is None:",
            "-            noise = torch.randn(",
            "+            noise = randn_tensor(",
            "(",
            "batch_size,",
            "self.unet.in_channels,"
        ]
    },
    {
        "number": 6108,
        "comments": "",
        "commit_message": "fix linter errors\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Tester(unittest.TestCase):",
            "# create transformation (rotation)",
            "M = torch.tensor([[",
            "[torch.cos(alpha), -torch.sin(alpha), 0.],",
            "-            [torch.sin(alpha),  torch.cos(alpha), 0.],",
            "-            [              0.,                0., 1.],",
            "+            [torch.sin(alpha), torch.cos(alpha), 0.],",
            "+            [0., 0., 1.],",
            "]])  # Bx3x3",
            "M = utils.tensor_to_gradcheck_var(M, requires_grad=False)  # to var"
        ]
    },
    {
        "number": 6109,
        "comments": "",
        "commit_message": "fix doc\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def to_hetero_with_bases(module: Module, metadata: Metadata, num_bases: int,",
            "import torch",
            "from torch_geometric.nn import SAGEConv, to_hetero_with_bases",
            "",
            "-        Net(torch.nn.Module):",
            "+        class GNN(torch.nn.Module):",
            "def __init__(self):",
            "-                self.conv1 = SAGEConv(16, 16)",
            "-                self.conv2 = SAGEConv(16, 16)",
            "+                self.conv1 = SAGEConv((16, 16), 32)",
            "+                self.conv2 = SAGEConv((32, 32), 32)",
            "",
            "def forward(self, x, edge_index):",
            "x = self.conv1(x, edge_index).relu()",
            "x = self.conv2(x, edge_index).relu()",
            "return x",
            "",
            "-        model = Net()",
            "+        model = GNN()",
            "",
            "node_types = ['paper', 'author']",
            "edge_types = ["
        ]
    },
    {
        "number": 6112,
        "comments": "",
        "commit_message": "fix median of torch as torch.median's behavior is different from expected, so replace it with torch.quantile\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def median(",
            "temp = input",
            "if hasattr(axis, \"__iter__\"):",
            "for dim in axis:",
            "-            temp = torch.median(",
            "+            temp = torch.quantile(",
            "temp,",
            "+                0.5,",
            "dim=dim,",
            "keepdim=keepdims,",
            ")[0]",
            "-        return input",
            "+        return temp",
            "else:",
            "-        return torch.median(",
            "+        return torch.quantile(",
            "input,",
            "+            0.5,",
            "dim=axis,",
            "keepdim=keepdims,",
            ")[0]"
        ]
    },
    {
        "number": 6119,
        "comments": "",
        "commit_message": "Small fix.\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def in_top_k(predictions, targets, k):",
            "`output[i]` is `True` if `predictions[i, targets[i]]` is within top-`k`",
            "values of `predictions[i]`.",
            "\"\"\"",
            "-    return tf_math_ops.in_top_k(tf.cast(predictions, 'float32'),",
            "-                                tf.cast(targets, 'int32'),",
            "-                                k)",
            "+    return tf.nn.in_top_k(tf.cast(predictions, 'float32'),",
            "+                          tf.cast(targets, 'int32'),",
            "+                          k)",
            "",
            "",
            "# CONVOLUTIONS"
        ]
    },
    {
        "number": 6120,
        "comments": "",
        "commit_message": "fix buffer transfer bug (#2045)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class AGP_Pruner(Pruner):",
            "if epoch > 0:",
            "self.now_epoch = epoch",
            "for wrapper in self.get_modules_wrapper():",
            "-                wrapper.registered_buffers['if_calculated'].copy_(torch.tensor(0)) # pylint: disable=not-callable",
            "+                wrapper.if_calculated.copy_(torch.tensor(0)) # pylint: disable=not-callable",
            "",
            "class SlimPruner(Pruner):",
            "\"\"\""
        ]
    },
    {
        "number": 6121,
        "comments": "",
        "commit_message": "Fix failing CUDA tests on dev (#1277)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def test_welford_dense(n_samples, dim_size):",
            "samples.append(sample)",
            "w.update(sample)",
            "",
            "-    sample_cov = np.cov(torch.stack(samples).data.numpy(), bias=False, rowvar=False)",
            "-    estimates = w.get_covariance(regularize=False).data.numpy()",
            "+    sample_cov = np.cov(torch.stack(samples).data.cpu().numpy(), bias=False, rowvar=False)",
            "+    estimates = w.get_covariance(regularize=False).data.cpu().numpy()",
            "assert_equal(estimates, sample_cov)"
        ]
    },
    {
        "number": 6133,
        "comments": "",
        "commit_message": "fix mean_iou batch2 unit test\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def mean_iou(",
            "# TODO: is it possible to vectorize this ?",
            "# iterate over classes",
            "for class_id in range(num_classes):",
            "-        tp: torch.Tensor = conf_mat[..., class_id, class_id].float()",
            "+        tp: torch.Tensor = conf_mat[..., None, class_id, class_id]",
            "total = torch.sum(conf_mat[..., class_id, :], dim=-1, keepdim=True) + \\",
            "torch.sum(conf_mat[..., :, class_id], dim=-1, keepdim=True)",
            "iou_val: torch.Tensor = tp / (total.float() - tp + 1e-6)"
        ]
    },
    {
        "number": 6135,
        "comments": "",
        "commit_message": "weight_decay fix\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def train():",
            "torch.save(chkpt, wdir + 'backup%g.pt' % epoch)",
            "",
            "# Delete checkpoint",
            "-            del chkpt  # end epoch -------------------------------------------------------------------------------------",
            "+            del chkpt",
            "+",
            "+        # end epoch ----------------------------------------------------------------------------------------------------",
            "",
            "# Report time",
            "plot_results()  # save as results.png"
        ]
    },
    {
        "number": 6144,
        "comments": "",
        "commit_message": "Fix AutoRegNN with default CUDA tensor (#1308)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class AutoRegressiveNNTests(TestCase):",
            "# NOTE: the hidden dimension must be greater than the input_dim for the",
            "# masks to be well-defined!",
            "hidden_dim = input_dim * 5",
            "-                        permutation = torch.randperm(input_dim)",
            "+                        permutation = torch.randperm(input_dim, device='cpu')",
            "self._test_masks(",
            "input_dim,",
            "observed_dim,"
        ]
    },
    {
        "number": 6149,
        "comments": "",
        "commit_message": "fix mypy errors\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def mean_iou(",
            "torch.Tensor: a tensor representing the mean intersection-over union",
            "with shape :math:`(B, K)` where K is the number of classes.",
            "\"\"\"",
            "-    if not torch.is_tensor(input) and input.dtype is not torch.uint64:",
            "+    if not torch.is_tensor(input) and input.dtype is not torch.int64:",
            "raise TypeError(\"Input input type is not a torch.Tensor with \"",
            "\"torch.int64 dtype. Got {}\".format(type(input)))",
            "-    if not torch.is_tensor(target) and target.dtype is not torch.uint64:",
            "+    if not torch.is_tensor(target) and target.dtype is not torch.int64:",
            "raise TypeError(\"Input target type is not a torch.Tensor with \"",
            "\"torch.int64 dtype. Got {}\".format(type(target)))",
            "if not input.shape == target.shape:"
        ]
    },
    {
        "number": 6151,
        "comments": "",
        "commit_message": "bugfix\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test(epoch, loader, string):",
            "for data in loader:",
            "adj, slice = data['adj']['content'], data['adj']['slice'][:, 0]",
            "input, target = data['input'], data['target']",
            "-        input = torch.cat([input, input.new(input.size(0)).fill_(1)], dim=1)",
            "+        input = torch.cat([input, input.new(input.size(0), 1).fill_(1)], dim=1)",
            "num_examples += target.size(0)",
            "",
            "if torch.cuda.is_available():"
        ]
    },
    {
        "number": 6153,
        "comments": "",
        "commit_message": "fix(rllib): allow explore=False with tuple action distributions (#10443)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class StochasticSampling(Exploration):",
            "logp = action_dist.sampled_action_logp()",
            "else:",
            "action = action_dist.deterministic_sample()",
            "-            logp = torch.zeros((action.size()[0], ), dtype=torch.float32)",
            "+            logp = torch.zeros_like(action_dist.sampled_action_logp())",
            "return action, logp"
        ]
    },
    {
        "number": 6162,
        "comments": "",
        "commit_message": "docs: \u270f\ufe0f fix TranslationVariableLanguages example (#4731)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TranslationVariableLanguages:",
            "",
            "```python",
            ">>> # At construction time:",
            "-    >>> datasets.features.Translation(languages=['en', 'fr', 'de'])",
            "+    >>> datasets.features.TranslationVariableLanguages(languages=['en', 'fr', 'de'])",
            ">>> # During data generation:",
            ">>> yield {",
            "...         'en': 'the cat',"
        ]
    },
    {
        "number": 6164,
        "comments": "",
        "commit_message": "Fix memory issue of MCMC for multiple chains (#1956)\n\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "if __name__ == \"__main__\":",
            "help=\"run this example in GPU\")",
            "args = parser.parse_args()",
            "",
            "+    # work around the error \"CUDA error: initialization error\" when arg.cuda is False",
            "+    # see https://github.com/pytorch/pytorch/issues/2517",
            "+    if six.PY3:",
            "+        torch.multiprocessing.set_start_method(\"spawn\")",
            "pyro.set_rng_seed(args.rng_seed)",
            "# Enable validation checks",
            "pyro.enable_validation(__debug__)"
        ]
    },
    {
        "number": 6165,
        "comments": "",
        "commit_message": "upgrade to pytorch 0.4.0 (#1126)\n\n* bump pytorch to 0.4 + fix sanitize\n\n* remove check for Variable in block_orthogonal\n\n* rename parameter split_size=\n\n* remove checks for Variable\n\n* fix more tests\n\n* fixes\n\n* get tests to pass\n\n* fix warnings\n\n* get rid of some of the Variables\n\n* more tests passing\n\n* more elimination of variables\n\n* finish removing all Variables\n\n* pylint and such\n\n* a few fixes\n\n* move torch.no_grad into model.forward_on_instances\n\n* more pytorch 0.4 changes\n\n* detach() -> data\n\n* pylint\n\n* fix bad tensor creation\n\n* fix types\n\n* remove print statement\n\n* more 0.4 goodness\n\n* factor out is_tensor\n\n* add no_grad to elmo command\n\n* cleanup\n\n* remove TODO\n\n* address PR feedback\n\n* replace all() with item()\n\n* more cleanup\n\n* further cleanup\n\n* remove Variable\n\n* really fix merge conflict\n\n* fix pylint\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestIntraSentenceAttentionEncoder(AllenNlpTestCase):",
            "similarity_function=similarity,",
            "num_attention_heads=3,",
            "combination=\"1+2\")",
            "-        input_tensor = Variable(torch.from_numpy(numpy.random.rand(4, 6, 24))).float()",
            "+        input_tensor = torch.from_numpy(numpy.random.rand(4, 6, 24)).float()",
            "encoder_output = encoder(input_tensor, None)",
            "assert list(encoder_output.size()) == [4, 6, 24]"
        ]
    },
    {
        "number": 6167,
        "comments": "",
        "commit_message": "fix Error in calc iou\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class yolov3(object):",
            "# caculate iou between true boxes and pred boxes",
            "intersect_xy1 = tf.maximum(true_box_xy - true_box_wh / 2.0,",
            "pred_box_xy - pred_box_xy / 2.0)",
            "-        intersect_xy2 = tf.maximum(true_box_xy + true_box_wh / 2.0,",
            "+        intersect_xy2 = tf.minimum(true_box_xy + true_box_wh / 2.0,",
            "pred_box_xy + pred_box_wh / 2.0)",
            "intersect_wh = tf.maximum(intersect_xy2 - intersect_xy1, 0.)",
            "intersect_area = intersect_wh[..., 0] * intersect_wh[..., 1]"
        ]
    },
    {
        "number": 6170,
        "comments": "",
        "commit_message": "Fix SEW-D implementation differences (#14191)\n\n* Fix SEW-D\n\n* Update tests\n\n* isort\n",
        "label": "",
        "answer": "no",
        "change": [
            "class SEWDModelIntegrationTest(unittest.TestCase):",
            ")",
            "expected_output_sum = 54201.0469",
            "",
            "-        self.assertTrue(torch.allclose(outputs[:, :4, :4], expected_outputs_first, atol=5e-3))",
            "-        self.assertTrue(torch.allclose(outputs[:, -4:, -4:], expected_outputs_last, atol=5e-3))",
            "-        self.assertTrue(abs(outputs.sum() - expected_output_sum) < 5)",
            "+        self.assertTrue(torch.allclose(outputs[:, :4, :4], expected_outputs_first, atol=1e-3))",
            "+        self.assertTrue(torch.allclose(outputs[:, -4:, -4:], expected_outputs_last, atol=1e-3))",
            "+        self.assertTrue(abs(outputs.sum() - expected_output_sum) < 1)",
            "",
            "def test_inference_ctc_batched(self):",
            "model = SEWDForCTC.from_pretrained(\"asapp/sew-d-tiny-100k-ft-ls100h\").to(torch_device)"
        ]
    },
    {
        "number": 6172,
        "comments": "",
        "commit_message": "[rllib] Fix LSTM regression on truncated sequences and add regression test (#2898)\n\n* fix\n\n* add test\n\n* yapf\n\n* yapf\n\n* fix space\n\n* Oops that should be lstm: True\n\n* Update cartpole_lstm.py\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class LSTM(Model):",
            "self.state_in = [c_in, h_in]",
            "",
            "# Setup LSTM outputs",
            "+        state_in = rnn.LSTMStateTuple(c_in, h_in)",
            "lstm_out, lstm_state = tf.nn.dynamic_rnn(",
            "lstm,",
            "last_layer,",
            "+            initial_state=state_in,",
            "sequence_length=self.seq_lens,",
            "time_major=False,",
            "dtype=tf.float32)"
        ]
    },
    {
        "number": 6174,
        "comments": "",
        "commit_message": "Update lstm_crf_layer.py\n\nfix bug for https://github.com/macanv/BERT-BiLSTM-CRF-NER/issues/159 and https://github.com/macanv/BERT-BiLSTM-CRF-NER/issues/254\n",
        "label": "",
        "answer": "no",
        "change": [
            "class BLSTM_CRF(object):",
            "b = tf.get_variable(\"b\", shape=[self.hidden_unit], dtype=tf.float32,",
            "initializer=tf.zeros_initializer())",
            "output = tf.reshape(lstm_outputs, shape=[-1, self.hidden_unit * 2])",
            "-                hidden = tf.tanh(tf.nn.xw_plus_b(output, W, b))",
            "+                hidden = tf.nn.xw_plus_b(output, W, b)",
            "",
            "# project to score of tags",
            "with tf.variable_scope(\"logits\"):"
        ]
    },
    {
        "number": 6178,
        "comments": "",
        "commit_message": "fix loss\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def loss(net):",
            "confs = tf.mul(best_box, _confs)",
            "",
            "# take care of the weight terms",
            "-    weight_con = snoob*(1.-best_box) + sconf*best_box",
            "+    weight_con = snoob * (1. - confs) + sconf * confs",
            "conid = tf.mul(_conid, weight_con)",
            "-    weight_coo = tf.concat(3, 4 * [tf.expand_dims(best_box, -1)])",
            "+    weight_coo = tf.concat(3, 4 * [tf.expand_dims(confs, -1)])",
            "cooid = tf.mul(_cooid, scoor * weight_coo)",
            "proid = sprob * _proid"
        ]
    },
    {
        "number": 6181,
        "comments": "",
        "commit_message": "default fast model loading \ud83d\udd25 (#1115)\n\n* make accelerate hard dep\n\n* default fast init\n\n* move params to cpu when device map is None\n\n* handle device_map=None\n\n* handle torch < 1.9\n\n* remove device_map=\"auto\"\n\n* style\n\n* add accelerate in torch extra\n\n* remove accelerate from extras[\"test\"]\n\n* raise an error if torch is available but not accelerate\n\n* update installation docs\n\n* Apply suggestions from code review\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* improve defautl loading speed even further, allow disabling fats loading\n\n* address review comments\n\n* adapt the tests\n\n* fix test_stable_diffusion_fast_load\n\n* fix test_read_init\n\n* temp fix for dummy checks\n\n* Trigger Build\n\n* Apply suggestions from code review\n\nCo-authored-by: Anton Lozhkov <anton@huggingface.co>\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\nCo-authored-by: Anton Lozhkov <anton@huggingface.co>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class StableDiffusionImg2ImgPipelineIntegrationTests(unittest.TestCase):",
            "init_image = init_image.resize((768, 512))",
            "",
            "pipe = StableDiffusionImg2ImgPipeline.from_pretrained(",
            "-            \"CompVis/stable-diffusion-v1-4\", revision=\"fp16\", torch_dtype=torch.float16, device_map=\"auto\"",
            "+            \"CompVis/stable-diffusion-v1-4\",",
            "+            revision=\"fp16\",",
            "+            torch_dtype=torch.float16,",
            ")",
            "pipe.to(torch_device)",
            "pipe.set_progress_bar_config(disable=None)"
        ]
    },
    {
        "number": 6183,
        "comments": "",
        "commit_message": "Fix keras 2.1.6 error in parser.\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Keras2Parser(Parser):",
            "# load model files into Keras graph",
            "if isinstance(model, _string_types):",
            "try:",
            "-                from _keras.applications.mobilenet import relu6",
            "-                from _keras.applications.mobilenet import DepthwiseConv2D",
            "+                from keras.applications.mobilenet import relu6",
            "+                from keras.applications.mobilenet import DepthwiseConv2D",
            "model = _keras.models.load_model(",
            "model,",
            "custom_objects={"
        ]
    },
    {
        "number": 6184,
        "comments": "",
        "commit_message": "fix lint and docstring failures\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def nonzero(x: torch.Tensor) -> Tuple[torch.Tensor]:",
            "return torch.nonzero(x, as_tuple=True)",
            "",
            "",
            "-def where(",
            "-    condition: torch.Tensor,",
            "-    x1: torch.Tensor,",
            "-    x2: torch.Tensor",
            "-) -> torch.Tensor:",
            "+def where(condition: torch.Tensor, x1: torch.Tensor, x2: torch.Tensor) -> torch.Tensor:",
            "promoted_type = torch.promote_types(x1.dtype, x2.dtype)",
            "x1 = x1.to(promoted_type)",
            "x2 = x2.to(promoted_type)"
        ]
    },
    {
        "number": 6186,
        "comments": "",
        "commit_message": "breaking: unified complex_network and network, fixed synchronization optimizer problem\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Synchronization(Optimizer):",
            "",
            "last_sync = tf.get_variable(",
            "name='last-sync',",
            "-            dtype=tf.int32,",
            "-            initializer=(-self.sync_frequency),",
            "+            dtype=tf.int64,",
            "+            initializer=tf.constant_initializer(value=(-self.sync_frequency), dtype=tf.int64),",
            "trainable=False",
            ")"
        ]
    },
    {
        "number": 6187,
        "comments": "",
        "commit_message": "fix bn performance\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class SingleCostFeedfreeTrainer(FeedfreeTrainer):",
            "cost_var = self.model.get_cost()",
            "# GATE_NONE faster?",
            "grads = self.config.optimizer.compute_gradients(",
            "-                cost_var, gate_gradients=0)",
            "+                cost_var,",
            "+                gate_gradients=tf.train.Optimizer.GATE_NONE,",
            "+                colocate_gradients_with_ops=False)",
            "add_moving_summary(cost_var)",
            "return cost_var, grads"
        ]
    },
    {
        "number": 6189,
        "comments": "",
        "commit_message": "Fix batch normaliziation (1-x)\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def create_modules(module_defs):",
            ")",
            "if bn:",
            "modules.add_module(f\"batch_norm_{module_i}\",",
            "-                                   nn.BatchNorm2d(filters, momentum=0.9, eps=1e-5))",
            "+                                   nn.BatchNorm2d(filters, momentum=0.1, eps=1e-5))",
            "if module_def[\"activation\"] == \"leaky\":",
            "modules.add_module(f\"leaky_{module_i}\", nn.LeakyReLU(0.1))",
            "if module_def[\"activation\"] == \"mish\":"
        ]
    },
    {
        "number": 6191,
        "comments": "",
        "commit_message": "Module namespace cleanup for v2.0 (#3875)\n\n* Imports cleaning\n\n* Small change\n\n* Remove unused methods\n\n* Small fix\n\n* Additional fix\n\n* Final fix\n\n* Fix benchmark test\n\n* Fix benchmark test #2\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_table_to_pandas(dtype, dummy_value):",
            "features = datasets.Features({\"foo\": datasets.Array2D(dtype=dtype, shape=(2, 2))})",
            "dataset = datasets.Dataset.from_dict({\"foo\": [[[dummy_value] * 2] * 2]}, features=features)",
            "df = dataset._data.to_pandas()",
            "-    assert type(df.foo.dtype) == datasets.features.PandasArrayExtensionDtype",
            "+    assert type(df.foo.dtype) == PandasArrayExtensionDtype",
            "arr = df.foo.to_numpy()",
            "np.testing.assert_equal(arr, np.array([[[dummy_value] * 2] * 2], dtype=np.dtype(dtype)))"
        ]
    },
    {
        "number": 6194,
        "comments": "",
        "commit_message": "fixed indendentation\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def attn(x, scope, n_state, *, past, params, train=False):",
            "",
            "def split_heads(x, last_dim):",
            "with tf.variable_scope('split_heads'):",
            "-        # From [batch, sequence, features] to [batch, heads, sequence, features_per_head]",
            "-        # heads is split out of features!",
            "-        x = mtf.reshape(x, [dim_batch, dim_seq, dim_heads, last_dim], name=\"split_heads_reshape\")",
            "-        x = mtf.transpose(x, [dim_batch, dim_heads, dim_seq, last_dim], name=\"split_heads_transpose\")",
            "+            # From [batch, sequence, features] to [batch, heads, sequence, features_per_head]",
            "+            # heads is split out of features!",
            "+            x = mtf.reshape(x, [dim_batch, dim_seq, dim_heads, last_dim], name=\"split_heads_reshape\")",
            "+            x = mtf.transpose(x, [dim_batch, dim_heads, dim_seq, last_dim], name=\"split_heads_transpose\")",
            "return x",
            "",
            "def merge_heads(x):"
        ]
    },
    {
        "number": 6195,
        "comments": "",
        "commit_message": "Issue #689 fix - documentation refactoring (#691)\n\n* Documentation Refactoring\n\n* tl.visualize warning fixed\n\n* moving mean initializer moved in the arguments\n\n* Refactored Changelog\n\n* Docs Restructured with Subsection Title\n\n* https://github.com/zsdonghao/tensorlayer/ replaced by https://github.com/tensorlayer/tensorlayer/\n\n* Get Involved and Contribution Refactored\n\n* Redirections Fixed\n\n* get_involved added\n\n* Hao added\n\n* TL Layer Doc refactoring\n\n* Update contributing.rst\n\n* Core Team Updated\n\n* super resolution layers moved to convolution folder\n\n* YAPF Correction\n\n* Layer Documentation Refactored\n\n* Core Team Updated\n\n* Adding Travis TF 1.9.0rc0\n\n* Fangde Added to core team\n\n* Contributors page updated\n\n* Decorator Deprecated added\n\n* Changelog Updated\n\n* Update requirements.txt\n\n* Python 2 Bug Fix\n\n* Missing  Import fixed\n\n* YAPF Error Fix\n\n* 1.8.6rc6 released\n\n* Update README.rst\n\n* Setup Warning Fixed\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class BatchNormLayer(Layer):",
            "gamma = None",
            "",
            "# 2.",
            "-            moving_mean_init = tf.zeros_initializer()",
            "",
            "moving_mean = tf.get_variable(",
            "'moving_mean', params_shape, initializer=moving_mean_init, dtype=LayersConfig.tf_dtype, trainable=False"
        ]
    },
    {
        "number": 6196,
        "comments": "",
        "commit_message": "move test case to confusion_matrix_test.py\nfix dtype error in update_state()\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def _update_confusion_matrix_variables_optimized(",
            "y_pred)",
            "if not multi_label:",
            "label_weights = tf.reshape(label_weights, [-1])",
            "-  weights = tf.multiply(sample_weights, label_weights)",
            "+  weights = tf.cast(tf.multiply(sample_weights, label_weights), y_true.dtype)",
            "",
            "# We shouldn't need this, but in case there are predict value that is out of",
            "# the range of [0.0, 1.0]"
        ]
    },
    {
        "number": 6197,
        "comments": "",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Attention(torch.nn.Module, Registrable):",
            "",
            "@overrides",
            "def forward(",
            "-        self, vector: torch.Tensor, matrix: torch.Tensor, matrix_mask: torch.Tensor = None",
            "+        self, vector: torch.Tensor, matrix: torch.Tensor, matrix_mask: torch.BoolTensor = None",
            ") -> torch.Tensor:",
            "similarities = self._forward_internal(vector, matrix)",
            "if self._normalize:"
        ]
    },
    {
        "number": 6200,
        "comments": "",
        "commit_message": "Fix MultiheadAttention and torch hub\n\nSummary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/895\n\nReviewed By: akinh\n\nDifferential Revision: D18246479\n\nPulled By: myleott\n\nfbshipit-source-id: a610f1e4943619d32a523601a572fb09cdc5638d\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class MultiheadAttention(nn.Module):",
            "",
            "for key, value in items_to_add.items():",
            "state_dict[key] = value",
            "-",
            "-        return state_dict"
        ]
    },
    {
        "number": 6201,
        "comments": "",
        "commit_message": "Type promotion fixes (#2620)\n\n* casting fixes\n\n* lint fixes\n\n* changes\n\n* more changes\n\n* lint fixes\n\n* Fixed matrix_Rank\n\n* update matrix_rank return dtype\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def matrix_rank(",
            "x = tf.expand_dims(x, 0)",
            "x, rtol = ivy.promote_types_of_inputs(x, rtol)",
            "ret = tf.linalg.matrix_rank(x, rtol)",
            "-    ret = tf.cast(ret, ivy.default_float_dtype(as_native=True))",
            "+    ret = tf.cast(ret, ivy.default_int_dtype(as_native=True))",
            "return ret"
        ]
    },
    {
        "number": 6202,
        "comments": "",
        "commit_message": "ONNX supervised inpainting (#906)\n\n* ONNX supervised inpainting\n\n* sync with the torch pipeline\n\n* fix concat\n\n* update ref values\n\n* back to 8 steps\n\n* type fix\n\n* make fix-copies\n",
        "label": "",
        "answer": "no",
        "change": [
            "def convert_models(model_path: str, output_path: str, opset: int):",
            "unet_path = output_path / \"unet\" / \"model.onnx\"",
            "onnx_export(",
            "pipeline.unet,",
            "-        model_args=(torch.randn(2, 4, 64, 64), torch.LongTensor([0, 1]), torch.randn(2, 77, 768), False),",
            "+        model_args=(",
            "+            torch.randn(2, pipeline.unet.in_channels, 64, 64),",
            "+            torch.LongTensor([0, 1]),",
            "+            torch.randn(2, 77, 768),",
            "+            False,",
            "+        ),",
            "output_path=unet_path,",
            "ordered_input_names=[\"sample\", \"timestep\", \"encoder_hidden_states\", \"return_dict\"],",
            "output_names=[\"out_sample\"],  # has to be different from \"sample\" for correct tracing"
        ]
    },
    {
        "number": 6206,
        "comments": "",
        "commit_message": "fix tests hopefully\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "from tinygrad.llops.ops_cpu import unary_op, binary_op, reduce_op, movement_op",
            "",
            "from tinygrad.ops import ProcessingOps",
            "",
            "-def processing_op(op,x,w,ret,C):",
            "+def processing_op(ctx,op,x,w,out_shape,C):",
            "assert op == ProcessingOps.CONV, f\"{op} isn't supported\"",
            "-  ret[:] = torch.conv2d(x, w, stride=(C.ys, C.xs), groups=C.groups, dilation=(C.dy, C.dx), padding=(C.py, C.px))",
            "+  return torch.conv2d(x, w, stride=(C.ys, C.xs), groups=C.groups, dilation=(C.dy, C.dx), padding=(C.py, C.px))"
        ]
    },
    {
        "number": 6207,
        "comments": "",
        "commit_message": "Changed memory and multithreaded test, added documentation, fixed memory retrieve_sequences problems\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Dropout(Layer):",
            "return tf.cond(",
            "pred=update,",
            "true_fn=(lambda: tf.nn.dropout(x=x, keep_prob=(1.0 - self.rate))),",
            "-            false_fn=(lambda: x)",
            "+            false_fn=(lambda: tf.identity(input=x))",
            ")"
        ]
    },
    {
        "number": 6208,
        "comments": "",
        "commit_message": "Linter, deprecated type()\n\nSummary: Run linter after recent changes. Fix long comment in knn.h which clang-format has reflowed badly. Add crude test that code doesn't call deprecated `.type()` or `.data()`.\n\nReviewed By: nikhilaravi\n\nDifferential Revision: D20692935\n\nfbshipit-source-id: 28ce0308adae79a870cb41a810b7cf8744f41ab8\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestKNN(unittest.TestCase):",
            "",
            "def test_knn_vs_python_cuda(self):",
            "\"\"\" Test CUDA output vs PyTorch implementation \"\"\"",
            "-        device = torch.device('cuda')",
            "+        device = torch.device(\"cuda\")",
            "Ns = [1, 4]",
            "Ds = [2, 3, 8]",
            "P1s = [1, 8, 64, 128, 1001]"
        ]
    },
    {
        "number": 6212,
        "comments": "",
        "commit_message": "Fix for TensorFlow v0.12 (tf.inv was removed)\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def sparse_dropout(x, keep_prob, noise_shape):",
            "random_tensor += tf.random_uniform(noise_shape)",
            "dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)",
            "pre_out = tf.sparse_retain(x, dropout_mask)",
            "-    return pre_out * tf.inv(keep_prob)",
            "+    return pre_out * (1./keep_prob)",
            "",
            "",
            "def dot(x, y, sparse=False):"
        ]
    },
    {
        "number": 6216,
        "comments": "",
        "commit_message": "[Ray RLlib] Fix tree import (#7662)\n\n* Rollback.\n\n* Fix import tree error by adding meaningful error and replacing by tf.nest wherever possible.\n\n* LINT.\n\n* LINT.\n\n* Fix.\n\n* Fix log-likelihood test case failing on travis.\n",
        "label": "",
        "answer": "no",
        "change": [
            "def do_test_log_likelihood(run,",
            "prev_reward_batch=np.array([prev_r]))",
            "check(np.exp(logp), expected_prob, atol=0.2)",
            "",
            "+        if eager_ctx:",
            "+            eager_ctx.__exit__(None, None, None)",
            "+",
            "",
            "class TestComputeLogLikelihood(unittest.TestCase):",
            "def test_dqn(self):"
        ]
    },
    {
        "number": 6218,
        "comments": "",
        "commit_message": "fix: ignore update net1 in train2\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def train(args, logdir1, logdir2):",
            "# )",
            "",
            "session_inits = []",
            "-    ckpt2 = args.ckpt if args.ckpt else tf.train.latest_checkpoint(logdir2)",
            "+    ckpt2 = '{}/{}'.format(logdir2, args.ckpt) if args.ckpt else tf.train.latest_checkpoint(logdir2)",
            "if ckpt2:",
            "session_inits.append(SaverRestore(ckpt2))",
            "ckpt1 = tf.train.latest_checkpoint(logdir1)"
        ]
    },
    {
        "number": 6219,
        "comments": "",
        "commit_message": "Fixing and error related to Floor Division (#4221)\n\nSummary:\n# Before submitting\n\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\n- [ ] Did you make sure to update the docs?\n- [ ] Did you write any new necessary tests?\n\n## What does this PR do?\nFixes https://github.com/pytorch/fairseq/issues/4058\nWhile using the library the following warnings are shown which sometimes hinder the workflow. The warnings are\n\n`<USER_PATH>/fairseq/search.py:140: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n  beams_buf = indices_buf // vocab_size`\n\n`<USER_PATH>/fairseq/sequence_generator.py:666: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n  unfin_idx = bbsz_idx // beam_size`\n\nThe methodology was simple, instead of using the `//`, it was replaced by `torch.div(arg1, arg2, rounding_mode='trunc')` and the variable alues do not change for both before and after, just the warning is resolved.\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding \ufffd\nYes, I did! Thanks!\n\nPull Request resolved: https://github.com/pytorch/fairseq/pull/4221\n\nReviewed By: arbabu123\n\nDifferential Revision: D34538147\n\nPulled By: alexeib\n\nfbshipit-source-id: 143897a249129a163b6a30ba9b5cf5595ef42330\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class BeamSearch(Search):",
            "scores_buf = top_prediction[0]",
            "indices_buf = top_prediction[1]",
            "# Project back into relative indices and beams",
            "-        beams_buf = indices_buf // vocab_size",
            "+        beams_buf = torch.div(indices_buf, vocab_size, rounding_mode='trunc')",
            "indices_buf = indices_buf.fmod(vocab_size)",
            "",
            "# At this point, beams_buf and indices_buf are single-dim and contain relative indices"
        ]
    },
    {
        "number": 6221,
        "comments": "",
        "commit_message": "Fix bug of multi-gpu training in lm finetuning\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def main():",
            "global_step += 1",
            "",
            "# Save a trained model",
            "-        if args.do_train and ( n_gpu > 1 and torch.distributed.get_rank() == 0  or n_gpu <=1):",
            "+        if args.do_train and (args.local_rank == -1 or torch.distributed.get_rank() == 0):",
            "logger.info(\"** ** * Saving fine - tuned model ** ** * \")",
            "model.save_pretrained(args.output_dir)",
            "tokenizer.save_pretrained(args.output_dir)"
        ]
    },
    {
        "number": 6224,
        "comments": "",
        "commit_message": "Fix clone_model to consider input_tensors\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestModelCloning(keras_parameterized.TestCase):",
            "run_eagerly=testing_utils.should_run_eagerly())",
            "new_model.train_on_batch([val_a, val_b], val_out)",
            "",
            "+    # New model should use provided input tensors",
            "+    self.assertListEqual(new_model.inputs, new_input_tensors)",
            "+",
            "# On top of new, non-Keras tensors",
            "if not tf.executing_eagerly():",
            "# TODO(b/121277734):Skip Eager contexts, as Input() layers raise an error"
        ]
    },
    {
        "number": 6225,
        "comments": "",
        "commit_message": "enable black in the precommit (#1777)\n\n* enable black in the precommit\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add some fixes\n\n* update libface detection url\n\n* added url from kornia checkpoint\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def cart2pol(x: torch.Tensor, y: torch.Tensor, eps: float = 1.0e-8) -> Tuple[tor",
            "if not (isinstance(x, torch.Tensor) & isinstance(y, torch.Tensor)):",
            "raise TypeError(f\"Input type is not a torch.Tensor. Got {type(x)}, {type(y)}\")",
            "",
            "-    rho = torch.sqrt(x ** 2 + y ** 2 + eps)",
            "+    rho = torch.sqrt(x**2 + y**2 + eps)",
            "phi = torch.atan2(y, x)",
            "return rho, phi"
        ]
    },
    {
        "number": 6228,
        "comments": "",
        "commit_message": "rnn->encoders, removes _th suffix, same filenames for chainer and pytorch, modify tests to not rely on _th\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_train_acc():",
            "pytest.importorskip(\"torch\")",
            "import torch",
            "",
            "-    from espnet.nets.pytorch.e2e_asr_th import pad_list",
            "-    from espnet.nets.pytorch.e2e_asr_th import th_accuracy",
            "+    from espnet.nets.pytorch.e2e_asr import pad_list",
            "+    from espnet.nets.pytorch.e2e_asr import th_accuracy",
            "",
            "n_out = 7",
            "_eos = n_out - 1"
        ]
    },
    {
        "number": 6229,
        "comments": "",
        "commit_message": "fixed indententation\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def spdz_mul(x, y, workers, mod=field):",
            "delta = delta.broadcast(workers)",
            "epsilon = epsilon.broadcast(workers)",
            "",
            "-    z = torch.fmod((c",
            "-                    + torch.fmod((delta * b), mod)",
            "-                    + torch.fmod((epsilon * a), mod)",
            "-                    ), mod)",
            "-",
            "+    z = torch.fmod(",
            "+        (c + torch.fmod((delta * b), mod) + torch.fmod((epsilon * a), mod)), mod",
            "+    )",
            "",
            "z.child.public_add_(epsilon_delta)"
        ]
    },
    {
        "number": 6230,
        "comments": "",
        "commit_message": "Fix unused variable found (#1250)\n\n* Fix Unused variable found\n\n* more fixes\n\n* Update kornia/enhance/histogram.py\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestPatchSequential:",
            "input = torch.randn(*shape, device=device, dtype=dtype)",
            "out = seq(input)",
            "if seq.return_label:",
            "-            out, label = out",
            "+            out, _ = out",
            "assert out.shape[-3:] == input.shape[-3:]",
            "",
            "reproducibility_test(input, seq)"
        ]
    },
    {
        "number": 6234,
        "comments": "",
        "commit_message": "Fix log stability in logits.\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Categorical(Distribution):",
            "self.probabilities = tf.nn.softmax(logits=self.logits, dim=-1)",
            "",
            "# \"normalized\" logits",
            "-        self.logits = tf.log(x=self.probabilities)",
            "+        self.logits = tf.log(x=self.probabilities + util.epsilon)",
            "",
            "# General distribution values",
            "self.distribution = (self.logits,)"
        ]
    },
    {
        "number": 6235,
        "comments": "",
        "commit_message": "Fix make fix-copies with type annotations (#13586)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def _compute_mask_indices(",
            "tf.ones_like(spec_aug_mask_idxs), spec_aug_mask_idxs, spec_aug_mask.shape",
            ")",
            "",
            "-    return tf.cast(spec_aug_mask, tf.float32)",
            "+    return spec_aug_mask",
            "",
            "",
            "# Copied from transformers.models.bart.modeling_tf_bart._expand_mask"
        ]
    },
    {
        "number": 6241,
        "comments": "",
        "commit_message": "fix image summary\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def get_config():",
            "optimizer=tf.train.AdamOptimizer(lr, beta1=0.5, epsilon=1e-3),",
            "callbacks=Callbacks([",
            "StatPrinter(), ModelSaver(),",
            "-            ScheduledHyperParamSetter('learning_rate', [(200, 1e-4)])",
            "]),",
            "session_config=get_default_sess_config(0.5),",
            "model=Model(),",
            "step_per_epoch=300,",
            "-        max_epoch=300,",
            "+        max_epoch=200,",
            ")",
            "",
            "def sample(model_path):"
        ]
    },
    {
        "number": 6242,
        "comments": "",
        "commit_message": "fix string (#18568)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Pipeline(_ScikitCompat):",
            "elif device < 0:",
            "self.device = torch.device(\"cpu\")",
            "else:",
            "-                self.device = torch.device(\"cuda:{device}\")",
            "+                self.device = torch.device(f\"cuda:{device}\")",
            "else:",
            "self.device = device",
            "self.binary_output = binary_output"
        ]
    },
    {
        "number": 6245,
        "comments": "",
        "commit_message": "Improved non-latin `Annotator()` plotting (#7488)\n\n* Improved non-latin labels Annotator plotting\n\nMay resolve https://github.com/ultralytics/yolov5/issues/7460\n\n* Update train.py\n\n* Update train.py\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add progress arg\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def check_file(file, suffix=''):",
            "return files[0]  # return file",
            "",
            "",
            "-def check_font(font=FONT):",
            "+def check_font(font=FONT, progress=False):",
            "# Download font to CONFIG_DIR if necessary",
            "font = Path(font)",
            "if not font.exists() and not (CONFIG_DIR / font.name).exists():",
            "url = \"https://ultralytics.com/assets/\" + font.name",
            "LOGGER.info(f'Downloading {url} to {CONFIG_DIR / font.name}...')",
            "-        torch.hub.download_url_to_file(url, str(font), progress=False)",
            "+        torch.hub.download_url_to_file(url, str(font), progress=progress)",
            "",
            "",
            "def check_dataset(data, autodownload=True):"
        ]
    },
    {
        "number": 6247,
        "comments": "",
        "commit_message": "bug fix on resnet\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Model(ModelDesc):",
            "# weight decay on all W of fc layers",
            "wd_w = tf.train.exponential_decay(1e-4, get_global_step_var(),",
            "200000, 0.7, True)",
            "-        wd_w = wd_w / tf.get_default_graph().get_tensor_by_name('learning_rate')",
            "wd_cost = tf.mul(wd_w, regularize_cost('.*/W', tf.nn.l2_loss), name='l2_regularize_loss')",
            "add_moving_summary(loss, wd_cost)"
        ]
    },
    {
        "number": 6248,
        "comments": "",
        "commit_message": "Identified/Fixed Several Bugs in Keras Worker (#87)\n\n* revamped keras interface into an initial client\n\n* updated notebook\n\n* added initial torch service\n\n* initial service for torch\n\n* initial service for torch\n\n* improved logging a bit\n\n* typo\n\n* typo\n\n* missing numpy import\n\n* another bugfix in fit\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "\"execution_count\": 127,",
            "\"metadata\": {},",
            "\"outputs\": [],",
            "-   \"source\": [",
            "-    \"c = a.add(b).get()\"",
            "-   ]",
            "+   \"source\": []",
            "},",
            "{",
            "\"cell_type\": \"code\","
        ]
    },
    {
        "number": 6250,
        "comments": "",
        "commit_message": "fix feature maps in Nest, thanks to @MarkYangjiayi\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class NesT(nn.Module):",
            "Aggregate(dim_in, dim_out) if not is_last else nn.Identity()",
            "]))",
            "",
            "+",
            "self.mlp_head = nn.Sequential(",
            "-            LayerNorm(dim),",
            "+            LayerNorm(last_dim),",
            "Reduce('b c h w -> b c', 'mean'),",
            "-            nn.Linear(dim, num_classes)",
            "+            nn.Linear(last_dim, num_classes)",
            ")",
            "",
            "def forward(self, img):"
        ]
    },
    {
        "number": 6251,
        "comments": "",
        "commit_message": "Fixed inconsistent action placeholder (#5213)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class MARWILPolicy(MARWILPostprocessing, TFPolicy):",
            "self.output_actions = action_dist.sample()",
            "",
            "# Training inputs",
            "-        self.act_t = tf.placeholder(tf.int32, [None], name=\"action\")",
            "+        self.act_t = ModelCatalog.get_action_placeholder(action_space)",
            "self.cum_rew_t = tf.placeholder(tf.float32, [None], name=\"reward\")",
            "",
            "# v network evaluation"
        ]
    },
    {
        "number": 6257,
        "comments": "",
        "commit_message": "Bugfixes (#1164)\n\nSummary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1164\n\nReviewed By: ngoyal2707\n\nDifferential Revision: D21373232\n\nPulled By: myleott\n\nfbshipit-source-id: f31c65c6f2ebd9a603099e0cbe9e32c47585f50d\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class DummyMaskedLMTask(FairseqTask):",
            "'id': 1,",
            "'net_input': {",
            "'src_tokens': torch.stack([self.dummy_src for _ in range(bsz)]),",
            "-                    'src_lengths': torch.full((bsz, ), self.args.tokens_per_sample),",
            "+                    'src_lengths': torch.full(",
            "+                        (bsz, ), self.args.tokens_per_sample, dtype=torch.long",
            "+                    ),",
            "},",
            "'target': torch.stack([self.dummy_tgt for _ in range(bsz)]),",
            "'nsentences': bsz,"
        ]
    },
    {
        "number": 6260,
        "comments": "",
        "commit_message": "Misc fixes (#2448)\n\nSummary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/2448\n\nReviewed By: ngoyal2707\n\nDifferential Revision: D23011193\n\nPulled By: myleott\n\nfbshipit-source-id: 1a29481707108e4465aca78ec1581fb79f05efba\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class NanDetector:",
            "def _detect(self, tensor, name, backward):",
            "err = None",
            "if (",
            "-            tensor.numel() >= 2",
            "-        ):  # single value tensors (like the loss) will not provide much info",
            "+            torch.is_floating_point(tensor)",
            "+            # single value tensors (like the loss) will not provide much info",
            "+            and tensor.numel() >= 2",
            "+        ):",
            "with torch.no_grad():",
            "if torch.isnan(tensor).any():",
            "err = \"NaN\""
        ]
    },
    {
        "number": 6262,
        "comments": "",
        "commit_message": "fix flow direction for meta example\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class MetaLayer(torch.nn.Module):",
            "# u: [B, F_u]",
            "# batch: [N] with max entry B - 1.",
            "row, col = edge_index",
            "-                out = torch.cat([x[col], edge_attr], dim=1)",
            "+                out = torch.cat([x[row], edge_attr], dim=1)",
            "out = self.node_mlp_1(out)",
            "-                out = scatter_mean(out, row, dim=0, dim_size=x.size(0))",
            "+                out = scatter_mean(out, col, dim=0, dim_size=x.size(0))",
            "out = torch.cat([x, out, u[batch]], dim=1)",
            "return self.node_mlp_2(out)"
        ]
    },
    {
        "number": 6269,
        "comments": "",
        "commit_message": "fix: hot fix of tf_model & removal of unused code\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TFModel(metaclass=TfModelMeta):",
            "print('\\n:: Model saved to {} \\n'.format(self._model_path.as_posix()))",
            "",
            "def get_checkpoint_state(self):",
            "-        return tf.train.get_checkpoint_state(self._model_path.as_posix())",
            "+        return tf.train.get_checkpoint_state(self._model_path.parent)",
            "",
            "def load(self):",
            "\"\"\""
        ]
    },
    {
        "number": 6270,
        "comments": "",
        "commit_message": "Fix all-zero duration case for GlowTTS\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class GlowTTS(BaseTTS):",
            "o_mean, o_log_scale, o_dur_log, x_mask = self.encoder(x, x_lengths, g=g)",
            "# compute output durations",
            "w = (torch.exp(o_dur_log) - 1) * x_mask * self.length_scale",
            "-        w_ceil = torch.ceil(w)",
            "+        w_ceil = torch.clamp_min(torch.ceil(w), 1)",
            "y_lengths = torch.clamp_min(torch.sum(w_ceil, [1, 2]), 1).long()",
            "y_max_length = None",
            "# compute masks"
        ]
    },
    {
        "number": 6272,
        "comments": "",
        "commit_message": "[torch] remove deprecated uint8 in favor of bool (#21384)\n\n* uint8 -> bool\n\n* fix copies\n\n* style\n\n* update test modeling commen when checking attention buffers\n\n* style\n\n* use logical not on random mask instead of subtraction with 1\n\n* remove torch uint8\n\n* quality\n\n* remove modified modeling utils\n\n* Update based on review\n\nCo-authored-by: sgugger <sylvain.gugger@gmail.com>\n\n---------\n\nCo-authored-by: sgugger <sylvain.gugger@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TransfoXLModel(TransfoXLPreTrainedModel):",
            "mask_shift_len = qlen",
            "dec_attn_mask = (torch.triu(all_ones, 1 + mlen) + torch.tril(all_ones, -mask_shift_len))[:, :, None]  # -1",
            "else:",
            "-            dec_attn_mask = torch.triu(word_emb.new_ones((qlen, klen), dtype=torch.uint8), diagonal=1 + mlen)[",
            "+            dec_attn_mask = torch.triu(word_emb.new_ones((qlen, klen), dtype=torch.bool), diagonal=1 + mlen)[",
            ":, :, None",
            "]"
        ]
    },
    {
        "number": 6274,
        "comments": "",
        "commit_message": "quickfix batch shape\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Categorical(Distribution):",
            "def log_prob(self, dist, actions):",
            "prob = dist['policy_output']",
            "",
            "-        return tf.log(tf.reduce_sum(tf.multiply(prob, actions), [1]) + self.epsilon)",
            "+        return tf.log(tf.reduce_sum(tf.multiply(prob, actions), [2]) + self.epsilon)",
            "",
            "def fixed_kl(self, dist):",
            "\"\"\""
        ]
    },
    {
        "number": 6277,
        "comments": "",
        "commit_message": "fix for issue #209\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def mu_law_encode(audio, quantization_channels):",
            "with tf.name_scope('encode'):",
            "mu = quantization_channels - 1",
            "# Perform mu-law companding transformation (ITU-T, 1988).",
            "-        magnitude = tf.log(1 + mu * tf.abs(audio)) / tf.log(1. + mu)",
            "+        # Minimum operation is here to deal with rare large amplitudes caused by resampling.",
            "+        magnitude = tf.log(1 + mu * tf.minimum(tf.abs(audio), 1.0)) / tf.log(1. + mu)",
            "signal = tf.sign(audio) * magnitude",
            "# Quantize signal to the specified number of levels.",
            "return tf.cast((signal + 1) / 2 * mu + 0.5, tf.int32)"
        ]
    },
    {
        "number": 6282,
        "comments": "",
        "commit_message": "fix a bug in tutorial 1\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Model():",
            "# convert inputs to tensor if it is originally not",
            "if isinstance(inputs, list):",
            "for idx in range(len(inputs)):",
            "-                if isinstance(inputs[idx], np.ndarray):",
            "-                    inputs[idx] = tf.convert_to_tensor(inputs[idx])",
            "+                inputs[idx] = tf.convert_to_tensor(inputs[idx])",
            "elif isinstance(inputs, np.ndarray):",
            "inputs = tf.convert_to_tensor(inputs)"
        ]
    },
    {
        "number": 6288,
        "comments": "",
        "commit_message": "fixed\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def decode(args):",
            "def _convert_att_ws_to_durations(att_ws):",
            "# get the most diagonal attention among the all of the sournce attentions for transformer",
            "if len(att_ws.shape) == 4:",
            "-            att_ws = torch.cat([att_w for att_w in att_ws], axis=0)  # (#heads * #layers, L, T)",
            "+            att_ws = torch.cat([att_w for att_w in att_ws], dim=0)  # (#heads * #layers, L, T)",
            "diagonal_scores = att_ws.max(dim=-1)[0].mean(dim=-1).mean(dim=0)  # (#heads * #layers,)",
            "diagonal_head_idx = diagonal_scores.argmax()",
            "att_ws = att_ws[diagonal_head_idx]  # (L, T)"
        ]
    },
    {
        "number": 6294,
        "comments": "",
        "commit_message": "Fix torch bfloat import version (#9089)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class AMPTestModel(BoringModel):",
            "16,",
            "pytest.param(",
            "\"bf16\",",
            "-            marks=pytest.mark.skipif(not _TORCH_GREATER_EQUAL_1_10, reason=\"torch.bfloat16 not available\"),",
            "+            marks=pytest.mark.skipif(not _TORCH_BFLOAT_AVAILABLE, reason=\"torch.bfloat16 not available\"),",
            "),",
            "],",
            ")"
        ]
    },
    {
        "number": 6306,
        "comments": "",
        "commit_message": "quick fix for dropout_node when training=False (#5513)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def dropout_node(edge_index: Tensor, p: float = 0.5,",
            "num_nodes = maybe_num_nodes(edge_index, num_nodes)",
            "",
            "if not training or p == 0.0:",
            "-        node_mask = edge_index.new_zeros(num_nodes, dtype=torch.bool)",
            "-        edge_mask = edge_index.new_zeros(edge_index.size(1), dtype=torch.bool)",
            "+        node_mask = edge_index.new_ones(num_nodes, dtype=torch.bool)",
            "+        edge_mask = edge_index.new_ones(edge_index.size(1), dtype=torch.bool)",
            "return edge_index, edge_mask, node_mask",
            "",
            "prob = torch.rand(num_nodes, device=edge_index.device)"
        ]
    },
    {
        "number": 6309,
        "comments": "",
        "commit_message": "Fix typo in docstring for `DenseFeatures`\n",
        "label": "",
        "answer": "no",
        "change": [
            "class DenseFeatures(dense_features.DenseFeatures):",
            "```python",
            "price = tf.feature_column.numeric_column('price')",
            "keywords_embedded = tf.feature_column.embedding_column(",
            "-      tf.feature_column.categorical_column_with_hash_bucket(\"keywords\", 10K),",
            "+      tf.feature_column.categorical_column_with_hash_bucket(\"keywords\", 10000),",
            "dimensions=16)",
            "columns = [price, keywords_embedded, ...]",
            "feature_layer = tf.keras.layers.DenseFeatures(columns)"
        ]
    },
    {
        "number": 6310,
        "comments": "",
        "commit_message": "Fix `ViTMSNForImageClassification` doctest (#19275)\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class ViTMSNForImageClassification(ViTMSNPreTrainedModel):",
            ">>> from PIL import Image",
            ">>> import requests",
            "",
            "-        >>> torch.manual_seed(2)",
            "+        >>> torch.manual_seed(2)  # doctest: +IGNORE_RESULT",
            "",
            ">>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"",
            ">>> image = Image.open(requests.get(url, stream=True).raw)"
        ]
    },
    {
        "number": 6311,
        "comments": "",
        "commit_message": "fixing loss (#1420)\n\nCo-authored-by: holly1238 <77758406+holly1238@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def optimize_model():",
            "expected_state_action_values = (next_state_values * GAMMA) + reward_batch",
            "",
            "# Compute Huber loss",
            "-    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))",
            "+    criterion = nn.SmoothL1Loss()",
            "+    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))",
            "",
            "# Optimize the model",
            "optimizer.zero_grad()"
        ]
    },
    {
        "number": 6315,
        "comments": "",
        "commit_message": "Fix typos\n",
        "label": "",
        "answer": "no",
        "change": [
            "def MusicTaggerCRNN(weights='msd', input_tensor=None,",
            "else:",
            "# Load weights",
            "if K.image_dim_ordering() == 'tf':",
            "-            raise RuntimeError(\"Please set image_dim_ordering == 'th'.\"",
            "-                               \"You can set it at ~/.keras/keras.json\")",
            "+            raise RuntimeError('Please set `image_dim_ordering` to \"th\".'",
            "+                               'You can set it at `~/.keras/keras.json`.')",
            "",
            "if K._BACKEND == 'theano':",
            "weights_path = get_file('music_tagger_crnn_weights_theano.h5',"
        ]
    },
    {
        "number": 6316,
        "comments": "",
        "commit_message": "fix torch.nonzero usage in pytorch 1.5 (#2602)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class GARPNHead(GuidedAnchorHead):",
            "if cfg.min_bbox_size > 0:",
            "w = proposals[:, 2] - proposals[:, 0]",
            "h = proposals[:, 3] - proposals[:, 1]",
            "-                valid_inds = torch.nonzero((w >= cfg.min_bbox_size) &",
            "-                                           (h >= cfg.min_bbox_size)).squeeze()",
            "+                valid_inds = torch.nonzero(",
            "+                    (w >= cfg.min_bbox_size) & (h >= cfg.min_bbox_size),",
            "+                    as_tuple=False).squeeze()",
            "proposals = proposals[valid_inds, :]",
            "scores = scores[valid_inds]",
            "proposals = torch.cat([proposals, scores.unsqueeze(-1)], dim=-1)"
        ]
    },
    {
        "number": 6321,
        "comments": "",
        "commit_message": "YOLACT (#3456)\n\n* YOLACT (not the final commit, so please don't review)\n\n* Implementation notes for YOLACT\n\n* Modify yolact_r50.py\n\n* Update YOLACT configs\n\n* Add YOLACT README\n\n* Adjust according to the first round of review\n\n* Handle the corner case when gt_mask is empty\n\n* Add unit tests for YOLACT heads\n\n* Add unit tests for YOLACT forward\n\n* Adjust according to Kai's comments, except for gt_mask filtering\n\n* Change use_OHEM to use_ohem in unit tests\n\n* Update config in YOLACT unit tests\n\n* merge bbox overlap and batch bbox overlap\n\n* fix unit test\n\n* refactor some codes and fix some bugs\n\n* fix isort\n\n* fix ci\n\n* update model link\n\n* fix cfg path\n\n* support batch test\n\n* Make get_seg_masks faster\n\n* fix format\n\n* update readme\n\nCo-authored-by: Cao Yuhang <yhcao6@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_max_iou_assigner_with_empty_gt():",
            "[5, 5, 15, 15],",
            "[32, 32, 38, 42],",
            "])",
            "-    gt_bboxes = torch.FloatTensor([])",
            "+    gt_bboxes = torch.empty(0, 4)",
            "assign_result = self.assign(bboxes, gt_bboxes)",
            "",
            "expected_gt_inds = torch.LongTensor([0, 0, 0, 0])"
        ]
    },
    {
        "number": 6323,
        "comments": "",
        "commit_message": "Changes I need for the TransformerQA model (#3646)\n\n* Adds a vocab class for transformer vocabularies\n\n* Tix fypo\n\n* Remove special token stuff from vocab\n\n* Include type ids in the output if we have them\n\n* Use correct padding value even if the padding token id isn't 0\n\n* The tokenizer knows about special tokens\n\n* Typo\n\n* Don't complain about too many tensors if we are going to use those tensors\n\n* Support token type ids in the pretrained transformer wrapper\n\n* Adds the threaded generator\n\n* Fixed type annotations for token\n\n* Fix problem introduced with the merge\n\n* Productivity through formatting\n\n* Removes test we no longer need\n\n* Productivity through formatting\n\n* Better documentation\n\n* Add helpful documentation\n\n* Remove unused import\n\n* Fix tests\n\n* Fix another test\n\n* Make mypy happy\n\n* Remove the special token functions\n\nNow that `tokenizer` is public, we don't need them like this.\n\n* Fix flake8\n\n* Fixes the treatment of type_ids in the mismatched embedder and indexer\n\n* Productivity through formatting\n\n* Don't save the padding value anymore\n\n* Adds test for different padding values\n\n* Productivity through formatting\n\n* Brendan doesn't like threaded_generator\n\n* Better comments\n\n* Remove unused imports\n\n* Provide a clear error message when your type ids are too big\n\n* Adds code coverage test\n\n* Mark flaky test as flaky\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def get_text_field_mask(",
            "",
            "TODO(joelgrus): can we change this?",
            "NOTE: Our functions for generating masks create torch.LongTensors, because using",
            "-    torch.ByteTensors  makes it easy to run into overflow errors",
            "+    torch.ByteTensors makes it easy to run into overflow errors",
            "when doing mask manipulation, such as summing to get the lengths of sequences - see below.",
            ">>> mask = torch.ones([260]).byte()",
            ">>> mask.sum() # equals 260."
        ]
    },
    {
        "number": 6327,
        "comments": "",
        "commit_message": "Transformers fast import part 2 (#9446)\n\n* Main init work\n\n* Add version\n\n* Change from absolute to relative imports\n\n* Fix imports\n\n* One more typo\n\n* More typos\n\n* Styling\n\n* Make quality script pass\n\n* Add necessary replace in template\n\n* Fix typos\n\n* Spaces are ignored in replace for some reason\n\n* Forgot one models.\n\n* Fixes for import\n\nCo-authored-by: LysandreJik <lysandre.debut@reseau.eseo.fr>\n\n* Add documentation\n\n* Styling\n\nCo-authored-by: LysandreJik <lysandre.debut@reseau.eseo.fr>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class DownloadCommand(BaseTransformersCLICommand):",
            "self._force = force",
            "",
            "def run(self):",
            "-        from transformers import AutoModel, AutoTokenizer",
            "+        from ..models.auto import AutoModel, AutoTokenizer",
            "",
            "AutoModel.from_pretrained(self._model, cache_dir=self._cache, force_download=self._force)",
            "AutoTokenizer.from_pretrained(self._model, cache_dir=self._cache, force_download=self._force)"
        ]
    },
    {
        "number": 6328,
        "comments": "",
        "commit_message": "fix problem of dataparallel\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def train(args):",
            "elif ngpu > 1:",
            "gpu_id = range(ngpu)",
            "logging.info('gpu id: ' + str(gpu_id))",
            "-        model = torch.nn.DataParallel(model, device_ids=gpu_id)",
            "+        model = DataParallel(model, device_ids=gpu_id)",
            "model.cuda()",
            "logging.info('batch size is automatically increased (%d -> %d)' % (",
            "args.batch_size, args.batch_size * args.ngpu))"
        ]
    },
    {
        "number": 6333,
        "comments": "",
        "commit_message": "CI: yesqa (#8564)\n\n* add yesqa\n* fix flake8\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class PyTorchProfiler(BaseProfiler):",
            "# close profiler if it is already opened. might happen if 2 profilers",
            "# are created and the first one did not call `describe`",
            "try:",
            "-                torch.autograd._disable_profiler()  # noqa",
            "+                torch.autograd._disable_profiler()",
            "except (AttributeError, RuntimeError):",
            "pass"
        ]
    },
    {
        "number": 6341,
        "comments": "",
        "commit_message": "[WIP] - Documentation Cleaning and Coding Style (#516)\n\n* Activation Cleaning Docstring Test\n\n* Requirements Pinned with range to insure tested versions are used. Range are used to prevent updating requirements all the time.\n\n* setup.cfg file added with PEP8 configuration\n\n* activation.py refactored\n\n* docstring fixed - ready for documentation unittest\n\n* Yapf correction for max_line_length: 120\n\n* test yapf refactored\n\n* Requirements conflict solved\n\n* Yapf Style modified and merged in file \"setup.cfg\"\n\n* Yapf Confiuguration Updated\n\n* Code Refactored with new YAPF formating style\n\n* Code Refactored with new YAPF formating style\n\n* Code Refactored with new YAPF formating style\n\n* tl.layers.pooling YAPF reformat\n\n* yapf updated\n\n* gitignore updated\n\n* YAPF Style Fixing Attempt\n\n* Space Error Fix\n\n* Style Correction\n\n* Assertion Codacy Errors Corrected\n\n* Error Fix\n\n* Assertion Refactored\n\n* YAPF Style Applied to Master\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class PadLayer(Layer):",
            "self.inputs = prev_layer.outputs",
            "",
            "if padding is None:",
            "-            raise Exception(\"padding should be a Tensor of type int32. see https://www.tensorflow.org/api_docs/python/tf/pad\")",
            "+            raise Exception(",
            "+                \"padding should be a Tensor of type int32. see https://www.tensorflow.org/api_docs/python/tf/pad\"",
            "+            )",
            "",
            "self.outputs = tf.pad(self.inputs, paddings=padding, mode=mode, name=name)",
            "self.all_layers.append(self.outputs)"
        ]
    },
    {
        "number": 6347,
        "comments": "",
        "commit_message": "Fix FP16 and attention masks in FunnelTransformer (#7374)\n\n* Fix #7371\n\n* Fix training\n\n* Fix test values\n\n* Apply the fix to TF as well\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TFFunnelRelMultiheadAttention(tf.keras.layers.Layer):",
            "attn_score = tf.cast(attn_score, tf.float32)",
            "# perform masking",
            "if attention_mask is not None:",
            "-            attn_score = attn_score - INF * tf.cast(attention_mask[:, None, None], tf.float32)",
            "+            attn_score = attn_score - INF * (1 - tf.cast(attention_mask[:, None, None], tf.float32))",
            "# attention probability",
            "attn_prob = tf.nn.softmax(attn_score, axis=-1)",
            "if dtype != tf.float32:"
        ]
    },
    {
        "number": 6348,
        "comments": "",
        "commit_message": "Fix fail test (#1896)\n\n* fix OOM\n\n* Union is not supported still\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestRandomEqualizeAlternative(CommonTests):",
            "",
            "with pytest.raises(ValueError):",
            "self._create_augmentation_from_params(p=1.0)(",
            "-                torch.ones((1, 3, 4, 5) * 200, device=self.device, dtype=self.dtype)",
            "+                torch.ones((1, 3, 4, 5) * 3, device=self.device, dtype=self.dtype)",
            ")"
        ]
    },
    {
        "number": 6351,
        "comments": "",
        "commit_message": "unittest bug fix\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Layer_Convolution_1D_Test(CustomTestCase):",
            "cls.input_layer = Input(cls.inputs_shape, name='input_layer')",
            "",
            "cls.dense = tl.layers.Dense(",
            "-            n_units=100, act=tf.nn.relu, in_channels=200, name='dense'",
            "+            n_units=100, act=tf.nn.relu, in_channels=200",
            ")(cls.input_layer)",
            "",
            "cls.noiselayer = tl.layers.GaussianNoise("
        ]
    },
    {
        "number": 6352,
        "comments": "",
        "commit_message": "fix torch.nonzero usage in pytorch 1.5 (#2602)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class InstanceBalancedPosSampler(RandomSampler):",
            "num_per_gt = int(round(num_expected / float(num_gts)) + 1)",
            "sampled_inds = []",
            "for i in unique_gt_inds:",
            "-                inds = torch.nonzero(assign_result.gt_inds == i.item())",
            "+                inds = torch.nonzero(",
            "+                    assign_result.gt_inds == i.item(), as_tuple=False)",
            "if inds.numel() != 0:",
            "inds = inds.squeeze(1)",
            "else:"
        ]
    },
    {
        "number": 6355,
        "comments": "",
        "commit_message": "pylint -> flake8 (#3288)\n\n* pylint\n\n* update pylint\n\n* undo a lot of the raise / else\n\n* add bound on typed-ast\n\n* first mypy fixes\n\n* new flag\n\n* fix mypy errors\n\n* requirements.txt\n\n* pylint -> flake8\n\n* mypy 0.720 -> mypy 0.730\n\n* add back erroneously removed initial newline\n\n* remove .pylintrc\n\n* remove pylintrc from Dockerfile\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class BagOfEmbeddingsEncoder(Seq2VecEncoder):",
            "def get_output_dim(self) -> int:",
            "return self._embedding_dim",
            "",
            "-    def forward(self, tokens: torch.Tensor, mask: torch.Tensor = None):  #pylint: disable=arguments-differ",
            "+    def forward(self, tokens: torch.Tensor, mask: torch.Tensor = None):",
            "if mask is not None:",
            "tokens = tokens * mask.unsqueeze(-1).float()"
        ]
    },
    {
        "number": 6356,
        "comments": "",
        "commit_message": "small fix\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "with tf.Graph().as_default() as G:",
            "init = sessinit.ParamRestore(np.load(args.model).item())",
            "else:",
            "init = sessinit.SaverRestore(args.model)",
            "-    sess = tf.Session()",
            "+    sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))",
            "init.init(sess)",
            "",
            "# dump ..."
        ]
    },
    {
        "number": 6363,
        "comments": "",
        "commit_message": "Karras VE, DDIM and DDPM flax schedulers (#508)\n\n* beta never changes removed from state\n\n* fix typos in docs\n\n* removed unused var\n\n* initial ddim flax scheduler\n\n* import\n\n* added dummy objects\n\n* fix style\n\n* fix typo\n\n* docs\n\n* fix typo in comment\n\n* set return type\n\n* added flax ddom\n\n* fix style\n\n* remake\n\n* pass PRNG key as argument and split before use\n\n* fix doc string\n\n* use config\n\n* added flax Karras VE scheduler\n\n* make style\n\n* fix dummy\n\n* fix ndarray type annotation\n\n* replace returns a new state\n\n* added lms_discrete scheduler\n\n* use self.config\n\n* add_noise needs state\n\n* use config\n\n* use config\n\n* docstring\n\n* added flax score sde ve\n\n* fix imports\n\n* fix typos\n",
        "label": "",
        "answer": "no",
        "change": [
            "class KarrasVeScheduler(SchedulerMixin, ConfigMixin):",
            "",
            "TODO Args:",
            "\"\"\"",
            "-        if self.s_min <= sigma <= self.s_max:",
            "-            gamma = min(self.s_churn / self.num_inference_steps, 2**0.5 - 1)",
            "+        if self.config.s_min <= sigma <= self.config.s_max:",
            "+            gamma = min(self.config.s_churn / self.num_inference_steps, 2**0.5 - 1)",
            "else:",
            "gamma = 0",
            "",
            "# sample eps ~ N(0, S_noise^2 * I)",
            "-        eps = self.s_noise * torch.randn(sample.shape, generator=generator).to(sample.device)",
            "+        eps = self.config.s_noise * torch.randn(sample.shape, generator=generator).to(sample.device)",
            "sigma_hat = sigma + gamma * sigma",
            "sample_hat = sample + ((sigma_hat**2 - sigma**2) ** 0.5 * eps)"
        ]
    },
    {
        "number": 6365,
        "comments": "",
        "commit_message": "Stability improvements (#99)\n\n* bugfix & torch fx implementation\n\n* update torch tensorrt installation\n\n* fix sparseml colab\n\n* fix sparseml training on gpu\n\n* restored lost fixes\n\n* fixed torch_tensorrt install\n\n* fix quantization\n\n* improved tests & bugfix\n\n* prevent deepsparse installation for arm cpu\n\n* bugfix tvm & improved tests\n\n* added tests for tensorflow and onnx\n\n* fix test torchscript\n\n* fix tensorrt static quant\n\n* update notebooks\n\n* fix deepsparse bugs and implemented tests\n\n* add test for sparseml compressor\n\n* bug fixes on tensorflow backend and added tests\n\n* add version limit to tensorflow due to protobuffers 2.x not being supported from tf2onnx\n\n* update onnx version\n\n* remove numpy update\n\n* restored cpu tests\n\n* add python 3.10 in cpu tests\n\n* limit tensorflow gpu usage\n\n* fix python 3.10\n\n* improved tests\n\n* add warmup in model latency computation & add original model latency\n\n* fix pytorch tensorrt for transformers models\n\n* fixed bugs on onnx model handling\n\n* added onnx simplifier to fix tensorrt in onnx pipeline\n\n* fix deepsparse support to NO_COMPILER_INSTALLATION flag\n\n* fix model to onnx conversion problem and tensorrt issue with static quantization\n\n* add valerio citation code\n\n* added readme to notebooks folder\n\n* added tensorflow and onnx notebooks\n\n* style fix\n\n* fix tensor RT bug with static quantization when using new version of polygraphy and update pytorch resnet50 notebook\n\n* fix huggingface bug when passing tokenizer to optimize_model\n\n* updated notebooks readme and bugfix\n\n* minor fixes & added pruning with intel neural compressor\n\n* fixes and added test for intel pruning\n\n* fixes & added neural compressor quantization\n\n* fix test intel pruning compressor\n\n* added tests for neural compressor optimizer & bug fixes\n\n* removed transformers from requirements\n\n* changed openvino dynamic shape\n\n* computing latency using different data\n\n* bugfix openvino\n\n* make onnxsim optional\n\n* install onnx_sim only on intel machines\n\n* added bf16 and dynamic_quantization to neural_compressor\n\n* check output of compiled models\n\n* internal fixes\n\n* fix when no optimized model is found\n\nCo-authored-by: Valerio Sofi <v.sofi@nebuly.ai>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def create_model_inputs_tf(",
            "def run_tf_model(",
            "model: tf.Module, input_tensors: Tuple[tf.Tensor]",
            ") -> Tuple[tf.Tensor]:",
            "-    pred = model.predict(*input_tensors)",
            "+    pred = model.predict(input_tensors)",
            "if isinstance(pred, tf.Module) and pred is not None:",
            "pred = (pred,)",
            "return pred"
        ]
    },
    {
        "number": 6366,
        "comments": "",
        "commit_message": "Fix dropout\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Decoder(torch.nn.Module):",
            "self.decoder = torch.nn.ModuleList()",
            "self.dropout_dec = torch.nn.ModuleList()",
            "self.decoder += [torch.nn.LSTMCell(dunits + eprojs, dunits)]",
            "+        self.dropout_dec += [torch.nn.Dropout(p=dropout)]",
            "for _ in six.moves.range(1, self.dlayers):",
            "self.decoder += [torch.nn.LSTMCell(dunits, dunits)]",
            "self.dropout_dec += [torch.nn.Dropout(p=dropout)]"
        ]
    },
    {
        "number": 6370,
        "comments": "",
        "commit_message": "lintfixbot: Auto-commit fixed lint errors in codebase\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def xlogy(",
            "return torch.xlogy(x, y, out=out)",
            "",
            "",
            "-def real(x: Union[torch.Tensor],",
            "-         /,",
            "-         *,",
            "-         out: Optional[torch.Tensor] = None) -> torch.Tensor:",
            "+def real(",
            "+    x: Union[torch.Tensor], /, *, out: Optional[torch.Tensor] = None",
            "+) -> torch.Tensor:",
            "return torch.real(x)"
        ]
    },
    {
        "number": 6372,
        "comments": "",
        "commit_message": "Release 1.8.6rc0 (#660)\n\n* Release 1.8.6rc0\n\n* Update CHANGELOG.md\n\n* Typo Fix\n\n* Changelog Updated\n\n* tl_logging more readable\n\n* Typo Fixed\n\n* https enforced for badges\n\n* RTD badges pointing to Latest Commit\n\n* Decorators API Refactored\n\n* extra_requires `all`, `all_cpu` and `all_gpu` added\n\n* Error fix\n\n* YAPF Formating Correction\n\n* Test for private method decorator added\n\n* Test Logging Verbosity Fixed to Debug when runned individually\n\n* YAPF corrections applied\n\n* Changelog Added\n\n* Changelog updated\n\n* PR number changed\n\n* Changelog updated\n\n* Update .travis.yml\n\n* Update CONTRIBUTING.md\n\n* Update CONTRIBUTING.md\n\n* Update CHANGELOG.md\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Layer_DeformableConvolution_Test(unittest.TestCase):",
            "",
            "",
            "if __name__ == '__main__':",
            "-    # tl.logging.set_verbosity(tl.logging.INFO)",
            "+",
            "+    tf.logging.set_verbosity(tf.logging.DEBUG)",
            "tl.logging.set_verbosity(tl.logging.DEBUG)",
            "",
            "unittest.main()"
        ]
    },
    {
        "number": 6374,
        "comments": "",
        "commit_message": "[FLAX] Minor fixes in CLM example (#12914)\n\n* readme: fix retrieval of vocab size for flax clm example\n\n* examples: fix flax clm example when using training/evaluation files\n",
        "label": "",
        "answer": "no",
        "change": [
            "def main():",
            "extension = \"text\"",
            "dataset = load_dataset(extension, data_files=data_files, cache_dir=model_args.cache_dir)",
            "",
            "-        if \"validation\" not in datasets.keys():",
            "-            datasets[\"validation\"] = load_dataset(",
            "+        if \"validation\" not in dataset.keys():",
            "+            dataset[\"validation\"] = load_dataset(",
            "extension,",
            "data_files=data_files,",
            "split=f\"train[:{data_args.validation_split_percentage}%]\",",
            "cache_dir=model_args.cache_dir,",
            ")",
            "-            datasets[\"train\"] = load_dataset(",
            "+            dataset[\"train\"] = load_dataset(",
            "extension,",
            "data_files=data_files,",
            "split=f\"train[{data_args.validation_split_percentage}%:]\","
        ]
    },
    {
        "number": 6376,
        "comments": "",
        "commit_message": "Added container operation weights and OneOf documentation (#1443)\n\n* Added OneOf documentation\n\n* Added random apply weights\n\n* Updated docs\n\n* Added tests\n\n* doctest fix\n\n* doctest fix\n\n* Apply suggestions from code review\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class VideoSequential(ImageSequential):",
            "data_format: str = \"BTCHW\",",
            "same_on_frame: bool = True,",
            "random_apply: Union[int, bool, Tuple[int, int]] = False,",
            "+        random_apply_weights: Optional[List[float]] = None,",
            ") -> None:",
            "-        super().__init__(*args, same_on_batch=None, return_transform=None, keepdim=None, random_apply=random_apply)",
            "+        super().__init__(",
            "+            *args, same_on_batch=None, return_transform=None, keepdim=None, random_apply=random_apply,",
            "+            random_apply_weights=random_apply_weights",
            "+        )",
            "self.same_on_frame = same_on_frame",
            "self.data_format = data_format.upper()",
            "if self.data_format not in [\"BCTHW\", \"BTCHW\"]:"
        ]
    },
    {
        "number": 6380,
        "comments": "",
        "commit_message": "Fix albert example\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class PreTrainedModel(nn.Module):",
            "model = BertModel.from_pretrained('./tf_model/my_tf_checkpoint.ckpt.index', from_tf=True, config=config)",
            "",
            "\"\"\"",
            "-        if pretrained_model_name_or_path is not None and (",
            "-                \"albert\" in pretrained_model_name_or_path and \"v2\" in pretrained_model_name_or_path):",
            "-            logger.warning(\"There is currently an upstream reproducibility issue with ALBERT v2 models. Please see \" +",
            "-                           \"https://github.com/google-research/google-research/issues/119 for more information.\")",
            "-",
            "config = kwargs.pop('config', None)",
            "state_dict = kwargs.pop('state_dict', None)",
            "cache_dir = kwargs.pop('cache_dir', None)"
        ]
    },
    {
        "number": 6382,
        "comments": "",
        "commit_message": "Fix some dimensions\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def model(X, params, mesh, labels=None, past=None, scope='model', reuse=False, t",
            "# wpe has shape [ctx, embd]",
            "# positions_for would have shape [batch, seq]",
            "# h has shape [batch, seq, embd]",
            "-        zerodim = mtf.Dimension('singleton', 0)",
            "",
            "-        h = mtf.gather(wte, X, zerodim) + mtf.gather(wpe, positions_for(X, past_length, batch_dim), zerodim)",
            "+        h = mtf.gather(wte, X, vocab_dim) + mtf.gather(wpe, positions_for(X, past_length, batch_dim), vocab_dim)",
            "",
            "# Transformer",
            "presents = []"
        ]
    },
    {
        "number": 6384,
        "comments": "",
        "commit_message": "Adding new export for Triton (#2078)\n\n* Adding new export for triton.  Fixes for load model for neuropod export, add output dict format\n\n* Adding test for triton.  Fix to cast int to string for os.path.join.  Added annotation for neurpod\n\n* Minor tweaks to config.pbtxt output\n\n* Remove logger that is not being used\n\n* Restrict torchmetrics<0.9 and whylogs<1.0 until compatibility fixed\n\n* Update export_triton to return model path, and added docstrings\n\n* Update api to return both model path and config path\n\nCo-authored-by: Travis Addair <tgaddair@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def _get_output_dicts(config: Dict[str, Any]) -> str:",
            "results = []",
            "for feature in config[\"output_features\"]:",
            "name = feature[NAME]",
            "-        results.append(\"{\" + f'\"{name}\": results[\"{name}\"][\"predictions\"]' + \"}\")",
            "-    return \", \".join(results)",
            "+        results.append(f'\"{name}\": results[\"{name}\"][\"predictions\"]')",
            "+    return \"{\" + \", \".join(results) + \"}\"",
            "",
            "",
            "def generate_neuropod_torchscript(model: LudwigModel):"
        ]
    },
    {
        "number": 6386,
        "comments": "",
        "commit_message": "fix issues for pytorch<1.9.0.\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Detect(nn.Module):",
            "",
            "@staticmethod",
            "def _make_grid(nx=20, ny=20):",
            "-        yv, xv = torch.meshgrid([torch.arange(ny), torch.arange(nx)], indexing=\"ij\")",
            "+        # yv, xv = torch.meshgrid([torch.arange(ny), torch.arange(nx)], indexing=\"ij\") # for pytorch>=1.10",
            "+        yv, xv = torch.meshgrid([torch.arange(ny), torch.arange(nx)])",
            "return torch.stack((xv, yv), 2).view((1, 1, ny, nx, 2)).float()"
        ]
    },
    {
        "number": 6389,
        "comments": "",
        "commit_message": "fix mypy\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class CRF(torch.nn.Module):",
            "self.transitions.detach()[:, tag_dictionary.get_idx_for_item(STOP_TAG)] = -10000",
            "self.to(flair.device)",
            "",
            "-    def forward(self, features: torch.tensor) -> torch.tensor:",
            "+    def forward(self, features: torch.Tensor) -> torch.Tensor:",
            "\"\"\"",
            "Forward propagation of Conditional Random Field.",
            ":param features: output from RNN / Linear layer in shape (batch size, seq len, hidden size)",
            ":return: CRF scores (emission scores for each token + transitions prob from previous state) in",
            "shape (batch_size, seq len, tagset size, tagset size)",
            "\"\"\"",
            "-        batch_size = features.size(0)",
            "-        seq_len = features.size(1)",
            "+        batch_size, seq_len, _, _ = features.size()",
            "",
            "emission_scores = features",
            "emission_scores = emission_scores.unsqueeze(-1).expand(batch_size, seq_len, self.tagset_size, self.tagset_size)"
        ]
    },
    {
        "number": 6394,
        "comments": "",
        "commit_message": "fix batched_index_select (#2765)\n\n* fix batched_index_select\n\n* fix batched_index_select\n\n* add unit test\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def flatten_and_batch_shift_indices(indices: torch.Tensor,",
            "offset_indices : ``torch.LongTensor``",
            "\"\"\"",
            "# Shape: (batch_size)",
            "+    if torch.max(indices) >= sequence_length or torch.min(indices) < 0:",
            "+        raise ConfigurationError(f\"All elements in indices should be in range (0, {sequence_length - 1})\")",
            "offsets = get_range_vector(indices.size(0), get_device_of(indices)) * sequence_length",
            "for _ in range(len(indices.size()) - 1):",
            "offsets = offsets.unsqueeze(1)"
        ]
    },
    {
        "number": 6400,
        "comments": "",
        "commit_message": "TRPO working, other fixes and improvements\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Beta(Distribution):",
            "",
            "sampled = beta_sample / tf.maximum(x=(alpha_sample + beta_sample), y=epsilon)",
            "",
            "-        sampled = tf.where(condition=(temperature < epsilon), x=definite, y=sampled)",
            "+        action = tf.where(condition=(temperature < epsilon), x=definite, y=sampled)",
            "",
            "min_value = tf_util.constant(value=self.action_spec.min_value, dtype='float')",
            "max_value = tf_util.constant(value=self.action_spec.max_value, dtype='float')",
            "",
            "with tf.control_dependencies(control_inputs=dependencies):",
            "-            return min_value + (max_value - min_value) * sampled",
            "+            return min_value + (max_value - min_value) * action",
            "",
            "@tf_function(num_args=2)",
            "def log_probability(self, *, parameters, action):"
        ]
    },
    {
        "number": 6401,
        "comments": "",
        "commit_message": "Fix TF s2s models (#9478)\n\n* Fix Seq2Seq models for serving\n\n* Apply style\n\n* Fix lonfgormer\n\n* Fix mBart/Pegasus/Blenderbot\n\n* Apply style\n\n* Add a main intermediate layer\n\n* Apply style\n\n* Remove import\n\n* Apply tf.function to Longformer\n\n* Fix utils check_copy\n\n* Update S2S template\n\n* Fix BART + Blenderbot\n\n* Fix BlenderbotSmall\n\n* Fix BlenderbotSmall\n\n* Fix BlenderbotSmall\n\n* Fix MBart\n\n* Fix Marian\n\n* Fix Pegasus + template\n\n* Apply style\n\n* Fix common attributes test\n\n* Forgot to fix the LED test\n\n* Apply Patrick's comment on LED Decoder\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TFT5ForConditionalGeneration(TFT5PreTrainedModel, TFCausalLanguageModeling",
            ")",
            "",
            "def serving_output(self, output):",
            "-        pkv = (tf.convert_to_tensor(output.past_key_values[1:]) if self.config.use_cache else None,)",
            "+        pkv = tf.convert_to_tensor(output.past_key_values[1:]) if self.config.use_cache else None",
            "dec_hs = tf.convert_to_tensor(output.decoder_hidden_states) if self.config.output_hidden_states else None",
            "dec_attns = tf.convert_to_tensor(output.decoder_attentions) if self.config.output_attentions else None",
            "enc_hs = tf.convert_to_tensor(output.encoder_hidden_states) if self.config.output_hidden_states else None"
        ]
    },
    {
        "number": 6402,
        "comments": "",
        "commit_message": "fix diff of torch backend so not casting None to tensor\n",
        "label": "",
        "answer": "no",
        "change": [
            "def diff(",
            "append: Optional[Union[torch.Tensor, int, float, list, tuple]] = None,",
            ") -> torch.Tensor:",
            "x = x if type(x) == torch.Tensor else torch.Tensor(x)",
            "-    prepend = prepend if type(prepend) == torch.Tensor else torch.Tensor(prepend)",
            "-    append = append if type(append) == torch.Tensor else torch.Tensor(append)",
            "+    prepend = prepend if type(prepend) == torch.Tensor or prepend == None else torch.Tensor(prepend)",
            "+    append = append if type(append) == torch.Tensor or append == None else torch.Tensor(append)",
            "return torch.diff(x, n=n, dim=axis, prepend=prepend, append=append)"
        ]
    },
    {
        "number": 6404,
        "comments": "",
        "commit_message": "Fixed set explanations (#3160)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class SetInputFeature(SetFeatureMixin, InputFeature):",
            "",
            "def forward(self, inputs):",
            "assert isinstance(inputs, torch.Tensor)",
            "-        assert inputs.dtype in [torch.bool, torch.int64]",
            "+        assert inputs.dtype in [torch.bool, torch.int64, torch.float32]",
            "",
            "encoder_output = self.encoder_obj(inputs)"
        ]
    },
    {
        "number": 6405,
        "comments": "",
        "commit_message": "[torch] remove deprecated uint8 in favor of bool (#21384)\n\n* uint8 -> bool\n\n* fix copies\n\n* style\n\n* update test modeling commen when checking attention buffers\n\n* style\n\n* use logical not on random mask instead of subtraction with 1\n\n* remove torch uint8\n\n* quality\n\n* remove modified modeling utils\n\n* Update based on review\n\nCo-authored-by: sgugger <sylvain.gugger@gmail.com>\n\n---------\n\nCo-authored-by: sgugger <sylvain.gugger@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class ImageGPTAttention(nn.Module):",
            "if not self.is_cross_attention:",
            "# if only \"normal\" attention layer implements causal mask",
            "query_length, key_length = query.size(-2), key.size(-2)",
            "-            causal_mask = self.bias[:, :, key_length - query_length : key_length, :key_length].bool()",
            "+            causal_mask = self.bias[:, :, key_length - query_length : key_length, :key_length]",
            "mask_value = torch.finfo(attn_weights.dtype).min",
            "# Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.",
            "# Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`"
        ]
    },
    {
        "number": 6408,
        "comments": "",
        "commit_message": "fix issue with torch sum where keepdims wasnt working when no axis is given\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def sum(",
            "return x.type(dtype)",
            "axis = tuple(axis) if isinstance(axis, list) else axis",
            "if axis is None:",
            "-        return torch.sum(input=x, dtype=dtype)",
            "+        return torch.sum(input=x, dim=(), dtype=dtype, keepdim=keepdims)",
            "return torch.sum(input=x, dim=axis, dtype=dtype, keepdim=keepdims)"
        ]
    },
    {
        "number": 6410,
        "comments": "",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class EntropyTest(AllenNlpTestCase):",
            "logits = torch.tensor(",
            "[[1, 1, 1, 1], [10000, -10000, -10000, -1000]], dtype=torch.float, device=device",
            ")",
            "-        mask = torch.tensor([0, 1], device=device)",
            "+        mask = torch.BoolTensor([False, True], device=device)",
            "metric(logits, mask)",
            "assert metric.get_metric() == 0.0"
        ]
    },
    {
        "number": 6412,
        "comments": "",
        "commit_message": "enable black in the precommit (#1777)\n\n* enable black in the precommit\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add some fixes\n\n* update libface detection url\n\n* added url from kornia checkpoint\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class OriNet(nn.Module):",
            "# use torch.hub to load pretrained model",
            "if pretrained:",
            "storage_fcn: Callable = lambda storage, loc: storage",
            "-            pretrained_dict = torch.hub.load_state_dict_from_url(",
            "-                urls['orinet'], map_location=storage_fcn",
            "-            )",
            "+            pretrained_dict = torch.hub.load_state_dict_from_url(urls['orinet'], map_location=storage_fcn)",
            "self.load_state_dict(pretrained_dict['state_dict'], strict=False)",
            "self.eval()"
        ]
    },
    {
        "number": 6413,
        "comments": "",
        "commit_message": "Allow loading checkpoints from urls (#1667)\n\n* allow loading checkpoints from urls\n\n* tmpdir_server fixture\n\n* test cases for loading checkpoints from url\n\n* dir => root_dir\n\n* default map_location to None\n\n* test case for resume_from_checkpoint\n\n* changelog\n\n* doc update\n\n* monkeypatch TORCH_HOME to avoid caching\n\n* Use a threading server with random ports so that it is easier to clean up\n\n* test fixes\n\n* pep8 fix\n\n* ThreadingHTTPServer support in 3.6\n\n* pep8 fix\n\n* fix changelog\n\n* separate tests for urls\n\n* typo\n\nCo-authored-by: Peter Yu <2057325+yukw777@users.noreply.github.com>\n\n* Apply suggestions from code review\n\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TrainerIOMixin(ABC):",
            "#     checkpoint = torch.load(checkpoint_path)",
            "# else:",
            "# load on CPU first",
            "-        checkpoint = torch.load(checkpoint_path, map_location=lambda storage, loc: storage)",
            "+        checkpoint = pl_load(checkpoint_path, map_location=lambda storage, loc: storage)",
            "",
            "# load model state",
            "model = self.get_model()"
        ]
    },
    {
        "number": 6415,
        "comments": "",
        "commit_message": "Fix remaining GPU tests (#1603)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def run_api_experiment(input_features, output_features, dataset, **kwargs):",
            "loaded_state = loaded_model.model.state_dict()",
            "bcast_state = hvd.broadcast_object(loaded_state)",
            "for loaded, bcast in zip(loaded_state.values(), bcast_state.values()):",
            "-            assert np.allclose(loaded, bcast)",
            "+            assert torch.allclose(loaded, bcast)",
            "finally:",
            "if output_dir:",
            "shutil.rmtree(output_dir, ignore_errors=True)"
        ]
    },
    {
        "number": 6421,
        "comments": "",
        "commit_message": "Remove Re-definition found for builtin input function - Update tests (#2255)\n\n* change input to sample in test/feature folder\n\n* change input to sample in test/filters folder\n\n* change input to sample in test/morphology folder\n\n* change input to sample in test/geometry/subpix/test_spatial_softargmax.py\n\n* change input to sample in test/geometry/transfrom in test_imgwarp3d.py and test_pyramid.py\n\n* change input to input_org in test/geometry/transfrom in test_imgwarp.py\n\n* change input to inpt in test/grad_estimator/test_ste.py\n\n* change input to inpt and sample in test/enhance/ folder\n\n* change input to input_tensor in test/augmentation/test_augmentation_3d.py\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nCo-authored-by: oleksadobush <oleksandra.stasiuk@ucu.edu.ua>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestLaplacian(BaseTester):",
            "kernel_size = 3",
            "",
            "# evaluate function gradient",
            "-        input = torch.rand(batch_shape, device=device)",
            "-        input = tensor_to_gradcheck_var(input)",
            "-        self.gradcheck(laplacian, (input, kernel_size))",
            "+        sample = torch.rand(batch_shape, device=device)",
            "+        sample = tensor_to_gradcheck_var(sample)",
            "+        self.gradcheck(laplacian, (sample, kernel_size))",
            "",
            "def test_module(self, device, dtype):",
            "params = [3]"
        ]
    },
    {
        "number": 6423,
        "comments": "",
        "commit_message": "Fix interpolation comments.\n\nPiperOrigin-RevId: 329882604\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def interpolate(x_values,",
            "# dx = x_data[indices + 1] - x_data[indices]",
            "# dy = y_data[indices + 1] - y_data[indices]",
            "# indices is a tensor with different values per row/spline",
            "-    # Hence use a selection matrix with gather_nd",
            "def get_slice(x, encoding):",
            "if optimize_for_tpu:",
            "return tf.math.reduce_sum(tf.expand_dims(x, axis=-2) * encoding,"
        ]
    },
    {
        "number": 6428,
        "comments": "",
        "commit_message": "Shubhamagarwal92 master (#1349)\n\n* SA: for #958: set torch cuda device when finding root\n\n* SA: for #958: removing root gpu hack in trainer/evaluation_loop\n\n* SA: setting torch cuda device\n\n* comment line too long\n\n* check if root gpu exists or available\n\n* Incorporating suggestions on #1094\n\n* since root gpu returns none instead of -1 for cpu\n\n* undo changes\n\n* fixed dp memory thing\n\nCo-authored-by: Shubham Agarwal <shubhamagarwal92@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TrainerDPMixin(ABC):",
            "if isinstance(device_ids, int):",
            "device_ids = list(range(device_ids))",
            "",
            "+        # set dp device",
            "+        torch.cuda.set_device(self.root_gpu)",
            "+",
            "model = LightningDataParallel(model, device_ids=device_ids)",
            "",
            "self.run_pretrain_routine(model)"
        ]
    },
    {
        "number": 6437,
        "comments": "",
        "commit_message": "fixing it to work properly in multi-node environment\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "bn_opt = optim.SGD(bn.parameters(), lr=1.0)",
            "for group_num in (range(args.world_size//args.group_size)):",
            "group_ids = range(group_num*args.group_size, (group_num+1)*args.group_size)",
            "cur_group = torch.distributed.new_group(ranks=group_ids)",
            "-   if (args.local_rank//args.group_size == group_num):",
            "+   if (torch.distributed.get_rank()//args.group_size == group_num):",
            "group = cur_group",
            "",
            "sbn = apex.parallel.SyncBatchNorm(feature_size, process_group=group).cuda()"
        ]
    },
    {
        "number": 6438,
        "comments": "",
        "commit_message": "Fixes to enable dev branch to move to pytorch-1.2 (#2001)\n\n* Fixes to enable dev branch to move to pytorch-1.2\n\n* change name for published cpu wheels\n\n* fix failing unit tests\n\n* change docs conf\n\n* change tracking example\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "ARGS = {",
            "dist.Geometric: [0.5],",
            "dist.Independent: [dist.Normal(torch.zeros(2), torch.ones(2)), 1],",
            "dist.LowRankMultivariateNormal: [torch.zeros(2), torch.ones(2, 2), torch.ones(2)],",
            "-    dist.MaskedMixture: [torch.tensor([1, 0]).byte(), dist.Normal(0, 1), dist.Normal(0, 2)],",
            "+    dist.MaskedMixture: [torch.tensor([1, 0]).bool(), dist.Normal(0, 1), dist.Normal(0, 2)],",
            "dist.MixtureOfDiagNormals: [torch.ones(2, 3), torch.ones(2, 3), torch.ones(2)],",
            "dist.MixtureOfDiagNormalsSharedCovariance: [torch.ones(2, 3), torch.ones(3), torch.ones(2)],",
            "dist.Multinomial: [2, torch.ones(2)],"
        ]
    },
    {
        "number": 6439,
        "comments": "",
        "commit_message": "Use f-strings in the dataset scripts (#3291)\n\n* Finishes #3257\n\nUsed f-strings to format the .py files in the dataset folder\n\n* Fix style\n\n* Fix hkcancor dataset\n\nCo-authored-by: Mario \u0160a\u0161ko <mario@huggingface.co>\nCo-authored-by: mariosasko <mariosasko777@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class RecipeNlg(datasets.GeneratorBasedBuilder):",
            "path_to_manual_file = os.path.abspath(os.path.expanduser(dl_manager.manual_dir))",
            "if not os.path.exists(path_to_manual_file):",
            "raise FileNotFoundError(",
            "-                \"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('recipe_nlg', data_dir=...)` that includes file name {}. Manual download instructions: {}\".format(",
            "-                    path_to_manual_file,",
            "-                    _FILENAME,",
            "-                    self.manual_download_instructions,",
            "-                )",
            "+                f\"{path_to_manual_file} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('recipe_nlg', data_dir=...)` that includes file name {_FILENAME}. Manual download instructions: {self.manual_download_instructions}\"",
            ")",
            "return [",
            "datasets.SplitGenerator("
        ]
    },
    {
        "number": 6446,
        "comments": "",
        "commit_message": "fixed error in replay memory/retrieve episodes, added preliminary ddpg agent without target networks\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Replay(Queue):",
            "random_episode_indices = tf.random_uniform(shape=(n,), maxval=(self.episode_count + 1), dtype=tf.int32)",
            "starts = tf.gather(params=self.episode_indices, indices=random_episode_indices) + 1",
            "limits = tf.gather(params=self.episode_indices, indices=(random_episode_indices + 1))",
            "-        limits += tf.where(condition=(starts < limits), x=0, y=self.capacity)",
            "+        limits += tf.where(condition=(starts < limits), x=(0,), y=self.capacity)",
            "episodes = [tf.range(start=starts[n], limit=limits[n]) for k in range(n)]",
            "indices = tf.concat(values=episodes, axis=0) % self.capacity",
            "return self.retrieve_indices(indices=indices)"
        ]
    },
    {
        "number": 6447,
        "comments": "",
        "commit_message": "[tune/raysgd] Tune API for TorchTrainer + Fix State Restoration (#7547)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "import logging",
            "logger = logging.getLogger(__name__)",
            "",
            "TorchTrainer = None",
            "-TorchTrainable = None",
            "TrainingOperator = None",
            "+BaseTorchTrainable = None",
            "",
            "try:",
            "import torch  # noqa: F401",
            "",
            "-    from ray.util.sgd.torch.torch_trainer import (TorchTrainer, TorchTrainable)",
            "+    from ray.util.sgd.torch.torch_trainer import (TorchTrainer,",
            "+                                                  BaseTorchTrainable)",
            "",
            "from ray.util.sgd.torch.training_operator import TrainingOperator",
            "",
            "-    __all__ = [\"TorchTrainer\", \"TorchTrainable\", \"TrainingOperator\"]",
            "+    __all__ = [\"TorchTrainer\", \"BaseTorchTrainable\", \"TrainingOperator\"]",
            "except ImportError:",
            "logger.warning(\"PyTorch not found. TorchTrainer will not be available\")"
        ]
    },
    {
        "number": 6453,
        "comments": "",
        "commit_message": "fix flake8 and refactor pre_beam\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class BeamSearch(torch.nn.Module):",
            "",
            "\"\"\"",
            "if self._do_pre_beam(scores):",
            "-            return torch.topk(scores[self.pre_beam_score_key], self.pre_beam_size)[1]",
            "+            return torch.topk(scores[self.pre_beam_score_key], self.pre_beam_size, dim=-1)[1]",
            "else:",
            "return torch.arange(self.n_vocab, device=device)"
        ]
    },
    {
        "number": 6455,
        "comments": "",
        "commit_message": "Fix rand_segment edge case (input_len == seg_len - 1)\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def rand_segments(",
            "T = segment_size",
            "if _x_lenghts is None:",
            "_x_lenghts = T",
            "-    len_diff = _x_lenghts - segment_size + 1",
            "+    len_diff = _x_lenghts - segment_size",
            "if let_short_samples:",
            "_x_lenghts[len_diff < 0] = segment_size",
            "-        len_diff = _x_lenghts - segment_size + 1",
            "+        len_diff = _x_lenghts - segment_size",
            "else:",
            "assert all(",
            "len_diff > 0",
            "), f\" [!] At least one sample is shorter than the segment size ({segment_size}). \\n {_x_lenghts}\"",
            "-    segment_indices = (torch.rand([B]).type_as(x) * len_diff).long()",
            "-    ret = segment(x, segment_indices, segment_size)",
            "+    segment_indices = (torch.rand([B]).type_as(x) * (len_diff + 1)).long()",
            "+    ret = segment(x, segment_indices, segment_size, pad_short=pad_short)",
            "return ret, segment_indices"
        ]
    },
    {
        "number": 6458,
        "comments": "",
        "commit_message": "Extended api of LSTM encoder. (#2030)\n\nSummary:\n# Before submitting\n\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\n- [x] Did you make sure to update the docs?\n- [ ] Did you write any new necessary tests?\n\n## What does this PR do?\nFixes https://github.com/pytorch/fairseq/issues/1862.\nNobody responded to my issue. Nevertheless change is very small, therefore i think is doesn't need much discussion.\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding \ufffd\nPull Request resolved: https://github.com/pytorch/fairseq/pull/2030\n\nReviewed By: joshim5, ngoyal2707\n\nDifferential Revision: D21584250\n\nPulled By: myleott\n\nfbshipit-source-id: 28f0ccaca0df2860806178dbce02bcc12d7115d4\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class LSTMEncoder(FairseqEncoder):",
            "x = x.transpose(0, 1)",
            "",
            "# pack embedded source tokens into a PackedSequence",
            "-        packed_x = nn.utils.rnn.pack_padded_sequence(x, src_lengths.data)",
            "+        packed_x = nn.utils.rnn.pack_padded_sequence(",
            "+            x, src_lengths.data, enforce_sorted=enforce_sorted",
            "+        )",
            "",
            "# apply LSTM",
            "if self.bidirectional:"
        ]
    },
    {
        "number": 6459,
        "comments": "",
        "commit_message": "Fix two warnings related to deprecated function names.\n\nPiperOrigin-RevId: 284760914\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def find_interval_index(query_xs,",
            "# cap to last_index - 1.",
            "caps = last_index - tf.cast(should_cap, dtype=tf.dtypes.int32)",
            "",
            "-    return tf.where(last_interval_is_closed, tf.minimum(indices, caps), indices)",
            "+    return tf.compat.v1.where(last_interval_is_closed,",
            "+                              tf.minimum(indices, caps), indices)",
            "",
            "",
            "def _piecewise_constant_function(x, jump_locations, values, side='left'):"
        ]
    },
    {
        "number": 6461,
        "comments": "",
        "commit_message": "Further fixes and improvements.\n",
        "label": "",
        "answer": "no",
        "change": [
            "def ndim(x):",
            "2",
            "```",
            "\"\"\"",
            "-    if is_sparse(x):",
            "-        return x._dims",
            "-",
            "dims = x.get_shape()._dims",
            "if dims is not None:",
            "return len(dims)"
        ]
    },
    {
        "number": 6463,
        "comments": "",
        "commit_message": "Fix MixConv2d() remove shortcut + apply depthwise (#5410)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class BottleneckCSP(nn.Module):",
            "self.cv3 = nn.Conv2d(c_, c_, 1, 1, bias=False)",
            "self.cv4 = Conv(2 * c_, c2, 1, 1)",
            "self.bn = nn.BatchNorm2d(2 * c_)  # applied to cat(cv2, cv3)",
            "-        self.act = nn.LeakyReLU(0.1, inplace=True)",
            "+        self.act = nn.SiLU()",
            "self.m = nn.Sequential(*(Bottleneck(c_, c_, shortcut, g, e=1.0) for _ in range(n)))",
            "",
            "def forward(self, x):"
        ]
    },
    {
        "number": 6467,
        "comments": "",
        "commit_message": "zero-mAP fix remove `torch.empty()` forward pass in `.train()` mode (#9068)\n\n* Fix Zero Map Issue\n\nSigned-off-by: 0zppd <111682241+0zppd@users.noreply.github.com>\n\n* Update __init__.py\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\nSigned-off-by: 0zppd <111682241+0zppd@users.noreply.github.com>\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\nCo-authored-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def log_tensorboard_graph(tb, model, imgsz=(640, 640)):",
            "try:",
            "p = next(model.parameters())  # for device, type",
            "imgsz = (imgsz, imgsz) if isinstance(imgsz, int) else imgsz  # expand",
            "-        im = torch.empty((1, 3, *imgsz)).to(p.device).type_as(p)  # input image",
            "+        im = torch.zeros((1, 3, *imgsz)).to(p.device).type_as(p)  # input image (WARNING: must be zeros, not empty)",
            "with warnings.catch_warnings():",
            "warnings.simplefilter('ignore')  # suppress jit trace warning",
            "tb.add_graph(torch.jit.trace(de_parallel(model), im, strict=False), [])"
        ]
    },
    {
        "number": 6468,
        "comments": "",
        "commit_message": "Added example for compute grads in ray tutorial (#238)\n\n* Added example for compute grads in ray\n\n* Added formatting\n\n* Removed need for placeholders in apply gradient\n\n* Streamlined examples\n\n* Fixed docs\n\n* Added formatting\n\n* Removed old references\n\n* Simplified code some\n\n* Addressed comments\n\n* Changes to first code block\n\n* Added test for training and updated code snippets\n\n* Formatting\n\n* Removed mean\n\n* Removed all mention of mean\n\n* Added comments\n\n* Added comments\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TensorFlowVariables(object):",
            "def set_weights(self, new_weights):",
            "\"\"\"Sets the weights to new_weights.\"\"\"",
            "self._check_sess()",
            "-    self.sess.run(self.assignment_nodes, feed_dict={self.assignment_placeholders[name]: value for (name, value) in new_weights.items()})",
            "+    self.sess.run(self.assignment_nodes, feed_dict={self.placeholders[name]: value for (name, value) in new_weights.items()})"
        ]
    },
    {
        "number": 6470,
        "comments": "",
        "commit_message": "for discussion: incorporate black code formatter (#3308)\n\n* setup files\n\n* run black\n\n* undo\n\n* update CONTRIBUTING.md\n\n* fix quotes in test_other_modules\n\n* make flake8 happy\n\n* set black to 100 characters per line\n\n* move type: ignore to where mypy wants them\n\n* more flake8\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def get_best_span(span_start_logits: torch.Tensor, span_end_logits: torch.Tensor",
            "span_log_probs = span_start_logits.unsqueeze(2) + span_end_logits.unsqueeze(1)",
            "# Only the upper triangle of the span matrix is valid; the lower triangle has entries where",
            "# the span ends before it starts.",
            "-    span_log_mask = torch.triu(torch.ones((passage_length, passage_length),",
            "-                                          device=device)).log()",
            "+    span_log_mask = torch.triu(torch.ones((passage_length, passage_length), device=device)).log()",
            "valid_span_log_probs = span_log_probs + span_log_mask",
            "",
            "# Here we take the span matrix and flatten it, then find the best span using argmax.  We"
        ]
    },
    {
        "number": 6476,
        "comments": "",
        "commit_message": "[layers] fix BiDynamicRNNLayer to avoid error in gradient updating\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class BiDynamicRNNLayer(Layer):",
            "",
            "print(\"     n_params : %d\" % (len(rnn_variables)))",
            "# Manage the outputs",
            "-            outputs = tf.concat(-1, outputs)",
            "+            outputs = tf.concat(2, outputs)",
            "if return_last:",
            "# [batch_size, 2 * n_hidden]",
            "self.outputs = advanced_indexing_op(outputs, sequence_length)"
        ]
    },
    {
        "number": 6479,
        "comments": "",
        "commit_message": "static padding fixed\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Conv2dStaticSamePadding(nn.Conv2d):",
            "pad_h = max((oh - 1) * self.stride[0] + (kh - 1) * self.dilation[0] + 1 - ih, 0)",
            "pad_w = max((ow - 1) * self.stride[1] + (kw - 1) * self.dilation[1] + 1 - iw, 0)",
            "if pad_h > 0 or pad_w > 0:",
            "-            self.static_padding = nn.ZeroPad2d((pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2))",
            "+            self.static_padding = nn.ZeroPad2d((pad_w - pad_w // 2, pad_w - pad_w // 2,",
            "+                                                pad_h - pad_h // 2, pad_h - pad_h // 2))",
            "else:",
            "self.static_padding = Identity()"
        ]
    },
    {
        "number": 6486,
        "comments": "",
        "commit_message": "lint fixes\n\nSummary:\nRan the linter.\nTODO: need to update the linter as per D21353065.\n\nReviewed By: bottler\n\nDifferential Revision: D21362270\n\nfbshipit-source-id: ad0e781de0a29f565ad25c43bc94a19b1828c020\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestSamplePoints(TestCaseMixin, unittest.TestCase):",
            "x, y, z = samples[1, :].unbind(1)",
            "radius = torch.sqrt(x ** 2 + y ** 2 + z ** 2)",
            "",
            "-        self.assertClose(radius, torch.ones((num_samples)))",
            "+        self.assertClose(radius, torch.ones(num_samples))",
            "",
            "# Pyramid: points shoudl lie on one of the faces.",
            "pyramid_verts = samples[2, :]"
        ]
    },
    {
        "number": 6489,
        "comments": "",
        "commit_message": "unbiased=False for torch.var_mean path of ConvNeXt LN. Fix #1090\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class LayerNorm2d(nn.LayerNorm):",
            "return F.layer_norm(",
            "x.permute(0, 2, 3, 1), self.normalized_shape, self.weight, self.bias, self.eps).permute(0, 3, 1, 2)",
            "else:",
            "-            s, u = torch.var_mean(x, dim=1, keepdim=True)",
            "+            s, u = torch.var_mean(x, dim=1, unbiased=False, keepdim=True)",
            "x = (x - u) * torch.rsqrt(s + self.eps)",
            "x = x * self.weight[:, None, None] + self.bias[:, None, None]",
            "return x"
        ]
    },
    {
        "number": 6490,
        "comments": "",
        "commit_message": "Fix control_flow_ops (#380)\n\nTensorflow 0.11 removes the ability to access python.control_flow_ops\ntf.cond is (and was) the correct function\nREF: https://github.com/tensorflow/tensorflow/issues/4616\n",
        "label": "",
        "answer": "no",
        "change": [
            "def batch_normalization(incoming, beta=0.0, gamma=1.0, epsilon=1e-5,",
            "",
            "# Retrieve variable managing training mode",
            "is_training = tflearn.get_training_mode()",
            "-        mean, var = tf.python.control_flow_ops.cond(",
            "+        mean, var = tf.cond(",
            "is_training, update_mean_var, lambda: (moving_mean, moving_variance))",
            "",
            "try:"
        ]
    },
    {
        "number": 6494,
        "comments": "",
        "commit_message": "Fix FP16 and attention masks in FunnelTransformer (#7374)\n\n* Fix #7371\n\n* Fix training\n\n* Fix test values\n\n* Apply the fix to TF as well\n",
        "label": "",
        "answer": "no",
        "change": [
            "class FunnelModelIntegrationTest(unittest.TestCase):",
            "inputs = tokenizer(\"Hello! I am the Funnel Transformer model.\", return_tensors=\"pt\")",
            "output = model(**inputs)[0]",
            "",
            "-        expected_output_sum = torch.tensor(235.7827)",
            "+        expected_output_sum = torch.tensor(235.7246)",
            "expected_output_mean = torch.tensor(0.0256)",
            "self.assertTrue(torch.allclose(output.sum(), expected_output_sum, atol=1e-4))",
            "self.assertTrue(torch.allclose(output.mean(), expected_output_mean, atol=1e-4))"
        ]
    },
    {
        "number": 6495,
        "comments": "",
        "commit_message": "MNIST Prototype Duet Notebooks (#4696)\n\n* added request_block support\n\n* ran black\n\n* fixed return types\n\n* Added loopback=True to launch_duet and join_duet\n\n- loopback automatically writes and reads the IDs to a shared tmp file\n- The purpose of this helper is to speed up development or local usage\n\n* Converted prototype MNIST notebook into Duet Notebooks\n\n- Added additional required ops for MNIST\n- Added matplotlib when displaying images for inference\n\n* ignoring bandit warning on /tmp folder\n\n- How else can we get a deterministic tmp folder between processes?\n\n* Fixed /tmp reference with tempfile.gettempdir()\n\n- Unique path on macOS seems to be deterministic between procs\n- This should now work on Windows as well\n- Tested on MNIST notebooks\n\n* Added tests for torch.Tensor.backward and torch.Tensor.__getitem__\n\n- Added requires_grad to Tensor since its legal to use requires_grad\n  without it being a nn.Parameter and required by the .backward test\n\n* Rebase off syft_0.3.0\n\n* Re-enabling Windows CI from syft_0.3.0\n\n* Renamed return types of SyNone to new _SyNone class\n\n- SyNone is the singleton and _SyNone is the constructor Class\n\n* Made improvements to get / request blocking mode\n\n- Added customizable timeout\n- Tested on MNIST notebooks\n\n* proto regen noise\n\n* Updated MNIST duet notebooks with blocking get requests\n\n- Added torchvision DataLoader pointers from remote side\n\n* Override __module__ for syft types (#4724)\n\n* Renamed examples/pytorch to examples/duet\n\n- Updated .gitignore for data download dirs\n\n* Added request_name and reason to blocking get request\n\n- Added more verbose output and UID in sy.VERBOSE mode\n\n* Added sy.Module which aims to provide similar functionality to nn.Module\n\n* Updated MNIST notebooks to use sy.Module\n\nCo-authored-by: Anubhav Raj Singh <anubhavraj.08@gmail.com>\nCo-authored-by: Jasopaum <jason.paumier@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def create_torch_ast() -> Globals:",
            "path=method, framework_reference=torch, return_type_name=return_type",
            ")",
            "else:",
            "-            print(f\"Skipping torch.{method} not supported in {TORCH_VERSION}\")",
            "+            pass",
            "+            # TODO: Replace with logging",
            "+            # print(f\"Skipping {method} not supported in {TORCH_VERSION}\")",
            "",
            "for klass in ast.classes:",
            "klass.create_pointer_class()"
        ]
    },
    {
        "number": 6496,
        "comments": "",
        "commit_message": "fixed trpo\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TFOptimizer(Optimizer):",
            "loss = fn_loss(**arguments)",
            "",
            "# The actual tensorflow minimize op.",
            "-        with tf.control_dependencies(control_inputs=(loss,)):",
            "-            # colocate_gradients_with_ops=True",
            "-            applied = self.optimizer.minimize(loss=loss, var_list=variables)",
            "+        applied = self.optimizer.minimize(loss=loss, var_list=variables)",
            "+        # colocate_gradients_with_ops=True",
            "",
            "# Return deltas after actually having change the variables.",
            "with tf.control_dependencies(control_inputs=(applied,)):"
        ]
    },
    {
        "number": 6500,
        "comments": "",
        "commit_message": "Take care with single integers on gpu\n\nSummary:\nPytorch seems to be becoming stricter about integer tensors of shape `(1,)` on GPU and not allowing them to be used as `int`s. For example the following no longer works on pytorch master,\n    foo = torch.tensor([3, 5, 3], device=\"cuda:0\")\n    torch.arange(10) + foo[0]\nbecause this is the sum of tensors on different devices.\n\nHere fix tests which recently broke because of this.\n\nReviewed By: nikhilaravi\n\nDifferential Revision: D21929745\n\nfbshipit-source-id: 25374f70468d1c895372766f1a9dd61df0833957\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestPointMeshDistance(TestCaseMixin, unittest.TestCase):",
            "start = faces_first_idx[i]",
            "end = faces_first_idx[i + 1] if i < N - 1 else faces_packed.shape[0]",
            "",
            "-            min_idx = idx_cuda.cpu()[start:end] - points_first_idx[i]",
            "+            min_idx = idx_cuda.cpu()[start:end] - points_first_idx[i].cpu()",
            "iidx = torch.arange(tris.shape[0], device=device)",
            "min_dist = dists_temp[iidx, min_idx]"
        ]
    },
    {
        "number": 6504,
        "comments": "",
        "commit_message": "fix bug with padding mask + add corresponding test\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def build_lm_labels(sequence, pad_token):",
            "def build_mask(sequence, pad_token):",
            "\"\"\" Builds the mask. The attention mechanism will only attend to positions",
            "with value 1. \"\"\"",
            "-    mask = sequence.clone()",
            "-    mask[mask != pad_token] = 1",
            "-    mask[mask == pad_token] = 0",
            "+    mask = torch.ones_like(sequence)",
            "+    idx_pad_tokens = (sequence == pad_token)",
            "+    mask[idx_pad_tokens] = 0",
            "return mask"
        ]
    },
    {
        "number": 6505,
        "comments": "",
        "commit_message": "fix unused imports and style\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TFModelTesterMixin:",
            "for module_member_name in dir(module)",
            "if module_member_name.endswith(\"MainLayer\")",
            "for module_member in (getattr(module, module_member_name),)",
            "-            if isinstance(module_member, type) and tf.keras.layers.Layer in module_member.__bases__",
            "-            and getattr(module_member, '_keras_serializable', False)",
            "+            if isinstance(module_member, type)",
            "+            and tf.keras.layers.Layer in module_member.__bases__",
            "+            and getattr(module_member, \"_keras_serializable\", False)",
            ")",
            "for main_layer_class in tf_main_layer_classes:",
            "main_layer = main_layer_class(config)"
        ]
    },
    {
        "number": 6508,
        "comments": "",
        "commit_message": "Fix TransfoXL (#9302)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TFTransfoXLMainLayer(tf.keras.layers.Layer):",
            "",
            "# There are `mlen + qlen` steps that can be cached into mems",
            "new_mems = []",
            "-        end_idx = mlen + max(0, qlen)",
            "-        beg_idx = max(0, end_idx - self.mem_len)",
            "+        end_idx = mlen + tf.math.maximum(0, qlen)",
            "+        beg_idx = tf.math.maximum(0, end_idx - tf.convert_to_tensor(self.mem_len))",
            "for i in range(len(hids)):",
            "",
            "cat = tf.concat([mems[i], hids[i]], axis=0)"
        ]
    },
    {
        "number": 6510,
        "comments": "",
        "commit_message": "Update manual download URL in newsroom dataset (#3779)\n\n* Update manual download URL in newsroom dataset\n\n* Update homepage URL\n\n* Fix malformed dataset card\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Newsroom(datasets.GeneratorBasedBuilder):",
            "description=_DESCRIPTION,",
            "features=datasets.Features(features),",
            "supervised_keys=(_DOCUMENT, _SUMMARY),",
            "-            homepage=\"http://lil.datasets.cornell.edu/newsroom/\",",
            "+            homepage=\"https://lil.nlp.cornell.edu/newsroom/index.html\",",
            "citation=_CITATION,",
            ")"
        ]
    },
    {
        "number": 6511,
        "comments": "",
        "commit_message": "Fix Compatibility with Nvidia NGC Containers (#919)\n\nCheck if MPS backend is registered before calling is_available()\n",
        "label": "",
        "answer": "no",
        "change": [
            "if is_torch_available():",
            ")",
            "",
            "if is_torch_higher_equal_than_1_12:",
            "-        torch_device = \"mps\" if torch.backends.mps.is_available() else torch_device",
            "+        # Some builds of torch 1.12 don't have the mps backend registered. See #892 for more details",
            "+        mps_backend_registered = hasattr(torch.backends, \"mps\")",
            "+        torch_device = \"mps\" if (mps_backend_registered and torch.backends.mps.is_available()) else torch_device",
            "",
            "",
            "def get_tests_dir(append_path=None):"
        ]
    },
    {
        "number": 6515,
        "comments": "",
        "commit_message": "fixed global\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "\"torch.Size([5])\"",
            "]",
            "},",
            "-     \"execution_count\": 10,",
            "+     \"execution_count\": 11,",
            "\"metadata\": {},",
            "\"output_type\": \"execute_result\"",
            "}"
        ]
    },
    {
        "number": 6516,
        "comments": "",
        "commit_message": "fix for alphabet import\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class ConvDecoder(AbsDecoder):",
            ")",
            "",
            "def forward(self, input: torch.Tensor, ilens: torch.Tensor):",
            "-        \"\"\"",
            "+        \"\"\"Forward.",
            "+",
            "Args:",
            "-            input (torch.Tensor): spectrum [Batch, T, F]",
            "-            ilens (torch.Tensor): input lengths [Batch]",
            "+        input (torch.Tensor): spectrum [Batch, T, F]",
            "+        ilens (torch.Tensor): input lengths [Batch]",
            "\"\"\"",
            "input = input.transpose(1, 2)",
            "batch_size = input.shape[0]"
        ]
    },
    {
        "number": 6517,
        "comments": "",
        "commit_message": "Fix lint errors\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def from_tfds_to_path(tfds_dataset_name: str, split: str, hub_ds_path: str, batc",
            "return from_tfds(tfds_ds=tfds_ds, ds=ds)",
            "",
            "",
            "-def from_tfds(tfds_ds: (tensorflow.data.Dataset), ds: (Dataset)):",
            "+def from_tfds(tfds_ds: tensorflow.data.Dataset, ds: Dataset):",
            "+    \"\"\"Converts a tfds dataset to hub dataset",
            "+    Args:",
            "+        tfds_ds (tensorflow.data.Dataset): A tfds_dataset object.",
            "+        ds (Dataset) : A Hub dataset object where Tensor will be created.",
            "+    Returns:",
            "+        A hub dataset",
            "+    \"\"\"",
            "tfds_numpy = tfds.as_numpy(tfds_ds)  # Convert `tf.data.Dataset` to Python generator",
            "",
            "for ex in tqdm(tfds_numpy):"
        ]
    },
    {
        "number": 6518,
        "comments": "",
        "commit_message": "Fix PerceiverMLP and test (#16405)\n\nCo-authored-by: Jaesun Park <jaesun.park1@navercorp.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class PerceiverMLP(nn.Module):",
            "self.intermediate_act_fn = ACT2FN[config.hidden_act]",
            "else:",
            "self.intermediate_act_fn = config.hidden_act",
            "-        self.dense2 = nn.Linear(input_size, input_size)",
            "+        self.dense2 = nn.Linear(widening_factor * input_size, input_size)",
            "",
            "def forward(self, hidden_states):",
            "hidden_states = self.dense1(hidden_states)"
        ]
    },
    {
        "number": 6519,
        "comments": "",
        "commit_message": "Use NHWC in batch_norm when shape is 2d. fix #190\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def BatchNorm(x, use_local_stat=None, decay=0.9, epsilon=1e-5,",
            "return tf.identity(xn, name='output')",
            "",
            "",
            "+# TODO support NCHW",
            "@layer_register(log_shape=False)",
            "def BatchRenorm(x, rmax, dmax, decay=0.9, epsilon=1e-5,",
            "use_scale=True, use_bias=True):"
        ]
    },
    {
        "number": 6526,
        "comments": "",
        "commit_message": "Refactor YOLOX (#6443)\n\n* Fix aug test error when the number of prediction bboxes is 0 (#6398)\n\n* Fix aug test error when the number of prediction bboxes is 0\n\n* test\n\n* test\n\n* fix lint\n\n* Support custom pin_memory and persistent_workers\n\n* [Docs] Chinese version of robustness_benchmarking.md (#6375)\n\n* Chinese version of robustness_benchmarking.md\n\n* Update docs_zh-CN/robustness_benchmarking.md\n\nCo-authored-by: RangiLyu <lyuchqi@gmail.com>\n\n* Update docs_zh-CN/robustness_benchmarking.md\n\nCo-authored-by: RangiLyu <lyuchqi@gmail.com>\n\n* Update docs_zh-CN/robustness_benchmarking.md\n\nCo-authored-by: RangiLyu <lyuchqi@gmail.com>\n\n* Update docs_zh-CN/robustness_benchmarking.md\n\nCo-authored-by: RangiLyu <lyuchqi@gmail.com>\n\n* Update docs_zh-CN/robustness_benchmarking.md\n\nCo-authored-by: RangiLyu <lyuchqi@gmail.com>\n\n* Update docs_zh-CN/robustness_benchmarking.md\n\nCo-authored-by: RangiLyu <lyuchqi@gmail.com>\n\n* Update robustness_benchmarking.md\n\n* Update robustness_benchmarking.md\n\n* Update robustness_benchmarking.md\n\n* Update robustness_benchmarking.md\n\n* Update robustness_benchmarking.md\n\n* Update robustness_benchmarking.md\n\nCo-authored-by: RangiLyu <lyuchqi@gmail.com>\n\n* update yolox_s\n\n* update yolox_s\n\n* support dynamic eval interval\n\n* fix some error\n\n* support ceph\n\n* fix none error\n\n* fix batch error\n\n* replace resize\n\n* fix comment\n\n* fix docstr\n\n* Update the link of checkpoints (#6460)\n\n* [Feature]: Support plot confusion matrix. (#6344)\n\n* remove pin_memory\n\n* update\n\n* fix unittest\n\n* update cfg\n\n* fix error\n\n* add unittest\n\n* [Fix] Fix SpatialReductionAttention in PVT. (#6488)\n\n* [Fix] Fix SpatialReductionAttention in PVT\n\n* Add warning\n\n* Save coco summarize print information to logger (#6505)\n\n* Fix type error in 2_new_data_mode (#6469)\n\n* Always map location to cpu when load checkpoint (#6405)\n\n* configs: update groie README (#6401)\n\nSigned-off-by: Leonardo Rossi <leonardo.rossi@unipr.it>\n\n* [Fix] fix config path in docs (#6396)\n\n* [Enchance] Set a random seed when the user does not set a seed. (#6457)\n\n* fix random seed bug\n\n* add comment\n\n* enchance random seed\n\n* rename\n\nCo-authored-by: Haobo Yuan <yuanhaobo@whu.edu.cn>\n\n* [BugFixed] fix wrong trunc_normal_init use (#6432)\n\n* fix wrong trunc_normal_init use\n\n* fix wrong trunc_normal_init use\n\n* fix #6446\n\nCo-authored-by: Uno Wu <st9007a@gmail.com>\nCo-authored-by: Leonardo Rossi <leonardo.rossi@unipr.it>\nCo-authored-by: BigDong <yudongwang@tju.edu.cn>\nCo-authored-by: Haian Huang(\u6df1\u5ea6\u7738) <1286304229@qq.com>\nCo-authored-by: Haobo Yuan <yuanhaobo@whu.edu.cn>\nCo-authored-by: Shusheng Yang <shusheng.yang@qq.com>\n\n* bump version to v2.18.1 (#6510)\n\n* bump version to v2.18.1\n\n* Update changelog.md\n\n* add some comment\n\n* fix some comment\n\n* update readme\n\n* fix lint\n\n* add reduce mean\n\n* update\n\n* update readme\n\n* update params\n\nCo-authored-by: Cedric Luo <luochunhua1996@outlook.com>\nCo-authored-by: RangiLyu <lyuchqi@gmail.com>\nCo-authored-by: Guangchen Lin <347630870@qq.com>\nCo-authored-by: Andrea Panizza <8233615+AndreaPi@users.noreply.github.com>\nCo-authored-by: Uno Wu <st9007a@gmail.com>\nCo-authored-by: Leonardo Rossi <leonardo.rossi@unipr.it>\nCo-authored-by: BigDong <yudongwang@tju.edu.cn>\nCo-authored-by: Haobo Yuan <yuanhaobo@whu.edu.cn>\nCo-authored-by: Shusheng Yang <shusheng.yang@qq.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class SimOTAAssigner(BaseAssigner):",
            "def dynamic_k_matching(self, cost, pairwise_ious, num_gt, valid_mask):",
            "matching_matrix = torch.zeros_like(cost)",
            "# select candidate topk ious for dynamic-k calculation",
            "-        topk_ious, _ = torch.topk(pairwise_ious, self.candidate_topk, dim=0)",
            "+        candidate_topk = min(self.candidate_topk, pairwise_ious.size(0))",
            "+        topk_ious, _ = torch.topk(pairwise_ious, candidate_topk, dim=0)",
            "# calculate dynamic k for each gt",
            "dynamic_ks = torch.clamp(topk_ious.sum(0).int(), min=1)",
            "for gt_idx in range(num_gt):"
        ]
    },
    {
        "number": 6528,
        "comments": "",
        "commit_message": "Fixes to Multihead Attention with LayerNorm and Dropout-Add (#860)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class EncdecMultiheadAttn(nn.Module):",
            "self.register_parameter('lyr_norm_beta_weights', None)",
            "self.lyr_nrm_gamma_weights = None",
            "self.lyr_nrm_beta_weights  = None",
            "-                self.lyr_nrm = torch.nn.LayerNorm(embed_dim)",
            "+                self.lyr_nrm = FusedLayerNorm(embed_dim)",
            "self.reset_parameters()",
            "-",
            "+",
            "if self.include_norm_add:",
            "if   impl == 'fast'    : self.attn_func = fast_encdec_attn_norm_add_func",
            "elif impl == 'default' : self.attn_func = encdec_attn_func"
        ]
    },
    {
        "number": 6529,
        "comments": "",
        "commit_message": "Add test-cuda make target, and fix cuda bugs (#338)\n\n* Add test-cuda make target; fix cuda bugs\n\n* Fix more cuda bugs; distributions now passes\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class AutoRegressiveNN(nn.Module):",
            "",
            "if mask_encoding is None:",
            "# the dependency structure is chosen at random",
            "-            self.mask_encoding = 1 + torch.multinomial(torch.ones(input_dim - 1) / (input_dim - 1),",
            "+            self.mask_encoding = 1 + torch_multinomial(torch.ones(input_dim - 1) / (input_dim - 1),",
            "num_samples=hidden_dim, replacement=True)",
            "else:",
            "# the dependency structure is given by the user"
        ]
    },
    {
        "number": 6534,
        "comments": "",
        "commit_message": "refactor(internal): move model store to common store implementation (#2029)\n\n* delete models/store.py\n\n* feat(internal): add Model class\n\n* feat(internal): expose new model APIs\n\n* fix(internal): allow stores to be initialized with an FS\n\n* update internal modules to use new models\n\n* refactor(internal): refactor frameworks to use new modelstore\n\n* test: fix integration tests\n\n* fix(internal): fix SysPath lint\n\n* fix(internal): remove use of cached property\n\n* feat(internal): add bentoml.models.create\n\n* fix(internal): use shutil.move over os.rename\n\n* chore: patch\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\nci: don't use docker (#2045)\n\n* ci: don't use docker for checks\n* ci: remove docker from Makefile\n\nCo-authored-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\nchore: drop python 3.6 support (#2046)\n\nchore: enable pyright for testing (#2047)\n\nfix: revert&fix typing of di containers (#2044)\n\n* fix: revert&fix typing of di containers\n\n* addpend\n\n* fix: dependencies issues\n\nfix: type\n\nfix: tests\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix: CI for onnxmlir installation\n\nfix: onnxmlir tests and move to attrs\n\nfix(onnxmlir): activate env\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* chore: uses attr.define instead\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* chore: remove poetry\n\nfix: simple_di bug doesn't seem to work with attr.define\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix: flake8\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix: None check behaviour\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* revert\n\nCo-authored-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\nfix: schema typing (#2048)\n\nfix(internal): BentoInfo: don't try to parse creation time again (#2049)\n\n* fix: MLflow models\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix: onnxmlir\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix(wip): paddlepaddle\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix: paddle\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix: spacy\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix: tensorflow\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix: transformers\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* delete models/store.py\n\n* feat(internal): add Model class\n\n* feat(internal): expose new model APIs\n\n* fix(internal): allow stores to be initialized with an FS\n\n* update internal modules to use new models\n\n* refactor(internal): refactor frameworks to use new modelstore\n\n* test: fix integration tests\n\n* fix(internal): fix SysPath lint\n\n* fix(internal): remove use of cached property\n\n* feat(internal): add bentoml.models.create\n\n* fix(internal): use shutil.move over os.rename\n\n* chore: patch\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\nci: don't use docker (#2045)\n\n* ci: don't use docker for checks\n* ci: remove docker from Makefile\n\nCo-authored-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\nchore: drop python 3.6 support (#2046)\n\nchore: enable pyright for testing (#2047)\n\nfix: revert&fix typing of di containers (#2044)\n\n* fix: revert&fix typing of di containers\n\n* addpend\n\n* fix: dependencies issues\n\nfix: type\n\nfix: tests\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix: CI for onnxmlir installation\n\nfix: onnxmlir tests and move to attrs\n\nfix(onnxmlir): activate env\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* chore: uses attr.define instead\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* chore: remove poetry\n\nfix: simple_di bug doesn't seem to work with attr.define\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix: flake8\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix: None check behaviour\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* revert\n\nCo-authored-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\nfix: schema typing (#2048)\n\nfix(internal): BentoInfo: don't try to parse creation time again (#2049)\n\n* fix: MLflow models\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix: onnxmlir\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix(wip): paddlepaddle\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix: paddle\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix: spacy\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix: tensorflow\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix: transformers\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* chore: fix lint errors\n\n* Update test_onnx_impl.py\n\n* chore: ignore typing\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix: CI\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\nCo-authored-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "logger = logging.getLogger(__name__)",
            "def list(  # pylint: disable=redefined-builtin",
            "tag: t.Optional[t.Union[Tag, str]] = None,",
            "_bento_store: \"BentoStore\" = Provide[BentoMLContainer.bento_store],",
            "-) -> t.List[SysPathBento]:",
            "+) -> \"t.List[SysPathBento]\":",
            "return _bento_store.list(tag)"
        ]
    },
    {
        "number": 6535,
        "comments": "",
        "commit_message": "Graph summary, no assertions option, deprecated exception, fixes and improvements\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class NaturalGradient(Optimizer):",
            "]",
            "",
            "# delta' * grad(kldiv)",
            "-                multiply = functools.partial(tf_util.lift_indexedslices, tf.math.multiply)",
            "+                multiply = functools.partial(",
            "+                    tf_util.lift_indexedslices, tf.math.multiply,",
            "+                    with_assertions=self.config.create_tf_assertions",
            "+                )",
            "delta_kldiv_grads = tf.math.add_n(inputs=[",
            "tf.math.reduce_sum(input_tensor=multiply(delta, grad))",
            "for delta, grad in zip(deltas.values(), kldiv_grads)"
        ]
    },
    {
        "number": 6536,
        "comments": "",
        "commit_message": "Fix for issue with Torch Library - \"RuntimeError: some of the strides of a given numpy array are negative. This is currently not supported, but will be added in future releases.\"\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def detect(net, img, device):",
            "# Creates a batch of 1",
            "img = np.expand_dims(img, 0)",
            "",
            "-    img = torch.from_numpy(img).to(device, dtype=torch.float32)",
            "+    img = torch.from_numpy(img.copy()).to(device, dtype=torch.float32)",
            "",
            "return batch_detect(net, img, device)"
        ]
    },
    {
        "number": 6541,
        "comments": "",
        "commit_message": "tl.layers API Refactoring and various modifications (#667)\n\n* Decorators API Refactored\n\n* extra_requires `all`, `all_cpu` and `all_gpu` added\n\n* Error fix\n\n* YAPF Formating Correction\n\n* Test for private method decorator added\n\n* Test Logging Verbosity Fixed to Debug when runned individually\n\n* YAPF corrections applied\n\n* Changelog Added\n\n* Changelog updated\n\n* PR number changed\n\n* First Refactoring Pass done\n\n* cleaning second pass\n\n* Refactoring 3rd pass\n\n* Refactoring 4th Pass\n\n* Code Error fix\n\n* YAPF Formating Fix\n\n* Arguments now using self\n\n* YAPF error correction\n\n* Bug Fix in Decorator\n\n* act name bug fix\n\n* Error Correction\n\n* YAPF formating fix\n\n* Useless tf.identity removed\n\n* Error Fix\n\n* Changelog Updated\n\n* Error fix in tl.activation\n\n* Documentation error fix\n\n* Lazy Import added\n\n* Import Refactoring with LazyImport when necessary\n\n* Changelog Updated\n\n* Gitter Removed\n\n* Fixed proposed by @zsdonghao\n\n* Documentation updated\n\n* Missing requirements added\n\n* Update to TensorLayer 1.8.6rc1\n\n* Requirements error fix\n\n* Docker Files updated\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Simple_MNIST_Test(CustomTestCase):",
            "# the softmax is implemented internally in tl.cost.cross_entropy(y, y_) to",
            "# speed up computation, so we use identity here.",
            "# see tf.nn.sparse_softmax_cross_entropy_with_logits()",
            "-        cls.network = tl.layers.DenseLayer(network, n_units=10, act=tf.identity, name='output')",
            "+        cls.network = tl.layers.DenseLayer(network, n_units=10, name='output')",
            "",
            "# define cost function and metric.",
            "y = cls.network.outputs"
        ]
    },
    {
        "number": 6546,
        "comments": "",
        "commit_message": "[MaskRCNN] Support empty training images (fix #1256,#1230)\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def unpackbits_masks(masks):",
            "unpacked = tf.bitwise.bitwise_and(tf.expand_dims(masks, -1), bits) > 0",
            "unpacked = tf.reshape(",
            "unpacked,",
            "-        tf.concat([tf.shape(masks)[:-1], [-1]], axis=0))",
            "+        tf.concat([tf.shape(masks)[:-1], [8 * tf.shape(masks)[-1]]], axis=0))",
            "return unpacked"
        ]
    },
    {
        "number": 6549,
        "comments": "",
        "commit_message": "Fix `ConditionalDetrForSegmentation` doc example (#20531)\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class ConditionalDetrForSegmentation(ConditionalDetrPreTrainedModel):",
            "...     ConditionalDetrConfig,",
            "...     ConditionalDetrForSegmentation,",
            "... )",
            "-        >>> from transformers.models.conditional_detr.feature_extraction_conditional_detr import rgb_to_id",
            "+        >>> from transformers.image_transforms import rgb_to_id",
            "",
            ">>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"",
            ">>> image = Image.open(requests.get(url, stream=True).raw)"
        ]
    },
    {
        "number": 6551,
        "comments": "",
        "commit_message": "Fix bug in _take_channels (#148)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def _take_channels(*xs, ignore_channels=None):",
            "return xs",
            "else:",
            "channels = [channel for channel in range(xs[0].shape[1]) if channel not in ignore_channels]",
            "-        xs = [torch.index_select(x, dim=1, index=torch.tensor(channels)) for x in xs]",
            "+        xs = [torch.index_select(x, dim=1, index=torch.tensor(channels).to(x.device)) for x in xs]",
            "return xs"
        ]
    },
    {
        "number": 6553,
        "comments": "",
        "commit_message": "Style fix\n",
        "label": "",
        "answer": "no",
        "change": [
            "_MANUAL_VAR_INIT = False",
            "def clear_session():",
            "global _SESSION",
            "global _LEARNING_PHASE",
            "-    reset_default_graph()",
            "+    tf.reset_default_graph()",
            "reset_uids()",
            "_SESSION = None",
            "_LEARNING_PHASE = tf.placeholder(dtype='uint8', name='keras_learning_phase')"
        ]
    },
    {
        "number": 6559,
        "comments": "",
        "commit_message": "fixed positional_dropout_rate\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Transformer(TTSInterface, torch.nn.Module):",
            "]",
            "if self.use_scaled_pos_enc:",
            "report_keys += [",
            "-                {\"encoder_alpha\": self.encoder.embed[-2].alpha.data.item()},",
            "-                {\"decoder_alpha\": self.decoder.embed[-2].alpha.data.item()},",
            "+                {\"encoder_alpha\": self.encoder.embed[-1].alpha.data.item()},",
            "+                {\"decoder_alpha\": self.decoder.embed[-1].alpha.data.item()},",
            "]",
            "self.reporter.report(report_keys)"
        ]
    },
    {
        "number": 6563,
        "comments": "",
        "commit_message": "rnn->encoders, removes _th suffix, same filenames for chainer and pytorch, modify tests to not rely on _th\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def main():",
            "# extract",
            "logging.info('backend = ' + args.backend)",
            "if args.backend == \"pytorch\":",
            "-        from espnet.tts.pytorch.tts_pytorch import decode",
            "+        from espnet.tts.pytorch.tts import decode",
            "decode(args)",
            "else:",
            "raise NotImplementedError(\"Only pytorch is supported.\")"
        ]
    },
    {
        "number": 6566,
        "comments": "",
        "commit_message": "[SGD] Sgd v2 Dataset Integration (#17626)\n\n* wip\n\n* wip\n\n* wip\n\n* draft\n\n* disable tf autosharding\n\n* wip\n\n* wip\n\n* wip\n\n* wip\n\n* add example\n\n* wip\n\n* wip\n\n* wip\n\n* use dataset.split\n\n* add unit tests\n\n* add linear example\n\n* concatenate tensors and fix example\n\n* WIP tune example\n\n* add tensorflow example\n\n* wip\n\n* random_shuffle_each_window\n\n* fault tolerance test\n\n* GPU, examples, CI\n\n* formatting\n\n* fix\n\n* Update python/ray/util/sgd/v2/tests/test_trainer.py\n\nCo-authored-by: matthewdeng <matthew.j.deng@gmail.com>\n\n* wip\n\n* type hints\n\n* wip\n\n* update user guide\n\n* fix\n\n* fix immediate issues\n\n* update example\n\n* update\n\n* fix tune gpu test\n\n* fix resources for smoke test - 1 CPU for dataset tasks\n\n* update tests, docs, examples\n\n* Apply suggestions from code review\n\nCo-authored-by: Clark Zinzow <clarkzinzow@gmail.com>\n\n* address comments\n\n* add warning\n\n* fix tests\n\n* minor doc updates\n\n* update example in doc\n\n* configure tests\n\n* Update doc/source/raysgd/v2/user_guide.rst\n\nCo-authored-by: Clark Zinzow <clarkzinzow@gmail.com>\n\n* Update python/ray/data/dataset.py\n\nCo-authored-by: matthewdeng <matthew.j.deng@gmail.com>\n\n* fix docstring\n\nCo-authored-by: Matthew Deng <matthew.j.deng@gmail.com>\nCo-authored-by: matthewdeng <matt@anyscale.com>\nCo-authored-by: Clark Zinzow <clarkzinzow@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_to_torch_feature_columns(ray_start_regular_shared):",
            "iterations = []",
            "",
            "for batch in iter(torchd):",
            "-        iterations.append(torch.cat((*batch[0], batch[1]), axis=1).numpy())",
            "+        iterations.append(torch.cat((batch[0], batch[1]), dim=1).numpy())",
            "combined_iterations = np.concatenate(iterations)",
            "assert np.array_equal(df.values, combined_iterations)"
        ]
    },
    {
        "number": 6570,
        "comments": "",
        "commit_message": "Module namespace cleanup for v2.0 (#3875)\n\n* Imports cleaning\n\n* Small change\n\n* Remove unused methods\n\n* Small fix\n\n* Additional fix\n\n* Final fix\n\n* Fix benchmark test\n\n* Fix benchmark test #2\n",
        "label": "",
        "answer": "no",
        "change": [
            "class DownloadManager:",
            "\"\"\"",
            "Ship the files using Beam FileSystems to the pipeline temp dir.",
            "\"\"\"",
            "-        from datasets.utils.beam_utils import upload_local_to_remote",
            "+        from .beam_utils import upload_local_to_remote",
            "",
            "remote_dir = pipeline._options.get_all_options().get(\"temp_location\")",
            "if remote_dir is None:"
        ]
    },
    {
        "number": 6573,
        "comments": "",
        "commit_message": "fix incorrect class loss calculation. cross entropy being averaged twice due to PyTorch defaults\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class YOLOLayer(nn.Module):",
            "loss_conf = self.bce_loss(pred_conf[conf_mask_false], tconf[conf_mask_false]) + self.bce_loss(",
            "pred_conf[conf_mask_true], tconf[conf_mask_true]",
            ")",
            "-            loss_cls = (1 / nB) * self.ce_loss(pred_cls[mask], torch.argmax(tcls[mask], 1))",
            "+            loss_cls = self.ce_loss(pred_cls[mask], torch.argmax(tcls[mask], 1))",
            "loss = loss_x + loss_y + loss_w + loss_h + loss_conf + loss_cls",
            "",
            "return ("
        ]
    },
    {
        "number": 6578,
        "comments": "",
        "commit_message": "Refactor `TorchElasticEnvironment.detect` to use `torch.distributed.is_torchelastic_launched` (#12376)\n\n* Refactor TorchElasticEnvironment.detect to use native utility from torch.distributed\n\n* fix version and tests\n\n* fix version\n\n* Update tests/accelerators/test_accelerator_connector.py\n\nCo-authored-by: Adrian W\u00e4lchli <aedu.waelchli@gmail.com>\n\nCo-authored-by: Adrian W\u00e4lchli <aedu.waelchli@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TorchElasticEnvironment(ClusterEnvironment):",
            "@staticmethod",
            "def detect() -> bool:",
            "\"\"\"Returns ``True`` if the current process was launched using the torchelastic command.\"\"\"",
            "+        if _TORCH_GREATER_EQUAL_1_9_1:",
            "+            return torch.distributed.is_torchelastic_launched()",
            "required_env_vars = {\"RANK\", \"GROUP_RANK\", \"LOCAL_RANK\", \"LOCAL_WORLD_SIZE\"}",
            "return required_env_vars.issubset(os.environ.keys())"
        ]
    },
    {
        "number": 6580,
        "comments": "",
        "commit_message": "fix cosine  bbeta schedule, thanks to @Zhengxinyang\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def cosine_beta_schedule(timesteps, s = 0.008):",
            "as proposed in https://openreview.net/forum?id=-NEXDKk8gZ",
            "\"\"\"",
            "steps = timesteps + 1",
            "-    x = torch.linspace(0, steps, steps)",
            "-    alphas_cumprod = torch.cos(((x / steps) + s) / (1 + s) * torch.pi * 0.5) ** 2",
            "+    x = torch.linspace(0, timesteps, steps)",
            "+    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * torch.pi * 0.5) ** 2",
            "alphas_cumprod = alphas_cumprod / alphas_cumprod[0]",
            "betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])",
            "return torch.clip(betas, 0, 0.999)"
        ]
    },
    {
        "number": 6585,
        "comments": "",
        "commit_message": "Fix IterableDataset with __len__ in Trainer (#8095)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Trainer:",
            "dataset.set_format(type=dataset.format[\"type\"], columns=columns)",
            "",
            "def _get_train_sampler(self) -> Optional[torch.utils.data.sampler.Sampler]:",
            "-        if not isinstance(self.train_dataset, collections.abc.Sized):",
            "+        if isinstance(self.train_dataset, torch.utils.data.IterableDataset) or not isinstance(",
            "+            self.train_dataset, collections.abc.Sized",
            "+        ):",
            "return None",
            "elif is_torch_tpu_available():",
            "return get_tpu_sampler(self.train_dataset)"
        ]
    },
    {
        "number": 6586,
        "comments": "",
        "commit_message": "[Fix] Fix no attribute 'samples_per_gpu' bug in `auto_scale_lr` (#7862)\n\n* Fix\n\n* update docstring\n",
        "label": "",
        "answer": "no",
        "change": [
            "def auto_scale_lr(cfg, distributed, logger):",
            "num_gpus = len(cfg.gpu_ids)",
            "",
            "# calculate the batch size",
            "-    batch_size = num_gpus * cfg.data.samples_per_gpu",
            "-    logger.info(f'You are using {num_gpus} GPU(s) '",
            "-                f'and {cfg.data.samples_per_gpu} samples per GPU. '",
            "-                f'Total batch size is {batch_size}.')",
            "+    samples_per_gpu = cfg.data.train_dataloader.samples_per_gpu",
            "+    batch_size = num_gpus * samples_per_gpu",
            "+    logger.info(f'Training with {num_gpus} GPU(s) with {samples_per_gpu} '",
            "+                f'samples per GPU. The total batch size is {batch_size}.')",
            "",
            "if batch_size != base_batch_size:",
            "# scale LR with"
        ]
    },
    {
        "number": 6588,
        "comments": "",
        "commit_message": "[Train] Strip \"module.\" from state dict (#30705)\n\nThis PR adds logic to automatically strip the \"module.\" prefix from a user-saved state dict in TorchCheckpoint, which is present if a user obtains the state dict from a DistributedDataParallel module directly. We already obtain the underlying module if a user saves the model object, so this merely makes the logic consistent.\n\nThis PR also edits our examples to remove instances where this operation was conducted in the example itself. This led to issues if train.torch.prepare_model was used with num_workers=1 (eg. on Google Colab), as the module was not wrapped around, thus leading to the .module attribute being missing.\n\nSigned-off-by: Antoni Baum <antoni.baum@protonmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "\"\\n\",",
            "\"        # Checkpoint model after every epoch.\\n\",",
            "\"        state_dict = model.state_dict()\\n\",",
            "-    \"        consume_prefix_in_state_dict_if_present(state_dict, \\\"module.\\\")\\n\",",
            "\"        checkpoint = Checkpoint.from_dict(dict(model=state_dict))\\n\",",
            "\"        session.report({\\\"loss\\\": running_loss}, checkpoint=checkpoint)\"",
            "]"
        ]
    },
    {
        "number": 6589,
        "comments": "",
        "commit_message": "bug fix using dataset for text input\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Adapter(serializable.Serializable):",
            "tf.data.Dataset. The converted dataset.",
            "\"\"\"",
            "if isinstance(dataset, np.ndarray):",
            "-            dataset = tf.data.Dataset.from_tensor_slices(",
            "-                dataset.astype(np.float32))",
            "+            dataset = tf.data.Dataset.from_tensor_slices(dataset)",
            "return data_utils.batch_dataset(dataset, self.batch_size)",
            "",
            "def fit(self, dataset):"
        ]
    },
    {
        "number": 6591,
        "comments": "",
        "commit_message": "fix temp_seed for tensorflow 2.4.0\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def temp_seed(seed: int, set_pytorch=False, set_tensorflow=False):",
            "if not tf.executing_eagerly():",
            "raise ValueError(\"Setting random seed for TensorFlow is only available in eager mode\")",
            "",
            "-        tf_context = tf.python.context.context()  # eager mode context",
            "+        tf_context = tfpy.context.context()  # eager mode context",
            "tf_seed = tf_context._seed",
            "tf_rng_initialized = hasattr(tf_context, \"_rng\")",
            "if tf_rng_initialized:"
        ]
    },
    {
        "number": 6594,
        "comments": "",
        "commit_message": "Fix CUDA error on illegal memory access.\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def convert_points_from_homogeneous(",
            "# https://github.com/opencv/opencv/pull/14411/files",
            "mask: torch.Tensor = torch.abs(z_vec) > eps",
            "scale: torch.Tensor = torch.ones_like(z_vec).masked_scatter_(",
            "-        mask, torch.tensor(1.0) / z_vec[mask])",
            "+        mask, torch.tensor(1.0).to(points.device) / z_vec[mask])",
            "",
            "return scale * points[..., :-1]"
        ]
    },
    {
        "number": 6596,
        "comments": "",
        "commit_message": "fix for issue #211\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class WaveNetModel(object):",
            "",
            "# In this case, the number of global_embedding channels must be",
            "# equal to the the last dimension of the global_condition tensor.",
            "-            gc_batch_rank = len(global_condition.get_shape)",
            "+            gc_batch_rank = len(global_condition.get_shape())",
            "dims_match = (global_condition.get_shape()[gc_batch_rank - 1] ==",
            "self.global_condition_channels)",
            "if not dims_match:",
            "raise ValueError('Shape of global_condition {} does not'",
            "' match global_condition_channels {}.'.",
            "-                                 format(self.global_condition.get_shape(),",
            "+                                 format(global_condition.get_shape(),",
            "self.global_condition_channels))",
            "embedding = global_condition"
        ]
    },
    {
        "number": 6600,
        "comments": "",
        "commit_message": "Fix crash in mixed precision stateful RNNs.\n\nFixes https://github.com/keras-team/keras/issues/15140.\n\nThe issue was self.states is created as a float32 variable, which causes TypeErrors, so I cast self.states. Alternatively, self.states could be created as a float16 variable, which would simply the logic and have better performance, but this could break checkpoint compatibility between float32 and mixed precision.\n\nPiperOrigin-RevId: 390475709\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class LSTM(recurrent.DropoutRNNCellMixin, recurrent.LSTM):",
            "",
            "if self.stateful:",
            "updates = [",
            "-          tf.compat.v1.assign(self_state, state)",
            "+          tf.compat.v1.assign(self_state, tf.cast(state, self_state.dtype))",
            "for self_state, state in zip(self.states, states)",
            "]",
            "self.add_update(updates)"
        ]
    },
    {
        "number": 6604,
        "comments": "",
        "commit_message": "RTD Compilation Fix (#663)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "if 'TENSORLAYER_PACKAGE_BUILDING' not in os.environ:",
            "\" - `pip install --upgrade tensorflow-gpu`\"",
            ")",
            "",
            "-    if tensorflow.__version__ < \"1.6.0\":",
            "+    if tensorflow.__version__ < \"1.6.0\" and os.environ.get('READTHEDOCS', None) != 'True':",
            "raise RuntimeError(",
            "\"TensorLayer does not support Tensorflow version older than 1.6.0.\\n\"",
            "\"Please update Tensorflow with:\\n\""
        ]
    },
    {
        "number": 6606,
        "comments": "",
        "commit_message": "speed-reproducibility fix #17\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "import torch.nn.functional as F",
            "def init_seeds(seed=0):",
            "torch.manual_seed(seed)",
            "",
            "-    # Reduce randomness (may be slower on Tesla GPUs) # https://pytorch.org/docs/stable/notes/randomness.html",
            "-    if seed == 0:",
            "+    # Speed-reproducibility tradeoff https://pytorch.org/docs/stable/notes/randomness.html",
            "+    if seed == 0:  # slower, more reproducible",
            "+        cudnn.deterministic = True",
            "+        cudnn.benchmark = False",
            "+    else:  # faster, less reproducible",
            "cudnn.deterministic = False",
            "cudnn.benchmark = True"
        ]
    },
    {
        "number": 6607,
        "comments": "",
        "commit_message": "fix self conditioning shape in diffusion prior\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class DiffusionPriorNetwork(nn.Module):",
            "",
            "# setup self conditioning",
            "",
            "-        self_cond = None",
            "if self.self_cond:",
            "-            self_cond = default(self_cond, lambda: torch.zeros(batch, 1, self.dim, device = device, dtype = dtype))",
            "+            self_cond = default(self_cond, lambda: torch.zeros(batch, self.dim, device = device, dtype = dtype))",
            "+            self_cond = rearrange(self_cond, 'b d -> b 1 d')",
            "",
            "# in section 2.2, last paragraph",
            "# \"... consisting of encoded text, CLIP text embedding, diffusion timestep embedding, noised CLIP image embedding, final embedding for prediction\""
        ]
    },
    {
        "number": 6608,
        "comments": "",
        "commit_message": "Fix issue calling TextVectorization on non-tensor input\n\ntf.keras.layers.TextVectorization(vocabulary=[\"foo\"])([\"foo\"]) would fail with an error.\nThis was due to checking rank of input during build. We should instead check rank during\ncall, after converting to tensor.\n\nPiperOrigin-RevId: 406160462\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class IntegerLookupSavingTest(keras_parameterized.TestCase,",
            "",
            "",
            "if __name__ == \"__main__\":",
            "+  # IntegerLookup is only exported as a TF2 API.",
            "+  tf.compat.v1.enable_v2_behavior()",
            "tf.test.main()"
        ]
    },
    {
        "number": 6609,
        "comments": "",
        "commit_message": "Added random param gen tests. Added device awareness for parameter generators. (#757)\n\n* Updated tests and docs for ColorJitter\n\n* Fixed perspective random gen and tests.\n\n* Added tests for randomAffineGen\n\n* Added tests for random rotation gen\n\n* Added random crop gen tests\n\n* Added random crop size gen tests\n\n* Added random rectangle gen tests\n\n* Added tests for center crop\n\n* Added tests for random motion blur gen\n\n* Added tests for random solarize gen\n\n* Added posterize gen tests\n\n* Added random sharpness gen tests\n\n* Fixed tests\n\n* Updated test_augmentation\n\n* Tests fix\n\n* Additional fix\n\n* Added tests for mix random gen\n\n* Added 3D perspective/affine tests\n\n* Added 3D rotation gen\n\n* Added the rest 3D augmentation tests\n\n* Some format fixes\n\n* Bug fixes\n\n* Fixed #761\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestRandomCutMix:",
            "expected = torch.tensor([[[[1., 0., 0., 1.],",
            "[1., 0., 0., 1.],",
            "[1., 1., 1., 1.]]],",
            "-                                 [[[1., 1., 0., 0.],",
            "-                                   [1., 1., 0., 0.],",
            "+                                 [[[0., 1., 1., 0.],",
            "+                                   [0., 1., 1., 0.],",
            "[0., 0., 0., 0.]]]], device=device, dtype=dtype)",
            "",
            "out_image, out_label = f(input, label)"
        ]
    },
    {
        "number": 6611,
        "comments": "",
        "commit_message": "Remove type ignore from the codebase (#2030)\n\n* Remove type ignore from the codebase\n\n* undo `NamedTuple` property with `List[Self]`\n\n- This is causing `Segmentation fault (core dumped)` when running `mypy --cobertura-xml-report ./`\n\n* add TODO for next mypy release\n\n* fix typo\n\n* fix F401\n",
        "label": "",
        "answer": "no",
        "change": [
            "class MixAugmentationBaseV2(_BasicAugmentationBase):",
            "in_tensor: Tensor = input[in_tensor_idx]",
            "in_tensor = self.transform_tensor(in_tensor)",
            "self._params = self.forward_parameters(in_tensor.shape)",
            "-            self._params.update({\"dtype\": torch.tensor(DType.get(in_tensor.dtype).value)})",
            "+            self._params.update({\"dtype\": tensor(DType.get(in_tensor.dtype).value)})",
            "else:",
            "self._params = params",
            "",
            "-        outputs = []",
            "+        outputs: List[Tensor] = []",
            "for dcate, _input in zip(keys, input):",
            "output: Tensor",
            "if dcate == DataKey.INPUT:"
        ]
    },
    {
        "number": 6613,
        "comments": "",
        "commit_message": "pylint -> flake8 (#3288)\n\n* pylint\n\n* update pylint\n\n* undo a lot of the raise / else\n\n* add bound on typed-ast\n\n* first mypy fixes\n\n* new flag\n\n* fix mypy errors\n\n* requirements.txt\n\n* pylint -> flake8\n\n* mypy 0.720 -> mypy 0.730\n\n* add back erroneously removed initial newline\n\n* remove .pylintrc\n\n* remove pylintrc from Dockerfile\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TokenCharactersEncoder(TokenEmbedder):",
            "self._dropout = lambda x: x",
            "",
            "def get_output_dim(self) -> int:",
            "-        return self._encoder._module.get_output_dim()  # pylint: disable=protected-access",
            "+        return self._encoder._module.get_output_dim()",
            "",
            "-    def forward(self, token_characters: torch.Tensor) -> torch.Tensor:  # pylint: disable=arguments-differ",
            "+    def forward(self, token_characters: torch.Tensor) -> torch.Tensor:",
            "mask = (token_characters != 0).long()",
            "return self._dropout(self._encoder(self._embedding(token_characters), mask))",
            "",
            "# The setdefault requires a custom from_params",
            "@classmethod",
            "def from_params(cls, vocab: Vocabulary, params: Params) -> 'TokenCharactersEncoder':  # type: ignore",
            "-        # pylint: disable=arguments-differ",
            "+",
            "embedding_params: Params = params.pop(\"embedding\")",
            "# Embedding.from_params() uses \"tokens\" as the default namespace, but we need to change",
            "# that to be \"token_characters\" by default. If num_embeddings is present, set default namespace"
        ]
    },
    {
        "number": 6615,
        "comments": "",
        "commit_message": "fixed fake node bug\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Net(nn.Module):",
            "data, _ = voxel_max_pool(",
            "data, 14, origin=0, fake_nodes=True, transform=transform)",
            "",
            "-        x = data.input.view(-1, 4 * 64)",
            "+        x = data.input.view(-1, 4 * 128)",
            "x = F.elu(self.fc1(x))",
            "x = F.dropout(x, training=self.training)",
            "x = self.fc2(x)"
        ]
    },
    {
        "number": 6617,
        "comments": "",
        "commit_message": "fix torch.nonzero usage in pytorch 1.5 (#2602)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class MaskedConv2dFunction(Function):",
            "out_w = int(",
            "math.floor((features.size(3) + 2 * pad_w -",
            "(kernel_h - 1) - 1) / stride_w + 1))",
            "-        mask_inds = torch.nonzero(mask[0] > 0)",
            "+        mask_inds = torch.nonzero(mask[0] > 0, as_tuple=False)",
            "output = features.new_zeros(batch_size, out_channel, out_h, out_w)",
            "if mask_inds.numel() > 0:",
            "mask_h_idx = mask_inds[:, 0].contiguous()"
        ]
    },
    {
        "number": 6619,
        "comments": "",
        "commit_message": "pylint -> flake8 (#3288)\n\n* pylint\n\n* update pylint\n\n* undo a lot of the raise / else\n\n* add bound on typed-ast\n\n* first mypy fixes\n\n* new flag\n\n* fix mypy errors\n\n* requirements.txt\n\n* pylint -> flake8\n\n* mypy 0.720 -> mypy 0.730\n\n* add back erroneously removed initial newline\n\n* remove .pylintrc\n\n* remove pylintrc from Dockerfile\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class BertPooler(Seq2VecEncoder):",
            "def get_output_dim(self) -> int:",
            "return self._embedding_dim",
            "",
            "-    def forward(self, tokens: torch.Tensor, mask: torch.Tensor = None):  # pylint: disable=arguments-differ,unused-argument",
            "+    def forward(self, tokens: torch.Tensor, mask: torch.Tensor = None):",
            "pooled = self.pooler(tokens)",
            "pooled = self._dropout(pooled)",
            "return pooled"
        ]
    },
    {
        "number": 6620,
        "comments": "",
        "commit_message": "feat: multilingual and english BERT-based models for classification and SQuAD (#742)\n\n* feat: add multi_squad_retr dataset\n\n* feat: add config file\n\n* feat: add multi_squad_ru_retr dataset\n\n* fix: paths and squad datareaders\n\n* fix: squad preprocess\n\n* feat: add context from squad rate sampling ratio\n\n* fix: dataset reader\n\n* fix: squad rate attr in squad_iterator\n\n* feat: add link to trained noans_ru model\n\n* feat: create odqa retr noans config, update top n for ru tfidf ranker\n\n* fix: upd metrics docs\n\n* feat: add bert for classification tasks\n\n* fix: remove squad model from registry\n\n* fix: add bert preprocessor\n\n* fix: upd names in configs\n\n* fix: set ru tfidf ranker top_n to 5\n\n* fix: set paragraphs false in odqa\n\n* fix: multilabel classification with bert\n\n* fix: bert dropout config names\n\n* feat: add AdamWeighDecay optimizer for bert\n\n* config for rusentiment bert 1m ckpt\n\n* chore: bert configs\n\n* feat: add bert model for squad\n\n* fix: squad bert config\n\n* feat: add loading pretrained bert for squad\n\n* feat: add multilingual bert for sbersquad\n\n* feat: add url arg to squad dataset reader\n\n* fix: bert answer postprocessor\n\n* feat&fix: add bert squad infer model and config fixes\n\n* chore: add squad bert infer config\n\n* feat: add batching to bert inference model\n\n* feat: add squad with rubert and odqa configs\n\n* fix: squad rubert config\n\n* chore: add ruodqa evaluation config\n\n* fix: squad iterator empty batch\n\n* chore: add one more bert based squad config\n\n* feat: add noans score and model trained on retr dump\n\n* chore: add inference config for bert on noans\n\n* feat: add bert_ner (with bugs for now)\n\n* fix: attribute assign in classifier\n\n* fix: Adam inited twice\n\n* fix: ner bert preprocessor\n\n* refactor&docs: add docs for bert preprocessor\n\n* feat: add bert_ner for inference\n\n* docs: add bert preprocessor call method docs\n\n* docs: bert squad model\n\n* feat: add parametrization of taken encoder layers\n\n* fix: rubert download link\n\n* docs: train_on_batch and call for bert squad model\n\n* docs: add docstring for BertSQuADInferModel\n\n* fix: sigmoid -> softmax output\n\n* docs: upd docstring\n\n* fix: list -> tuple default value\n\n* docs: add docstrings for bert classifier\n\n* chore: rm configs\n\n* feat: paraphraser config, model and scores\n\n* feat: rusentiment with multi-lingual bert\n\n* feat: rusentiment with multi-lingual bert\n\n* feat: insults kaggle on English BERT\n\n* chore: remove configs\n\n* docs: add ru squad results\n\n* feat: add bert models to tests\n\n* docs: upd docs\n\n* feat: upd configs and tests\n\n* feat: rm bert configs\n\n* refactor: rm debug logging\n\n* fix: paraphraser dataset reader\n\n* docs: update bert_ner docstring\n\n* fix: restore tests for squad noans model\n\n* fix: paraphraser tests\n\n* fix: add bert_dp requirement to squad_bert config\n\n* fix: download path for model files\n\n* fix: rusentiment tests\n\n* fix: import order\n\n* fix: remove extra tabulation\n\n* refactor: rename squad metrics\n\n* refactor: url argument in squad_dataset_reader has higher priority than dataset\n\n* refactor: remove process_event method from bert models\n\n* feat: add bert-base model for EN SQuAD\n\n* feat: add bert dstc2 model\n\n* fix: squad bert infer config\n\n* docs: add docs for NerPreprocessor\n\n* feat: give higher priority to url arg in multi_squad_dataset_reader\n\n* fix: bert NER preprocessor\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TFModel(NNModel, metaclass=TfModelMeta):",
            "variables_to_train.extend(tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope_name))",
            "",
            "if optimizer is None:",
            "-                optimizer = tf.train.AdamOptimizer(learning_rate, **kwargs)",
            "+                optimizer = tf.train.AdamOptimizer",
            "",
            "# For batch norm it is necessary to update running averages",
            "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)"
        ]
    },
    {
        "number": 6623,
        "comments": "",
        "commit_message": "Unittest for Deformable Convolution Added + Issue Fix #572 (#573)\n\n* Unittest for Deformable Convolution Added\n\n* Issue #572 fixed\n\n* Changelog Refactored\n\n* Fix for PyUP and Tensorflow\n\n* Stale Bot Configuration Added\n\n* Danger Config Error Fix\n\n* Changelog Updated\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class DeformableConv2d(Layer):",
            "name='b_deformableconv2d', shape=(shape[-1]), initializer=b_init, dtype=LayersConfig.tf_dtype,",
            "**b_init_args",
            ")",
            "-                tf.reshape()",
            "+",
            "self.outputs = tf.reshape(",
            "tensor=act(tf.nn.conv3d(input_deform, W, strides=[1, 1, 1, 1, 1], padding='VALID', name=None) + b),",
            "shape=(tf.shape(self.inputs)[0], input_h, input_w, shape[-1])"
        ]
    },
    {
        "number": 6624,
        "comments": "",
        "commit_message": "Fix half of the tests that are failing in v1.\n\nMost of them are failing since the actual code are expected to run only in v2 (eg need eager/resource variable, or certain fix we added is only applied to the v2 code path).\n\nPiperOrigin-RevId: 394765626\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class ParameterServerCustomTrainingLoopTest(tf.test.TestCase):",
            "",
            "",
            "if __name__ == \"__main__\":",
            "-  tf.__internal__.distribute.multi_process_runner.test_main()",
            "+  if tf.__internal__.tf2.enabled():",
            "+    tf.__internal__.distribute.multi_process_runner.test_main()"
        ]
    },
    {
        "number": 6628,
        "comments": "",
        "commit_message": "support for TF 1.13, improved while loops, fixed lstm problem\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TFOptimizer(Optimizer):",
            "",
            "variables.extend(self.optimizer.variables())",
            "",
            "-        # variables.extend(",
            "-        #     self.optimizer._slots[slot][key] for slot in sorted(self.optimizer._slots)",
            "-        #     for key in sorted(self.optimizer._slots[slot])",
            "-        # )",
            "-",
            "-        # if isinstance(self.optimizer, (tf.train.AdamOptimizer, tf.contrib.opt.NadamOptimizer)):",
            "-        #     variables.extend(self.optimizer._get_beta_accumulators())",
            "-",
            "return variables"
        ]
    },
    {
        "number": 6629,
        "comments": "",
        "commit_message": "formatting, fixes, etc\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Normalize(Preprocessor):",
            "\"\"\"",
            "",
            "def __init__(self, scope='normalize', summary_labels=()):",
            "-        super(Normalize).__init__(scope, summary_labels)",
            "+        super(Normalize, self).__init__(scope=scope, summary_labels=summary_labels)",
            "",
            "def tf_process(self, tensor):",
            "# Min/max across every axis except batch dimension.",
            "-        min = tf.reduce_min(input_tensor=tensor, axis=np.arange(1, util.rank(tensor)))",
            "-        max = tf.reduce_max(input_tensor=tensor, axis=np.arange(1, util.rank(tensor)))",
            "+        min_value = tf.reduce_min(input_tensor=tensor, axis=np.arange(1, util.rank(tensor)))",
            "+        max_value = tf.reduce_max(input_tensor=tensor, axis=np.arange(1, util.rank(tensor)))",
            "",
            "-        return (tensor - min) / (max - min + util.epsilon)",
            "+        return (tensor - min_value) / (max_value - min_value + util.epsilon)"
        ]
    },
    {
        "number": 6631,
        "comments": "",
        "commit_message": "fixed lint errors\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def random_uniform(",
            "rand_range = high - low",
            "if shape is None:",
            "shape = []",
            "-    return torch.rand(shape, device=default_device(device), dtype=dtype) * rand_range + low",
            "+    return (",
            "+        torch.rand(shape, device=default_device(device), dtype=dtype) * rand_range + low",
            "+    )",
            "",
            "",
            "def random_normal("
        ]
    },
    {
        "number": 6632,
        "comments": "",
        "commit_message": "More TF int dtype fixes (#20384)\n\n* Add a test to ensure int dummy inputs are int64\n\n* Move the test into the existing int64 test and update a lot of existing dummies\n\n* Fix remaining dummies\n\n* Fix remaining dummies\n\n* Test for int64 serving sigs as well\n\n* Update core tests to use tf.int64\n\n* Add better messages to the assertions\n\n* Update all serving sigs to int64\n\n* More sneaky hiding tf.int32s\n\n* Add an optional int32 signature in save_pretrained\n\n* make fixup\n\n* Add Amy's suggestions\n\n* Switch all serving sigs back to tf.int32\n\n* Switch all dummies to tf.int32\n\n* Adjust tests to check for tf.int32 instead of tf.int64\n\n* Fix base dummy_inputs dtype\n\n* Start casting to tf.int32 in input_processing\n\n* Change dtype for unpack_inputs test\n\n* Add proper tf.int32 test\n\n* Make the alternate serving signature int64\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TF{{cookiecutter.camelcase_modelname}}ForMultipleChoice(TF{{cookiecutter.c",
            "Returns:",
            "tf.Tensor with dummy inputs",
            "\"\"\"",
            "-        return {\"input_ids\": tf.constant(MULTIPLE_CHOICE_DUMMY_INPUTS)}",
            "+        return {\"input_ids\": tf.constant(MULTIPLE_CHOICE_DUMMY_INPUTS, dtype=tf.int64)}",
            "",
            "@unpack_inputs",
            "@add_start_docstrings_to_model_forward({{cookiecutter.uppercase_modelname}}_INPUTS_DOCSTRING.format(\"batch_size, num_choices, sequence_length\"))"
        ]
    },
    {
        "number": 6633,
        "comments": "",
        "commit_message": "Quick fix :) (#606)\n\n* Changing the name\n\n* style + quality\n\n* update doc and logo\n\n* clean up\n\n* circle-CI on the branche for now\n\n* fix daily dialog dataset\n\n* fix urls\n\nCo-authored-by: Quentin Lhoest <lhoest.q@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Emotion(nlp.GeneratorBasedBuilder):",
            "valid_path = dl_manager.download_and_extract(_VALIDATION_DOWNLOAD_URL)",
            "test_path = dl_manager.download_and_extract(_TEST_DOWNLOAD_URL)",
            "return [",
            "-            nlp.SplitGenerator(name=nlp.Split.TRAIN, gen_kwargs={\"filepath\": train_path}),",
            "-            nlp.SplitGenerator(name=nlp.Split.VALIDATION, gen_kwargs={\"filepath\": valid_path}),",
            "-            nlp.SplitGenerator(name=nlp.Split.TEST, gen_kwargs={\"filepath\": test_path}),",
            "+            datasets.SplitGenerator(name=datasets.Split.TRAIN, gen_kwargs={\"filepath\": train_path}),",
            "+            datasets.SplitGenerator(name=datasets.Split.VALIDATION, gen_kwargs={\"filepath\": valid_path}),",
            "+            datasets.SplitGenerator(name=datasets.Split.TEST, gen_kwargs={\"filepath\": test_path}),",
            "]",
            "",
            "def _generate_examples(self, filepath):"
        ]
    },
    {
        "number": 6637,
        "comments": "",
        "commit_message": "[RLlib] Tf2x preparation; part 2 (upgrading `try_import_tf()`). (#9136)\n\n* WIP.\n\n* Fixes.\n\n* LINT.\n\n* WIP.\n\n* WIP.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* WIP.\n\n* Fixes.\n\n* Test\n\n* Fix.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* LINT.\n",
        "label": "",
        "answer": "no",
        "change": [
            "def postprocess_trajectory(policy,",
            "",
            "def choose_optimizer(policy, config):",
            "if policy.config[\"opt_type\"] == \"adam\":",
            "-        return tf.train.AdamOptimizer(policy.cur_lr)",
            "+        return tf1.train.AdamOptimizer(policy.cur_lr)",
            "else:",
            "-        return tf.train.RMSPropOptimizer(policy.cur_lr, config[\"decay\"],",
            "-                                         config[\"momentum\"], config[\"epsilon\"])",
            "+        return tf1.train.RMSPropOptimizer(",
            "+            policy.cur_lr,",
            "+            config[\"decay\"], config[\"momentum\"], config[\"epsilon\"])",
            "",
            "",
            "def clip_gradients(policy, optimizer, loss):"
        ]
    },
    {
        "number": 6638,
        "comments": "",
        "commit_message": "lintfixbot: Auto-commit fixed lint errors in codebase\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def real(",
            "",
            "",
            "def isposinf(",
            "-        x: Union[torch.Tensor],",
            "-        /,",
            "-        *,",
            "-        out: Optional[torch.Tensor] = None,",
            "+    x: Union[torch.Tensor],",
            "+    /,",
            "+    *,",
            "+    out: Optional[torch.Tensor] = None,",
            ") -> torch.Tensor:",
            "return torch.isposinf(x)"
        ]
    },
    {
        "number": 6640,
        "comments": "",
        "commit_message": "saver fix\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class ValueFunction(object):",
            "\"\"\"",
            "",
            "self.session = tf.Session()",
            "-        # TODO fix",
            "-        #self.saver = tf.train.Saver()",
            "+        self.saver = None",
            "",
            "def get_action(self, state):",
            "raise NotImplementedError"
        ]
    },
    {
        "number": 6641,
        "comments": "",
        "commit_message": "lintfixbot: Auto-commit fixed lint errors in codebase\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def unravel_index(",
            "for dim in reversed(shape):",
            "output.append(temp % dim)",
            "temp = temp // dim",
            "-    ret= tf.constant(reversed(output), dtype=tf.int32)",
            "+    ret = tf.constant(reversed(output), dtype=tf.int32)",
            "return tuple(ret)"
        ]
    },
    {
        "number": 6644,
        "comments": "",
        "commit_message": "Use f-strings in the dataset scripts (#3291)\n\n* Finishes #3257\n\nUsed f-strings to format the .py files in the dataset folder\n\n* Fix style\n\n* Fix hkcancor dataset\n\nCo-authored-by: Mario \u0160a\u0161ko <mario@huggingface.co>\nCo-authored-by: mariosasko <mariosasko777@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TurkishShrinkedNER(datasets.GeneratorBasedBuilder):",
            "path_to_manual_file = os.path.abspath(os.path.expanduser(dl_manager.manual_dir))",
            "if not os.path.exists(path_to_manual_file):",
            "raise FileNotFoundError(",
            "-                \"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('turkish_shrinked_ner', data_dir=...)` that includes file name {}. Manual download instructions: {}\".format(",
            "-                    path_to_manual_file,",
            "-                    _FILENAME,",
            "-                    self.manual_download_instructions,",
            "-                )",
            "+                \"{path_to_manual_file} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('turkish_shrinked_ner', data_dir=...)` that includes file name {_FILENAME}. Manual download instructions: {self.manual_download_instructions}\"",
            ")",
            "return [",
            "datasets.SplitGenerator("
        ]
    },
    {
        "number": 6648,
        "comments": "",
        "commit_message": "fix bugs in magnitude/spectrum loss for enh; add egs2/dirha/enh1 and egs2/reverb/enh1\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class DNN_WPE(torch.nn.Module):",
            "",
            "# Averaging along the channel axis: (..., C, T) -> (..., T)",
            "power = power.mean(dim=-2)",
            "+            power = torch.clamp(power, min=1e-6)",
            "",
            "# enhanced: (..., C, T) -> (..., C, T)",
            "# NOTE(kamo): Calculate in double precision"
        ]
    },
    {
        "number": 6650,
        "comments": "",
        "commit_message": "py2 fixes\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def stack(sequence, horizontal=True, vertical=True):",
            "# Concat all indices and values to one new large sparse matrix.",
            "indices = torch.cat(indices, dim=1)",
            "values = torch.cat([mat._values() for mat in sequence])",
            "-    size = torch.Size([y_sum, x_sum, *sequence[0].size()[2:]])",
            "+    size = torch.Size([y_sum, x_sum, *(sequence[0].size()[2:])])",
            "slices = torch.LongTensor(slices)",
            "",
            "return torch.sparse.FloatTensor(indices, values, size), slices"
        ]
    },
    {
        "number": 6651,
        "comments": "",
        "commit_message": "minor fixes\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Categorical(Distribution):",
            "repeat=r))",
            "",
            "if r == 1:",
            "-            return iter([Variable(torch.Tensor([[i]])) for i in range(c)])",
            "+            return (Variable(torch.Tensor([[i]])) for i in range(c))",
            "return (Variable(torch.Tensor(list(x)).unsqueeze(1))",
            "for x in itertools.product(torch.arange(0, c),",
            "repeat=r))"
        ]
    },
    {
        "number": 6653,
        "comments": "",
        "commit_message": "fix typo (#834)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "\"metadata\": {},",
            "\"outputs\": [],",
            "\"source\": [",
            "-    \"# equivalent to pyro.condition(scale, data={\\\"measurement\\\": Variable(torch.ones(1))})\\n\",",
            "+    \"# equivalent to pyro.condition(scale, data={\\\"measurement\\\": Variable(torch.Tensor([9.5]))})\\n\",",
            "\"def scale_obs(guess):\\n\",",
            "\"    weight = pyro.sample(\\\"weight\\\", dist.Normal(guess, Variable(torch.ones(1))))\\n\",",
            "\"     # here we attach an observation measurement == 9.5\\n\","
        ]
    },
    {
        "number": 6657,
        "comments": "",
        "commit_message": "Squeezenet reshape outputs fix (#10222)\n\n@AyushExel\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def reshape_classifier_output(model, n=1000):",
            "elif nn.Conv2d in types:",
            "i = types.index(nn.Conv2d)  # nn.Conv2d index",
            "if m[i].out_channels != n:",
            "-                m[i] = nn.Conv2d(m[i].in_channels, n, m[i].kernel_size, m[i].stride, bias=m[i].bias)",
            "+                m[i] = nn.Conv2d(m[i].in_channels, n, m[i].kernel_size, m[i].stride, bias=m[i].bias is not None)",
            "",
            "",
            "@contextmanager"
        ]
    },
    {
        "number": 6672,
        "comments": "",
        "commit_message": "fix a bug cause by torch.tensor()\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class DefaultBoxes(object):",
            "self.default_boxes.append((cx, cy, w, h))",
            "",
            "# \u5c06default_boxes\u8f6c\u4e3atensor\u683c\u5f0f",
            "-        self.dboxes = torch.tensor(self.default_boxes, dtype=torch.float32)  # \u8fd9\u91cc\u4e0d\u8f6c\u7c7b\u578b\u4f1a\u62a5\u9519",
            "+        self.dboxes = torch.as_tensor(self.default_boxes, dtype=torch.float32)  # \u8fd9\u91cc\u4e0d\u8f6c\u7c7b\u578b\u4f1a\u62a5\u9519",
            "self.dboxes.clamp_(min=0, max=1)  # \u5c06\u5750\u6807\uff08x, y, w, h\uff09\u90fd\u9650\u5236\u57280-1\u4e4b\u95f4",
            "",
            "# For IoU calculation"
        ]
    },
    {
        "number": 6673,
        "comments": "",
        "commit_message": "Confusion matrix native image-space fix (#2046)\n\nMake sure the labels and predictions are equally scaled on confusion_matrix.process_batch\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test(data,",
            "tbox = xywh2xyxy(labels[:, 1:5])",
            "scale_coords(img[si].shape[1:], tbox, shapes[si][0], shapes[si][1])  # native-space labels",
            "if plots:",
            "-                    confusion_matrix.process_batch(pred, torch.cat((labels[:, 0:1], tbox), 1))",
            "+                    confusion_matrix.process_batch(predn, torch.cat((labels[:, 0:1], tbox), 1))",
            "",
            "# Per target class",
            "for cls in torch.unique(tcls_tensor):"
        ]
    },
    {
        "number": 6681,
        "comments": "",
        "commit_message": "lintfixbot: Auto-commit fixed lint errors in codebase\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def arange(",
            "stop = start",
            "if dtype is None:",
            "if isinstance(start, int) and isinstance(stop, int) and isinstance(step, int):",
            "-            return torch.arange(",
            "-                start, stop, step, dtype=torch.int64, device=device",
            "-            ).to(torch.int32)",
            "+            return torch.arange(start, stop, step, dtype=torch.int64, device=device).to(",
            "+                torch.int32",
            "+            )",
            "else:",
            "return torch.arange(start, stop, step, device=device)",
            "else:"
        ]
    },
    {
        "number": 6683,
        "comments": "",
        "commit_message": "fixed retrieve layer, added iterable callback option, simplified tensor spec handling\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class SubsamplingStep(MetaOptimizer):",
            "if some_argument:",
            "arguments_iter = iter(some_argument)",
            "some_argument = next(arguments_iter)",
            "-                elif some_argument is None or util.rank(some_argument) == 0:",
            "+                elif some_argument is None or util.rank(x=some_argument) == 0:",
            "# Non-batched argument",
            "some_argument = next(arguments_iter)",
            "else:"
        ]
    },
    {
        "number": 6687,
        "comments": "",
        "commit_message": "Remove Re-definition found for builtin input function - Update tests (#2255)\n\n* change input to sample in test/feature folder\n\n* change input to sample in test/filters folder\n\n* change input to sample in test/morphology folder\n\n* change input to sample in test/geometry/subpix/test_spatial_softargmax.py\n\n* change input to sample in test/geometry/transfrom in test_imgwarp3d.py and test_pyramid.py\n\n* change input to input_org in test/geometry/transfrom in test_imgwarp.py\n\n* change input to inpt in test/grad_estimator/test_ste.py\n\n* change input to inpt and sample in test/enhance/ folder\n\n* change input to input_tensor in test/augmentation/test_augmentation_3d.py\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nCo-authored-by: oleksadobush <oleksandra.stasiuk@ucu.edu.ua>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Testunsharp(BaseTester):",
            "",
            "def test_noncontiguous(self, device, dtype):",
            "batch_size = 3",
            "-        input = torch.rand(3, 5, 5, device=device, dtype=dtype).expand(batch_size, -1, -1, -1)",
            "+        inpt = torch.rand(3, 5, 5, device=device, dtype=dtype).expand(batch_size, -1, -1, -1)",
            "",
            "kernel_size = (3, 3)",
            "sigma = (1.5, 2.1)",
            "-        actual = unsharp_mask(input, kernel_size, sigma, \"replicate\")",
            "+        actual = unsharp_mask(inpt, kernel_size, sigma, \"replicate\")",
            "assert actual.is_contiguous()",
            "",
            "def test_gradcheck(self, device):"
        ]
    },
    {
        "number": 6688,
        "comments": "",
        "commit_message": "[RLlib] Fix `use_lstm` flag for ModelV2 (w/o ModelV1 wrapping) and add it for PyTorch. (#8734)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "from ray.rllib.utils import try_import_torch",
            "_, nn = try_import_torch()",
            "",
            "",
            "-class VisionNetwork(TorchModelV2, nn.Module):",
            "+class VisionNetwork(TorchModelV2):",
            "\"\"\"Generic vision network.\"\"\"",
            "",
            "def __init__(self, obs_space, action_space, num_outputs, model_config,",
            "name):",
            "TorchModelV2.__init__(self, obs_space, action_space, num_outputs,",
            "model_config, name)",
            "-        nn.Module.__init__(self)",
            "",
            "activation = get_activation_fn(",
            "model_config.get(\"conv_activation\"), framework=\"torch\")"
        ]
    },
    {
        "number": 6693,
        "comments": "",
        "commit_message": "Detect.py supports running against a Triton container (#9228)\n\n* update coco128-seg comments\n\n* Enables detect.py to use Triton for inference\n\nTriton Inference Server is an open source inference serving software\nthat streamlines AI inferencing.\nhttps://github.com/triton-inference-server/server\n\nThe user can now provide a \"--triton-url\" argument to detect.py to use\na local or remote Triton server for inference.\nFor e.g., http://localhost:8000 will use http over port 8000\nand grpc://localhost:8001 will use grpc over port 8001.\nNote, it is not necessary to specify a weights file to use Triton.\n\nA Triton container can be created by first exporting the Yolov5 model\nto a Triton supported runtime. Onnx, Torchscript, TensorRT are\nsupported by both Triton and the export.py script.\n\nThe exported model can then be containerized via the OctoML CLI.\nSee https://github.com/octoml/octo-cli#getting-started for a guide.\n\n* added triton client to requirements\n\n* fixed support for TFSavedModels in Triton\n\n* reverted change\n\n* Test CoreML update\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Update ci-testing.yml\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Use pathlib\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Refacto DetectMultiBackend to directly accept triton url as --weights http://...\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Deploy category\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Update detect.py\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Update common.py\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Update common.py\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Update predict.py\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Update predict.py\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Update predict.py\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Update triton.py\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Update triton.py\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Add printout and requirements check\n\n* Cleanup\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* triton fixes\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fixed triton model query over grpc\n\n* Update check_requirements('tritonclient[all]')\n\n* group imports\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix likely remote URL bug\n\n* update comment\n\n* Update is_url()\n\n* Fix 2x download attempt on http://path/to/model.pt\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\nCo-authored-by: glennjocher <glenn.jocher@ultralytics.com>\nCo-authored-by: Gaz Iqbal <giqbal@octoml.ai>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def run(",
            "",
            "def parse_opt():",
            "parser = argparse.ArgumentParser()",
            "-    parser.add_argument('--weights', nargs='+', type=str, default=ROOT / 'yolov5s.pt', help='model path(s)')",
            "+    parser.add_argument('--weights', nargs='+', type=str, default=ROOT / 'yolov5s.pt', help='model path or triton URL')",
            "parser.add_argument('--source', type=str, default=ROOT / 'data/images', help='file/dir/URL/glob/screen/0(webcam)')",
            "parser.add_argument('--data', type=str, default=ROOT / 'data/coco128.yaml', help='(optional) dataset.yaml path')",
            "parser.add_argument('--imgsz', '--img', '--img-size', nargs='+', type=int, default=[640], help='inference size h,w')"
        ]
    },
    {
        "number": 6695,
        "comments": "",
        "commit_message": "Fix mixed precision serialization of group convs\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Conv(Layer):",
            "inputs = tf.pad(inputs, self._compute_causal_padding(inputs))",
            "",
            "if self.groups > 1:",
            "-            outputs = self._jit_compiled_convolution_op(inputs, self.kernel)",
            "+            outputs = self._jit_compiled_convolution_op(",
            "+                inputs, tf.convert_to_tensor(self.kernel)",
            "+            )",
            "else:",
            "outputs = self.convolution_op(inputs, self.kernel)"
        ]
    },
    {
        "number": 6696,
        "comments": "",
        "commit_message": "fix encoder outputs (#8368)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TFGenerationMixin:",
            "shape=(-1,),",
            ")",
            "# expand encoder_outputs",
            "-            encoder_outputs = (tf.gather(encoder_outputs[0], expanded_batch_idxs, axis=0), *encoder_outputs[1:])",
            "-",
            "+            encoder_outputs = (tf.gather(encoder_outputs[0], expanded_batch_idxs, axis=0),)",
            "else:",
            "encoder_outputs = None",
            "cur_len = shape_list(input_ids)[-1]"
        ]
    },
    {
        "number": 6697,
        "comments": "",
        "commit_message": "fixes for exporter issue of bi-transformer model (#597)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/fairseq/pull/597\n\nPull Request resolved: https://github.com/facebookresearch/pytext/pull/424\n\nFixes two issues:\n1. the new Layernorm has issues in exporting\n2. fix tensorboard writing by using the \"RAW\" operator_export_type\n\nDifferential Revision: D14610694\n\nfbshipit-source-id: 1b859f54c571a90766128ab28539a9901375c3e6\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "import torch",
            "",
            "",
            "-def LayerNorm(normalized_shape, eps=1e-5, elementwise_affine=True):",
            "-    if torch.cuda.is_available():",
            "+def LayerNorm(normalized_shape, eps=1e-5, elementwise_affine=True, export=False):",
            "+    if not export and torch.cuda.is_available():",
            "try:",
            "from apex.normalization import FusedLayerNorm",
            "return FusedLayerNorm(normalized_shape, eps, elementwise_affine)"
        ]
    },
    {
        "number": 6700,
        "comments": "",
        "commit_message": "Fix for FloorDiv Function Warning (#1760)\n\n* Fix for Floor Function Warning\n\nFix for Floor Function Warning\n\n* Adding double quotes to fix formatting\n\nAdding double quotes to fix formatting\n\n* Update glow_tts.py\n\n* Update glow_tts.py\n",
        "label": "",
        "answer": "no",
        "change": [
            "class GlowTTS(BaseTTS):",
            "y = y[:, :, :y_max_length]",
            "if attn is not None:",
            "attn = attn[:, :, :, :y_max_length]",
            "-        y_lengths = (y_lengths // self.num_squeeze) * self.num_squeeze",
            "+        y_lengths = torch.div(y_lengths, self.num_squeeze, rounding_mode=\"floor\") * self.num_squeeze",
            "return y, y_lengths, y_max_length, attn",
            "",
            "def store_inverse(self):"
        ]
    },
    {
        "number": 6706,
        "comments": "",
        "commit_message": "fix QAT ema issue and tensor type error (#3219)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class QuantGrad(torch.autograd.Function):",
            "raise ValueError(\"unrecognized QuantType.\")",
            "",
            "bits = QuantGrad.get_bits_length(wrapper.config, quant_type)",
            "-        qmin, qmax = torch.Tensor([0], device=tensor.device), torch.Tensor([(1 << bits) - 1], device=tensor.device)",
            "+        qmin, qmax = torch.Tensor([0]).to(device=tensor.device), torch.Tensor([(1 << bits)-1]).to(device=tensor.device)",
            "ctx.save_for_backward(tensor, wrapper.module.scale, wrapper.module.zero_point, qmin, qmax)",
            "return output"
        ]
    },
    {
        "number": 6709,
        "comments": "",
        "commit_message": "small fix in grayscale conversion and add opencv tests\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def rgb_to_grayscale(input: torch.Tensor) -> torch.Tensor:",
            ".format(input.shape))",
            "",
            "r, g, b = torch.chunk(input, chunks=3, dim=-3)",
            "-    gray: torch.Tensor = 0.299 * r + 0.587 * g + 0.110 * b",
            "+    gray: torch.Tensor = 0.299 * r + 0.587 * g + 0.114 * b",
            "return gray"
        ]
    },
    {
        "number": 6710,
        "comments": "",
        "commit_message": "fix async test (#2820)\n\n* fix async test\n\n* minor fix\n",
        "label": "",
        "answer": "no",
        "change": [
            "class AsyncInferenceTestCase(AsyncTestCase):",
            "img_path = os.path.join(root_dir, 'demo/demo.jpg')",
            "bboxes, _ = await detector.apredict(img_path)",
            "self.assertTrue(bboxes)",
            "+            # asy inference detector will hack grad_enabled,",
            "+            # so restore here to avoid it to influence other tests",
            "+            torch.set_grad_enabled(ori_grad_enabled)"
        ]
    },
    {
        "number": 6711,
        "comments": "",
        "commit_message": "Fix bug in dot-style attention\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def att_sum_bahdanau(v_att, keys, query):",
            "@function.Defun(tf.float32, tf.float32, func_name=\"att_sum_dot\", noinline=True)",
            "def att_sum_dot(keys, query):",
            "\"\"\"Calculates a batch- and timweise dot product\"\"\"",
            "-  return tf.reduce_sum(keys + tf.expand_dims(query, 1), [2])",
            "+  return tf.reduce_sum(keys * tf.expand_dims(query, 1), [2])",
            "",
            "",
            "class AttentionLayer(GraphModule):"
        ]
    },
    {
        "number": 6714,
        "comments": "",
        "commit_message": "[BugFix] fix compression bugs (#5140)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def main():",
            "'op_names': ['features.6', 'features.9', 'features.13', 'features.16', 'features.20', 'classifier.2', 'classifier.5']",
            "}]",
            "",
            "-    quantizer = BNNQuantizer(model, configure_list)",
            "+    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)",
            "+    quantizer = BNNQuantizer(model, configure_list, optimizer)",
            "model = quantizer.compress()",
            "",
            "print('=' * 10 + 'train' + '=' * 10)",
            "-    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)",
            "best_top1 = 0",
            "for epoch in range(400):",
            "print('# Epoch {} #'.format(epoch))"
        ]
    },
    {
        "number": 6716,
        "comments": "",
        "commit_message": "update expected results of slow tests (#268)\n\n* update expected results of slow tests\n\n* relax sum and mean tests\n\n* Print shapes when reporting exception\n\n* formatting\n\n* fix sentence\n\n* relax test_stable_diffusion_fast_ddim for gpu fp16\n\n* relax flakey tests on GPU\n\n* added comment on large tolerences\n\n* black\n\n* format\n\n* set scheduler seed\n\n* added generator\n\n* use np.isclose\n\n* set num_inference_steps to 50\n\n* fix dep. warning\n\n* update expected_slice\n\n* preprocess if image\n\n* updated expected results\n\n* updated expected from CI\n\n* pass generator to VAE\n\n* undo change back to orig\n\n* use orignal\n\n* revert back the expected on cpu\n\n* revert back values for CPU\n\n* more undo\n\n* update result after using gen\n\n* update mean\n\n* set generator for mps\n\n* update expected on CI server\n\n* undo\n\n* use new seed every time\n\n* cpu manual seed\n\n* reduce num_inference_steps\n\n* style\n\n* use generator for randn\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class StableDiffusionImg2ImgPipeline(DiffusionPipeline):",
            "",
            "self.scheduler.set_timesteps(num_inference_steps, **extra_set_kwargs)",
            "",
            "-        if not isinstance(init_image, torch.FloatTensor):",
            "+        if isinstance(init_image, PIL.Image.Image):",
            "init_image = preprocess(init_image)",
            "",
            "# encode the init image into latents and scale the latents"
        ]
    },
    {
        "number": 6717,
        "comments": "",
        "commit_message": "Fix flake8 (#1975)\n\n* Fix flake8\n\n* Rename k to line\n",
        "label": "",
        "answer": "no",
        "change": [
            "class GigaFrenConfig(datasets.BuilderConfig):",
            "",
            "",
            "class GigaFren(datasets.GeneratorBasedBuilder):",
            "-    BUILDER_CONFIGS = [GigaFrenConfig(description=f\"Translating en to fr \", version=datasets.Version(_VERSION))]",
            "+    BUILDER_CONFIGS = [GigaFrenConfig(description=\"Translating en to fr \", version=datasets.Version(_VERSION))]",
            "BUILDER_CONFIG_CLASS = GigaFrenConfig",
            "",
            "def _info(self):"
        ]
    },
    {
        "number": 6720,
        "comments": "",
        "commit_message": "Better error message when using the wrong load_from_disk (#2437)\n\n* better error message when using the wrong load_from_disk\n\n* better message\n\n* fix\n\n* Update arrow_dataset.py\n\n* Update dataset_dict.py\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Dataset(DatasetInfoMixin, IndexableMixin):",
            "- if `dataset_path` is a path of a dataset dict directory: a :class:`DatasetDict` with each split.",
            "\"\"\"",
            "# copies file from filesystem if it is remote filesystem to local filesystem and modifies dataset_path to temp directory containing local copies",
            "+        fs = fsspec.filesystem(\"file\") if fs is None else fs",
            "+        dataset_dict_json_path = Path(dataset_path, config.DATASETDICT_JSON_FILENAME).as_posix()",
            "+        dataset_info_path = Path(dataset_path, config.DATASET_INFO_FILENAME).as_posix()",
            "+        if not fs.isfile(dataset_info_path) and fs.isfile(dataset_dict_json_path):",
            "+            raise FileNotFoundError(",
            "+                f\"No such file or directory: '{dataset_info_path}'. Expected to load a Dataset object, but got a DatasetDict. Please use datasets.load_from_disk instead.\"",
            "+            )",
            "+",
            "if is_remote_filesystem(fs):",
            "src_dataset_path = extract_path_from_uri(dataset_path)",
            "tmp_dir = tempfile.TemporaryDirectory()"
        ]
    },
    {
        "number": 6725,
        "comments": "",
        "commit_message": "fixed a bug in the case of dlayers>2 and dtype=gru\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Decoder(torch.nn.Module):",
            "self.dropout_dec += [torch.nn.Dropout(p=dropout)]",
            "for _ in six.moves.range(1, self.dlayers):",
            "self.decoder += [",
            "-                torch.nn.LSTMCell(dunits, dunits) if self.dtype == \"lstm\" else torch.nn.GRUCell(dunits + eprojs,",
            "-                                                                                                dunits)]",
            "+                torch.nn.LSTMCell(dunits, dunits) if self.dtype == \"lstm\" else torch.nn.GRUCell(dunits, dunits)]",
            "self.dropout_dec += [torch.nn.Dropout(p=dropout)]",
            "# NOTE: dropout is applied only for the vertical connections",
            "# see https://arxiv.org/pdf/1409.2329.pdf"
        ]
    },
    {
        "number": 6726,
        "comments": "",
        "commit_message": "improved global tensor handling, various other fixes\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Lstm(TransformationBase):",
            "",
            "self.cell = tf.contrib.rnn.LSTMCell(num_units=self.lstm_size)",
            "# if self.lstm_dropout is not None:",
            "-        #     keep_prob = tf.cond(pred=update, true_fn=(lambda: 1.0 - self.lstm_dropout), false_fn=(lambda: 1.0))",
            "+        #     keep_prob = self.cond(pred=update, true_fn=(lambda: 1.0 - self.lstm_dropout), false_fn=(lambda: 1.0))",
            "#     self.lstm_cell = tf.contrib.rnn.DropoutWrapper(cell=self.lstm_cell, output_keep_prob=keep_prob)",
            "",
            "def tf_apply(self, x, sequence_length=None):"
        ]
    },
    {
        "number": 6729,
        "comments": "",
        "commit_message": "fix output\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "\"source\": [",
            "\"import numpy as np\\n\",",
            "\"a = np.ones(5)\\n\",",
            "-    \"b = torch.DoubleTensor(a)\\n\",",
            "+    \"b = torch.from_numpy(a)\\n\",",
            "\"np.add(a, 1, out=a)\\n\",",
            "\"print(a)\\n\",",
            "\"print(b) # see how changing the np array changed the torch Tensor automatically\""
        ]
    },
    {
        "number": 6734,
        "comments": "",
        "commit_message": "Bug fix about multi gpu due to recent changings\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class E2E(torch.nn.Module):",
            "else:",
            "logging.warning('loss (=%f) is not correct', loss_data)",
            "",
            "-        return self.loss, loss_ctc, loss_att, acc, cer, wer",
            "+        # Note(kamo): In order to work with DataParallel, on pytorch==0.4,",
            "+        # the return value must be torch.CudaTensor, or tuple/list/dict of it.",
            "+        # Neither CPUTensor nor float/int value can be used",
            "+        # because NCCL communicates between GPU devices.",
            "+        return self.loss",
            "",
            "def recognize(self, x, recog_args, char_list, rnnlm=None):",
            "\"\"\"E2E beam search"
        ]
    },
    {
        "number": 6737,
        "comments": "",
        "commit_message": "add support for tensorflow1.15 (#818)\n\n* add support for tensorflow1.15\n\n* Fix typo in requirements/linux-py3.txt\n\nCo-authored-by: HX Lin <>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def KitModel(weight_file = None):",
            "IR_node.name)",
            "return code",
            "",
            "-    def emit_Maxmum(self, IR_node):",
            "-        code = \"{:<15} = tf.maxmum({}, {}, name='{}')\".format(",
            "+    def emit_Maximum(self, IR_node):",
            "+        code = \"{:<15} = tf.maximum({}, {}, name='{}')\".format(",
            "IR_node.variable_name,",
            "self.parent_variable_name(IR_node),",
            "self.parent_variable_name(IR_node, [1]),"
        ]
    },
    {
        "number": 6739,
        "comments": "",
        "commit_message": "Forward/backward compatibility around pytorch 3aeb78, to fix #191\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "if \"--cuda_ext\" in sys.argv:",
            "'csrc/welford.cu']))",
            "ext_modules.append(",
            "CUDAExtension(name='fused_layer_norm_cuda',",
            "-                          sources=['apex/normalization/csrc/layer_norm_cuda.cpp',",
            "-                                   'apex/normalization/csrc/layer_norm_cuda_kernel.cu'],",
            "+                          sources=['csrc/layer_norm_cuda.cpp',",
            "+                                   'csrc/layer_norm_cuda_kernel.cu'],",
            "extra_compile_args={'cxx': ['-O3'] + version_ge_1_1,",
            "'nvcc':['-maxrregcount=50',",
            "'-O3',"
        ]
    },
    {
        "number": 6740,
        "comments": "",
        "commit_message": "fix named pipe open mode (#3183)\n\nCo-authored-by: liuzhe <zhe.liu@microsoft.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "else:",
            "",
            "def connect(self) -> BufferedIOBase:",
            "conn, _ = self._socket.accept()",
            "-            self.file = conn.makefile('w+b')",
            "+            self.file = conn.makefile('rwb')",
            "return self.file",
            "",
            "def close(self) -> None:"
        ]
    },
    {
        "number": 6753,
        "comments": "",
        "commit_message": "More TF int dtype fixes (#20384)\n\n* Add a test to ensure int dummy inputs are int64\n\n* Move the test into the existing int64 test and update a lot of existing dummies\n\n* Fix remaining dummies\n\n* Fix remaining dummies\n\n* Test for int64 serving sigs as well\n\n* Update core tests to use tf.int64\n\n* Add better messages to the assertions\n\n* Update all serving sigs to int64\n\n* More sneaky hiding tf.int32s\n\n* Add an optional int32 signature in save_pretrained\n\n* make fixup\n\n* Add Amy's suggestions\n\n* Switch all serving sigs back to tf.int32\n\n* Switch all dummies to tf.int32\n\n* Adjust tests to check for tf.int32 instead of tf.int64\n\n* Fix base dummy_inputs dtype\n\n* Start casting to tf.int32 in input_processing\n\n* Change dtype for unpack_inputs test\n\n* Add proper tf.int32 test\n\n* Make the alternate serving signature int64\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TFMBartPreTrainedModel(TFPreTrainedModel):",
            "decoder_input_ids = tf.cast(tf.convert_to_tensor(DUMMY_INPUTS), tf.int32)",
            "dummy_inputs = {",
            "\"decoder_input_ids\": decoder_input_ids,",
            "-            \"attention_mask\": tf.math.not_equal(input_ids, pad_token),",
            "+            \"attention_mask\": tf.cast(input_ids != pad_token, tf.int32),",
            "\"input_ids\": input_ids,",
            "}",
            "return dummy_inputs"
        ]
    },
    {
        "number": 6755,
        "comments": "",
        "commit_message": "fixed problems with multiple LSTMs, introduced scope argument for layers\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Categorical(Distribution):",
            "self.probabilities = tf.maximum(x=self.probabilities, y=util.epsilon)",
            "",
            "# \"Normalized\" logits",
            "-        self.logits = tf.log(x=self.probabilities + util.epsilon)",
            "+        self.logits = tf.log(x=self.probabilities)",
            "",
            "def sample(self):",
            "# Deterministic: maximum likelihood action"
        ]
    },
    {
        "number": 6757,
        "comments": "",
        "commit_message": "fixed DQN LSTM problem, modified memories to support next_internals, added test for VPG with LSTM\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def lstm(x, size=None):",
            "",
            "with tf.variable_scope('lstm'):",
            "internal_input = tf.placeholder(dtype=tf.float32, shape=(None, 2, size))",
            "-        lstm = tf.contrib.rnn.LSTMCell(num_units=size)",
            "+        lstm_cell = tf.contrib.rnn.LSTMCell(num_units=size)",
            "c = internal_input[:, 0, :]",
            "h = internal_input[:, 1, :]",
            "state = tf.contrib.rnn.LSTMStateTuple(c=c, h=h)",
            "-        x, state = lstm(inputs=x, state=state)",
            "+        x, state = lstm_cell(inputs=x, state=state)",
            "",
            "internal_output = tf.stack(values=(state.c, state.h), axis=1)",
            "internal_init = np.zeros(shape=(2, size))"
        ]
    },
    {
        "number": 6759,
        "comments": "",
        "commit_message": "Fix `DBLP` download, move `get_edge_index` to `testing` package (#6901)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def test_graph_store():",
            "def test_graph_store_conversion():",
            "graph_store = MyGraphStore()",
            "",
            "-    coo = (row, col) = get_edge_index(100, 100, 300)",
            "+    coo = (row, col) = get_random_edge_index(100, 100, 300)",
            "adj = SparseTensor(row=row, col=col, sparse_sizes=(100, 100))",
            "csr, csc = adj.csr()[:2], adj.csc()[:2][::-1]"
        ]
    },
    {
        "number": 6763,
        "comments": "",
        "commit_message": "Improve reproduceability 2/3 (#1906)\n\n* [Repro] Correct reproducability\n\n* up\n\n* up\n\n* uP\n\n* up\n\n* need better image\n\n* allow conversion from no state dict checkpoints\n\n* up\n\n* up\n\n* up\n\n* up\n\n* check tensors\n\n* check tensors\n\n* check tensors\n\n* check tensors\n\n* next try\n\n* up\n\n* up\n\n* better name\n\n* up\n\n* up\n\n* Apply suggestions from code review\n\n* correct more\n\n* up\n\n* replace all torch randn\n\n* fix\n\n* correct\n\n* correct\n\n* finish\n\n* fix more\n\n* up\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class StableDiffusionInpaintPipelineLegacy(DiffusionPipeline):",
            "init_latents_orig = init_latents",
            "",
            "# add noise to latents using the timesteps",
            "-        noise = torch.randn(init_latents.shape, generator=generator, device=self.device, dtype=dtype)",
            "+        noise = randn_tensor(init_latents.shape, generator=generator, device=self.device, dtype=dtype)",
            "init_latents = self.scheduler.add_noise(init_latents, noise, timestep)",
            "latents = init_latents",
            "return latents, init_latents_orig, noise"
        ]
    },
    {
        "number": 6775,
        "comments": "",
        "commit_message": "Fix for `load_from_checkpoint` (#2776)\n\n* Fix.\n\n* Fix #2550: allow to load model from checkpoint if self.save_hyperparameters() was not called.\n\n* Fix? Cleaner way of not calling self.save_hyperparameters in EvalModelTemplate.\n\n* Fix? `_load_model_state` cleanup\n\n* Fix?\n\n* Fix #2550: allow to load model from checkpoint if self.save_hyperparameters() was not called.\n\n* Fix.\n\n* Fix? Cleaner way of not calling self.save_hyperparameters in EvalModelTemplate.\n\n* Fix? `_load_model_state` cleanup\n\n* Fixed side effect in `test_load_model_from_checkpoint_extra_args`.\n\n* Apply suggestions from code review\n\n* fix\n\n* try\n\n* fixed missing arg in evalmodel\n\n* fixed missing arg in evalmodel\n\n* fix\n\n* update\n\n* fix loading\n\n* add test\n\n* prune\n\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\nCo-authored-by: Jirka Borovec <jirka@pytorchlightning.ai>\nCo-authored-by: William Falcon <waf2107@columbia.edu>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_load_model_from_checkpoint(tmpdir, model_template):",
            "for (old_name, old_p), (new_name, new_p) in zip(model.named_parameters(), pretrained_model.named_parameters()):",
            "assert torch.all(torch.eq(old_p, new_p)), 'loaded weights are not the same as the saved weights'",
            "",
            "+    # Check `test` on pretrained model:",
            "new_trainer = Trainer(**trainer_options)",
            "new_trainer.test(pretrained_model)"
        ]
    },
    {
        "number": 6777,
        "comments": "",
        "commit_message": "lintfixbot: Auto-commit fixed lint errors in codebase\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def unique_all(",
            "decimal = tf.range(tf.size(inverse_indices)) / tf.size(inverse_indices)",
            "inv_sorted = tf.argsort(tf.cast(inverse_indices, dtype=decimal.dtype) + decimal)",
            "tot_counts = tf.concat(",
            "-            [tf.zeros((1,), dtype=counts.dtype), tf.cumsum(counts, axis=0)[:-1]], 0)",
            "+            [tf.zeros((1,), dtype=counts.dtype), tf.cumsum(counts, axis=0)[:-1]], 0",
            "+        )",
            "indices = inv_sorted.numpy()[tot_counts]",
            "",
            "return Results("
        ]
    },
    {
        "number": 6779,
        "comments": "",
        "commit_message": "Fixed the matrix_rank function for numpy, jax and tensorflow (#4452)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def matrix_rank(",
            "out: Optional[torch.Tensor] = None,",
            ") -> torch.Tensor:",
            "# ToDo: add support for default rtol value here, for the case where None is provided",
            "-    ret = torch.linalg.matrix_rank(x, atol=rtol, out=out)",
            "+    ret = torch.linalg.matrix_rank(x, rtol=rtol, out=out)",
            "return torch.tensor(ret, dtype=ivy.default_int_dtype(as_native=True))",
            "",
            "",
            "-matrix_rank.unsupported_dtypes = (\"float16\",)",
            "+matrix_rank.unsupported_dtypes = (",
            "+    \"float16\",",
            "+    \"bfloat16\",",
            "+)",
            "matrix_rank.support_native_out = True"
        ]
    },
    {
        "number": 6780,
        "comments": "",
        "commit_message": "FeedfreePredictor and example on ImageNet eval (fix #772)\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class NewSessionCreator(tf.train.ChiefSessionCreator):",
            "\"\"\"",
            "Args:",
            "target, graph, config: same as :meth:`Session.__init__()`.",
            "-            config: defaults to :func:`tfutils.get_default_sess_config()`",
            "+            config: a :class:`tf.ConfigProto` instance, defaults to :func:`tfutils.get_default_sess_config()`",
            "\"\"\"",
            "assert graph is None"
        ]
    },
    {
        "number": 6784,
        "comments": "",
        "commit_message": "Fix pylint errors\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class PipelineTest(tf.test.TestCase):",
            "tf.app.flags.FLAGS.output_dir = self.output_dir",
            "tf.app.flags.FLAGS.metrics = yaml.dump([",
            "\"log_perplexity\", \"bleu\", \"rouge_1/f_score\", \"rouge_l/f_score\"])",
            "-",
            "-    \"\"\"",
            "-      log_perplexity,bleu,rouge_1/f_score,rouge_l/f_score\"\"\"",
            "-    # tf.app.flags.FLAGS.train_source = sources_train.name",
            "-    # tf.app.flags.FLAGS.train_target = targets_train.name",
            "-    # tf.app.flags.FLAGS.vocab_source = vocab_source.name",
            "-    # tf.app.flags.FLAGS.vocab_target = vocab_target.name",
            "tf.app.flags.FLAGS.model = \"AttentionSeq2Seq\"",
            "tf.app.flags.FLAGS.model_params = \"\"\"",
            "attention.params:"
        ]
    },
    {
        "number": 6797,
        "comments": "",
        "commit_message": "Small fixes (#1215)\n\nSummary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1215\n\nReviewed By: ngoyal2707, msbaines\n\nDifferential Revision: D22514719\n\nPulled By: myleott\n\nfbshipit-source-id: 5f15ba501fd66af1eb49b5702aff940f06c3d91f\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def _main(args, output_file):",
            "# Fix seed for stochastic decoding",
            "if args.seed is not None and not args.no_seed_provided:",
            "np.random.seed(args.seed)",
            "-        torch.manual_seed(args.seed)",
            "+        utils.set_torch_seed(args.seed)",
            "",
            "use_cuda = torch.cuda.is_available() and not args.cpu"
        ]
    },
    {
        "number": 6801,
        "comments": "",
        "commit_message": "Beautified code, fixed docstrings, added future statements\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def get_flattened_gradient(loss, variables):",
            "\"\"\"",
            "gradients = tf.gradients(loss, variables)",
            "",
            "-    return tf.concat(axis=0, values=[tf.reshape(grad, [get_number_of_elements(v)])",
            "-                         for (v, grad) in zip(variables, gradients)])",
            "+    return tf.concat(axis=0, values=[tf.reshape(grad, [get_number_of_elements(v)]) for (v, grad) in zip(variables, gradients)])",
            "",
            "",
            "class FlatVarHelper(object):"
        ]
    },
    {
        "number": 6803,
        "comments": "",
        "commit_message": "fix FC_with_w\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def fully_connected_with_w(x, use_bias=True, sn=False, reuse=False, scope='linea",
            "if sn :",
            "w = tf.get_variable(\"kernel\", [channels, 1], tf.float32,",
            "initializer=weight_init, regularizer=weight_regularizer)",
            "+            w = spectral_norm(w)",
            "+",
            "if use_bias :",
            "bias = tf.get_variable(\"bias\", [1],",
            "initializer=tf.constant_initializer(0.0))",
            "",
            "-                x = tf.matmul(x, spectral_norm(w)) + bias",
            "+                x = tf.matmul(x, w) + bias",
            "else :",
            "-                x = tf.matmul(x, spectral_norm(w))",
            "+                x = tf.matmul(x, w)",
            "+",
            "else :",
            "x = tf.layers.dense(x, units=1, kernel_initializer=weight_init, kernel_regularizer=weight_regularizer, use_bias=use_bias)"
        ]
    },
    {
        "number": 6804,
        "comments": "",
        "commit_message": "minor fix in blockwise sparsifier (#1130)\n\nSummary:\nPull Request resolved: https://github.com/facebookresearch/pytext/pull/1130\n\ncompute absolute value of the parameters\n\nReviewed By: arbabu123\n\nDifferential Revision: D18466312\n\nfbshipit-source-id: ff8546e5f4071e7a5be767071fdae9b363dba035\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class BlockwiseMagnitudeSparsifier(L0_projection_sparsifier):",
            "pre_mask=(pre_mask.transpose(1, 0) if pre_mask else None),",
            ").transpose(1, 0)",
            "padded_param = self._padding_into_full_blocks(param)",
            "-        block_l1norms = padded_param.reshape(-1, 1, self.block_size).sum(dim=2)",
            "+        block_l1norms = (",
            "+            torch.abs(padded_param).reshape(-1, 1, self.block_size).sum(dim=2)",
            "+        )",
            "max_num_blocks = self._num_blocks_kept(param)",
            "",
            "topk_threshold = ("
        ]
    },
    {
        "number": 6807,
        "comments": "",
        "commit_message": "Add Image feature (#3163)\n\n* Initial commit\n\n* Add basic decoding\n\n* Replace features.Audio with Audio\n\n* Add Image to package reference\n\n* Use np.array\n\n* Update error msg\n\n* Add mode and channel decoding\n\n* Fix return value\n\n* Finish decoding\n\n* Make CI happy\n\n* Some more fixes\n\n* Minor doc fix\n\n* Remove animated option\n\n* Pin version\n\n* Remove unused imports in setup.py\n\n* Add vision requirements to setup.py\n\n* Add initial tests\n\n* Delete other formats\n\n* Make Image feature hashable\n\n* Add more tests\n\n* Support numpy array in alter data check in TypedSequence\n\n* Fix TypedSequence converion\n\n* Finish tests\n\n* Update Image - add ImageExtensionType and supporting functions\n\n* Update encoding functions\n\n* Add support in TypedSequence for ImageExtensionType\n\n* Add tests\n\n* Remove unused import\n\n* Fix doc and style\n\n* Fix doc indentation\n\n* Improve comment\n\n* Return single image instead of dict\n\n* Return PIL Image and not dict\n\n* Encode dict\n\n* Update tests\n\n* Style\n\n* np.ndarray encoding/decoding\n\n* Minor improvements\n\n* PIL Image support in cast_to_python_objects\n\n* Test cast\n\n* Doc fix\n\n* Extension type fixes\n\n* Style\n\n* Use types_mapper in Dataset.to_pandas\n\n* Add pandas extension array for image type\n\n* Update tests\n\n* image type inference\n\n* Remvoe cast_to_python test after Quentin's change\n\n* Improve tests\n\n* Add storage type\n\n* Improve tests\n\n* Test map that returns np.ndarray\n\n* Rename functions\n\n* Add streaming test\n\n* Use image struct in all situations\n\n* Update src/datasets/features/image.py - encode_example type hint\n\nCo-authored-by: Quentin Lhoest <42851186+lhoestq@users.noreply.github.com>\n\n* Update src/datasets/features/image.py -list_image_compression_formats type hint\n\nCo-authored-by: Quentin Lhoest <42851186+lhoestq@users.noreply.github.com>\n\n* Support str in encode_objects_to_image_dicts\n\n* Update src/datasets/features/image.py - objects_to_list_of_image_dicts type hint\n\nCo-authored-by: Quentin Lhoest <42851186+lhoestq@users.noreply.github.com>\n\n* Style\n\nCo-authored-by: Quentin Lhoest <lhoest.q@gmail.com>\nCo-authored-by: Quentin Lhoest <42851186+lhoestq@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class LJSpeech(datasets.GeneratorBasedBuilder):",
            "features=datasets.Features(",
            "{",
            "\"id\": datasets.Value(\"string\"),",
            "-                    \"audio\": datasets.features.Audio(sampling_rate=22050),",
            "+                    \"audio\": datasets.Audio(sampling_rate=22050),",
            "\"file\": datasets.Value(\"string\"),",
            "\"text\": datasets.Value(\"string\"),",
            "\"normalized_text\": datasets.Value(\"string\"),"
        ]
    },
    {
        "number": 6809,
        "comments": "",
        "commit_message": "batch fix if items are only sparsely available\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Batch(Data):",
            "for i, data in enumerate(data_list):",
            "num_nodes = data.num_nodes",
            "batch.batch.append(torch.full((num_nodes, ), i, dtype=torch.long))",
            "-            for key in keys:",
            "+            for key in data.keys:",
            "item = data[key]",
            "item = item + cumsum if batch.cumsum(key, item) else item",
            "batch[key].append(item)",
            "cumsum += num_nodes",
            "",
            "for key in keys:",
            "-            batch[key] = torch.cat(batch[key], dim=data_list[0].cat_dim(key))",
            "+            batch[key] = torch.cat(",
            "+                batch[key], dim=data_list[0].cat_dim(key, batch[key][0]))",
            "batch.batch = torch.cat(batch.batch, dim=-1)",
            "return batch.contiguous()"
        ]
    },
    {
        "number": 6810,
        "comments": "",
        "commit_message": "Fixes to enable dev branch to move to pytorch-1.2 (#2001)\n\n* Fixes to enable dev branch to move to pytorch-1.2\n\n* change name for published cpu wheels\n\n* fix failing unit tests\n\n* change docs conf\n\n* change tracking example\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def scale_and_mask(tensor, scale=1.0, mask=None):",
            ":param scale: a positive scale",
            ":type scale: torch.Tensor or number",
            ":param mask: an optional packed tensor mask",
            "-    :type mask: torch.ByteTensor or None",
            "+    :type mask: torch.BoolTensor or None",
            "\"\"\"",
            "if isinstance(scale, torch.Tensor) and scale.dim():",
            "raise NotImplementedError('non-scalar scale is not supported')"
        ]
    },
    {
        "number": 6811,
        "comments": "",
        "commit_message": "Fixed audio/bag/binary feature tests to work on GPU (#1600)\n\n* Fixed audio/bag/binary feature tests to work on GPU\n\n* Added timeseries feature test (#1601)\n\n* Removed device transfer calls from encoders to features\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def audio_config():",
            "@pytest.mark.parametrize(\"encoder\", [\"rnn\", \"stacked_cnn\", \"parallel_cnn\", \"stacked_parallel_cnn\", \"rnn\", \"cnnrnn\"])",
            "def test_audio_input_feature(audio_config: Dict, encoder: str) -> None:",
            "audio_config.update({\"encoder\": encoder})",
            "-    audio_input_feature = AudioInputFeature(audio_config)",
            "-    audio_tensor = torch.randn([BATCH_SIZE, SEQ_SIZE, AUDIO_W_SIZE], dtype=torch.float32)",
            "+    audio_input_feature = AudioInputFeature(audio_config).to(DEVICE)",
            "+    audio_tensor = torch.randn([BATCH_SIZE, SEQ_SIZE, AUDIO_W_SIZE], dtype=torch.float32).to(DEVICE)",
            "encoder_output = audio_input_feature(audio_tensor)",
            "-    assert encoder_output[\"encoder_output\"].shape[1:] == audio_input_feature.encoder_obj.output_shape",
            "+    assert encoder_output[\"encoder_output\"].shape[1:] == audio_input_feature.output_shape"
        ]
    },
    {
        "number": 6816,
        "comments": "",
        "commit_message": "bugfix in GST\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def main(args):  # pylint: disable=redefined-outer-name",
            "print(\" > Partial model initialization.\")",
            "model_dict = model.state_dict()",
            "model_dict = set_init_dict(model_dict, checkpoint['model'], c)",
            "+            # torch.save(model_dict, os.path.join(OUT_PATH, 'state_dict.pt'))",
            "+            # print(\"State Dict saved for debug in: \", os.path.join(OUT_PATH, 'state_dict.pt'))",
            "model.load_state_dict(model_dict)",
            "del model_dict"
        ]
    },
    {
        "number": 6819,
        "comments": "",
        "commit_message": "Move complex tensor related functions to espnet2/enh/layers/complex_utils.py; Refactor complex support code; fix unit tests\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class MaskEstimator(torch.nn.Module):",
            "",
            "# Calculate amplitude: (B, C, T, F) -> (B, C, T, F)",
            "if isinstance(xs, ComplexTensor) or (",
            "-            is_torch_1_8_plus and torch.is_complex(xs)",
            "+            is_torch_1_9_plus and torch.is_complex(xs)",
            "):",
            "xs = (xs.real ** 2 + xs.imag ** 2) ** 0.5",
            "# xs: (B, C, T, F) -> xs: (B * C, T, F)"
        ]
    },
    {
        "number": 6820,
        "comments": "",
        "commit_message": "Fix Batch Normalization inference behavior when virtual_batch_size is set\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class BatchNormalizationBase(Layer):",
            "offset += then_offset",
            "return (scale, offset)",
            "",
            "-        # Determine a boolean value for `training`: could be True, False, or",
            "-        # None.",
            "-        training_value = control_flow_util.constant_value(training)",
            "if training_value == False:  # noqa: E712",
            "mean, variance = self.moving_mean, self.moving_variance",
            "else:"
        ]
    },
    {
        "number": 6823,
        "comments": "",
        "commit_message": "New termwarn, fixed tensorflow2 tensorboard with logging, history._process works, made keras backwards compatable, removed http log from cli\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Watcher(object):",
            "self._generator = directory_watcher.DirectoryWatcher(",
            "logdir,",
            "loader(save, namespace),",
            "-            io_wrapper.IsTensorFlowEventsFile)",
            "+            IsNewTensorFlowEventsFile)",
            "self._first_event_timestamp = None",
            "self._shutdown = False",
            "self._thread = threading.Thread(target=self._thread_body)"
        ]
    },
    {
        "number": 6824,
        "comments": "",
        "commit_message": "fix linter error\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class SSIM(nn.Module):",
            "channel: int) -> torch.Tensor:",
            "return F.conv2d(input, kernel, padding=self.padding, groups=channel)",
            "",
            "-    def forward(self, img1: torch.Tensor, img2: torch.Tensor) -> torch.Tensor:  # type: ignore",
            "+    def forward(  # type: ignore",
            "+            self,",
            "+            img1: torch.Tensor,",
            "+            img2: torch.Tensor) -> torch.Tensor:",
            "if not torch.is_tensor(img1):",
            "raise TypeError(\"Input img1 type is not a torch.Tensor. Got {}\"",
            ".format(type(img1)))"
        ]
    },
    {
        "number": 6832,
        "comments": "",
        "commit_message": "Fixed failing test for searching where (#4744)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def where(",
            "out: Optional[Union[tf.Tensor, tf.Variable]] = None,",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "x1, x2 = ivy.promote_types_of_inputs(x1, x2)",
            "-    return tf.experimental.numpy.where(condition, x1, x2)",
            "+    return tf.cast(tf.experimental.numpy.where(condition, x1, x2), x1.dtype)",
            "",
            "",
            "# Extra #"
        ]
    },
    {
        "number": 6833,
        "comments": "",
        "commit_message": "fix using gpu when builtin float16\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class CTC(torch.nn.Module):",
            "ys_hat = ys_hat.transpose(0, 1)",
            "if self.ctc_type == \"warpctc\" or dtype == torch.float16:",
            "# warpctc only supports float32",
            "+            # torch.ctc does not support float16 (#1751)",
            "ys_hat = ys_hat.to(dtype=torch.float32)",
            "-        else:",
            "+        if self.ctc_type == \"builtin\":",
            "# use GPU when using the cuDNN implementation",
            "ys_true = to_device(self, ys_true)",
            "self.loss = to_device(self, self.loss_fn(ys_hat, ys_true, hlens, olens)).to(dtype=dtype)"
        ]
    },
    {
        "number": 6834,
        "comments": "",
        "commit_message": "bugfix\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "print('F', F.grad)",
            "i = torch.LongTensor([[0, 1], [2, 0]])",
            "v = torch.FloatTensor([3, 4])",
            "a = torch.sparse.FloatTensor(i, v, torch.Size([3, 3]))",
            "+print(a.to_dense())",
            "f = torch.FloatTensor([[1, 2], [3, 4], [5, 6]])",
            "A = Variable(a, requires_grad=True)",
            "F = Variable(f, requires_grad=True)",
            "-",
            "out = mm()(A, F)",
            "out = out.mean()",
            "out.backward()"
        ]
    },
    {
        "number": 6835,
        "comments": "",
        "commit_message": "support prefix for FeedfreeInferenceRunner\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Callback(object):",
            "def trigger_epoch(self):",
            "\"\"\"",
            "Triggered after every epoch.",
            "-",
            "-        In this function, ``self.epoch_num`` would be the number of epoch finished.",
            "\"\"\"",
            "self.epoch_num += 1",
            "self._trigger_epoch()"
        ]
    },
    {
        "number": 6838,
        "comments": "",
        "commit_message": "Fix unit test failures with torch==1.1.0 (#1840)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def setup(app):",
            "",
            "# @jpchen's hack to get rtd builder to install latest pytorch",
            "if 'READTHEDOCS' in os.environ:",
            "-    os.system('pip install https://download.pytorch.org/whl/cpu/torch-1.0.0-cp27-cp27mu-linux_x86_64.whl')",
            "+    os.system('pip install https://download.pytorch.org/whl/cpu/torch-1.1.0-cp27-cp27mu-linux_x86_64.whl')"
        ]
    },
    {
        "number": 6839,
        "comments": "",
        "commit_message": "Fixed failing gradient test for elementwise trunc (#6255)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def trunc(",
            "elif not (\"int\" in str(x.dtype)):",
            "if not ret.get_shape().ndims == 0:",
            "ret = tf.tensor_scatter_nd_update(",
            "-                x, tf.where(tf.greater(x, 0)), tf.math.floor(x[x > 0])",
            "+                x, tf.where(tf.greater_equal(x, 0)), tf.math.floor(x[x >= 0])",
            ")",
            "ret = tf.tensor_scatter_nd_update(",
            "ret, tf.where(tf.less(x, 0)), tf.math.ceil(x[x < 0])",
            ")",
            "else:",
            "-            ret = (tf.math.floor if ret > 0 else tf.math.ceil)(ret)",
            "+            ret = (tf.math.floor if ret >= 0 else tf.math.ceil)(ret)",
            "return ret"
        ]
    },
    {
        "number": 6841,
        "comments": "",
        "commit_message": "TF: Fix #2247, explicitly squeeze image to support matplotlib<3.3.0 (#2248)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def predict_seq2seq(net, src_sentence, src_vocab, tgt_vocab, num_steps,",
            "if pred == tgt_vocab['<eos>']:",
            "break",
            "output_seq.append(pred.numpy())",
            "-    return ' '.join(tgt_vocab.to_tokens(tf.reshape(output_seq, shape = -1).numpy().tolist())), attention_weight_seq# Alias defined in config.ini",
            "+    return ' '.join(tgt_vocab.to_tokens(tf.reshape(output_seq, shape = -1).numpy().tolist())), attention_weight_seq",
            "+",
            "+",
            "+# Alias defined in config.ini",
            "size = lambda a: tf.size(a).numpy()",
            "",
            "reshape = tf.reshape"
        ]
    },
    {
        "number": 6843,
        "comments": "",
        "commit_message": "fix -inf issue in focal loss\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class FocalLoss(nn.Module):",
            "device=input.device, dtype=input.dtype)",
            "",
            "# compute the actual focal loss",
            "-        prob = input_soft * target_one_hot",
            "+        prob = input_soft * target_one_hot + self.eps",
            "focal = -torch.log(prob) * self.alpha * (1. - prob) ** self.gamma",
            "-        loss_tmp = 1. - torch.sum(focal, dim=1)",
            "+        loss_tmp = torch.sum(focal, dim=1)",
            "",
            "loss = -1",
            "if self.reduction == 'none':"
        ]
    },
    {
        "number": 6848,
        "comments": "",
        "commit_message": "Test fixes for compatibility with PyTorch master (#1416)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_gaussian_mixture_model():",
            "cluster_assignments = dist.Categorical(true_mix_proportions).sample(torch.Size((N,)))",
            "data = dist.Normal(true_cluster_means[cluster_assignments], 1.0).sample()",
            "nuts_kernel = NUTS(gmm, adapt_step_size=True, max_iarange_nesting=1)",
            "-    mcmc_run = MCMC(nuts_kernel, num_samples=500, warmup_steps=200).run(data)",
            "+    mcmc_run = MCMC(nuts_kernel, num_samples=300, warmup_steps=100).run(data)",
            "posterior = EmpiricalMarginal(mcmc_run, sites=[\"phi\", \"cluster_means\"]).mean.sort()[0]",
            "assert_equal(posterior[0], true_mix_proportions, prec=0.05)",
            "assert_equal(posterior[1], true_cluster_means, prec=0.2)"
        ]
    },
    {
        "number": 6850,
        "comments": "",
        "commit_message": "Fix tensors to literal\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def tensors_to_literals(tensor_list):",
            "",
            "for tensor in tensor_list:",
            "",
            "-            if type(tensor) == torch.tensor:",
            "+            if type(tensor) == torch.Tensor:",
            "",
            "literal_list.append(tensor.item())"
        ]
    },
    {
        "number": 6852,
        "comments": "",
        "commit_message": "Update link in wiki_bio dataset (#3651)\n\n* update link in wiki_bio dataset\n\n* run linter and update dummy data\n\n* fix markdown so that test passes (even though I didnt break it)\n",
        "label": "",
        "answer": "no",
        "change": [
            "class WikiBio(datasets.GeneratorBasedBuilder):",
            "",
            "def _split_generators(self, dl_manager):",
            "\"\"\"Returns SplitGenerators.\"\"\"",
            "-        my_urls = _URL",
            "-        data_dir = dl_manager.download_and_extract(my_urls)",
            "+        data_dir = dl_manager.download_and_extract(_URL)",
            "data_path = os.path.join(data_dir, \"wikipedia-biography-dataset\")",
            "return [",
            "datasets.SplitGenerator("
        ]
    },
    {
        "number": 6857,
        "comments": "",
        "commit_message": "Fix complex support unit tests; Fix deprecation warnings in enh related tests\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TemporalConvNet(nn.Module):",
            "elif self.mask_nonlinear == \"relu\":",
            "est_mask = F.relu(score)",
            "elif self.mask_nonlinear == \"sigmoid\":",
            "-            est_mask = F.sigmoid(score)",
            "+            est_mask = torch.sigmoid(score)",
            "elif self.mask_nonlinear == \"tanh\":",
            "-            est_mask = F.tanh(score)",
            "+            est_mask = torch.tanh(score)",
            "else:",
            "raise ValueError(\"Unsupported mask non-linear function\")",
            "return est_mask"
        ]
    },
    {
        "number": 6863,
        "comments": "",
        "commit_message": "[RLlib] Tf2x preparation; part 2 (upgrading `try_import_tf()`). (#9136)\n\n* WIP.\n\n* Fixes.\n\n* LINT.\n\n* WIP.\n\n* WIP.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* WIP.\n\n* Fixes.\n\n* Test\n\n* Fix.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* LINT.\n",
        "label": "",
        "answer": "no",
        "change": [
            "class _QueueRunner(threading.Thread):",
            "",
            "def __init__(self, input_reader, queue, keys, dtypes):",
            "threading.Thread.__init__(self)",
            "-        self.sess = tf.get_default_session()",
            "+        self.sess = tf1.get_default_session()",
            "self.daemon = True",
            "self.input_reader = input_reader",
            "self.keys = keys",
            "self.queue = queue",
            "-        self.placeholders = [tf.placeholder(dtype) for dtype in dtypes]",
            "+        self.placeholders = [tf1.placeholder(dtype) for dtype in dtypes]",
            "self.enqueue_op = queue.enqueue(dict(zip(keys, self.placeholders)))",
            "",
            "def enqueue(self, batch):"
        ]
    },
    {
        "number": 6865,
        "comments": "",
        "commit_message": "Fix typo\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def gpt2_model_mesh(features, labels, mode, params):",
            "",
            "output = gpt2.model(X=features, params=params, mesh=mesh,",
            "past=None, reuse=tf.AUTO_REUSE,",
            "-                                    train=mode==tf.estimator.ModeKeys.TRAIN, mesh=mesh)",
            "+                                    train=mode==tf.estimator.ModeKeys.TRAIN)",
            "",
            "",
            "# logits :: [batch, seq, vocab]"
        ]
    },
    {
        "number": 6868,
        "comments": "",
        "commit_message": "[Train] Fix `prepare_data_loader` with `enable_reproducibility` (#30266)\n\nCalling train.torch.enable_reproducibility before train.torch.prepare_data_loader causes an exception to be raised of the num_workers in DataLoader is bigger than 0 and the worker_init_fn in DataLoader is not set. The exception is caused by the worker_init_fn, which has a value of None, being used as a callable in seeded_worker_init_fn. This was untested.\n\nThis PR fixes this oversight and ensures that this is tested in CI (and also removes a duplicate test in the process).\n\nSigned-off-by: Antoni Baum <antoni.baum@protonmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_enable_reproducibility(ray_start_4_cpus_2_gpus, use_gpu):",
            "torch.randn(dataset_length, 3, 32, 32),",
            "torch.randint(low=0, high=1000, size=(dataset_length,)),",
            ")",
            "-        dataloader = torch.utils.data.DataLoader(dataset, batch_size=64)",
            "+",
            "+        # num_workers > 0 tests for https://github.com/ray-project/ray/issues/30247",
            "+        dataloader = torch.utils.data.DataLoader(",
            "+            dataset, batch_size=64, num_workers=data_loader_num_workers",
            "+        )",
            "dataloader = train.torch.prepare_data_loader(dataloader)",
            "",
            "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)"
        ]
    },
    {
        "number": 6872,
        "comments": "",
        "commit_message": "[TPU] Doc, fix xla_spawn.py, only preprocess dataset once (#4223)\n\n* [TPU] Doc, fix xla_spawn.py, only preprocess dataset once\n\n* Update examples/README.md\n\n* [xla_spawn] Add `_mp_fn` to other Trainer scripts\n\n* [TPU] Fix: eval dataloader was None\n",
        "label": "",
        "answer": "no",
        "change": [
            "def main():",
            "eval_datasets = [eval_dataset]",
            "if data_args.task_name == \"mnli\":",
            "mnli_mm_data_args = dataclasses.replace(data_args, task_name=\"mnli-mm\")",
            "-            eval_datasets.append(",
            "-                GlueDataset(mnli_mm_data_args, tokenizer=tokenizer, local_rank=training_args.local_rank, evaluate=True)",
            "-            )",
            "+            eval_datasets.append(GlueDataset(mnli_mm_data_args, tokenizer=tokenizer, evaluate=True))",
            "",
            "for eval_dataset in eval_datasets:",
            "result = trainer.evaluate(eval_dataset=eval_dataset)"
        ]
    },
    {
        "number": 6876,
        "comments": "",
        "commit_message": "Fix RoI pooling for resnet101.\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Resnet101(Network):",
            "self._layers['conv5_3'] = net",
            "with tf.variable_scope('resnet_v1_101', 'resnet_v1_101',",
            "regularizer=tf.contrib.layers.l2_regularizer(cfg.TRAIN.WEIGHT_DECAY)):",
            "-   # build the anchors for the image",
            "+      # build the anchors for the image",
            "self._anchor_component()",
            "",
            "# rpn"
        ]
    },
    {
        "number": 6877,
        "comments": "",
        "commit_message": "fix more tests\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class UnetModelTests(ModelTesterMixin, unittest.TestCase):",
            "# fmt: off",
            "expected_output_slice = torch.tensor([0.2891, -0.1899, 0.2595, -0.6214, 0.0968, -0.2622, 0.4688, 0.1311, 0.0053])",
            "# fmt: on",
            "-        self.assertTrue(torch.allclose(output_slice, expected_output_slice, rtol=1e-3))",
            "+        self.assertTrue(torch.allclose(output_slice, expected_output_slice, rtol=1e-2))",
            "",
            "",
            "class GlideSuperResUNetTests(ModelTesterMixin, unittest.TestCase):"
        ]
    },
    {
        "number": 6888,
        "comments": "",
        "commit_message": "upgrade detection / some v2 compat fix\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class SingleCostTrainer(TowerTrainer):",
            "",
            "grads_no_vars = xla.compile(xla_func)",
            "if ctx.has_own_variables:",
            "-                    varlist = ctx.get_collection_in_tower(tf.GraphKeys.TRAINABLE_VARIABLES)",
            "+                    varlist = ctx.get_collection_in_tower(tfv1.GraphKeys.TRAINABLE_VARIABLES)",
            "else:",
            "-                    varlist = tf.trainable_variables()",
            "+                    varlist = tfv1.trainable_variables()",
            "return list(zip(grads_no_vars, varlist))",
            "",
            "return get_grad_fn"
        ]
    },
    {
        "number": 6895,
        "comments": "",
        "commit_message": "bugfix\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class InMemoryDataset(Dataset):",
            "",
            "for item, key in product(data_list, keys):",
            "data[key].append(item[key])",
            "-            s = slices[key][-1] + item[key].size(item.cat_dim(key))",
            "+            s = slices[key][-1] + item[key].size(item.cat_dim(key, item))",
            "slices[key].append(s)",
            "",
            "for key in keys:",
            "-            data[key] = torch.cat(data[key], dim=data_list[0].cat_dim(key))",
            "+            data[key] = torch.cat(",
            "+                data[key], dim=data_list[0].cat_dim(key, item))",
            "slices[key] = torch.LongTensor(slices[key])",
            "",
            "return data, slices"
        ]
    },
    {
        "number": 6896,
        "comments": "",
        "commit_message": "[CLIP] fix logit_scale init (#13436)\n\n* fix logit_scale init\n\n* add logit_scale_init_value as config param\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class CLIPModel(CLIPPreTrainedModel):",
            "",
            "self.visual_projection = nn.Linear(self.vision_embed_dim, self.projection_dim, bias=False)",
            "self.text_projection = nn.Linear(self.text_embed_dim, self.projection_dim, bias=False)",
            "-        self.logit_scale = nn.Parameter(torch.ones([]))",
            "+        self.logit_scale = nn.Parameter(torch.ones([]) * self.config.logit_scale_init_value)",
            "",
            "self.init_weights()"
        ]
    },
    {
        "number": 6898,
        "comments": "",
        "commit_message": "fixed uint8 warning (#2241)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class BBoxHead(nn.Module):",
            "keep_inds = pos_is_gts_.new_ones(num_rois)",
            "keep_inds[:len(pos_is_gts_)] = pos_keep",
            "",
            "-            bboxes_list.append(bboxes[keep_inds])",
            "+            bboxes_list.append(bboxes[keep_inds.type(torch.bool)])",
            "",
            "return bboxes_list"
        ]
    },
    {
        "number": 6899,
        "comments": "",
        "commit_message": "fix the supernode context\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class GatSuperNode(MessagePassing):",
            "",
            "# this is not correct it should be more hs and not x_i there based on the paper supplementary table 3!",
            "# in the paper it's h_s_ex in the pytorch it's x !",
            "-        cs_i = scatter_add( torch.mul(asv, self.mol_attend(self.dropout(h_s_ex))).transpose(0,1), \\",
            "+        cs_i = scatter_add( torch.mul(asv, self.mol_attend(self.dropout(x))).transpose(0,1), \\",
            "batch, dim_size=superatom_num).transpose(0,1)",
            "",
            "cs_i = F.elu(cs_i)"
        ]
    },
    {
        "number": 6908,
        "comments": "",
        "commit_message": "Making TF TransfoXL model compliant with AMP (#10264)\n\n* Fix AMP\n\n* Apply style\n\n* Remove unused import\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TFAdaptiveSoftmaxMask(tf.keras.layers.Layer):",
            "cur_logprob = self._gather_logprob(cur_tail_logprob, cur_target)",
            "cur_logprob += cur_head_logprob[:, self.cutoff_ends[1] + i - 1]",
            "if target is not None:",
            "-                    loss += tf.scatter_nd(mask_idx, -cur_logprob, tf.cast(shape_list(loss), dtype=tf.int64))",
            "+                    loss += tf.scatter_nd(mask_idx, -cur_logprob, shape_list(loss))",
            "out = tf.concat(out, axis=-1)",
            "",
            "if target is not None:"
        ]
    },
    {
        "number": 6912,
        "comments": "",
        "commit_message": "BatchNorm2D -> BatchNorm2d (#558)\n\n* BatchNorm2D -> BatchNorm2d\n\n* Fix typo\n",
        "label": "",
        "answer": "no",
        "change": [
            "class SpeedyResNet:",
            "# TODO: add whitening",
            "self.net = [",
            "nn.Conv2d(3, 64, kernel_size=1),",
            "-      nn.BatchNorm2D(64, track_running_stats=False, eps=1e-12, momentum=0.8),",
            "+      nn.BatchNorm2d(64, track_running_stats=False, eps=1e-12, momentum=0.8),",
            "lambda x: x.relu(),",
            "ConvGroup(64, 128, short=False),",
            "ConvGroup(128, 256, short=True),"
        ]
    },
    {
        "number": 6913,
        "comments": "",
        "commit_message": "Clean background_labels in the dense heads (#3221)\n\n* Clean background_labels in the dense heads\n\n* clean format\n\n* Reformat\n\n* Fix CI bug\n\n* fix fsaf and the comments\n\n* fix newly added heads\n\n* fix yolact\n\n* complete comments\n",
        "label": "",
        "answer": "no",
        "change": [
            "def binary_cross_entropy(pred,",
            "torch.Tensor: The calculated loss",
            "\"\"\"",
            "if pred.dim() != label.dim():",
            "-        label, weight = _expand_binary_labels(label, weight, pred.size(-1))",
            "+        label, weight = _expand_onehot_labels(label, weight, pred.size(-1))",
            "",
            "# weighted element-wise losses",
            "if weight is not None:"
        ]
    },
    {
        "number": 6916,
        "comments": "",
        "commit_message": "Fix a few issues loading pretrained vit/bit npz weights w/ num_classes=0 __init__ arg. Missed a few other small classifier handling detail on Mlp, GhostNet, Levit. Should fix #713\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Visformer(nn.Module):",
            "self.num_features = embed_dim if self.vit_stem else embed_dim * 2",
            "self.norm = norm_layer(self.num_features)",
            "self.global_pool, self.head = create_classifier(self.num_features, self.num_classes, pool_type=global_pool)",
            "-        self.head = nn.Linear(self.num_features, num_classes)",
            "",
            "# weights init",
            "if self.pos_embed:"
        ]
    },
    {
        "number": 6921,
        "comments": "",
        "commit_message": "fix typo in test gpt2\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class GPT2ModelLanguageGenerationTest(unittest.TestCase):",
            "@slow",
            "def test_lm_generate_gpt2(self):",
            "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")",
            "-        input_ids = torch.tensor([[463, 3290]], dtype=torch.long, device=torch_device)  # The dog",
            "+        input_ids = torch.tensor([[464, 3290]], dtype=torch.long, device=torch_device)  # The dog",
            "expected_output_ids = [",
            "464,",
            "3290,"
        ]
    },
    {
        "number": 6925,
        "comments": "",
        "commit_message": "Clarify and fix for ResNeXt\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class ResNeXtBlock(tf.keras.Model):",
            "bot_channels = int(round(num_channels * bot_mul))",
            "self.conv1 = tf.keras.layers.Conv2D(bot_channels, 1, strides=1)",
            "self.conv2 = tf.keras.layers.Conv2D(bot_channels, 3, strides=strides,",
            "-                                            padding=\"same\",  groups=bot_channels//groups)",
            "+                                            padding=\"same\",",
            "+                                            groups=bot_channels//groups)",
            "self.conv3 = tf.keras.layers.Conv2D(num_channels, 1, strides=1)",
            "self.bn1 = tf.keras.layers.BatchNormalization()",
            "self.bn2 = tf.keras.layers.BatchNormalization()",
            "self.bn3 = tf.keras.layers.BatchNormalization()",
            "if use_1x1conv:",
            "-            self.conv4 = tf.keras.layers.Conv2D(num_channels, 1, strides=strides)",
            "+            self.conv4 = tf.keras.layers.Conv2D(num_channels, 1,",
            "+                                                strides=strides)",
            "self.bn4 = tf.keras.layers.BatchNormalization()",
            "else:",
            "self.conv4 = None"
        ]
    },
    {
        "number": 6930,
        "comments": "",
        "commit_message": "Fix CIs for PyTorch 1.13 (#20686)\n\n* fix 1\n\n* fix 2\n\n* fix 3\n\n* fix 4\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class BloomForSequenceClassification(BloomPreTrainedModel):",
            "sequence_lengths = -1",
            "else:",
            "if input_ids is not None:",
            "-                sequence_lengths = torch.ne(input_ids, self.config.pad_token_id).sum(dim=-1) - 1",
            "+                sequence_lengths = (torch.ne(input_ids, self.config.pad_token_id).sum(-1) - 1).to(logits.device)",
            "else:",
            "sequence_lengths = -1",
            "logger.warning("
        ]
    },
    {
        "number": 6931,
        "comments": "",
        "commit_message": "misc small fixes for old TF version (#810)\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class StagingInput(FeedfreeInput):",
            "def _setup_graph(self):",
            "self.stage_op = self._input._get_stage_op()",
            "unstage_ops = self._input._get_unstage_ops()",
            "-            unstage_op = tf.group(unstage_ops, name='unstage_all')",
            "+            unstage_op = tf.group(*unstage_ops, name='unstage_all')",
            "self._check_dependency_op = unstage_ops[0]",
            "self.fetches = tf.train.SessionRunArgs(",
            "fetches=[self.stage_op, unstage_op])"
        ]
    },
    {
        "number": 6933,
        "comments": "",
        "commit_message": "fix a bug cause by torch.tensor()\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Loss(nn.Module):",
            "# positive mask will never selected",
            "# \u83b7\u53d6\u8d1f\u6837\u672c",
            "con_neg = con.clone()",
            "-        con_neg[mask] = torch.tensor(0.0)",
            "+        con_neg[mask] = 0.0",
            "# \u6309\u7167confidence_loss\u964d\u5e8f\u6392\u5217 con_idx(Tensor: [N, 8732])",
            "_, con_idx = con_neg.sort(dim=1, descending=True)",
            "_, con_rank = con_idx.sort(dim=1)  # \u8fd9\u4e2a\u6b65\u9aa4\u6bd4\u8f83\u5de7\u5999"
        ]
    },
    {
        "number": 6940,
        "comments": "",
        "commit_message": "Fix model type in lightning (#4451)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class LightningModule(pl.LightningModule):",
            "Lightning modules used in NNI should inherit this class.",
            "\"\"\"",
            "",
            "-    def set_model(self, model: Union[Type[nn.Module], nn.Module]) -> NoReturn:",
            "-        if isinstance(model, type):",
            "-            self.model = model()",
            "-        else:",
            "+    def set_model(self, model: Union[Type[nn.Module], nn.Module]) -> None:",
            "+        if isinstance(model, nn.Module):",
            "self.model = model",
            "+        else:",
            "+            self.model = model()",
            "",
            "",
            "Trainer = nni.trace(pl.Trainer)"
        ]
    },
    {
        "number": 6951,
        "comments": "",
        "commit_message": "Fix generation bug with large beam sizes (>50)\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class SequenceGenerator(object):",
            "# and values < cand_size indicate candidate active hypos.",
            "# After, the min values per row are the top candidate active hypos",
            "active_mask = buffer('active_mask')",
            "-            torch.add((eos_mask*cand_size).type_as(cand_offsets), cand_offsets[:eos_mask.size(1)],",
            "+            torch.add(eos_mask.type_as(cand_offsets)*cand_size, cand_offsets[:eos_mask.size(1)],",
            "out=active_mask)",
            "",
            "# get the top beam_size active hypotheses, which are just the hypos"
        ]
    },
    {
        "number": 6954,
        "comments": "",
        "commit_message": "fix test_sum for torch. out arg not provided by torch. handled by decorator\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def sum(",
            "dtype=dtype,",
            "out=out,",
            ")",
            "-    return torch.sum(input=x, dim=axis, dtype=dtype, keepdim=keepdims, out=out)",
            "+    return torch.sum(input=x, dim=axis, dtype=dtype, keepdim=keepdims)",
            "",
            "",
            "def var("
        ]
    },
    {
        "number": 6955,
        "comments": "",
        "commit_message": "Fix import order in model, reformatting.\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Synchronization(Optimizer):",
            "self.update_weight = update_weight",
            "",
            "def tf_step(self, time, variables, source_variables, **kwargs):",
            "-        last_update = tf.get_variable(name='last-update', dtype=tf.int32, initializer=(-self.update_frequency), trainable=False)",
            "+        last_update = tf.get_variable(",
            "+            name='last-update',",
            "+            dtype=tf.int32,",
            "+            initializer=(-self.update_frequency),",
            "+            trainable=False",
            "+        )",
            "",
            "def true_fn():",
            "diffs = list()"
        ]
    },
    {
        "number": 6956,
        "comments": "",
        "commit_message": "fix some unit tests\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_train_resume_sequence_tagging_training(results_base_path, tasks_base_pa",
            "corpus_1 = flair.datasets.ColumnCorpus(",
            "data_folder=tasks_base_path / \"fashion\", column_format={0: \"text\", 2: \"ner\"}",
            ")",
            "-    corpus_2 = flair.datasets.GERMEVAL(base_path=tasks_base_path)",
            "+    corpus_2 = flair.datasets.GERMEVAL_14(base_path=tasks_base_path)",
            "",
            "corpus = MultiCorpus([corpus_1, corpus_2])",
            "tag_dictionary = corpus.make_tag_dictionary(\"ner\")"
        ]
    },
    {
        "number": 6962,
        "comments": "",
        "commit_message": "Fix unbalanced gradients bug in ZeRO-2 gradient accumulation (#545)\n\n* Use zero-tensors for missing gradients to avoid size mismatch\n\n* Unit test for unbalanced gradients in ZeRO\n\n* Formatting fixes\n",
        "label": "",
        "answer": "no",
        "change": [
            "class LinearStack(torch.nn.Module):",
            "self.output_dim = output_dim",
            "self.hidden_dim = hidden_dim",
            "",
            "-        self.input_layer = VerboseLinear(in_features=self.input_dim,",
            "-                                         out_features=self.hidden_dim)",
            "+        self.input_layer = torch.nn.Linear(in_features=self.input_dim,",
            "+                                           out_features=self.hidden_dim)",
            "self.layers = torch.nn.ModuleList([",
            "torch.nn.Linear(in_features=self.hidden_dim,",
            "out_features=self.hidden_dim,"
        ]
    },
    {
        "number": 6963,
        "comments": "",
        "commit_message": "Fix out of index issue in topk.\n\nIn topk, it makes dense and fixed node tensor according to x.shape and initialize all values to -2.\nSo, a graph which is smaller than biggest graph in batch has dummy nodes.\nAnd then, x is copied to dense tensor and ordered by score.\nIf some scores in a graph which is smaller than biggest graph are smaller than -2,\nthey are moved to out of node size of original graph and dummy nodes are filled to those position.\nSo, returned perm has dummy node that have out of index.\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def topk(x, ratio, batch, min_score=None, tol=1e-7):",
            "index = torch.arange(batch.size(0), dtype=torch.long, device=x.device)",
            "index = (index - cum_num_nodes[batch]) + (batch * max_num_nodes)",
            "",
            "-        dense_x = x.new_full((batch_size * max_num_nodes, ), -2)",
            "+        dense_x = x.new_full((batch_size * max_num_nodes, ),",
            "+                             torch.finfo(x.dtype).min)",
            "dense_x[index] = x",
            "dense_x = dense_x.view(batch_size, max_num_nodes)"
        ]
    },
    {
        "number": 6965,
        "comments": "",
        "commit_message": "Fix TF2 / rllib test (#5846)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class RecurrentTFModelV2(TFModelV2):",
            "shape=(None, obs_space.shape[0]))",
            "state_in_h = tf.keras.layers.Input(shape=(256, ))",
            "state_in_c = tf.keras.layers.Input(shape=(256, ))",
            "-                seq_in = tf.keras.layers.Input(shape=())",
            "+                seq_in = tf.keras.layers.Input(shape=(), dtype=tf.int32)",
            "",
            "# Send to LSTM cell",
            "lstm_out, state_h, state_c = tf.keras.layers.LSTM("
        ]
    },
    {
        "number": 6968,
        "comments": "",
        "commit_message": "Rename variable to fix mypy issue\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class ScalarMix(torch.nn.Module):",
            "normed_weights = torch.nn.functional.softmax(",
            "torch.cat([parameter for parameter in self.scalar_parameters]), dim=0",
            ")",
            "-        normed_weights = torch.split(normed_weights, split_size_or_sections=1)",
            "+        normed_weights_split = torch.split(normed_weights, split_size_or_sections=1)",
            "",
            "pieces = []",
            "-        for weight, tensor in zip(normed_weights, tensors):",
            "+        for weight, tensor in zip(normed_weights_split, tensors):",
            "pieces.append(weight * tensor)",
            "return self.gamma * sum(pieces)"
        ]
    },
    {
        "number": 6970,
        "comments": "",
        "commit_message": "fix small formatting error in functional/backends/tensorflow/experimental/activations\n",
        "label": "",
        "answer": "no",
        "change": [
            "def batch_norm(",
            "variance = tf.math.reduce_variance(x, axis=dims)",
            "x = tf.transpose(x, perm=(0, *range(2, ndims), 1))",
            "ret = tf.nn.batch_normalization(x, mean, variance, offset, scale, eps)",
            "-    return tf.transpose(ret, perm=(0, ndims-1, *range(1, ndims-1)))",
            "+    return tf.transpose(ret, perm=(0, ndims - 1, *range(1, ndims - 1)))"
        ]
    },
    {
        "number": 6975,
        "comments": "",
        "commit_message": "fix #1416\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "logger = logging.getLogger(__name__)  # pylint: disable=invalid-name",
            "",
            "try:",
            "import tensorflow as tf",
            "-    assert int(tf.__version__[0]) >= 2",
            "+    assert hasattr(tf, '__version__') and int(tf.__version__[0]) >= 2",
            "_tf_available = True  # pylint: disable=invalid-name",
            "logger.info(\"TensorFlow version {} available.\".format(tf.__version__))",
            "except (ImportError, AssertionError):"
        ]
    },
    {
        "number": 6977,
        "comments": "",
        "commit_message": "Abstract accelerator (step 3) (#2677)\n\n* Integrate accelerator abstraction interface into deepspeed/\n\n* Fix error message in fp16/fused_optimizer\n\n* fix error message in fp16/unfused_optimizer.py\n\n* assign get_accelerator().pin_memory() result to input Tensor name\n\n* no need to check cuda and whether nvtx supported\n\n* move try-except into inner most block\n\n* call Event() and Stream() in get_accelerator() for data type\n\n* Make Stream and Event as properties of abstract interface so they can be used as data type in deepspeed\n\n* Apply op_builder backend api change from #2705 from @jeffra\n\n* fix tests where Builder NAME is used\n\n* keep original ...Builder.NAME interface instead of ...Builder().NAME interface\n\n* fix builder closure for installation\n\n* fix randomltd builder\n\n* add comments to clarify create_op_builder and get_op_builder\n\n* fix compatibility with pip install -e\n\nCo-authored-by: Cheng Li <pistasable@gmail.com>\nCo-authored-by: Olatunji Ruwase <olruwase@microsoft.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class NcclBackend(object):",
            "recvbuf_scale = [",
            "torch.zeros(1,",
            "dtype=worker_scale.dtype,",
            "-                        device=torch.device(local_rank)) for i in range(self.size)",
            "+                        device=torch.device(get_accelerator().device_name(local_rank)))",
            "+            for i in range(self.size)",
            "]",
            "",
            "# communication phase 1"
        ]
    },
    {
        "number": 6979,
        "comments": "",
        "commit_message": "Fix setting device when creating \"inf\" monitor value in `ModelCheckpoint` (#10118)\n\nCo-authored-by: thomas chaton <thomas@grid.ai>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class ModelCheckpoint(Callback):",
            "",
            "# do not save nan, replace with +/- inf",
            "if isinstance(current, torch.Tensor) and torch.isnan(current):",
            "-            current = torch.tensor(float(\"inf\" if self.mode == \"min\" else \"-inf\"))",
            "+            current = torch.tensor(float(\"inf\" if self.mode == \"min\" else \"-inf\"), device=current.device)",
            "",
            "filepath = self._get_metric_interpolated_filepath_name(monitor_candidates, trainer, del_filepath)"
        ]
    },
    {
        "number": 6981,
        "comments": "",
        "commit_message": "lots of fixes, tests working\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class QNAFModel(QModel):",
            "l_matrix = flat_stddev",
            "l_matrix = tf.exp(l_matrix)",
            "else:",
            "-            l_matrix = tf.map_fn(fn=tf.diag, elems=flat_stddev)",
            "+            l_matrix = tf.linalg.diag(diagonal=flat_stddev)",
            "",
            "l_entries = self.l_entries[name].apply(x=embedding)",
            "l_entries = tf.exp(l_entries)"
        ]
    },
    {
        "number": 6982,
        "comments": "",
        "commit_message": "Doc styler examples (#14953)\n\n* Fix bad examples\n\n* Add black formatting to style_doc\n\n* Use first nonempty line\n\n* Put it at the right place\n\n* Don't add spaces to empty lines\n\n* Better templates\n\n* Deal with triple quotes in docstrings\n\n* Result of style_doc\n\n* Enable mdx treatment and fix code examples in MDXs\n\n* Result of doc styler on doc source files\n\n* Last fixes\n\n* Break copy from\n",
        "label": "",
        "answer": "no",
        "change": [
            "class UniSpeechSatForPreTraining(UniSpeechSatPreTrainedModel):",
            "...     outputs = model(input_values, mask_time_indices=mask_time_indices)",
            "",
            ">>> # compute cosine similarity between predicted (=projected_states) and target (=projected_quantized_states)",
            "-        >>> cosine_sim = torch.cosine_similarity(",
            "-        ...     outputs.projected_states, outputs.projected_quantized_states, dim=-1",
            "-        ... )",
            "+        >>> cosine_sim = torch.cosine_similarity(outputs.projected_states, outputs.projected_quantized_states, dim=-1)",
            "",
            ">>> # show that cosine similarity is much higher than random",
            ">>> assert cosine_sim[mask_time_indices].mean() > 0.5"
        ]
    },
    {
        "number": 6984,
        "comments": "",
        "commit_message": "default fast model loading \ud83d\udd25 (#1115)\n\n* make accelerate hard dep\n\n* default fast init\n\n* move params to cpu when device map is None\n\n* handle device_map=None\n\n* handle torch < 1.9\n\n* remove device_map=\"auto\"\n\n* style\n\n* add accelerate in torch extra\n\n* remove accelerate from extras[\"test\"]\n\n* raise an error if torch is available but not accelerate\n\n* update installation docs\n\n* Apply suggestions from code review\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* improve defautl loading speed even further, allow disabling fats loading\n\n* address review comments\n\n* adapt the tests\n\n* fix test_stable_diffusion_fast_load\n\n* fix test_read_init\n\n* temp fix for dummy checks\n\n* Trigger Build\n\n* Apply suggestions from code review\n\nCo-authored-by: Anton Lozhkov <anton@huggingface.co>\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\nCo-authored-by: Anton Lozhkov <anton@huggingface.co>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class StableDiffusionInpaintLegacyPipelineIntegrationTests(unittest.TestCase):",
            ")",
            "",
            "pipe = StableDiffusionInpaintPipeline.from_pretrained(",
            "-            \"CompVis/stable-diffusion-v1-4\", revision=\"fp16\", torch_dtype=torch.float16, device_map=\"auto\"",
            "+            \"CompVis/stable-diffusion-v1-4\", revision=\"fp16\", torch_dtype=torch.float16",
            ")",
            "pipe.to(torch_device)",
            "pipe.set_progress_bar_config(disable=None)"
        ]
    },
    {
        "number": 6995,
        "comments": "",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class SelfAttentiveSpanExtractor(SpanExtractor):",
            "# Above we were masking the widths of spans with respect to the max",
            "# span width in the batch. Here we are masking the spans which were",
            "# originally passed in as padding.",
            "-            return attended_text_embeddings * span_indices_mask.unsqueeze(-1).float()",
            "+            return attended_text_embeddings * span_indices_mask.unsqueeze(-1)",
            "",
            "return attended_text_embeddings"
        ]
    },
    {
        "number": 6997,
        "comments": "",
        "commit_message": "fixing outer() tensorflow backend to pass array-api test\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def svd(",
            "def outer(",
            "x1: Union[tf.Tensor, tf.Variable],",
            "x2: Union[tf.Tensor, tf.Variable],",
            "-    out: Optional[tf.Tensor] = None",
            "+    out: Optional[tf.Tensor] = None,",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "-    ret = tf.experimental.numpy.outer(x1, x2, out=out)",
            "+    ret = tf.experimental.numpy.outer(x1, x2)",
            "if ivy.exists(out):",
            "return ivy.inplace_update(out, ret)",
            "return ret"
        ]
    },
    {
        "number": 6999,
        "comments": "",
        "commit_message": "bug fix in im2im\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Model(ModelDesc):",
            "fake_output = tf.image.grayscale_to_rgb(fake_output)",
            "viz = (tf.concat(2, [input, output, fake_output]) + 1.0) * 128.0",
            "viz = tf.cast(tf.clip_by_value(viz, 0, 255), tf.uint8, name='viz')",
            "-        tf.image_summary('gen', viz, max_outputs=max(30, BATCH))",
            "+        tf.summary.image('gen', viz, max_outputs=max(30, BATCH))",
            "",
            "all_vars = tf.trainable_variables()",
            "self.g_vars = [v for v in all_vars if v.name.startswith('gen/')]"
        ]
    },
    {
        "number": 7010,
        "comments": "",
        "commit_message": "Add metrics usage examples and tests (#1820)\n\n* add metrics usage examples and tests\n\n* update template\n\n* remove instruction sentence\n\n* add metrics dependencies to the tests requirements\n\n* try fix pip install timeout\n\n* try again\n\n* try again\n\n* try again\n\n* try again by moving unbabel-comet\n\n* try again by ignoring fixed deps of comet\n\n* fix some comet deps\n\n* move deps\n\n* try again\n\n* download wordnet for meteor\n\n* style\n\n* don't test comet on windows\n\n* style\n\n* remove comet comment\n",
        "label": "",
        "answer": "no",
        "change": [
            "class BERTScore(datasets.Metric):",
            "batch_size=batch_size,",
            ")",
            "output_dict = {",
            "-            \"precision\": P,",
            "-            \"recall\": R,",
            "-            \"f1\": F,",
            "+            \"precision\": P.tolist(),",
            "+            \"recall\": R.tolist(),",
            "+            \"f1\": F.tolist(),",
            "\"hashcode\": hashcode,",
            "}",
            "return output_dict"
        ]
    },
    {
        "number": 7013,
        "comments": "",
        "commit_message": "Inference support for `mps` device (#355)\n\n* Initial support for mps in Stable Diffusion pipeline.\n\n* Initial \"warmup\" implementation when using mps.\n\n* Make some deterministic tests pass with mps.\n\n* Disable training tests when using mps.\n\n* SD: generate latents in CPU then move to device.\n\nThis is especially important when using the mps device, because\ngenerators are not supported there. See for example\nhttps://github.com/pytorch/pytorch/issues/84288.\n\nIn addition, the other pipelines seem to use the same approach: generate\nthe random samples then move to the appropriate device.\n\nAfter this change, generating an image in MPS produces the same result\nas when using the CPU, if the same seed is used.\n\n* Remove prints.\n\n* Pass AutoencoderKL test_output_pretrained with mps.\n\nSampling from `posterior` must be done in CPU.\n\n* Style\n\n* Do not use torch.long for log op in mps device.\n\n* Perform incompatible padding ops in CPU.\n\nUNet tests now pass.\nSee https://github.com/pytorch/pytorch/issues/84535\n\n* Style: fix import order.\n\n* Remove unused symbols.\n\n* Remove MPSWarmupMixin, do not apply automatically.\n\nWe do apply warmup in the tests, but not during normal use.\nThis adopts some PR suggestions by @patrickvonplaten.\n\n* Add comment for mps fallback to CPU step.\n\n* Add README_mps.md for mps installation and use.\n\n* Apply `black` to modified files.\n\n* Restrict README_mps to SD, show measures in table.\n\n* Make PNDM indexing compatible with mps.\n\nAddresses #239.\n\n* Do not use float64 when using LDMScheduler.\n\nFixes #358.\n\n* Fix typo identified by @patil-suraj\n\nCo-authored-by: Suraj Patil <surajp815@gmail.com>\n\n* Adapt example to new output style.\n\n* Restore 1:1 results reproducibility with CompVis.\n\nHowever, mps latents need to be generated in CPU because generators\ndon't work in the mps device.\n\n* Move PyTorch nightly to requirements.\n\n* Adapt `test_scheduler_outputs_equivalence` ton MPS.\n\n* mps: skip training tests instead of ignoring silently.\n\n* Make VQModel tests pass on mps.\n\n* mps ddim tests: warmup, increase tolerance.\n\n* ScoreSdeVeScheduler indexing made mps compatible.\n\n* Make ldm pipeline tests pass using warmup.\n\n* Style\n\n* Simplify casting as suggested in PR.\n\n* Add Known Issues to readme.\n\n* `isort` import order.\n\n* Remove _mps_warmup helpers from ModelMixin.\n\nAnd just make changes to the tests.\n\n* Skip tests using unittest decorator for consistency.\n\n* Remove temporary var.\n\n* Remove spurious blank space.\n\n* Remove unused symbol.\n\n* Remove README_mps.\n\nCo-authored-by: Suraj Patil <surajp815@gmail.com>\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com> \n",
        "label": "",
        "answer": "no",
        "change": [
            "class NCSNppModelTests(ModelTesterMixin, unittest.TestCase):",
            "num_channels = 3",
            "",
            "noise = floats_tensor((batch_size, num_channels) + sizes).to(torch_device)",
            "-        time_step = torch.tensor(batch_size * [10]).to(torch_device)",
            "+        time_step = torch.tensor(batch_size * [10]).to(dtype=torch.int32, device=torch_device)",
            "",
            "return {\"sample\": noise, \"timestep\": time_step}"
        ]
    },
    {
        "number": 7018,
        "comments": "",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class CnnEncoder(Seq2VecEncoder):",
            "def get_output_dim(self) -> int:",
            "return self._output_dim",
            "",
            "-    def forward(self, tokens: torch.Tensor, mask: torch.Tensor):",
            "+    def forward(self, tokens: torch.Tensor, mask: torch.BoolTensor):",
            "if mask is not None:",
            "-            tokens = tokens * mask.unsqueeze(-1).float()",
            "+            tokens = tokens * mask.unsqueeze(-1)",
            "",
            "# Our input is expected to have shape `(batch_size, num_tokens, embedding_dim)`.  The",
            "# convolution layers expect input of shape `(batch_size, in_channels, sequence_length)`,"
        ]
    },
    {
        "number": 7024,
        "comments": "",
        "commit_message": "add options for anchors; fix alignment issue with resnet maxpool; more documentation.\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Network(object):",
            "y1 = tf.slice(rois, [0, 2], [-1, 1], name=\"y1\") / height",
            "x2 = tf.slice(rois, [0, 3], [-1, 1], name=\"x2\") / width",
            "y2 = tf.slice(rois, [0, 4], [-1, 1], name=\"y2\") / height",
            "-      bboxes = tf.concat([y1, x1, y2, x2], axis=1)",
            "+      # Won't be backpropagated to rois anyway, but to save time",
            "+      bboxes = tf.stop_gradient(tf.concat([y1, x1, y2, x2], axis=1))",
            "pre_pool_size = cfg.POOLING_SIZE * 2",
            "crops = tf.image.crop_and_resize(bottom, bboxes, tf.to_int32(batch_ids), [pre_pool_size, pre_pool_size], name=\"crops\")"
        ]
    },
    {
        "number": 7025,
        "comments": "",
        "commit_message": "[Bart] Fix: put dummy_inputs on correct device (#3398)\n\n* Dummy inputs to model.device\n\n* Move self.device to ModuleUtilsMixin\n",
        "label": "",
        "answer": "no",
        "change": [
            "class PretrainedBartModel(PreTrainedModel):",
            "@property",
            "def dummy_inputs(self):",
            "pad_token = self.config.pad_token_id",
            "-        input_ids = torch.tensor([[0, 6, 10, 4, 2], [0, 8, 12, 2, pad_token]])",
            "-        decoder_input_ids, decoder_attn_mask = _prepare_bart_decoder_inputs(self.config, input_ids,)",
            "+        input_ids = torch.tensor([[0, 6, 10, 4, 2], [0, 8, 12, 2, pad_token]], device=self.device)",
            "+        decoder_input_ids, decoder_attn_mask = _prepare_bart_decoder_inputs(self.config, input_ids)",
            "dummy_inputs = {",
            "\"decoder_input_ids\": decoder_input_ids,",
            "\"attention_mask\": input_ids.ne(pad_token),"
        ]
    },
    {
        "number": 7027,
        "comments": "",
        "commit_message": "Fix reference to tpu short seq length (#13686)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class FNetBasicFourierTransform(nn.Module):",
            "\"dft_mat_hidden\", torch.tensor(linalg.dft(config.hidden_size), dtype=torch.complex64)",
            ")",
            "self.register_buffer(",
            "-                    \"dft_mat_seq\", torch.tensor(linalg.dft(config.tpu_short_sequence_length), dtype=torch.complex64)",
            "+                    \"dft_mat_seq\", torch.tensor(linalg.dft(config.tpu_short_seq_length), dtype=torch.complex64)",
            ")",
            "self.fourier_transform = partial(",
            "two_dim_matmul, matrix_dim_one=self.dft_mat_seq, matrix_dim_two=self.dft_mat_hidden"
        ]
    },
    {
        "number": 7028,
        "comments": "",
        "commit_message": "Fix docstring issues (#2072)\n\n* Fix docstring module name of filesystems\n\n* Fix docstring missing blank after comma\n\n* Fix docstring missing trailing dot\n\n* Fix docstring cross-referencing in Returns\n\n* Fix docstring cross-referencing in description\n\n* Fix docstring non-rendered args descriptions\n\n* Fix docstring cross-reference content prefix\n\n* Fix docstring line length\n\n* Fix docstring of SplitGenerator\n\n* Fix docstring of Split\n\n* Fix docstring document cross-reference in Split\n\n* Fix docstring of DatasetDict.shuffle\n\n* Fix docstring of DatasetBuilder\n\n* Add docstring attributes of DatasetInfo\n\n* Change docstring default rendering of Attributes in conf.py\n\n* Fix docstring of list_datasets\n\n* Fix docstring of load_dataset\n",
        "label": "",
        "answer": "no",
        "change": [
            "def list_datasets(with_community_datasets=True, with_details=False):",
            "\"\"\"List all the datasets scripts available on HuggingFace AWS bucket.",
            "",
            "Args:",
            "-        with_community_datasets (Optional ``bool``): Include the community provided datasets (default: ``True``)",
            "-        with_details (Optional ``bool``): Return the full details on the datasets instead of only the short name (default: ``False``)",
            "+        with_community_datasets (``bool``, optional, default ``True``): Include the community provided datasets.",
            "+        with_details (``bool``, optional, default ``False``): Return the full details on the datasets instead of only the short name.",
            "\"\"\"",
            "api = HfApi()",
            "return api.dataset_list(with_community_datasets=with_community_datasets, id_only=bool(not with_details))"
        ]
    },
    {
        "number": 7029,
        "comments": "",
        "commit_message": "[Bug] Fix/flipping returns wrong transformation matrices (#648)\n\n* Proper transformation matrices returned test fixed\n\n* Coordinate tests added to hflip and vflip\n\n* Rounding added before converting to int\n\n* Out of bound assertions fixed\n\nCo-authored-by: Feher Gergo (CC-AD/EPE-Bp) <bosch.feher@extaccount.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestVerticalFlipFn:",
            "input.to(device)",
            "",
            "expected_transform = torch.tensor([[[1., 0., 0.],",
            "-                                            [0., -1., 3.],",
            "+                                            [0., -1., 2.],",
            "[0., 0., 1.]]])  # 1 x 3 x 3",
            "",
            "identity = torch.tensor([[[1., 0., 0.],"
        ]
    },
    {
        "number": 7030,
        "comments": "",
        "commit_message": "fix torch.nonzero usage in pytorch 1.5 (#2602)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class FSAFHead(RetinaHead):",
            "loc_weight = torch.ones_like(reg_loss)",
            "cls_weight = torch.ones_like(cls_loss)",
            "pos_flags = assigned_gt_inds >= 0  # positive pixel flag",
            "-        pos_indices = torch.nonzero(pos_flags).flatten()",
            "+        pos_indices = torch.nonzero(pos_flags, as_tuple=False).flatten()",
            "",
            "if pos_flags.any():  # pos pixels exist",
            "pos_assigned_gt_inds = assigned_gt_inds[pos_flags]"
        ]
    },
    {
        "number": 7035,
        "comments": "",
        "commit_message": "Fixed audio/bag/binary feature tests to work on GPU (#1600)\n\n* Fixed audio/bag/binary feature tests to work on GPU\n\n* Added timeseries feature test (#1601)\n\n* Removed device transfer calls from encoders to features\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def bag_config():",
            "@pytest.mark.parametrize(\"encoder\", [\"embed\"])",
            "def test_bag_input_feature(bag_config: Dict, encoder: str) -> None:",
            "bag_config.update({\"encoder\": encoder})",
            "-    bag_input_feature = BagInputFeature(bag_config)",
            "-    bag_tensor = torch.randn([BATCH_SIZE, SEQ_SIZE, BAG_W_SIZE], dtype=torch.float32)",
            "+    bag_input_feature = BagInputFeature(bag_config).to(DEVICE)",
            "+    bag_tensor = torch.randn([BATCH_SIZE, SEQ_SIZE, BAG_W_SIZE], dtype=torch.float32).to(DEVICE)",
            "encoder_output = bag_input_feature(bag_tensor)",
            "-    assert encoder_output[\"encoder_output\"].shape[1:][1:] == bag_input_feature.encoder_obj.output_shape",
            "+    assert encoder_output[\"encoder_output\"].shape[1:][1:] == bag_input_feature.output_shape"
        ]
    },
    {
        "number": 7036,
        "comments": "",
        "commit_message": "fix dump callback\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Callback(object):",
            "def before_train(self, trainer):",
            "self.trainer = trainer",
            "self.graph = tf.get_default_graph()",
            "-        self.sess = tf.get_default_session()",
            "self.epoch_num = 0",
            "self._before_train()"
        ]
    },
    {
        "number": 7037,
        "comments": "",
        "commit_message": "lintfixbot: Auto-commit fixed lint errors in codebase\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_tensorflow_shape(",
            "",
            "",
            "@handle_frontend_test(",
            "-fn_tree=\"tensorflow.shape_n\",",
            "-dtype_and_x=helpers.dtype_and_values(",
            "-    available_dtypes=helpers.get_dtypes(\"valid\"),",
            "-    max_num_dims=5),",
            "-output_dtype=st.sampled_from([\"int32\", \"int64\"]),",
            "+    fn_tree=\"tensorflow.shape_n\",",
            "+    dtype_and_x=helpers.dtype_and_values(",
            "+        available_dtypes=helpers.get_dtypes(\"valid\"), max_num_dims=5",
            "+    ),",
            "+    output_dtype=st.sampled_from([\"int32\", \"int64\"]),",
            ")",
            "def test_tensorflow_shape_n(",
            "*,"
        ]
    },
    {
        "number": 7038,
        "comments": "",
        "commit_message": "Sync for 4.0b4 release (#950)\n\n* sync for 4.0b4 release\n\n* fix extra space character in build.sh and add a simple prediction test for smoke testing on older macs\n",
        "label": "",
        "answer": "no",
        "change": [
            "def convert(",
            "if convert_to == 'mil':",
            "return proto_spec # Returns the MIL program",
            "",
            "-    model = coremltools.models.MLModel(proto_spec, useCPUOnly=True)",
            "+    useCPUOnly = kwargs.get(\"useCPUOnly\", True)",
            "+    model = coremltools.models.MLModel(proto_spec, useCPUOnly=useCPUOnly)",
            "",
            "if minimum_deployment_target is not None:",
            "check_deployment_compatibility("
        ]
    },
    {
        "number": 7045,
        "comments": "",
        "commit_message": "limit auto scaling batch size to the size of the training dataset (#3271)\n\n* fix\n\n* fix and test\n\n* fix merge error\n\n* test for max dataset size\n\n* changelog\n\n* update docs\n\n* fix merge\n\n* unused imports\n\n* imports\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def test_auto_scale_batch_size_set_model_attribute(tmpdir, use_hparams):",
            "",
            "trainer = Trainer(default_root_dir=tmpdir, max_epochs=1, auto_scale_batch_size=True)",
            "trainer.tune(model, datamodule_fit)",
            "-    assert trainer.datamodule == datamodule_fit",
            "after_batch_size = model.hparams.batch_size if use_hparams else model.batch_size",
            "+    assert trainer.datamodule == datamodule_fit",
            "assert before_batch_size != after_batch_size",
            "+    assert after_batch_size <= len(trainer.train_dataloader.dataset)",
            "assert datamodule_fit.batch_size == after_batch_size",
            "# should be left unchanged, since it was not passed to .tune()",
            "assert datamodule_model.batch_size == 111"
        ]
    },
    {
        "number": 7048,
        "comments": "",
        "commit_message": "RANSAC improvements (#1435)\n\n- Added LU solver for RANSAC-homography, it speeds-up the minimal-solver part x10\n- Now find_fundamental weights is optional, as it was for homography\n- Fix test precision and made one xFail test to be compulsory\n- Fixed tests for fundamental matrix, as one have to have >= 8 points (also added to docstring)\n- updates deprecated torch.svd to torch.linalg.svd\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestSolveWithMask:",
            ")",
            "def test_all_bad(self, device, dtype):",
            "A = torch.ones(10, 3, 3, device=device, dtype=dtype)",
            "-        B = torch.ones(3, 10, device=device, dtype=dtype)",
            "+        B = torch.ones(10, 3, device=device, dtype=dtype)",
            "",
            "X, _, mask = safe_solve_with_mask(B, A)",
            "assert torch.equal(mask, torch.zeros_like(mask))"
        ]
    },
    {
        "number": 7053,
        "comments": "",
        "commit_message": "Clean up host model API (#298)\n\n* Add new columns to TorchModel\n\n* Add allow_model_copy and allow_run_inference args\n\n* Snapshot shouldn't include model objects\n\n* Fix tests\n\n* Create get_serialized_model_with_id\n\n* Create get_model route\n\n* Create get_model_copy\n\n* Create global variables for storing error messages\n\n* Rename allow_model_copy -> allow_get_model_copy\n\n* Fix typo\n\n* Implement serve_encrypted_model\n\n* Improve comments\n\n* Add tests for new arguments\n\n* Fix test\n\n* Standardize results\n\n* Update tests\n\n* Fix small bug\n\n* Improve delete error messages\n\n* Fix typo\n\n* Fix test\n\n* Extend syft with a friendly api\n\n* Better argument names\n\n* Try to fix tests\n\n* Try to fix tests\n\n* Fix tests\n\n* Added TODO\n\n* Update notebooks\n\n* download -> download_model\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def snapshot(worker):",
            "objects.append(WorkerObject(worker_id=worker.id, object=obj, id=key))",
            "",
            "db.session.add_all(objects)",
            "-    db.session.add_all(models)",
            "db.session.commit()",
            "last_snapshot_keys = current_keys"
        ]
    },
    {
        "number": 7059,
        "comments": "",
        "commit_message": "mlp layernorm fix (#676)\n\nSummary:\nPull Request resolved: https://github.com/facebookresearch/pytext/pull/676\n\nfix oss test\n\nReviewed By: hikushalhere, seayoung1112\n\nDifferential Revision: D15725808\n\nfbshipit-source-id: e3916e59df55e4f54123b451521c439bf3d07aa5\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class MLPDecoder(DecoderBase):",
            "layers.append(nn.Linear(in_dim, dim))",
            "layers.append(nn.ReLU())",
            "if config.layer_norm:",
            "-                layers.append(LayerNorm(dim))",
            "+                layers.append(nn.LayerNorm(dim))",
            "in_dim = dim",
            "if config.out_dim:",
            "out_dim = config.out_dim"
        ]
    },
    {
        "number": 7060,
        "comments": "",
        "commit_message": "merge code for kaggle house (#1186)\n\n* merge code for kaggle house\n\n* fix\n",
        "label": "",
        "answer": "no",
        "change": [
            "arange = tf.range",
            "astype = tf.cast",
            "int32 = tf.int32",
            "float32 = tf.float32",
            "+concat = tf.concat",
            "numpy = lambda x, *args, **kwargs: x.numpy(*args, **kwargs)"
        ]
    },
    {
        "number": 7061,
        "comments": "",
        "commit_message": "fix\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class FrameScoreFeats(AbsFeatsExtract):",
            "pad = self.win_length // 2",
            "input_lengths = input_lengths + 2 * pad",
            "",
            "-            olens = torch.div((input_lengths - self.win_length), self.hop_length) + 1",
            "+            olens = (",
            "+                torch.div(",
            "+                    (input_lengths - self.win_length),",
            "+                    self.hop_length,",
            "+                    rounding_mode=\"floor\",",
            "+                )",
            "+                + 1",
            "+            )",
            "output.masked_fill_(make_pad_mask(olens, output, 1), 0.0)",
            "else:",
            "olens = None"
        ]
    },
    {
        "number": 7063,
        "comments": "",
        "commit_message": "Fix torchscript fp16 on int inputs\n\nuse .half() only when input is float\n\nSigned-off-by: Jonathan Chang <31893406+cccntu@users.noreply.github.com>\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class PytorchBackendCompiler(Compiler):",
            "input_sample = input_data.get_list(1)[0]",
            "if self.device is Device.GPU:",
            "if quantization_type is QuantizationType.HALF:",
            "-                input_sample = [t.cuda().half() for t in input_sample]",
            "+                input_sample = [",
            "+                    t.cuda().half() if torch.is_floating_point(t) else t.cuda()",
            "+                    for t in input_sample",
            "+                ]",
            "else:",
            "input_sample = [t.cuda() for t in input_sample]"
        ]
    },
    {
        "number": 7067,
        "comments": "",
        "commit_message": "add vq-wav2vec (#1029)\n\nSummary:\nsanitized vq-wav2vec implementation. i will also add docs to this. i have a fixed-up checkpoint that this code can load and verified that it produces same results as what we used in paper\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1029\n\nDifferential Revision: D20129246\n\nPulled By: alexeib\n\nfbshipit-source-id: f72f455e0c309168e644ab86ec18c768c308da98\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "the corresponding GitHub repo: https://github.com/hendrycks/GELUs",
            "import math",
            "",
            "import torch",
            "+import torch.nn as nn",
            "",
            "",
            "def gelu_accurate(x):",
            "if not hasattr(gelu_accurate, \"_a\"):",
            "gelu_accurate._a = math.sqrt(2 / math.pi)",
            "-    return 0.5 * x * (1 + torch.tanh(gelu_accurate._a * (x + 0.044715 * torch.pow(x, 3))))",
            "+    return (",
            "+        0.5 * x * (1 + torch.tanh(gelu_accurate._a * (x + 0.044715 * torch.pow(x, 3))))",
            "+    )",
            "",
            "",
            "def gelu(x: torch.Tensor) -> torch.Tensor:",
            "-    if hasattr(torch.nn.functional, 'gelu'):",
            "+    if hasattr(torch.nn.functional, \"gelu\"):",
            "return torch.nn.functional.gelu(x.float()).type_as(x)",
            "else:",
            "return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))"
        ]
    },
    {
        "number": 7068,
        "comments": "",
        "commit_message": "fixed and improved shape handling for exploration\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class GaussianNoise(Exploration):",
            "",
            "super(GaussianNoise, self).__init__(scope=scope, summary_labels=summary_labels)",
            "",
            "-    def tf_explore(self, episode, timestep, action_spec):",
            "-        return tf.random_normal(shape=action_spec['shape'], mean=self.mu, stddev=self.sigma)",
            "+    def tf_explore(self, episode, timestep, shape):",
            "+        return tf.random_normal(shape=shape, mean=self.mu, stddev=self.sigma)"
        ]
    },
    {
        "number": 7070,
        "comments": "",
        "commit_message": "Improve reproduceability 2/3 (#1906)\n\n* [Repro] Correct reproducability\n\n* up\n\n* up\n\n* uP\n\n* up\n\n* need better image\n\n* allow conversion from no state dict checkpoints\n\n* up\n\n* up\n\n* up\n\n* up\n\n* check tensors\n\n* check tensors\n\n* check tensors\n\n* check tensors\n\n* next try\n\n* up\n\n* up\n\n* better name\n\n* up\n\n* up\n\n* Apply suggestions from code review\n\n* correct more\n\n* up\n\n* replace all torch randn\n\n* fix\n\n* correct\n\n* correct\n\n* finish\n\n* fix more\n\n* up\n",
        "label": "",
        "answer": "no",
        "change": [
            "class PaintByExampleImageEncoder(CLIPPreTrainedModel):",
            "self.proj_out = nn.Linear(config.hidden_size, self.proj_size)",
            "",
            "# uncondition for scaling",
            "-        self.uncond_vector = nn.Parameter(torch.rand((1, 1, self.proj_size)))",
            "+        self.uncond_vector = nn.Parameter(torch.randn((1, 1, self.proj_size)))",
            "",
            "def forward(self, pixel_values):",
            "clip_output = self.model(pixel_values=pixel_values)"
        ]
    },
    {
        "number": 7071,
        "comments": "",
        "commit_message": "Speech2TextTransformer (#10175)\n\n* s2t\n\n* fix config\n\n* conversion script\n\n* fix import\n\n* add tokenizer\n\n* fix tok init\n\n* fix tokenizer\n\n* first version working\n\n* fix embeds\n\n* fix lm head\n\n* remove extra heads\n\n* fix convert script\n\n* handle encoder attn mask\n\n* style\n\n* better enc attn mask\n\n* override _prepare_attention_mask_for_generation\n\n* handle attn_maks in encoder and decoder\n\n* input_ids => input_features\n\n* enable use_cache\n\n* remove old code\n\n* expand embeddings if needed\n\n* remove logits bias\n\n* masked_lm_loss => loss\n\n* hack tokenizer to support feature processing\n\n* fix model_input_names\n\n* style\n\n* fix error message\n\n* doc\n\n* remove inputs_embeds\n\n* remove input_embeds\n\n* remove unnecessary docstring\n\n* quality\n\n* SpeechToText => Speech2Text\n\n* style\n\n* remove shared_embeds\n\n* subsample => conv\n\n* remove Speech2TextTransformerDecoderWrapper\n\n* update output_lengths formula\n\n* fix table\n\n* remove max_position_embeddings\n\n* update conversion scripts\n\n* add possibility to do upper case for now\n\n* add FeatureExtractor and Processor\n\n* add tests for extractor\n\n* require_torch_audio => require_torchaudio\n\n* add processor test\n\n* update import\n\n* remove classification head\n\n* attention mask is now 1D\n\n* update docstrings\n\n* attention mask should be of type long\n\n* handle attention mask from generate\n\n* alwyas return attention_mask\n\n* fix test\n\n* style\n\n* doc\n\n* Speech2TextTransformer => Speech2Text\n\n* Speech2TextTransformerConfig => Speech2TextConfig\n\n* remove dummy_inputs\n\n* nit\n\n* style\n\n* multilinguial tok\n\n* fix tokenizer\n\n* add tgt_lang setter\n\n* save lang_codes\n\n* fix tokenizer\n\n* add forced_bos_token_id to tokenizer\n\n* apply review suggestions\n\n* add torchaudio to extra deps\n\n* add speech deps to CI\n\n* fix dep\n\n* add libsndfile to ci\n\n* libsndfile1\n\n* add speech to extras all\n\n* libsndfile1 -> libsndfile1\n\n* libsndfile\n\n* libsndfile1-dev\n\n* apt update\n\n* add sudo to install\n\n* update deps table\n\n* install libsndfile1-dev on CI\n\n* tuple to list\n\n* init conv layer\n\n* add model tests\n\n* quality\n\n* add integration tests\n\n* skip_special_tokens\n\n* add speech_to_text_transformer in toctree\n\n* fix tokenizer\n\n* fix fp16 tests\n\n* add tokenizer tests\n\n* fix copyright\n\n* input_values => input_features\n\n* doc\n\n* add model in readme\n\n* doc\n\n* change checkpoint names\n\n* fix copyright\n\n* fix code example\n\n* add max_model_input_sizes in tokenizer\n\n* fix integration tests\n\n* add do_lower_case to tokenizer\n\n* remove clamp trick\n\n* fix \"Add modeling imports here\"\n\n* fix copyrights\n\n* fix tests\n\n* SpeechToTextTransformer => SpeechToText\n\n* fix naming\n\n* fix table formatting\n\n* fix typo\n\n* style\n\n* fix typos\n\n* remove speech dep from extras[testing]\n\n* fix copies\n\n* rename doc file,\n\n* put imports under is_torch_available\n\n* run feat extract tests when torch is available\n\n* dummy objects for processor and extractor\n\n* fix imports in tests\n\n* fix import in modeling test\n\n* fxi imports\n\n* fix torch import\n\n* fix imports again\n\n* fix positional embeddings\n\n* fix typo in import\n\n* adapt new extractor refactor\n\n* style\n\n* fix torchscript test\n\n* doc\n\n* doc\n\n* Apply suggestions from code review\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* fix docs, copied from, style\n\n* fix docstring\n\n* handle imports\n\n* remove speech from all extra deps\n\n* remove s2t from seq2seq lm mapping\n\n* better names\n\n* skip training tests\n\n* add install instructions\n\n* List => Tuple\n\n* doc\n\n* fix conversion script\n\n* fix urls\n\n* add instruction for libsndfile\n\n* fix fp16 test\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class GenerationMixin:",
            ") -> torch.LongTensor:",
            "decoder_start_token_id = self._get_decoder_start_token_id(decoder_start_token_id, bos_token_id)",
            "decoder_input_ids = (",
            "-            torch.ones((input_ids.shape[0], 1), dtype=input_ids.dtype, device=input_ids.device)",
            "-            * decoder_start_token_id",
            "+            torch.ones((input_ids.shape[0], 1), dtype=torch.long, device=input_ids.device) * decoder_start_token_id",
            ")",
            "return decoder_input_ids"
        ]
    },
    {
        "number": 7077,
        "comments": "",
        "commit_message": "[WIP] - Documentation Cleaning and Coding Style (#516)\n\n* Activation Cleaning Docstring Test\n\n* Requirements Pinned with range to insure tested versions are used. Range are used to prevent updating requirements all the time.\n\n* setup.cfg file added with PEP8 configuration\n\n* activation.py refactored\n\n* docstring fixed - ready for documentation unittest\n\n* Yapf correction for max_line_length: 120\n\n* test yapf refactored\n\n* Requirements conflict solved\n\n* Yapf Style modified and merged in file \"setup.cfg\"\n\n* Yapf Confiuguration Updated\n\n* Code Refactored with new YAPF formating style\n\n* Code Refactored with new YAPF formating style\n\n* Code Refactored with new YAPF formating style\n\n* tl.layers.pooling YAPF reformat\n\n* yapf updated\n\n* gitignore updated\n\n* YAPF Style Fixing Attempt\n\n* Space Error Fix\n\n* Style Correction\n\n* Assertion Codacy Errors Corrected\n\n* Error Fix\n\n* Assertion Refactored\n\n* YAPF Style Applied to Master\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class PReluLayer(Layer):",
            "",
            "# with tf.name_scope(name) as scope:",
            "with tf.variable_scope(name):",
            "-            alphas = tf.get_variable(name='alphas', shape=w_shape, initializer=a_init, dtype=LayersConfig.tf_dtype, **a_init_args)",
            "+            alphas = tf.get_variable(",
            "+                name='alphas', shape=w_shape, initializer=a_init, dtype=LayersConfig.tf_dtype, **a_init_args",
            "+            )",
            "try:  # TF 1.0",
            "self.outputs = tf.nn.relu(self.inputs) + tf.multiply(alphas, (self.inputs - tf.abs(self.inputs))) * 0.5",
            "except Exception:  # TF 0.12"
        ]
    },
    {
        "number": 7079,
        "comments": "",
        "commit_message": "[torch] Enable GPU training (#1508)\n\n* Added more descriptive description_summary, fixed bug in TabNet combiner\n\n* Updated modules to do proper GPU conversion\n\n* Functional GPU training\n\n* Tabnet working, removed RayRemoteTrainer\n\n* Temporary changes to unblock GPU training\n\n* Functional test_seq_encoder\n\n* Cleaned up PR\n\n* Fixed failing CPU tests\n\n* Responded to comments\n\n* Fix failing GPU tests in test_api.py\n\n* [torch] Fix GPU tests for test_visualizations (#1539)\n\nUpdated torch.aranges to registered buffers in embedding_modules.py\n\n* [torch] Fix test_encoder.py for GPU (#1544)\n\n* Accept and move model to GPU as part of Predictor init (#1559)\n\n* [torch] Fixed tests in test_experiment.py (#1562)\n\n* Fixed tests in tests/integration_tests/test_experiment.py\n\n* Moved set embed and inputs to correct device in tests/ludwig/encoders/test_set_encoders.py\n\n* Moved set date and inputs to correct device in tests/ludwig/encoders/test_date_encoders.py\n\n* Moved bag embed and inputs to correct device in tests/ludwig/encoders/test_bag_encoders.py\n\n* Moved sequence embed and inputs to correct device in tests/ludwig/features/test_sequence_features.py\n\n* [torch] Fixed GPU issues in combiners.py (#1568)\n\n* Fix GPU tests in test_collect.py (#1569)\n\n* Added model to Predictor initialization\n\n* Fixed embedding modules tests (#1554)\n\n* Fixed embedding modules tests\n* Moved embedding modules to device in tests, removed duplicate argument\n\nCo-authored-by: Shreya Rajpal <shreya.rajpal@gmail.com>\n\n* Fixed encoders tests except test_h3_rnn_embed (#1556)\n\n* Fixed encoders tests except test_h3_rnn_embed\n\n* Addressed Shreya's comments\n\n* Moved embedding modules to device, used registered buffers\n\nCo-authored-by: Shreya Rajpal <shreya.rajpal@gmail.com>\n\n* Pass ECD object to Predictor\n\n* Removed model arg from predict function\n\nCo-authored-by: Jeffrey Tang <jeff@predibase.com>\nCo-authored-by: Jeffrey Tang <810895+jeffreyftang@users.noreply.github.com>\nCo-authored-by: Animesh Kumar <anmshkmr@users.noreply.github.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def test_api_training_determinism(csv_filename):",
            "",
            "divergence = False",
            "for weight_1, weight_2 in zip(model_weights_1, model_weights_2):",
            "-            if not np.allclose(weight_1, weight_2):",
            "+            if not torch.allclose(weight_1, weight_2):",
            "divergence = True",
            "break",
            "assert divergence, 'model_1 and model_2 have identical weights with different seeds!'",
            "",
            "for weight_1, weight_3 in zip(model_weights_1, model_weights_3):",
            "-            assert np.allclose(weight_1, weight_3)",
            "+            assert torch.allclose(weight_1, weight_3)",
            "",
            "",
            "def run_api_commands("
        ]
    },
    {
        "number": 7080,
        "comments": "",
        "commit_message": "refactor(framework): pytorch splitting refactor (#2317)\n\n* refactor(framework): split pytorch impl to pytorch and torchscript\n\n* style: reformat\n\n* style: minor fix\n\n* doc: fix pytorch/torchscript changes\n\n* chore: add torchscript github workflow\n\n* style: minor fix\n\n* Update bentoml/_internal/frameworks/common/pytorch.py\n\nCo-authored-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* Update bentoml/_internal/frameworks/common/pytorch.py\n\nCo-authored-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* Update bentoml/_internal/frameworks/common/pytorch.py\n\nCo-authored-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* Update bentoml/_internal/frameworks/common/pytorch.py\n\nCo-authored-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* Update bentoml/_internal/frameworks/pytorch.py\n\nCo-authored-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* Update bentoml/_internal/frameworks/common/pytorch.py\n\nCo-authored-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* Update bentoml/_internal/frameworks/common/pytorch.py\n\nCo-authored-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* Update bentoml/_internal/frameworks/common/pytorch.py\n\nCo-authored-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* style: minor fix\n\n* test: pytorch frameworks test save format\n\n* chore: update codecov.yml for torchscript\n\n* chore: github workflow add torchscript\n\n* test(framework): disable pytorch container batch_axis=1 test\n\n* chore: remove double import\n\n* fix: pytorch tensor to device\n\nCo-authored-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def predict_df(model: pl.LightningModule, df: pd.DataFrame):",
            "def test_pl_save_load():",
            "model: \"pl.LightningModule\" = AdditionModel()",
            "tag = bentoml.pytorch_lightning.save(\"pytorch_lightning_test\", model)",
            "-    info = bentoml.models.get(tag)",
            "-    assert_have_file_extension(info.path, \".pt\")",
            "+    bentoml_model = bentoml.models.get(tag)",
            "+    assert_have_file_extension(bentoml_model.path, \".pt\")",
            "+    assert bentoml_model.info.context.get(\"model_format\") == \"pytorch_lightning:v1\"",
            "",
            "pl_loaded: \"pl.LightningModule\" = bentoml.pytorch_lightning.load(tag)"
        ]
    },
    {
        "number": 7082,
        "comments": "",
        "commit_message": "[Enhance]: Avoid crash in empty gt training of GFL (#4631)\n\n* avoid crash in empty gt training of GFL\n\n* real fix of the bug\n",
        "label": "",
        "answer": "no",
        "change": [
            "class GFLHead(AnchorHead):",
            "else:",
            "loss_bbox = bbox_pred.sum() * 0",
            "loss_dfl = bbox_pred.sum() * 0",
            "-            weight_targets = torch.tensor(0).cuda()",
            "+            weight_targets = bbox_pred.new_tensor(0)",
            "",
            "# cls (qfl) loss",
            "loss_cls = self.loss_cls("
        ]
    },
    {
        "number": 7088,
        "comments": "",
        "commit_message": "Scope TokenIndexer output by indexer name (#3597)\n\n* Indexer tests are now passing, at least\n\n* Fixed some masking issues, and padding keys for one config file\n\n* TextFieldEmbedder tests pass\n\n* Fixing fixtures, some more tests passing\n\n* Fixed weird ordering bug\n\n* All TokenEmbedder tests passing?\n\n* Update fixtures\n\n* Fix more hard-coded references\n\n* fix field tests\n\n* fix dataset reader tests\n\n* Fix iterator tests\n\n* More tests passing\n\n* fix hotflip and some other tests\n\n* more tests\n\n* more test fixes\n\n* more tests\n\n* most tests passing; I think the remaining ones are spacy model changes\n\n* hard-code POS tag test\n\n* last test, I think\n\n* black\n\n* updated black\n\n* flake8\n\n* mypy\n\n* black again\n\n* fix training configs\n\n* remove reference to embedder_to_indexer_map\n\n* Other fixes from PR comments\n\n* fix breakage from incorrect merge during rebase\n\n* flake, some docstring formatting\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class BiaffineDependencyParser(Model):",
            "\"\"\"",
            "# Parameters",
            "",
            "-        words : Dict[str, torch.LongTensor], required",
            "+        words : TextFieldTensors, required",
            "The output of ``TextField.as_array()``, which should typically be passed directly to a",
            "``TextFieldEmbedder``. This output is a dictionary mapping keys to ``TokenIndexer``",
            "tensors.  At its most basic, using a ``SingleIdTokenIndexer`` this is : ``{\"tokens\":"
        ]
    },
    {
        "number": 7090,
        "comments": "",
        "commit_message": "Add yesqa in the pre-commit (#1246)\n\n* Add yesqa in the pre-commit\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Apply suggestions from code review\n\nCo-authored-by: Christian Clauss <cclauss@me.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Christian Clauss <cclauss@me.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class SIFTDescriptor(nn.Module):",
            "",
            "ang_bins = []",
            "for i in range(0, self.num_ang_bins):",
            "-            out = self.pk((bo0_big == i).to(input.dtype) * wo0_big + (bo1_big == i).to(input.dtype) * wo1_big)  # noqa",
            "+            out = self.pk((bo0_big == i).to(input.dtype) * wo0_big + (bo1_big == i).to(input.dtype) * wo1_big)",
            "ang_bins.append(out)",
            "ang_bins = torch.cat(ang_bins, dim=1)",
            "ang_bins = ang_bins.view(B, -1)"
        ]
    },
    {
        "number": 7092,
        "comments": "",
        "commit_message": "Fix end-of-epoch with record_stream\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class data_prefetcher():",
            "torch.cuda.current_stream().wait_stream(self.stream)",
            "input = self.next_input",
            "target = self.next_target",
            "-        input.record_stream(torch.cuda.current_stream())",
            "-        target.record_stream(torch.cuda.current_stream())",
            "+        if input is not None:",
            "+            input.record_stream(torch.cuda.current_stream())",
            "+        if target is not None:",
            "+            target.record_stream(torch.cuda.current_stream())",
            "self.preload()",
            "return input, target"
        ]
    },
    {
        "number": 7093,
        "comments": "",
        "commit_message": "fix lint tests for test files\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Tester(unittest.TestCase):",
            "self.assertAlmostEqual(error.item(), 0.0, places=4)",
            "",
            "# functional",
            "-        self.assertTrue(torch.allclose(points_dst,",
            "-            tgm.TransformPoints()(dst_homo_src, points_src)))",
            "+        self.assertTrue(",
            "+            torch.allclose(",
            "+                points_dst,",
            "+                tgm.TransformPoints()(",
            "+                    dst_homo_src,",
            "+                    points_src)))",
            "",
            "def test_transform_points_gradcheck(self):",
            "# generate input data"
        ]
    },
    {
        "number": 7094,
        "comments": "",
        "commit_message": "bugfixes\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Net(nn.Module):",
            "x = F.elu(self.conv3(adjs[1], x))",
            "x = F.elu(self.conv4(adjs[1], x))",
            "x = self.pool(x)",
            "-        x = x.contiguous().view(-1, num_first_fc * 64)",
            "+        x = x.contiguous().view(-1, num_first_fc * 16)",
            "x = F.elu(self.fc1(x))",
            "x = F.elu(self.fc2(x))",
            "x = F.dropout(x, training=self.training)"
        ]
    },
    {
        "number": 7096,
        "comments": "",
        "commit_message": "FIX E523,E541,E741\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class WarpCTC(chainer.Chain):",
            "# get ctc loss",
            "from chainer_ctc.warpctc import ctc as warp_ctc",
            "",
            "-        self.loss = warp_ctc(y_hat, ilens, [cuda.to_cpu(l.data) for l in ys])[0]",
            "+        self.loss = warp_ctc(y_hat, ilens, [cuda.to_cpu(y.data) for y in ys])[0]",
            "logging.info(\"ctc loss:\" + str(self.loss.data))",
            "",
            "return self.loss"
        ]
    },
    {
        "number": 7102,
        "comments": "",
        "commit_message": "fix use of deprecated TF functions.\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Model(ModelDesc):",
            "",
            "log_pi_a_given_s = tf.reduce_sum(",
            "log_probs * tf.one_hot(action, NUM_ACTIONS), 1)",
            "-        advantage = tf.sub(tf.stop_gradient(self.value), futurereward, name='advantage')",
            "+        advantage = tf.subtract(tf.stop_gradient(self.value), futurereward, name='advantage')",
            "policy_loss = tf.reduce_sum(log_pi_a_given_s * advantage, name='policy_loss')",
            "xentropy_loss = tf.reduce_sum(",
            "self.logits * log_probs, name='xentropy_loss')"
        ]
    },
    {
        "number": 7106,
        "comments": "",
        "commit_message": "generator seed fix for DDP mAP drop (#9545)\n\n* Try to fix DDP mAP drop by setting generator's seed to RANK\n\n* Fix default activation bug\n\n* Update dataloaders.py\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Update dataloaders.py\n\nCo-authored-by: Glenn Jocher <glenn.jocher@ultralytics.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def parse_model(d, ch):  # model_dict, input_channels(3)",
            "LOGGER.info(f\"\\n{'':>3}{'from':>18}{'n':>3}{'params':>10}  {'module':<40}{'arguments':<30}\")",
            "anchors, nc, gd, gw, act = d['anchors'], d['nc'], d['depth_multiple'], d['width_multiple'], d.get('activation')",
            "if act:",
            "-        Conv.act = eval(act)  # redefine default activation, i.e. Conv.act = nn.SiLU()",
            "+        Conv.default_act = eval(act)  # redefine default activation, i.e. Conv.default_act = nn.SiLU()",
            "LOGGER.info(f\"{colorstr('activation:')} {act}\")  # print",
            "na = (len(anchors[0]) // 2) if isinstance(anchors, list) else anchors  # number of anchors",
            "no = na * (nc + 5)  # number of outputs = anchors * (classes + 5)"
        ]
    },
    {
        "number": 7107,
        "comments": "",
        "commit_message": "add testing PT 1.12 (#13386)\n\n* add testing PT 1.12\n* Fix quantization tests\n* Fix another set of tests\n* Fix check since https://github.com/pytorch/pytorch/pull/80139 is only going to be available for 1.13\n* Skip this test for now for 1.12\n\nCo-authored-by: SeanNaren <sean@grid.ai>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_unresolvable_import_paths():",
            "with mock.patch(\"sys.argv\", [\"any.py\", \"--print_config\"]), redirect_stdout(out), pytest.raises(SystemExit):",
            "LightningCLI(TestModel, run=False)",
            "",
            "-    assert \"a_func: torch.softmax\" in out.getvalue()",
            "+    assert \"a_func: torch.nn.Softmax\" in out.getvalue()",
            "",
            "",
            "def test_pytorch_profiler_init_args():"
        ]
    },
    {
        "number": 7108,
        "comments": "",
        "commit_message": "bug fix for retinanet with 2 classes (fg/bg)\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def anchor_target_single(flat_anchors,",
            "",
            "",
            "def expand_binary_labels(labels, label_weights, label_channels):",
            "-    bin_labels = labels.new_full(",
            "-        (labels.size(0), label_channels), 0, dtype=torch.float32)",
            "+    bin_labels = labels.new_full((labels.size(0), label_channels), 0)",
            "inds = torch.nonzero(labels >= 1).squeeze()",
            "if inds.numel() > 0:",
            "bin_labels[inds, labels[inds] - 1] = 1"
        ]
    },
    {
        "number": 7109,
        "comments": "",
        "commit_message": "fixed lstm, added gru, other fixes\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TFOptimizer(Optimizer):",
            "arguments: Dict of arguments for passing to fn_loss as **kwargs.",
            "fn_loss: A callable taking arguments as kwargs and returning the loss op.",
            "\"\"\"",
            "-        loss = fn_loss(**arguments)",
            "+        # Trivial operation to enforce control dependency",
            "+        previous_variables = [util.identity_operation(x=variable) for variable in variables]",
            "",
            "# Force loss value to be calculated.",
            "-        with tf.control_dependencies(control_inputs=(loss,)):",
            "-            # Trivial operation to enforce control dependency",
            "-            previous_variables = [util.identity_operation(x=variable) for variable in variables]",
            "+        with tf.control_dependencies(control_inputs=previous_variables):",
            "+            loss = fn_loss(**arguments)",
            "",
            "# The actual tensorflow minimize op.",
            "-        with tf.control_dependencies(control_inputs=previous_variables):",
            "+        with tf.control_dependencies(control_inputs=(loss,)):",
            "# colocate_gradients_with_ops=True",
            "applied = self.optimizer.minimize(loss=loss, var_list=variables)"
        ]
    },
    {
        "number": 7120,
        "comments": "",
        "commit_message": "fix empty tensor and create test\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Cropping1D(Layer):",
            "return tf.TensorShape([input_shape[0], length, input_shape[2]])",
            "",
            "def call(self, inputs):",
            "-    if sum(self.cropping) >= inputs.shape[1]:",
            "+    if tf.not_equal(tf.size(inputs), 0) and sum(self.cropping) >= inputs.shape[1]:",
            "raise ValueError(",
            "'cropping parameter of Cropping layer is too high,' +",
            "'the result of crop' + str(inputs.shape) + ' with cropping ' +"
        ]
    },
    {
        "number": 7121,
        "comments": "",
        "commit_message": "fix flake8 style in tensorpack/\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class SessionUpdate(object):",
            "self.sess = sess",
            "self.assign_ops = defaultdict(list)",
            "for v in vars_to_update:",
            "-            #p = tf.placeholder(v.dtype, shape=v.get_shape())",
            "+            # p = tf.placeholder(v.dtype, shape=v.get_shape())",
            "with tf.device('/cpu:0'):",
            "p = tf.placeholder(v.dtype)",
            "savename = get_savename_from_varname(v.name)"
        ]
    },
    {
        "number": 7128,
        "comments": "",
        "commit_message": "fix deprecation warnings; remove pydoop dependency; update README\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def main_fun(argv, ctx):",
            "# Print the summaries to screen.",
            "for name, value in names_to_values.iteritems():",
            "summary_name = 'eval/%s' % name",
            "-      op = tf.scalar_summary(summary_name, value, collections=[])",
            "+      op = tf.summary.scalar(summary_name, value, collections=[])",
            "op = tf.Print(op, [value], summary_name)",
            "tf.add_to_collection(tf.GraphKeys.SUMMARIES, op)"
        ]
    },
    {
        "number": 7129,
        "comments": "",
        "commit_message": "fix swig name. add some df\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def get_predict_func(config):",
            "",
            "# check output_var_names against output_vars",
            "if output_var_names is not None:",
            "-        output_vars = [tf.get_default_graph().get_tensor_by_name(n) for n in output_var_names]",
            "+        output_vars = [tf.get_default_graph().get_tensor_by_name(get_op_var_name(n)[1])",
            "+                       for n in output_var_names]",
            "else:",
            "output_vars = []"
        ]
    },
    {
        "number": 7130,
        "comments": "",
        "commit_message": "ci: Redo `format.sh --all` script & backfill lint fixes (#9956)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class EpsilonGreedy(Exploration):",
            "tf.random.categorical(random_valid_action_logits, 1), axis=1)",
            "",
            "chose_random = tf.random.uniform(",
            "-            tf.stack([batch_size]),",
            "-            minval=0, maxval=1, dtype=tf.float32) < epsilon",
            "+            tf.stack([batch_size]), minval=0, maxval=1,",
            "+            dtype=tf.float32) < epsilon",
            "",
            "action = tf.cond(",
            "pred=tf.constant(explore, dtype=tf.bool)"
        ]
    },
    {
        "number": 7132,
        "comments": "",
        "commit_message": "Catch nan loss in training loop (#2029)\n\n* catch nan loss\n\n* small fix\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Trainer(Registrable):",
            "self.optimizer.zero_grad()",
            "",
            "loss = self.batch_loss(batch, for_training=True)",
            "+            if torch.isnan(loss):",
            "+                raise ValueError(\"nan loss encountered\")",
            "+",
            "loss.backward()",
            "",
            "train_loss += loss.item()"
        ]
    },
    {
        "number": 7137,
        "comments": "",
        "commit_message": "[style] consistent nn. and nn.functional (#12124)\n\n* consistent nn. and nn.functional\n\n* fix glitch\n\n* fix glitch #2\n",
        "label": "",
        "answer": "no",
        "change": [
            "class BertGenerationEmbeddings(nn.Module):",
            "self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)",
            "# self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load",
            "# any TensorFlow checkpoint file",
            "-        self.LayerNorm = torch.nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)",
            "+        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)",
            "self.dropout = nn.Dropout(config.hidden_dropout_prob)",
            "",
            "# position_ids (1, len position emb) is contiguous in memory and exported when serialized"
        ]
    },
    {
        "number": 7142,
        "comments": "",
        "commit_message": "[RLlib] Tf2x preparation; part 2 (upgrading `try_import_tf()`). (#9136)\n\n* WIP.\n\n* Fixes.\n\n* LINT.\n\n* WIP.\n\n* WIP.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* WIP.\n\n* Fixes.\n\n* Test\n\n* Fix.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* LINT.\n",
        "label": "",
        "answer": "no",
        "change": [
            "class WorkerSet:",
            "def session_creator():",
            "logger.debug(\"Creating TF session {}\".format(",
            "config[\"tf_session_args\"]))",
            "-            return tf.Session(",
            "-                config=tf.ConfigProto(**config[\"tf_session_args\"]))",
            "+            return tf1.Session(",
            "+                config=tf1.ConfigProto(**config[\"tf_session_args\"]))",
            "",
            "if isinstance(config[\"input\"], FunctionType):",
            "input_creator = config[\"input\"]"
        ]
    },
    {
        "number": 7143,
        "comments": "",
        "commit_message": "Fix and enable some tfcoreml converter convent tests\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def constant_propagation(nnssa):",
            "if len(constant_nodes) > 0:",
            "with tf.Graph().as_default() as graph:",
            "tf.import_graph_def(new_graph, name=\"\")",
            "-                with tf.Session(graph=graph) as sess:",
            "+                with tf.compat.v1.Session(graph=graph) as sess:",
            "query_list = []",
            "for c in constant_nodes:",
            "for j in range(constant_node_num_outputs[c]):"
        ]
    },
    {
        "number": 7145,
        "comments": "",
        "commit_message": "Fix broken keras image logging with tf1 (#513)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def patch_tf_keras():",
            "from tensorflow.python.keras.engine import training_arrays",
            "from tensorflow.python.keras.engine import training_generator",
            "",
            "-    training_v2 = wandb.util.import_module('tensorflow.python.keras.engine.training_v2')",
            "+    training_v2 = wandb.util.get_module('tensorflow.python.keras.engine.training_v2')",
            "old_arrays = training_arrays.fit_loop",
            "old_generator = training_generator.fit_generator",
            "if training_v2:"
        ]
    },
    {
        "number": 7148,
        "comments": "",
        "commit_message": "fixed f1 score\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class SignedGCN(torch.nn.Module):",
            "with torch.no_grad():",
            "pos_p = self.discriminate(z, pos_edge_index)[:, :2].max(dim=1)[1]",
            "neg_p = self.discriminate(z, neg_edge_index)[:, :2].max(dim=1)[1]",
            "-        pred = torch.cat([pos_p, neg_p]).cpu()",
            "+        pred = 1 - torch.cat([pos_p, neg_p]).cpu()",
            "y = torch.cat(",
            "-            [pred.new_zeros((pos_p.size(0))),",
            "-             pred.new_ones(neg_p.size(0))])",
            "+            [pred.new_ones((pos_p.size(0))),",
            "+             pred.new_zeros(neg_p.size(0))])",
            "pred, y = pred.numpy(), y.numpy()",
            "",
            "auc = roc_auc_score(y, pred)"
        ]
    },
    {
        "number": 7154,
        "comments": "",
        "commit_message": "Fix CI build by reverting changes in corr2d() (#1042)\n\nSigned-off-by: terrytangyuan <terrytangyuan@gmail.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def corr2d(X, K):  #@save",
            "Y = tf.Variable(tf.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1)))",
            "for i in range(Y.shape[0]):",
            "for j in range(Y.shape[1]):",
            "-            Y[i, j].assign(tf.reduce_sum(X[i: i + h, j: j + w] * K))",
            "+            Y[i, j].assign(tf.cast(tf.reduce_sum(",
            "+                X[i: i + h, j: j + w] * K), dtype=tf.float32))",
            "return Y"
        ]
    },
    {
        "number": 7155,
        "comments": "",
        "commit_message": "Fix get layer variables issue (#245)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def get_layer_variables_by_name(name):",
            "A list of Variables.",
            "",
            "\"\"\"",
            "-    return tf.get_collection(tf.GraphKeys.LAYER_VARIABLES + '/' + name + '/')",
            "+    return tf.get_collection(tf.GraphKeys.LAYER_VARIABLES + '/' + name)",
            "",
            "",
            "def get_value(var, session=None):"
        ]
    },
    {
        "number": 7157,
        "comments": "",
        "commit_message": "Search space zoo example fix (#2801)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "if __name__ == \"__main__\":",
            "parser = ArgumentParser(\"enas\")",
            "parser.add_argument(\"--batch-size\", default=128, type=int)",
            "parser.add_argument(\"--log-frequency\", default=10, type=int)",
            "-    # parser.add_argument(\"--search-for\", choices=[\"macro\", \"micro\"], default=\"macro\")",
            "parser.add_argument(\"--epochs\", default=None, type=int, help=\"Number of epochs (default: macro 310, micro 150)\")",
            "parser.add_argument(\"--visualization\", default=False, action=\"store_true\")",
            "args = parser.parse_args()"
        ]
    },
    {
        "number": 7159,
        "comments": "",
        "commit_message": "Bump torch, torchvision, python versions (#2663)\n\n* Bump torch, torchvision, python versions\n\n* Fix .nonzero() usage\n\n* Add missing licenses\n\n* Fix more .nonzero() errors\n\n* Fix errors\n\n* Fix pyplot.hist invocation\n\n* Work around torch 1.6 incompatibility with pyplot.hist\n\n* Fix more pyplot.hist() errors\n",
        "label": "",
        "answer": "no",
        "change": [
            "def count_accuracy(X, true_counts, air, batch_size):",
            "inferred_counts_m = count_vec_to_mat(inferred_counts, 3)",
            "counts += torch.mm(true_counts_m.t(), inferred_counts_m)",
            "error_ind = 1 - (true_counts_batch == inferred_counts)",
            "-        error_ix = error_ind.nonzero().squeeze()",
            "+        error_ix = error_ind.nonzero(as_tuple=False).squeeze()",
            "error_latents.append(latents_to_tensor((z_where, z_pres)).index_select(0, error_ix))",
            "error_indicators.append(error_ind)",
            "",
            "acc = counts.diag().sum().float() / X.size(0)",
            "-    error_indices = torch.cat(error_indicators).nonzero().squeeze()",
            "+    error_indices = torch.cat(error_indicators).nonzero(as_tuple=False).squeeze()",
            "if X.is_cuda:",
            "error_indices = error_indices.cuda()",
            "return acc, counts, torch.cat(error_latents), error_indices"
        ]
    },
    {
        "number": 7160,
        "comments": "",
        "commit_message": "rnn->encoders, removes _th suffix, same filenames for chainer and pytorch, modify tests to not rely on _th\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def main():",
            "# recog",
            "logging.info('backend = ' + args.backend)",
            "if args.backend == \"chainer\":",
            "-        from espnet.asr.chain.asr_chainer import recog",
            "+        from espnet.asr.chain.asr import recog",
            "recog(args)",
            "elif args.backend == \"pytorch\":",
            "-        from espnet.asr.pytorch.asr_pytorch import recog",
            "+        from espnet.asr.pytorch.asr import recog",
            "recog(args)",
            "else:",
            "raise ValueError(\"Only chainer and pytorch are supported.\")"
        ]
    },
    {
        "number": 7167,
        "comments": "",
        "commit_message": "Fix unit test failures with torch==1.1.0 (#1840)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class EKFState(object):",
            "S = H.mm(P).mm(H.transpose(-1, -2)) + R  # innovation cov",
            "",
            "K_prefix = self._cov.mm(H.transpose(-1, -2))",
            "-        dx = K_prefix.mm(torch.gesv(dz.unsqueeze(1), S)[0]).squeeze(1)  # K*dz",
            "+        dx = K_prefix.mm(torch.solve(dz.unsqueeze(1), S)[0]).squeeze(1)  # K*dz",
            "x = self._dynamic_model.geodesic_difference(x, -dx)",
            "",
            "I = eye_like(x, self._dynamic_model.dimension)  # noqa: E741",
            "-        ImKH = I - K_prefix.mm(torch.gesv(H, S)[0])",
            "+        ImKH = I - K_prefix.mm(torch.solve(H, S)[0])",
            "# *Joseph form* of covariance update for numerical stability.",
            "P = ImKH.mm(self.cov).mm(ImKH.transpose(-1, -2)) \\",
            "-            + K_prefix.mm(torch.gesv((K_prefix.mm(torch.gesv(R, S)[0])).transpose(-1, -2),",
            "+            + K_prefix.mm(torch.solve((K_prefix.mm(torch.solve(R, S)[0])).transpose(-1, -2),",
            "S)[0])",
            "",
            "pred_mean = x"
        ]
    },
    {
        "number": 7168,
        "comments": "",
        "commit_message": "fix typo\n",
        "label": "",
        "answer": "no",
        "change": [
            "def split_cross_attention_forward(self, x, context=None, mask=None):",
            "q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h=h), (q_in, k_in, v_in))",
            "del q_in, k_in, v_in",
            "",
            "-    r1 = torch.zeros(q.shape[0], q.shape[1], v.shape[2], device=q.device. dtype=q.dtype)",
            "+    r1 = torch.zeros(q.shape[0], q.shape[1], v.shape[2], device=q.device, dtype=q.dtype)",
            "",
            "stats = torch.cuda.memory_stats(q.device)",
            "mem_active = stats['active_bytes.all.current']"
        ]
    },
    {
        "number": 7169,
        "comments": "",
        "commit_message": "fix flake8 style in tensorpack/\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class FeedfreeInferenceRunner(Callback):",
            "for inf in self.infs:",
            "inf.before_inference()",
            "",
            "-        sess = tf.get_default_session()",
            "sz = self._input_data.size()",
            "with get_tqdm(total=sz) as pbar:",
            "for _ in range(sz):"
        ]
    },
    {
        "number": 7171,
        "comments": "",
        "commit_message": "CDN urls (#4030)\n\n* [file_utils] use_cdn + documentation\n\n* Move to cdn. urls for weights\n\n* [urls] Hotfix for bert-base-japanese\n",
        "label": "",
        "answer": "no",
        "change": [
            "logger = logging.getLogger(__name__)",
            "# for the pretrained weights provided with the models",
            "####################################################",
            "TF_XXX_PRETRAINED_MODEL_ARCHIVE_MAP = {",
            "-    \"xxx-base-uncased\": \"https://s3.amazonaws.com/models.huggingface.co/bert/xxx-base-uncased-tf_model.h5\",",
            "-    \"xxx-large-uncased\": \"https://s3.amazonaws.com/models.huggingface.co/bert/xxx-large-uncased-tf_model.h5\",",
            "+    \"xxx-base-uncased\": \"https://cdn.huggingface.co/xxx-base-uncased-tf_model.h5\",",
            "+    \"xxx-large-uncased\": \"https://cdn.huggingface.co/xxx-large-uncased-tf_model.h5\",",
            "}"
        ]
    },
    {
        "number": 7172,
        "comments": "",
        "commit_message": "generator seed fix for DDP mAP drop (#9545)\n\n* Try to fix DDP mAP drop by setting generator's seed to RANK\n\n* Fix default activation bug\n\n* Update dataloaders.py\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Update dataloaders.py\n\nCo-authored-by: Glenn Jocher <glenn.jocher@ultralytics.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def autopad(k, p=None, d=1):  # kernel, padding, dilation",
            "",
            "class Conv(nn.Module):",
            "# Standard convolution with args(ch_in, ch_out, kernel, stride, padding, groups, dilation, activation)",
            "-    act = nn.SiLU()  # default activation",
            "+    default_act = nn.SiLU()  # default activation",
            "",
            "def __init__(self, c1, c2, k=1, s=1, p=None, g=1, d=1, act=True):",
            "super().__init__()",
            "self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p, d), groups=g, dilation=d, bias=False)",
            "self.bn = nn.BatchNorm2d(c2)",
            "-        self.act = self.act if act is True else act if isinstance(act, nn.Module) else nn.Identity()",
            "+        self.act = self.default_act if act is True else act if isinstance(act, nn.Module) else nn.Identity()",
            "",
            "def forward(self, x):",
            "return self.act(self.bn(self.conv(x)))"
        ]
    },
    {
        "number": 7176,
        "comments": "",
        "commit_message": "best possible recall fix\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def check_img_size(img_size, s=32):",
            "",
            "def check_best_possible_recall(dataset, anchors, thr):",
            "# Check best possible recall of dataset with current anchors",
            "-    wh = torch.tensor(np.concatenate([l[:, 3:5] * s for s, l in zip(dataset.shapes, dataset.labels)]))  # width-height",
            "-    ratio = wh[:, None] / anchors.view(-1, 2)[None]  # ratio",
            "+    wh = torch.tensor(np.concatenate([l[:, 3:5] * s for s, l in zip(dataset.shapes, dataset.labels)])).float()  # wh",
            "+    ratio = wh[:, None] / anchors.view(-1, 2).cpu()[None]  # ratio",
            "m = torch.max(ratio, 1. / ratio).max(2)[0]  # max ratio",
            "bpr = (m.min(1)[0] < thr).float().mean()  # best possible recall",
            "mr = (m < thr).float().mean()  # match ratio"
        ]
    },
    {
        "number": 7178,
        "comments": "",
        "commit_message": "Generalize TorchHook to FrameworkHook (#2561)\n\n* create FrameworkHook and move generic hooking functionality over\n\n* PEP-ify import statements all around\n\n* PR comment - fix docstring\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_spinup_time(hook):",
            "spun up inside web frameworks are created quickly enough to not cause timeout errors\"\"\"",
            "data = []",
            "for i in range(10000):",
            "-        data.append(th.Tensor(5, 5).random_(100))",
            "+        data.append(torch.Tensor(5, 5).random_(100))",
            "start_time = time()",
            "dummy = sy.VirtualWorker(hook, id=\"dummy\", data=data)",
            "end_time = time()"
        ]
    },
    {
        "number": 7179,
        "comments": "",
        "commit_message": "Fix circle ci flaky fail of wmt example  (#3485)\n\n* force bleu\n\n* fix wrong file name\n\n* rename file\n\n* different filenames for each example test\n\n* test files should clean up after themselves\n\n* test files should clean up after themselves\n\n* do not force bleu\n\n* correct typo\n\n* fix isort\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestBartExamples(unittest.TestCase):",
            "def test_bart_cnn_cli(self):",
            "stream_handler = logging.StreamHandler(sys.stdout)",
            "logger.addHandler(stream_handler)",
            "-        tmp = Path(tempfile.gettempdir()) / \"utest_generations.hypo\"",
            "+        tmp = Path(tempfile.gettempdir()) / \"utest_generations_bart_sum.hypo\"",
            "with tmp.open(\"w\") as f:",
            "f.write(\"\\n\".join(articles))",
            "-        testargs = [\"evaluate_cnn.py\", str(tmp), \"output.txt\"]",
            "+        testargs = [\"evaluate_cnn.py\", str(tmp), output_file_name]",
            "with patch.object(sys, \"argv\", testargs):",
            "_run_generate()",
            "-            self.assertTrue(Path(\"output.txt\").exists())",
            "+            self.assertTrue(Path(output_file_name).exists())",
            "+            os.remove(Path(output_file_name))"
        ]
    },
    {
        "number": 7183,
        "comments": "",
        "commit_message": "Fix unused variable found (#1250)\n\n* Fix Unused variable found\n\n* more fixes\n\n* Update kornia/enhance/histogram.py\n",
        "label": "",
        "answer": "no",
        "change": [
            "def find_homography_dlt_iterated(",
            "'''Function, which finds homography via iteratively-reweighted",
            "least squares ToDo: add citation'''",
            "H: torch.Tensor = find_homography_dlt(points1, points2, weights)",
            "-    for i in range(n_iter - 1):",
            "+    for _ in range(n_iter - 1):",
            "errors: torch.Tensor = symmetric_transfer_error(points1, points2, H, False)",
            "weights_new: torch.Tensor = torch.exp(-errors / (2.0 * (soft_inl_th ** 2)))",
            "H = find_homography_dlt(points1, points2, weights_new)"
        ]
    },
    {
        "number": 7184,
        "comments": "",
        "commit_message": "upgrade to pytorch 0.4.0 (#1126)\n\n* bump pytorch to 0.4 + fix sanitize\n\n* remove check for Variable in block_orthogonal\n\n* rename parameter split_size=\n\n* remove checks for Variable\n\n* fix more tests\n\n* fixes\n\n* get tests to pass\n\n* fix warnings\n\n* get rid of some of the Variables\n\n* more tests passing\n\n* more elimination of variables\n\n* finish removing all Variables\n\n* pylint and such\n\n* a few fixes\n\n* move torch.no_grad into model.forward_on_instances\n\n* more pytorch 0.4 changes\n\n* detach() -> data\n\n* pylint\n\n* fix bad tensor creation\n\n* fix types\n\n* remove print statement\n\n* more 0.4 goodness\n\n* factor out is_tensor\n\n* add no_grad to elmo command\n\n* cleanup\n\n* remove TODO\n\n* address PR feedback\n\n* replace all() with item()\n\n* more cleanup\n\n* further cleanup\n\n* remove Variable\n\n* really fix merge conflict\n\n* fix pylint\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestBilinearSimilarityFunction(AllenNlpTestCase):",
            "bilinear._bias = Parameter(torch.from_numpy(numpy.asarray([0])).float())",
            "a_vectors = numpy.random.rand(5, 4, 3, 6, 4)",
            "b_vectors = numpy.random.rand(5, 4, 3, 6, 7)",
            "-        a_variables = Variable(torch.from_numpy(a_vectors).float())",
            "-        b_variables = Variable(torch.from_numpy(b_vectors).float())",
            "+        a_variables = torch.from_numpy(a_vectors).float()",
            "+        b_variables = torch.from_numpy(b_vectors).float()",
            "result = bilinear(a_variables, b_variables).data.numpy()",
            "assert result.shape == (5, 4, 3, 6)",
            "expected_result = numpy.dot(numpy.dot(numpy.transpose(a_vectors[3, 2, 1, 3]), weights),"
        ]
    },
    {
        "number": 7185,
        "comments": "",
        "commit_message": "small fix for `get_item`.\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def get_item(",
            "x: torch.Tensor,",
            "query: torch.Tensor,",
            ") -> torch.Tensor:",
            "-    if ivy.dtype(query, as_native=True) is torch.bool:",
            "-        return x.__getitem__(query)",
            "-    return x.__getitem__(query.to(torch.int64))",
            "+    if ivy.is_array(query) and ivy.dtype(query, as_native=True) is not torch.bool:",
            "+        return x.__getitem__(query.to(torch.int64))",
            "+    return x.__getitem__(query)",
            "",
            "",
            "def to_numpy(x: torch.Tensor, /, *, copy: bool = True) -> np.ndarray:"
        ]
    },
    {
        "number": 7187,
        "comments": "",
        "commit_message": "Fix Example in BertBlock Comment\n",
        "label": "",
        "answer": "no",
        "change": [
            "class BertBlock(block_module.Block):",
            "```python",
            "# Using the Transformer Block with AutoModel.",
            "import autokeras as ak",
            "-        from autokeras import BERTBlock",
            "+        from autokeras.blocks import BertBlock",
            "from tensorflow.keras import losses",
            "",
            "input_node = ak.TextInput()",
            "-        output_node = BERTBlock(max_seq_len=128)(input_node)",
            "+        output_node = BertBlock(max_sequence_length=128)(input_node)",
            "output_node = ak.ClassificationHead()(output_node)",
            "clf = ak.AutoModel(inputs=input_node, outputs=output_node, max_trials=10)",
            "```"
        ]
    },
    {
        "number": 7191,
        "comments": "",
        "commit_message": "Fix Pulsar backend batched radius handling.\n\nSummary: This fixes a corner case for multi-radius handling for the pulsar backend. The additional dimensionality check ensures that the batched parsing for radiuses is only performed when appropriate.\n\nReviewed By: bottler\n\nDifferential Revision: D25387708\n\nfbshipit-source-id: c486dcf327f812265b7ca8ca5ef5c6a31e6d4549\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class PulsarPointsRenderer(nn.Module):",
            "raster_rad = self.rasterizer.raster_settings.radius",
            "if kwargs.get(\"radius_world\", False):",
            "return raster_rad",
            "-        if isinstance(raster_rad, torch.Tensor) and raster_rad.numel() > 1:",
            "+        if (",
            "+            isinstance(raster_rad, torch.Tensor)",
            "+            and raster_rad.numel() > 1",
            "+            and raster_rad.ndim > 1",
            "+        ):",
            "# In this case it must be a batched torch tensor.",
            "raster_rad = raster_rad[cloud_idx]",
            "if orthogonal_projection:"
        ]
    },
    {
        "number": 7193,
        "comments": "",
        "commit_message": "fix: added eps in geometry/rotmat_to_quaternion (#1665)\n\nFor numerical stability and to avoid nan gradients added an eps in case the trace is -1 so that the sqrt would be negative.\n",
        "label": "",
        "answer": "no",
        "change": [
            "def rotation_matrix_to_quaternion(",
            "trace: torch.Tensor = m00 + m11 + m22",
            "",
            "def trace_positive_cond():",
            "-        sq = torch.sqrt(trace + 1.0) * 2.0  # sq = 4 * qw.",
            "+        sq = torch.sqrt(trace + 1.0 + eps) * 2.0  # sq = 4 * qw.",
            "qw = 0.25 * sq",
            "qx = safe_zero_division(m21 - m12, sq)",
            "qy = safe_zero_division(m02 - m20, sq)"
        ]
    },
    {
        "number": 7194,
        "comments": "",
        "commit_message": "Fix batch log pdf masked view (#322)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Bernoulli(Distribution):",
            "# when the data is a ragged tensor. also useful for KL annealing. this entire logic",
            "# will likely be done in a better/cleaner way in the future",
            "if log_pdf_mask is not None:",
            "-            # TODO fix this to broadcasting as below, e.g. by instead:",
            "-            # logsum *= log_pdf_mask  # Then continue with broadcasting logic below.",
            "-            return torch.sum(log_pdf_mask * logsum, -1)",
            "+            logsum = logsum * log_pdf_mask",
            "batch_log_pdf_shape = self.batch_shape(ps) + (1,)",
            "return torch.sum(logsum, -1).contiguous().view(batch_log_pdf_shape)"
        ]
    },
    {
        "number": 7195,
        "comments": "",
        "commit_message": "update on when learn is defiend with test query, fix on documentation pip3, and use ensamble net by default\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class ModelTrainer(BaseModule):",
            "model_name = self.transaction.persistent_model_metadata.model_name",
            "",
            "ml_models = [",
            "-            ('pytorch.models.fully_connected_net', {})",
            "+            #('pytorch.models.fully_connected_net', {})",
            "#, ('pytorch.models.ensemble_conv_net', {})",
            "-            #, ('pytorch.models.ensemble_fully_connected_net', {})",
            "+            ('pytorch.models.ensemble_fully_connected_net', {})",
            "]",
            "",
            "self.train_start_time = time.time()"
        ]
    },
    {
        "number": 7201,
        "comments": "",
        "commit_message": "lintfixbot: Auto-commit fixed lint errors in codebase\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def gelu(",
            "",
            "",
            "@with_unsupported_dtypes({\"1.11.0 and below\": (\"float16\",)}, backend_version)",
            "-def sigmoid(",
            "-    x: torch.Tensor,",
            "-    /,",
            "-    *,",
            "-    out: Optional[torch.Tensor] = None",
            "-) -> torch.Tensor:",
            "+def sigmoid(x: torch.Tensor, /, *, out: Optional[torch.Tensor] = None) -> torch.Tensor:",
            "if not ivy.is_array(x):",
            "x = torch.tensor(x)",
            "return torch.sigmoid(x, out=out)"
        ]
    },
    {
        "number": 7202,
        "comments": "",
        "commit_message": "Update to PyTorch 1.9.0 (#2887)\n\n* Update to PyTorch 1.9.0\n\n* Update torch.cholesky, torch.symeig\n\n* Fix torch.tensordot, torch.qr, funsor dependency\n\n* Fix torch import, AutoGuideList usage\n\n* Ignore bug in PyTorch jit\n\n* Ignore tracer warnings\n\n* Replace torch.solve with torch.linalg.solve\n\n* Xfail vectorized markov funsor tests\n\n* Update funsor version\n\n* Switch MNIST mirrors\n\n* Pin pillow version\n\n* Bump torchvision version\n",
        "label": "",
        "answer": "no",
        "change": [
            "def vsgp_multiclass(num_steps, whiten):",
            "pyro.set_rng_seed(0)",
            "X = torch.rand(100, 1)",
            "K = (-0.5 * (X - X.t()).pow(2) / 0.01).exp() + torch.eye(100) * 1e-6",
            "-    f = K.cholesky().matmul(torch.randn(100, 3))",
            "+    f = torch.linalg.cholesky(K).matmul(torch.randn(100, 3))",
            "y = f.argmax(dim=-1)",
            "",
            "kernel = gp.kernels.Sum(gp.kernels.Matern32(1),"
        ]
    },
    {
        "number": 7204,
        "comments": "",
        "commit_message": "Rnn scratch fix (#1450)\n\n* Update rnn-scratch.md\n\n* Update rnn-scratch.md\n\n* Revert \"Update rnn-scratch.md\"\n\nThis reverts commit 16c00a668da740d0eded48b6e83805e067a763ea.\n\n* Revert \"Update rnn-scratch.md\"\n\nThis reverts commit 7cd51e90e4d03c3af568db64a2cad7c985a34855.\n\n* Update rnn-scratch.md\n\n* Update tensorflow.py\n",
        "label": "",
        "answer": "no",
        "change": [
            "def train_ch8(model, train_iter, vocab, num_hiddens, lr, num_epochs,",
            "use_random_iter=False):",
            "\"\"\"Train a model (defined in Chapter 8).\"\"\"",
            "params = get_params(len(vocab), num_hiddens)",
            "-    loss = tf.keras.losses.SparseCategoricalCrossentropy()",
            "+    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)",
            "animator = d2l.Animator(xlabel='epoch', ylabel='perplexity',",
            "legend=['train'], xlim=[1, num_epochs])",
            "updater = tf.keras.optimizers.SGD(lr)"
        ]
    },
    {
        "number": 7215,
        "comments": "",
        "commit_message": "Fix GPT-NeoX-20B past handling, attention computation (#17811)\n\n* Fix GPT-NeoX-20B past handling, swap attention computation to hopefully avoid NaN, update docs\n\n* 20B tests\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class GPTNeoXForCausalLM(GPTNeoXPreTrainedModel):",
            "attention_mask = input_ids.new_ones(input_shape)",
            "",
            "# cut decoder_input_ids if past is used",
            "-        if past is not None:",
            "+        if past and past[0] is not None:",
            "input_ids = input_ids[:, -1:]",
            "",
            "return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"past_key_values\": past}"
        ]
    },
    {
        "number": 7216,
        "comments": "",
        "commit_message": "Fix keep_dims/keepdims compatibility issues(#64)\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def routing(input, b_IJ):",
            "input = tf.tile(input, [1, 1, 160, 1, 1])",
            "assert input.get_shape() == [cfg.batch_size, 1152, 160, 8, 1]",
            "",
            "-    u_hat = tf.reduce_sum(W * input, axis=3, keepdims=True)",
            "+    u_hat = reduce_sum(W * input, axis=3, keepdims=True)",
            "u_hat = tf.reshape(u_hat, shape=[-1, 1152, 10, 16, 1])",
            "assert u_hat.get_shape() == [cfg.batch_size, 1152, 10, 16, 1]"
        ]
    },
    {
        "number": 7222,
        "comments": "",
        "commit_message": "Update to PyTorch 1.9.0 (#2887)\n\n* Update to PyTorch 1.9.0\n\n* Update torch.cholesky, torch.symeig\n\n* Fix torch.tensordot, torch.qr, funsor dependency\n\n* Fix torch import, AutoGuideList usage\n\n* Ignore bug in PyTorch jit\n\n* Ignore tracer warnings\n\n* Replace torch.solve with torch.linalg.solve\n\n* Xfail vectorized markov funsor tests\n\n* Update funsor version\n\n* Switch MNIST mirrors\n\n* Pin pillow version\n\n* Bump torchvision version\n",
        "label": "",
        "answer": "no",
        "change": [
            "class SpanningTree(TorchDistribution):",
            "import gpytorch",
            "log_det = gpytorch.lazy.NonLazyTensor(truncated).logdet()",
            "except ImportError:",
            "-            log_det = torch.cholesky(truncated).diag().log().sum() * 2",
            "+            log_det = torch.linalg.cholesky(truncated).diag().log().sum() * 2",
            "return log_det + log_diag[:-1].sum()",
            "",
            "def log_prob(self, edges):"
        ]
    },
    {
        "number": 7223,
        "comments": "",
        "commit_message": "tf solve fix (#6522)\n\n* solve fix-- square checking before det computation\n",
        "label": "",
        "answer": "no",
        "change": [
            "def solve(",
            "else:",
            "x1 = tf.broadcast_to(x1, output_shape + x1.shape[-2:])",
            "x2 = tf.broadcast_to(x2, output_shape + x2.shape[-2:])",
            "-        if tf.math.reduce_any(tf.linalg.det(x1) == 0) or tf.math.reduce_any(",
            "-            tf.linalg.det(x2) == 0",
            "-        ):",
            "+        if tf.math.reduce_any(tf.linalg.det(x1) == 0) or (",
            "+            x2.shape[-1] == x2.shape[-2] and tf.math.reduce_any(",
            "+                tf.linalg.det(x2) == 0)):",
            "return x1",
            "ret = tf.linalg.solve(x1, x2)"
        ]
    },
    {
        "number": 7228,
        "comments": "",
        "commit_message": "for discussion: incorporate black code formatter (#3308)\n\n* setup files\n\n* run black\n\n* undo\n\n* update CONTRIBUTING.md\n\n* fix quotes in test_other_modules\n\n* make flake8 happy\n\n* set black to 100 characters per line\n\n* move type: ignore to where mypy wants them\n\n* more flake8\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Entropy(Metric):",
            "",
            "log_probs = torch.nn.functional.log_softmax(logits, dim=-1)",
            "probabilities = torch.exp(log_probs) * mask.unsqueeze(-1)",
            "-        weighted_negative_likelihood = - log_probs * probabilities",
            "+        weighted_negative_likelihood = -log_probs * probabilities",
            "entropy = weighted_negative_likelihood.sum(-1)",
            "",
            "self._entropy += entropy.sum() / mask.sum()"
        ]
    },
    {
        "number": 7231,
        "comments": "",
        "commit_message": "Fix interpolation comments.\n\nPiperOrigin-RevId: 329882604\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def interpolate(x,",
            "# won't go out of bounds.",
            "lower_encoding = tf.math.maximum(upper_indices - 1, 0)",
            "upper_encoding = tf.math.minimum(upper_indices, x_data_size - 1)",
            "-      # Prepare indices for `tf.gather_nd` or `tf.one_hot`",
            "+      # Prepare indices for `tf.gather` or `tf.one_hot`",
            "# TODO(b/156720909): Extract get_slice logic into a common utilities",
            "# module for cubic and linear interpolation",
            "if optimize_for_tpu:"
        ]
    },
    {
        "number": 7233,
        "comments": "",
        "commit_message": "Restore TF embeddings and attention layers to their previous version (#9890)\n\n* Refacto BERT\n\n* Restore all the concerned models\n\n* Remove print\n\n* Update template\n\n* Apply Sylvain's and Morgan's comments\n\n* Fix cast\n\n* Put the cast inside call\n\n* Remove cond in ebds\n\n* Fix funnel\n\n* Restore previous dot product (attention_scores) computation\n\n* Add ConvBERT and BART\n\n* Make all the S2S models ONNX compliant\n\n* Fix test\n\n* Fix check copies\n",
        "label": "",
        "answer": "no",
        "change": [
            "def _make_causal_mask(input_ids_shape: tf.TensorShape, past_key_values_length: i",
            "",
            "if past_key_values_length > 0:",
            "mask = tf.concat([tf.zeros((tgt_len, past_key_values_length), dtype=tf.float32), mask], axis=-1)",
            "-    return tf.broadcast_to(mask[None, None, :, :], (bsz, 1, tgt_len, tgt_len + past_key_values_length))",
            "+",
            "+    return tf.tile(mask[None, None, :, :], (bsz, 1, 1, 1))",
            "",
            "",
            "def _expand_mask(mask: tf.Tensor, tgt_len: Optional[int] = None, past_key_values_length: int = 0):",
            "\"\"\"",
            "Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.",
            "\"\"\"",
            "-    bsz, src_len = shape_list(mask)",
            "+    src_len = shape_list(mask)[1]",
            "tgt_len = tgt_len if tgt_len is not None else src_len",
            "-",
            "-    expanded_mask = tf.cast(tf.broadcast_to(mask[:, None, None, :], (bsz, 1, tgt_len, src_len)), tf.float32)",
            "+    expanded_mask = tf.cast(tf.tile(mask[:, None, None, :], (1, 1, tgt_len, 1)), tf.float32)",
            "",
            "return (1.0 - expanded_mask) * LARGE_NEGATIVE"
        ]
    },
    {
        "number": 7238,
        "comments": "",
        "commit_message": "minor fix\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def anchor_target_single(flat_anchors,",
            "num_valid_anchors = anchors.shape[0]",
            "bbox_targets = torch.zeros_like(anchors)",
            "bbox_weights = torch.zeros_like(anchors)",
            "-    labels = gt_labels.new_zeros(num_valid_anchors)",
            "-    label_weights = gt_labels.new_zeros(num_valid_anchors, dtype=torch.float)",
            "+    labels = anchors.new_zeros(num_valid_anchors, dtype=torch.long)",
            "+    label_weights = anchors.new_zeros(num_valid_anchors, dtype=torch.float)",
            "",
            "pos_inds = sampling_result.pos_inds",
            "neg_inds = sampling_result.neg_inds"
        ]
    },
    {
        "number": 7246,
        "comments": "",
        "commit_message": "fix device issue (#20227)\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def _segment_reduce(values, index, segment_reduce_fn, name):",
            "dim=0,",
            ")",
            "",
            "-    output_values = segment_means.clone().view(new_shape.tolist())",
            "+    output_values = segment_means.clone().view(new_shape.tolist()).to(values.dtype)",
            "output_index = range_index_map(index.batch_shape(), index.num_segments)",
            "return output_values, output_index"
        ]
    },
    {
        "number": 7261,
        "comments": "",
        "commit_message": "various scope and test fixes\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class PolicyGradientModel(Model):",
            "",
            "with tf.variable_scope('distribution'):",
            "for action, distribution in self.distribution.items():",
            "-                distribution.create_tf_operations(x=self.network.output, deterministic=self.deterministic)",
            "+                with tf.variable_scope(action):",
            "+                    distribution.create_tf_operations(x=self.network.output, deterministic=self.deterministic)",
            "self.action_taken[action] = distribution.sample()",
            "",
            "if self.baseline:"
        ]
    },
    {
        "number": 7263,
        "comments": "",
        "commit_message": "Fix summary iterator import for TF 2 compatibility (#732)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def stream_tfevents(path, file_api, run, step=0, namespace=\"\"):",
            "last_row = {}",
            "global_step_key = namespaced_tag(\"global_step\", namespace)",
            "try:",
            "-        for summary in tf.train.summary_iterator(path):",
            "+        for summary in summary_iterator(path):",
            "parsed = tf_summary_to_dict(summary, namespace=namespace)",
            "if last_step != parsed[global_step_key]:",
            "last_step = parsed[global_step_key]"
        ]
    },
    {
        "number": 7267,
        "comments": "",
        "commit_message": "add doc and fix bug in experiment resume/view (#3524)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Experiments:",
            "if os.path.exists(self.experiment_file):",
            "try:",
            "with open(self.experiment_file, 'r') as file:",
            "-                    return json.load(file)",
            "+                    return json_tricks.load(file)",
            "except ValueError:",
            "return {}",
            "return {}"
        ]
    },
    {
        "number": 7268,
        "comments": "",
        "commit_message": "Fixing conversation test for torch 1.8 (#10545)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class SimpleConversationPipelineTests(unittest.TestCase):",
            "model = GPT2LMHeadModel(config)",
            "# Force model output to be L",
            "V, D = model.lm_head.weight.shape",
            "-        bias = torch.zeros(V, requires_grad=True)",
            "-        weight = torch.zeros((V, D), requires_grad=True)",
            "+        bias = torch.zeros(V)",
            "bias[76] = 1",
            "+        weight = torch.zeros((V, D), requires_grad=True)",
            "",
            "model.lm_head.bias = torch.nn.Parameter(bias)",
            "model.lm_head.weight = torch.nn.Parameter(weight)"
        ]
    },
    {
        "number": 7270,
        "comments": "",
        "commit_message": "bugfix\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TransLoss(torch.nn.Module):",
            "loss (torch.Tensor): transducer loss",
            "",
            "\"\"\"",
            "-        if trans_type == \"warp-transducer\" and pred_pad.dtype != torch.float32:",
            "+        if self.trans_type == \"warp-transducer\" and pred_pad.dtype != torch.float32:",
            "# warprnnt_pytorch only supports float32",
            "pred_pad = pred_pad.to(dtype=torch.float32)",
            "if self.trans_type == \"warp-rnnt\":"
        ]
    },
    {
        "number": 7271,
        "comments": "",
        "commit_message": "Fix ArrowWriter closes stream at exit (#1971)\n\n* Test ArrowWriter closes stream at exit\n\n* Implement ArrowWriter as context manager and close stream at exit\n\n* Specify exception class\n\n* Use ArrowWriter as context manager\n\n* Fix ArrowWriter.close\n\n* Fix ArrowWriter.close\n\n* Test stream already closed at ArrowWriter context exit\n\n* Revert \"Use ArrowWriter as context manager\"\n\nThis reverts commit b28967fe0bd0bed1cf460eba7f789f1ff8f8370a.\n\n* Use ArrowWriter as context manager in tests\n\n* Generalize Exception type in ArrowWriter.close\n\n* Use ArrowWriter as context manager in benchmarks\n\n* Use ArrowWriter as context manager in arrow_writer and builder\n\n* Use ArrowWriter as context manager in arrow_dataset\n\n* Use ArrowWriter as context manager in metric\n\n* Revert \"Use ArrowWriter as context manager in metric\"\n\nThis reverts commit 7f287fb380479ee829dda2296188080774b72621.\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def generate_examples(features: dict, num_examples=100, seq_shapes=None):",
            "def generate_example_dataset(dataset_path, features, num_examples=100, seq_shapes=None):",
            "dummy_data = generate_examples(features, num_examples=num_examples, seq_shapes=seq_shapes)",
            "",
            "-    writer = datasets.ArrowWriter(features=features, path=dataset_path)",
            "-    for key, record in dummy_data:",
            "-        example = features.encode_example(record)",
            "-        writer.write(example)",
            "+    with datasets.ArrowWriter(features=features, path=dataset_path) as writer:",
            "+        for key, record in dummy_data:",
            "+            example = features.encode_example(record)",
            "+            writer.write(example)",
            "",
            "-    num_final_examples, num_bytes = writer.finalize()",
            "+        num_final_examples, num_bytes = writer.finalize()",
            "",
            "assert (",
            "num_final_examples == num_examples"
        ]
    },
    {
        "number": 7274,
        "comments": "",
        "commit_message": "style: code style fixes (#1046)\n\n* fix: automatic code style correction\n\n* fix: squad_iterator\n\n* fix: delete document_bert_ner_iterator\n\n* fix: revert json files to dev version\n\n* fix: removed from registry\n\n* refactor: fix merge mistakes\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def make_module_spec(options, weight_file):",
            "",
            "# Input placeholders to the biLM.",
            "tokens = tf.placeholder(shape=(None, None), dtype=tf.string, name='ph2tokens')",
            "-        sequence_len = tf.placeholder(shape=(None, ), dtype=tf.int32, name='ph2sequence_len')",
            "+        sequence_len = tf.placeholder(shape=(None,), dtype=tf.int32, name='ph2sequence_len')",
            "",
            "tok_shape = tf.shape(tokens)",
            "line_tokens = tf.reshape(tokens, shape=[-1], name='reshape2line_tokens')"
        ]
    },
    {
        "number": 7276,
        "comments": "",
        "commit_message": "Update helpers.py\n\nFixing out of memory error by loading the checkpoint onto the CPU.\n",
        "label": "",
        "answer": "no",
        "change": [
            "def resume_checkpoint(model, checkpoint_path):",
            "optimizer_state = None",
            "resume_epoch = None",
            "if os.path.isfile(checkpoint_path):",
            "-        checkpoint = torch.load(checkpoint_path)",
            "+        checkpoint = torch.load(checkpoint_path, map_location='cpu')",
            "if isinstance(checkpoint, dict) and 'state_dict' in checkpoint:",
            "new_state_dict = OrderedDict()",
            "for k, v in checkpoint['state_dict'].items():"
        ]
    },
    {
        "number": 7279,
        "comments": "",
        "commit_message": "Fix keras docstrings\n\nPiperOrigin-RevId: 424275818\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TruncatedNormal(tf.compat.v1.truncated_normal_initializer):",
            ">>> a = initializer(shape=(2, 2))",
            ">>> b = initializer(shape=(2, 2))",
            ">>> tf.reduce_sum(a - b) == 0",
            "-  <tf.Tensor: shape=(), dtype=bool, numpy=True>",
            "+  <tf.Tensor: shape=(), dtype=bool, numpy=False>",
            "",
            "@end_compatibility",
            "\"\"\""
        ]
    },
    {
        "number": 7282,
        "comments": "",
        "commit_message": "TF port of ESM (#19587)\n\n* Partial TF port for ESM model\n\n* Add ESM-TF tests\n\n* Add the various imports for TF-ESM\n\n* TF weight conversion almost ready\n\n* Stop ignoring the decoder weights in PT\n\n* Add tests and lots of fixes\n\n* fix-copies\n\n* Fix imports, add model docs\n\n* Add get_vocab() to tokenizer\n\n* Fix vocab links for pretrained files\n\n* Allow multiple inputs with a sep\n\n* Use EOS as SEP token because ESM vocab lacks SEP\n\n* Correctly return special tokens mask from ESM tokenizer\n\n* make fixup\n\n* Stop testing unsupported embedding resizing\n\n* Handle TF bias correctly\n\n* Skip all models with slow tokenizers in the token classification test\n\n* Fixing the batch/unbatcher of pipelines to accomodate the `None` being\n\npassed around.\n\n* Fixing pipeline bug caused by slow tokenizer  being different.\n\n* Update src/transformers/models/esm/modeling_tf_esm.py\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>\n\n* Update src/transformers/models/esm/modeling_tf_esm.py\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>\n\n* Update src/transformers/models/esm/modeling_tf_esm.py\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>\n\n* Update set_input_embeddings and the copyright notices\n\nCo-authored-by: Your Name <you@example.com>\nCo-authored-by: Nicolas Patry <patry.nicolas@protonmail.com>\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class PipelineIterator(IterableDataset):",
            "elif isinstance(element[0], np.ndarray):",
            "loader_batched[k] = tuple(np.expand_dims(el[self._loader_batch_index], 0) for el in element)",
            "continue",
            "-                if isinstance(element[self._loader_batch_index], torch.Tensor):",
            "+                if element is None:",
            "+                    # This can happen for optional data that get passed around",
            "+                    loader_batched[k] = None",
            "+                elif isinstance(element[self._loader_batch_index], torch.Tensor):",
            "# Take correct batch data, but make it looked like batch_size=1",
            "# For compatibility with other methods within transformers"
        ]
    },
    {
        "number": 7291,
        "comments": "",
        "commit_message": "Fix doc formatting and improve doc styling (#4072)\n\n* switch to pydoc-markdown with custom processor\n\n* add some extra css\n\n* fixes\n\n* fixes\n\n* minor tweaks\n\n* fixes\n\n* add breadcrumbs\n\n* fixes\n\n* fix arg formatting\n\n* fix\n\n* fixes\n\n* more fixes\n\n* fix\n\n* fix cross-refs within module\n\n* fix dev requirements\n\n* pin pydoc-markdown to latest commit\n\n* small refactor and docstring fixes\n\n* more small fixes\n",
        "label": "",
        "answer": "no",
        "change": [
            "class ElmoTokenEmbedder(TokenEmbedder):",
            "",
            "# Returns",
            "",
            "-        The ELMo representations for the input sequence, shape",
            "-        `(batch_size, timesteps, embedding_dim)`",
            "+        `torch.Tensor`",
            "+            The ELMo representations for the input sequence, shape",
            "+            `(batch_size, timesteps, embedding_dim)`",
            "\"\"\"",
            "elmo_output = self._elmo(tokens, word_inputs)",
            "elmo_representations = elmo_output[\"elmo_representations\"][0]"
        ]
    },
    {
        "number": 7293,
        "comments": "",
        "commit_message": "fix step increment\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class DistributedModel(object):",
            "",
            "grad_var_list = list(zip(self.gradients, self.global_network.get_variables()))",
            "",
            "-            global_step_inc = self.global_step.assign_add(self.batch_size)",
            "+            global_step_inc = self.global_step.assign_add(tf.shape(self.state)[0])",
            "",
            "self.assign_global_to_local = tf.group(*[v1.assign(v2) for v1, v2 in",
            "zip(self.local_network.get_variables(),"
        ]
    },
    {
        "number": 7296,
        "comments": "",
        "commit_message": "fix example docs style (#517)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "with tf.Session() as sess:",
            "",
            "## Note that, the rewards here with random action",
            "running_reward = rAll if running_reward is None else running_reward * 0.99 + rAll * 0.01",
            "-        print(",
            "-            \"Episode [%d/%d] sum reward:%f running reward:%f took:%.5fs %s\" %",
            "-            (i, num_episodes, rAll, running_reward, time.time() - episode_time, '' if rAll == 0 else ' !!!!!!!!')",
            "-        )",
            "+        print(\"Episode [%d/%d] sum reward:%f running reward:%f took:%.5fs %s\" % \\",
            "+            (i, num_episodes, rAll, running_reward, time.time() - episode_time, '' if rAll == 0 else ' !!!!!!!!'))"
        ]
    },
    {
        "number": 7298,
        "comments": "",
        "commit_message": "bug fix in HED example\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Model(ModelDesc):",
            "for idx, b in enumerate([b1, b2, b3, b4, b5, final_map]):",
            "output = tf.nn.sigmoid(b, name='output{}'.format(idx+1))",
            "xentropy = class_balanced_sigmoid_cross_entropy(",
            "-                b, edgemap,",
            "+                tf.squeeze(b, [3]), edgemap,",
            "name='xentropy{}'.format(idx+1))",
            "costs.append(xentropy)"
        ]
    },
    {
        "number": 7300,
        "comments": "",
        "commit_message": "Fix twin q bug (#4108)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class ActorCriticLoss(object):",
            "",
            "q_t_selected = tf.squeeze(q_t, axis=len(q_t.shape) - 1)",
            "if twin_q:",
            "-            twin_q_t_selected = tf.squeeze(q_t, axis=len(q_t.shape) - 1)",
            "+            twin_q_t_selected = tf.squeeze(twin_q_t, axis=len(q_t.shape) - 1)",
            "q_tp1 = tf.minimum(q_tp1, twin_q_tp1)",
            "",
            "q_tp1_best = tf.squeeze(input=q_tp1, axis=len(q_tp1.shape) - 1)"
        ]
    },
    {
        "number": 7305,
        "comments": "",
        "commit_message": "Fix examples and tutorials to use the updated tensor API (#886)\n\n* Fix examples and tutorials to use the updated tensor API\n\n* small fixes\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "\"source\": [",
            "\"def model(xs, ys=None):\\n\",",
            "\"    # sample z from the prior \\n\",",
            "-    \"    prior_mu = Variable(torch.zeros([batch_size, z_dim]))\\n\",",
            "-    \"    prior_sigma = Variable(torch.ones([batch_size, z_dim]))\\n\",",
            "+    \"    prior_mu = torch.zeros([batch_size, z_dim])\\n\",",
            "+    \"    prior_sigma = torch.ones([batch_size, z_dim])\\n\",",
            "\"    zs = pyro.sample(\\\"z\\\", dist.Normal(prior_mu, prior_sigma))\\n\",",
            "\"\\n\",",
            "\"    # if the label y is observed, sample from the prior.\\n\",",
            "\"    # otherwise, observe the value \\n\",",
            "-    \"    alpha_prior = Variable(torch.ones([batch_size, 10]) / (10.))\\n\",",
            "+    \"    alpha_prior = torch.ones([batch_size, 10]) / (10.)\\n\",",
            "\"    if ys is None:\\n\",",
            "\"        ys = pyro.sample(\\\"y\\\", dist.OneHotCategorical(alpha_prior))\\n\",",
            "\"    else:\\n\","
        ]
    },
    {
        "number": 7307,
        "comments": "",
        "commit_message": "lintfixbot: Auto-commit fixed lint errors in codebase\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "from .. import backend_version",
            "",
            "",
            "@with_unsupported_dtypes({\"1.11.0 and below\": (\"float16\",)}, backend_version)",
            "-def l2_normalize(x: torch.Tensor,",
            "-                 axis: int = None,",
            "-                 out: torch.Tensor = None",
            "-                 ) -> torch.Tensor:",
            "+def l2_normalize(",
            "+    x: torch.Tensor, axis: int = None, out: torch.Tensor = None",
            "+) -> torch.Tensor:",
            "",
            "return torch.nn.functional.normalize(x, p=2, dim=axis, out=out)"
        ]
    },
    {
        "number": 7315,
        "comments": "",
        "commit_message": "PEP8 fixes.\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "y = img_input",
            "for i in range(nlayers):",
            "y_prepool = convresblock(y, nfeats=nfeats_all[i + 1], ksize=ksize)",
            "y = MaxPooling2D(pool_size=(pool_sizes[i], pool_sizes[i]))(y_prepool)",
            "-    wheres[i] = layers.Lambda(getwhere, output_shape=lambda x: x[0])([y_prepool, y])",
            "+    wheres[i] = layers.Lambda(",
            "+        getwhere, output_shape=lambda x: x[0])([y_prepool, y])",
            "",
            "# Now build the decoder, and use the stored 'where' masks to place the features",
            "for i in range(nlayers):"
        ]
    },
    {
        "number": 7317,
        "comments": "",
        "commit_message": "fix error calculator for report false\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class E2E(ASRInterface, chainer.Chain):",
            "xs = xs.reshape(batch, -1, self.dims)",
            "xs = [xs[i, :ilens[i], :] for i in range(len(ilens))]",
            "loss_ctc = self.ctc(xs, ys_pad_cpu)",
            "-            with chainer.no_backprop_mode():",
            "-                ys_hat = chainer.backends.cuda.to_cpu(self.ctc.argmax(xs).data)",
            "-            cer_ctc = self.error_calculator(ys_hat, ys_pad_cpu, is_ctc=True)",
            "+            if self.error_calculator is not None:",
            "+                with chainer.no_backprop_mode():",
            "+                    ys_hat = chainer.backends.cuda.to_cpu(self.ctc.argmax(xs).data)",
            "+                cer_ctc = self.error_calculator(ys_hat, ys_pad_cpu, is_ctc=True)",
            "",
            "# Compute cer/wer",
            "with chainer.no_backprop_mode():"
        ]
    },
    {
        "number": 7323,
        "comments": "",
        "commit_message": "py 2.7 fix\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def read_ply(path):",
            "face = None",
            "if 'face' in data:",
            "faces = data['face']['vertex_indices']",
            "-        faces = [torch.tensor(f, dtype=torch.long) for f in faces]",
            "+        faces = [torch.tensor(fa, dtype=torch.long) for fa in faces]",
            "face = torch.stack(faces, dim=-1)",
            "",
            "data = Data(pos=pos)"
        ]
    },
    {
        "number": 7328,
        "comments": "",
        "commit_message": "Fix float zeros format (#5491)\n\n* Fix float zeros format\n\n* 255 to integer\n",
        "label": "",
        "answer": "no",
        "change": [
            "def kmean_anchors(dataset='./data/coco128.yaml', n=9, img_size=640, thr=4.0, gen",
            "\"\"\"",
            "from scipy.cluster.vq import kmeans",
            "",
            "-    thr = 1. / thr",
            "+    thr = 1 / thr",
            "prefix = colorstr('autoanchor: ')",
            "",
            "def metric(k, wh):  # compute metrics",
            "r = wh[:, None] / k[None]",
            "-        x = torch.min(r, 1. / r).min(2)[0]  # ratio metric",
            "+        x = torch.min(r, 1 / r).min(2)[0]  # ratio metric",
            "# x = wh_iou(wh, torch.tensor(k))  # iou metric",
            "return x, x.max(1)[0]  # x, best_x"
        ]
    },
    {
        "number": 7333,
        "comments": "",
        "commit_message": "Quick fixes\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TensorBoard(Callback):",
            "embedding_input = tf.reshape(embedding_input,",
            "(step, int(embedding_size)))",
            "shape = (self.embeddings_data[0].shape[0], int(embedding_size))",
            "-                    embedding = tf.Variable(tf.zeros(shape),",
            "-                                            name=layer.name + '_embedding')",
            "+                    embedding = K.variable(K.zeros(shape),",
            "+                                           name=layer.name + '_embedding')",
            "embeddings_vars[layer.name] = embedding",
            "batch = tf.assign(embedding[batch_id:batch_id + step],",
            "embedding_input)"
        ]
    },
    {
        "number": 7334,
        "comments": "",
        "commit_message": "Simplify logs management (#1696)\n\n* Move each haystack module's logger configuration into the respective file and configure the handlers properly\n\n* Implement most changes from #1714\n\n* Remove accidentally committed git merge tags ':D\n\n* Remove the debug logs capture feature\n\n* Remove more references to debug_logs\n\n* Fix issue with FARMReader that somehow made it to master\n\n* Add devices parameter to Inferencer\n\n* Change log of APEX message to DEBUG and lower the 'Starting <docstore>...' messages to DEBUG as well\n\n* Change log level of a few logs from modeling\n\n* Silence the transformers warning\n\n* Remove empty line below the workers :)\n\n* Fix two more levels in the tutorials logs\n\nCo-authored-by: github-actions[bot] <41898282+github-actions[bot]@users.noreply.github.com>\nCo-authored-by: bogdankostic <bogdankostic@web.de>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Inferencer:",
            "",
            "model = AdaptiveModel.convert_from_transformers(model_name_or_path,",
            "revision=revision,",
            "-                                                            device=devices[0],",
            "+                                                            device=devices[0],  # type: ignore",
            "task_type=task_type,",
            "**kwargs)",
            "processor = Processor.convert_from_transformers(model_name_or_path,"
        ]
    },
    {
        "number": 7336,
        "comments": "",
        "commit_message": "bug fix\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def build_targets(p, targets, model):",
            "if use_all_anchors:",
            "na = anchor_vec.shape[0]  # number of anchors",
            "a = torch.arange(na).view(-1, 1).repeat(1, nt).view(-1)",
            "-                t = targets.repeat(na, 1)",
            "+                t = t.repeat(na, 1)",
            "else:  # use best anchor only",
            "iou, a = iou.max(0)  # best iou and anchor"
        ]
    },
    {
        "number": 7337,
        "comments": "",
        "commit_message": "[RLlib] Tf2x preparation; part 2 (upgrading `try_import_tf()`). (#9136)\n\n* WIP.\n\n* Fixes.\n\n* LINT.\n\n* WIP.\n\n* WIP.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* WIP.\n\n* Fixes.\n\n* Test\n\n* Fix.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* LINT.\n",
        "label": "",
        "answer": "no",
        "change": [
            "class LSTM(Model):",
            "if self.state_in:",
            "c_in, h_in = self.state_in",
            "else:",
            "-            c_in = tf.placeholder(",
            "+            c_in = tf1.placeholder(",
            "tf.float32, [None, lstm.state_size.c], name=\"c\")",
            "-            h_in = tf.placeholder(",
            "+            h_in = tf1.placeholder(",
            "tf.float32, [None, lstm.state_size.h], name=\"h\")",
            "self.state_in = [c_in, h_in]",
            "",
            "# Setup LSTM outputs",
            "-        state_in = tf.nn.rnn_cell.LSTMStateTuple(c_in, h_in)",
            "-        lstm_out, lstm_state = tf.nn.dynamic_rnn(",
            "+        state_in = tf1.nn.rnn_cell.LSTMStateTuple(c_in, h_in)",
            "+        lstm_out, lstm_state = tf1.nn.dynamic_rnn(",
            "lstm,",
            "last_layer,",
            "initial_state=state_in,"
        ]
    },
    {
        "number": 7343,
        "comments": "",
        "commit_message": "lint fixes\n\nSummary:\nRan the linter.\nTODO: need to update the linter as per D21353065.\n\nReviewed By: bottler\n\nDifferential Revision: D21362270\n\nfbshipit-source-id: ad0e781de0a29f565ad25c43bc94a19b1828c020\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestBlending(unittest.TestCase):",
            "# of the image with surrounding padded values.",
            "N, S, K = 1, 8, 2",
            "device = torch.device(\"cuda\")",
            "-        pix_to_face = -torch.ones((N, S, S, K), dtype=torch.int64, device=device)",
            "+        pix_to_face = torch.full(",
            "+            (N, S, S, K), fill_value=-1, dtype=torch.int64, device=device",
            "+        )",
            "h = int(S / 2)",
            "pix_to_face_full = torch.randint(",
            "size=(N, h, h, K), low=0, high=100, device=device"
        ]
    },
    {
        "number": 7346,
        "comments": "",
        "commit_message": "[bugfix] Always return batch indices to prevent duplicated logic for the users (#9432)\n\nCo-authored-by: Adrian W\u00e4lchli <aedu.waelchli@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class PredictionEpochLoop(Loop):",
            "self.current_batch_indices = batch_sampler.batch_indices",
            "if self.should_store_predictions:",
            "self._all_batch_indices.append(batch_sampler.batch_indices)",
            "+        else:",
            "+            warning_cache.warn(\"Lightning couldn't infer the indices fetched for your dataloader.\")"
        ]
    },
    {
        "number": 7348,
        "comments": "",
        "commit_message": "0.4.0 fixes\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class AttSplineConv(Module):",
            "is_open_spline = torch.ByteTensor(repeat_to(is_open_spline, dim))",
            "self.register_buffer('is_open_spline', is_open_spline)",
            "",
            "-        weight = torch.Tensor(kernel_size.prod(), in_channels, out_channels)",
            "-        self.weight = Parameter(weight)",
            "+        K = kernel_size.prod().item()",
            "+        self.weight = Parameter(torch.Tensor(K, in_channels, out_channels))",
            "",
            "self.root_weight = Parameter(torch.Tensor(in_channels, out_channels))"
        ]
    },
    {
        "number": 7356,
        "comments": "",
        "commit_message": "fix\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def tree_decomposition(mol):",
            "rows = [[i] * len(atom2clique[i]) for i in range(mol.GetNumAtoms())]",
            "row = torch.tensor(list(chain.from_iterable(rows)))",
            "col = torch.tensor(list(chain.from_iterable(atom2clique)))",
            "-    atom2clique = torch.stack([row, col], dim=0)",
            "+    atom2clique = torch.stack([row, col], dim=0).to(torch.long)",
            "",
            "return edge_index, atom2clique, len(cliques)"
        ]
    },
    {
        "number": 7362,
        "comments": "",
        "commit_message": "lots of bug fixes\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "import scipy.spatial",
            "",
            "",
            "def nn_graph(pos, k=6):",
            "-    row = torch.arange(0, pos.size(0)).view(-1, 1).repeat(1, 6).view(-1).long()",
            "+    row = torch.arange(0, pos.size(0)).view(-1, 1).repeat(1, k).view(-1).long()",
            "tree = scipy.spatial.cKDTree(pos.numpy())",
            "_, col = tree.query(pos, k + 1)",
            "col = torch.LongTensor(col[:len(col)])[:, 1:].contiguous().view(-1)"
        ]
    },
    {
        "number": 7363,
        "comments": "",
        "commit_message": "fix has_isolated_nodes for homogeneous graphs (#4138)\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def contains_isolated_nodes(edge_index, num_nodes=None):",
            "\"\"\"",
            "num_nodes = maybe_num_nodes(edge_index, num_nodes)",
            "edge_index, _ = remove_self_loops(edge_index)",
            "-    return torch.unique(edge_index.view(-1)).size(0) < num_nodes",
            "+    return torch.unique(edge_index.view(-1)).numel() < num_nodes",
            "",
            "",
            "def remove_isolated_nodes(edge_index, edge_attr=None, num_nodes=None):"
        ]
    },
    {
        "number": 7364,
        "comments": "",
        "commit_message": "Add fp16 support (#963)\n\n* Update imgwarp.py\n\n* update fix\n\n* move _torch_inverse_cast under utils\n\n* fix some tests in pyramid\n\n* fix issues\n\n* implement other autocasting functions\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def zca_mean(inp: torch.Tensor, dim: int = 0,",
            "else:",
            "cov = cov / float(N)",
            "",
            "-    U, S, _ = torch.svd(cov)",
            "+    U, S, _ = _torch_svd_cast(cov)",
            "",
            "S = S.reshape(-1, 1)",
            "S_inv_root: torch.Tensor = torch.rsqrt(S + eps)"
        ]
    },
    {
        "number": 7367,
        "comments": "",
        "commit_message": "Fixed general problems with variable handling and various related changes, removed DDQN\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Queue(Memory):",
            "updates=internal",
            "))",
            "for name, action in actions.items():",
            "-                assignments.append(tf.scatter_update(ref=self.actions_memory[name], indices=indices, updates=action))",
            "+                assignments.append(tf.scatter_update(",
            "+                    ref=self.actions_memory[name],",
            "+                    indices=indices,",
            "+                    updates=action",
            "+                ))",
            "assignments.append(tf.scatter_update(ref=self.terminal_memory, indices=indices, updates=terminal))",
            "assignments.append(tf.scatter_update(ref=self.reward_memory, indices=indices, updates=reward))"
        ]
    },
    {
        "number": 7368,
        "comments": "",
        "commit_message": "Fix device for TraceTailAdaptive_ELBO (#1860)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TraceTailAdaptive_ELBO(Trace_ELBO):",
            "# rank the particles according to p/q",
            "log_pq = log_p - log_q",
            "rank = torch.argsort(log_pq, descending=False)",
            "-        rank = torch.index_select(torch.arange(self.num_particles) + 1, -1, rank).type_as(log_pq)",
            "+        rank = torch.index_select(torch.arange(self.num_particles, device=log_pq.device) + 1, -1, rank).type_as(log_pq)",
            "",
            "# compute the particle-specific weights used to construct the surrogate loss",
            "gamma = torch.pow(rank, self.tail_adaptive_beta).detach()"
        ]
    },
    {
        "number": 7371,
        "comments": "",
        "commit_message": "Fix support for dataclasses with ClassVar/InitVar in `apply_to_collection` (#9702)\n\nCo-authored-by: Adrian W\u00e4lchli <aedu.waelchli@gmail.com>\nCo-authored-by: Carlos Mocholi <carlossmocholi@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_tpu_sync_dist():",
            "\"\"\"Test tpu spawn sync dist operation.\"\"\"",
            "",
            "def test_sync_dist(_):",
            "-        sync = _Sync(TPUSpawnPlugin().reduce, should=True, op=torch.distributed.ReduceOp.SUM)",
            "+        sync = _Sync(TPUSpawnPlugin().reduce, should=True, _op=torch.distributed.ReduceOp.SUM)",
            "value = torch.tensor([1.0])",
            "value = (sync(value),)",
            "assert value.item() == 8"
        ]
    },
    {
        "number": 7374,
        "comments": "",
        "commit_message": "fixed graph conv on gpu\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def mm_diagonal(a, b, transpose=False):",
            "",
            "value = a[target] * value",
            "",
            "-    return torch.sparse.FloatTensor(index, value, b.size())",
            "+    return SparseTensor(index, value, b.size())"
        ]
    },
    {
        "number": 7375,
        "comments": "",
        "commit_message": "Fix division by zero when there are zero-length spans in MismatchedEmbedder. (#4615)\n\n* Implment MattG's fix for NaN gradients in MismatchedEmbedder.\n\nFix `clamp_min` on embeddings.\n\nImplment MattG's fix for NaN gradients in MismatchedEmbedder.\n\n* Fix NaN gradients caused by weird tokens in MismatchedEmbedder.\n\nFixed division by zero error when there are zero-length spans in the input to a\nmismatched embedder.\n\n* Add changelog message.\n\n* Re-run `black` to get code formatting right.\n\n* combine fixed sections after merging with master\n\nCo-authored-by: Matt Gardner <mattg@allenai.org>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class PretrainedTransformerMismatchedEmbedder(TokenEmbedder):",
            "span_embeddings_sum = span_embeddings.sum(2)",
            "span_embeddings_len = span_mask.sum(2)",
            "# Shape: (batch_size, num_orig_tokens, embedding_size)",
            "-        orig_embeddings = span_embeddings_sum / span_embeddings_len",
            "+        orig_embeddings = span_embeddings_sum / torch.clamp_min(span_embeddings_len, 1)",
            "",
            "# All the places where the span length is zero, write in zeros.",
            "orig_embeddings[(span_embeddings_len == 0).expand(orig_embeddings.shape)] = 0"
        ]
    },
    {
        "number": 7378,
        "comments": "",
        "commit_message": "Bug fix: make the CNN LM compatible with GPU\n\nSummary:\nSo we could run offline eval in GPU.\n**It does NOT affect CPU and NNPI inference**\n1. Let padding to use same device and dtype as input tensor in case states is already in GPU.\n2. register offsets as buffer so it could be moved to GPU\nElse it reports error when running in GPU: P340717906\nFound it's an optimization performed by jit.trace, which treat the rep.offset as const:\n```\n_12 = torch.view(CONSTANTS.c1, [1, 1, 1, -1])\n....\nstate_ind_offset = torch.expand(_12, [_14, _16, int(_17), -1], implicit=False)\n```\nSee full traced code before(P340717906) vs after (P340940281)\n\nSolution: https://discuss.pytorch.org/t/right-way-to-place-tensors-on-devices/78012\n\nDifferential Revision: D27318640\n\nfbshipit-source-id: c8767c2986af10e618314a0e3c3df1a002cdd452\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def pad_float_tensor(",
            "bs = shape[0]",
            "max_batch_len = pad_length(bs, batch_padding_control, -1)",
            "shape[0] = max_batch_len - bs",
            "-    pad_tensor = torch.zeros(shape, dtype=torch.float)",
            "+    pad_tensor = input_tensor.new_zeros(shape)",
            "new_tensor = torch.cat([input_tensor, pad_tensor], 0)",
            "return new_tensor"
        ]
    },
    {
        "number": 7379,
        "comments": "",
        "commit_message": "[RLlib] Minor fixes (torch GPU bugs + some cleanup). (#11609)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def after_init(policy: Policy, obs_space: gym.spaces.Space,",
            "config: TrainerConfigDict) -> None:",
            "ComputeTDErrorMixin.__init__(policy)",
            "TargetNetworkMixin.__init__(policy, obs_space, action_space, config)",
            "-    # Move target net to device (this is done autoatically for the",
            "+    # Move target net to device (this is done automatically for the",
            "# policy.model, but not for any other models the policy has).",
            "policy.target_q_model = policy.target_q_model.to(policy.device)"
        ]
    },
    {
        "number": 7384,
        "comments": "",
        "commit_message": "fix errors flagged by lint\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def as_ivy_dtype(dtype_in: Union[torch.dtype, str, bool, int, float], /) -> ivy.",
            "",
            "",
            "@with_unsupported_dtypes({\"1.11.0 and below\": (\"uint16\",)}, backend_version)",
            "-def as_native_dtype(",
            "-    dtype_in: Union[torch.dtype, str, bool, int, float], /",
            "-) -> torch.dtype:",
            "+def as_native_dtype(dtype_in: Union[torch.dtype, str, bool, int, float]) -> torch.dtype:",
            "if dtype_in is int:",
            "return ivy.default_int_dtype(as_native=True)",
            "if dtype_in is float:"
        ]
    },
    {
        "number": 7386,
        "comments": "",
        "commit_message": "Fix common tests for FP16 (#1588)\n\n* Fix common tests for FP16\n\n* revert\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class PipelineTesterMixin:",
            "",
            "with tempfile.TemporaryDirectory() as tmpdir:",
            "pipe.save_pretrained(tmpdir)",
            "-            pipe_loaded = self.pipeline_class.from_pretrained(tmpdir)",
            "+            pipe_loaded = self.pipeline_class.from_pretrained(tmpdir, torch_dtype=torch.float16)",
            "pipe_loaded.to(torch_device)",
            "pipe_loaded.set_progress_bar_config(disable=None)"
        ]
    },
    {
        "number": 7391,
        "comments": "",
        "commit_message": "fix deprecation warnings; remove pydoop dependency; update README\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def repeat_op(repetitions, inputs, op, *args, **kwargs):",
            "ValueError: if the op is unknown or wrong.",
            "\"\"\"",
            "scope = kwargs.pop('scope', None)",
            "-  with tf.variable_op_scope([inputs], scope, 'RepeatOp'):",
            "+  with tf.variable_scope(scope, 'RepeatOp', [inputs]):",
            "tower = inputs",
            "for _ in range(repetitions):",
            "tower = op(tower, *args, **kwargs)"
        ]
    },
    {
        "number": 7392,
        "comments": "",
        "commit_message": "rnn->encoders, removes _th suffix, same filenames for chainer and pytorch, modify tests to not rely on _th\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def main():",
            "# train",
            "logging.info('backend = ' + args.backend)",
            "if args.backend == \"chainer\":",
            "-        from espnet.lm.chain.lm_chainer import train",
            "+        from espnet.lm.chain.lm import train",
            "train(args)",
            "elif args.backend == \"pytorch\":",
            "-        from espnet.lm.pytorch.lm_pytorch import train",
            "+        from espnet.lm.pytorch.lm import train",
            "train(args)",
            "else:",
            "raise ValueError(\"Only chainer and pytorch are supported.\")"
        ]
    },
    {
        "number": 7394,
        "comments": "",
        "commit_message": "Minor fix for `mask_feature` (#6043)\n\nA careless error - it should be `rand_like` instead of `randn_like`.\n",
        "label": "",
        "answer": "no",
        "change": [
            "def mask_feature(x: Tensor, p: float = 0.5, mode: str = 'col',",
            "mask = torch.rand(x.size(1), device=x.device) >= p",
            "mask = mask.view(1, -1)",
            "else:",
            "-        mask = torch.randn_like(x) >= p",
            "+        mask = torch.rand_like(x) >= p",
            "",
            "x = x.masked_fill(~mask, fill_value)",
            "return x, mask"
        ]
    },
    {
        "number": 7400,
        "comments": "",
        "commit_message": "fix linter (#630)\n\n* fix linter\n\n* no imports okay\n\n* explicit bases\n\n* disable in pylintrc\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TorchBuffer(InterpretedBuffer):",
            "fxn_for_op : ClassVar = torch_fxn_for_op",
            "",
            "@staticmethod",
            "-  def fromCPU(data): return TorchBuffer(torch.from_numpy(data).requires_grad_(False).to(device))",
            "-  def toCPU(x): return x._buf.cpu().numpy()",
            "+  def fromCPU(x): return TorchBuffer(torch.from_numpy(x).requires_grad_(False).to(device))",
            "+  def toCPU(self): return self._buf.cpu().numpy()"
        ]
    },
    {
        "number": 7401,
        "comments": "",
        "commit_message": "adopt torch.testing.assert_close (#1031)\n\n* adopt torch.testing.assert_close\n\n* use torch.testing.assert_close\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add TODO\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestMedianBlur:",
            "op_module = kornia.filters.MedianBlur((3, 5))",
            "actual = op_module(img)",
            "expected = op(img, kernel_size)",
            "-        assert_allclose(actual, expected)",
            "+        assert_close(actual, expected)"
        ]
    },
    {
        "number": 7402,
        "comments": "",
        "commit_message": "Fixed the issue of NaN gradients by adding epsilon in focal loss (#924)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def binary_focal_loss_with_logits(",
            "",
            "probs = torch.sigmoid(input)",
            "target = target.unsqueeze(dim=1)",
            "-    loss_tmp = -alpha * torch.pow((1. - probs), gamma) * target * torch.log(probs + eps) \\",
            "-               - (1 - alpha) * torch.pow(probs, gamma) * (1. - target) * torch.log(1. - probs + eps)",
            "+    loss_tmp = - alpha * torch.pow((1. - probs + eps), gamma) * target * torch.log(probs + eps) \\",
            "+               - (1 - alpha) * torch.pow(probs + eps, gamma) * (1. - target) * torch.log(1. - probs + eps)",
            "+",
            "loss_tmp = loss_tmp.squeeze(dim=1)",
            "",
            "if reduction == 'none':"
        ]
    },
    {
        "number": 7403,
        "comments": "",
        "commit_message": "TF: Fix #2247, explicitly squeeze image to support matplotlib<3.3.0 (#2248)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def predict_seq2seq(net, src_sentence, src_vocab, tgt_vocab, num_steps,",
            "if pred == tgt_vocab['<eos>']:",
            "break",
            "output_seq.append(pred)",
            "-    return ' '.join(tgt_vocab.to_tokens(output_seq)), attention_weight_seq# Alias defined in config.ini",
            "+    return ' '.join(tgt_vocab.to_tokens(output_seq)), attention_weight_seq",
            "+",
            "+",
            "+# Alias defined in config.ini",
            "nn_Module = nn.Module",
            "",
            "ones_like = torch.ones_like"
        ]
    },
    {
        "number": 7405,
        "comments": "",
        "commit_message": "Scale Factor Fix (#2039)\n\n* fix (scale_factor): use new_tensor(scale_factor) in case it is numpy array\n\n* reformat (models): reformat with flake8, yapf, and isort to pass CI\n\n* fix (bbox_mapping): fix scale_factor bug in bbox_mapping\n\n* fix img_meta bug\n\nCo-authored-by: beansi <zhangwenwei@sensetime.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class CascadeRCNN(BaseDetector, RPNTestMixin):",
            "scale_factor if rescale else det_bboxes)",
            "else:",
            "_bboxes = (",
            "-                        det_bboxes[:, :4] *",
            "-                        torch.from_numpy(scale_factor).to(det_bboxes.device)",
            "+                        det_bboxes[:, :4] * det_bboxes.new_tensor(scale_factor)",
            "if rescale else det_bboxes)",
            "",
            "mask_rois = bbox2roi([_bboxes])"
        ]
    },
    {
        "number": 7406,
        "comments": "",
        "commit_message": "fix the optimizer iteration increment logic when gradient accumulation is enabled (#3631)\n\nSigned-off-by: Vignesh Kothapalli <k.vignesh1420@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class LocalGradientAggregationHelper:",
            "# If optimizer tracks iterations, we increment it on steps where we",
            "# are not going to call `apply_gradients()`.",
            "def increment_optimizer_iteration():",
            "-            if hasattr(optimizer, \"_iterations\") and optimizer._iterations is not None:",
            "-                return optimizer._iterations.assign_add(1).op",
            "+            # (kvignesh1420): Since all `tf.OptimizerV2` instances have the `iterations`",
            "+            # property for modifying the underlying `optimizer._iterations`, it is safe to use",
            "+            # the property instead of the private variable. For instance, the keras",
            "+            # `LossScaleOptimizer` inherits `tf.Optimizer` and exposes the cleaner `iterations`",
            "+            # property instead of the unsafe `_iterations`.",
            "+",
            "+            if hasattr(optimizer, \"iterations\") and optimizer.iterations is not None:",
            "+                return optimizer.iterations.assign_add(1).op",
            "return tf.no_op()",
            "",
            "with tf.control_dependencies([tf.group(*get_not_none_from_list(flattended_args0))]):"
        ]
    },
    {
        "number": 7407,
        "comments": "",
        "commit_message": "fix variable_scope for older TF\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def bidirectional_rnn(incoming, rnncell_fw, rnncell_bw, return_seq=False,",
            "",
            "input_shape = utils.get_incoming_shape(incoming)",
            "",
            "-    with tf.variable_scope(scope, name, [incoming]) as scope:",
            "+    with tf.variable_scope(scope, name, values=[incoming]) as scope:",
            "name = scope.name",
            "",
            "# TODO: DropoutWrapper"
        ]
    },
    {
        "number": 7411,
        "comments": "",
        "commit_message": "style: code style fixes (#1046)\n\n* fix: automatic code style correction\n\n* fix: squad_iterator\n\n* fix: delete document_bert_ner_iterator\n\n* fix: revert json files to dev version\n\n* fix: removed from registry\n\n* refactor: fix merge mistakes\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class BertClassifierModel(LRScheduledTFModel):",
            "self.token_types_ph = tf.placeholder(shape=(None, None), dtype=tf.int32, name='token_types_ph')",
            "",
            "if not self.one_hot_labels:",
            "-            self.y_ph = tf.placeholder(shape=(None, ), dtype=tf.int32, name='y_ph')",
            "+            self.y_ph = tf.placeholder(shape=(None,), dtype=tf.int32, name='y_ph')",
            "else:",
            "self.y_ph = tf.placeholder(shape=(None, self.n_classes), dtype=tf.float32, name='y_ph')"
        ]
    },
    {
        "number": 7415,
        "comments": "",
        "commit_message": "several bug fix and improvement\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def FixedUnPooling(x, shape, unpool_mat=None):",
            "shape = shape2d(shape)",
            "",
            "# a faster implementation for this special case",
            "-    if shape[0] == 2 and  shape[1] == 2 and unpool_mat is None:",
            "+    if shape[0] == 2 and shape[1] == 2 and unpool_mat is None:",
            "return UnPooling2x2ZeroFilled(x)",
            "",
            "input_shape = tf.shape(x)",
            "if unpool_mat is None:",
            "mat = np.zeros(shape, dtype='float32')",
            "mat[0][0] = 1",
            "-        unpool_mat = tf.Variable(mat, trainable=False, name='unpool_mat')",
            "+        unpool_mat = tf.constant(mat, name='unpool_mat')",
            "elif isinstance(unpool_mat, np.ndarray):",
            "-        unpool_mat = tf.Variable(unpool_mat, trainable=False, name='unpool_mat')",
            "+        unpool_mat = tf.constant(unpool_mat, name='unpool_mat')",
            "assert unpool_mat.get_shape().as_list() == list(shape)",
            "",
            "# perform a tensor-matrix kronecker product"
        ]
    },
    {
        "number": 7416,
        "comments": "",
        "commit_message": "Fixed typo. Fixed action_shape for DQN\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class DeepQNetwork(ValueFunction):",
            "",
            "with tf.name_scope(\"update\"):",
            "self.q_targets = tf.placeholder(tf.float32, [None], name='q_targets')",
            "-            self.actions = tf.placeholder(tf.int32, [None, self.action_count], name='actions')",
            "+            self.actions = tf.placeholder(tf.int32, [None], name='actions')",
            "",
            "# Q values for actions taken in batch",
            "print(self.actions)"
        ]
    },
    {
        "number": 7421,
        "comments": "",
        "commit_message": "Fix torch version comparisons (#18460)\n\nComparisons like\nversion.parse(torch.__version__) > version.parse(\"1.6\")\nare True for torch==1.6.0+cu101 or torch==1.6.0+cpu\n\nversion.parse(version.parse(torch.__version__).base_version) are preferred (and available in pytorch_utils.py\n",
        "label": "",
        "answer": "no",
        "change": [
            "def _get_learning_rate(self):",
            "last_lr = (",
            "# backward compatibility for pytorch schedulers",
            "self.lr_scheduler.get_last_lr()[0]",
            "-            if version.parse(torch.__version__) >= version.parse(\"1.4\")",
            "+            if version.parse(version.parse(torch.__version__).base_version) >= version.parse(\"1.4\")",
            "else self.lr_scheduler.get_lr()[0]",
            ")",
            "return last_lr"
        ]
    },
    {
        "number": 7422,
        "comments": "",
        "commit_message": "fix bug in tf-onnx conversion\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def get_outputs_sizes_tf(",
            ") -> List[Tuple[int, ...]]:",
            "outputs = tf_model(*input_tensors)",
            "if isinstance(outputs, tf.Tensor):",
            "-        return [tuple(tf.shape(outputs))]",
            "-    return [tuple(x.size()) for x in outputs]",
            "+        return [tuple(outputs.shape)]",
            "+    return [tuple(x.shape) for x in outputs]",
            "",
            "",
            "def create_model_inputs_tf("
        ]
    },
    {
        "number": 7425,
        "comments": "",
        "commit_message": "Replaced our queue with tensorflow queue and corrected quantization_channels key error.\n\nFix accidental restore comment.\n\nRemove unnecessary print.\n\nAdd newline to wavenet_ops.py.\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def main():",
            "next_sample,",
            "feed_dict={samples: window})",
            "",
            "-        sample = np.random.choice(np.arange(quantization_steps), p=prediction)",
            "+        sample = np.random.choice(np.arange(quantization_channels), p=prediction)",
            "waveform.append(sample)",
            "print('Sample {:3<d}/{:3<d}: {}'",
            ".format(step + 1, args.samples, sample))"
        ]
    },
    {
        "number": 7428,
        "comments": "",
        "commit_message": "Fix half of the tests that are failing in v1.\n\nMost of them are failing since the actual code are expected to run only in v2 (eg need eager/resource variable, or certain fix we added is only applied to the v2 code path).\n\nPiperOrigin-RevId: 394765626\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def make_preprocessing_model(file_dir):",
            "",
            "def make_training_model():",
            "\"\"\"Make a trainable model for the preprocessed inputs.\"\"\"",
            "-  float_in = tf.keras.Input(shape=(1,), dtype=\"float64\", name=\"float_col\")",
            "+  float_in = tf.keras.Input(shape=(1,), dtype=\"float32\", name=\"float_col\")",
            "# After preprocessing, both the string and int column are integer ready for",
            "# embedding.",
            "int_in = tf.keras.Input(shape=(1,), dtype=\"int64\", name=\"int_col\")"
        ]
    },
    {
        "number": 7429,
        "comments": "",
        "commit_message": "fix: enable binary dense encoder (#2957)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "-import torch",
            "-",
            "-from ludwig.encoders.binary_encoders import BinaryPassthroughEncoder",
            "-",
            "-",
            "-def test_binary_passthrough_encoder():",
            "-    binary_encoder = BinaryPassthroughEncoder()",
            "-    inputs = torch.rand(2, 1)",
            "-    outputs = binary_encoder(inputs)",
            "-    assert outputs.shape[1:] == binary_encoder.output_shape"
        ]
    },
    {
        "number": 7430,
        "comments": "",
        "commit_message": "RANSAC improvements (#1435)\n\n- Added LU solver for RANSAC-homography, it speeds-up the minimal-solver part x10\n- Now find_fundamental weights is optional, as it was for homography\n- Fix test precision and made one xFail test to be compulsory\n- Fixed tests for fundamental matrix, as one have to have >= 8 points (also added to docstring)\n- updates deprecated torch.svd to torch.linalg.svd\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def triangulate_points(",
            "# 1. Solve the system Ax=0 with smallest eigenvalue",
            "# 2. Return homogeneous coordinates",
            "",
            "-    _, _, V = torch.svd(X)",
            "+    _, _, V = _torch_svd_cast(X)",
            "",
            "points3d_h = V[..., -1]",
            "points3d: torch.Tensor = convert_points_from_homogeneous(points3d_h)"
        ]
    },
    {
        "number": 7431,
        "comments": "",
        "commit_message": "Update Makefile with target field\n\nSummary:\n- update makefile with target for export to aggregate all export configs into one file\n- fix the bug that keeps flags set at fp16 precision and cuda enable after gpu-fp16 export\n\nReviewed By: mikekgfb\n\nDifferential Revision: D27278841\n\nfbshipit-source-id: d27794cb73f5ea3fbb9bfbedfdb671e0bb80770b\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def export_saved_model_to_caffe2(",
            "def export_saved_model_to_torchscript(",
            "saved_model_path: str, path: str, export_config: ExportConfig",
            ") -> None:",
            "+    cuda.CUDA_ENABLED = False",
            "+    precision.FP16_ENABLED = False",
            "task, train_config, _training_state = load(saved_model_path)",
            "task.torchscript_export(task.model, path, False, 1, export_config=export_config)"
        ]
    },
    {
        "number": 7432,
        "comments": "",
        "commit_message": "l2 reg fix\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestDQFDAgent(unittest.TestCase):",
            "tf.reset_default_graph()",
            "",
            "# DQFD uses l2-reg",
            "-        network_builder = layered_network_builder(layers_config=[{'type': 'dense', 'size': 32,",
            "-                                                                  'weights_regularizer': 'tensorflow.contrib.layers.python.layers.regularizers.l2_regularizer',",
            "-                                                                  'weights_regularizer_kwargs': {",
            "-                                                                      'scale': 0.001",
            "-                                                                  }",
            "+        network_builder = layered_network_builder(layers_config=[{'type': 'dense',",
            "+                                                                  'size': 32,",
            "+                                                                  'l2_regularization': 0.001",
            "}])",
            "",
            "agent = DQFDAgent(config=config, network_builder=network_builder)"
        ]
    },
    {
        "number": 7433,
        "comments": "",
        "commit_message": "Fix two warnings related to deprecated function names.\n\nPiperOrigin-RevId: 284760914\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def interpolate(x,",
            "args[3], args[4], validate_args)",
            "",
            "with tf.control_dependencies(",
            "-        [tf.assert_equal(tf.shape(x_data), tf.shape(y_data))]):",
            "+        [tf.compat.v1.assert_equal(tf.shape(x_data), tf.shape(y_data))]):",
            "# Call 1D linear interpolation function for each batch separately.",
            "return tf.reshape(",
            "tf.map_fn("
        ]
    },
    {
        "number": 7437,
        "comments": "",
        "commit_message": "Fixes fp16 inference for speech recognition example (#2438)\n\nSummary:\nCurrently the speech recognition fails for fp16 option as the input is\nnot converted to half. This fixes this issue.\n\n# Before submitting\n\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\n- [x] Did you make sure to update the docs?\n- [ ] Did you write any new necessary tests?\n\n## What does this PR do?\nFixes fp16 inference for speech recognition example.\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding \ufffd\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2438\n\nReviewed By: apoorv2904, xuqiantong\n\nDifferential Revision: D31594529\n\nPulled By: alexeib\n\nfbshipit-source-id: 75d1fe3f00b2e86381f28f79daea2cd516233e22\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def main(args, task=None, model_state=None):",
            "wps_meter = TimeMeter()",
            "for sample in t:",
            "sample = utils.move_to_cuda(sample) if use_cuda else sample",
            "+            if use_fp16:",
            "+                sample = utils.apply_to_sample(apply_half, sample)",
            "if \"net_input\" not in sample:",
            "continue"
        ]
    },
    {
        "number": 7439,
        "comments": "",
        "commit_message": "fix bugs in WGAN, WGAN-GP, DRAGAN\n\noutput of discriminator is changed into linear output (logit value) from\nsigmoid output\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class WGAN(object):",
            "D_fake, D_fake_logits, _ = self.discriminator(G, is_training=True, reuse=True)",
            "",
            "# get loss for discriminator",
            "-        d_loss_real = - tf.reduce_mean(D_real)",
            "-        d_loss_fake = tf.reduce_mean(D_fake)",
            "+        d_loss_real = - tf.reduce_mean(D_real_logits)",
            "+        d_loss_fake = tf.reduce_mean(D_fake_logits)",
            "",
            "self.d_loss = d_loss_real + d_loss_fake"
        ]
    },
    {
        "number": 7440,
        "comments": "",
        "commit_message": "TF Examples Rewrite (#18451)\n\n* Finished QA example\n\n* Dodge a merge conflict\n\n* Update text classification and LM examples\n\n* Update NER example\n\n* New Keras metrics WIP, fix NER example\n\n* Update NER example\n\n* Update MC, summarization and translation examples\n\n* Add XLA warnings when shapes are variable\n\n* Make sure batch_size is consistently scaled by num_replicas\n\n* Add PushToHubCallback to all models\n\n* Add docs links for KerasMetricCallback\n\n* Add docs links for prepare_tf_dataset and jit_compile\n\n* Correct inferred model names\n\n* Don't assume the dataset has 'lang'\n\n* Don't assume the dataset has 'lang'\n\n* Write metrics in text classification\n\n* Add 'framework' to TrainingArguments and TFTrainingArguments\n\n* Export metrics in all examples and add tests\n\n* Fix training args for Flax\n\n* Update command line args for translation test\n\n* make fixup\n\n* Fix accidentally running other tests in fp16\n\n* Remove do_train/do_eval from run_clm.py\n\n* Remove do_train/do_eval from run_mlm.py\n\n* Add tensorflow tests to circleci\n\n* Fix circleci\n\n* Update examples/tensorflow/language-modeling/run_mlm.py\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>\n\n* Update examples/tensorflow/test_tensorflow_examples.py\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>\n\n* Update examples/tensorflow/translation/run_translation.py\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>\n\n* Update examples/tensorflow/token-classification/run_ner.py\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>\n\n* Fix save path for tests\n\n* Fix some model card kwargs\n\n* Explain the magical -1000\n\n* Actually enable tests this time\n\n* Skip text classification PR until we fix shape inference\n\n* make fixup\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TFTrainingArguments(TrainingArguments):",
            "def _setup_strategy(self) -> Tuple[\"tf.distribute.Strategy\", int]:",
            "logger.info(\"Tensorflow: setting up strategy\")",
            "",
            "-        if self.xla:",
            "-            tf.config.optimizer.set_jit(True)",
            "-",
            "gpus = tf.config.list_physical_devices(\"GPU\")",
            "",
            "# Set to float16 at first"
        ]
    },
    {
        "number": 7443,
        "comments": "",
        "commit_message": "Fix torch 1.8.0 segmentation fault (#10546)\n\n* Only run one test\n\n* Patch segfault\n\n* Fix summarization pipeline\n\n* Ready for merge\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class SimpleSummarizationPipelineTests(unittest.TestCase):",
            "# Bias output towards L",
            "V, C = model.lm_head.weight.shape",
            "",
            "-        bias = torch.zeros(V, requires_grad=True)",
            "+        bias = torch.zeros(V)",
            "bias[76] = 10",
            "",
            "model.lm_head.bias = torch.nn.Parameter(bias)"
        ]
    },
    {
        "number": 7446,
        "comments": "",
        "commit_message": "Fix unused variable found (#1250)\n\n* Fix Unused variable found\n\n* more fixes\n\n* Update kornia/enhance/histogram.py\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestRemap:",
            "grid = kornia.utils.create_meshgrid(height, width, normalized_coordinates=False, device=device).to(dtype)",
            "grid += 1.0  # apply some shift",
            "input_tuple = (img, grid[..., 0], grid[..., 1])",
            "-        op_traced = torch.jit.trace(op_script, input_tuple)",
            "+        _ = torch.jit.trace(op_script, input_tuple)",
            "",
            "# 2. Generate different input",
            "batch_size, channels, height, width = 2, 2, 2, 5"
        ]
    },
    {
        "number": 7447,
        "comments": "",
        "commit_message": "Data loader bug fix and Attention bug fix\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class AttentionRNN(nn.Module):",
            "def __init__(self, out_dim, annot_dim, memory_dim,",
            "score_mask_value=-float(\"inf\")):",
            "super(AttentionRNN, self).__init__()",
            "-        self.rnn_cell = nn.GRUCell(annot_dim + memory_dim, out_dim)",
            "+        self.rnn_cell = nn.GRUCell(out_dim + memory_dim, out_dim)",
            "self.alignment_model = BahdanauAttention(annot_dim, out_dim, out_dim)",
            "self.score_mask_value = score_mask_value"
        ]
    },
    {
        "number": 7448,
        "comments": "",
        "commit_message": "upgrade to pytorch 0.4.0 (#1126)\n\n* bump pytorch to 0.4 + fix sanitize\n\n* remove check for Variable in block_orthogonal\n\n* rename parameter split_size=\n\n* remove checks for Variable\n\n* fix more tests\n\n* fixes\n\n* get tests to pass\n\n* fix warnings\n\n* get rid of some of the Variables\n\n* more tests passing\n\n* more elimination of variables\n\n* finish removing all Variables\n\n* pylint and such\n\n* a few fixes\n\n* move torch.no_grad into model.forward_on_instances\n\n* more pytorch 0.4 changes\n\n* detach() -> data\n\n* pylint\n\n* fix bad tensor creation\n\n* fix types\n\n* remove print statement\n\n* more 0.4 goodness\n\n* factor out is_tensor\n\n* add no_grad to elmo command\n\n* cleanup\n\n* remove TODO\n\n* address PR feedback\n\n* replace all() with item()\n\n* more cleanup\n\n* further cleanup\n\n* remove Variable\n\n* really fix merge conflict\n\n* fix pylint\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class PytorchSeq2VecWrapper(Seq2VecEncoder):",
            "# batch size is the second dimension here, because pytorch",
            "# returns RNN state as a tensor of shape (num_layers * num_directions,",
            "# batch_size, hidden_size)",
            "-            zeros = state.data.new(num_layers_times_directions,",
            "-                                   batch_size - num_valid,",
            "-                                   encoding_dim).fill_(0)",
            "-            zeros = Variable(zeros)",
            "+            zeros = state.new_zeros(num_layers_times_directions,",
            "+                                    batch_size - num_valid,",
            "+                                    encoding_dim)",
            "state = torch.cat([state, zeros], 1)",
            "",
            "# Restore the original indices and return the final state of the"
        ]
    },
    {
        "number": 7450,
        "comments": "",
        "commit_message": "docstring fixes and some backend fixes\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def cross(",
            "promote_type = torch.promote_types(x1.dtype, x2.dtype)",
            "x1 = x1.type(promote_type)",
            "x2 = x2.type(promote_type)",
            "+    print(torch.__version__)",
            "return torch.cross(input=x1, other=x2, dim=axis, out=out)"
        ]
    },
    {
        "number": 7451,
        "comments": "",
        "commit_message": "fix doc\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def to_hetero(module: Module, metadata: Metadata, aggr: str = \"sum\",",
            "import torch",
            "from torch_geometric.nn import SAGEConv, to_hetero",
            "",
            "-        Net(torch.nn.Module):",
            "+        class GNN(torch.nn.Module):",
            "def __init__(self):",
            "-                self.conv1 = SAGEConv(-1, 16)",
            "-                self.conv2 = SAGEConv(16, 16)",
            "+                self.conv1 = SAGEConv((-1, -1), 32)",
            "+                self.conv2 = SAGEConv((32, 32), 32)",
            "",
            "def forward(self, x, edge_index):",
            "x = self.conv1(x, edge_index).relu()",
            "x = self.conv2(x, edge_index).relu()",
            "return x",
            "",
            "-        model = Net()",
            "+        model = GNN()",
            "",
            "node_types = ['paper', 'author']",
            "edge_types = ["
        ]
    },
    {
        "number": 7456,
        "comments": "",
        "commit_message": "Fix indexing\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def get_batch_statistics(outputs, targets, iou_threshold):",
            "continue",
            "",
            "# Filter target_boxes by pred_label so that we only match against boxes of our own label",
            "-                filtered_target_position, filtered_targets = zip(*filter(lambda x: target_labels[x] == pred_label, enumerate(target_boxes)))",
            "-",
            "+                filtered_target_position, filtered_targets = zip(*filter(lambda x: target_labels[x[0]] == pred_label, enumerate(target_boxes)))",
            "+",
            "# Find the best matching target for our predicted box",
            "-                iou, box_filtered_index = bbox_iou(pred_box.unsqueeze(0), filtered_targets).max(0)",
            "-",
            "+                iou, box_filtered_index = bbox_iou(pred_box.unsqueeze(0), torch.stack(filtered_targets)).max(0)",
            "+",
            "# Remap the index in the list of filtered targets for that label to the index in the list with all targets.",
            "box_index = filtered_target_position[box_filtered_index]"
        ]
    },
    {
        "number": 7457,
        "comments": "",
        "commit_message": "Fix `pyg-lib` `hetero_neighbor_sampler` usage (#5697)\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class NeighborSampler(BaseSampler):",
            "# TODO (matthias) Add `disjoint` option to `NeighborSampler`",
            "# TODO (matthias) `return_edge_id` if edge features present",
            "disjoint = self.node_time_dict is not None",
            "-                out = torch.ops.pyg.hetero_neighbor_sample_cpu(",
            "+                out = torch.ops.pyg.hetero_neighbor_sample(",
            "self.node_types,",
            "self.edge_types,",
            "self.colptr_dict,"
        ]
    },
    {
        "number": 7463,
        "comments": "",
        "commit_message": "small fixes for various gradient tests.\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_stop_gradient(x_raw, dtype, tensor_fn, device, call):",
            "# Tf graph mode cannot create variables as part of the computation graph",
            "assert np.array_equal(",
            "call(ivy.stop_gradient, x),",
            "-            ivy.functional.backends.numpy.array(x_raw, dtype),",
            "+            np.array(x_raw, dtype=dtype),",
            ")",
            "# compilation test",
            "if call in [helpers.torch_call]:"
        ]
    },
    {
        "number": 7465,
        "comments": "",
        "commit_message": "linter fixes\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def run_model_torch(model, inputs, CONFIG, truncated, speaker_id=None, style_mel",
            "decoder_output, postnet_output, alignments, stop_tokens = model.inference(",
            "inputs, speaker_ids=speaker_id, speaker_embeddings=speaker_embeddings)",
            "elif 'glow' in CONFIG.model.lower():",
            "-        inputs_lengths = torch.tensor(inputs.shape[1:2]).to(inputs.device)",
            "+        inputs_lengths = torch.tensor(inputs.shape[1:2]).to(inputs.device)  # pylint: disable=not-callable",
            "postnet_output, _, _, _, alignments, _, _ = model.inference(inputs, inputs_lengths)",
            "postnet_output = postnet_output.permute(0, 2, 1)",
            "# these only belong to tacotron models."
        ]
    },
    {
        "number": 7466,
        "comments": "",
        "commit_message": "fix\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class FrameScoreFeats(AbsFeatsExtract):",
            "pad = self.win_length // 2",
            "input_lengths = input_lengths + 2 * pad",
            "",
            "-            olens = (",
            "-                torch.div(",
            "-                    (input_lengths - self.win_length),",
            "-                    self.hop_length,",
            "-                    rounding_mode=\"floor\",",
            "-                )",
            "-                + 1",
            "-            )",
            "+            olens = torch.div((input_lengths - self.win_length), self.hop_length) + 1",
            "output.masked_fill_(make_pad_mask(olens, output, 1), 0.0)",
            "else:",
            "olens = None"
        ]
    },
    {
        "number": 7471,
        "comments": "",
        "commit_message": "`feature_visualization()` CUDA fix (#3925)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def feature_visualization(x, module_type, stage, n=64, save_dir=Path('runs/detec",
            "f = f\"stage{stage}_{module_type.split('.')[-1]}_features.png\"  # filename",
            "",
            "plt.figure(tight_layout=True)",
            "-            blocks = torch.chunk(x[0], channels, dim=0)  # select batch index 0, block by channels",
            "+            blocks = torch.chunk(x[0].cpu(), channels, dim=0)  # select batch index 0, block by channels",
            "n = min(n, channels)  # number of plots",
            "ax = plt.subplots(math.ceil(n / 8), 8, tight_layout=True)[1].ravel()  # 8 rows x n/8 cols",
            "for i in range(n):"
        ]
    },
    {
        "number": 7472,
        "comments": "",
        "commit_message": "Gelu10 (#15676)\n\n* Add GeLU10 (clipped version of GeLU) to transformers to improve quantization performances.\n\n* Add unittests.\n\n* Import tensorflow after `is_tf_available` check.\n\n* Fix tensorflow wrong function `tf.tensor` to `tf.constant`\n\n* style.\n\n* use `tf.math.max`\n\n* Fix tf tests.\n\n* style.\n\n* style style style style style style\n\n* style style style style style style\n\n* Address @sgugger comments.\n\n* Fix wrong operator for raising ValueError for ClippedGELUActivation.\n",
        "label": "",
        "answer": "no",
        "change": [
            "ACT2FN = {",
            "\"gelu_new\": NewGELUActivation(),",
            "\"gelu_fast\": FastGELUActivation(),",
            "\"quick_gelu\": QuickGELUActivation(),",
            "+    \"gelu_10\": ClippedGELUActivation(-10, 10),",
            "\"mish\": MishActivation(),",
            "\"linear\": LinearActivation(),",
            "\"sigmoid\": nn.Sigmoid(),"
        ]
    },
    {
        "number": 7475,
        "comments": "",
        "commit_message": "Fix tests for bleu score\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def accumulate_strings(values, name=\"strings\"):",
            "initial_value=[],",
            "dtype=tf.string,",
            "trainable=False,",
            "-      collections=[tf.GraphKeys.LOCAL_VARIABLES],",
            "+      collections=[],",
            "validate_shape=True)",
            "value_tensor = tf.identity(strings)",
            "update_op = tf.assign("
        ]
    },
    {
        "number": 7480,
        "comments": "",
        "commit_message": "fix lint errors\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def transform_points(dst_homo_src, points_src):",
            "# to homogeneous",
            "points_src_h = convert_points_to_homogeneous(points_src)  # BxNx3",
            "# transform coordinates",
            "-    points_dst_h = torch.matmul(dst_homo_src, points_src_h.transpose(1, 2))  # Bx3xN",
            "+    points_dst_h = torch.matmul(dst_homo_src, points_src_h.transpose(1, 2))",
            "points_dst_h = points_dst_h.permute(0, 2, 1)  # BxNx3",
            "# to euclidean",
            "points_dst = convert_points_from_homogeneous(points_dst_h)  # BxNx2"
        ]
    },
    {
        "number": 7483,
        "comments": "",
        "commit_message": "Fixes for torch 1.14 due to new torch.numel return type (#2522)\n\n* fixes for new torch.numel return type\n\n* address comment\n",
        "label": "",
        "answer": "no",
        "change": [
            "class NcclBackend(object):",
            "buffer_m = torch.cat([buffer_m, empty_tensor])",
            "",
            "buffer_m.add_(worker_error)",
            "-        worker_scale = torch.norm(buffer_m) / np.sqrt(torch.numel(buffer_m))",
            "+        worker_scale = torch.norm(buffer_m) / np.sqrt(buffer_m.numel())",
            "worker_error.set_(buffer_m - worker_scale *",
            "buffer_m.sign().add_(1).bool().float().add_(-0.5).mul_(2.0))"
        ]
    },
    {
        "number": 7493,
        "comments": "",
        "commit_message": "Fix test fails and warnings\n\nAttention output was in bnij ordering instead of ijbn which everything\nelse will expect. This was an oversight on my part, and keeps the\nattention inputs/outputs identical to the original code.\n\nAlso moved back from tensor slicing to index_select in rel_shift_bnij to\nmake the tracer happy.\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class XLNetRelativeAttention(nn.Module):",
            "attn_vec = torch.einsum('bnij,jbnd->ibnd', attn_prob, v_head_h)",
            "",
            "if self.output_attentions:",
            "-            return attn_vec, attn_prob",
            "+            return attn_vec, torch.einsum('bnij->ijbn', attn_prob)",
            "",
            "return attn_vec"
        ]
    },
    {
        "number": 7495,
        "comments": "",
        "commit_message": "Fix ViT in21k representation (pre_logits) layer handling across old and new npz checkpoints\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def _load_weights(model: nn.Module, checkpoint_path: str, prefix: str = 'resnet/",
            "model.stem.conv.weight.copy_(stem_conv_w)",
            "model.norm.weight.copy_(t2p(weights[f'{prefix}group_norm/gamma']))",
            "model.norm.bias.copy_(t2p(weights[f'{prefix}group_norm/beta']))",
            "-    if isinstance(model.head.fc, nn.Conv2d) and \\",
            "+    if isinstance(getattr(model.head, 'fc', None), nn.Conv2d) and \\",
            "model.head.fc.weight.shape[0] == weights[f'{prefix}head/conv2d/kernel'].shape[-1]:",
            "model.head.fc.weight.copy_(t2p(weights[f'{prefix}head/conv2d/kernel']))",
            "model.head.fc.bias.copy_(t2p(weights[f'{prefix}head/conv2d/bias']))"
        ]
    },
    {
        "number": 7497,
        "comments": "",
        "commit_message": "Minor fix for `pyg-lib` usage in `HeteroLinear` and `RGCNConv` (#5510)\n\npyg_lib pathway is chosen if cuda is available and pyg_lib is available.\nbut if the input is not cuda we should not be using the pyg_lib pathway.\nsimple fix\n\nwithout this a ton of CI fails for testing rgcnconv and heterolinear w/\ncpu inputs\n\n\n[errors.txt](https://github.com/pyg-team/pytorch_geometric/files/9629902/errors.txt)\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class HeteroLinear(torch.nn.Module):",
            "self.is_sorted = is_sorted",
            "self.kwargs = kwargs",
            "",
            "-        self._WITH_PYG_LIB = torch.cuda.is_available() and _WITH_PYG_LIB",
            "+        self._WITH_PYG_LIB = _WITH_PYG_LIB",
            "",
            "if self._WITH_PYG_LIB:",
            "self.weight = torch.nn.Parameter("
        ]
    },
    {
        "number": 7499,
        "comments": "",
        "commit_message": "hyperpose compatible:\n(1)maxpool and batchnorm dataformat debuged,support \"channels_first\"\n(2)vgg forward fixed\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class VGG(Model):",
            "",
            "inputs = inputs * 255 - np.array([123.68, 116.779, 103.939], dtype=np.float32).reshape([1, 1, 1, 3])",
            "",
            "-        out = self.layers(inputs)",
            "+        out = self.layers.forward(inputs)",
            "return out"
        ]
    },
    {
        "number": 7504,
        "comments": "",
        "commit_message": "Use f-strings in the dataset scripts (#3291)\n\n* Finishes #3257\n\nUsed f-strings to format the .py files in the dataset folder\n\n* Fix style\n\n* Fix hkcancor dataset\n\nCo-authored-by: Mario \u0160a\u0161ko <mario@huggingface.co>\nCo-authored-by: mariosasko <mariosasko777@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TeluguBooks(datasets.GeneratorBasedBuilder):",
            "path_to_manual_file = os.path.abspath(os.path.expanduser(dl_manager.manual_dir))",
            "if not os.path.exists(path_to_manual_file):",
            "raise FileNotFoundError(",
            "-                \"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('telugu_books', data_dir=...)` that includes file name {}. Manual download instructions: {}\".format(",
            "-                    path_to_manual_file,",
            "-                    _FILENAME,",
            "-                    self.manual_download_instructions,",
            "-                )",
            "+                f\"{path_to_manual_file} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('telugu_books', data_dir=...)` that includes file name {_FILENAME}. Manual download instructions: {self.manual_download_instructions}\"",
            ")",
            "return [",
            "datasets.SplitGenerator("
        ]
    },
    {
        "number": 7511,
        "comments": "",
        "commit_message": "Update to PyTorch 1.9.0 (#2887)\n\n* Update to PyTorch 1.9.0\n\n* Update torch.cholesky, torch.symeig\n\n* Fix torch.tensordot, torch.qr, funsor dependency\n\n* Fix torch import, AutoGuideList usage\n\n* Ignore bug in PyTorch jit\n\n* Ignore tracer warnings\n\n* Replace torch.solve with torch.linalg.solve\n\n* Xfail vectorized markov funsor tests\n\n* Update funsor version\n\n* Switch MNIST mirrors\n\n* Pin pillow version\n\n* Bump torchvision version\n",
        "label": "",
        "answer": "no",
        "change": [
            "class GammaGaussian:",
            "Integrates out all latent state (i.e. operating on event dimensions) of Gaussian component.",
            "\"\"\"",
            "n = self.dim()",
            "-        chol_P = self.precision.cholesky()",
            "+        chol_P = torch.linalg.cholesky(self.precision)",
            "chol_P_u = self.info_vec.unsqueeze(-1).triangular_solve(chol_P, upper=False).solution.squeeze(-1)",
            "u_P_u = chol_P_u.pow(2).sum(-1)",
            "# considering GammaGaussian as a Gaussian with precision = s * precision, info_vec = s * info_vec,"
        ]
    },
    {
        "number": 7513,
        "comments": "",
        "commit_message": "fix #2285 (#2286)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class MultiLabelField(Field[torch.Tensor]):",
            "def as_tensor(self, padding_lengths: Dict[str, int]) -> torch.Tensor:",
            "# pylint: disable=unused-argument",
            "",
            "-        tensor = torch.zeros(self._num_labels)  # vector of zeros",
            "+        tensor = torch.zeros(self._num_labels, dtype=torch.long)  # vector of zeros",
            "if self._label_ids:",
            "tensor.scatter_(0, torch.LongTensor(self._label_ids), 1)"
        ]
    },
    {
        "number": 7514,
        "comments": "",
        "commit_message": "Fix RepPoints TTA (#3435)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class RepPointsDetector(SingleStageDetector):",
            "for x, img_meta in zip(feats, img_metas):",
            "# only one image in the batch",
            "outs = self.bbox_head(x)",
            "-            bbox_inputs = outs + (img_metas, self.test_cfg, False, False)",
            "+            bbox_inputs = outs + (img_meta, self.test_cfg, False, False)",
            "det_bboxes, det_scores = self.bbox_head.get_bboxes(*bbox_inputs)[0]",
            "aug_bboxes.append(det_bboxes)",
            "aug_scores.append(det_scores)"
        ]
    },
    {
        "number": 7517,
        "comments": "",
        "commit_message": "upgrade to pytorch 1.2 (#3182)\n\n* first attempt at pytorch 1.2\n\n* explicit is better than implicit\n\n* more explicit\n\n* attempt to fix flaky tests\n\n* pylint\n\n* no disable dropout\n\n* disable dropout by default\n\n* restore dropout, don't deepcopy\n\n* change batch size for biaffine_dependency_parser_multilang_test, maybe that will make it pass? :(\n\n* try batch size 10\n\n* ignore bad gradient parameter\n\n* cleanup\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class FBetaMeasure(Metric):",
            "",
            "if mask is None:",
            "mask = torch.ones_like(gold_labels)",
            "-        mask = mask.to(torch.uint8)",
            "+        mask = mask.to(dtype=torch.bool)",
            "gold_labels = gold_labels.float()",
            "",
            "argmax_predictions = predictions.max(dim=-1)[1].float()"
        ]
    },
    {
        "number": 7518,
        "comments": "",
        "commit_message": "fix init method in order to load models with previous SequenceTagger\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class ViterbiLoss(torch.nn.Module):",
            "targets = torch.tensor(formatted_targets, dtype=torch.long).unsqueeze(2).to(flair.device)",
            "",
            "# Squeeze crf scores matrices in 1-dim shape and gather scores at targets by matrix indices",
            "-        scores_at_targets = torch.gather(features.view(batch_size, seq_len, -1), 2, targets).squeeze(0).squeeze(0)",
            "+        scores_at_targets = torch.gather(features.view(batch_size, seq_len, -1), 2, targets)",
            "scores_at_targets = pack_padded_sequence(scores_at_targets, lengths.values, batch_first=True)[0]",
            "gold_score = scores_at_targets.sum()"
        ]
    },
    {
        "number": 7519,
        "comments": "",
        "commit_message": "Fix keep_dims/dims parameter deprecation of new vension of TF\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class CapsNet(object):",
            "else:",
            "# self.masked_v = tf.matmul(tf.squeeze(self.caps2), tf.reshape(self.Y, (-1, 10, 1)), transpose_a=True)",
            "self.masked_v = tf.multiply(tf.squeeze(self.caps2), tf.reshape(self.Y, (-1, 10, 1)))",
            "-                self.v_length = tf.sqrt(tf.reduce_sum(tf.square(self.caps2), axis=2, keep_dims=True) + epsilon)",
            "+                self.v_length = tf.sqrt(reduce_sum(tf.square(self.caps2), axis=2, keepdims=True) + epsilon)",
            "",
            "# 2. Reconstructe the MNIST images with 3 FC layers",
            "# [batch_size, 1, 16, 1] => [batch_size, 16] => [batch_size, 512]"
        ]
    },
    {
        "number": 7523,
        "comments": "",
        "commit_message": "Fix docs requirements and cevae link (#2225)\n\n* Fix docs requirements and cevae link\n\n* Install only cpu version of torchvision on rtd\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def setup(app):",
            "",
            "",
            "# @jpchen's hack to get rtd builder to install latest pytorch",
            "+# See similar line in the install section of .travis.yml",
            "if 'READTHEDOCS' in os.environ:",
            "-    os.system('pip install torch==1.3.0+cpu -f https://download.pytorch.org/whl/torch_stable.html')",
            "+    os.system('pip install torch==1.3.0+cpu torchvision==0.4.0+cpu '",
            "+              '-f https://download.pytorch.org/whl/torch_stable.html')"
        ]
    },
    {
        "number": 7527,
        "comments": "",
        "commit_message": "Cleanup model_factory imports, consistent __all__ for models, fixed inception_v4 weight url\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class InceptionV4(nn.Module):",
            "x = self.forward_features(x)",
            "if self.drop_rate > 0:",
            "x = F.dropout(x, p=self.drop_rate, training=self.training)",
            "-        x = self.classif(x)",
            "+        x = self.last_linear(x)",
            "return x"
        ]
    },
    {
        "number": 7528,
        "comments": "",
        "commit_message": "Fix lint errors due to flake8 upgrade\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Uniform(Distribution):",
            "if x.size != a.size():",
            "a = a.expand_as(x)",
            "b = b.expand_as(x)",
            "-        l = x.ge(a).type_as(a)",
            "-        u = x.le(b).type_as(b)",
            "+        lb = x.ge(a).type_as(a)",
            "+        ub = x.le(b).type_as(b)",
            "batch_log_pdf_shape = self.batch_shape(a, b) + (1,)",
            "-        return torch.sum(torch.log(l.mul(u)) - torch.log(b - a), -1).contiguous().view(batch_log_pdf_shape)",
            "+        return torch.sum(torch.log(lb.mul(ub)) - torch.log(b - a), -1).contiguous().view(batch_log_pdf_shape)",
            "",
            "def analytic_mean(self, a=None, b=None):",
            "a, b = self._sanitize_input(a, b)"
        ]
    },
    {
        "number": 7539,
        "comments": "",
        "commit_message": "Add bilinear attention (#1349)\n\n* Add bilinear attention, some code cleanup, make semantic parsers use new attention\n\n* Fix pylint and docs\n\n* Increase beam size for wikitables ERM parser, so test doesn't fail\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class CosineAttention(Attention):",
            "\"\"\"",
            "Computes attention between a vector and a matrix using cosine similarity.",
            "\"\"\"",
            "-",
            "@overrides",
            "-    def _forward_internal(self,",
            "-                          vector: torch.Tensor,",
            "-                          matrix: torch.Tensor,",
            "-                          matrix_mask: torch.Tensor = None) -> torch.Tensor:",
            "+    def _forward_internal(self, vector: torch.Tensor, matrix: torch.Tensor) -> torch.Tensor:",
            "a_norm = vector / (vector.norm(p=2, dim=-1, keepdim=True) + 1e-13)",
            "b_norm = matrix / (matrix.norm(p=2, dim=-1, keepdim=True) + 1e-13)",
            "return torch.bmm(a_norm.unsqueeze(dim=1), b_norm.transpose(-1, -2)).squeeze(1)"
        ]
    },
    {
        "number": 7549,
        "comments": "",
        "commit_message": "fix reshape for depth2normals\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class SpatialGradient(nn.Module):",
            "self.kernel.size(2) // 2,",
            "self.kernel.size(2) // 2]",
            "out_channels: int = 3 if self.order == 2 else 2",
            "-        padded_inp: torch.Tensor = F.pad(input.view(b * c, 1, h, w), spatial_pad, 'replicate')[:, :, None]",
            "+        padded_inp: torch.Tensor = F.pad(input.reshape(b * c, 1, h, w), spatial_pad, 'replicate')[:, :, None]",
            "return F.conv3d(padded_inp, kernel_flip, padding=0).view(b, c, out_channels, h, w)"
        ]
    },
    {
        "number": 7559,
        "comments": "",
        "commit_message": "Fix device allocation\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class SSIMLoss(torch.nn.Module):",
            "",
            "if ssim_loss.item() > 1.0:",
            "print(f\" > SSIM loss is out-of-range {ssim_loss.item()}, setting it 1.0\")",
            "-            ssim_loss = torch.tensor([1.0])",
            "+            ssim_loss = torch.tensor([1.0], device=ssim_loss.device)",
            "",
            "if ssim_loss.item() < 0.0:",
            "print(f\" > SSIM loss is out-of-range {ssim_loss.item()}, setting it 0.0\")",
            "-            ssim_loss = torch.tensor([0.0])",
            "+            ssim_loss = torch.tensor([0.0], device=ssim_loss.device)",
            "",
            "return ssim_loss"
        ]
    },
    {
        "number": 7560,
        "comments": "",
        "commit_message": "Fix Spanish data file URL in wiki_lingua dataset (#3806)\n\n* Fix Spanish data file URL in wiki_lingua dataset\n\n* Update metadata JSON\n\n* Fix URL also in create_dummy.py script\n\n* Change dataset version\n\n* Update metadata JSON\n\n* Update dummy data paths\n\n* Fix Spanish URL in path to dummy data\n",
        "label": "",
        "answer": "no",
        "change": [
            "_URLs = {",
            "class WikiLingua(datasets.GeneratorBasedBuilder):",
            "\"\"\"TODO: Short description of my dataset.\"\"\"",
            "",
            "-    VERSION = datasets.Version(\"1.1.0\")",
            "+    VERSION = datasets.Version(\"1.1.1\")",
            "",
            "# This is an example of a dataset with multiple configurations.",
            "# If you don't want/need to define several sub-sets in your dataset,"
        ]
    },
    {
        "number": 7564,
        "comments": "",
        "commit_message": "Fix the flakiness of oed example (#1645)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class GPBayesOptimizer(pyro.optim.multi.MultiOptimizer):",
            "x_init = self.gpmodel.X.new_empty(1).uniform_(",
            "self.constraints.lower_bound, self.constraints.upper_bound)",
            "x, y = self.find_a_candidate(differentiable, x_init)",
            "+            if torch.isnan(y):",
            "+                continue",
            "candidates.append(x)",
            "values.append(y)"
        ]
    },
    {
        "number": 7567,
        "comments": "",
        "commit_message": "Fix `GraphStore` with empty edge indices (#4968)\n\n* fix graphstore\n\n* changelog:\n",
        "label": "",
        "answer": "no",
        "change": [
            "def edge_tensor_type_to_adj_type(",
            "size=(src.size()[0] + dst.size()[0], ))",
            "return out.view(2, -1)",
            "",
            "-        return torch.stack(tensor_tuple, dim=0)",
            "+        return torch.stack([src, dst], dim=0)",
            "",
            "elif attr.layout == EdgeLayout.CSR:  # CSR: (rowptr, col)",
            "return SparseTensor(rowptr=src, col=dst, is_sorted=True,"
        ]
    },
    {
        "number": 7569,
        "comments": "",
        "commit_message": "FeedfreePredictor and example on ImageNet eval (fix #772)\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def dependency_of_fetches(fetches, op):",
            "\"\"\"",
            "try:",
            "from tensorflow.python.client.session import _FetchHandler as FetchHandler",
            "-        handler = FetchHandler(tf.get_default_graph(), fetches, {})",
            "+        # use the graph of the op, so that this function can be called without being under a default graph",
            "+        handler = FetchHandler(op.graph, fetches, {})",
            "targets = tuple(handler.fetches() + handler.targets())",
            "except ImportError:",
            "if isinstance(fetches, list):"
        ]
    },
    {
        "number": 7572,
        "comments": "",
        "commit_message": "Temporary patch to fix CI build\n\nPiperOrigin-RevId: 439587488\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class ShardedVariableTest(tf.test.TestCase):",
            "self.assertAllClose(got, expect)",
            "self.assertGreater(len(model.variables), len(loaded_model.variables))",
            "",
            "-    with self.assertRaises(ValueError):",
            "-      with self.strategy.scope():",
            "-        keras.models.load_model(saved_dir)",
            "+    with self.strategy.scope():",
            "+      keras.models.load_model(saved_dir)",
            "",
            "def test_slot_variable_checkpointing(self):"
        ]
    },
    {
        "number": 7579,
        "comments": "",
        "commit_message": "Unify Runner classes via BaseRunner parent class (#328)\n\n* - Finished one TODO: n-step cumulated discounted reward (added horizon parameter with default=0 (no horizon) to the method).\n- Completed commenting on model.py and runner.py class(es).\n- Defined all Model variables in __init__ (mostly None) and commented their purpose and functionality.\n- Fixed pep8 compliance in the Runner classes.\n- ThreadedRunner: Renamed `max_timesteps` into `max_episode_timesteps` to match Runner's `run` implementation (all backwards compatible).\n\n* - corrected some comments in model.py regarding parallel RL (threaded and replica)\nand nature of distributed_spec.\n\n* - added returned game_name to `connect` in unreal_engine.py\n\n* Added registry to Environment class (containing only MinimalTest).\nAdded from_spec method to Environment class so that Envs can now be created through this factory.\nChanged a comment in UE4 script.\n\n* - added returned game_name to `connect` in unreal_engine.py\n\n* Added registry to Environment class (containing only MinimalTest).\nAdded from_spec method to Environment class so that Envs can now be created through this factory.\nChanged a comment in UE4 script.\n\n* Added registry to Environment class (containing only MinimalTest).\nAdded from_spec method to Environment class so that Envs can now be created through this factory.\nChanged a comment in UE4 script.\n\n* - added returned game_name to `connect` in unreal_engine.py\n\n* - solved conflicts with TF master branch\n- fixed grayscale image capture for examples/unreal_engine.py\n\n* Fixed bug for distributed_spec != None.\ntf.train.Saver complained about duplicate variables.\n\n* Fixed warnings due to deprecated function parameters.\n\n* Avoid race-condition where order in episode_rewards won't match order in episode_lengths.\n\n* Add possiblity to pass in model saver-frequency by a) episode count, b) seconds, c) timesteps.\n\n* Introduced BaseRunner class.\nRestructured existing Runner classes (Runner and ThreadedRunner) to both inherit from BaseRunner.\nUnified the interfaces used to control these classes and to handle per-episode and per-n-episode reporting.\nAdded multi-threaded testing to random and constant agent tests (configurable for all agents via run_type flag).\n\n* - added returned game_name to `connect` in unreal_engine.py\n\n* Added registry to Environment class (containing only MinimalTest).\nAdded from_spec method to Environment class so that Envs can now be created through this factory.\nChanged a comment in UE4 script.\n\n* - solved conflicts with TF master branch\n- fixed grayscale image capture for examples/unreal_engine.py\n\n* Fixed warnings due to deprecated function parameters.\n\n* Avoid race-condition where order in episode_rewards won't match order in episode_lengths.\n\n* Add possiblity to pass in model saver-frequency by a) episode count, b) seconds, c) timesteps.\n\n* Introduced BaseRunner class.\nRestructured existing Runner classes (Runner and ThreadedRunner) to both inherit from BaseRunner.\nUnified the interfaces used to control these classes and to handle per-episode and per-n-episode reporting.\nAdded multi-threaded testing to random and constant agent tests (configurable for all agents via run_type flag).\n\n* - added returned game_name to `connect` in unreal_engine.py\n\n* Added registry to Environment class (containing only MinimalTest).\nAdded from_spec method to Environment class so that Envs can now be created through this factory.\nChanged a comment in UE4 script.\n\n* - added returned game_name to `connect` in unreal_engine.py\n\n* - solved conflicts with TF master branch\n- fixed grayscale image capture for examples/unreal_engine.py\n\n* Fixed warnings due to deprecated function parameters.\n\n* Avoid race-condition where order in episode_rewards won't match order in episode_lengths.\n\n* Add possiblity to pass in model saver-frequency by a) episode count, b) seconds, c) timesteps.\n\n* Introduced BaseRunner class.\nRestructured existing Runner classes (Runner and ThreadedRunner) to both inherit from BaseRunner.\nUnified the interfaces used to control these classes and to handle per-episode and per-n-episode reporting.\nAdded multi-threaded testing to random and constant agent tests (configurable for all agents via run_type flag).\n\n* Added registry to Environment class (containing only MinimalTest).\nAdded from_spec method to Environment class so that Envs can now be created through this factory.\nChanged a comment in UE4 script.\n\n* - solved conflicts with TF master branch\n- fixed grayscale image capture for examples/unreal_engine.py\n\n* Fix python 2.x incompatibility (inspect.signature not supported by 2.x)\n\n* Next attempt: Fix python 2.x incompatibility (inspect.signature not supported by 2.x)\n\n* Added run_mode bit-field to test cases. Different modes for any test-agent may be set:\nsingle, multi-threaded, and distributed-tf. The default is `single`.\nAlso added num_parallel_worker setting to all test-agents. This setting determines how many parallel agents\nare used in non-single run_mode tests.\n\n* Bug fix: \"No network-spec\"-error in multi-threaded runs using the WorkerAgent factory.\n\n* Bug fix: Add episode run time to runner stats.\nSend network_spec to WorkerAgent-factory regardless (robust against non-learning agents that don't have a network)\n\n* Bug fix: Multithreaded runner would save many times per one save-interval.\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Grayscale(Preprocessor):",
            "",
            "def tf_process(self, tensor):",
            "weights = tf.reshape(tensor=self.weights, shape=(tuple(1 for _ in range(util.rank(tensor) - 1)) + (3,)))",
            "-        return tf.reduce_sum(input_tensor=(weights * tensor), axis=-1, keep_dims=True)",
            "+        return tf.reduce_sum(input_tensor=(weights * tensor), axis=-1, keepdims=True)",
            "",
            "def processed_shape(self, shape):",
            "return tuple(shape[:-1]) + (1,)"
        ]
    },
    {
        "number": 7581,
        "comments": "",
        "commit_message": "More cuda fixes (#379)\n\nDefault use_cuda=torch.Tensor.is_cuda in irange and friends\n",
        "label": "",
        "answer": "no",
        "change": [
            "class RaoBlackwellizationTests(TestCase):",
            "mean_1_error = torch.sum(torch.pow(pyro.param(\"mean_1_%d\" % k), 2.0))",
            "mean_2_error = torch.sum(torch.pow(pyro.param(\"mean_2_%d\" % k), 2.0))",
            "superfluous_error = torch.max(torch.max(mean_0_error, mean_1_error), mean_2_error)",
            "-                    superfluous_errors.append(superfluous_error.data.numpy()[0])",
            "+                    superfluous_errors.append(superfluous_error.data.cpu().numpy()[0])",
            "",
            "if step % 500 == 0 and self.verbose:",
            "print(\"mu error, log(sigma) error:  %.4f, %.4f\" % (mu_error, log_sig_error))"
        ]
    },
    {
        "number": 7586,
        "comments": "",
        "commit_message": "Fix MNIST url redirection (#2775)\n\n* Fix MNIST url redirection\n\n* Bump pytorch and torchvision versions to test release\n",
        "label": "",
        "answer": "no",
        "change": [
            "if 'READTHEDOCS' in os.environ:",
            "# TODO replace with torch_stable before release",
            "# os.system('pip install torch==1.8.0+cpu torchvision==0.9.0+cpu '",
            "#           '-f https://download.pytorch.org/whl/torch_stable.html')",
            "-    # TODO replace with torch_test once torchvision binaries are released",
            "-    # os.system('pip install torch torchvision '",
            "-    #           '-f https://download.pytorch.org/whl/test/cpu/torch_test.html')",
            "-    # This is the last nightly release of 1.8.0 before splitting to 1.9.0.",
            "-    os.system('pip install --pre '",
            "-              'torch==1.8.0.dev20210210+cpu '",
            "-              'torchvision==0.9.0.dev20210210+cpu '",
            "-              '-f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html')",
            "+    os.system('pip install torch==1.8.0+cpu torchvision==0.9.0 '",
            "+              '-f https://download.pytorch.org/whl/test/cpu/torch_test.html')"
        ]
    },
    {
        "number": 7587,
        "comments": "",
        "commit_message": "layer tests and model tests bug fixed\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class LayerNode_Test(CustomTestCase):",
            ")",
            "emb, nce = emb_net([inputs, labels])",
            "",
            "-            model = tl.models.Model(inputs=[inputs, labels], outputs=[emb, nce], name=\"word2vec_model\")",
            "+            model = tl.models.Model(inputs=[inputs, labels], outputs=[emb, nce])",
            "return model",
            "",
            "net = get_word2vec()"
        ]
    },
    {
        "number": 7588,
        "comments": "",
        "commit_message": "enable black in the precommit (#1777)\n\n* enable black in the precommit\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add some fixes\n\n* update libface detection url\n\n* added url from kornia checkpoint\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_get_hanning_kernel1d_5(device, dtype):",
            "",
            "def test_get_hanning_kernel2d_3x4(device, dtype):",
            "kernel = kornia.filters.get_hanning_kernel2d((3, 4), dtype=dtype, device=device)",
            "-    expected = torch.tensor([[0., 0.00, 0.00, 0.],",
            "-                             [0., 0.75, 0.75, 0.],",
            "-                             [0., 0.00, 0.00, 0.]], dtype=dtype, device=device)",
            "+    expected = torch.tensor(",
            "+        [[0.0, 0.00, 0.00, 0.0], [0.0, 0.75, 0.75, 0.0], [0.0, 0.00, 0.00, 0.0]], dtype=dtype, device=device",
            "+    )",
            "assert kernel.shape == (3, 4)",
            "assert_close(kernel, expected)"
        ]
    },
    {
        "number": 7590,
        "comments": "",
        "commit_message": "Fix `DBLP` download, move `get_edge_index` to `testing` package (#6901)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def hetero_data():",
            "data['paper'].x = torch.randn(8, 16)",
            "data['author'].x = torch.randn(10, 8)",
            "",
            "-    data['paper', 'paper'].edge_index = get_edge_index(8, 8, num_edges=10)",
            "+    data['paper', 'paper'].edge_index = get_random_edge_index(8, 8, 10)",
            "data['paper', 'paper'].edge_attr = torch.randn(10, 16)",
            "-    data['paper', 'author'].edge_index = get_edge_index(8, 10, num_edges=10)",
            "+    data['paper', 'author'].edge_index = get_random_edge_index(8, 10, 10)",
            "data['paper', 'author'].edge_attr = torch.randn(10, 8)",
            "-    data['author', 'paper'].edge_index = get_edge_index(10, 8, num_edges=10)",
            "+    data['author', 'paper'].edge_index = get_random_edge_index(10, 8, 10)",
            "data['author', 'paper'].edge_attr = torch.randn(10, 8)",
            "",
            "return data"
        ]
    },
    {
        "number": 7591,
        "comments": "",
        "commit_message": "fix nightly test (#1873)\n\n* fix nightly test\n\n* test coverage\n",
        "label": "",
        "answer": "no",
        "change": [
            "def deserialize_keras_object(",
            ")",
            ")",
            "else:",
            "-        return tf.keras.utils.deserialize_keras_object(",
            "+        return tf.keras.utils.deserialize_keras_object(  # pragma: no cover",
            "config, custom_objects, module_objects, printable_module_name",
            ")"
        ]
    },
    {
        "number": 7593,
        "comments": "",
        "commit_message": "serde, torch: fix typo in numpy tensor serializer. (#3644)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def numpy_tensor_serializer(worker: AbstractWorker, tensor: torch.Tensor) -> bin",
            "\"Torch to Numpy serializer can only be used with tensors that do not require grad. \"",
            "\"Detaching tensor to continue\"",
            ")",
            "-        tensor = torch.detach()",
            "+        tensor = tensor.detach()",
            "",
            "np_tensor = tensor.numpy()",
            "outfile = io.BytesIO()"
        ]
    },
    {
        "number": 7594,
        "comments": "",
        "commit_message": "[RLlib] Tf2x preparation; part 2 (upgrading `try_import_tf()`). (#9136)\n\n* WIP.\n\n* Fixes.\n\n* LINT.\n\n* WIP.\n\n* WIP.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* WIP.\n\n* Fixes.\n\n* Test\n\n* Fix.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* LINT.\n",
        "label": "",
        "answer": "no",
        "change": [
            "class ModelCatalog:",
            "",
            "dtype, shape = ModelCatalog.get_action_shape(action_space)",
            "",
            "-        return tf.placeholder(dtype, shape=shape, name=name)",
            "+        return tf1.placeholder(dtype, shape=shape, name=name)",
            "",
            "@staticmethod",
            "@DeveloperAPI"
        ]
    },
    {
        "number": 7595,
        "comments": "",
        "commit_message": "Update distirbuted training APIs. (#847)\n\n* update distirbuted training APIs.\n\n* update prefetch\n\n* fix\n\n* format\n\n* format\n\n* fix format\n\n* format\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "if __name__ == '__main__':",
            "# validation_dataset = training_dataset.map(data_aug_valid, num_parallel_calls=multiprocessing.cpu_count())",
            "trainer = tl.distributed.Trainer(",
            "build_training_func=build_train, training_dataset=training_dataset, optimizer=tf.train.AdamOptimizer,",
            "-        optimizer_args={'learning_rate': 0.0001}, batch_size=128, num_epochs=50000, prefetch_buffer_size=4096",
            "+        optimizer_args={'learning_rate': 0.0001}, batch_size=128, prefetch_size=128",
            "# validation_dataset=validation_dataset, build_validation_func=build_validation",
            ")"
        ]
    },
    {
        "number": 7597,
        "comments": "",
        "commit_message": "Fix the annotation for a method's return type (#3376)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class LstmTagger(Model):",
            "",
            "def forward(self,",
            "sentence: Dict[str, torch.Tensor],",
            "-                labels: torch.Tensor = None) -> torch.Tensor:",
            "+                labels: torch.Tensor = None) -> Dict[str, torch.Tensor]:",
            "mask = get_text_field_mask(sentence)",
            "embeddings = self.word_embeddings(sentence)",
            "encoder_out = self.encoder(embeddings, mask)"
        ]
    },
    {
        "number": 7601,
        "comments": "",
        "commit_message": "Fix broken test due to summary writer\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TensorFlowEstimator(BaseEstimator):",
            "# Set up a single operator to merge all the summaries",
            "summary_op = tf.merge_all_summaries()",
            "# Set up summary writer to a tmp directory",
            "-        self._summary_writer = tf.train.SummaryWriter('/tmp/tensorflow_logs', graph_def=sess.graph_def)",
            "+        self._summary_writer = tf.train.SummaryWriter('/tmp/tensorflow_logs', graph_def=self._session.graph_def)",
            "",
            "def fit(self, X, y):",
            "\"\"\"Builds a neural network model given provided `model_fn` and training"
        ]
    },
    {
        "number": 7602,
        "comments": "",
        "commit_message": "tests: rename tests and fix mlflow integration (#3108)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def test_inputs(framework: str | None) -> list[tuple[ModuleType, FrameworkTestMo",
            ")",
            "except ModuleNotFoundError as e:",
            "logger.warning(",
            "-                f\"Failed to find test module for framework {framework_name} (tests.integration.frameworks.models.{framework_name})\"",
            "+                f\"Failed to find test module for framework {framework_name} (tests.integration.frameworks.models.{framework_name}): {e}\"",
            ")",
            "",
            "return ["
        ]
    },
    {
        "number": 7603,
        "comments": "",
        "commit_message": "fix torch.nonzero usage in pytorch 1.5 (#2602)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "from ..builder import LOSSES",
            "",
            "def _expand_onehot_labels(labels, label_weights, label_channels):",
            "bin_labels = labels.new_full((labels.size(0), label_channels), 0)",
            "-    inds = torch.nonzero((labels >= 0) & (labels < label_channels)).squeeze()",
            "+    inds = torch.nonzero(",
            "+        (labels >= 0) & (labels < label_channels), as_tuple=False).squeeze()",
            "if inds.numel() > 0:",
            "bin_labels[inds, labels[inds]] = 1",
            "bin_label_weights = label_weights.view(-1, 1).expand("
        ]
    },
    {
        "number": 7606,
        "comments": "",
        "commit_message": "fix: tf_model save\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TFModel(Trainable, Inferable, metaclass=TfModelMeta):",
            "print('\\n:: Model saved to {} \\n'.format(self.model_path_.as_posix()))",
            "",
            "def get_checkpoint_state(self):",
            "-        if self.model_path_.is_dir():",
            "-            return tf.train.get_checkpoint_state(self.model_path_)",
            "-        return tf.train.get_checkpoint_state(self.model_path_.parent)",
            "+        if self._model_file:",
            "+            return tf.train.get_checkpoint_state(self.model_path_.parent)",
            "+        return tf.train.get_checkpoint_state(self.model_path_)",
            "",
            "@check_path_exists('dir')",
            "@overrides"
        ]
    },
    {
        "number": 7607,
        "comments": "",
        "commit_message": "small format fixes\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def random_normal(",
            "true_shape: List[int] = shape",
            "mean = mean.item() if isinstance(mean, torch.Tensor) else mean",
            "std = std.item() if isinstance(std, torch.Tensor) else std",
            "-    return torch.normal(",
            "-        mean, std, true_shape, device=default_device(device)",
            "-    )",
            "+    return torch.normal(mean, std, true_shape, device=default_device(device))",
            "",
            "",
            "def multinomial("
        ]
    },
    {
        "number": 7613,
        "comments": "",
        "commit_message": "Fixed lint failure\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "gelu.unsupported_dtypes = (\"float16\",)",
            "",
            "def sigmoid(x: torch.Tensor, /, *, out: Optional[torch.Tensor] = None) -> torch.Tensor:",
            "if not ivy.is_array(x):",
            "-        x=torch.tensor(x)",
            "+        x = torch.tensor(x)",
            "return torch.sigmoid(x, out=out)"
        ]
    },
    {
        "number": 7614,
        "comments": "",
        "commit_message": "fix format failures\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def one_hot(indices, depth: int, device: Optional[str] = None):",
            ")",
            "",
            "",
            "-def shape(",
            "-    x: torch.Tensor, as_tensor: bool = False",
            "-) -> Union[torch.Tensor, List[int]]:",
            "+def shape(x: torch.Tensor, as_tensor: bool = False) -> Union[torch.Tensor, List[int]]:",
            "if as_tensor:",
            "return torch.tensor(x.shape)",
            "else:"
        ]
    },
    {
        "number": 7616,
        "comments": "",
        "commit_message": "Replace strided slice with tf.expand_dims (#10078)\n\n* Replace tf.newaxis -> tf.expand_dims\n\n* Fix tests\n\n* Fix tests\n\n* Use reshape when a tensors needs a double expand\n\n* Fix GPT2\n\n* Fix GPT2\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TFSequenceSummary(tf.keras.layers.Layer):",
            ")  # A tensor full of shape [batch] or [batch, num choices] full of sequence length",
            "cls_shape = shape_list(cls_index)",
            "if len(cls_shape) <= len(hidden_shape) - 2:",
            "-                cls_index = cls_index[..., tf.newaxis]",
            "+                cls_index = tf.expand_dims(cls_index, axis=-1)",
            "# else:",
            "# cls_index = cls_index[..., tf.newaxis]",
            "# cls_index = cls_index.expand((-1,) * (cls_index.dim()-1) + (hidden_states.size(-1),))"
        ]
    },
    {
        "number": 7617,
        "comments": "",
        "commit_message": "Fix #813 - GlowTTS training  (#814)\n\n* Fix #813\n\n* Update glow_tts recipe\n\n* Fix glow-tts test\n\n* Linter fix\n\n* Run data dep init only in training\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Encoder(nn.Module):",
            "# set duration predictor input",
            "if g is not None:",
            "g_exp = g.expand(-1, -1, x.size(-1))",
            "-            x_dp = torch.cat([torch.detach(x), g_exp], 1)",
            "+            x_dp = torch.cat([x.detach(), g_exp], 1)",
            "else:",
            "-            x_dp = torch.detach(x)",
            "+            x_dp = x.detach()",
            "# final projection layer",
            "x_m = self.proj_m(x) * x_mask",
            "if not self.mean_only:"
        ]
    },
    {
        "number": 7618,
        "comments": "",
        "commit_message": "Fixed problem in running standardize, added tf/fn_preprocess to model\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class NaturalGradient(Optimizer):",
            "deltas = self.solver.solve(fn_x=fisher_matrix_product, x_init=None, b=[-grad for grad in loss_gradients], f_args=(kldiv_gradients,))",
            "",
            "# delta' * F",
            "-        delta_fisher_matrix_product = fisher_matrix_product(x=deltas)",
            "+        delta_fisher_matrix_product = fisher_matrix_product(deltas=deltas, kldiv_grads=kldiv_gradients)",
            "",
            "# c' = 0.5 * delta' * F * delta'  (= lambda * c)",
            "# TODO: Why constant and hence KL-divergence sometimes negative?"
        ]
    },
    {
        "number": 7619,
        "comments": "",
        "commit_message": "Make Trainer.__test_using_best_weights use cloud_io's load to support more storage backends (#3694)\n\n* Split out changes from #3563 to make that PR easier to review. This formats the file according to the Black formatter\n\n* Store a reference to the trainer on the datamodule\n\nFixes #3682\n\n* Update data_connector.py\n\n* Update data_connector.py\n\n* Update test_datamodules.py\n\n* Support more storage backends in trainer.test using best weights\n\nSimilar to #3692\n\n* Update trainer.py\n\n* Update trainer.py\n\nuse cloud_io load directly\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Trainer(",
            ")",
            "return {}",
            "",
            "-            ckpt = torch.load(ckpt_path, map_location=lambda storage, loc: storage)",
            "+            ckpt = pl_load(ckpt_path, map_location=lambda storage, loc: storage)",
            "model.load_state_dict(ckpt['state_dict'])",
            "",
            "# attach dataloaders"
        ]
    },
    {
        "number": 7621,
        "comments": "",
        "commit_message": "Fixed from_networkx to support empty edge lists\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def from_networkx(G):",
            "",
            "G = nx.convert_node_labels_to_integers(G)",
            "G = G.to_directed() if not nx.is_directed(G) else G",
            "-    edge_index = torch.tensor(list(G.edges)).t().contiguous()",
            "+    edge_index = torch.LongTensor(list(G.edges)).t().contiguous()",
            "",
            "data = {}"
        ]
    },
    {
        "number": 7625,
        "comments": "",
        "commit_message": "Fix build test. (#1057)\n\n* 2.2.1 release\n\n* Fix build test.\n\n* apply yapf\n\n* ping yapf to 0.28.0\n\n* fix build\n\n* use yapf 0.29\n\n* fix yapf\n\n* include tests in make format.\n\n* ping autoflake and isort version.\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def _map_fn_train(img, target):",
            "",
            "def _map_fn_test(img, target):",
            "# 1. Crop the central [height, width] of the image.",
            "-    img = tf.image.resize_with_pad(img, 24, 24)",
            "+    img = tf.image.resize_with_pad(img, 24, 24)",
            "# 2. Subtract off the mean and divide by the variance of the pixels.",
            "img = tf.image.per_image_standardization(img)",
            "img = tf.reshape(img, (24, 24, 3))"
        ]
    },
    {
        "number": 7627,
        "comments": "",
        "commit_message": "Fix TF input for np.ndarray (#9294)\n\n* Fix input for np.ndarray\"\n\n* add a test\n\n* add a test\n\n* Add a test\n\n* Apply style\n\n* Fix test\n",
        "label": "",
        "answer": "no",
        "change": [
            "def input_processing(func, config, input_ids, **kwargs):",
            "signature.pop(\"kwargs\", None)",
            "parameter_names = list(signature.keys())",
            "output = {}",
            "-    allowed_types = (tf.Tensor, bool, int, ModelOutput, tuple, list, dict)",
            "+    allowed_types = (tf.Tensor, bool, int, ModelOutput, tuple, list, dict, np.ndarray)",
            "",
            "if \"inputs\" in kwargs[\"kwargs_call\"]:",
            "warnings.warn("
        ]
    },
    {
        "number": 7630,
        "comments": "",
        "commit_message": "fix linting.\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Model(GANModelDesc):",
            "GaussianDistribution(\"uni_a\", 1),",
            "GaussianDistribution(\"uni_b\", 1)])",
            "# prior: the assumption how the factors are presented in the dataset",
            "-        prior = tf.constant([0.1] * 10 + [0,0], tf.float32, [12], name='prior')",
            "+        prior = tf.constant([0.1] * 10 + [0, 0], tf.float32, [12], name='prior')",
            "batch_prior = tf.tile(tf.expand_dims(prior, 0), [BATCH, 1], name='batch_prior')",
            "",
            "# sample the latent code zc:",
            "sample = self.factors.dists[0].sample(",
            "-            BATCH, tf.constant([0.1]*10, tf.float32, shape=[10]))",
            "+            BATCH, tf.constant([0.1] * 10, tf.float32, shape=[10]))",
            "z_cat = symbf.remove_shape(sample, 0, name='z_cat')",
            "# still sample the latent code from a uniform distribution.",
            "z_uni_a = symbf.remove_shape("
        ]
    },
    {
        "number": 7631,
        "comments": "",
        "commit_message": "small fix for tensorflow iinfo.\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def closest_valid_dtype(type):",
            "",
            "",
            "def iinfo(type):",
            "-    return tf.experimental.numpy.iinfo(dtype_from_str(type))",
            "+    return tf.experimental.numpy.iinfo(dtype_to_str(type))",
            "",
            "",
            "class Finfo:"
        ]
    },
    {
        "number": 7632,
        "comments": "",
        "commit_message": "[RLlib] Minor cleanup in preparation to tf2.x support. (#9130)\n\n* WIP.\n\n* Fixes.\n\n* LINT.\n\n* Fixes.\n\n* Fixes and LINT.\n\n* WIP.\n",
        "label": "",
        "answer": "no",
        "change": [
            "def huber_loss(x, delta=1.0):",
            "\"\"\"Reference: https://en.wikipedia.org/wiki/Huber_loss\"\"\"",
            "return tf.where(",
            "tf.abs(x) < delta,",
            "-        tf.square(x) * 0.5, delta * (tf.abs(x) - 0.5 * delta))",
            "+        tf.math.square(x) * 0.5, delta * (tf.abs(x) - 0.5 * delta))",
            "",
            "",
            "def reduce_mean_ignore_inf(x, axis):"
        ]
    },
    {
        "number": 7635,
        "comments": "",
        "commit_message": "Reorganize repo (#8580)\n\n* Put models in subfolders\n\n* Styling\n\n* Fix imports in tests\n\n* More fixes in test imports\n\n* Sneaky hidden imports\n\n* Fix imports in doc files\n\n* More sneaky imports\n\n* Finish fixing tests\n\n* Fix examples\n\n* Fix path for copies\n\n* More fixes for examples\n\n* Fix dummy files\n\n* More fixes for example\n\n* More model import fixes\n\n* Is this why you're unhappy GitHub?\n\n* Fix imports in conver command\n",
        "label": "",
        "answer": "no",
        "change": [
            "class HfArgumentParserTest(unittest.TestCase):",
            "logger = logging.logging.getLogger()",
            "with CaptureLogger(logger) as cl:",
            "# this action activates the env var",
            "-            logging.get_logger(\"transformers.tokenization_bart\")",
            "+            logging.get_logger(\"transformers.models.bart.tokenization_bart\")",
            "self.assertIn(\"Unknown option TRANSFORMERS_VERBOSITY=super-error\", cl.out)",
            "",
            "# no need to restore as nothing was changed"
        ]
    },
    {
        "number": 7639,
        "comments": "",
        "commit_message": "Sinc Convs - fixed scale tests and pytorch compability\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class BarkScale:",
            "def convert(f):",
            "\"\"\"Convert Hz to Bark.\"\"\"",
            "b = torch.div(f, 1000.0)",
            "-        b = torch.square(b) * 1.4",
            "+        b = torch.pow(b, 2.0) * 1.4",
            "b = torch.pow(b + 1.0, 0.69)",
            "return b * 75.0 + 25.0"
        ]
    },
    {
        "number": 7640,
        "comments": "",
        "commit_message": "codemod for 0.4 (#331) (#336)\n\n* codemod for 0.4\n\n* better IN key removal\n\n* fix ac\n\n* update to use some device\n\n* rnn flatten_param\n",
        "label": "",
        "answer": "no",
        "change": [
            "def finish_episode():",
            "for r in policy.rewards[::-1]:",
            "R = r + args.gamma * R",
            "rewards.insert(0, R)",
            "-    rewards = torch.Tensor(rewards)",
            "-    rewards = (rewards - rewards.mean()) / (rewards.std() + np.finfo(np.float32).eps)",
            "+    rewards = torch.tensor(rewards)",
            "+    rewards = (rewards - rewards.mean()) / (rewards.std() + eps)",
            "for log_prob, reward in zip(policy.saved_log_probs, rewards):",
            "policy_loss.append(-log_prob * reward)",
            "optimizer.zero_grad()"
        ]
    },
    {
        "number": 7643,
        "comments": "",
        "commit_message": "Fix attention scores shape definition\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class AttentionDecoder(RNNDecoder):",
            "logits=self.vocab_size,",
            "predicted_ids=tf.TensorShape([]),",
            "cell_output=self.cell.output_size,",
            "-        attention_scores=tf.concat([0, self.attention_values[1:-1]], 0),",
            "+        attention_scores=tf.concat(",
            "+            [[0], tf.shape(self.attention_values)[1:-1]], 0),",
            "attention_context=self.attention_values.get_shape()[-1])",
            "",
            "@property"
        ]
    },
    {
        "number": 7647,
        "comments": "",
        "commit_message": "fix deprecations about casting & initializers in tf1.13\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Model(ModelDesc):",
            "cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)",
            "cost = tf.reduce_mean(cost, name='cross_entropy_loss')",
            "",
            "-        correct = tf.to_float(tf.nn.in_top_k(logits, label, 1), name='correct')",
            "+        correct = tf.cast(tf.nn.in_top_k(logits, label, 1), tf.float32, name='correct')",
            "# monitor training error",
            "add_moving_summary(tf.reduce_mean(correct, name='accuracy'))"
        ]
    },
    {
        "number": 7648,
        "comments": "",
        "commit_message": "set up precommit and fixed flake8 compliance issues\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestTorchHook(TestCase):",
            "'torch.CharTensor': torch.CharTensor,",
            "'torch.ShortTensor': torch.ShortTensor,",
            "'torch.IntTensor': torch.IntTensor,",
            "-            'torch.LongTensor': torch.LongTensor",
            "+            'torch.LongTensor': torch.LongTensor,",
            "}",
            "",
            "for k, v in tensor_types.items():"
        ]
    },
    {
        "number": 7650,
        "comments": "",
        "commit_message": "[FasterRCNN] fix bug in gradient propagation\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def sample_fast_rcnn_targets(boxes, gt_boxes, gt_labels):",
            "ret_labels = tf.concat(",
            "[tf.gather(gt_labels, fg_inds_wrt_gt),",
            "tf.zeros_like(bg_inds, dtype=tf.int64)], axis=0, name='sampled_labels')",
            "-    return ret_boxes, tf.stop_gradient(ret_labels), fg_inds_wrt_gt",
            "+    return tf.stop_gradient(ret_boxes), tf.stop_gradient(ret_labels), fg_inds_wrt_gt",
            "",
            "",
            "@under_name_scope()"
        ]
    },
    {
        "number": 7651,
        "comments": "",
        "commit_message": "fix deprecations about casting & initializers in tf1.13\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Model(ModelDesc):",
            "cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)",
            "cost = tf.reduce_mean(cost, name='cross_entropy_loss')",
            "",
            "-        wrong = tf.to_float(tf.logical_not(tf.nn.in_top_k(logits, label, 1)), name='wrong_vector')",
            "+        wrong = tf.cast(tf.logical_not(tf.nn.in_top_k(logits, label, 1)), tf.float32, name='wrong_vector')",
            "# monitor training error",
            "add_moving_summary(tf.reduce_mean(wrong, name='train_error'))"
        ]
    },
    {
        "number": 7653,
        "comments": "",
        "commit_message": "Rewrite multigpu testing with mmddp (#622)\n\n* rewrite multigpu testing with mmddp\n\n* fix an indent\n\n* update slurm testing script\n\n* update readme and scripts\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class DistributedGroupSampler(Sampler):",
            "",
            "indices = [",
            "indices[j] for i in list(",
            "-                torch.randperm(",
            "-                    len(indices) // self.samples_per_gpu, generator=g))",
            "+                torch.randperm(len(indices) // self.samples_per_gpu,",
            "+                               generator=g))",
            "for j in range(i * self.samples_per_gpu, (i + 1) *",
            "self.samples_per_gpu)",
            "]"
        ]
    },
    {
        "number": 7654,
        "comments": "",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class BiMpmMatching(nn.Module, FromParams):",
            "len_1 = get_lengths_from_binary_sequence_mask(mask_1)",
            "len_2 = get_lengths_from_binary_sequence_mask(mask_2)",
            "",
            "-        # (batch, seq_len*)",
            "-        mask_1, mask_2 = mask_1.float(), mask_2.float()",
            "-",
            "# explicitly set masked weights to zero",
            "# (batch_size, seq_len*, hidden_dim)",
            "context_1 = context_1 * mask_1.unsqueeze(-1)"
        ]
    },
    {
        "number": 7658,
        "comments": "",
        "commit_message": "fix mypy errors\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def convert_points_from_homogeneous(points: torch.Tensor,",
            "",
            "# we check for points at infinity",
            "z_vec: torch.Tensor = points[..., -1:]",
            "-    scale: torch.Tensor = 1. / torch.clamp(z_vec, eps)",
            "+    scale: torch.Tensor = torch.tensor(1.) / torch.clamp(z_vec, eps)",
            "",
            "return scale * points[..., :-1]"
        ]
    },
    {
        "number": 7665,
        "comments": "",
        "commit_message": "Fix stray objects from FSS `Worker/Plan` initialization (#3525)\n\n* Fix stray objects from FSS `Worker/Plan` initialization\n\n* Update test/torch/tensors/test_additive_shared.py\n\nCo-authored-by: Th\u00e9o Ryffel <theo.leffyr@gmail.com>\n\n* Update test/torch/tensors/test_additive_shared.py\n\nCo-authored-by: Th\u00e9o Ryffel <theo.leffyr@gmail.com>\n\n* Update test/torch/tensors/test_additive_shared.py\n\nCo-authored-by: Th\u00e9o Ryffel <theo.leffyr@gmail.com>\n\n* Update test/torch/tensors/test_additive_shared.py\n\nCo-authored-by: Th\u00e9o Ryffel <theo.leffyr@gmail.com>\n\nCo-authored-by: Th\u00e9o Ryffel <theo.leffyr@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class BaseWorker(AbstractWorker):",
            "",
            "# storage object for crypto primitives",
            "self.crypto_store = PrimitiveStorage(owner=self)",
            "-        # declare the plans used for crypto computations",
            "-        sy.frameworks.torch.mpc.fss.initialize_crypto_plans(self)",
            "",
            "def register_obj(self, obj):",
            "self.object_store.register_obj(self, obj)"
        ]
    },
    {
        "number": 7666,
        "comments": "",
        "commit_message": "Disable tests in TF1 using tf2.enabled().\n\nAlso fix image conversion in mnist_export_v2 to prevent float overflow in dbg mode.\n\nPiperOrigin-RevId: 307367586\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class ExportTest(tf.test.TestCase):",
            "",
            "if __name__ == \"__main__\":",
            "# This test is only supported in TF 2.0.",
            "-  if LooseVersion(tf.__version__) >= LooseVersion(\"2.0.0-beta0\"):",
            "+  if tf.executing_eagerly():",
            "logging.info(\"Using TF version: %s\", tf.__version__)",
            "tf.test.main()",
            "else:",
            "logging.warning(\"Skipping running tests for TF Version: %s\", tf.__version__)",
            "-"
        ]
    },
    {
        "number": 7671,
        "comments": "",
        "commit_message": "Fix Conv1D block for FastSpeech\n\nFrom the paper:\n\n> Different from the 2-layer dense network in Transformer, we use a\n> 2-layer 1D convolutional network with ReLU activation\n\n> dropout are added after the self-attention network and 1D\nconvolutional network respectively.\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class MultiLayeredConv1d(torch.nn.Module):",
            "",
            "\"\"\"",
            "x = torch.relu(self.w_1(x.transpose(-1, 1))).transpose(-1, 1)",
            "-        return self.w_2(self.dropout(x).transpose(-1, 1)).transpose(-1, 1)",
            "+        x = torch.relu(self.w_2(x.transpose(-1, 1))).transpose(-1, 1)",
            "+        return self.dropout(x)"
        ]
    },
    {
        "number": 7672,
        "comments": "",
        "commit_message": "\ud83d\udd27 Fix #131 and Fix #132\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TFTacotron2(tf.keras.Model):",
            "training=True,",
            ")",
            "",
            "-    @tf.function(experimental_relax_shapes=True)",
            "def call(",
            "self,",
            "input_ids,"
        ]
    },
    {
        "number": 7673,
        "comments": "",
        "commit_message": "Import fixes, lint fixes and numpy style docstring fix.\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def diff(",
            "x = x if type(x) == torch.Tensor else torch.Tensor(x)",
            "prepend = (",
            "prepend",
            "-        if type(prepend) == torch.Tensor or prepend == None",
            "+        if type(prepend) == torch.Tensor or prepend is None",
            "else torch.Tensor(prepend)",
            ")",
            "append = (",
            "append",
            "-        if type(append) == torch.Tensor or append == None",
            "+        if type(append) == torch.Tensor or append is None",
            "else torch.Tensor(append)",
            ")",
            "return torch.diff(x, n=n, dim=axis, prepend=prepend, append=append)"
        ]
    },
    {
        "number": 7676,
        "comments": "",
        "commit_message": "more numpy tests fixes\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def argmax(",
            "keepdims: bool = False,",
            "out: Optional[Union[tf.Tensor, tf.Variable]] = None,",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "-    ret = tf.constant(x).numpy().argmax(axis=axis, keepdims=keepdims)",
            "-    ret = tf.convert_to_tensor(ret, dtype=ret.dtype)",
            "-",
            "-    return ret",
            "+    ret = x.numpy().argmax(axis=axis, keepdims=keepdims)",
            "+    return tf.convert_to_tensor(ret, dtype=ret.dtype)",
            "",
            "",
            "def argmin("
        ]
    },
    {
        "number": 7684,
        "comments": "",
        "commit_message": "[Fix] Fixes jit tests in feature for pytorch 1.6 (#858)\n\n* fix test jit issues in feature module\n\n* add github actions job for fast test pytorch versions\n\n* fixed orientation module mypy issues\n\n* add torch.jit.annotation for mypy issues\n\n* remove import cast\n\n* preset self.angle_detector as nn.Module\n",
        "label": "",
        "answer": "no",
        "change": [
            "class SOSNet(nn.Module):",
            "",
            "return",
            "",
            "-    def forward(self, input: torch.Tensor, eps: float = 1e-10) -> torch.Tensor:   # type: ignore",
            "+    def forward(self, input: torch.Tensor, eps: float = 1e-10) -> torch.Tensor:",
            "descr = self.desc_norm(self.layers(input) + eps)",
            "descr = descr.view(descr.size(0), -1)",
            "return descr"
        ]
    },
    {
        "number": 7691,
        "comments": "",
        "commit_message": "Token characters encoder (#40)\n\n* Added TokenCharactersEncoder (still needs tests)\n\n* Added tests, but it's not working with the LSTM\n\n* Got tests to pass with CNN\n\n* Better name\n\n* Fix lint errors\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestCnnEncoder(AllenNlpTestCase):",
            "",
            "def test_forward_does_correct_computation(self):",
            "encoder = CnnEncoder(embedding_dim=2, num_filters=1, ngram_filter_sizes=(1, 2))",
            "-        const_init = lambda tensor: torch.nn.init.constant(tensor, 1.)",
            "-        initializer = InitializerApplicator(default_initializer=const_init)",
            "+        constant_init = lambda tensor: torch.nn.init.constant(tensor, 1.)",
            "+        initializer = InitializerApplicator(default_initializer=constant_init)",
            "initializer(encoder)",
            "input_tensor = Variable(torch.FloatTensor([[[.7, .8], [.1, 1.5]]]))",
            "encoder_output = encoder(input_tensor)"
        ]
    },
    {
        "number": 7695,
        "comments": "",
        "commit_message": "Update kernels.py (#1940)\n\n* Update kernels.py\n\nChanged x.pow(2.0) to x**2.0 to make it stylistically consistent with sigma**2\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def gaussian(window_size: int, sigma: float) -> torch.Tensor:",
            "x = torch.arange(window_size, device=device, dtype=dtype) - window_size // 2",
            "if window_size % 2 == 0:",
            "x = x + 0.5",
            "-    gauss = torch.exp(-x.pow(2.0) / (2 * sigma**2))",
            "+    gauss = torch.exp(-(x**2.0) / (2 * sigma**2))",
            "return gauss / gauss.sum()"
        ]
    },
    {
        "number": 7697,
        "comments": "",
        "commit_message": "CDN urls (#4030)\n\n* [file_utils] use_cdn + documentation\n\n* Move to cdn. urls for weights\n\n* [urls] Hotfix for bert-base-japanese\n",
        "label": "",
        "answer": "no",
        "change": [
            "from .modeling_tf_utils import TFPreTrainedModel, get_initializer, shape_list",
            "logger = logging.getLogger(__name__)",
            "",
            "TF_ROBERTA_PRETRAINED_MODEL_ARCHIVE_MAP = {",
            "-    \"roberta-base\": \"https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-tf_model.h5\",",
            "-    \"roberta-large\": \"https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-tf_model.h5\",",
            "-    \"roberta-large-mnli\": \"https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-mnli-tf_model.h5\",",
            "-    \"distilroberta-base\": \"https://s3.amazonaws.com/models.huggingface.co/bert/distilroberta-base-tf_model.h5\",",
            "+    \"roberta-base\": \"https://cdn.huggingface.co/roberta-base-tf_model.h5\",",
            "+    \"roberta-large\": \"https://cdn.huggingface.co/roberta-large-tf_model.h5\",",
            "+    \"roberta-large-mnli\": \"https://cdn.huggingface.co/roberta-large-mnli-tf_model.h5\",",
            "+    \"distilroberta-base\": \"https://cdn.huggingface.co/distilroberta-base-tf_model.h5\",",
            "}"
        ]
    },
    {
        "number": 7698,
        "comments": "",
        "commit_message": "Topological sort, zero_grads (#119)\n\n* Topological sort, zero_grads\n\n* Bug fix, add test\n\n* Add zero_grads\n\n* Put deepwalk function in backward\n\n* Move zero_grad to optim\n\n* Fix gradcheck hack\n\nCo-authored-by: holonomicjl <58403584+holonomicjl@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TinyConvNet:",
            "def train(model, optim, steps, BS=128, gpu=False):",
            "losses, accuracies = [], []",
            "for i in (t := trange(steps, disable=os.getenv('CI') is not None)):",
            "+    optim.zero_grad()",
            "samp = np.random.randint(0, X_train.shape[0], size=(BS))",
            "",
            "x = Tensor(X_train[samp].reshape((-1, 28*28)).astype(np.float32), gpu=gpu)"
        ]
    },
    {
        "number": 7700,
        "comments": "",
        "commit_message": "Remove redundant padding_value from LSTMEncoder (fixes #1807) (#1816)\n\nSummary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/1816\n\nDifferential Revision: D20365924\n\nPulled By: myleott\n\nfbshipit-source-id: e0349004d53f2d5e42eb7f3bf98db093f8c27fbf\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class LSTMEncoder(FairseqEncoder):",
            "packed_outs, (final_hiddens, final_cells) = self.lstm(packed_x, (h0, c0))",
            "",
            "# unpack outputs and apply dropout",
            "-        x, _ = nn.utils.rnn.pad_packed_sequence(packed_outs, padding_value=self.padding_value)",
            "+        x, _ = nn.utils.rnn.pad_packed_sequence(packed_outs, padding_value=self.padding_idx)",
            "x = F.dropout(x, p=self.dropout_out, training=self.training)",
            "assert list(x.size()) == [seqlen, bsz, self.output_units]"
        ]
    },
    {
        "number": 7702,
        "comments": "",
        "commit_message": "fixed uint8 warning (#2241)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class CascadeRCNN(BaseDetector, RPNTestMixin):",
            "device=device,",
            "dtype=torch.uint8))",
            "pos_inds = torch.cat(pos_inds)",
            "-                    mask_feats = bbox_feats[pos_inds]",
            "+                    mask_feats = bbox_feats[pos_inds.type(torch.bool)]",
            "mask_head = self.mask_head[i]",
            "mask_pred = mask_head(mask_feats)",
            "mask_targets = mask_head.get_target(sampling_results, gt_masks,"
        ]
    },
    {
        "number": 7704,
        "comments": "",
        "commit_message": "Ref #118: Additional fixes to neural_translation_word example.\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "if os.path.exists(os.path.join(PATH, 'graph.pbtxt')):",
            "translator = skflow.TensorFlowEstimator.restore(PATH)",
            "else:",
            "translator = skflow.TensorFlowEstimator(model_fn=translate_model,",
            "-        n_classes=n_words,",
            "+        n_classes=n_fr_words,",
            "optimizer='Adam', learning_rate=0.01, batch_size=128,",
            "continue_training=True, steps=100)"
        ]
    },
    {
        "number": 7705,
        "comments": "",
        "commit_message": "[RLlib] Minor cleanup in preparation to tf2.x support. (#9130)\n\n* WIP.\n\n* Fixes.\n\n* LINT.\n\n* Fixes.\n\n* Fixes and LINT.\n\n* WIP.\n",
        "label": "",
        "answer": "no",
        "change": [
            "class ParameterNoise(Exploration):",
            "added_noises.append(",
            "tf.assign(",
            "noise,",
            "-                    tf.random_normal(",
            "+                    tf.random.normal(",
            "shape=noise.shape,",
            "stddev=self.stddev,",
            "dtype=tf.float32)))"
        ]
    },
    {
        "number": 7712,
        "comments": "",
        "commit_message": "remove excess examples & fix docstrings\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def vander(",
            "increasing: Optional[bool] = False,",
            "out: Optional[Union[tf.Tensor, tf.Variable]] = None,",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "-    return tf.experimental.numpy.vander(",
            "-        x, N=N, increasing=increasing",
            "-    )",
            "+    return tf.experimental.numpy.vander(x, N=N, increasing=increasing)"
        ]
    },
    {
        "number": 7714,
        "comments": "",
        "commit_message": "fix CI error\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class LogMel(torch.nn.Module):",
            "return \", \".join(f\"{k}={v}\" for k, v in self.mel_options.items())",
            "",
            "def forward(",
            "-        self,",
            "-        feat: torch.Tensor,",
            "-        ilens: torch.Tensor = None,",
            "+        self, feat: torch.Tensor, ilens: torch.Tensor = None",
            ") -> Tuple[torch.Tensor, torch.Tensor]:",
            "# feat: (B, T, D1) x melmat: (D1, D2) -> mel_feat: (B, T, D2)",
            "mel_feat = torch.matmul(feat, self.melmat)"
        ]
    },
    {
        "number": 7717,
        "comments": "",
        "commit_message": "progress on distributed mode, optimizer None allowed, various fixes\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Categorical(Distribution):",
            "# \"Normalized\" logits",
            "logits = tf.log(x=probabilities)",
            "",
            "-        logits = tf.Print(logits, (logits, probabilities, state_value))",
            "-",
            "return logits, probabilities, state_value",
            "",
            "def state_value(self, distr_params):"
        ]
    },
    {
        "number": 7719,
        "comments": "",
        "commit_message": "[Metrics] Unification of regression (#4166)\n\n* moved to utility\n\n* add files\n\n* unify\n\n* add desc\n\n* update\n\n* end of line\n\n* Apply suggestions from code review\n\nCo-authored-by: Justus Schock <12886177+justusschock@users.noreply.github.com>\n\n* Apply suggestions from code review\n\nCo-authored-by: Justus Schock <12886177+justusschock@users.noreply.github.com>\n\n* add back functional test in new interface\n\n* pep8\n\n* doctest fix\n\n* test name fix\n\n* unify psnr + add class psnr, TODO: psnr test refactor ala mean squared error\n\n* unify psnr\n\n* rm unused code\n\n* pep8\n\n* docs\n\n* unify ssim\n\n* lower tolerance for ssim\n\n* fix import\n\n* pep8\n\n* docs\n\n* flake8\n\n* test smaller images\n\n* trying to fix test\n\n* no ddp test for ssim\n\n* pep8\n\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\nCo-authored-by: Justus Schock <12886177+justusschock@users.noreply.github.com>\nCo-authored-by: Teddy Koker <teddy.koker@gmail.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class MeanAbsoluteError(Metric):",
            "preds: Predictions from model",
            "target: Ground truth values",
            "\"\"\"",
            "-        self._check_same_shape(preds, target)",
            "-        abs_error = torch.abs(preds - target)",
            "+        sum_abs_error, n_obs = _mean_absolute_error_update(preds, target)",
            "",
            "-        self.sum_abs_error += torch.sum(abs_error)",
            "-        self.total += target.numel()",
            "+        self.sum_abs_error += sum_abs_error",
            "+        self.total += n_obs",
            "",
            "def compute(self):",
            "\"\"\"",
            "Computes mean absolute error over state.",
            "\"\"\"",
            "-        return self.sum_abs_error / self.total",
            "+        return _mean_absolute_error_compute(self.sum_abs_error, self.total)"
        ]
    },
    {
        "number": 7720,
        "comments": "",
        "commit_message": "fix batchnorm for TF 0.12\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def init_training_mode():",
            "# 'is_training' collection stores the training mode variable",
            "coll = tf.get_collection('is_training')",
            "if len(coll) == 0:",
            "-        tr_var = tf.get_variable(",
            "+        tr_var = variable(",
            "\"is_training\", dtype=tf.bool, shape=[],",
            "initializer=tf.constant_initializer(False),",
            "trainable=False)"
        ]
    },
    {
        "number": 7723,
        "comments": "",
        "commit_message": "[Fix]: Fix torch.randperm (#5014)\n\n* Update random_sampler.py\n\n* Update random_sampler.py\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class RandomSampler(BaseSampler):",
            "else:",
            "device = 'cpu'",
            "gallery = torch.tensor(gallery, dtype=torch.long, device=device)",
            "-        perm = torch.randperm(gallery.numel(), device=gallery.device)[:num]",
            "+        # This is a temporary fix. We can revert the following code",
            "+        # when PyTorch fixes the abnormal return of torch.randperm.",
            "+        # See: https://github.com/open-mmlab/mmdetection/pull/5014",
            "+        perm = torch.randperm(gallery.numel())[:num].to(device=gallery.device)",
            "rand_inds = gallery[perm]",
            "if not is_tensor:",
            "rand_inds = rand_inds.cpu().numpy()"
        ]
    },
    {
        "number": 7724,
        "comments": "",
        "commit_message": "Fixing `to_heterogeneous` to work with GPU (#4910)\n\n* fixing to_heterogeneous to work with gpu data\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\nCo-authored-by: Your Name <devnull@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Data(BaseData, FeatureStore, GraphStore):",
            "node_ids, index_map = {}, torch.empty_like(node_type)",
            "for i, key in enumerate(node_type_names):",
            "node_ids[i] = (node_type == i).nonzero(as_tuple=False).view(-1)",
            "-            index_map[node_ids[i]] = torch.arange(len(node_ids[i]))",
            "+            index_map[node_ids[i]] = torch.arange(len(node_ids[i]),",
            "+                                                  device=index_map.device)",
            "",
            "# We iterate over edge types to find the local edge indices:",
            "edge_ids = {}"
        ]
    },
    {
        "number": 7738,
        "comments": "",
        "commit_message": "Update distirbuted training APIs. (#847)\n\n* update distirbuted training APIs.\n\n* update prefetch\n\n* fix\n\n* format\n\n* format\n\n* fix format\n\n* format\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "if __name__ == '__main__':",
            "# validation_dataset = make_dataset(X_val, y_val)",
            "trainer = tl.distributed.Trainer(",
            "build_training_func=build_train, training_dataset=training_dataset, optimizer=tf.train.AdamOptimizer,",
            "-        optimizer_args={'learning_rate': 0.001}, batch_size=500, num_epochs=500, prefetch_buffer_size=4096",
            "+        optimizer_args={'learning_rate': 0.001}, batch_size=500, prefetch_size=500",
            "# validation_dataset=validation_dataset, build_validation_func=build_validation",
            ")"
        ]
    },
    {
        "number": 7739,
        "comments": "",
        "commit_message": "fix\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Stft(torch.nn.Module, InversibleInterface):",
            "pad = self.n_fft // 2",
            "ilens = ilens + 2 * pad",
            "",
            "-            olens = torch.div((ilens - self.n_fft), self.hop_length) + 1",
            "+            olens = (",
            "+                torch.div((ilens - self.n_fft), self.hop_length, rounding_mode=\"floor\")",
            "+                + 1",
            "+            )",
            "output.masked_fill_(make_pad_mask(olens, output, 1), 0.0)",
            "else:",
            "olens = None"
        ]
    },
    {
        "number": 7743,
        "comments": "",
        "commit_message": "fix terminal reward order in runner\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Categorical(Distribution):",
            "# \"Normalized\" logits",
            "logits = tf.log(x=probabilities)",
            "",
            "-        # logits = tf.Print(logits, (logits, probabilities, state_value))",
            "+        logits = tf.Print(logits, (logits, probabilities, state_value))",
            "",
            "return logits, probabilities, state_value"
        ]
    },
    {
        "number": 7744,
        "comments": "",
        "commit_message": "Add `combine` support to `MultiAggregation` (#5000)\n\n* Add combine supoort to MultiAggregation\n\n* changelog\n\n* fix\n\n* Add multi_aggr_kwargs in MessagePassing\n\n* Fix jit\n\n* update\n\n* update\n\n* fix\n\n* update\n\nCo-authored-by: Padarn Wilson <padarn.wilson@grabtaxi.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* update\n\n* fix\n\n* update\n\n* docs\n\n* Add attn combine and use dense reduce\n\n* fix\n\n* Fix docs and many typos\n\n* fix docs\n\n* Change kwwargs for MultiAggregation\n\n* update test\n\n* update\n\n* update\n\n* update\n\nCo-authored-by: Guohao Li <lighaime@gmail.com>\nCo-authored-by: Padarn Wilson <padarn.wilson@grabtaxi.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class MessagePassing(torch.nn.Module):",
            "edge_mask = self._edge_mask",
            "",
            "if edge_mask is None:",
            "-            raise ValueError(f\"Could not found a pre-defined 'edge_mask' as \"",
            "-                             f\"part of {self.__class__.__name__}\")",
            "+            raise ValueError(f\"Could not find a pre-defined 'edge_mask' as \"",
            "+                             f\"part of {self.__class__.__name__}.\")",
            "",
            "if self._apply_sigmoid:",
            "edge_mask = edge_mask.sigmoid()"
        ]
    },
    {
        "number": 7751,
        "comments": "",
        "commit_message": "compile works (#688)\n\n* compile works\n\n* runtimes\n\n* line count\n\n* fix custom, to tg dtype\n\n* meh, that's fine with lazy import\n",
        "label": "",
        "answer": "no",
        "change": [
            "torch_fxn_for_op: Dict[Op, Callable] = {**base_fxn_for_op, **{",
            "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else (\"mps\" if getenv(\"MPS\", 0) else \"cpu\"))",
            "class TorchBuffer(InterpretedBuffer):",
            "fxn_for_op: ClassVar = torch_fxn_for_op",
            "-  to_tinygrad_dtype = staticmethod(lambda lbuf: {torch.float16: dtypes.float16, torch.float32: dtypes.float32}[lbuf.dtype])",
            "+  def to_tinygrad_dtype(self): return {torch.float16: dtypes.float16, torch.float32: dtypes.float32}[self._buf.dtype]",
            "",
            "@staticmethod",
            "def fromCPU(x): return TorchBuffer(torch.from_numpy(x).requires_grad_(False).to(device))"
        ]
    },
    {
        "number": 7754,
        "comments": "",
        "commit_message": "fix typo pucuda.gl --> pycuda.gl (#1379)\n\nSummary:\nEvery time I try to run code, I get this warning:\n\n```\n  warnings.warn(\"Can't import pucuda.gl, not importing MeshRasterizerOpenGL.\")\n```\n\nOf course, `pucuda` is a typo of `pycuda`.\n\nThis PR fixes the typo\n\nPull Request resolved: https://github.com/facebookresearch/pytorch3d/pull/1379\n\nReviewed By: kjchalup\n\nDifferential Revision: D41295562\n\nPulled By: bottler\n\nfbshipit-source-id: 2bfa2a2dbe20a5347861d36fbff5094994c1253d\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def _can_import_egl_and_pycuda():",
            "try:",
            "import pycuda.gl",
            "except (ImportError, ImportError, ModuleNotFoundError):",
            "-        warnings.warn(\"Can't import pucuda.gl, not importing MeshRasterizerOpenGL.\")",
            "+        warnings.warn(\"Can't import pycuda.gl, not importing MeshRasterizerOpenGL.\")",
            "return False",
            "",
            "return True"
        ]
    },
    {
        "number": 7757,
        "comments": "",
        "commit_message": "Random erasing crash fix and args pass through\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class RandomErasingTorch:",
            "left = random.randint(0, img_w - w)",
            "if self.per_pixel:",
            "img[:, top:top + h, left:left + w] = torch.empty(",
            "-                            (chan, h, w), dtype=batch.dtype).cuda().normal_()",
            "+                            (chan, h, w), dtype=batch.dtype).normal_().cuda()",
            "else:",
            "img[:, top:top + h, left:left + w] = c",
            "break"
        ]
    },
    {
        "number": 7767,
        "comments": "",
        "commit_message": "* lint fixes\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TensorFlowEstimator(BaseEstimator):",
            "self._summaries = self._graph.get_operation_by_name('MergeSummary/MergeSummary')",
            "",
            "# Restore session.",
            "-            self._session = tf.Session(self.tf_master, config=ConfigAddon(verbose=self.verbose).config)",
            "+            self._session = tf.Session(",
            "+                self.tf_master,",
            "+                config=ConfigAddon(verbose=self.verbose).config)",
            "checkpoint_path = tf.train.latest_checkpoint(path)",
            "if checkpoint_path is None:",
            "raise ValueError(\"Missing checkpoint files in the %s. Please \""
        ]
    },
    {
        "number": 7774,
        "comments": "",
        "commit_message": "fix crash when num procs > 1 (#2623)\n\nSummary:\nthis fixes a crash when using num_workers > 0 and num processes > 1 (e.g. by launching on slurm with tasks-per-node > 1 or using torch.distributed/torchrun with num gpus > 1). the root cause is some kind of bug in torch.split() that holds on to memory. when data loaders recycle, the program crashes when trying to release already released memory\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2623\n\nReviewed By: arbabu123\n\nDifferential Revision: D32342431\n\nPulled By: alexeib\n\nfbshipit-source-id: 278c7e0b09cfd770f375865d472bc52fb5835bd2\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def all_reduce_dict(data: Mapping[str, Any], device, group) -> Dict[str, Any]:",
            "return data",
            "buf = torch.cat([t.view(-1) for t in data.values()]).to(device=device)",
            "all_reduce(buf, group=group)",
            "-        split_buf = torch.split(buf, [t.numel() for t in data.values()])",
            "+        split_buf = torch.split(buf.clone(), [t.numel() for t in data.values()])",
            "reduced_data = [t.view_as(orig) for t, orig in zip(split_buf, data.values())]",
            "return OrderedDict(zip(data.keys(), reduced_data))"
        ]
    },
    {
        "number": 7776,
        "comments": "",
        "commit_message": "Torch audio refactor (#1976)\n\n* Switched soundfile read for torch audio\n\n* Removed unused import\n\n* Working on test_experiment_audio_inputs\n\n* Fixed shape issue by indexing into tensor before preprocessing to padded matrix\n\n* Switched type setting to torch.float32\n\n* Removed soundfile dependency, added html read flow\n\n* Working on test_experiment_audio_inputs\n\n* Fixed shape issue by indexing into tensor before preprocessing to padded matrix\n\n* Added retry for flakey loads, catch for failed reads, and default audio logic for missing audio data\n\n* Fixed read audio logic\n\n* Fixed read audio logic again\n\n* Shortened error messages because they were redundant\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def generate_audio(feature):",
            "os.makedirs(destination_folder)",
            "",
            "audio_dest_path = os.path.join(destination_folder, audio_filename)",
            "-        soundfile.write(audio_dest_path, audio, sampling_rate)",
            "+        torchaudio.save(audio_dest_path, audio_tensor, sampling_rate)",
            "",
            "except OSError as e:",
            "raise OSError(\"Unable to create a folder for audio or save audio to disk.\" \"{}\".format(e))"
        ]
    },
    {
        "number": 7777,
        "comments": "",
        "commit_message": "fix metrics to ensure cpu vs gpu (#178)\n\n* fix metrics to ensure cpu vs gpu\n\n* factor out function\n\n* this time it should really be correct\n\n* switch copy_ to fill_\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class F1Measure(Metric):",
            "raise ConfigurationError(\"A gold label passed to F1Measure contains an id >= {}, \"",
            "\"the number of classes.\".format(num_classes))",
            "if mask is None:",
            "-            mask = torch.ones(gold_labels.size())",
            "+            mask = ones_like(gold_labels)",
            "mask = mask.float()",
            "gold_labels = gold_labels.float()",
            "positive_label_mask = gold_labels.eq(self._positive_label).float()"
        ]
    },
    {
        "number": 7778,
        "comments": "",
        "commit_message": "pylint -> flake8 (#3288)\n\n* pylint\n\n* update pylint\n\n* undo a lot of the raise / else\n\n* add bound on typed-ast\n\n* first mypy fixes\n\n* new flag\n\n* fix mypy errors\n\n* requirements.txt\n\n* pylint -> flake8\n\n* mypy 0.720 -> mypy 0.730\n\n* add back erroneously removed initial newline\n\n* remove .pylintrc\n\n* remove pylintrc from Dockerfile\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class ScalarMix(torch.nn.Module):",
            "in range(mixture_size)])",
            "self.gamma = Parameter(torch.FloatTensor([1.0]), requires_grad=trainable)",
            "",
            "-    def forward(self, tensors: List[torch.Tensor],  # pylint: disable=arguments-differ",
            "+    def forward(self, tensors: List[torch.Tensor],",
            "mask: torch.Tensor = None) -> torch.Tensor:",
            "\"\"\"",
            "Compute a weighted average of the ``tensors``.  The input tensors an be any shape"
        ]
    },
    {
        "number": 7781,
        "comments": "",
        "commit_message": "Fix default image on image read failure (#3073)\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class ImageInputFeature(ImageFeatureMixin, InputFeature):",
            ")",
            "",
            "def forward(self, inputs: torch.Tensor) -> torch.Tensor:",
            "-        assert isinstance(inputs, torch.Tensor)",
            "-        assert inputs.dtype in [torch.float32]",
            "+        assert isinstance(inputs, torch.Tensor), f\"inputs to image feature must be a torch tensor, got {type(inputs)}\"",
            "+        assert inputs.dtype in [torch.float32], f\"inputs to image feature must be a float32 tensor, got {inputs.dtype}\"",
            "",
            "inputs_encoded = self.encoder_obj(inputs)"
        ]
    },
    {
        "number": 7784,
        "comments": "",
        "commit_message": "Fix example USE semantic similarity calculation\n\nPiperOrigin-RevId: 323331362\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "\"  sts_encode2 = tf.nn.l2_normalize(embed(tf.constant(batch['sent_2'].tolist())), axis=1)\\n\",",
            "\"  cosine_similarities = tf.reduce_sum(tf.multiply(sts_encode1, sts_encode2), axis=1)\\n\",",
            "\"  clip_cosine_similarities = tf.clip_by_value(cosine_similarities, -1.0, 1.0)\\n\",",
            "-        \"  scores = 1.0 - tf.acos(clip_cosine_similarities)\\n\",",
            "+        \"  scores = 1.0 - tf.acos(clip_cosine_similarities) / math.pi\\n\",",
            "\"  \\\"\\\"\\\"Returns the similarity scores\\\"\\\"\\\"\\n\",",
            "\"  return scores\\n\",",
            "\"\\n\","
        ]
    },
    {
        "number": 7789,
        "comments": "",
        "commit_message": "fix: to_undirected when num_nodes == 2 (#3627) (#3628)\n\n* fix: to_undirected when num_nodes == 2\n\n* fix\n\n* linting\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def test_hetero_to_undirected():",
            "data['v', 'w'].edge_weight = edge_weight",
            "data['v', 'w'].edge_attr = edge_attr",
            "",
            "+    from torch_geometric.transforms import ToUndirected",
            "data = ToUndirected()(data)",
            "-    assert data['v', 'v'].edge_index.tolist() == [[0, 0, 1, 2, 2, 3],",
            "-                                                  [1, 2, 0, 0, 3, 2]]",
            "+    assert data['v', 'v'].edge_index.tolist() == [[0, 1, 2, 3], [1, 0, 3, 2]]",
            "assert data['v', 'v'].edge_weight.tolist() == edge_weight[perm].tolist()",
            "assert data['v', 'v'].edge_attr.tolist() == edge_attr[perm].tolist()",
            "assert data['v', 'w'].edge_index.tolist() == edge_index.tolist()",
            "assert data['v', 'w'].edge_weight.tolist() == edge_weight.tolist()",
            "assert data['v', 'w'].edge_attr.tolist() == edge_attr.tolist()",
            "-    assert data['w', 'v'].edge_index.tolist() == [[3, 1, 0], [2, 0, 2]]",
            "+    assert data['w', 'v'].edge_index.tolist() == [[3, 1], [2, 0]]",
            "assert data['w', 'v'].edge_weight.tolist() == edge_weight.tolist()",
            "assert data['w', 'v'].edge_attr.tolist() == edge_attr.tolist()"
        ]
    },
    {
        "number": 7791,
        "comments": "",
        "commit_message": " Upgrade to modern Python syntax (#1213)\n\n* Upgrade to modern Python syntax\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Use f-strings\n\n* Placate DeepSource Python\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def joint_pdf(kernel_values1: torch.Tensor, kernel_values2: torch.Tensor, epsilo",
            "\"\"\"",
            "",
            "if not isinstance(kernel_values1, torch.Tensor):",
            "-        raise TypeError(\"Input kernel_values1 type is not a torch.Tensor. Got {}\".format(type(kernel_values1)))",
            "+        raise TypeError(f\"Input kernel_values1 type is not a torch.Tensor. Got {type(kernel_values1)}\")",
            "",
            "if not isinstance(kernel_values2, torch.Tensor):",
            "-        raise TypeError(\"Input kernel_values2 type is not a torch.Tensor. Got {}\".format(type(kernel_values2)))",
            "+        raise TypeError(f\"Input kernel_values2 type is not a torch.Tensor. Got {type(kernel_values2)}\")",
            "",
            "if not kernel_values1.dim() == 3:",
            "raise ValueError(\"Input kernel_values1 must be a of the shape BxN.\" \" Got {}\".format(kernel_values1.shape))"
        ]
    },
    {
        "number": 7794,
        "comments": "",
        "commit_message": "[torch] Enable GPU training (#1508)\n\n* Added more descriptive description_summary, fixed bug in TabNet combiner\n\n* Updated modules to do proper GPU conversion\n\n* Functional GPU training\n\n* Tabnet working, removed RayRemoteTrainer\n\n* Temporary changes to unblock GPU training\n\n* Functional test_seq_encoder\n\n* Cleaned up PR\n\n* Fixed failing CPU tests\n\n* Responded to comments\n\n* Fix failing GPU tests in test_api.py\n\n* [torch] Fix GPU tests for test_visualizations (#1539)\n\nUpdated torch.aranges to registered buffers in embedding_modules.py\n\n* [torch] Fix test_encoder.py for GPU (#1544)\n\n* Accept and move model to GPU as part of Predictor init (#1559)\n\n* [torch] Fixed tests in test_experiment.py (#1562)\n\n* Fixed tests in tests/integration_tests/test_experiment.py\n\n* Moved set embed and inputs to correct device in tests/ludwig/encoders/test_set_encoders.py\n\n* Moved set date and inputs to correct device in tests/ludwig/encoders/test_date_encoders.py\n\n* Moved bag embed and inputs to correct device in tests/ludwig/encoders/test_bag_encoders.py\n\n* Moved sequence embed and inputs to correct device in tests/ludwig/features/test_sequence_features.py\n\n* [torch] Fixed GPU issues in combiners.py (#1568)\n\n* Fix GPU tests in test_collect.py (#1569)\n\n* Added model to Predictor initialization\n\n* Fixed embedding modules tests (#1554)\n\n* Fixed embedding modules tests\n* Moved embedding modules to device in tests, removed duplicate argument\n\nCo-authored-by: Shreya Rajpal <shreya.rajpal@gmail.com>\n\n* Fixed encoders tests except test_h3_rnn_embed (#1556)\n\n* Fixed encoders tests except test_h3_rnn_embed\n\n* Addressed Shreya's comments\n\n* Moved embedding modules to device, used registered buffers\n\nCo-authored-by: Shreya Rajpal <shreya.rajpal@gmail.com>\n\n* Pass ECD object to Predictor\n\n* Removed model arg from predict function\n\nCo-authored-by: Jeffrey Tang <jeff@predibase.com>\nCo-authored-by: Jeffrey Tang <810895+jeffreyftang@users.noreply.github.com>\nCo-authored-by: Animesh Kumar <anmshkmr@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class CategoryInputFeature(CategoryFeatureMixin, InputFeature):",
            "inputs = inputs.unsqueeze(dim=1)",
            "",
            "if inputs.dtype == torch.int8 or inputs.dtype == torch.int16:",
            "-            inputs = inputs.type(torch.IntTensor)",
            "+            inputs = inputs.type(torch.int)",
            "encoder_output = self.encoder_obj(inputs)",
            "",
            "return {\"encoder_output\": encoder_output}"
        ]
    },
    {
        "number": 7797,
        "comments": "",
        "commit_message": "fix a missing tensor for visualization (fix #530)\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def generate_rpn_proposals(boxes, scores, img_shape):",
            "topk_valid_boxes,",
            "nms_indices, name='boxes')",
            "final_scores = tf.gather(topk_valid_scores, nms_indices, name='scores')",
            "+    tf.sigmoid(final_scores, name='probs')  # for visualization",
            "return final_boxes, final_scores"
        ]
    },
    {
        "number": 7799,
        "comments": "",
        "commit_message": "Fix pruners (#2153)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class L2FilterPruner(WeightRankFilterPruner):",
            "w_l2_norm = torch.sqrt((w ** 2).sum(dim=1))",
            "threshold = torch.topk(w_l2_norm.view(-1), num_prune, largest=False)[0].max()",
            "mask_weight = torch.gt(w_l2_norm, threshold)[:, None, None, None].expand_as(weight).type_as(weight)",
            "-        mask_bias = torch.gt(w_l2_norm, threshold).type_as(weight)",
            "+        mask_bias = torch.gt(w_l2_norm, threshold).type_as(weight).detach() if base_mask['bias_mask'] is not None else None",
            "",
            "-        return {'weight_mask': mask_weight.detach(), 'bias_mask': mask_bias.detach()}",
            "+        return {'weight_mask': mask_weight.detach(), 'bias_mask': mask_bias}",
            "",
            "",
            "class FPGMPruner(WeightRankFilterPruner):"
        ]
    },
    {
        "number": 7800,
        "comments": "",
        "commit_message": "Fixed CUDA/CPU drop_connect bug\n",
        "label": "",
        "answer": "no",
        "change": [
            "def drop_connect(inputs, p, training):",
            "batch_size = inputs.shape[0]",
            "keep_prob = 1 - p",
            "random_tensor = keep_prob",
            "-    random_tensor += torch.rand([batch_size, 1, 1, 1], dtype=inputs.dtype)  # uniform [0,1)",
            "+    random_tensor += torch.rand([batch_size, 1, 1, 1], dtype=inputs.dtype, device=inputs.device)",
            "binary_tensor = torch.floor(random_tensor)",
            "output = inputs / keep_prob * binary_tensor",
            "return output"
        ]
    },
    {
        "number": 7808,
        "comments": "",
        "commit_message": "tl.layers API Refactoring and various modifications (#667)\n\n* Decorators API Refactored\n\n* extra_requires `all`, `all_cpu` and `all_gpu` added\n\n* Error fix\n\n* YAPF Formating Correction\n\n* Test for private method decorator added\n\n* Test Logging Verbosity Fixed to Debug when runned individually\n\n* YAPF corrections applied\n\n* Changelog Added\n\n* Changelog updated\n\n* PR number changed\n\n* First Refactoring Pass done\n\n* cleaning second pass\n\n* Refactoring 3rd pass\n\n* Refactoring 4th Pass\n\n* Code Error fix\n\n* YAPF Formating Fix\n\n* Arguments now using self\n\n* YAPF error correction\n\n* Bug Fix in Decorator\n\n* act name bug fix\n\n* Error Correction\n\n* YAPF formating fix\n\n* Useless tf.identity removed\n\n* Error Fix\n\n* Changelog Updated\n\n* Error fix in tl.activation\n\n* Documentation error fix\n\n* Lazy Import added\n\n* Import Refactoring with LazyImport when necessary\n\n* Changelog Updated\n\n* Gitter Removed\n\n* Fixed proposed by @zsdonghao\n\n* Documentation updated\n\n* Missing requirements added\n\n* Update to TensorLayer 1.8.6rc1\n\n* Requirements error fix\n\n* Docker Files updated\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def slim_block(x):",
            "x = slim.dropout(x, 0.5, is_training=is_training)",
            "x = slim.fully_connected(x, 800, activation_fn=tf.nn.relu)",
            "x = slim.dropout(x, 0.5, is_training=is_training)",
            "-        logits = slim.fully_connected(x, 10, activation_fn=tf.identity)",
            "+        logits = slim.fully_connected(x, 10, activation_fn=None)",
            "return logits, {}"
        ]
    },
    {
        "number": 7809,
        "comments": "",
        "commit_message": "ONNX export bug fix #93\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "if __name__ == '__main__':",
            "",
            "# Load pytorch model",
            "google_utils.attempt_download(opt.weights)",
            "-    model = torch.load(opt.weights, map_location=torch.device('cpu'))['model']",
            "+    model = torch.load(opt.weights, map_location=torch.device('cpu'))['model'].float()",
            "model.eval()",
            "model.fuse()"
        ]
    },
    {
        "number": 7814,
        "comments": "",
        "commit_message": "Fix MHA and LayerNorm on devices other than cuda:0 (fixes #1860) (#1873)\n\nSummary:\nThis also includes a fix required for LayerNorm due to a bug in apex: https://github.com/NVIDIA/apex/issues/770\nPull Request resolved: https://github.com/pytorch/fairseq/pull/1873\n\nReviewed By: ngoyal2707\n\nDifferential Revision: D20564542\n\nPulled By: myleott\n\nfbshipit-source-id: f1603779351671a5a44f606f550281bcdd2aa9b7\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "try:",
            "class FusedLayerNorm(_FusedLayerNorm):",
            "@torch.jit.unused",
            "def forward(self, x):",
            "-            return super().forward(x)",
            "+            with torch.cuda.device(x.device):",
            "+                return super().forward(x)",
            "",
            "except ImportError:",
            "has_fused_layernorm = False"
        ]
    },
    {
        "number": 7815,
        "comments": "",
        "commit_message": "fixes from mari-linhares feedback\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestSerde(object):",
            "",
            "@pytest.mark.parametrize(\"compress\", [True, False])",
            "def test_torch_Tensor_convenience(self, compress):",
            "+        \"\"\"This test evaluates torch.Tensor.serialize()",
            "+",
            "+        As opposed to using syft.serde.serialize(), torch objects",
            "+        have a convenience function which lets you call .serialize()",
            "+        directly on the tensor itself. This tests to makes sure it",
            "+        works correctly.\"\"\"",
            "+",
            "hook = TorchHook(torch)",
            "",
            "t = Tensor(numpy.random.random((100, 100)))"
        ]
    },
    {
        "number": 7821,
        "comments": "",
        "commit_message": "fixed wrong sum axis\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def test_forward(use_token_averaged_energy):",
            ")",
            "xs = torch.randn(2, 256)",
            "if not use_token_averaged_energy:",
            "-        layer(xs, torch.LongTensor([256, 128]))",
            "+        es, elens = layer(xs, torch.LongTensor([256, 128]))",
            "+        assert es.shape[1] == max(elens)",
            "else:",
            "ds = torch.LongTensor([[3, 0, 2], [3, 0, 0]])",
            "dlens = torch.LongTensor([3, 1])"
        ]
    },
    {
        "number": 7823,
        "comments": "",
        "commit_message": "Fix forward in EndpointSpanExtractor (#3042)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class EndpointSpanExtractor(SpanExtractor):",
            "span_widths = span_ends - span_starts",
            "",
            "span_width_embeddings = self._span_width_embedding(span_widths)",
            "-            return torch.cat([combined_tensors, span_width_embeddings], -1)",
            "+            combined_tensors = torch.cat([combined_tensors, span_width_embeddings], -1)",
            "",
            "if span_indices_mask is not None:",
            "return combined_tensors * span_indices_mask.unsqueeze(-1).float()",
            "+",
            "return combined_tensors"
        ]
    },
    {
        "number": 7824,
        "comments": "",
        "commit_message": "Fix tapas issue (#12063)\n\n* Fix scatter function to be compatible with torch-scatter 2.7.0\n\n* Allow test again\n",
        "label": "",
        "answer": "no",
        "change": [
            "def _segment_reduce(values, index, segment_reduce_fn, name):",
            "",
            "segment_means = scatter(",
            "src=flat_values,",
            "-        index=flat_index.indices.type(torch.long),",
            "+        index=flat_index.indices.long(),",
            "dim=0,",
            "-        dim_size=flat_index.num_segments,",
            "+        dim_size=int(flat_index.num_segments),",
            "reduce=segment_reduce_fn,",
            ")"
        ]
    },
    {
        "number": 7825,
        "comments": "",
        "commit_message": "Fix sonnet/v2/src/nets/dnc:util_test_gpu test\n\nPiperOrigin-RevId: 362986883\nChange-Id: If791985603e97af2f0bca8c2255a2ba995f27b28\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class LinearTest(test_utils.TestCase, parameterized.TestCase):",
            "lin_b = linear.Linear(output_size_b, name='lin_b')",
            "input_a = tf.random.uniform([batch_size, input_size])",
            "input_b = tf.random.uniform([batch_size, input_size])",
            "-    with self.assertRaisesRegexp(tf.errors.InvalidArgumentError,",
            "-                                 'Incompatible shapes'):",
            "+    with self.assertRaisesIncompatibleShapesError(",
            "+        tf.errors.InvalidArgumentError):",
            "util.apply_linear((input_a, input_b), (lin_a, lin_b))",
            "",
            "@parameterized.parameters("
        ]
    },
    {
        "number": 7831,
        "comments": "",
        "commit_message": "[Feat] Enabled doctest for CI (#641)\n\n* Added doctest for CI and validated docs\n\n* Fixed doctests\n\n* Fixed flake8\n\n* test doctest ci\n",
        "label": "",
        "answer": "no",
        "change": [
            "class PSNRLoss(nn.Module):",
            "- output: :math:`()` a scalar",
            "",
            "Examples:",
            "-        >>> kornia.losses.psnr_loss(torch.ones(1), 1.2*torch.ones(1), 2)",
            "-        tensor(20.0000) # 10 * log(4/((1.2-1)**2)) / log(10)",
            "+        >>> psnr_loss(torch.ones(1), 1.2*torch.ones(1), 2) # 10 * log(4/((1.2-1)**2)) / log(10)",
            "+        tensor(20.0000)",
            "",
            "Reference:",
            "https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio#Definition"
        ]
    },
    {
        "number": 7834,
        "comments": "",
        "commit_message": "Update Biosses (#3054)\n\n* add swedish_medical_ner dataset\n\n* update swedish_medical_ner\n\n* fix biosses variable names\n\nCo-authored-by: Quentin Lhoest <42851186+lhoestq@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Biosses(datasets.GeneratorBasedBuilder):",
            "df = pd.read_csv(filepath, sep=\"\\t\", encoding=\"utf-8\")",
            "for idx, row in df.iterrows():",
            "yield idx, {",
            "-                \"sentence 1\": row[\"sentence1\"],",
            "-                \"sentence 2\": row[\"sentence2\"],",
            "+                \"sentence1\": row[\"sentence1\"],",
            "+                \"sentence2\": row[\"sentence2\"],",
            "\"score\": row[\"score\"],",
            "}"
        ]
    },
    {
        "number": 7843,
        "comments": "",
        "commit_message": "Fix BART tests on GPU (#4298)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class BartForConditionalGeneration(PretrainedBartModel):",
            "if new_num_tokens <= old_num_tokens:",
            "new_bias = self.final_logits_bias[:, :new_num_tokens]",
            "else:",
            "-            extra_bias = torch.zeros((1, new_num_tokens - old_num_tokens))",
            "+            extra_bias = torch.zeros((1, new_num_tokens - old_num_tokens), device=self.final_logits_bias.device)",
            "new_bias = torch.cat([self.final_logits_bias, extra_bias], dim=1)",
            "self.register_buffer(\"final_logits_bias\", new_bias)"
        ]
    },
    {
        "number": 7846,
        "comments": "",
        "commit_message": "Fixed label datatype for STS-B (#6492)\n\n* fixed label datatype for sts-b\n\n* naming update\n\n* make style\n\n* make style\n",
        "label": "",
        "answer": "no",
        "change": [
            "if is_tf_available():",
            "",
            "return tf.data.Dataset.from_generator(",
            "gen,",
            "-            ({k: tf.int32 for k in input_names}, tf.int64),",
            "+            ({k: tf.int32 for k in input_names}, label_type),",
            "({k: tf.TensorShape([None]) for k in input_names}, tf.TensorShape([])),",
            ")"
        ]
    },
    {
        "number": 7863,
        "comments": "",
        "commit_message": "[feat] Added mix augmentations in containers (#1139)\n\n* Enabled mix augmentations\n\n* Added mix augmentations for video data.\n\n* Enabled None label for mix augmentatons\n\n* init patch mix\n\n* Get it run\n\n* Get it worked\n\n* Bug locate\n\n* Samall update\n\n* ready for refactor\n\n* refactor\n\n* Update\n\n* update\n\n* Refactor init\n\n* Fixed everything\n\n* Message refine\n\n* Bug fix\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Code refactoring\n\n* bug fix\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fixed float64 bug, removed GaussianBlur annoying warning, and lint\n\n* Improved pattern\n\n* Fixed cuda\n\n* Added backward compatibility to torch 1.6\n\n* Improved docs\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Refactored code\n\n* Added more tests\n\n* bug fix\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class RandomMixUp(MixAugmentationBase):",
            "return rg.random_mixup_generator(batch_shape[0], self.p, lambda_val, same_on_batch=self.same_on_batch)",
            "",
            "def apply_transform(  # type: ignore",
            "-        self, input: torch.Tensor, label: torch.Tensor, params: Dict[str, torch.Tensor]  # type: ignore",
            "+        self, input: torch.Tensor, label: torch.Tensor, params: Dict[str, torch.Tensor]",
            ") -> Tuple[torch.Tensor, torch.Tensor]:",
            "input_permute = input.index_select(dim=0, index=params['mixup_pairs'].to(input.device))",
            "labels_permute = label.index_select(dim=0, index=params['mixup_pairs'].to(label.device))"
        ]
    },
    {
        "number": 7866,
        "comments": "",
        "commit_message": "fixed transform tests\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "from .degree import DegreeAdj",
            "",
            "class DegreeTest(TestCase):",
            "def test_degree_adj(self):",
            "+        index = torch.LongTensor([[0, 0, 1, 2], [1, 2, 0, 1]])",
            "weight = torch.FloatTensor([2, 3, 4, 6])",
            "-        edge = torch.LongTensor([[0, 0, 1, 2], [1, 2, 0, 1]])",
            "-        adj = torch.sparse.FloatTensor(edge, weight, torch.Size([3, 3]))",
            "+        adj = torch.sparse.FloatTensor(index, weight, torch.Size([3, 3]))",
            "",
            "transform = DegreeAdj()",
            "",
            "-        _, adj = transform((None, adj))",
            "+        _, adj, _ = transform((None, adj, None))",
            "adj = adj.to_dense()",
            "",
            "expected_adj_out = ["
        ]
    },
    {
        "number": 7870,
        "comments": "",
        "commit_message": "Fix tests\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class StackBidirectionalRNNEncoderTest(tf.test.TestCase):",
            "[self.batch_size, cell.output_size])",
            "",
            "def test_encode_with_multi_cell(self):",
            "-    cell = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.LSTMCell(32)] * 4)",
            "+    cell = tf.contrib.rnn.MultiRNNCell(",
            "+        [tf.contrib.rnn.LSTMCell(32) for _ in range(4)])",
            "encoder_output_ = self._test_encode_with_cell(cell)",
            "",
            "for layer_idx in range(4):"
        ]
    },
    {
        "number": 7871,
        "comments": "",
        "commit_message": "Fix examples and tutorials to use the updated tensor API (#886)\n\n* Fix examples and tutorials to use the updated tensor API\n\n* small fixes\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "\"Conceptually `iarange` is the same as `irange` except that it is a vectorized operation (as `torch.arange` is to `range`). As such it potentially enables large speed-ups compared to the explicit `for` loop that appears with `irange`. Let's see how this looks for our running example. First we need `data` to be in the form of a tensor:\\n\",",
            "\"\\n\",",
            "\"```python\\n\",",
            "-    \"data = Variable(torch.zeros(10, 1))\\n\",",
            "+    \"data = torch.zeros(10, 1)\\n\",",
            "\"data[0:6, 0] = torch.ones(6)  # 6 heads and 4 tails\\n\",",
            "\"```\\n\",",
            "\"\\n\","
        ]
    },
    {
        "number": 7874,
        "comments": "",
        "commit_message": "support torch 1.4.0 (OpenMined#4963) (#4982)\n\n* support torch 1.4.0 (OpenMined#4963)\n\n* Fixed namedtuple test for torch==1.4.0\n\n- Updated README.md's with 1.4\n- Linting\n\n* Adding 1.4.0 to CI\n\n* Changed 1.4.0 in CI back to just the target system Ubuntu python 3.7\n\n- We need this for a specific application so this will keep CI more free\n\n* Windows still occasionally causing OOM errors\n\n- Adding a 7th chunk\n\n* Added asyncio to sympc test\n\nCo-authored-by: Madhava Jay <me@madhavajay.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "from syft.core.common.serde.deserialize import _deserialize",
            "from syft.lib.python import ValuesIndices",
            "",
            "",
            "-def test_torch_valuesindices_serde():",
            "-    x = torch.Tensor([1, 2, 3])",
            "-    y = x.cummax(0)",
            "+def test_torch_valuesindices_serde() -> None:",
            "+    x = torch.Tensor([[1, 2], [1, 2]])",
            "+    y = x.mode()",
            "values = y.values",
            "indices = y.indices"
        ]
    },
    {
        "number": 7875,
        "comments": "",
        "commit_message": "fix minor bug\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class _ProposalTargetLayer(nn.Module):",
            "num_proposal = overlaps.size(1)",
            "num_boxes_per_img = overlaps.size(2)",
            "",
            "-        offset = torch.arange(0, batch_size)*20",
            "+        offset = torch.arange(0, batch_size)*gt_boxes.size(1)",
            "offset = offset.view(-1, 1).type_as(gt_assignment) + gt_assignment",
            "",
            "labels = gt_boxes[:,:,4].contiguous().view(-1).index(offset.view(-1))\\"
        ]
    },
    {
        "number": 7878,
        "comments": "",
        "commit_message": "add compat.is_gpu_available(); fix test failure\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def start_cluster_server(ctx, num_gpus=1, rdma=False):",
            "cluster_spec = ctx.cluster_spec",
            "logging.info(\"{0}: Cluster spec: {1}\".format(ctx.worker_num, cluster_spec))",
            "",
            "-  if tf.test.is_built_with_cuda() and num_gpus > 0:",
            "+  if compat.is_gpu_available() and num_gpus > 0:",
            "# compute my index relative to other nodes placed on the same host (for GPU allocation)",
            "my_addr = cluster_spec[ctx.job_name][ctx.task_index]",
            "my_host = my_addr.split(':')[0]"
        ]
    },
    {
        "number": 7881,
        "comments": "",
        "commit_message": "improved global tensor handling, various other fixes\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Evolutionary(Optimizer):",
            "",
            "return deltas_sum, perturbations",
            "",
            "-            deltas_sum, perturbations = tf.while_loop(",
            "+            deltas_sum, perturbations = self.while_loop(",
            "cond=util.tf_always_true, body=body, loop_vars=(deltas_sum, perturbations),",
            "maximum_iterations=(self.num_samples - 1)",
            ")"
        ]
    },
    {
        "number": 7887,
        "comments": "",
        "commit_message": "[Fix] Increase speed of transform_points (#687)\n\n* Faster projection with broadcasting\n\n* transform_points modification\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestRandomCutMix:",
            "",
            "assert_allclose(out_image, expected, rtol=1e-4, atol=1e-4)",
            "assert (out_label[0, :, 0] == label).all()",
            "-        assert (out_label[0, :, 1] == torch.tensor([0, 1])).all()",
            "+        assert (out_label[0, :, 1] == torch.tensor([0, 1], device=device, dtype=dtype)).all()",
            "# cut area = 4 / 12",
            "-        assert_allclose(out_label[0, :, 2], torch.tensor([0.33333, 0.33333], dtype=dtype))",
            "+        assert_allclose(out_label[0, :, 2], torch.tensor([0.33333, 0.33333], device=device, dtype=dtype))",
            "",
            "def test_random_mixup_num2(self, device, dtype):",
            "torch.manual_seed(76)"
        ]
    },
    {
        "number": 7890,
        "comments": "",
        "commit_message": "fixed failing core.container unit tests.\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "from ivy.core.device import default_device",
            "",
            "",
            "def random_uniform(low=0., high=1., shape=None, dev=None):",
            "-    dev = default_device(dev)",
            "-    with _tf.device('/' + dev.upper()):",
            "+    with _tf.device(default_device(dev)):",
            "return _tf.random.uniform(shape if shape else (), low, high)"
        ]
    },
    {
        "number": 7898,
        "comments": "",
        "commit_message": "fix: minor\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def train(args, logdir):",
            "# dataflow",
            "df = Net1DataFlow(hp.train1.data_path, hp.train1.batch_size)",
            "",
            "-    ckpt1 = tf.train.latest_checkpoint(logdir1)",
            "-",
            "# set logger for event and model saver",
            "logger.set_logger_dir(logdir)"
        ]
    },
    {
        "number": 7904,
        "comments": "",
        "commit_message": "tl.layers API Refactoring and various modifications (#667)\n\n* Decorators API Refactored\n\n* extra_requires `all`, `all_cpu` and `all_gpu` added\n\n* Error fix\n\n* YAPF Formating Correction\n\n* Test for private method decorator added\n\n* Test Logging Verbosity Fixed to Debug when runned individually\n\n* YAPF corrections applied\n\n* Changelog Added\n\n* Changelog updated\n\n* PR number changed\n\n* First Refactoring Pass done\n\n* cleaning second pass\n\n* Refactoring 3rd pass\n\n* Refactoring 4th Pass\n\n* Code Error fix\n\n* YAPF Formating Fix\n\n* Arguments now using self\n\n* YAPF error correction\n\n* Bug Fix in Decorator\n\n* act name bug fix\n\n* Error Correction\n\n* YAPF formating fix\n\n* Useless tf.identity removed\n\n* Error Fix\n\n* Changelog Updated\n\n* Error fix in tl.activation\n\n* Documentation error fix\n\n* Lazy Import added\n\n* Import Refactoring with LazyImport when necessary\n\n* Changelog Updated\n\n* Gitter Removed\n\n* Fixed proposed by @zsdonghao\n\n* Documentation updated\n\n* Missing requirements added\n\n* Update to TensorLayer 1.8.6rc1\n\n* Requirements error fix\n\n* Docker Files updated\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def Vgg19_simple_api(rgb):",
            "net = FlattenLayer(net, name='flatten')",
            "net = DenseLayer(net, n_units=4096, act=tf.nn.relu, name='fc6')",
            "net = DenseLayer(net, n_units=4096, act=tf.nn.relu, name='fc7')",
            "-    net = DenseLayer(net, n_units=1000, act=tf.identity, name='fc8')",
            "+    net = DenseLayer(net, n_units=1000, act=None, name='fc8')",
            "print(\"build model finished: %fs\" % (time.time() - start_time))",
            "return net"
        ]
    },
    {
        "number": 7909,
        "comments": "",
        "commit_message": "fix style\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class LogMel(torch.nn.Module):",
            "return ', '.join(f'{k}={v}' for k, v in self.mel_options.items())",
            "",
            "def forward(",
            "-        self, feat: torch.Tensor, ilens: torch.Tensor=None,",
            "+        self, feat: torch.Tensor, ilens: torch.Tensor = None,",
            ") -> Tuple[torch.Tensor, torch.Tensor]:",
            "# feat: (B, T, D1) x melmat: (D1, D2) -> mel_feat: (B, T, D2)",
            "mel_feat = torch.matmul(feat, self.melmat)"
        ]
    },
    {
        "number": 7912,
        "comments": "",
        "commit_message": "Fix trainer logging_nan_inf_filter in torch_xla mode (#13896)\n\n* Fix logging_nan_inf_filter in torch_xla mode\n\n* Update src/transformers/trainer.py\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n\n* Fix format\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Trainer:",
            "else:",
            "tr_loss_step = self.training_step(model, inputs)",
            "",
            "-                if args.logging_nan_inf_filter and (torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step)):",
            "-                    # if loss is nan or inf simply add the average of previous logged losses",
            "-                    tr_loss += tr_loss / (1 + self.state.global_step - self._globalstep_last_logged)",
            "+                if args.logging_nan_inf_filter and not is_torch_tpu_available():",
            "+                    if torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step):",
            "+                        # if loss is nan or inf simply add the average of previous logged losses",
            "+                        tr_loss += tr_loss / (1 + self.state.global_step - self._globalstep_last_logged)",
            "else:",
            "tr_loss += tr_loss_step"
        ]
    },
    {
        "number": 7915,
        "comments": "",
        "commit_message": "weight_decay fix\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def train():",
            "",
            "# Compute loss",
            "loss, loss_items = compute_loss(pred, targets, model)",
            "-            if torch.isnan(loss):",
            "-                print('WARNING: nan loss detected, skipping batch ', loss_items)",
            "+            if not torch.isfinite(loss):",
            "+                print('WARNING: non-finite loss, skipping batch ', loss_items)",
            "continue",
            "",
            "# Scale loss by nominal batch_size of 64"
        ]
    },
    {
        "number": 7917,
        "comments": "",
        "commit_message": "Fix grad_norm handling\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Trainer:",
            "optimizer.step()",
            "",
            "# pytorch skips the step when the norm is 0. So ignore the norm value when it is NaN",
            "-        if isinstance(grad_norm ,torch.Tensor) and (torch.isnan(grad_norm) or torch.isinf(grad_norm)):",
            "+        if isinstance(grad_norm, torch.Tensor) and (torch.isnan(grad_norm) or torch.isinf(grad_norm)):",
            "grad_norm = 0",
            "",
            "step_time = time.time() - step_start_time"
        ]
    },
    {
        "number": 7920,
        "comments": "",
        "commit_message": "fixed trpo\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class NaturalGradient(Optimizer):",
            "kldiv_gradients = [",
            "tf.convert_to_tensor(value=grad) for grad in tf.gradients(ys=kldiv, xs=variables)",
            "]",
            "-        # if not all(isinstance(grad, tf.Tensor)):  warning!!!",
            "",
            "# Calculates the product x * F of a given vector x with the fisher matrix F.",
            "# Incorporating the product prevents having to calculate the entire matrix explicitly."
        ]
    },
    {
        "number": 7923,
        "comments": "",
        "commit_message": "CDN urls (#4030)\n\n* [file_utils] use_cdn + documentation\n\n* Move to cdn. urls for weights\n\n* [urls] Hotfix for bert-base-japanese\n",
        "label": "",
        "answer": "no",
        "change": [
            "from .modeling_xlm import (",
            "logger = logging.getLogger(__name__)",
            "",
            "FLAUBERT_PRETRAINED_MODEL_ARCHIVE_MAP = {",
            "-    \"flaubert-small-cased\": \"https://s3.amazonaws.com/models.huggingface.co/bert/flaubert/flaubert_small_cased/pytorch_model.bin\",",
            "-    \"flaubert-base-uncased\": \"https://s3.amazonaws.com/models.huggingface.co/bert/flaubert/flaubert_base_uncased/pytorch_model.bin\",",
            "-    \"flaubert-base-cased\": \"https://s3.amazonaws.com/models.huggingface.co/bert/flaubert/flaubert_base_cased/pytorch_model.bin\",",
            "-    \"flaubert-large-cased\": \"https://s3.amazonaws.com/models.huggingface.co/bert/flaubert/flaubert_large_cased/pytorch_model.bin\",",
            "+    \"flaubert-small-cased\": \"https://cdn.huggingface.co/flaubert/flaubert_small_cased/pytorch_model.bin\",",
            "+    \"flaubert-base-uncased\": \"https://cdn.huggingface.co/flaubert/flaubert_base_uncased/pytorch_model.bin\",",
            "+    \"flaubert-base-cased\": \"https://cdn.huggingface.co/flaubert/flaubert_base_cased/pytorch_model.bin\",",
            "+    \"flaubert-large-cased\": \"https://cdn.huggingface.co/flaubert/flaubert_large_cased/pytorch_model.bin\",",
            "}"
        ]
    },
    {
        "number": 7924,
        "comments": "",
        "commit_message": "[Fix] Fix solov2 cannot dealing with empty gt image (#9185)\n\n* [Fix] Fix solov2 cannot dealing with empty gt image\n\n* minor fix\n",
        "label": "",
        "answer": "no",
        "change": [
            "class SOLOV2Head(SOLOHead):",
            "if num_pos > 0:",
            "loss_mask = torch.cat(loss_mask).sum() / num_pos",
            "else:",
            "-            loss_mask = torch.cat(loss_mask).mean()",
            "+            loss_mask = mask_feats.sum() * 0",
            "",
            "# cate",
            "flatten_labels = ["
        ]
    },
    {
        "number": 7925,
        "comments": "",
        "commit_message": "bug fixes\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Decoder(nn.Module):",
            "else:",
            "stop_token = self.stopnet(stopnet_input)",
            "output = output[:, : self.r * self.memory_dim]",
            "-        return output, stop_token, self.attention_layer.attention_weights",
            "+        return output, stop_token, self.attention.attention_weights",
            "",
            "def _update_memory_input(self, new_memory):",
            "if self.use_memory_queue:"
        ]
    },
    {
        "number": 7927,
        "comments": "",
        "commit_message": "Fix mypy typing errors in pytorch_lightning/callbacks/model_checkpoint.py (#13617)\n\nCo-authored-by: Carlos Mochol\u00ed <carlossmocholi@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class LightningModule(",
            "return torch.tensor(value, device=self.device)",
            "",
            "@staticmethod",
            "-    def __check_numel_1(value: torch.Tensor, name: str) -> None:",
            "+    def __check_numel_1(value: Tensor, name: str) -> None:",
            "if not torch.numel(value) == 1:",
            "raise ValueError(",
            "f\"`self.log({name}, {value})` was called, but the tensor must have a single element.\""
        ]
    },
    {
        "number": 7930,
        "comments": "",
        "commit_message": "Refactor category orders in heads (#2374)\n\n* Refactor (all): all category -1 in anchor/bbox head and anchor/bbox target assign\n\n* Fix (datasets): remove label + 1 in datasets\n\n* Fix (bbox_head): fix bug of fc_cls that forget + 1\n\n* Fix (atss_head & free_anchor): fix cat -1 bugs\n\n* Fix (mask_head): remove label + 1 in mask heads\n\n* fix atss\n\n* Fix (rpn): fix cross_entropy_loss bug of RPN\n\n* Fix (anchor_head): fix typo\n\n* Refactor (anchor_head): use background_label rather than num_classes to indicate background class\n\n* Refactor (docstring): add and reformat docstrings\n\n* fix mask iou head\n\n* Fix (mask_head): fix cat -1 bug\n\n* Fix (mask_head): fix bug in mask inference\n\n* Add (tests): add tests for mask rcnn and mask heads\n\n* Refactor (unittest): refactor test_forward\n\n* Refactor (new_empty): use new_full rather than new_empty\n\n* Refactor (background_label): check background_label\n\n* Add TODO\n\n* Refactor (unittest): allow BP in unittest\n\nCo-authored-by: Cao Yuhang <yhcao6@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class GHMC(nn.Module):",
            "\"\"\"",
            "# the target should be binary class label",
            "if pred.dim() != target.dim():",
            "-            target, label_weight = _expand_binary_labels(",
            "+            target, label_weight = _expand_onehot_labels(",
            "target, label_weight, pred.size(-1))",
            "target, label_weight = target.float(), label_weight.float()",
            "edges = self.edges"
        ]
    },
    {
        "number": 7935,
        "comments": "",
        "commit_message": "Fixes to enable dev branch to move to pytorch-1.2 (#2001)\n\n* Fixes to enable dev branch to move to pytorch-1.2\n\n* change name for published cpu wheels\n\n* fix failing unit tests\n\n* change docs conf\n\n* change tracking example\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Rejector(TorchDistribution):",
            "x = self.propose(sample_shape) if sample_shape else self.propose()",
            "log_prob_accept = self.log_prob_accept(x)",
            "probs = torch.exp(log_prob_accept).clamp_(0.0, 1.0)",
            "-        done = torch.bernoulli(probs).byte()",
            "+        done = torch.bernoulli(probs).bool()",
            "while not done.all():",
            "proposed_x = self.propose(sample_shape) if sample_shape else self.propose()",
            "log_prob_accept = self.log_prob_accept(proposed_x)",
            "prob_accept = torch.exp(log_prob_accept).clamp_(0.0, 1.0)",
            "-            accept = torch.bernoulli(prob_accept).byte() & ~done",
            "+            accept = torch.bernoulli(prob_accept).bool() & ~done",
            "if accept.any():",
            "x[accept] = proposed_x[accept]",
            "done |= accept"
        ]
    },
    {
        "number": 7936,
        "comments": "",
        "commit_message": "fix bugs around optimizer\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Plan(Serializable):",
            "# prevent circular dependency",
            "# syft relative",
            "from ...core.node.vm.vm import VirtualMachine  # noqa: F401",
            "-        if self.local_executor is not None:",
            "-            # this is necessary for syfts nn.module, because the plan contains state from the module",
            "-            # in order to use this state, we first need to send the model, and then execute te plan",
            "-            return self.local_executor(**kwargs)",
            "",
            "alice = VirtualMachine(name=\"plan_executor\")",
            "alice_client: client.Client = alice.get_client()"
        ]
    },
    {
        "number": 7938,
        "comments": "",
        "commit_message": "Moved Keras implementation into private module (#650)\n\n* Moved Keras implementation into private module to avoid importing standalone keras when using tf.keras\n\n* Fixed unit tests\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class KerasTests(tf.test.TestCase):",
            "new_opt = new_model.optimizer",
            "os.remove(fname)",
            "",
            "-            self.assertEqual(type(new_opt).__module__, 'horovod.keras.impl')",
            "+            self.assertEqual(type(new_opt).__module__, 'horovod._keras')",
            "self.assertEqual(type(new_opt).__name__, 'TestOptimizer')",
            "self.assertEqual(K.get_value(opt.lr), K.get_value(new_opt.lr))",
            "self.assertEqual(len(opt.get_weights()), len(new_opt.get_weights()))"
        ]
    },
    {
        "number": 7940,
        "comments": "",
        "commit_message": "fix the unravel_index to makesure it retures tuple of arrays\n",
        "label": "",
        "answer": "no",
        "change": [
            "def unravel_index(",
            "/,",
            "*,",
            "out: Optional[torch.Tensor] = None,",
            "-) -> torch.Tensor:",
            "-    temp = indices.detach()",
            "+) -> Tuple:",
            "+    temp = indices.to(torch.int64)",
            "output = []",
            "for dim in reversed(shape):",
            "output.append(temp % dim)",
            "temp = temp // dim",
            "-    return torch.tensor(reversed(output), dtype=torch.int64)",
            "+    return tuple(reversed(output))",
            "",
            "",
            "unravel_index.support_native_out = False"
        ]
    },
    {
        "number": 7941,
        "comments": "",
        "commit_message": "Karras VE, DDIM and DDPM flax schedulers (#508)\n\n* beta never changes removed from state\n\n* fix typos in docs\n\n* removed unused var\n\n* initial ddim flax scheduler\n\n* import\n\n* added dummy objects\n\n* fix style\n\n* fix typo\n\n* docs\n\n* fix typo in comment\n\n* set return type\n\n* added flax ddom\n\n* fix style\n\n* remake\n\n* pass PRNG key as argument and split before use\n\n* fix doc string\n\n* use config\n\n* added flax Karras VE scheduler\n\n* make style\n\n* fix dummy\n\n* fix ndarray type annotation\n\n* replace returns a new state\n\n* added lms_discrete scheduler\n\n* use self.config\n\n* add_noise needs state\n\n* use config\n\n* use config\n\n* docstring\n\n* added flax score sde ve\n\n* fix imports\n\n* fix typos\n",
        "label": "",
        "answer": "no",
        "change": [
            "def main():",
            "noise = torch.randn(latents.shape).to(latents.device)",
            "bsz = latents.shape[0]",
            "# Sample a random timestep for each image",
            "-                timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (bsz,), device=latents.device).long()",
            "+                timesteps = torch.randint(",
            "+                    0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device",
            "+                ).long()",
            "",
            "# Add noise to the latents according to the noise magnitude at each timestep",
            "# (this is the forward diffusion process)"
        ]
    },
    {
        "number": 7942,
        "comments": "",
        "commit_message": "new unique fixes\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def consecutive_cluster(src, batch=None):",
            "output = arg[src.view(-1)]",
            "output = output.view(size).long()",
            "",
            "-    return (output, None) if batch is None else (output, batch[value])",
            "+    return (output, None) if batch is None else (output, batch[perm])"
        ]
    },
    {
        "number": 7944,
        "comments": "",
        "commit_message": "Minor typo fix\n",
        "label": "",
        "answer": "no",
        "change": [
            "def cross_entropy_seq(logits, target_seqs, batch_size=None):#, batch_size=1, num",
            ">>> see PTB tutorial for more details",
            ">>> input_data = tf.placeholder(tf.int32, [batch_size, num_steps])",
            ">>> targets = tf.placeholder(tf.int32, [batch_size, num_steps])",
            "-    >>> cost = tf.cost.cross_entropy_seq(network.outputs, targets)",
            "+    >>> cost = tl.cost.cross_entropy_seq(network.outputs, targets)",
            "\"\"\"",
            "try: # TF 1.0",
            "sequence_loss_by_example_fn = tf.contrib.legacy_seq2seq.sequence_loss_by_example"
        ]
    },
    {
        "number": 7946,
        "comments": "",
        "commit_message": "chore: enable `singleton-comparison` and cleanup (#3849)\n\n* enable singleton-comparison\n\n* fix triadaptive_model bug\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TriAdaptiveModel(nn.Module):",
            "",
            "# Forward pass for text passages and tables",
            "if \"passage_input_ids\" in kwargs.keys():",
            "-            table_mask = torch.flatten(kwargs[\"is_table\"]) == True",
            "+            table_mask = torch.flatten(kwargs[\"is_table\"]) == 1",
            "",
            "# Current batch consists of only tables",
            "if all(table_mask):"
        ]
    },
    {
        "number": 7948,
        "comments": "",
        "commit_message": "Fix SGD when using tf.function.\n\nscatter_nd_sub behaves the same as scatter_sub for IndexedSlices in Eager mode but not in Graph mode. We can just use scatter_sub though.\n\nPiperOrigin-RevId: 256568297\nChange-Id: Ie8267e0f09d26cf92e8256f821423f170e63493a\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class SGD(base.Module):",
            "optimizer_utils.check_same_dtype(update, parameter)",
            "learning_rate = tf.cast(self.learning_rate, update.dtype.base_dtype)",
            "if isinstance(update, tf.IndexedSlices):",
            "-          parameter.scatter_nd_sub(",
            "-              update.indices, update.values * learning_rate)",
            "+          parameter.scatter_sub(",
            "+              tf.IndexedSlices(update.values * learning_rate, update.indices))",
            "else:",
            "parameter.assign_sub(update * learning_rate)"
        ]
    },
    {
        "number": 7954,
        "comments": "",
        "commit_message": "Fix ELBO term scaling\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "for it in range(1000000):",
            "T_sample = T(torch.cat([X, z_sample], 1))",
            "",
            "disc = torch.mean(-T_sample)",
            "-    loglike = -nn.binary_cross_entropy(X_sample, X)",
            "+    loglike = -nn.binary_cross_entropy(X_sample, X, size_average=False) / mb_size",
            "",
            "elbo = -(disc + loglike)"
        ]
    },
    {
        "number": 7958,
        "comments": "",
        "commit_message": "lintfixbot: Auto-commit fixed lint errors in codebase\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def diff(",
            "x: Union[tf.Tensor, tf.Variable, int, float, list, tuple],",
            "/,",
            "*,",
            "-    out: Optional[Union[tf.Tensor, tf.Variable]] = None",
            "+    out: Optional[Union[tf.Tensor, tf.Variable]] = None,",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "return tf.experimental.numpy.diff(x)",
            "",
            "-",
            "+",
            "@with_unsupported_dtypes({\"2.9.1 and below\": (\"bfloat16, float16,\")}, backend_version)",
            "def zeta(",
            "x: Union[tf.Tensor, tf.Variable],"
        ]
    },
    {
        "number": 7961,
        "comments": "",
        "commit_message": "modelnet windows fix\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class SamplePoints(object):",
            "pos = pos / pos_max",
            "",
            "area = (pos[face[1]] - pos[face[0]]).cross(pos[face[2]] - pos[face[0]])",
            "-        area = torch.sqrt((area**2).sum(dim=-1)) / 2",
            "+        area = area.norm(p=2, dim=1) / 2",
            "",
            "prob = area / area.sum()",
            "sample = torch.multinomial(prob, self.num, replacement=True)"
        ]
    },
    {
        "number": 7962,
        "comments": "",
        "commit_message": "Parameter Metadata Updates (#2736)\n\n* Added split param metadata and changed sample ratio to medium impact\n\n* Schema updates\n\n* Adjusted Expected Impacts\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class ECDTrainerConfig(BaseTrainerConfig):",
            "",
            "bucketing_field: str = schema_utils.String(",
            "default=None,",
            "-        description=\"When not null, when creating batches, instead of shuffling randomly, the length along the last \"",
            "-        \"dimension of the matrix of the specified input feature is used for bucketing examples and then \"",
            "-        \"randomly shuffled examples from the same bin are sampled. Padding is trimmed to the longest \"",
            "-        \"example in the batch. The specified feature should be either a sequence or text feature and the \"",
            "-        \"encoder encoding it has to be rnn. When used, bucketing improves speed of rnn encoding up to \"",
            "-        \"1.5x, depending on the length distribution of the inputs.\",",
            "+        description=\"Feature to use for bucketing datapoints\",",
            "parameter_metadata=TRAINER_METADATA[\"bucketing_field\"],",
            ")"
        ]
    },
    {
        "number": 7963,
        "comments": "",
        "commit_message": "fixed and improved shape handling for exploration\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class OrnsteinUhlenbeckProcess(Exploration):",
            "",
            "super(OrnsteinUhlenbeckProcess, self).__init__(scope=scope, summary_labels=summary_labels)",
            "",
            "-    def tf_explore(self, episode, timestep, action_spec):",
            "-        normal_sample = tf.random_normal(shape=action_spec['shape'], mean=0.0, stddev=1.0)",
            "+    def tf_explore(self, episode, timestep, shape):",
            "+        normal_sample = tf.random_normal(shape=shape, mean=0.0, stddev=1.0)",
            "state = tf.get_variable(",
            "name='ornstein_uhlenbeck',",
            "dtype=util.tf_dtype('float'),",
            "-            shape=action_spec['shape'],",
            "-            initializer=tf.constant_initializer(self.mu)",
            "+            shape=shape,",
            "+            initializer=tf.constant_initializer(self.mu),",
            "+            trainable=False",
            ")",
            "return tf.assign_add(ref=state, value=(self.theta * (self.mu - state) + self.sigma * normal_sample))"
        ]
    },
    {
        "number": 7965,
        "comments": "",
        "commit_message": "lint fixes\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def linear_resample(x, num_samples: int, axis: int = -1):",
            "else:",
            "x_pre_shape = x_shape[:-1]",
            "x = torch.reshape(x, ([-1, 1] + [num_vals]))",
            "-    if x.dtype not in ['float16','float32','float64']:",
            "-        x=x.type(torch.float32)",
            "+    if x.dtype not in [\"float16\", \"float32\", \"float64\"]:",
            "+        x = x.type(torch.float32)",
            "ret = torch.nn.functional.interpolate(",
            "x, num_samples, mode=\"linear\", align_corners=True",
            ")"
        ]
    },
    {
        "number": 7967,
        "comments": "",
        "commit_message": "[Testing] Fix LINT/sphinx errors. (#8874)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class NestedSpacesTest(unittest.TestCase):",
            "ModelCatalog.register_custom_model(\"invalid\", InvalidModel)",
            "self.assertRaisesRegexp(",
            "ValueError,",
            "-            \"optimizer got an empty parameter list\",",
            "+            \"Subclasses of TorchModelV2 must also inherit from nn.Module\",",
            "lambda: PGTrainer(",
            "env=\"CartPole-v0\",",
            "config={"
        ]
    },
    {
        "number": 7968,
        "comments": "",
        "commit_message": "Allow directly passing text embeddings to Stable Diffusion Pipeline for prompt weighting (#2071)\n\n* add text embeds to sd\n\n* add text embeds to sd\n\n* finish tests\n\n* finish\n\n* finish\n\n* make style\n\n* fix tests\n\n* make style\n\n* make style\n\n* up\n\n* better docs\n\n* fix\n\n* fix\n\n* new try\n\n* up\n\n* up\n\n* finish\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class PaintByExamplePipeline(DiffusionPipeline):",
            "image_embeddings = image_embeddings.view(bs_embed * num_images_per_prompt, seq_len, -1)",
            "",
            "if do_classifier_free_guidance:",
            "-            uncond_embeddings = uncond_embeddings.repeat(1, image_embeddings.shape[0], 1)",
            "-            uncond_embeddings = uncond_embeddings.view(bs_embed * num_images_per_prompt, 1, -1)",
            "+            negative_prompt_embeds = negative_prompt_embeds.repeat(1, image_embeddings.shape[0], 1)",
            "+            negative_prompt_embeds = negative_prompt_embeds.view(bs_embed * num_images_per_prompt, 1, -1)",
            "",
            "# For classifier free guidance, we need to do two forward passes.",
            "# Here we concatenate the unconditional and text embeddings into a single batch",
            "# to avoid doing two forward passes",
            "-            image_embeddings = torch.cat([uncond_embeddings, image_embeddings])",
            "+            image_embeddings = torch.cat([negative_prompt_embeds, image_embeddings])",
            "",
            "return image_embeddings"
        ]
    },
    {
        "number": 7975,
        "comments": "",
        "commit_message": "Use f-strings in the dataset scripts (#3291)\n\n* Finishes #3257\n\nUsed f-strings to format the .py files in the dataset folder\n\n* Fix style\n\n* Fix hkcancor dataset\n\nCo-authored-by: Mario \u0160a\u0161ko <mario@huggingface.co>\nCo-authored-by: mariosasko <mariosasko777@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class NarrativeqaManual(datasets.GeneratorBasedBuilder):",
            "",
            "if not os.path.exists(manual_dir):",
            "raise FileNotFoundError(",
            "-                \"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('narrativeqa_manual', data_dir=...)` that includes the stories downloaded from the original repository. Manual download instructions: {}\".format(",
            "-                    manual_dir, self.manual_download_instructions",
            "-                )",
            "+                f\"{manual_dir} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('narrativeqa_manual', data_dir=...)` that includes the stories downloaded from the original repository. Manual download instructions: {self.manual_download_instructions}\"",
            ")",
            "",
            "return ["
        ]
    },
    {
        "number": 7977,
        "comments": "",
        "commit_message": "update sequence generator device change (#2989)\n\nSummary:\n# Before submitting\n\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\n- [ ] Did you make sure to update the docs?\n- [ ] Did you write any new necessary tests?\n\n## What does this PR do?\nFixes Sequence Generator\n\nReproduce the bug:\n1. Background:\n**fairseq.utils.move_to_cuda** function support to change device.\n2. When you change the file **fairseq_cli/generate.py**\nsuppose `gpu_id = 1`\n- line 134 change from\n`model.cuda()`\nto\n`model.cuda(gpu_id)`\n- line 189 change from\n`sample = utils.move_to_cuda(sample) if use_cuda else sample`\nto\n`sample = utils.move_to_cuda(sample, gpu_id) if use_cuda else sample`\n2. you will get an error\n```\nfairseq/sequence_generator.py, line 382, in generate\n    cand_bbsz_idx = cand_beams.add(bbsz_offsets)\nRuntimeError: binary_op(): expected both inputs to be on same device, but input a is on cuda:1 and input b is on cuda:0\n\n```\n\nThe reason of this bug is that `bbsz_offsets` is not assigned to proper cuda device. Therefore, we need to change the line 281 and line 281 of the file **fairseq/sequence_generator.py**\n\nThe test file for this fix is the file **tests/test_sequence_generator.py**\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding \ufffd\nvery happy {emoji:1f642}\n\nPull Request resolved: https://github.com/pytorch/fairseq/pull/2989\n\nReviewed By: alexeib\n\nDifferential Revision: D25344265\n\nPulled By: myleott\n\nfbshipit-source-id: 8cb8b389e59a9aa67aec84dbdadcfa2c08c9648f\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class SequenceGenerator(nn.Module):",
            "cand_size = 2 * beam_size  # 2 x beam size in case half are EOS",
            "",
            "# offset arrays for converting between different indexing schemes",
            "-        bbsz_offsets = (torch.arange(0, bsz) * beam_size).unsqueeze(1).type_as(tokens)",
            "-        cand_offsets = torch.arange(0, cand_size).type_as(tokens)",
            "+        bbsz_offsets = (torch.arange(0, bsz) * beam_size).unsqueeze(1).type_as(tokens).to(src_tokens.device)",
            "+        cand_offsets = torch.arange(0, cand_size).type_as(tokens).to(src_tokens.device)",
            "",
            "reorder_state: Optional[Tensor] = None",
            "batch_idxs: Optional[Tensor] = None"
        ]
    },
    {
        "number": 7982,
        "comments": "",
        "commit_message": "[RLlib] Minor cleanup in preparation to tf2.x support. (#9130)\n\n* WIP.\n\n* Fixes.\n\n* LINT.\n\n* Fixes.\n\n* Fixes and LINT.\n\n* WIP.\n",
        "label": "",
        "answer": "no",
        "change": [
            "class DistributionalQTFModel(TFModelV2):",
            "return tf.nn.relu(action_activation)",
            "",
            "def _f_epsilon(self, x):",
            "-        return tf.sign(x) * tf.sqrt(tf.abs(x))",
            "+        return tf.math.sign(x) * tf.math.sqrt(tf.math.abs(x))"
        ]
    },
    {
        "number": 7985,
        "comments": "",
        "commit_message": "Move complex tensor related functions to espnet2/enh/layers/complex_utils.py; Refactor complex support code; fix unit tests\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class STFTDecoder(AbsDecoder):",
            "ilens (torch.Tensor): input lengths [Batch]",
            "\"\"\"",
            "if not isinstance(input, ComplexTensor) and (",
            "-            is_torch_1_8_plus and not torch.is_complex(input)",
            "+            is_torch_1_9_plus and not torch.is_complex(input)",
            "):",
            "raise TypeError(\"Only support complex tensors for stft decoder\")"
        ]
    },
    {
        "number": 7987,
        "comments": "",
        "commit_message": "refactor(sdk&framework): [Broken] framework runners (#2288)\n\n* refactor(sdk): runner\n\n* refactor(sdk): simpler runner creation\n\n* refactor(frameworks): many fixes\n\n* fix resources_quota\n\n* fi\n\n* fix for <py38\n\n* fix runner\n\n* fix it\n\n* remove model_store in integration tests\n\n* remove runner args\n\n* mod default model_store for integration tests\n\n* try fix keras\n\n* fix picklable_model tests\n\n* fix detectron\n\n* review\n\n* fix picklable_model\n\n* append\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_easyocr_save_load(metadata, image_array, modelstore, save_proc):",
            "_model = save_proc(LANG_LIST, RECOG_NETWORK, DETECT_MODEL, metadata)",
            "assert _model.info.metadata is not None",
            "",
            "-    easyocr_loaded = bentoml.easyocr.load(",
            "-        _model.tag,",
            "-        gpu=False,",
            "-        model_store=modelstore,",
            "-    )",
            "+    easyocr_loaded = bentoml.easyocr.load(_model.tag)",
            "",
            "raw_res = easyocr_loaded.readtext(image_array)"
        ]
    },
    {
        "number": 7992,
        "comments": "",
        "commit_message": "Refactor library namespaces [pre-release][0.6-rc1] (#1412)\n\n* flake fixes\n\n* initial flake8 fixeS\n\n* remove top level from kornia.color\n\n* kornia filters\n\n* kornia losses\n\n* kornia features\n\n* geomtry and all ok\n\n* removed jit module\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* apply formatting and few fixes\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add keep block for isort\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* skip init\n\n* fix the docs\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* remove old code\n\n* simplify ci workflow\n\n* fix circular dependency\n\n* few format fixes\n\n* fix code format test\n\n* remove kornia.jit imports\n\n* final fixes\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* ci fixes\n\n* add versioneer\n\n* fix pnp import\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* exclude version files from pre-commit\n\n* exclude files precommit\n\n* update to pytorch 1.10 and add fixes\n\n* Update tests_cpu.yml\n\n* Update setup_dev_env.sh\n\n* Update tests_cpu.yml\n\n* undo versioneer\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix no_grad\n\n* Apply suggestions from code review\n\n* fix skip windows tests\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Update test_integrated.py\n\n* Apply suggestions from code review\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestErode:",
            "rtol=1e-4,",
            ")",
            "assert_close(",
            "-            erosion(tensor,",
            "-                    torch.ones_like(structural_element),",
            "-                    structuring_element=structural_element,",
            "-                    engine='convolution'),",
            "+            erosion(",
            "+                tensor,",
            "+                torch.ones_like(structural_element),",
            "+                structuring_element=structural_element,",
            "+                engine='convolution',",
            "+            ),",
            "expected,",
            "atol=1e-3,",
            "rtol=1e-3,"
        ]
    },
    {
        "number": 7998,
        "comments": "",
        "commit_message": "[RLlib] DQN torch version. (#7597)\n\n* Fix.\n\n* Rollback.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* Fix.\n\n* Fix.\n\n* Fix.\n\n* Fix.\n\n* Fix.\n\n* WIP.\n\n* WIP.\n\n* Fix.\n\n* Test case fixes.\n\n* Test case fixes and LINT.\n\n* Test case fixes and LINT.\n\n* Rollback.\n\n* WIP.\n\n* WIP.\n\n* Test case fixes.\n\n* Fix.\n\n* Fix.\n\n* Fix.\n\n* Add regression test for DQN w/ param noise.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* Comment\n\n* Regression test case.\n\n* WIP.\n\n* WIP.\n\n* LINT.\n\n* LINT.\n\n* WIP.\n\n* Fix.\n\n* Fix.\n\n* Fix.\n\n* LINT.\n\n* Fix (SAC does currently not support eager).\n\n* Fix.\n\n* WIP.\n\n* LINT.\n\n* Update rllib/evaluation/sampler.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* Update rllib/evaluation/sampler.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* Update rllib/utils/exploration/exploration.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* Update rllib/utils/exploration/exploration.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* WIP.\n\n* WIP.\n\n* Fix.\n\n* LINT.\n\n* LINT.\n\n* Fix and LINT.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* Fix.\n\n* LINT.\n\n* Fix.\n\n* Fix and LINT.\n\n* Update rllib/utils/exploration/exploration.py\n\n* Update rllib/policy/dynamic_tf_policy.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* Update rllib/policy/dynamic_tf_policy.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* Update rllib/policy/dynamic_tf_policy.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* Fixes.\n\n* WIP.\n\n* LINT.\n\n* Fixes and LINT.\n\n* LINT and fixes.\n\n* LINT.\n\n* Move action_dist back into torch extra_action_out_fn and LINT.\n\n* Working SimpleQ learning cartpole on both torch AND tf.\n\n* Working Rainbow learning cartpole on tf.\n\n* Working Rainbow learning cartpole on tf.\n\n* WIP.\n\n* LINT.\n\n* LINT.\n\n* Update docs and add torch to APEX test.\n\n* LINT.\n\n* Fix.\n\n* LINT.\n\n* Fix.\n\n* Fix.\n\n* Fix and docstrings.\n\n* Fix broken RLlib tests in master.\n\n* Split BAZEL learning tests into cartpole and pendulum (reached the 60min barrier).\n\n* Fix error_outputs option in BAZEL for RLlib regression tests.\n\n* Fix.\n\n* Tune param-noise tests.\n\n* LINT.\n\n* Fix.\n\n* Fix.\n\n* test\n\n* test\n\n* test\n\n* Fix.\n\n* Fix.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* LINT.\n\n* WIP.\n\nCo-authored-by: Eric Liang <ekhliang@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class SyncReplayOptimizer(PolicyOptimizer):",
            "self.learner_stats[policy_id] = get_learner_stats(info)",
            "replay_buffer = self.replay_buffers[policy_id]",
            "if isinstance(replay_buffer, PrioritizedReplayBuffer):",
            "-                    td_error = info[\"td_error\"]",
            "+                    # TODO(sven): This is currently structured differently for",
            "+                    #  torch/tf. Clean up these results/info dicts across",
            "+                    #  policies (note: fixing this in torch_policy.py will",
            "+                    #  break e.g. DDPPO!).",
            "+                    td_error = info.get(",
            "+                        \"td_error\", info[\"learner_stats\"].get(\"td_error\"))",
            "new_priorities = (",
            "np.abs(td_error) + self.prioritized_replay_eps)",
            "replay_buffer.update_priorities("
        ]
    },
    {
        "number": 8002,
        "comments": "",
        "commit_message": "CDN urls (#4030)\n\n* [file_utils] use_cdn + documentation\n\n* Move to cdn. urls for weights\n\n* [urls] Hotfix for bert-base-japanese\n",
        "label": "",
        "answer": "no",
        "change": [
            "from .modeling_utils import Conv1D, PreTrainedModel, SequenceSummary, prune_conv",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "-OPENAI_GPT_PRETRAINED_MODEL_ARCHIVE_MAP = {",
            "-    \"openai-gpt\": \"https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-pytorch_model.bin\"",
            "-}",
            "+OPENAI_GPT_PRETRAINED_MODEL_ARCHIVE_MAP = {\"openai-gpt\": \"https://cdn.huggingface.co/openai-gpt-pytorch_model.bin\"}",
            "",
            "",
            "def load_tf_weights_in_openai_gpt(model, config, openai_checkpoint_folder_path):"
        ]
    },
    {
        "number": 8008,
        "comments": "",
        "commit_message": "add options for anchors; fix alignment issue with resnet maxpool; more documentation.\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class vgg16(object):",
            "y1 = tf.slice(rois, [0, 2], [-1, 1], name=\"y1\") / height",
            "x2 = tf.slice(rois, [0, 3], [-1, 1], name=\"x2\") / width",
            "y2 = tf.slice(rois, [0, 4], [-1, 1], name=\"y2\") / height",
            "-      bboxes = tf.concat([y1, x1, y2, x2], 1)",
            "+      bboxes = tf.stop_gradient(tf.concat([y1, x1, y2, x2], 1))",
            "pre_pool_size = cfg.POOLING_SIZE * 2",
            "crops = tf.image.crop_and_resize(bottom, bboxes, tf.to_int32(batch_ids), [pre_pool_size, pre_pool_size], name=\"crops\")"
        ]
    },
    {
        "number": 8012,
        "comments": "",
        "commit_message": "changed and improved tune.py, fixed policy gradient problem, other internal improvements\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class PolicyGradient(Objective):",
            ")",
            "return tf.math.minimum(x=(scaling * reward), y=(clipped_scaling * reward))",
            "",
            "-        skip_clipping = tf.math.equal(x=clipping_value, y=zero)",
            "+        skip_clipping = tf.math.equal(x=clipping_value, y=one)",
            "scaled = tf.cond(pred=skip_clipping, true_fn=no_clipping, false_fn=apply_clipping)",
            "",
            "loss = -scaled"
        ]
    },
    {
        "number": 8014,
        "comments": "",
        "commit_message": "[Metrics] Unification of regression (#4166)\n\n* moved to utility\n\n* add files\n\n* unify\n\n* add desc\n\n* update\n\n* end of line\n\n* Apply suggestions from code review\n\nCo-authored-by: Justus Schock <12886177+justusschock@users.noreply.github.com>\n\n* Apply suggestions from code review\n\nCo-authored-by: Justus Schock <12886177+justusschock@users.noreply.github.com>\n\n* add back functional test in new interface\n\n* pep8\n\n* doctest fix\n\n* test name fix\n\n* unify psnr + add class psnr, TODO: psnr test refactor ala mean squared error\n\n* unify psnr\n\n* rm unused code\n\n* pep8\n\n* docs\n\n* unify ssim\n\n* lower tolerance for ssim\n\n* fix import\n\n* pep8\n\n* docs\n\n* flake8\n\n* test smaller images\n\n* trying to fix test\n\n* no ddp test for ssim\n\n* pep8\n\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\nCo-authored-by: Justus Schock <12886177+justusschock@users.noreply.github.com>\nCo-authored-by: Teddy Koker <teddy.koker@gmail.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class MeanSquaredError(Metric):",
            "preds: Predictions from model",
            "target: Ground truth values",
            "\"\"\"",
            "-        self._check_same_shape(preds, target)",
            "-        squared_error = torch.pow(preds - target, 2)",
            "+        sum_squared_error, n_obs = _mean_squared_error_update(preds, target)",
            "",
            "-        self.sum_squared_error += torch.sum(squared_error)",
            "-        self.total += target.numel()",
            "+        self.sum_squared_error += sum_squared_error",
            "+        self.total += n_obs",
            "",
            "def compute(self):",
            "\"\"\"",
            "Computes mean squared error over state.",
            "\"\"\"",
            "-        return self.sum_squared_error / self.total",
            "+        return _mean_squared_error_compute(self.sum_squared_error, self.total)"
        ]
    },
    {
        "number": 8015,
        "comments": "",
        "commit_message": "Fix a few last paths for the new repo org (#8666)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class CaptureLogger:",
            "",
            ">>> msg = \"Testing 1, 2, 3\"",
            ">>> logging.set_verbosity_info()",
            "-        >>> logger = logging.get_logger(\"transformers.tokenization_bart\")",
            "+        >>> logger = logging.get_logger(\"transformers.models.bart.tokenization_bart\")",
            ">>> with CaptureLogger(logger) as cl:",
            "...     logger.info(msg)",
            ">>> assert cl.out, msg+\"\\n\""
        ]
    },
    {
        "number": 8016,
        "comments": "",
        "commit_message": "Fix an issue related to complex_norm argument\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_cat(dim):",
            "assert complex_module.allclose(ret, ret2)",
            "",
            "",
            "-@pytest.mark.parametrize(\"dim\", [0, 1, 2])",
            "+@pytest.mark.parametrize(\"dim\", [None, 0, 1, 2])",
            "@pytest.mark.skipif(not is_torch_1_9_plus, reason=\"Require torch 1.9.0+\")",
            "def test_complex_norm(dim):",
            "mat = ComplexTensor(torch.rand(2, 3, 4), torch.rand(2, 3, 4))",
            "mat_th = torch.complex(mat.real, mat.imag)",
            "norm = complex_norm(mat, dim=dim, keepdim=True)",
            "norm_th = complex_norm(mat_th, dim=dim, keepdim=True)",
            "-    assert (",
            "-        torch.allclose(norm, norm_th)",
            "-        and norm.ndim == mat.ndim",
            "-        and mat.numel() == norm.numel() * mat.size(dim)",
            "-    )",
            "+    assert torch.allclose(norm, norm_th)",
            "+    if dim is not None:",
            "+        assert norm.ndim == mat.ndim and mat.numel() == norm.numel() * mat.size(dim)",
            "",
            "",
            "@pytest.mark.parametrize(\"real_vec\", [True, False])"
        ]
    },
    {
        "number": 8017,
        "comments": "",
        "commit_message": "fixed graph_sage_unsup Attribute error\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "data = dataset[0]",
            "class NeighborSampler(RawNeighborSampler):",
            "def sample(self, batch):",
            "batch = torch.tensor(batch)",
            "-        row, col, _ = self.adj.coo()",
            "+        row, col, _ = self.adj_t.coo()#change adj to adj_t",
            "",
            "# For each node in `batch`, we sample a direct neighbor (as positive",
            "# example) and a random node (as negative example):",
            "pos_batch = random_walk(row, col, batch, walk_length=1,",
            "coalesced=False)[:, 1]",
            "",
            "-        neg_batch = torch.randint(0, self.adj.size(0), (batch.numel(), ),",
            "-                                  dtype=torch.long)",
            "+        neg_batch = torch.randint(0, self.adj_t.size(0), (batch.numel(), ),",
            "+                                  dtype=torch.long)#change adj to adj_t",
            "",
            "batch = torch.cat([batch, pos_batch, neg_batch], dim=0)",
            "return super(NeighborSampler, self).sample(batch)"
        ]
    },
    {
        "number": 8018,
        "comments": "",
        "commit_message": "fixing fn_tree args for atleast_2d and count_nonzero\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_dstack(",
            "",
            "# atleast_2d",
            "@handle_test(",
            "+    fn_tree=\"functional.experimental.atleast_2d\",",
            "dtype_and_x=helpers.dtype_and_values(",
            "available_dtypes=helpers.get_dtypes(\"valid\"),",
            "num_arrays=helpers.ints(min_value=1, max_value=5),"
        ]
    },
    {
        "number": 8019,
        "comments": "",
        "commit_message": "Fixed update of centers for center loss\n",
        "label": "",
        "answer": "no",
        "change": [
            "def center_loss(features, label, alfa, nrof_classes):",
            "centers_batch = tf.gather(centers, label)",
            "diff = (1 - alfa) * (centers_batch - features)",
            "centers = tf.scatter_sub(centers, label, diff)",
            "-    loss = tf.reduce_mean(tf.square(features - centers_batch))",
            "+    with tf.control_dependencies([centers]):",
            "+        loss = tf.reduce_mean(tf.square(features - centers_batch))",
            "return loss, centers",
            "",
            "def get_image_paths_and_labels(dataset):"
        ]
    },
    {
        "number": 8020,
        "comments": "",
        "commit_message": "Fix q_model clip_gradients -> clip_loss\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class QModel(Model):",
            "",
            "# If loss clipping is used, calculate the huber loss",
            "if config.clip_loss > 0.0:",
            "-                huber_loss = tf.where(condition=(tf.abs(delta) < config.clip_gradients), x=(0.5 * self.loss_per_instance), y=(tf.abs(delta) - 0.5))",
            "+                huber_loss = tf.where(condition=(tf.abs(delta) < config.clip_loss), x=(0.5 * self.loss_per_instance), y=(tf.abs(delta) - 0.5))",
            "self.q_loss = tf.reduce_mean(input_tensor=huber_loss, axis=0)",
            "else:",
            "self.q_loss = tf.reduce_mean(input_tensor=self.loss_per_instance, axis=0)"
        ]
    },
    {
        "number": 8021,
        "comments": "",
        "commit_message": "Fix tensor_to_image\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def tensor_to_image(tensor):",
            "if not torch.is_tensor(tensor):",
            "raise TypeError(\"Input type is not a torch.Tensor. Got {}\".format(",
            "type(tensor)))",
            "+    tensor = torch.squeeze(tensor)",
            "if len(tensor.shape) > 3 or len(tensor.shape) < 2:",
            "raise ValueError(",
            "\"Input size must be a two or three dimensional tensor\")",
            "-    tensor = torch.squeeze(tensor)",
            "if len(tensor.shape) == 2:",
            "tensor = torch.unsqueeze(tensor, dim=0)",
            "return tensor.permute(1, 2, 0).contiguous().cpu().detach().numpy()"
        ]
    },
    {
        "number": 8024,
        "comments": "",
        "commit_message": "Fix typing of cifar10_convnet after previous change\n\nPiperOrigin-RevId: 297585986\nChange-Id: Icdcfbdec09feafb1026385bf794d252a626a8219\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Cifar10ConvNet(base.Module):",
            "",
            "logits = self._logits_module(flat_output)",
            "",
            "-    return {'logits': logits, 'activations': activations}  # pytype: disable=bad-return-type",
            "+    return {'logits': logits, 'activations': activations}"
        ]
    },
    {
        "number": 8025,
        "comments": "",
        "commit_message": "enable black in the precommit (#1777)\n\n* enable black in the precommit\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add some fixes\n\n* update libface detection url\n\n* added url from kornia checkpoint\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def get_hanning_kernel1d(kernel_size: int, device=torch.device('cpu'), dtype=tor",
            "return x",
            "",
            "",
            "-def get_hanning_kernel2d(kernel_size: Tuple[int, int],",
            "-                         device=torch.device('cpu'),",
            "-                         dtype=torch.float) -> torch.Tensor:",
            "+def get_hanning_kernel2d(kernel_size: Tuple[int, int], device=torch.device('cpu'), dtype=torch.float) -> torch.Tensor:",
            "r\"\"\"Returns 2d Hanning kernel, used in signal processing and KCF tracker.",
            "",
            "Args:"
        ]
    },
    {
        "number": 8027,
        "comments": "",
        "commit_message": "Fix option names\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def recog(args):",
            "word_dict = rnnlm_args.char_list_dict",
            "char_dict = {x: i for i, x in enumerate(train_args.char_list)}",
            "word_rnnlm = lm_pytorch.ClassifierWithState(lm_pytorch.RNNLM(",
            "-            len(word_dict), rnnlm_args.layers, rnnlm_args.units))",
            "+            len(word_dict), rnnlm_args.layer, rnnlm_args.unit))",
            "torch_load(args.word_rnnlm, word_rnnlm)",
            "word_rnnlm.eval()"
        ]
    },
    {
        "number": 8028,
        "comments": "",
        "commit_message": "Fix unbalanced gradients bug in ZeRO-2 gradient accumulation (#545)\n\n* Use zero-tensors for missing gradients to avoid size mismatch\n\n* Unit test for unbalanced gradients in ZeRO\n\n* Formatting fixes\n",
        "label": "",
        "answer": "no",
        "change": [
            "class FP16_DeepSpeedZeroOptimizer(object):",
            "current_size = 0",
            "for i, tensor in enumerate(tensor_list):",
            "if tensor.grad is None:",
            "-                continue",
            "+                tensor.grad = torch.zeros_like(tensor)",
            "",
            "tensor = tensor.grad",
            "num_elements = tensor.numel()"
        ]
    },
    {
        "number": 8032,
        "comments": "",
        "commit_message": "Replace Distribution+TorchDistribution with a thin Distribution mixin (#769)\n\n* Sketch Distribution class as mixin\n\n* Remove TorchDistribution class\n\n* Simplify TransformedDistribution\n\n* Update torch wrappers for most distributions\n\n* Fix docs\n\n* Use dist.Reshape() to set extra_event_dims\n\n* Fix bugs in Reshape distribution\n\n* Fix rejector tests\n\n* Update _Subsample distribution\n\n* Use .reshape() method for extra_event_dims and sample_shape\n\n* Refactor Distribution -> TorchDistribution class hierarchy\n\n* Update docs\n\n* Fix json error in air.ipynb\n\n* Fix bugs in air.ipynb and abstract_infer.py\n\n* Fix distributions docs\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class RaoBlackwellizationTests(TestCase):",
            "pyro.sample(\"z_%d_%d\" % (i, k),",
            "fakes.NonreparameterizedNormal(ng_zeros(4 - i), ng_ones(4 - i)))",
            "pyro.sample(\"obs_%d\" % i,",
            "-                            dist.Normal(mu_latent, torch.pow(self.lam, -0.5), extra_event_dims=1),",
            "+                            dist.Normal(mu_latent, torch.pow(self.lam, -0.5))",
            "+                                .reshape(extra_event_dims=1),",
            "obs=_x)",
            "for k in range(n_superfluous_top, n_superfluous_top + n_superfluous_bottom):",
            "pyro.sample(\"z_%d_%d\" % (i, k),"
        ]
    },
    {
        "number": 8033,
        "comments": "",
        "commit_message": "remove numpy import (#1116)\n\n* remove numpy import\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* undo and add setup develop in gh actions\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* remove create_checkerboard\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestErode:",
            "None, None, :, :",
            "]",
            "assert_allclose(",
            "-            erosion(tensor, torch.ones_like(structural_element), structuring_element=structural_element), expected,",
            "-            atol=1e-4, rtol=1e-4",
            "+            erosion(tensor, torch.ones_like(structural_element), structuring_element=structural_element),",
            "+            expected,",
            "+            atol=1e-4,",
            "+            rtol=1e-4,",
            ")",
            "",
            "def test_exception(self, device, dtype):"
        ]
    },
    {
        "number": 8035,
        "comments": "",
        "commit_message": "fixed small typo (#20490)\n\nCo-authored-by: Sandeep Kumar <sandeep.kumar@woven-planet.global>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class VanLayer(nn.Module):",
            "drop_path_rate: float = 0.5,",
            "):",
            "super().__init__()",
            "-        self.drop_path = VanDropPath(drop_path) if drop_path_rate > 0.0 else nn.Identity()",
            "+        self.drop_path = VanDropPath(drop_path_rate) if drop_path_rate > 0.0 else nn.Identity()",
            "self.pre_normomalization = nn.BatchNorm2d(hidden_size)",
            "self.attention = VanSpatialAttentionLayer(hidden_size, config.hidden_act)",
            "self.attention_scaling = VanLayerScaling(hidden_size, config.layer_scale_init_value)"
        ]
    },
    {
        "number": 8038,
        "comments": "",
        "commit_message": "Fix dict/tuple hybrid action space for tensorflow eager execution (#8781)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class DiagGaussian(TFActionDistribution):",
            "@override(ActionDistribution)",
            "def logp(self, x):",
            "return -0.5 * tf.reduce_sum(",
            "-            tf.square((x - self.mean) / self.std), axis=1) - \\",
            "+            tf.square((tf.to_float(x) - self.mean) / self.std), axis=1) - \\",
            "0.5 * np.log(2.0 * np.pi) * tf.to_float(tf.shape(x)[1]) - \\",
            "tf.reduce_sum(self.log_std, axis=1)"
        ]
    },
    {
        "number": 8045,
        "comments": "",
        "commit_message": "Fixes to enable dev branch to move to pytorch-1.2 (#2001)\n\n* Fixes to enable dev branch to move to pytorch-1.2\n\n* change name for published cpu wheels\n\n* fix failing unit tests\n\n* change docs conf\n\n* change tracking example\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def test_unnormalized_normal(kernel, jit):",
            "posterior.append(samples)",
            "",
            "posterior = torch.stack([sample[\"z\"] for sample in posterior])",
            "-    assert_equal(torch.mean(posterior), true_mean, prec=0.1)",
            "-    assert_equal(torch.std(posterior), true_std, prec=0.1)",
            "+    assert_close(torch.mean(posterior), true_mean, rtol=0.05)",
            "+    assert_close(torch.std(posterior), true_std, rtol=0.05)"
        ]
    },
    {
        "number": 8047,
        "comments": "",
        "commit_message": "Fix train_step, test_step and tests for CLIP (#18684)\n\n* Fix train_step and test_step, correctly enable CLIP fit test\n\n* Stop using get_args on older Python versions\n\n* Don't use get_origin either\n\n* UnionType is actually even newer, don't use that either\n\n* Apply the same fix to test_loss_computation\n\n* Just realized I was accidentally skipping a bunch of tests!\n\n* Fix test_loss_computation for models without separable labels\n\n* Fix scalar losses in test_step and train_step\n\n* Stop committing your breakpoints\n\n* Fix Swin loss shape\n\n* Fix Tapas loss shape\n\n* Shape fixes for TAPAS, DeIT, HuBERT and ViTMAE\n\n* Add loss computation to TFMobileBertForPreTraining\n\n* make fixup and move copied from statement\n\n* make fixup and move copied from statement\n\n* Correct copied from\n\n* Add labels and next_sentence_label inputs to TFMobileBERT\n\n* Make sure total_loss is always defined\n\n* Update tests/test_modeling_tf_common.py\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* Fix copied from\n\n* Ensure CTC models get labels in tests\n\n* Ensure CTC models get labels in tests\n\n* Fix tests for vit_mae\n\n* Fix tests for vit_mae\n\n* Fix tests for vit_mae\n\n* Reduce batch size for wav2vec2 testing because it was causing OOM\n\n* Skip some TAPAS tests that are failing\n\n* Skip a failing HuBERT test\n\n* make style\n\n* Fix mobilebertforpretraining test\n\n* Skip Wav2Vec2 tests that use huge amounts of mem\n\n* Skip keras_fit for Wav2Vec2 as well\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TFTapasForQuestionAnswering(TFTapasPreTrainedModel):",
            "logits_aggregation = self.aggregation_classifier(pooled_output)",
            "",
            "# Total loss calculation",
            "-        total_loss = 0.0",
            "+        total_loss = tf.zeros(shape=(1,), dtype=tf.float32)",
            "calculate_loss = False",
            "if labels is not None:",
            "calculate_loss = True"
        ]
    },
    {
        "number": 8052,
        "comments": "",
        "commit_message": "Fix issues suggested by codacy. (#344)\n\n* remove two dangerous default values\n\n* fix mnist tutorial based on codacy\n\n* address hao's comments.\n\n* remove unused y_op\n\n* hao conv.py\n\n* hao prepro.py\n\n* hao files.py\n\n* remove str statement\n\n* hao example mnist\n\n* yapf\n\n* hao cifar10\n\n* hao inceptionv3\n\n* hao ptb tfrecord image processing\n\n* hao tutorials\n\n* str comment\n\n* str docs\n\n* Update README.md\n\n* remove unused code\n\n* minor fix\n\n* small fix\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def main_lstm_generate_text():",
            "",
            "if __name__ == '__main__':",
            "sess = tf.InteractiveSession()",
            "-    \"\"\"Restore a pretrained embedding matrix.\"\"\"",
            "+    # Restore a pretrained embedding matrix",
            "# main_restore_embedding_layer()",
            "-    \"\"\"How to generate text from a given context.\"\"\"",
            "+",
            "+    # How to generate text from a given context",
            "main_lstm_generate_text()",
            "",
            "#"
        ]
    },
    {
        "number": 8063,
        "comments": "",
        "commit_message": "Fixed problem with triplet demultiplexing. Added a test case for triplet\nloss.\n",
        "label": "",
        "answer": "no",
        "change": [
            "def triplet_loss(anchor, positive, negative, alpha):",
            "the triplet loss according to the FaceNet paper as a float tensor.",
            "\"\"\"",
            "with tf.variable_scope('triplet_loss'):",
            "-        pos_dist = tf.reduce_sum(tf.square(tf.sub(anchor, positive)), 1)  # Summing over distances in each batch",
            "+        pos_dist = tf.reduce_sum(tf.square(tf.sub(anchor, positive)), 1)",
            "neg_dist = tf.reduce_sum(tf.square(tf.sub(anchor, negative)), 1)",
            "",
            "basic_loss = tf.add(tf.sub(pos_dist,neg_dist), alpha)"
        ]
    },
    {
        "number": 8066,
        "comments": "",
        "commit_message": "Update export.py (#7301)\n\n* Update export.py\n\nSimplify code.\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def run(",
            "im, model = im.half(), model.half()  # to FP16",
            "model.train() if train else model.eval()  # training mode = no Detect() layer grid construction",
            "for k, m in model.named_modules():",
            "-        # if isinstance(m, Conv):  # assign export-friendly activations",
            "-        #     if isinstance(m.act, nn.SiLU):",
            "-        #         m.act = SiLU()",
            "if isinstance(m, Detect):",
            "m.inplace = inplace",
            "m.onnx_dynamic = dynamic",
            "m.export = True",
            "-            if hasattr(m, 'forward_export'):",
            "-                m.forward = m.forward_export  # assign custom forward (optional)",
            "",
            "for _ in range(2):",
            "y = model(im)  # dry runs"
        ]
    },
    {
        "number": 8067,
        "comments": "",
        "commit_message": "Fixed ragged sample weights\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def compute_weighted_loss(losses,",
            "losses = tf.convert_to_tensor(losses)",
            "input_dtype = losses.dtype",
            "",
            "-    if not isinstance(sample_weight, keras_tensor.KerasTensor):",
            "+    if not isinstance(sample_weight,",
            "+                      (keras_tensor.KerasTensor, tf.RaggedTensor)):",
            "sample_weight = tf.convert_to_tensor(sample_weight)",
            "",
            "# TODO(psv): Handle casting here in a better way, eg. if losses is float64"
        ]
    },
    {
        "number": 8069,
        "comments": "",
        "commit_message": "Fix training Conditional Random Fields on GPU (#5313) (#5315)\n\nCo-authored-by: Pete <petew@allenai.org>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class ConditionalRandomField(torch.nn.Module):",
            "\"\"\"",
            "",
            "if mask is None:",
            "-            mask = torch.ones(*tags.size(), dtype=torch.bool)",
            "+            mask = torch.ones(*tags.size(), dtype=torch.bool, device=inputs.device)",
            "else:",
            "# The code below fails in weird ways if this isn't a bool tensor, so we make sure.",
            "mask = mask.to(torch.bool)"
        ]
    },
    {
        "number": 8071,
        "comments": "",
        "commit_message": "`blip` support for training (#21021)\n\n* `blip` support for training\n\n* remove labels creation\n\n* remove unneeded `decoder_input_ids` creation\n\n* final changes\n\n- add colab link to documentation\n- reduction = mean for loss\n\n* fix nits\n\n* update link\n\n* clearer error message\n",
        "label": "",
        "answer": "no",
        "change": [
            "class BlipTextModel(BlipTextPreTrainedModel):",
            "past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0",
            "",
            "if attention_mask is None:",
            "-            attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)))",
            "+            attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length))).to(device)",
            "",
            "# We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]",
            "# ourselves in which case we just need to make it broadcastable to all heads."
        ]
    },
    {
        "number": 8074,
        "comments": "",
        "commit_message": "Some more small fixes\n",
        "label": "",
        "answer": "no",
        "change": [
            "def main(args):",
            "print('Total number of examples: %d' % len(label_list))",
            "",
            "# Placeholder for the learning rate",
            "-        learning_rate_placeholder = tf.placeholder(tf.float32, name='learing_rate')",
            "+        learning_rate_placeholder = tf.placeholder(tf.float32, name='learning_rate')",
            "",
            "# Build the inference graph",
            "prelogits, _ = network.inference(image_batch, args.keep_probability,"
        ]
    },
    {
        "number": 8078,
        "comments": "",
        "commit_message": "fix deprecations about casting & initializers in tf1.13\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class MaintainStepCounter(Callback):",
            "# ensure it exists",
            "gs_var = get_global_step_var()",
            "with tf.name_scope(None):",
            "-            with self.graph.colocate_with(gs_var):",
            "-                self.gs_incr_op = tf.assign_add(",
            "-                    gs_var, 1,",
            "-                    name=GLOBAL_STEP_INCR_OP_NAME).op",
            "+            self.gs_incr_op = tf.assign_add(",
            "+                gs_var, 1,",
            "+                name=GLOBAL_STEP_INCR_OP_NAME).op",
            "self._fetches = tf.train.SessionRunArgs(self.gs_incr_op)",
            "",
            "def _before_train(self):"
        ]
    },
    {
        "number": 8088,
        "comments": "",
        "commit_message": "Fix bug for ignore_id\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class E2E(STInterface, torch.nn.Module):",
            "hs, _, _ = self.enc(hs, ilens)",
            "return hs.squeeze(0)",
            "",
            "-    def translate(self, x, trans_args, char_list, rnnlm=None):",
            "+    def translate(self, x, trans_args, char_list, rnnlm=None, ensemble_models=[]):",
            "\"\"\"E2E beam search.",
            "",
            ":param ndarray x: input acoustic feature (T, D)"
        ]
    },
    {
        "number": 8091,
        "comments": "",
        "commit_message": "fix Average metric (#4624)\n\n* fix Average metric\n\n* try spawn\n\n* try again\n\n* oops\n\n* clean up, fix evalb too\n\n* use different start method for GPU vs CPU tests\n\n* add comment\n",
        "label": "",
        "answer": "no",
        "change": [
            "class PearsonCorrelation(Metric):",
            "A tensor of the same shape as `predictions`.",
            "\"\"\"",
            "predictions, gold_labels, mask = self.detach_tensors(predictions, gold_labels, mask)",
            "-        self._device = gold_labels.device",
            "if not is_distributed():",
            "self._predictions_labels_covariance(predictions, gold_labels, mask)",
            "self._predictions_variance(predictions, predictions, mask)"
        ]
    },
    {
        "number": 8092,
        "comments": "",
        "commit_message": "Fix `run_clip.py` (#20234)\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Transform(torch.nn.Module):",
            "Normalize(mean, std),",
            ")",
            "",
            "-    def forward(self, x: Image) -> torch.Tensor:",
            "+    def forward(self, x) -> torch.Tensor:",
            "+        \"\"\"`x` should be an instance of `PIL.Image.Image`\"\"\"",
            "with torch.no_grad():",
            "x = self.transforms(x)",
            "return x"
        ]
    },
    {
        "number": 8097,
        "comments": "",
        "commit_message": "Fix GPT-J onnx conversion (#16780)\n\n* add gptj to TOKENIZER_MAPPING_NAMES\n\n* fix int32 to float to avoid problem in onnx\n\n* Update src/transformers/models/gptj/modeling_gptj.py\n\nCo-authored-by: ChainYo <t.chaigneau.tc@gmail.com>\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def fixed_pos_embedding(x, seq_dim=1, seq_len=None):",
            "if seq_len is None:",
            "seq_len = x.shape[seq_dim]",
            "inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2) / dim))",
            "-    sinusoid_inp = torch.einsum(\"i , j -> i j\", torch.arange(seq_len), inv_freq).to(x.device).float()",
            "+    sinusoid_inp = (",
            "+        torch.einsum(\"i , j -> i j\", torch.arange(seq_len, dtype=torch.float), inv_freq).to(x.device).float()",
            "+    )",
            "return torch.sin(sinusoid_inp), torch.cos(sinusoid_inp)"
        ]
    },
    {
        "number": 8100,
        "comments": "",
        "commit_message": "pylint -> flake8 (#3288)\n\n* pylint\n\n* update pylint\n\n* undo a lot of the raise / else\n\n* add bound on typed-ast\n\n* first mypy fixes\n\n* new flag\n\n* fix mypy errors\n\n* requirements.txt\n\n* pylint -> flake8\n\n* mypy 0.720 -> mypy 0.730\n\n* add back erroneously removed initial newline\n\n* remove .pylintrc\n\n* remove pylintrc from Dockerfile\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "from allennlp.training import NoOpTrainer",
            "",
            "class ConstantModel(Model):",
            "def forward(self, *inputs) -> Dict[str, torch.Tensor]:",
            "-        return {\"class\": torch.tensor(98)} # pylint: disable=not-callable",
            "+        return {\"class\": torch.tensor(98)}",
            "+",
            "",
            "class TestNoOpTrainer(AllenNlpTestCase):",
            "def setUp(self):"
        ]
    },
    {
        "number": 8103,
        "comments": "",
        "commit_message": "Fix for keras\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Graph(object):",
            "if relevant_nodes and in_node not in relevant_nodes:",
            "# node is not part of the current network",
            "continue",
            "-                    for in_layer in nest.flatten(in_node.inbound_layers):",
            "+                    for in_layer in nest(in_node.inbound_layers):",
            "inbound_keras_node = Node.from_keras(in_layer)",
            "",
            "if (inbound_keras_node.id not in graph.nodes_by_id):"
        ]
    },
    {
        "number": 8105,
        "comments": "",
        "commit_message": "Style fix\n",
        "label": "",
        "answer": "no",
        "change": [
            "def update_sub(x, decrement):",
            "",
            "",
            "def moving_average_update(variable, value, momentum):",
            "-    return tf.python.training.moving_averages.assign_moving_average(",
            "+    return moving_averages.assign_moving_average(",
            "variable, value, momentum)"
        ]
    },
    {
        "number": 8106,
        "comments": "",
        "commit_message": "small fixes for remainder in all backends, when modulus=False.\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def remainder(",
            "res_floored = tf.where(res >= 0, tf.math.floor(res), tf.math.ceil(res))",
            "diff = res - res_floored",
            "diff, x2 = ivy.promote_types_of_inputs(diff, x2)",
            "-        return tf.cast(diff * x2, x1.dtype)",
            "+        return tf.cast(tf.round(diff * x2), x1.dtype)",
            "return tf.experimental.numpy.remainder(x1, x2)"
        ]
    },
    {
        "number": 8110,
        "comments": "",
        "commit_message": "Bugfix/all gather (#5221)\n\n* resolve bug\n\n* add tests\n\n* add tests\n\n* resolve flake8\n\n* update\n\n* update\n\n* remove globals\n\n* typo\n\n* Update pytorch_lightning/utilities/distributed.py\n\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\n\n* update\n\n* update\n\n* add suport int, float\n\n* update\n\n* resolve pep8\n\n* Update pytorch_lightning/core/lightning.py\n\nCo-authored-by: Adrian W\u00e4lchli <aedu.waelchli@gmail.com>\n\n* Update tests/utilities/test_all_gather_grad.py\n\nCo-authored-by: Adrian W\u00e4lchli <aedu.waelchli@gmail.com>\n\n* update doc\n\n* add bool and np.ndarray\n\n* resolve conflicts\n\n* resolve conflicts\n\n* resolve pep8\n\n* add changelog\n\n* Update pytorch_lightning/core/lightning.py\n\nCo-authored-by: Adrian W\u00e4lchli <aedu.waelchli@gmail.com>\n\nCo-authored-by: Ubuntu <ubuntu@ip-172-31-62-109.ec2.internal>\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\nCo-authored-by: Adrian W\u00e4lchli <aedu.waelchli@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "import torch",
            "from pytorch_lightning import _logger as log",
            "",
            "if torch.distributed.is_available():",
            "-    from torch.distributed import ReduceOp, group",
            "+    from torch.distributed import group, ReduceOp",
            "else:",
            "class ReduceOp:",
            "SUM = None"
        ]
    },
    {
        "number": 8115,
        "comments": "",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestSelfAttentiveSpanExtractor:",
            "numpy.testing.assert_array_almost_equal(spans[1].data.numpy(), mean_embeddings.data.numpy())",
            "",
            "# Now test the case in which we have some masked spans in our indices.",
            "-        indices_mask = torch.LongTensor([[1, 1], [1, 0]])",
            "+        indices_mask = torch.BoolTensor([[True, True], [True, False]])",
            "span_representations = extractor(sequence_tensor, indices, span_indices_mask=indices_mask)",
            "",
            "# First element in the batch."
        ]
    },
    {
        "number": 8116,
        "comments": "",
        "commit_message": "[rllib] Use suppress_output instead of run_silent.sh script for tests (#4386)\n\n* fix\n\n* enable custom loss\n\n* Update run_rllib_tests.sh\n\n* enable tests\n\n* fix action prob\n\n* Update suppress_output\n\n* fix example\n\n* fix\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class CustomLossModel(Model):",
            "input_ops = reader.tf_input_ops()",
            "",
            "# define a secondary loss by building a graph copy with weight sharing",
            "+        obs = tf.cast(input_ops[\"obs\"], tf.float32)",
            "logits, _ = self._build_layers_v2({",
            "-            \"obs\": restore_original_dimensions(input_ops[\"obs\"],",
            "-                                               self.obs_space)",
            "+            \"obs\": restore_original_dimensions(obs, self.obs_space)",
            "}, self.num_outputs, self.options)",
            "",
            "# You can also add self-supervised losses easily by referencing tensors"
        ]
    },
    {
        "number": 8117,
        "comments": "",
        "commit_message": "pseudo labeling bug fix\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def detect(save_img=False):",
            "",
            "save_path = str(Path(out) / Path(p).name)",
            "s += '%gx%g ' % img.shape[2:]  # print string",
            "-            gn = torch.tensor(im0s.shape)[[1, 0, 1, 0]]  # \u00a0normalization gain whwh",
            "+            gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # \u00a0normalization gain whwh",
            "if det is not None and len(det):",
            "# Rescale boxes from imgsz to im0 size",
            "det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()"
        ]
    },
    {
        "number": 8118,
        "comments": "",
        "commit_message": "Fix typo for named argument\n",
        "label": "",
        "answer": "no",
        "change": [
            "import tensorflow as tf",
            "",
            "def masked_softmax_cross_entropy(preds, labels, mask):",
            "\"\"\"Softmax cross-entropy loss with masking.\"\"\"",
            "-    loss = tf.nn.softmax_cross_entropy_with_logits(logits=preds, lables=labels)",
            "+    loss = tf.nn.softmax_cross_entropy_with_logits(logits=preds, labels=labels)",
            "mask = tf.cast(mask, dtype=tf.float32)",
            "mask /= tf.reduce_mean(mask)",
            "loss *= mask"
        ]
    },
    {
        "number": 8125,
        "comments": "",
        "commit_message": "feat: modelstore + some fixes (#1836)\n\n* fix: exceptions lazy loader\n\n* use pyyaml, clean up configurations, add LocalBentoMgr interface\n\n* build: add interface\n\n* build: prototype attrs\n\n* refactor: renaming\n\n* feat: list models\n\n* fix: logging\n\n* fix: ci\n\n* revert: cherry-pick chaoyu commits + finish base modelstore\n\n* feat: finish modelstore + model_yaml interface\n\nCo-authored-by: Chaoyu <paranoyang@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class KerasModel(Model):",
            "",
            "return model",
            "",
            "-    @catch_exceptions(",
            "-        catch_exc=ModuleNotFoundError, throw_exc=MissingDependencyException, msg=_exc",
            "-    )",
            "def save(self, path: PathType) -> None:",
            "tf.compat.v1.keras.backend.get_session()"
        ]
    },
    {
        "number": 8128,
        "comments": "",
        "commit_message": "Fixed style\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def perform_analysis_torch(preds, indices, noise_eps=0.1, delta=1e-5, moments=8,",
            "data_ind_eps_list = (data_ind_log_mgf - math.log(delta)) / l_list",
            "",
            "return min(eps_list_nm), min(data_ind_eps_list)",
            "-",
            "-",
            "-",
            "-"
        ]
    },
    {
        "number": 8129,
        "comments": "",
        "commit_message": "fix test_shape and lints\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def sum(",
            "]",
            "),",
            "dtype=dtype,",
            "-                out=out,",
            ")",
            "return torch.sum(input=x, dim=axis, dtype=dtype, keepdim=keepdims)"
        ]
    },
    {
        "number": 8130,
        "comments": "",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class F1MeasureTest(AllenNlpTestCase):",
            "assert_allclose(f1, 0.8)",
            "",
            "# Test the same thing with a mask:",
            "-        mask = torch.tensor([[0, 1, 0], [1, 1, 1]], device=device)",
            "+        mask = torch.BoolTensor([[False, True, False], [True, True, True]], device=device)",
            "f1_measure(predictions, targets, mask)",
            "precision, recall, f1 = f1_measure.get_metric()",
            "assert f1_measure._true_positives == 1.0"
        ]
    },
    {
        "number": 8138,
        "comments": "",
        "commit_message": "Fixed test discovery errors (#7136)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_tensorflow_l2_normalize(",
            "",
            "# trace",
            "@handle_frontend_test(",
            "+    fn_tree=\"tensorflow.linalg.trace\",",
            "dtype_and_input=_get_dtype_and_matrix(),",
            "num_positional_args=helpers.num_positional_args(",
            "fn_name=\"ivy.functional.frontends.tensorflow.linalg.trace\""
        ]
    },
    {
        "number": 8140,
        "comments": "",
        "commit_message": "More TF int dtype fixes (#20384)\n\n* Add a test to ensure int dummy inputs are int64\n\n* Move the test into the existing int64 test and update a lot of existing dummies\n\n* Fix remaining dummies\n\n* Fix remaining dummies\n\n* Test for int64 serving sigs as well\n\n* Update core tests to use tf.int64\n\n* Add better messages to the assertions\n\n* Update all serving sigs to int64\n\n* More sneaky hiding tf.int32s\n\n* Add an optional int32 signature in save_pretrained\n\n* make fixup\n\n* Add Amy's suggestions\n\n* Switch all serving sigs back to tf.int32\n\n* Switch all dummies to tf.int32\n\n* Adjust tests to check for tf.int32 instead of tf.int64\n\n* Fix base dummy_inputs dtype\n\n* Start casting to tf.int32 in input_processing\n\n* Change dtype for unpack_inputs test\n\n* Add proper tf.int32 test\n\n* Make the alternate serving signature int64\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TFVisionEncoderDecoderModel(TFPreTrainedModel, TFCausalLanguageModelingLos",
            "Returns:",
            "`Dict[str, tf.Tensor]`: The dummy inputs.",
            "\"\"\"",
            "-        decoder_input_ids = tf.constant(DUMMY_INPUTS)",
            "+        decoder_input_ids = tf.constant(DUMMY_INPUTS, dtype=tf.int32)",
            "batch_size, seq_len = decoder_input_ids.shape",
            "",
            "VISION_DUMMY_INPUTS = tf.random.uniform("
        ]
    },
    {
        "number": 8144,
        "comments": "",
        "commit_message": "fixes to elementwise\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def subtract(",
            "x1, x2 = ivy.promote_types_of_inputs(x1, x2)",
            "if alpha not in (1, None):",
            "x2 = x2 * alpha",
            "-    return tf.experimental.numpy.subtract(x1, x2)",
            "+    return tf.subtract(x1, x2)",
            "",
            "",
            "def tan("
        ]
    },
    {
        "number": 8149,
        "comments": "",
        "commit_message": "reworked runner module, fixed some test issues\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class MemoryModel(Model):",
            "# e.g. F T F F F F",
            "",
            "# Store the steps until end of the episode(s) determined by the input terminal signals (True starts new count).",
            "-        lengths = tf.scan(fn=len_, elems=terminal, initializer=0)",
            "+        lengths = tf.scan(",
            "+            fn=len_, elems=terminal,",
            "+            initializer=tf.zeros_like(tensor=terminal[0], dtype=tf.int32)",
            "+        )",
            "# e.g. 1 1 2 3 4 5",
            "off_horizon = tf.greater(lengths, tf.fill(dims=tf.shape(lengths), value=horizon))",
            "# e.g. F F F F T T"
        ]
    },
    {
        "number": 8150,
        "comments": "",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class IntraSentenceAttentionEncoder(Seq2SeqEncoder):",
            "return False",
            "",
            "@overrides",
            "-    def forward(self, tokens: torch.Tensor, mask: torch.Tensor):",
            "+    def forward(self, tokens: torch.Tensor, mask: torch.BoolTensor):",
            "batch_size, sequence_length, _ = tokens.size()",
            "# Shape: (batch_size, sequence_length, sequence_length)",
            "similarity_matrix = self._matrix_attention(tokens, tokens)"
        ]
    },
    {
        "number": 8153,
        "comments": "",
        "commit_message": "Not use -1e4 as attn mask (#17306)\n\n* Use torch.finfo(self.dtype).min\n\n* for GPTNeoX\n\n* for Albert\n\n* For Splinter\n\n* Update src/transformers/models/data2vec/modeling_data2vec_audio.py\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* fix -inf used in Bart-like models\n\n* Fix a few remaining -inf\n\n* more fix\n\n* clean up\n\n* For CLIP\n\n* For FSMT\n\n* clean up\n\n* fix test\n\n* Add dtype argument and use it for LayoutLMv3\n\n* update FlaxLongT5Attention\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class DetrMHAttentionMap(nn.Module):",
            "weights = torch.einsum(\"bqnc,bnchw->bqnhw\", queries_per_head * self.normalize_fact, keys_per_head)",
            "",
            "if mask is not None:",
            "-            weights.masked_fill_(mask.unsqueeze(1).unsqueeze(1), float(\"-inf\"))",
            "+            weights.masked_fill_(mask.unsqueeze(1).unsqueeze(1), torch.finfo(weights.dtype).min)",
            "weights = nn.functional.softmax(weights.flatten(2), dim=-1).view(weights.size())",
            "weights = self.dropout(weights)",
            "return weights"
        ]
    },
    {
        "number": 8155,
        "comments": "",
        "commit_message": "support prefix for FeedfreeInferenceRunner\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Trainer(object):",
            "summary (tf.Summary or str): a summary object, or a str which will",
            "be interpreted as a serialized tf.Summary protobuf.",
            "\"\"\"",
            "-        if isinstance(summary, six.string_types):",
            "+        if isinstance(summary, six.binary_type):",
            "summary = tf.Summary.FromString(summary)",
            "+        assert isinstance(summary, tf.Summary), type(summary)",
            "for val in summary.value:",
            "if val.WhichOneof('value') == 'simple_value':",
            "val.tag = re.sub('tower[p0-9]+/', '', val.tag)   # TODO move to subclasses"
        ]
    },
    {
        "number": 8157,
        "comments": "",
        "commit_message": "fix #4708 (#4805)\n\nCo-authored-by: moslehpour <moslehpour@meta.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class SinusoidalPositionalEmbedding(nn.Module):",
            "if self.onnx_trace:",
            "flat_embeddings = self.weights.detach().index_select(0, positions.view(-1))",
            "embedding_shape = torch.cat(",
            "-                (bsz.view(1), seq_len.view(1), torch.tensor([-1], dtype=torch.long))",
            "+                (bsz, seq_len, torch.tensor([-1], dtype=torch.long))",
            ")",
            "embeddings = torch.onnx.operators.reshape_from_tensor_shape(",
            "flat_embeddings, embedding_shape"
        ]
    },
    {
        "number": 8158,
        "comments": "",
        "commit_message": "Not use -1e4 as attn mask (#17306)\n\n* Use torch.finfo(self.dtype).min\n\n* for GPTNeoX\n\n* for Albert\n\n* For Splinter\n\n* Update src/transformers/models/data2vec/modeling_data2vec_audio.py\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* fix -inf used in Bart-like models\n\n* Fix a few remaining -inf\n\n* more fix\n\n* clean up\n\n* For CLIP\n\n* For FSMT\n\n* clean up\n\n* fix test\n\n* Add dtype argument and use it for LayoutLMv3\n\n* update FlaxLongT5Attention\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class VisualBertRegionToPhraseAttention(nn.Module):",
            "def forward(self, query, key, attention_mask):",
            "attention_mask = attention_mask.to(query.dtype)",
            "attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)",
            "-        attention_mask = (1.0 - attention_mask) * -10000.0",
            "+        attention_mask = (1.0 - attention_mask) * torch.finfo(query.dtype).min",
            "",
            "mixed_query_layer = self.query(query)",
            "mixed_key_layer = self.key(key)"
        ]
    },
    {
        "number": 8162,
        "comments": "",
        "commit_message": "fix: typo miss codespelling documentation and commented code\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class ParabolicEquationStepperTest(tf.test.TestCase, parameterized.TestCase):",
            "sizes=[100],",
            "dtype=np.float64))",
            "",
            "-    initial_values = tf.math.exp(grid[0])  # Initial condtion",
            "+    initial_values = tf.math.exp(grid[0])  # Initial condition",
            "time_step = 0.001",
            "final_t = 0.1",
            "if default_bc == 'left':"
        ]
    },
    {
        "number": 8164,
        "comments": "",
        "commit_message": "Fix checkpoint loading for HuggingFace encoders (#3010)\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class InputFeature(BaseFeature, LudwigModule, ABC):",
            "def update_config_with_metadata(feature_config, feature_metadata, *args, **kwargs):",
            "pass",
            "",
            "+    def update_config_after_module_init(self, feature_config):",
            "+        \"\"\"Updates the config after the torch.nn.Module objects have been initialized.\"\"\"",
            "+        pass",
            "+",
            "def initialize_encoder(self, encoder_config):",
            "encoder_cls = get_encoder_cls(self.type(), encoder_config.type)",
            "encoder_schema = encoder_cls.get_schema_cls().Schema()"
        ]
    },
    {
        "number": 8166,
        "comments": "",
        "commit_message": "Fix: formulas should use spots and not forwards.\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def bjerksund_stensland(*,",
            "volatilities=volatilities,",
            "strikes=strikes,",
            "expiries=expiries,",
            "-                forwards=forwards,",
            "+                spots=spots,",
            "discount_rates=discount_rates,",
            "cost_of_carries=cost_of_carries,",
            "is_call_options=is_call_options),",
            "# For put options, adjust inputs according to call-put transformation",
            "# function:  P(S, X, T, r, b, sigma) = C(X, S, T, r - b, -b, sigma)",
            "tf.where(is_call_options,",
            "-                bjerksund_stensland_model(forwards, strikes, expiries, discount_rates,",
            "+                bjerksund_stensland_model(spots, strikes, expiries, discount_rates,",
            "cost_of_carries, volatilities),",
            "-                bjerksund_stensland_model(strikes, forwards, expiries, discount_rates -",
            "+                bjerksund_stensland_model(strikes, spots, expiries, discount_rates -",
            "cost_of_carries, -cost_of_carries, volatilities)))",
            "",
            "return american_prices"
        ]
    },
    {
        "number": 8167,
        "comments": "",
        "commit_message": "improved global tensor handling, various other fixes\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def detectMinVal(input_mat, var, threshold=1e-6, name='', debug=False):",
            "input_mat_clipped = clipoutNeg(input_mat, threshold)",
            "",
            "if debug:",
            "-        input_mat_clipped = tf.cond(tf.logical_or(tf.greater(eigen_ratio, 0.), tf.less(eigen_ratio, -500)), lambda: input_mat_clipped, lambda: tf.Print(",
            "+        input_mat_clipped = self.cond(tf.logical_or(tf.greater(eigen_ratio, 0.), tf.less(eigen_ratio, -500)), lambda: input_mat_clipped, lambda: tf.Print(",
            "input_mat_clipped, [tf.convert_to_tensor('screwed ratio ' + name + ' eigen values!!!'), tf.convert_to_tensor(var.name), eigen_min, eigen_max, eigen_ratio]))",
            "",
            "return input_mat_clipped"
        ]
    },
    {
        "number": 8170,
        "comments": "",
        "commit_message": "[DQN] fix the layout when channel>1\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def maskrcnn_loss(mask_logits, fg_labels, fg_target_masks):",
            "\"\"\"",
            "Args:",
            "mask_logits: #fg x #category xhxw",
            "-        fg_labels: #fg, in 1~#class",
            "+        fg_labels: #fg, in 1~#class, int64",
            "fg_target_masks: #fgxhxw, int",
            "\"\"\"",
            "-    num_fg = tf.size(fg_labels)",
            "-    indices = tf.stack([tf.range(num_fg), tf.to_int32(fg_labels) - 1], axis=1)  # #fgx2",
            "+    num_fg = tf.size(fg_labels, out_type=tf.int64)",
            "+    indices = tf.stack([tf.range(num_fg), fg_labels - 1], axis=1)  # #fgx2",
            "mask_logits = tf.gather_nd(mask_logits, indices)  # #fgxhxw",
            "mask_probs = tf.sigmoid(mask_logits)"
        ]
    },
    {
        "number": 8179,
        "comments": "",
        "commit_message": "Resuming bug fixes, keras callback with tf2, lazy load fastai\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Graph(object):",
            "if relevant_nodes and in_node not in relevant_nodes:",
            "# node is not part of the current network",
            "continue",
            "-                    for in_layer in in_node.inbound_layers:",
            "+                    for in_layer in nest.flatten(in_node.inbound_layers):",
            "inbound_keras_node = Node.from_keras(in_layer)",
            "",
            "if (inbound_keras_node.id not in graph.nodes_by_id):"
        ]
    },
    {
        "number": 8185,
        "comments": "",
        "commit_message": "refactor hdfs_path; fix more deprecation warnings; print label+preds\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def main_fun(argv, ctx):",
            "saver = tf.train.Saver(tf.global_variables())",
            "",
            "# Build the summary operation from the last tower summaries.",
            "-      summary_op = tf.merge_summary(summaries)",
            "+      summary_op = tf.summary.merge(summaries)",
            "",
            "# Build an initialization operation to run below.",
            "init = tf.global_variables_initializer()"
        ]
    },
    {
        "number": 8194,
        "comments": "",
        "commit_message": "Quick fix :) (#606)\n\n* Changing the name\n\n* style + quality\n\n* update doc and logo\n\n* clean up\n\n* circle-CI on the branche for now\n\n* fix daily dialog dataset\n\n* fix urls\n\nCo-authored-by: Quentin Lhoest <lhoest.q@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class LibrispeechLm(nlp.GeneratorBasedBuilder):",
            "\"\"\"Returns SplitGenerators.\"\"\"",
            "archive_path = dl_manager.download_and_extract(_DL_URL)",
            "return [",
            "-            nlp.SplitGenerator(name=nlp.Split.TRAIN, gen_kwargs={\"archive_path\": archive_path}),",
            "+            datasets.SplitGenerator(name=datasets.Split.TRAIN, gen_kwargs={\"archive_path\": archive_path}),",
            "]",
            "",
            "def _generate_examples(self, archive_path):"
        ]
    },
    {
        "number": 8195,
        "comments": "",
        "commit_message": "general.py::gather fixes (#4037)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def to_list(x: Union[tf.Tensor, tf.Variable], /) -> list:",
            "def gather(",
            "params: Union[tf.Tensor, tf.Variable],",
            "indices: Union[tf.Tensor, tf.Variable],",
            "-    axis: Optional[int] = -1,",
            "+    /,",
            "*,",
            "+    axis: Optional[int] = -1,",
            "out: Optional[Union[tf.Tensor, tf.Variable]] = None,",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "axis = axis % len(indices.shape)",
            "-    return tf.gather(params, indices, axis=axis, batch_dims=axis)",
            "+    return tf.gather(params, indices, axis=axis, batch_dims=None)",
            "",
            "",
            "def gather_nd("
        ]
    },
    {
        "number": 8199,
        "comments": "",
        "commit_message": "Add map_location when loading model checkpoint (#1084)\n\nFixes #1078\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Trainer:",
            "self.model = DDP(self.model, device_ids=[self.local_rank])",
            "",
            "def _load_snapshot(self, snapshot_path):",
            "-        snapshot = torch.load(snapshot_path)",
            "+        loc = f\"cuda:{self.gpu_id}\"",
            "+        snapshot = torch.load(snapshot_path, map_location=loc)",
            "self.model.load_state_dict(snapshot[\"MODEL_STATE\"])",
            "self.epochs_run = snapshot[\"EPOCHS_RUN\"]",
            "print(f\"Resuming training from snapshot at Epoch {self.epochs_run}\")"
        ]
    },
    {
        "number": 8205,
        "comments": "",
        "commit_message": "Deprecate LKJCorrCholesky distribution (#2771)\n\n* Deprecate LKJCorrCholesky distribution\n\n* fix tests; add FutureWarning\n\n* Fix failing examples/lkj.py\n\n* Fix deprecation warning in bart example\n\n* Address review comment\n\nCo-authored-by: Neeraj Pradhan <neerajprad@fb.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def model(y):",
            "# Vector of variances for each of the d variables",
            "theta = pyro.sample(\"theta\", dist.HalfCauchy(torch.ones(d, **options)))",
            "# Lower cholesky factor of a correlation matrix",
            "-    eta = torch.ones(1, **options)  # Implies a uniform distribution over correlation matrices",
            "-    L_omega = pyro.sample(\"L_omega\", dist.LKJCorrCholesky(d, eta))",
            "+    concentration = torch.ones((), **options)  # Implies a uniform distribution over correlation matrices",
            "+    L_omega = pyro.sample(\"L_omega\", dist.LKJCholesky(d, concentration))",
            "# Lower cholesky factor of the covariance matrix",
            "L_Omega = torch.mm(torch.diag(theta.sqrt()), L_omega)",
            "# For inference with SVI, one might prefer to use torch.bmm(theta.sqrt().diag_embed(), L_omega)"
        ]
    },
    {
        "number": 8207,
        "comments": "",
        "commit_message": "[fix] AugmentationSequential bbox keypoints transformation fix (#1570)\n\n* bug fix\n\n* added test\n\n* bug fix\n\n* bug fix\n\n* Update kornia/augmentation/container/utils.py\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestAugmentationSequential:",
            "points = torch.tensor([[[0.0, 0.0], [1.0, 1.0]]], device=device, dtype=dtype).expand(3, -1, -1)",
            "aug = K.AugmentationSequential(",
            "K.RandomCrop((3, 3), padding=1, cropping_mode='resample', fill=0),",
            "+            K.RandomAffine((360., 360.), p=1.),",
            "data_keys=[\"input\", \"mask\", \"bbox_xyxy\", \"keypoints\"],",
            ")"
        ]
    },
    {
        "number": 8208,
        "comments": "",
        "commit_message": "enable black in the precommit (#1777)\n\n* enable black in the precommit\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add some fixes\n\n* update libface detection url\n\n* added url from kornia checkpoint\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class MS_SSIMLoss(nn.Module):",
            "loss_l1 = F.l1_loss(img1, img2, reduction='none')",
            "",
            "# Compute average l1 loss in 3 channels",
            "-        gaussian_l1 = F.conv2d(loss_l1, g_masks[-CH:], groups=CH, padding=self.pad).mean(",
            "-            1",
            "-        )",
            "+        gaussian_l1 = F.conv2d(loss_l1, g_masks[-CH:], groups=CH, padding=self.pad).mean(1)",
            "",
            "# Compute MS-SSIM + L1 loss",
            "loss = self.alpha * loss_ms_ssim + (1 - self.alpha) * gaussian_l1 / self.DR"
        ]
    },
    {
        "number": 8213,
        "comments": "",
        "commit_message": "fix colocation problems\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def add_moving_summary(*args, **kwargs):",
            "ema_op = moving_averages.assign_moving_average(",
            "ema_var, c, decay,",
            "zero_debias=True, name=name + '_EMA_apply')",
            "-            tf.summary.scalar(name + '-summary', ema_op)    # write the EMA value as a summary",
            "ema_ops.append(ema_op)",
            "+        # cannot add it into colocate group -- will force everything to cpus",
            "+        tf.summary.scalar(name + '-summary', ema_op)    # write the EMA value as a summary",
            "if coll is not None:",
            "for op in ema_ops:",
            "# TODO a new collection to summary every step?"
        ]
    },
    {
        "number": 8215,
        "comments": "",
        "commit_message": "fix deprecations about casting & initializers in tf1.13\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Model(ModelDesc):",
            "cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)",
            "cost = tf.reduce_mean(cost, name='cross_entropy_loss')",
            "",
            "-        tf.reduce_mean(tf.to_float(tf.nn.in_top_k(logits, label, 1)), name='accuracy')",
            "+        tf.reduce_mean(tf.cast(tf.nn.in_top_k(logits, label, 1)), tf.float32, name='accuracy')",
            "",
            "wd_cost = tf.multiply(1e-5,",
            "regularize_cost('fc.*/W', tf.nn.l2_loss),"
        ]
    },
    {
        "number": 8216,
        "comments": "",
        "commit_message": "[FIX] Enable YOLOX training on different devices (#7912)\n\n* Enable yolox training on different devices\n\n* Enable yolox resize test on cpu\n",
        "label": "",
        "answer": "no",
        "change": [
            "class YOLOX(SingleStageDetector):",
            "gt_bbox[..., 1::2] = gt_bbox[..., 1::2] * scale_y",
            "return img, gt_bboxes",
            "",
            "-    def _random_resize(self):",
            "-        tensor = torch.LongTensor(2).cuda()",
            "+    def _random_resize(self, device):",
            "+        tensor = torch.LongTensor(2).to(device)",
            "",
            "if self.rank == 0:",
            "size = random.randint(*self._random_size_range)"
        ]
    },
    {
        "number": 8234,
        "comments": "",
        "commit_message": "[Fix] Missing device initializations (#672)\n\n* Fix device initialization in several variables\n\n* Fix device initialization in several variables\n\n* Skip CUDA tests for #666\n\n* Lint\n\n* Fix device initialization in several variables\n\n* Skip CUDA tests for #666\n\n* Lint\n\n* Remove half-precision tests (#649) and add a jit test\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestPerspective:",
            "",
            "def test_smoke(self, device):",
            "x_data = torch.rand(1, 2, 3, 4).to(device)",
            "-        batch_prob = torch.rand(1) < 0.5",
            "+        batch_prob = torch.rand(1, device=device) < 0.5",
            "start_points = torch.rand(1, 4, 2).to(device)",
            "end_points = torch.rand(1, 4, 2).to(device)"
        ]
    },
    {
        "number": 8238,
        "comments": "",
        "commit_message": "fix: rename attention_v\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Attention(BaseDenseAttention):",
            "# Reshape into [batch_size, 1, Tv, dim].",
            "k_reshaped = tf.expand_dims(key, axis=-3)",
            "if self.scale is not None:",
            "-        scores = self.attention_v * tf.reduce_sum(",
            "+        scores = self.concat_score_weight * tf.reduce_sum(",
            "tf.tanh(self.scale * (q_reshaped + k_reshaped)), axis=-1)",
            "else:",
            "-        scores = self.attention_v * tf.reduce_sum(",
            "+        scores = self.concat_score_weight * tf.reduce_sum(",
            "tf.tanh(q_reshaped + k_reshaped), axis=-1)",
            "",
            "return scores"
        ]
    },
    {
        "number": 8240,
        "comments": "",
        "commit_message": "fix pytest to import torch\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "import pytest",
            "-",
            "-try:",
            "-    import torch",
            "-    from torch.autograd import Variable",
            "-except:",
            "-    pytest.skip(\"pytorch is not installed\")",
            "-",
            "+pytest.importorskip('torch')",
            "+import torch",
            "from e2e_asr_attctc_th import pad_list, mask_by_length"
        ]
    },
    {
        "number": 8242,
        "comments": "",
        "commit_message": "Add `with torch.no_grad()` to DistilBERT integration test forward pass (#14979)\n\n* refactor: wrap forward pass around no_grad context\n\n* Update tests/test_modeling_distilbert.py\n\n* fix: rm `no_grad` from non-integration tests\n\n* chore: rm whitespace change\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class DistilBertModelIntergrationTest(unittest.TestCase):",
            "model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")",
            "input_ids = torch.tensor([[0, 345, 232, 328, 740, 140, 1695, 69, 6078, 1588, 2]])",
            "attention_mask = torch.tensor([[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])",
            "-        output = model(input_ids, attention_mask=attention_mask)[0]",
            "+        with torch.no_grad():",
            "+            output = model(input_ids, attention_mask=attention_mask)[0]",
            "expected_shape = torch.Size((1, 11, 768))",
            "self.assertEqual(output.shape, expected_shape)",
            "expected_slice = torch.tensor("
        ]
    },
    {
        "number": 8243,
        "comments": "",
        "commit_message": "fixed wild bfloat, implemented POC MOE model\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def model_fn(features, labels, mode, params):",
            "replica_cache_size = 300 * 1000000  # 300M per replica",
            "# Worker 0 caches all the TPU binaries.",
            "worker0_mem = replica_cache_size * ctx.num_replicas",
            "-        devices_memory_usage = [worker0_mem] + [0] * (num_hosts - 1)",
            "+        devices_memeory_usage = [worker0_mem] + [0] * (num_hosts - 1)",
            "var_placer = mtf.utils.BalancedVariablePlacer(device_list,",
            "-                                                      devices_memory_usage)",
            "+                                                      devices_memeory_usage)",
            "mesh_devices = [''] * mesh_shape.size",
            "mesh_impl = mtf.simd_mesh_impl.SimdMeshImpl(",
            "mesh_shape, layout_rules, mesh_devices, ctx.device_assignment)"
        ]
    },
    {
        "number": 8244,
        "comments": "",
        "commit_message": "some more fixes\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_batch():",
            "",
            "data_list = batch.to_data_list()",
            "assert len(data_list) == 2",
            "-    assert len(data_list[0]) == 4",
            "+    assert len(data_list[0]) == 3",
            "assert data_list[0].x.tolist() == [1, 2, 3]",
            "assert data_list[0].edge_index.tolist() == [[0, 1, 1, 2], [1, 0, 2, 1]]",
            "assert data_list[0].s == '1'"
        ]
    },
    {
        "number": 8245,
        "comments": "",
        "commit_message": "fix dtype\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "from ...proto.lib.torch.tensor_pb2 import TensorData",
            "",
            "",
            "def object2proto(obj: np.ndarray) -> TensorData:",
            "-    tensor = torch.Tensor(obj)",
            "+    tensor = torch.from_numpy(obj).clone()",
            "tensor_proto = protobuf_tensor_serializer(tensor)",
            "",
            "return tensor_proto"
        ]
    },
    {
        "number": 8246,
        "comments": "",
        "commit_message": "use assertClose\n\nSummary: use assertClose in some tests, which enforces shape equality. Fixes some small problems, including graph_conv on an empty graph.\n\nReviewed By: nikhilaravi\n\nDifferential Revision: D20556912\n\nfbshipit-source-id: 60a61eafe3c03ce0f6c9c1a842685708fb10ac5b\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestMeshes(TestCaseMixin, unittest.TestCase):",
            "# Multiple meshes in the batch with equal sized meshes",
            "meshes_extended = mesh.extend(3)",
            "for m in meshes_extended.verts_normals_list():",
            "-            self.assertTrue(torch.allclose(m, verts_normals_expected))",
            "+            self.assertClose(m, verts_normals_expected)",
            "for f in meshes_extended.faces_normals_list():",
            "-            self.assertTrue(torch.allclose(f, faces_normals_expected))",
            "+            self.assertClose(f, faces_normals_expected)",
            "",
            "# Multiple meshes in the batch with different sized meshes",
            "# Check padded and packed normals are the correct sizes."
        ]
    },
    {
        "number": 8248,
        "comments": "",
        "commit_message": "fix linter errors\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def homography_i_H_ref(pinhole_i, pinhole_ref):",
            "i_pose_base = get_optical_pose_base(pinhole_i)",
            "ref_pose_base = get_optical_pose_base(pinhole_ref)",
            "i_pose_ref = torch.matmul(i_pose_base, inverse_pose(ref_pose_base))",
            "-    return torch.matmul(pinhole_matrix(pinhole_i), \\",
            "-        torch.matmul(i_pose_ref, inv_pinhole_matrix(pinhole_ref)))",
            "+    return torch.matmul(",
            "+        pinhole_matrix(pinhole_i),",
            "+        torch.matmul(",
            "+            i_pose_ref,",
            "+            inv_pinhole_matrix(pinhole_ref)))"
        ]
    },
    {
        "number": 8249,
        "comments": "",
        "commit_message": "Fix `batch_dot` of TensorFlow when `axes=None` (#9922)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def batch_dot(x, y, axes=None):",
            "axes = (axes, axes)",
            "x_ndim = ndim(x)",
            "y_ndim = ndim(y)",
            "+    if axes is None:",
            "+        # behaves like tf.batch_matmul as default",
            "+        axes = [x_ndim - 1, y_ndim - 2]",
            "if x_ndim > y_ndim:",
            "diff = x_ndim - y_ndim",
            "y = tf.reshape(y, tf.concat([tf.shape(y), [1] * (diff)], axis=0))"
        ]
    },
    {
        "number": 8251,
        "comments": "",
        "commit_message": "Fix BART tests on GPU (#4298)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestSinusoidalPositionalEmbeddings(unittest.TestCase):",
            "# test that forward pass is just a lookup, there is no ignore padding logic",
            "input_ids = torch.tensor([[4, 10, pad, pad, pad]], dtype=torch.long, device=torch_device)",
            "no_cache_pad_zero = emb1(input_ids)",
            "-        self.assertTrue(torch.allclose(torch.Tensor(self.desired_weights), no_cache_pad_zero[:3, :5], atol=1e-3))",
            "+        self.assertTrue(",
            "+            torch.allclose(",
            "+                torch.tensor(self.desired_weights, device=torch_device), no_cache_pad_zero[:3, :5], atol=1e-3",
            "+            )",
            "+        )"
        ]
    },
    {
        "number": 8255,
        "comments": "",
        "commit_message": "Formatting fixes to meshgrid on all backends (#7579)\n\nCo-authored-by: @simonetgordon <simonegordon12@icloud.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def linspace_helper(start, stop, num, axis=None, *, dtype=None, device):",
            "",
            "",
            "def meshgrid(",
            "-    *arrays: torch.Tensor, sparse: bool = False, indexing=\"xy\"",
            "+    *arrays: torch.Tensor,",
            "+    sparse: bool = False,",
            "+    indexing: str = \"xy\",",
            ") -> List[torch.Tensor]:",
            "if not sparse:",
            "return list(torch.meshgrid(*arrays, indexing=indexing))"
        ]
    },
    {
        "number": 8256,
        "comments": "",
        "commit_message": "fix rnnlm load bug\n\nthis should be `torch_load()` instead of `torch.load()`\n",
        "label": "",
        "answer": "no",
        "change": [
            "def train(args):",
            "rnnlm = lm_pytorch.ClassifierWithState(",
            "lm_pytorch.RNNLM(",
            "len(args.char_list), rnnlm_args.layer, rnnlm_args.unit))",
            "-        torch.load(args.rnnlm, rnnlm)",
            "+        torch_load(args.rnnlm, rnnlm)",
            "model.rnnlm = rnnlm",
            "",
            "# write model config"
        ]
    },
    {
        "number": 8259,
        "comments": "",
        "commit_message": "agents and models base classes moved, various fixes\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Model(object):",
            "else:",
            "assert not config.global_model and config.session is None",
            "tf.reset_default_graph()",
            "-            self.session = tf.Session()",
            "+            self.session = config.session = tf.Session()",
            "",
            "if config.distributed and not config.global_model:",
            "# Global and local model for asynchronous updates"
        ]
    },
    {
        "number": 8261,
        "comments": "",
        "commit_message": "fix tests\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class PipelineTesterMixin(unittest.TestCase):",
            "image_slice = image[0, -1, -3:, -3:].cpu()",
            "",
            "assert image.shape == (1, 3, 32, 32)",
            "-        expected_slice = torch.tensor([-0.7688, -0.7690, -0.7597, -0.7660, -0.7713, -0.7531, -0.7009, -0.7098, -0.7350])",
            "+        expected_slice = torch.tensor([-0.7383, -0.7385, -0.7298, -0.7364, -0.7414, -0.7239, -0.6737, -0.6813, -0.7068])",
            "assert (image_slice.flatten() - expected_slice).abs().max() < 1e-2"
        ]
    },
    {
        "number": 8270,
        "comments": "",
        "commit_message": "only freeze trainable variables (fix #351)\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def freeze_variables():",
            "with varreplace.freeze_variable():",
            "x = FullyConnected('fc', x, 1000)   # fc/* will not be trained",
            "\"\"\"",
            "-    return remap_variables(lambda v: tf.stop_gradient(v))",
            "+    def custom_getter(getter, *args, **kwargs):",
            "+        v = getter(*args, **kwargs)",
            "+        if kwargs.pop('trainable', True):",
            "+            v = tf.stop_gradient(v)",
            "+        return v",
            "+    return custom_getter_scope(custom_getter)",
            "",
            "",
            "@deprecated(\"Renamed to remap_variables\", \"2017-11-06\")"
        ]
    },
    {
        "number": 8273,
        "comments": "",
        "commit_message": "Fix `bipartite_sage_unsup` example  (#6550)\n\nFixes #6549\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def test(loader):",
            "target = batch['user', 'item'].edge_label.long().cpu()",
            "",
            "preds.append(pred)",
            "-        targets.append(pred)",
            "+        targets.append(target)",
            "",
            "pred = torch.cat(preds, dim=0).numpy()",
            "-    target = torch.cat(target, dim=0).numpy()",
            "+    target = torch.cat(targets, dim=0).numpy()",
            "",
            "+    pred = pred > 0.5",
            "acc = accuracy_score(target, pred)",
            "prec = precision_score(target, pred)",
            "rec = recall_score(target, pred)"
        ]
    },
    {
        "number": 8275,
        "comments": "",
        "commit_message": "Fixes to enable dev branch to move to pytorch-1.2 (#2001)\n\n* Fixes to enable dev branch to move to pytorch-1.2\n\n* change name for published cpu wheels\n\n* fix failing unit tests\n\n* change docs conf\n\n* change tracking example\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class MaskedMixture(TorchDistribution):",
            "arg_constraints = {}  # nothing can be constrained",
            "",
            "def __init__(self, mask, component0, component1, validate_args=None):",
            "-        if not torch.is_tensor(mask) or mask.dtype != torch.uint8:",
            "-            raise ValueError('Expected mask to be a ByteTensor but got {}'.format(type(mask)))",
            "+        if not torch.is_tensor(mask) or mask.dtype != torch.bool:",
            "+            raise ValueError('Expected mask to be a BoolTensor but got {}'.format(type(mask)))",
            "if component0.event_shape != component1.event_shape:",
            "raise ValueError('components event_shape disagree: {} vs {}'",
            ".format(component0.event_shape, component1.event_shape))"
        ]
    },
    {
        "number": 8276,
        "comments": "",
        "commit_message": "[FasterRCNN] fix bug in last commit\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Model(ModelDesc):",
            "decoded_boxes = decode_bbox_target(",
            "fastrcnn_box_logits /",
            "tf.constant(config.FASTRCNN_BBOX_REG_WEIGHTS), anchors)",
            "-            decoded_boxes = clip_boxes(decoded_boxes, tf.shape(image)[:2], name='fastrcnn_all_boxes')",
            "+            decoded_boxes = clip_boxes(decoded_boxes, image_shape2d, name='fastrcnn_all_boxes')",
            "",
            "# indices: Nx2. Each index into (#proposal, #category)",
            "pred_indices, final_probs = fastrcnn_predictions(decoded_boxes, label_probs)"
        ]
    },
    {
        "number": 8280,
        "comments": "",
        "commit_message": "loca sens attn fix\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class AttentionRNNCell(nn.Module):",
            "out_dim (int): context vector feature dimension.",
            "rnn_dim (int): rnn hidden state dimension.",
            "annot_dim (int): annotation vector feature dimension.",
            "-            memory_dim (int): memory vector (decoder autogression) feature dimension.",
            "+            memory_dim (int): memory vector (decoder output) feature dimension.",
            "align_model (str): 'b' for Bahdanau, 'ls' Location Sensitive alignment.",
            "\"\"\"",
            "super(AttentionRNNCell, self).__init__()",
            "self.align_model = align_model",
            "-        self.rnn_cell = nn.GRUCell(out_dim + memory_dim, rnn_dim)",
            "+        self.rnn_cell = nn.GRUCell(annot_dim + memory_dim, rnn_dim)",
            "# pick bahdanau or location sensitive attention",
            "if align_model == 'b':",
            "-            self.alignment_model = BahdanauAttention(annot_dim, out_dim, out_dim)",
            "+            self.alignment_model = BahdanauAttention(annot_dim, rnn_dim, out_dim)",
            "if align_model == 'ls':",
            "self.alignment_model = LocationSensitiveAttention(annot_dim, rnn_dim, out_dim)",
            "else:"
        ]
    },
    {
        "number": 8282,
        "comments": "",
        "commit_message": "fix test_homography_warper_gradcheck\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def create_random_homography(batch_size, eye_size, std_val=1e-3):",
            "return eye + std.uniform_(-std_val, std_val)",
            "",
            "",
            "-def tensor_to_gradcheck_var(tensor):",
            "+def tensor_to_gradcheck_var(tensor, dtype=torch.float64, requires_grad=True):",
            "\"\"\"Converts the input tensor to a valid variable to check the gradient.",
            "`gradcheck` needs 64-bit floating point and requires gradient.",
            "\"\"\"",
            "assert torch.is_tensor(tensor), type(tensor)",
            "-    return tensor.requires_grad_(True).type(torch.DoubleTensor)",
            "+    return tensor.requires_grad_(requires_grad).type(dtype)",
            "",
            "",
            "def compute_mse(x, y):"
        ]
    },
    {
        "number": 8283,
        "comments": "",
        "commit_message": "[rllib] [tune] Custom preprocessors and models, various fixes (#1372)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "from ray.rllib.optimizers import SampleBatch, TFMultiGPUSupport",
            "class DQNEvaluator(TFMultiGPUSupport):",
            "\"\"\"The base DQN Evaluator that does not include the replay buffer.\"\"\"",
            "",
            "-    def __init__(self, env_creator, config, logdir):",
            "+    def __init__(self, registry, env_creator, config, logdir):",
            "env = env_creator()",
            "-        env = wrap_dqn(env, config[\"model\"])",
            "+        env = wrap_dqn(registry, env, config[\"model\"])",
            "self.env = env",
            "self.config = config",
            "",
            "tf_config = tf.ConfigProto(**config[\"tf_session_args\"])",
            "self.sess = tf.Session(config=tf_config)",
            "-        self.dqn_graph = models.DQNGraph(env, config, logdir)",
            "+        self.dqn_graph = models.DQNGraph(registry, env, config, logdir)",
            "",
            "# Create the schedule for exploration starting from 1.",
            "self.exploration = LinearSchedule("
        ]
    },
    {
        "number": 8286,
        "comments": "",
        "commit_message": "Reimplement Sparse R-CNN (#4219)\n\n* draft version train and test\n\n* remove get_bbox in sparse_head\n\n* refactor SparseRCNN model, align inference eval mAP with official repo\n\n* add focal_loss in hungarian_assigner.py\n\n* update hungarian_assigner.py\n\n* update detr config\n\n* update doc and codes\n\n* fix unitest\n\n* fix unitest\n\n* add embedding_rpn and traing code\n\n* modify code format\n\n* fix proposalist in simpletest\n\n* add focal loss traing\n\n* fix acc\n\n* fix reduce mean\n\n* add embedding rpn doc and add show initproposals\n\n* add docstr\n\n* fix backbone lr\n\n* fix typo and change detach_proposal_list to proposal_list\n\n* remove debug assert\n\n* add readme\n\n* add config for random crop when use 300 proposals\n\n* add aug for 300 proposals\n\n* add 300 proposals results and fix docstr\n\n* add 1x results\n\n* fix lint\n\n* fix unitest\n\n* fix docstr of embeddingrpn\n\n* fix according to the comments\n\n* fix get_bbox and a bug of dynamic conv\n\n* support cpu focal loss and fix docstr\n\n* fix docstr\n\n* fix unitest\n\n* fix bug of  init_weight\n\n* fix to device in assign\n\n* fix all commemt\n\n* fix test_config\n\n* remove dim 1 in forward\n\n* fix shape in comment\n\n* fix the case that train_cfg is None\n\n* fix shape of docstr\n\n* fix docstr\n\n* fix typo\n\n* simplify test\n\n* fix docstr\n\n* move train_cfg and test_cfg to model as\n\n* fix unitest\n\n* add link of ckpt and log\n\n* fix 1x results\n\nCo-authored-by: tianyuandu <tianyuandu@gmail.com>\nCo-authored-by: fangkairen <fangkairen@sensetime.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class HungarianAssigner(BaseAssigner):",
            "return AssignResult(",
            "num_gts, assigned_gt_inds, None, labels=assigned_labels)",
            "img_h, img_w, _ = img_meta['img_shape']",
            "-        factor = torch.Tensor([img_w, img_h, img_w,",
            "-                               img_h]).unsqueeze(0).to(gt_bboxes.device)",
            "+        factor = gt_bboxes.new_tensor([img_w, img_h, img_w,",
            "+                                       img_h]).unsqueeze(0)",
            "",
            "# 2. compute the weighted costs",
            "# classification and bboxcost."
        ]
    },
    {
        "number": 8292,
        "comments": "",
        "commit_message": "fix static batch size in Conv2DTranspose\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def Conv2DTranspose(",
            "shape4d(strides, data_format=data_format),",
            "padding=padding.upper(),",
            "data_format=data_format)",
            "-        conv.set_shape(tf.TensorShape([None] + out_shape3_sta))",
            "+        conv.set_shape(tf.TensorShape([shape_sta[0]] + out_shape3_sta))",
            "",
            "ret = tf.nn.bias_add(conv, b, data_format=data_format) if use_bias else conv",
            "if activation is not None:"
        ]
    },
    {
        "number": 8296,
        "comments": "",
        "commit_message": "Fix shape inference error for newly version Tensorflow in ctc_label_dense_to_sparse (#3955)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def ctc_label_dense_to_sparse(labels, label_lengths):",
            "max_num_labels_tns = tf.pack([label_shape[1]])",
            "",
            "def range_less_than(previous_state, current_input):",
            "-        return tf.expand_dims(tf.range(label_shape[1]), 0) < current_input",
            "+        return tf.expand_dims(tf.range(label_shape[1]), 0) < tf.fill(max_num_labels_tns, current_input)",
            "",
            "-    init = tf.cast(tf.fill(max_num_labels_tns, 0), tf.bool)",
            "+    init = tf.cast(tf.fill([1, label_shape[1]], 0), tf.bool)",
            "dense_mask = functional_ops.scan(range_less_than, label_lengths,",
            "initializer=init, parallel_iterations=1)",
            "dense_mask = dense_mask[:, 0, :]"
        ]
    },
    {
        "number": 8298,
        "comments": "",
        "commit_message": "lintfixbot: Auto-commit fixed lint errors in codebase\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_frontend_function(",
            "# strip the decorator to get an Ivy array",
            "# ToDo, fix testing for jax frontend for x32",
            "if frontend == \"jax\":",
            "-            importlib.import_module(",
            "-                'ivy.functional.frontends.jax').config.update('jax_enable_x64', True)",
            "+            importlib.import_module(\"ivy.functional.frontends.jax\").config.update(",
            "+                \"jax_enable_x64\", True",
            "+            )",
            "ret = get_frontend_ret(frontend_fn, *args_ivy, **kwargs_ivy)",
            "if with_out:",
            "if not inspect.isclass(ret):"
        ]
    },
    {
        "number": 8304,
        "comments": "",
        "commit_message": "fix resize_token_embeddings (#11572)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin, PushToHubMix",
            ")",
            "",
            "# Build new embeddings",
            "-        new_embeddings = nn.Embedding(new_num_tokens, old_embedding_dim).to(self.device)",
            "+        new_embeddings = nn.Embedding(new_num_tokens, old_embedding_dim).to(",
            "+            self.device, dtype=old_embeddings.weight.dtype",
            "+        )",
            "",
            "# initialize all new embeddings (in particular added tokens)",
            "self._init_weights(new_embeddings)"
        ]
    },
    {
        "number": 8306,
        "comments": "",
        "commit_message": "upgrade to pytorch 1.2 (#3182)\n\n* first attempt at pytorch 1.2\n\n* explicit is better than implicit\n\n* more explicit\n\n* attempt to fix flaky tests\n\n* pylint\n\n* no disable dropout\n\n* disable dropout by default\n\n* restore dropout, don't deepcopy\n\n* change batch size for biaffine_dependency_parser_multilang_test, maybe that will make it pass? :(\n\n* try batch size 10\n\n* ignore bad gradient parameter\n\n* cleanup\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class CrfTagger(Model):",
            "if tags is not None:",
            "# Add negative log-likelihood as loss",
            "log_likelihood = self.crf(logits, tags, mask)",
            "-            output[\"loss\"] = -log_likelihood",
            "+",
            "+            # It's not clear why, but pylint seems to think `log_likelihood` is tuple",
            "+            # (in fact, it's a torch.Tensor), so we need a disable.",
            "+            output[\"loss\"] = -log_likelihood  # pylint: disable=invalid-unary-operand-type",
            "",
            "# Represent viterbi tags as \"class probabilities\" that we can",
            "# feed into the metrics"
        ]
    },
    {
        "number": 8311,
        "comments": "",
        "commit_message": "Fix shape inference issue with TF.resize_images\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def resize_images(X, height_factor, width_factor, dim_ordering):",
            "positive integers.",
            "'''",
            "if dim_ordering == 'th':",
            "+        original_shape = int_shape(X)",
            "new_shape = tf.shape(X)[2:]",
            "new_shape *= tf.constant(np.array([height_factor, width_factor]).astype('int32'))",
            "X = permute_dimensions(X, [0, 2, 3, 1])",
            "X = tf.image.resize_nearest_neighbor(X, new_shape)",
            "-        return permute_dimensions(X, [0, 3, 1, 2])",
            "+        X = permute_dimensions(X, [0, 3, 1, 2])",
            "+        X.set_shape((None, None, original_shape[2] * height_factor, original_shape[3] * width_factor))",
            "+        return X",
            "elif dim_ordering == 'tf':",
            "+        original_shape = int_shape(X)",
            "new_shape = tf.shape(X)[1:3]",
            "new_shape *= tf.constant(np.array([height_factor, width_factor]).astype('int32'))",
            "-        return tf.image.resize_nearest_neighbor(X, new_shape)",
            "+        X = tf.image.resize_nearest_neighbor(X, new_shape)",
            "+        X.set_shape((None, original_shape[1] * height_factor, original_shape[2] * width_factor, None))",
            "+        return X",
            "else:",
            "raise Exception('Invalid dim_ordering: ' + dim_ordering)"
        ]
    },
    {
        "number": 8314,
        "comments": "",
        "commit_message": "Fixes warnings and add compatibility stub in torch solve (#1235)\n\n* add a compatibiliy stub for torch solve\n\n* change missing _torch_solve_cast calls\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* bump pytorch version to 1.7.1\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def _torch_solve_cast(input: torch.Tensor, A: torch.Tensor) -> Tuple[torch.Tenso",
            "if dtype not in (torch.float32, torch.float64):",
            "dtype = torch.float32",
            "",
            "-    out1, out2 = torch.solve(input.to(dtype), A.to(dtype))",
            "+    out = solve(A.to(dtype), input.to(dtype))",
            "",
            "-    return (out1.to(input.dtype), out2.to(input.dtype))",
            "+    return (out.to(input.dtype), out)"
        ]
    },
    {
        "number": 8315,
        "comments": "",
        "commit_message": "Fix visualization for ENAS micro (#2813)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Mutator(BaseMutator):",
            "\"\"\"",
            "if self._connect_all:",
            "return self._all_connect_tensor_reduction(mutable.reduction, tensor_list), \\",
            "-                torch.ones(mutable.n_candidates)",
            "+                torch.ones(mutable.n_candidates).bool()",
            "mask = self._get_decision(mutable)",
            "assert len(mask) == mutable.n_candidates, \\",
            "\"Invalid mask, expected {} to be of length {}.\".format(mask, mutable.n_candidates)"
        ]
    },
    {
        "number": 8319,
        "comments": "",
        "commit_message": "fix small bug\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class BaseModel():",
            "save_filename = '%s_net_%s.pth' % (which_epoch, name)",
            "save_path = os.path.join(self.save_dir, save_filename)",
            "net = getattr(self, 'net' + name)",
            "-                torch.module.save(net.cpu().state_dict(), save_path)",
            "+                torch.save(net.module.cpu().state_dict(), save_path)",
            "if len(self.gpu_ids) and torch.cuda.is_available():",
            "net.cuda(self.gpu_ids[0])"
        ]
    },
    {
        "number": 8321,
        "comments": "",
        "commit_message": "upgrade to pytorch 0.4.0 (#1126)\n\n* bump pytorch to 0.4 + fix sanitize\n\n* remove check for Variable in block_orthogonal\n\n* rename parameter split_size=\n\n* remove checks for Variable\n\n* fix more tests\n\n* fixes\n\n* get tests to pass\n\n* fix warnings\n\n* get rid of some of the Variables\n\n* more tests passing\n\n* more elimination of variables\n\n* finish removing all Variables\n\n* pylint and such\n\n* a few fixes\n\n* move torch.no_grad into model.forward_on_instances\n\n* more pytorch 0.4 changes\n\n* detach() -> data\n\n* pylint\n\n* fix bad tensor creation\n\n* fix types\n\n* remove print statement\n\n* more 0.4 goodness\n\n* factor out is_tensor\n\n* add no_grad to elmo command\n\n* cleanup\n\n* remove TODO\n\n* address PR feedback\n\n* replace all() with item()\n\n* more cleanup\n\n* further cleanup\n\n* remove Variable\n\n* really fix merge conflict\n\n* fix pylint\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class SimpleSeq2SeqWithoutAttentionTest(ModelTestCase):",
            "batch_size = 5",
            "num_decoding_steps = 5",
            "num_classes = 10",
            "-        sample_logits = Variable(torch.randn(batch_size, num_decoding_steps-1, num_classes))",
            "-        sample_targets = Variable(torch.from_numpy(numpy.random.randint(0, num_classes,",
            "-                                                                        (batch_size, num_decoding_steps))))",
            "+        sample_logits = torch.randn(batch_size, num_decoding_steps-1, num_classes)",
            "+        sample_targets = torch.from_numpy(numpy.random.randint(0, num_classes,",
            "+                                                               (batch_size, num_decoding_steps)))",
            "# Mask should be either 0 or 1",
            "-        sample_mask = Variable(torch.from_numpy(numpy.random.randint(0, 2,",
            "-                                                                     (batch_size, num_decoding_steps))))",
            "+        sample_mask = torch.from_numpy(numpy.random.randint(0, 2,",
            "+                                                            (batch_size, num_decoding_steps)))",
            "expected_loss = sequence_cross_entropy_with_logits(sample_logits, sample_targets[:, 1:].contiguous(),",
            "sample_mask[:, 1:].contiguous())",
            "# pylint: disable=protected-access"
        ]
    },
    {
        "number": 8327,
        "comments": "",
        "commit_message": "define and apply YAPF formatting  (#1039)\n\n* define YAPF\n\n* yapf kornia -rip\n\n* yapf examples -rip\n\n* yapf test -rip\n\n* Update CONTRIBUTING.rst\n\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\n\n* update config\n\n* fix 4lint\n\n* mypy\n\n* mypy\n\n* mypy\n\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def median_blur(input: torch.Tensor,",
            "b, c, h, w = input.shape",
            "",
            "# map the local window to single vector",
            "-    features: torch.Tensor = F.conv2d(",
            "-        input.reshape(b * c, 1, h, w), kernel, padding=padding, stride=1)",
            "+    features: torch.Tensor = F.conv2d(input.reshape(b * c, 1, h, w), kernel, padding=padding, stride=1)",
            "features = features.view(b, c, -1, h, w)  # BxCx(K_h * K_w)xHxW",
            "",
            "# compute the median along the feature axis"
        ]
    },
    {
        "number": 8329,
        "comments": "",
        "commit_message": "Fix for TF 0.12.1 and keras 1.2.1 (#65)\n\n* added fix for TF 0.12.1 and keras 1.2.1\n\n* updated readme\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def huber_loss(y_true, y_pred, clip_value):",
            "# Spacial case for infinity since Tensorflow does have problems",
            "# if we compare `K.abs(x) < np.inf`.",
            "return .5 * K.square(x)",
            "-",
            "+",
            "condition = K.abs(x) < clip_value",
            "squared_loss = .5 * K.square(x)",
            "linear_loss = clip_value * (K.abs(x) - .5 * clip_value)",
            "if K._BACKEND == 'tensorflow':",
            "import tensorflow as tf",
            "-        return tf.select(condition, squared_loss, linear_loss)  # condition, true, false",
            "+        return tf.where(condition, squared_loss, linear_loss)  # condition, true, false",
            "elif K._BACKEND == 'theano':",
            "from theano import tensor as T",
            "return T.switch(condition, squared_loss, linear_loss)"
        ]
    },
    {
        "number": 8332,
        "comments": "",
        "commit_message": "Fix attention order in unit tests (fixes #195) (#197)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestIncrementalDecoder(FairseqIncrementalDecoder):",
            "probs[:, i, self.dictionary.eos()] = 1.0",
            "",
            "# random attention",
            "-        attn = torch.rand(bbsz, src_len, tgt_len)",
            "+        attn = torch.rand(bbsz, tgt_len, src_len)",
            "",
            "return Variable(probs), Variable(attn)"
        ]
    },
    {
        "number": 8337,
        "comments": "",
        "commit_message": "fix tu squeeze bug\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def read_file(folder, prefix, name, dtype=None):",
            "def cat(seq):",
            "seq = [item for item in seq if item is not None]",
            "seq = [item.unsqueeze(-1) if item.dim() == 1 else item for item in seq]",
            "-    return torch.cat(seq, dim=-1).squeeze() if len(seq) > 0 else None",
            "+    return torch.cat(seq, dim=-1) if len(seq) > 0 else None",
            "",
            "",
            "def split(data, batch):"
        ]
    },
    {
        "number": 8347,
        "comments": "",
        "commit_message": "Add TableTextRetriever (#1529)\n\n* first draft / notes on new primitives\n\n* wip label / feedback refactor\n\n* rename doc.text -> doc.content. add doc.content_type\n\n* add datatype for content\n\n* remove faq_question_field from ES and weaviate. rename text_field -> content_field in docstores. update tutorials for content field\n\n* update converters for . Add warning for empty\n\n* renam label.question -> label.query. Allow sorting of Answers.\n\n* WIP primitives\n\n* update ui/reader for new Answer format\n\n* Improve Label. First refactoring of MultiLabel. Adjust eval code\n\n* fixed workflow conflict with introducing new one (#1472)\n\n* Add latest docstring and tutorial changes\n\n* make add_eval_data() work again\n\n* fix reader formats. WIP fix _extract_docs_and_labels_from_dict\n\n* fix test reader\n\n* Add latest docstring and tutorial changes\n\n* fix another test case for reader\n\n* fix mypy in farm reader.eval()\n\n* fix mypy in farm reader.eval()\n\n* WIP ORM refactor\n\n* Add latest docstring and tutorial changes\n\n* fix mypy weaviate\n\n* make label and multilabel dataclasses\n\n* bump mypy env in CI to python 3.8\n\n* WIP refactor Label ORM\n\n* WIP refactor Label ORM\n\n* simplify tests for individual doc stores\n\n* WIP refactoring markers of tests\n\n* test alternative approach for tests with existing parametrization\n\n* WIP refactor ORMs\n\n* fix skip logic of already parametrized tests\n\n* fix weaviate behaviour in tests - not parametrizing it in our general test cases.\n\n* Add latest docstring and tutorial changes\n\n* fix some tests\n\n* remove sql from document_store_types\n\n* fix markers for generator and pipeline test\n\n* remove inmemory marker\n\n* remove unneeded elasticsearch markers\n\n* add dataclasses-json dependency. adjust ORM to just store JSON repr\n\n* ignore type as dataclasses_json seems to miss functionality here\n\n* update readme and contributing.md\n\n* update contributing\n\n* adjust example\n\n* fix duplicate doc handling for custom index\n\n* Add latest docstring and tutorial changes\n\n* fix some ORM issues. fix get_all_labels_aggregated.\n\n* update drop flags where get_all_labels_aggregated() was used before\n\n* Add latest docstring and tutorial changes\n\n* add to_json(). add + fix tests\n\n* fix no_answer handling in label / multilabel\n\n* fix duplicate docs in memory doc store. change primary key for sql doc table\n\n* fix mypy issues\n\n* fix mypy issues\n\n* haystack/retriever/base.py\n\n* fix test_write_document_meta[elastic]\n\n* fix test_elasticsearch_custom_fields\n\n* fix test_labels[elastic]\n\n* fix crawler\n\n* fix converter\n\n* fix docx converter\n\n* fix preprocessor\n\n* fix test_utils\n\n* fix tfidf retriever. fix selection of docstore in tests with multiple fixtures / parameterizations\n\n* Add latest docstring and tutorial changes\n\n* fix crawler test. fix ocrconverter attribute\n\n* fix test_elasticsearch_custom_query\n\n* fix generator pipeline\n\n* fix ocr converter\n\n* fix ragenerator\n\n* Add latest docstring and tutorial changes\n\n* fix test_load_and_save_yaml for elasticsearch\n\n* fixes for pipeline tests\n\n* fix faq pipeline\n\n* fix pipeline tests\n\n* Add latest docstring and tutorial changes\n\n* Add MultimodalRetriever\n\n* Add latest docstring and tutorial changes\n\n* fix weaviate\n\n* Add latest docstring and tutorial changes\n\n* trigger CI\n\n* satisfy mypy\n\n* Add latest docstring and tutorial changes\n\n* satisfy mypy\n\n* Add latest docstring and tutorial changes\n\n* trigger CI\n\n* fix question generation test\n\n* fix ray. fix Q-generation\n\n* fix translator test\n\n* satisfy mypy\n\n* wip refactor feedback rest api\n\n* fix rest api feedback endpoint\n\n* fix doc classifier\n\n* remove relation of Labels -> Docs in SQL ORM\n\n* fix faiss/milvus tests\n\n* fix doc classifier test\n\n* fix eval test\n\n* fixing eval issues\n\n* Add latest docstring and tutorial changes\n\n* fix mypy\n\n* WIP replace dataclasses-json with manual serialization\n\n* Add methods to MultimodalRetriever\n\n* Add latest docstring and tutorial changes\n\n* revert to dataclass-json serialization for now. remove debug prints.\n\n* update docstrings\n\n* fix extractor. fix Answer Span init\n\n* fix api test\n\n* keep meta data of answers in reader.run()\n\n* fix meta handling\n\n* adress review feedback\n\n* Add latest docstring and tutorial changes\n\n* make document=None for open domain labels\n\n* add import\n\n* fix print utils\n\n* fix rest api\n\n* Add methods and tests\n\n* Add latest docstring and tutorial changes\n\n* Fix mypy\n\n* Add latest docstring and tutorial changes\n\n* Add type hints and doc strings\n\n* Make use of initialize_device_settings\n\n* Move serialization of pd.DataFrame to schema.py\n\n* Fix mypy\n\n* Adapt Document's from_dict method\n\n* Update docstrings\n\n* Add latest docstring and tutorial changes\n\n* Fix mypy\n\n* Fix mypy\n\n* Fix Document's from_dict method\n\n* Fix Document's to_dict method\n\n* Change handling of table metadata\n\n* Add latest docstring and tutorial changes\n\n* Change naming from Multimodal to TableText\n\n* Turn off tokenizers_parallelism in retriever tests\n\n* Add latest docstring and tutorial changes\n\n* Remove turning off tokenizers_parallelism in retriever tests\n\n* Adapt convert_es_hit_to_document\n\n* Change embed_surrounding_context to embed_meta_fields\n\n* Add latest docstring and tutorial changes\n\n* Add check if torch.distributed is available\n\n* Set n_gpu to 0 in training test\n\n* Set HIP_LAUNCH_BLOCKING to 1\n\n* Set HIP_LAUNCH_BLOCKING to \"1\"\n\n* Set use_gpu to False\n\n* Use DataParallel only if more than one device\n\n* Remove --find-links=https://download.pytorch.org/whl/torch_stable.html\n\nCo-authored-by: Malte Pietsch <malte.pietsch@deepset.ai>\nCo-authored-by: Markus Paff <markuspaff.mp@gmail.com>\nCo-authored-by: github-actions[bot] <41898282+github-actions[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TextSimilarityHead(PredictionHead):",
            "",
            "# Check if DDP is initialized",
            "try:",
            "-            rank = torch.distributed.get_rank()",
            "+            if torch.distributed.is_available():",
            "+                rank = torch.distributed.get_rank()",
            "+            else:",
            "+                rank = -1",
            "except (AssertionError, RuntimeError):",
            "rank = -1"
        ]
    },
    {
        "number": 8351,
        "comments": "",
        "commit_message": "fix linter and add docs\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class DecoderLayer(nn.Module):",
            "self.norm1 = LayerNorm(size)",
            "self.norm2 = LayerNorm(size)",
            "self.norm3 = LayerNorm(size)",
            "-        self.dropout = nn.Dropout(dropout)",
            "+        self.dropout = nn.Dropout(dropout_rate)",
            "",
            "def forward(self, tgt, tgt_mask, memory, memory_mask):",
            "\"\"\"Compute decoded features"
        ]
    },
    {
        "number": 8355,
        "comments": "",
        "commit_message": "Fix weight decay comment (#4228)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def train(hyp,  # path/to/hyp.yaml or hyp dictionary",
            "for v in model.modules():",
            "if hasattr(v, 'bias') and isinstance(v.bias, nn.Parameter):  # bias",
            "g2.append(v.bias)",
            "-        if isinstance(v, nn.BatchNorm2d):  # weight with decay",
            "+        if isinstance(v, nn.BatchNorm2d):  # weight (no decay)",
            "g0.append(v.weight)",
            "-        elif hasattr(v, 'weight') and isinstance(v.weight, nn.Parameter):  # weight without decay",
            "+        elif hasattr(v, 'weight') and isinstance(v.weight, nn.Parameter):  # weight (with decay)",
            "g1.append(v.weight)",
            "",
            "if opt.adam:"
        ]
    },
    {
        "number": 8358,
        "comments": "",
        "commit_message": "add min and max assertion for clip to all backends. (#3753)\n\n* add min and max assertion for clip to all backends.\n\n* remove min, max assertion for clip from `Ivy` backend.\n\n* fix core test for clip.\n\n* small reformat for clip core test.\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def clip(",
            "*,",
            "out: Optional[Union[tf.Tensor, tf.Variable]] = None,",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "+    assert tf.reduce_all(tf.less(x_min, x_max)), \"Min value must be less than max.\"",
            "if hasattr(x_min, \"dtype\") and hasattr(x_max, \"dtype\"):",
            "promoted_type = tf.experimental.numpy.promote_types(x.dtype, x_min.dtype)",
            "promoted_type = tf.experimental.numpy.promote_types(promoted_type, x_max.dtype)"
        ]
    },
    {
        "number": 8359,
        "comments": "",
        "commit_message": "[Longformer For Question Answering] Conversion script, doc, small fixes (#4593)\n\n* add new longformer for question answering model\n\n* add new config as well\n\n* fix links\n\n* fix links part 2\n",
        "label": "",
        "answer": "no",
        "change": [
            "logger = logging.getLogger(__name__)",
            "LONGFORMER_PRETRAINED_CONFIG_ARCHIVE_MAP = {",
            "\"longformer-base-4096\": \"https://s3.amazonaws.com/models.huggingface.co/bert/allenai/longformer-base-4096/config.json\",",
            "\"longformer-large-4096\": \"https://s3.amazonaws.com/models.huggingface.co/bert/allenai/longformer-large-4096/config.json\",",
            "+    \"longformer-large-4096-finetuned-triviaqa\": \"https://s3.amazonaws.com/models.huggingface.co/bert/allenai/longformer-large-4096-finetuned-triviaqa/config.json\",",
            "}"
        ]
    },
    {
        "number": 8362,
        "comments": "",
        "commit_message": "[RLlib] Tf2x preparation; part 2 (upgrading `try_import_tf()`). (#9136)\n\n* WIP.\n\n* Fixes.\n\n* LINT.\n\n* WIP.\n\n* WIP.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* WIP.\n\n* Fixes.\n\n* Test\n\n* Fix.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* LINT.\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Categorical(TFActionDistribution):",
            "",
            "@override(TFActionDistribution)",
            "def _build_sample_op(self):",
            "-        return tf.squeeze(tf.multinomial(self.inputs, 1), axis=1)",
            "+        return tf.squeeze(tf.random.categorical(self.inputs, 1), axis=1)",
            "",
            "@staticmethod",
            "@override(ActionDistribution)"
        ]
    },
    {
        "number": 8363,
        "comments": "",
        "commit_message": "[RLlib] Add testing framework_iterator. (#7852)\n\n* Add testing framework_iterator.\n\n* LINT.\n\n* WIP.\n\n* Fix and LINT.\n\n* LINT fix.\n",
        "label": "",
        "answer": "no",
        "change": [
            "def fc(x, weights, biases=None):",
            "isinstance(weights, torch.Tensor) else weights",
            "biases = biases.detach().numpy() if \\",
            "isinstance(biases, torch.Tensor) else biases",
            "-    if tf:",
            "+    if tf and tf.executing_eagerly():",
            "x = x.numpy() if isinstance(x, tf.Variable) else x",
            "weights = weights.numpy() if isinstance(weights, tf.Variable) else \\",
            "weights"
        ]
    },
    {
        "number": 8365,
        "comments": "",
        "commit_message": "Fix speaker encoder test\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class SpeakerManager:",
            "waveform = self.speaker_encoder_ap.load_wav(wav_file, sr=self.speaker_encoder_ap.sample_rate)",
            "if not self.speaker_encoder_config.model_params.get(\"use_torch_spec\", False):",
            "m_input = self.speaker_encoder_ap.melspectrogram(waveform)",
            "-                m_input = torch.from_numpy(m_input.T)",
            "+                m_input = torch.from_numpy(m_input)",
            "else:",
            "m_input = torch.from_numpy(waveform)"
        ]
    },
    {
        "number": 8374,
        "comments": "",
        "commit_message": "Add linting with black (#2678)\n\nSummary:\n# Before submitting\n\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\n- [ ] Did you make sure to update the docs?\n- [ ] Did you write any new necessary tests?\n\n## What does this PR do?\nFixes # (issue).\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding \ufffd\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2678\n\nReviewed By: Mortimerp9\n\nDifferential Revision: D32653381\n\nPulled By: dianaml0\n\nfbshipit-source-id: 2810d14867cd7d64f4d340740e2b590b82de47fe\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class CheckpointFunction(torch.autograd.Function):",
            "tensor_inputs = checkpoint.detach_variable(tensor_inputs)",
            "if ctx.fwd_device is not None:",
            "tensor_inputs = [",
            "-                t.to(ctx.fwd_device[i], non_blocking=True) for i, t in enumerate(tensor_inputs)",
            "+                t.to(ctx.fwd_device[i], non_blocking=True)",
            "+                for i, t in enumerate(tensor_inputs)",
            "]",
            "for i, need_grad in enumerate(ctx.grad_requirements):",
            "tensor_inputs[i].requires_grad = need_grad"
        ]
    },
    {
        "number": 8377,
        "comments": "",
        "commit_message": "fix code roles + move instance method examples to appropriate methods\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def kaiser_window(",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "if periodic is False:",
            "return tf.signal.kaiser_window(",
            "-            window_length, beta, dtype=tf.dtypes.float32, name=None)",
            "-    else:",
            "-        return tf.signal.kaiser_window(",
            "-            window_length + 1, beta, dtype=dtype, name=None)[:-1]",
            "+            window_length, beta, dtype=tf.dtypes.float32, name=None",
            "+        )",
            "+    else:",
            "+        return tf.signal.kaiser_window(window_length + 1, beta, dtype=dtype, name=None)[",
            "+            :-1",
            "+        ]",
            "",
            "",
            "def moveaxis("
        ]
    },
    {
        "number": 8382,
        "comments": "",
        "commit_message": "fix var name\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def main():",
            "model = create_model(num_classes=num_classes + 1, box_thresh=box_thresh)",
            "",
            "# load train weights",
            "-    assert os.path.exists(weight_path), \"{} file dose not exist.\".format(weight_path)",
            "-    model.load_state_dict(torch.load(weight_path, map_location='cpu')[\"model\"])",
            "+    assert os.path.exists(weights_path), \"{} file dose not exist.\".format(weights_path)",
            "+    model.load_state_dict(torch.load(weights_path, map_location='cpu')[\"model\"])",
            "model.to(device)",
            "",
            "# read class_indict"
        ]
    },
    {
        "number": 8385,
        "comments": "",
        "commit_message": "Fixed misuse of epsilon in Dice loss (issue #24)\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class DiceCoeff(Function):",
            "",
            "def forward(self, input, target):",
            "self.save_for_backward(input, target)",
            "-        self.inter = torch.dot(input.view(-1), target.view(-1)) + 0.0001",
            "-        self.union = torch.sum(input) + torch.sum(target) + 0.0001",
            "+        eps = 0.0001",
            "+        self.inter = torch.dot(input.view(-1), target.view(-1))",
            "+        self.union = torch.sum(input) + torch.sum(target) + eps",
            "",
            "-        t = 2 * self.inter.float() / self.union.float()",
            "+        t = (2 * self.inter.float() + eps) / self.union.float()",
            "return t",
            "",
            "# This function has only a single output, so it gets only one gradient"
        ]
    },
    {
        "number": 8388,
        "comments": "",
        "commit_message": "fix some errors\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def FullyConnected(x, out_dim, W_init=None, b_init=None, nl=tf.nn.relu):",
            "",
            "W = tf.get_variable('W', [in_dim, out_dim], initializer=W_init)",
            "b = tf.get_variable('b', [out_dim], initializer=b_init)",
            "-    return nl(tf.matmul(x, W) + b)",
            "+    return nl(tf.matmul(x, W) + b, name=tf.get_variable_scope().name + '_output')"
        ]
    },
    {
        "number": 8390,
        "comments": "",
        "commit_message": "More TF int dtype fixes (#20384)\n\n* Add a test to ensure int dummy inputs are int64\n\n* Move the test into the existing int64 test and update a lot of existing dummies\n\n* Fix remaining dummies\n\n* Fix remaining dummies\n\n* Test for int64 serving sigs as well\n\n* Update core tests to use tf.int64\n\n* Add better messages to the assertions\n\n* Update all serving sigs to int64\n\n* More sneaky hiding tf.int32s\n\n* Add an optional int32 signature in save_pretrained\n\n* make fixup\n\n* Add Amy's suggestions\n\n* Switch all serving sigs back to tf.int32\n\n* Switch all dummies to tf.int32\n\n* Adjust tests to check for tf.int32 instead of tf.int64\n\n* Fix base dummy_inputs dtype\n\n* Start casting to tf.int32 in input_processing\n\n* Change dtype for unpack_inputs test\n\n* Add proper tf.int32 test\n\n* Make the alternate serving signature int64\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TFWhisperPreTrainedModel(TFPreTrainedModel):",
            "self.main_input_name: tf.random.uniform(",
            "[2, self.config.num_mel_bins, self.config.max_source_positions * 2 - 1], dtype=tf.float32",
            "),",
            "-            \"decoder_input_ids\": tf.constant([[2, 3]], dtype=tf.int64),",
            "+            \"decoder_input_ids\": tf.constant([[2, 3]], dtype=tf.int32),",
            "}",
            "",
            "@tf.function(",
            "input_signature=[",
            "{",
            "\"input_features\": tf.TensorSpec((None, None, None), tf.float32, name=\"input_features\"),",
            "-                \"decoder_input_ids\": tf.TensorSpec((None, None), tf.int64, name=\"decoder_input_ids\"),",
            "-                \"decoder_attention_mask\": tf.TensorSpec((None, None), tf.int64, name=\"decoder_attention_mask\"),",
            "+                \"decoder_input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"decoder_input_ids\"),",
            "+                \"decoder_attention_mask\": tf.TensorSpec((None, None), tf.int32, name=\"decoder_attention_mask\"),",
            "}",
            "]",
            ")"
        ]
    },
    {
        "number": 8393,
        "comments": "",
        "commit_message": "Fix eval bug (#161)\n\n* change to subprocess beckend\n\n* fix eval bug\n\n* Update launch.py\n\nmake flake8 happy\n\nCo-authored-by: Feng Wang <wangfeng19950315@163.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class ValTransform:",
            "# assume input is cv2 img for now",
            "def __call__(self, img, res, input_size):",
            "img, _ = preproc(img, input_size, self.means, self.std, self.swap)",
            "-        return torch.tensor_as(img), torch.zeros(1, 5)",
            "+        return torch.as_tensor(img), torch.zeros(1, 5)"
        ]
    },
    {
        "number": 8394,
        "comments": "",
        "commit_message": "[cost] fix cross_entropy_seq\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def cross_entropy_seq(logits, target_seqs):#, batch_size=1, num_steps=None):",
            "loss = sequence_loss_by_example_fn(",
            "[logits],",
            "[tf.reshape(target_seqs, [-1])],",
            "-        [tf.ones_like(tf.reshape(targets, [-1]), dtype=tf.float32)])",
            "+        [tf.ones_like(tf.reshape(target_seqs, [-1]), dtype=tf.float32)])",
            "# [tf.ones([batch_size * num_steps])])",
            "-    cost = tf.reduce_sum(loss) / batch_size",
            "+    cost = tf.reduce_sum(loss) #/ batch_size",
            "return cost"
        ]
    },
    {
        "number": 8395,
        "comments": "",
        "commit_message": "Fix import\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class ForwardTTS(BaseTTS):",
            "# init speaker embedding layer",
            "if config.use_speaker_embedding and not config.use_d_vector_file:",
            "print(\" > Init speaker_embedding layer.\")",
            "-            self.emb_g = nn.Embedding(self.args.num_speakers, self.args.hidden_channels)",
            "+            self.emb_g = nn.Embedding(self.num_speakers, self.args.hidden_channels)",
            "nn.init.uniform_(self.emb_g.weight, -0.1, 0.1)",
            "",
            "@staticmethod"
        ]
    },
    {
        "number": 8396,
        "comments": "",
        "commit_message": "Add bilinear attention (#1349)\n\n* Add bilinear attention, some code cleanup, make semantic parsers use new attention\n\n* Fix pylint and docs\n\n* Increase beam size for wikitables ERM parser, so test doesn't fail\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class LegacyAttention(Attention):",
            "self._similarity_function = similarity_function or DotProductSimilarity()",
            "",
            "@overrides",
            "-    def _forward_internal(self,",
            "-                          vector: torch.Tensor,",
            "-                          matrix: torch.Tensor,",
            "-                          matrix_mask: torch.Tensor = None) -> torch.Tensor:",
            "+    def _forward_internal(self, vector: torch.Tensor, matrix: torch.Tensor) -> torch.Tensor:",
            "tiled_vector = vector.unsqueeze(1).expand(vector.size()[0],",
            "matrix.size()[1],",
            "vector.size()[1])"
        ]
    },
    {
        "number": 8402,
        "comments": "",
        "commit_message": "fix fast_neural_style --export_onnx (#397)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def stylize(args):",
            "style_model.to(device)",
            "if args.export_onnx:",
            "assert args.export_onnx.endswith(\".onnx\"), \"Export model file should end with .onnx\"",
            "-                output = torch.onnx._export(style_model, content_image, args.export_onnx)",
            "+                output = torch.onnx._export(style_model, content_image, args.export_onnx).cpu()",
            "else:",
            "output = style_model(content_image).cpu()",
            "utils.save_image(args.output_image, output[0])"
        ]
    },
    {
        "number": 8407,
        "comments": "",
        "commit_message": "Quick fix :) (#606)\n\n* Changing the name\n\n* style + quality\n\n* update doc and logo\n\n* clean up\n\n* circle-CI on the branche for now\n\n* fix daily dialog dataset\n\n* fix urls\n\nCo-authored-by: Quentin Lhoest <lhoest.q@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class abstractclassmethod(classmethod):  # pylint: disable=invalid-name",
            "super(abstractclassmethod, self).__init__(fn)",
            "",
            "",
            "-def get_nlp_path(relative_path):",
            "-    \"\"\"Returns absolute path to file given path relative to nlp root.\"\"\"",
            "-    path = os.path.join(nlp_dir(), relative_path)",
            "+def get_datasets_path(relative_path):",
            "+    \"\"\"Returns absolute path to file given path relative to datasets root.\"\"\"",
            "+    path = os.path.join(datasets_dir(), relative_path)",
            "return path"
        ]
    },
    {
        "number": 8410,
        "comments": "",
        "commit_message": "Add test to ensure models can take int64 inputs (#17210)\n\n* Add test to ensure models can take int64 inputs\n\n* is_integer is an attribute, not a method\n\n* Fix test when some inputs aren't tensors\n\n* Add casts to blenderbot and blenderbot-small\n\n* Add casts to the other failing models\n",
        "label": "",
        "answer": "no",
        "change": [
            "def get_masks(slen, lengths, causal, padding_mask=None):",
            "mask = padding_mask",
            "else:",
            "# assert lengths.max().item() <= slen",
            "-        alen = tf.range(slen)",
            "-        mask = tf.math.less(alen, tf.expand_dims(lengths, axis=1))",
            "+        alen = tf.range(slen, dtype=lengths.dtype)",
            "+        mask = alen < tf.expand_dims(lengths, axis=1)",
            "",
            "# attention mask is the same as mask, or triangular inferior attention (causal)",
            "if causal:"
        ]
    },
    {
        "number": 8412,
        "comments": "",
        "commit_message": "CDN urls (#4030)\n\n* [file_utils] use_cdn + documentation\n\n* Move to cdn. urls for weights\n\n* [urls] Hotfix for bert-base-japanese\n",
        "label": "",
        "answer": "no",
        "change": [
            "from .modeling_roberta import (",
            "logger = logging.getLogger(__name__)",
            "",
            "CAMEMBERT_PRETRAINED_MODEL_ARCHIVE_MAP = {",
            "-    \"camembert-base\": \"https://s3.amazonaws.com/models.huggingface.co/bert/camembert-base-pytorch_model.bin\",",
            "-    \"umberto-commoncrawl-cased-v1\": \"https://s3.amazonaws.com/models.huggingface.co/bert/Musixmatch/umberto-commoncrawl-cased-v1/pytorch_model.bin\",",
            "-    \"umberto-wikipedia-uncased-v1\": \"https://s3.amazonaws.com/models.huggingface.co/bert/Musixmatch/umberto-wikipedia-uncased-v1/pytorch_model.bin\",",
            "+    \"camembert-base\": \"https://cdn.huggingface.co/camembert-base-pytorch_model.bin\",",
            "+    \"umberto-commoncrawl-cased-v1\": \"https://cdn.huggingface.co/Musixmatch/umberto-commoncrawl-cased-v1/pytorch_model.bin\",",
            "+    \"umberto-wikipedia-uncased-v1\": \"https://cdn.huggingface.co/Musixmatch/umberto-wikipedia-uncased-v1/pytorch_model.bin\",",
            "}",
            "",
            "CAMEMBERT_START_DOCSTRING = r\"\"\""
        ]
    },
    {
        "number": 8428,
        "comments": "",
        "commit_message": "upgrade to pytorch 0.4.0 (#1126)\n\n* bump pytorch to 0.4 + fix sanitize\n\n* remove check for Variable in block_orthogonal\n\n* rename parameter split_size=\n\n* remove checks for Variable\n\n* fix more tests\n\n* fixes\n\n* get tests to pass\n\n* fix warnings\n\n* get rid of some of the Variables\n\n* more tests passing\n\n* more elimination of variables\n\n* finish removing all Variables\n\n* pylint and such\n\n* a few fixes\n\n* move torch.no_grad into model.forward_on_instances\n\n* more pytorch 0.4 changes\n\n* detach() -> data\n\n* pylint\n\n* fix bad tensor creation\n\n* fix types\n\n* remove print statement\n\n* more 0.4 goodness\n\n* factor out is_tensor\n\n* add no_grad to elmo command\n\n* cleanup\n\n* remove TODO\n\n* address PR feedback\n\n* replace all() with item()\n\n* more cleanup\n\n* further cleanup\n\n* remove Variable\n\n* really fix merge conflict\n\n* fix pylint\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class SpanBasedF1Measure(Metric):",
            "possible roles associated with it).",
            "\"\"\"",
            "if mask is None:",
            "-            mask = ones_like(gold_labels)",
            "-        # Get the data from the Variables.",
            "+            mask = torch.ones_like(gold_labels)",
            "+",
            "predictions, gold_labels, mask, prediction_map = self.unwrap_to_tensors(predictions,",
            "gold_labels,",
            "mask, prediction_map)"
        ]
    },
    {
        "number": 8429,
        "comments": "",
        "commit_message": "linter fixes and test fixes\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def interpolate_vocoder_input(scale_factor, spec):",
            "torch.tensor: interpolated spectrogram.",
            "\"\"\"",
            "print(\" > before interpolation :\", spec.shape)",
            "-    spec = torch.tensor(spec).unsqueeze(0).unsqueeze(0)",
            "+    spec = torch.tensor(spec).unsqueeze(0).unsqueeze(0)  # pylint: disable=not-callable",
            "spec = torch.nn.functional.interpolate(spec,",
            "scale_factor=scale_factor,",
            "recompute_scale_factor=True,"
        ]
    },
    {
        "number": 8431,
        "comments": "",
        "commit_message": "Fixing slow pipeline tests (#14260)\n\n* Fiixng slow pipeline tests\n\n* Remove the image-segmentaiton override.\n\n* Fixing clamping only in training.\n\n* Wav2vec2.\n\n* Remove last mention of `no_grad`.\n\n* Fixing copies.\n\n* Rename.\n",
        "label": "",
        "answer": "no",
        "change": [
            "class ImageSegmentationPipeline(Pipeline):",
            "",
            "return super().__call__(*args, **kwargs)",
            "",
            "-    def get_inference_context(self):",
            "-        return torch.no_grad",
            "-",
            "def preprocess(self, image):",
            "image = load_image(image)",
            "target_size = torch.IntTensor([[image.height, image.width]])"
        ]
    },
    {
        "number": 8438,
        "comments": "",
        "commit_message": "fix softmax and agnn\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class AGNN(Module):",
            "self.beta.data.uniform_(0, 1)",
            "",
            "def forward(self, x, edge_index):",
            "-        beta = self.beta if self.requires_grad else Var(self._buffers['beta'])",
            "+        beta = self.beta if self.requires_grad else self._buffers['beta']",
            "return agnn(x, edge_index, beta)",
            "",
            "def __repr__(self):"
        ]
    },
    {
        "number": 8440,
        "comments": "",
        "commit_message": "[RLlib] Issue 21334: Fix APPO when kl_loss is enabled. (#21855)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def stats(policy: Policy, train_batch: SampleBatch):",
            "stats_dict[\"var_IS\"] = is_stat_var",
            "",
            "if policy.config[\"use_kl_loss\"]:",
            "-        stats_dict[\"kl\"] = policy.get_tower_stats(\"mean_kl_loss\")",
            "+        stats_dict[\"kl\"] = torch.mean(",
            "+            torch.stack(policy.get_tower_stats(\"mean_kl_loss\")))",
            "stats_dict[\"KL_Coeff\"] = policy.kl_coeff",
            "",
            "return stats_dict"
        ]
    },
    {
        "number": 8442,
        "comments": "",
        "commit_message": "Drop torch 1.6 testing (#10390)\n\n* Drop torch 1.6 support\n\n* Drop 1.6 support\n\n* Update CHANGELOG\n\n* Fixes\n\n* Split change\n\n* Undo change\n\n* 1.7 -> 1.7.1\n\nhttps://github.com/pytorch/pytorch/issues/47354\n\n* Force trigger nightly\n\n* Update .github/workflows/events-nightly.yml\n\nCo-authored-by: Aki Nitta <nitta@akihironitta.com>\n\n* Revert 1.7.1 change - try wildcard\n\n* Update adjust versions and test it\n\n* Undo test changes\n\n* Revert \"Undo test changes\"\n\nThis reverts commit 3a6acadd115e86f02d83a788f1978372ab6764f3.\n\n* Update CHANGELOG.md\n\nCo-authored-by: Aki Nitta <nitta@akihironitta.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def atomic_save(checkpoint: Dict[str, Any], filepath: Union[str, Path]) -> None:",
            "\"\"\"",
            "",
            "bytesbuffer = io.BytesIO()",
            "-    # Can't use the new zipfile serialization for 1.6.0 because there's a bug in",
            "-    # torch.hub.load_state_dict_from_url() that prevents it from loading the new files.",
            "-    # More details can be found here: https://github.com/pytorch/pytorch/issues/42239",
            "-    if Version(torch.__version__).release[:3] == (1, 6, 0):",
            "-        torch.save(checkpoint, bytesbuffer, _use_new_zipfile_serialization=False)",
            "-    else:",
            "-        torch.save(checkpoint, bytesbuffer)",
            "+    torch.save(checkpoint, bytesbuffer)",
            "with fsspec.open(filepath, \"wb\") as f:",
            "f.write(bytesbuffer.getvalue())"
        ]
    },
    {
        "number": 8448,
        "comments": "",
        "commit_message": "Fix DDP bug in single process multiple device use cases\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def train(hyp):",
            "world_size=1,  # number of nodes",
            "rank=0)  # node rank",
            "model = torch.nn.parallel.DistributedDataParallel(model)",
            "+        # pip install torch==1.4.0+cu100 torchvision==0.5.0+cu100 -f https://download.pytorch.org/whl/torch_stable.html",
            "",
            "# Dataset",
            "dataset = LoadImagesAndLabels(train_path, imgsz, batch_size,"
        ]
    },
    {
        "number": 8451,
        "comments": "",
        "commit_message": "Fix docstring formats (#3196)\n\n* update docstring formats\n\n* fix :func:\n\n* minor update\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Collect(object):",
            "",
            "@PIPELINES.register_module()",
            "class WrapFieldsToLists(object):",
            "-    \"\"\"",
            "-    Wrap fields of the data dictionary into lists for evaluation.",
            "+    \"\"\"Wrap fields of the data dictionary into lists for evaluation.",
            "",
            "This class can be used as a last step of a test or validation",
            "pipeline for single image evaluation or inference."
        ]
    },
    {
        "number": 8453,
        "comments": "",
        "commit_message": "Fix bug in classification head\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class SwinTransformerV2CR(nn.Module):",
            "for stage in self.stages:",
            "output: torch.Tensor = stage(output)",
            "# Perform average pooling",
            "-        output: torch.Tensor = self.average_pool(output)",
            "+        output: torch.Tensor = self.average_pool(output).flatten(start_dim=1)",
            "# Predict classification",
            "classification: torch.Tensor = self.head(output)",
            "return classification"
        ]
    },
    {
        "number": 8454,
        "comments": "",
        "commit_message": "fix (#6946)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class LxmertModelTest(ModelTesterMixin, unittest.TestCase):",
            "def test_model_from_pretrained(self):",
            "for model_name in LXMERT_PRETRAINED_MODEL_ARCHIVE_LIST[:1]:",
            "model = LxmertModel.from_pretrained(model_name)",
            "+            model.to(torch_device)",
            "self.assertIsNotNone(model)",
            "",
            "def test_attention_outputs(self):"
        ]
    },
    {
        "number": 8455,
        "comments": "",
        "commit_message": "[RLlib] SlateQ: framework=tf fixes and SlateQ documentation update (#22543)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class SlateEpsilonGreedy(EpsilonGreedy):",
            ") -> \"tf.Tensor\":",
            "",
            "per_slate_q_values = action_distribution.inputs",
            "-        all_slates = self.model.slates",
            "+        all_slates = action_distribution.all_slates",
            "",
            "-        exploit_indices = action_distribution.deterministic_sample()",
            "-        exploit_action = tf.gather(all_slates, exploit_indices)",
            "+        exploit_action = action_distribution.deterministic_sample()",
            "",
            "batch_size = tf.shape(per_slate_q_values)[0]",
            "action_logp = tf.zeros(batch_size, dtype=tf.float32)"
        ]
    },
    {
        "number": 8457,
        "comments": "",
        "commit_message": "Fix classifier dropout in AlbertForMultipleChoice (#13087)\n\nClassification head of AlbertForMultipleChoice uses `hidden_dropout_prob` instead of `classifier_dropout_prob`.  This\nis not desirable as we cannot change classifer head dropout probability without changing the dropout probabilities of\nthe whole model.\n",
        "label": "",
        "answer": "no",
        "change": [
            "class AlbertForMultipleChoice(AlbertPreTrainedModel):",
            "super().__init__(config)",
            "",
            "self.albert = AlbertModel(config)",
            "-        self.dropout = nn.Dropout(config.hidden_dropout_prob)",
            "+        self.dropout = nn.Dropout(config.classifier_dropout_prob)",
            "self.classifier = nn.Linear(config.hidden_size, 1)",
            "",
            "self.init_weights()"
        ]
    },
    {
        "number": 8458,
        "comments": "",
        "commit_message": "Bump version to 0.5.1 that is python 3.5 compatible (#2089)\n\n* Bump version to 0.5.1 that is python 3.5 compatible\n\n* more fixes to setup.py\n\n* update minimum python version to 3.5\n\n* address comments\n\n* fix tqdm issue\n\n* bump up pyro-api version\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def setup(app):",
            "",
            "# @jpchen's hack to get rtd builder to install latest pytorch",
            "if 'READTHEDOCS' in os.environ:",
            "-    os.system('pip install torch==1.2.0+cpu -f https://download.pytorch.org/whl/torch_stable.html')",
            "+    os.system('pip install torch==1.3.0+cpu -f https://download.pytorch.org/whl/torch_stable.html')"
        ]
    },
    {
        "number": 8475,
        "comments": "",
        "commit_message": "BERT decoder: Fix causal mask dtype.\n\nPyTorch < 1.3 requires multiplication operands to be of the same type.\nThis was violated when using default attention mask (i.e.,\nattention_mask=None in arguments) given BERT in the decoder mode.\n\nIn particular, this was breaking Model2Model and made tutorial\nfrom the quickstart failing.\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class BertModel(BertPreTrainedModel):",
            "seq_ids = torch.arange(seq_length, device=device)",
            "causal_mask = seq_ids[None, None, :].repeat(batch_size, seq_length, 1) <= seq_ids[None, :, None]",
            "causal_mask = causal_mask.to(",
            "-                    torch.long",
            "-                )  # not converting to long will cause errors with pytorch version < 1.3",
            "+                    attention_mask.dtype",
            "+                )  # causal and attention masks must have same type with pytorch version < 1.3",
            "extended_attention_mask = causal_mask[:, None, :, :] * attention_mask[:, None, None, :]",
            "else:",
            "extended_attention_mask = attention_mask[:, None, None, :]"
        ]
    },
    {
        "number": 8480,
        "comments": "",
        "commit_message": "remove numpy import (#1116)\n\n* remove numpy import\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* undo and add setup develop in gh actions\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* remove create_checkerboard\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestDilate:",
            "None, None, :, :",
            "]",
            "assert_allclose(",
            "-            dilation(tensor, torch.ones_like(structural_element), structuring_element=structural_element), expected,",
            "-            atol=1e-4, rtol=1e-4",
            "+            dilation(tensor, torch.ones_like(structural_element), structuring_element=structural_element),",
            "+            expected,",
            "+            atol=1e-4,",
            "+            rtol=1e-4,",
            ")",
            "",
            "def test_exception(self, device, dtype):"
        ]
    },
    {
        "number": 8482,
        "comments": "",
        "commit_message": "fix naming for griff created files\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class FastSpeech2Trainer(Seq2SeqBasedTrainer):",
            "grif_before = self.griffin_lim_tf(tf.reshape(mel_before, [-1, 80])[tf.newaxis, :], n_iter=32)",
            "grif_after = self.griffin_lim_tf(tf.reshape(mel_after, [-1, 80])[tf.newaxis, :], n_iter=32)",
            "grif_gt = self.griffin_lim_tf(tf.reshape(mel_gt, [-1, 80])[tf.newaxis, :], n_iter=32)",
            "-                self.griffin_lim_tf.save_wav(grif_before, griff_dir_name, f\"{idx}_before.wav\")",
            "-                self.griffin_lim_tf.save_wav(grif_after, griff_dir_name, f\"{idx}_after.wav\")",
            "-                self.griffin_lim_tf.save_wav(grif_gt, griff_dir_name, f\"{idx}_gt.wav\")",
            "+                self.griffin_lim_tf.save_wav(grif_before, griff_dir_name, f\"{idx}_before\")",
            "+                self.griffin_lim_tf.save_wav(grif_after, griff_dir_name, f\"{idx}_after\")",
            "+                self.griffin_lim_tf.save_wav(grif_gt, griff_dir_name, f\"{idx}_gt\")",
            "",
            "mel_gt = tf.reshape(mel_gt, (-1, 80)).numpy()  # [length, 80]",
            "mel_before = tf.reshape(mel_before, (-1, 80)).numpy()  # [length, 80]"
        ]
    },
    {
        "number": 8483,
        "comments": "",
        "commit_message": "Fix bugs. Change the time-domain output to SPK* [Batch, Sample].\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class ESPnetFrontendModel(AbsESPnetModel):",
            "return loss.mean(), perm",
            "",
            "def collect_feats(",
            "-            self, speech_mix: torch.Tensor, speech_mix_lengths: torch.Tensor, **kwargs",
            "+        self, speech_mix: torch.Tensor, speech_mix_lengths: torch.Tensor, **kwargs",
            ") -> Dict[str, torch.Tensor]:",
            "# for data-parallel",
            "speech_mix = speech_mix[:, : speech_mix_lengths.max()]"
        ]
    },
    {
        "number": 8484,
        "comments": "",
        "commit_message": "Add Pre-commit bot fixes (#3722)\n\n* flake8\n\n* flake8\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "from torch_geometric.utils import remove_self_loops",
            "from typing import Optional",
            "",
            "",
            "-def pool_edge(cluster, edge_index,",
            "-              edge_attr: Optional[torch.Tensor] = None):",
            "+def pool_edge(cluster, edge_index, edge_attr: Optional[torch.Tensor] = None):",
            "num_nodes = cluster.size(0)",
            "edge_index = cluster[edge_index.view(-1)].view(2, -1)",
            "edge_index, edge_attr = remove_self_loops(edge_index, edge_attr)"
        ]
    },
    {
        "number": 8485,
        "comments": "",
        "commit_message": "fix LayerNorm (#4030)\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class LayerNorm(torch.nn.Module):",
            "self.eps = eps",
            "",
            "if affine:",
            "-            self.weight = Parameter(torch.Tensor([in_channels]))",
            "-            self.bias = Parameter(torch.Tensor([in_channels]))",
            "+            self.weight = Parameter(torch.Tensor(in_channels))",
            "+            self.bias = Parameter(torch.Tensor(in_channels))",
            "else:",
            "self.register_parameter('weight', None)",
            "self.register_parameter('bias', None)"
        ]
    },
    {
        "number": 8486,
        "comments": "",
        "commit_message": "summary / dataflow fix\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def summary_moving_average(cost_var):",
            "MOVING_SUMMARY_VARS_KEY, as well as the argument",
            "Return a op to maintain these average",
            "\"\"\"",
            "-    global_step_var = tf.get_default_graph().get_tensor_by_name(GLOBAL_STEP_VAR_NAME)",
            "+    global_step_var = get_global_step_var()",
            "averager = tf.train.ExponentialMovingAverage(",
            "0.99, num_updates=global_step_var, name='moving_averages')",
            "vars_to_summary = [cost_var] + \\"
        ]
    },
    {
        "number": 8490,
        "comments": "",
        "commit_message": "Fix typo in output parameter name in docstring (#2708)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def sort_batch_by_length(tensor: torch.Tensor, sequence_lengths: torch.Tensor):",
            "restoration_indices : torch.LongTensor",
            "Indices into the sorted_tensor such that",
            "``sorted_tensor.index_select(0, restoration_indices) == original_tensor``",
            "-    permuation_index : torch.LongTensor",
            "+    permutation_index : torch.LongTensor",
            "The indices used to sort the tensor. This is useful if you want to sort many",
            "tensors using the same ordering.",
            "\"\"\""
        ]
    },
    {
        "number": 8496,
        "comments": "",
        "commit_message": "Avoid edge cases with quantization by setting a known seed (#1295)\n\nSummary:\nPull Request resolved: https://github.com/facebookresearch/pytext/pull/1295\n\nStress test reveals some cases where the model returns NaN (https://our.intern.facebook.com/intern/testinfra/diagnostics/3377699742969609.844424947441516.1585199924/).  My guess is that some initializations play badly with quantization, so forcing a known seed to try and fix it.\n\nReviewed By: AkshatSh\n\nDifferential Revision: D20673784\n\nfbshipit-source-id: a341e59b7636d798f557c477a81655eb0975b28a\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Seq2SeqModelEvalTests(unittest.TestCase):",
            "\"\"\"",
            "tensorizers = get_tensorizers()",
            "",
            "+        # Avoid numeric issues with quantization by setting a known seed.",
            "+        torch.manual_seed(42)",
            "+",
            "model = Seq2SeqModel.from_config(",
            "Seq2SeqModel.Config(",
            "source_embedding=WordEmbedding.Config(embed_dim=512),"
        ]
    },
    {
        "number": 8501,
        "comments": "",
        "commit_message": "fix typo\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class RNN(object):",
            "l_in_y = tf.matmul(l_in_x, Wi) + bi",
            "l_in_y = tf.reshape(l_in_y, [-1, self._time_steps, self._cell_size], name='2_3D')",
            "",
            "-            with tf.variable_scope('lstm_cell'):",
            "+            with tf.variable_scope('cell'):",
            "cell = tf.nn.rnn_cell.BasicRNNCell(self._cell_size)",
            "with tf.name_scope('initial_state'):",
            "self._cell_initial_state = cell.zero_state(self._batch_size, dtype=tf.float32)"
        ]
    },
    {
        "number": 8510,
        "comments": "",
        "commit_message": "imgaug & fix svhn-dorefa double fw\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Model(ModelDesc):",
            ".apply(fg).BatchNorm('bn6')",
            ".apply(cabs)",
            ".FullyConnected('fc1', 10, nl=tf.identity)())",
            "+        tf.get_variable = old_get_variable",
            "prob = tf.nn.softmax(logits, name='output')",
            "",
            "cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, label)"
        ]
    },
    {
        "number": 8513,
        "comments": "",
        "commit_message": "Enable colocate_gradients_with_ops=True (#3620)\n\nBy default TensorFlow allocates all gradient matricies on gpu:0, which makes it pretty much impossible to do parallelize a large model.\n\ncolocate_gradients_with_ops puts these matricies next to the operations, allowing you to split your model across multiple GPUs. I ran into this issue myself and this fixed it for me.\n\nI think it's also meant to set gradient computations to be done on the device where the operations are stored, but my belief about that comes from https://github.com/tensorflow/tensorflow/issues/2441\n\nI'm not sure why this isn't the default in TF, so I'm not sure if this should be behind a flag or something, but having to make my own patches to keras to do multi-GPU training seems like the wrong answer.\n",
        "label": "",
        "answer": "no",
        "change": [
            "def gradients(loss, variables):",
            "'''Returns the gradients of `variables` (list of tensor variables)",
            "with regard to `loss`.",
            "'''",
            "-    return tf.gradients(loss, variables)",
            "+    return tf.gradients(loss, variables, colocate_gradients_with_ops=True)",
            "",
            "",
            "def stop_gradient(variables):"
        ]
    },
    {
        "number": 8514,
        "comments": "",
        "commit_message": "fix num_nodes\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def split(data, batch):",
            "",
            "# Edge indices should start at zero for every graph.",
            "data.edge_index -= node_slice[batch[row]].unsqueeze(0)",
            "-    data.__num_nodes__ = torch.bincount(batch).tolist()",
            "",
            "slices = {'edge_index': edge_slice}",
            "if data.x is not None:",
            "slices['x'] = node_slice",
            "+    else:",
            "+        data.__num_nodes__ = torch.bincount(batch).tolist()",
            "if data.edge_attr is not None:",
            "slices['edge_attr'] = edge_slice",
            "if data.y is not None:"
        ]
    },
    {
        "number": 8517,
        "comments": "",
        "commit_message": "`LOGGER` consolidation (#5569)\n\n* Logger consolidation\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def train(hyp,  # path/to/hyp.yaml or hyp dictionary",
            "",
            "# DP mode",
            "if cuda and RANK == -1 and torch.cuda.device_count() > 1:",
            "-        logging.warning('DP not recommended, instead use torch.distributed.run for best DDP Multi-GPU results.\\n'",
            "-                        'See Multi-GPU Tutorial at https://github.com/ultralytics/yolov5/issues/475 to get started.')",
            "+        LOGGER.warning('WARNING: DP not recommended, use torch.distributed.run for best DDP Multi-GPU results.\\n'",
            "+                       'See Multi-GPU Tutorial at https://github.com/ultralytics/yolov5/issues/475 to get started.')",
            "model = torch.nn.DataParallel(model)",
            "",
            "# SyncBatchNorm"
        ]
    },
    {
        "number": 8522,
        "comments": "",
        "commit_message": "dtype fix\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def add_remaining_self_loops(edge_index,",
            "loop_weight[row[inv_mask]] = edge_weight[inv_mask].view(-1)",
            "edge_weight = torch.cat([edge_weight[mask], loop_weight], dim=0)",
            "",
            "-    loop_index = torch.arange(0,",
            "-                              num_nodes,",
            "-                              dtype=torch.long,",
            "-                              device=row.device)",
            "+    loop_index = torch.arange(0, num_nodes, dtype=row.dtype, device=row.device)",
            "loop_index = loop_index.unsqueeze(0).repeat(2, 1)",
            "edge_index = torch.cat([edge_index[:, mask], loop_index], dim=1)"
        ]
    },
    {
        "number": 8524,
        "comments": "",
        "commit_message": "Fix loss_bbox in fovea_head.py when num_pos=0 (#2548)\n\n* fix bbox_loss in fovea_head.py when num_pos=0\n\n* Remove PIL from .isort.cfg\n\n* Make loss_bbox scalar in fovea_head.py when num_pos==0\n\n* Sum based reduction for loss_bbox in fovea_head\n\n* reverting to scalar implementation\n",
        "label": "",
        "answer": "no",
        "change": [
            "class FoveaHead(nn.Module):",
            "pos_weights,",
            "avg_factor=num_pos)",
            "else:",
            "-            loss_bbox = torch.tensor([0],",
            "-                                     dtype=flatten_bbox_preds.dtype,",
            "-                                     device=flatten_bbox_preds.device)",
            "+            loss_bbox = torch.tensor(",
            "+                0,",
            "+                dtype=flatten_bbox_preds.dtype,",
            "+                device=flatten_bbox_preds.device)",
            "return dict(loss_cls=loss_cls, loss_bbox=loss_bbox)",
            "",
            "def fovea_target(self, gt_bbox_list, gt_label_list, featmap_sizes, points):"
        ]
    },
    {
        "number": 8531,
        "comments": "",
        "commit_message": "mass linter fix\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Decoder(nn.Module):",
            "self.attention.init_win_idx()",
            "self.attention.init_states(inputs)",
            "outputs, stop_tokens, alignments, t = [], [], [], 0",
            "-        stop_flags = [True, False, False]",
            "while True:",
            "memory = self.prenet(self.memory_truncated)",
            "decoder_output, alignment, stop_token = self.decode(memory)"
        ]
    },
    {
        "number": 8532,
        "comments": "",
        "commit_message": "Use f-strings in the dataset scripts (#3291)\n\n* Finishes #3257\n\nUsed f-strings to format the .py files in the dataset folder\n\n* Fix style\n\n* Fix hkcancor dataset\n\nCo-authored-by: Mario \u0160a\u0161ko <mario@huggingface.co>\nCo-authored-by: mariosasko <mariosasko777@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class MsrGenomicsKbcomp(datasets.GeneratorBasedBuilder):",
            "",
            "if not os.path.exists(data_dir):",
            "raise FileNotFoundError(",
            "-                \"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('msr_genomics_kbcomp', data_dir=...)` that includes files unzipped from the reclor zip. Manual download instructions: {}\".format(",
            "-                    data_dir, self.manual_download_instructions",
            "-                )",
            "+                f\"{data_dir} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('msr_genomics_kbcomp', data_dir=...)` that includes files unzipped from the reclor zip. Manual download instructions: {self.manual_download_instructions}\"",
            ")",
            "return [",
            "datasets.SplitGenerator("
        ]
    },
    {
        "number": 8535,
        "comments": "",
        "commit_message": "Fix to ItoProcess and fwd_gradient docstring examples.\n\nPiperOrigin-RevId: 272845796\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def fwd_gradient(func, x, grad_x=None, use_gradient_tape=False):",
            "op and the standard `tf.gradients`",
            "```python",
            "t = tf.range(1, 3, dtype=tf.float32)  # Shape [2]",
            "-    y = tf.stack([t, t ** 2, t ** 3], axis=0)  # Shape [3, 2]",
            "+    def fn(t):",
            "+      return tf.stack([t, t ** 2, t ** 3], axis=0)  # Shape [3, 2]",
            "# Produces shape [3, 2] with values [[1, 1], [2, 4], [3, 12]]",
            "-    fwd_grad_y = fwd_gradient(y, t)",
            "+    fwd_grad_y = fwd_gradient(fn, t)",
            "# Produces shape [2] with values [6, 17].",
            "bck_grad_y = tf.gradients(y, t)[0]",
            "```"
        ]
    },
    {
        "number": 8538,
        "comments": "",
        "commit_message": "retro env improvement, some fixes\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Module(object):",
            "use_while_v2=False",
            "):",
            "Module.global_scope.append('while')",
            "-        if maximum_iterations is not None and maximum_iterations.dtype not in (tf.int32, tf.int64):",
            "+        if maximum_iterations is not None and maximum_iterations.dtype is not tf.int32:",
            "maximum_iterations = tf.dtypes.cast(x=maximum_iterations, dtype=tf.int32)",
            "if use_while_v2:",
            "x = while_v2.while_loop("
        ]
    },
    {
        "number": 8540,
        "comments": "",
        "commit_message": "Speech2TextTransformer (#10175)\n\n* s2t\n\n* fix config\n\n* conversion script\n\n* fix import\n\n* add tokenizer\n\n* fix tok init\n\n* fix tokenizer\n\n* first version working\n\n* fix embeds\n\n* fix lm head\n\n* remove extra heads\n\n* fix convert script\n\n* handle encoder attn mask\n\n* style\n\n* better enc attn mask\n\n* override _prepare_attention_mask_for_generation\n\n* handle attn_maks in encoder and decoder\n\n* input_ids => input_features\n\n* enable use_cache\n\n* remove old code\n\n* expand embeddings if needed\n\n* remove logits bias\n\n* masked_lm_loss => loss\n\n* hack tokenizer to support feature processing\n\n* fix model_input_names\n\n* style\n\n* fix error message\n\n* doc\n\n* remove inputs_embeds\n\n* remove input_embeds\n\n* remove unnecessary docstring\n\n* quality\n\n* SpeechToText => Speech2Text\n\n* style\n\n* remove shared_embeds\n\n* subsample => conv\n\n* remove Speech2TextTransformerDecoderWrapper\n\n* update output_lengths formula\n\n* fix table\n\n* remove max_position_embeddings\n\n* update conversion scripts\n\n* add possibility to do upper case for now\n\n* add FeatureExtractor and Processor\n\n* add tests for extractor\n\n* require_torch_audio => require_torchaudio\n\n* add processor test\n\n* update import\n\n* remove classification head\n\n* attention mask is now 1D\n\n* update docstrings\n\n* attention mask should be of type long\n\n* handle attention mask from generate\n\n* alwyas return attention_mask\n\n* fix test\n\n* style\n\n* doc\n\n* Speech2TextTransformer => Speech2Text\n\n* Speech2TextTransformerConfig => Speech2TextConfig\n\n* remove dummy_inputs\n\n* nit\n\n* style\n\n* multilinguial tok\n\n* fix tokenizer\n\n* add tgt_lang setter\n\n* save lang_codes\n\n* fix tokenizer\n\n* add forced_bos_token_id to tokenizer\n\n* apply review suggestions\n\n* add torchaudio to extra deps\n\n* add speech deps to CI\n\n* fix dep\n\n* add libsndfile to ci\n\n* libsndfile1\n\n* add speech to extras all\n\n* libsndfile1 -> libsndfile1\n\n* libsndfile\n\n* libsndfile1-dev\n\n* apt update\n\n* add sudo to install\n\n* update deps table\n\n* install libsndfile1-dev on CI\n\n* tuple to list\n\n* init conv layer\n\n* add model tests\n\n* quality\n\n* add integration tests\n\n* skip_special_tokens\n\n* add speech_to_text_transformer in toctree\n\n* fix tokenizer\n\n* fix fp16 tests\n\n* add tokenizer tests\n\n* fix copyright\n\n* input_values => input_features\n\n* doc\n\n* add model in readme\n\n* doc\n\n* change checkpoint names\n\n* fix copyright\n\n* fix code example\n\n* add max_model_input_sizes in tokenizer\n\n* fix integration tests\n\n* add do_lower_case to tokenizer\n\n* remove clamp trick\n\n* fix \"Add modeling imports here\"\n\n* fix copyrights\n\n* fix tests\n\n* SpeechToTextTransformer => SpeechToText\n\n* fix naming\n\n* fix table formatting\n\n* fix typo\n\n* style\n\n* fix typos\n\n* remove speech dep from extras[testing]\n\n* fix copies\n\n* rename doc file,\n\n* put imports under is_torch_available\n\n* run feat extract tests when torch is available\n\n* dummy objects for processor and extractor\n\n* fix imports in tests\n\n* fix import in modeling test\n\n* fxi imports\n\n* fix torch import\n\n* fix imports again\n\n* fix positional embeddings\n\n* fix typo in import\n\n* adapt new extractor refactor\n\n* style\n\n* fix torchscript test\n\n* doc\n\n* doc\n\n* Apply suggestions from code review\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* fix docs, copied from, style\n\n* fix docstring\n\n* handle imports\n\n* remove speech from all extra deps\n\n* remove s2t from seq2seq lm mapping\n\n* better names\n\n* skip training tests\n\n* add install instructions\n\n* List => Tuple\n\n* doc\n\n* fix conversion script\n\n* fix urls\n\n* add instruction for libsndfile\n\n* fix fp16 test\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "if is_torch_available():",
            "class GenerationTesterMixin:",
            "model_tester = None",
            "all_generative_model_classes = ()",
            "+    input_name = \"input_ids\"",
            "",
            "def _get_input_ids_and_config(self):",
            "config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()",
            "",
            "-        input_ids = inputs_dict[\"input_ids\"]",
            "-        attention_mask = torch.ones_like(input_ids)",
            "+        input_ids = inputs_dict[self.input_name]",
            "+        attention_mask = torch.ones_like(input_ids, dtype=torch.long)",
            "",
            "# cut to half length & take max batch_size 3",
            "max_batch_size = 2"
        ]
    },
    {
        "number": 8541,
        "comments": "",
        "commit_message": "fix rnn dropout\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TextCNN(object):",
            "with tf.name_scope(\"score\"):",
            "# \u5168\u8fde\u63a5\u5c42\uff0c\u540e\u9762\u63a5dropout\u4ee5\u53carelu\u6fc0\u6d3b",
            "fc = tf.layers.dense(gmp, self.config.hidden_dim, name='fc1')",
            "-            fc = tf.contrib.layers.dropout(fc,",
            "-                self.keep_prob)",
            "+            fc = tf.contrib.layers.dropout(fc, self.keep_prob)",
            "fc = tf.nn.relu(fc)",
            "",
            "# \u5206\u7c7b\u5668"
        ]
    },
    {
        "number": 8549,
        "comments": "",
        "commit_message": "fix backward in tests\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class CommonTestCases:",
            "inputs = inputs_dict.copy()",
            "inputs['head_mask'] = head_mask",
            "",
            "-                with torch.no_grad():",
            "-                    outputs = model(**inputs)",
            "+                outputs = model(**inputs)",
            "",
            "# Test that we can get a gradient back for importance score computation",
            "output = sum(t.sum() for t in outputs[0])"
        ]
    },
    {
        "number": 8551,
        "comments": "",
        "commit_message": "FIX BUG\n1. model parallel x_grad rescale\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class PartialFC(Module):",
            "logits.backward(grad)",
            "if total_features.grad is not None:",
            "total_features.grad.detach_()",
            "-        x_grad: torch.Tensor = torch.zeros_like(features)",
            "-        x_grad.mul_(self.world_size)",
            "-",
            "+        x_grad: torch.Tensor = torch.zeros_like(features, requires_grad=True)",
            "# feature gradient all-reduce",
            "dist.reduce_scatter(x_grad, list(total_features.grad.chunk(self.world_size, dim=0)))",
            "+        x_grad = x_grad * self.world_size",
            "# backward backbone",
            "return x_grad, loss_v"
        ]
    },
    {
        "number": 8554,
        "comments": "",
        "commit_message": "Release 1.8.6rc0 (#660)\n\n* Release 1.8.6rc0\n\n* Update CHANGELOG.md\n\n* Typo Fix\n\n* Changelog Updated\n\n* tl_logging more readable\n\n* Typo Fixed\n\n* https enforced for badges\n\n* RTD badges pointing to Latest Commit\n\n* Decorators API Refactored\n\n* extra_requires `all`, `all_cpu` and `all_gpu` added\n\n* Error fix\n\n* YAPF Formating Correction\n\n* Test for private method decorator added\n\n* Test Logging Verbosity Fixed to Debug when runned individually\n\n* YAPF corrections applied\n\n* Changelog Added\n\n* Changelog updated\n\n* PR number changed\n\n* Changelog updated\n\n* Update .travis.yml\n\n* Update CONTRIBUTING.md\n\n* Update CONTRIBUTING.md\n\n* Update CHANGELOG.md\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TL_Logger_Test(CustomTestCase):",
            "",
            "if __name__ == '__main__':",
            "",
            "-    # tl.logging.set_verbosity(tl.logging.INFO)",
            "-    # tl.logging.set_verbosity(tl.logging.INFO)",
            "-    tl.logging.set_verbosity(tl.logging.DEBUG)",
            "+    tf.logging.set_verbosity(tf.logging.DEBUG)",
            "tl.logging.set_verbosity(tl.logging.DEBUG)",
            "",
            "unittest.main()"
        ]
    },
    {
        "number": 8561,
        "comments": "",
        "commit_message": "Update DDP for `torch.distributed.run` with `gloo` backend (#3680)\n\n* Update DDP for `torch.distributed.run`\n\n* Add LOCAL_RANK\n\n* remove opt.local_rank\n\n* backend=\"gloo|nccl\"\n\n* print\n\n* print\n\n* debug\n\n* debug\n\n* os.getenv\n\n* gloo\n\n* gloo\n\n* gloo\n\n* cleanup\n\n* fix getenv\n\n* cleanup\n\n* cleanup destroy\n\n* try nccl\n\n* return opt\n\n* add --local_rank\n\n* add timeout\n\n* add init_method\n\n* gloo\n\n* move destroy\n\n* move destroy\n\n* move print(opt) under if RANK\n\n* destroy only RANK 0\n\n* move destroy inside train()\n\n* restore destroy outside train()\n\n* update print(opt)\n\n* cleanup\n\n* nccl\n\n* gloo with 60 second timeout\n\n* update namespace printing\n",
        "label": "",
        "answer": "no",
        "change": [
            "def torch_distributed_zero_first(local_rank: int):",
            "Decorator to make all processes in distributed training wait for each local_master to do something.",
            "\"\"\"",
            "if local_rank not in [-1, 0]:",
            "-        torch.distributed.barrier()",
            "+        dist.barrier()",
            "yield",
            "if local_rank == 0:",
            "-        torch.distributed.barrier()",
            "+        dist.barrier()",
            "",
            "",
            "def init_torch_seeds(seed=0):"
        ]
    },
    {
        "number": 8562,
        "comments": "",
        "commit_message": "lint things\n\nSummary:\nLint related fixes: Improve internal/OSS consistency. Fix the fight between black and certain pyre-ignore markers by moving them to the line before.\nUse clang-format-8 automatically if present. Small number of pyre fixes.\n\narc doesn't run pyre at the moment, so I put back the explicit call to pyre. I don't know if there's an option somewhere to change this.\n\nReviewed By: nikhilaravi\n\nDifferential Revision: D19780518\n\nfbshipit-source-id: ef1c243392322fa074130f6cff2dd8a6f7738a7f\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Textures(object):",
            "msg = \"Expected verts_rgb to be of shape (N, V, 3); got %r\"",
            "raise ValueError(msg % verts_rgb.shape)",
            "if maps is not None:",
            "-            if torch.is_tensor(map) and map.ndim != 4:",
            "+            if torch.is_tensor(maps) and maps.ndim != 4:",
            "msg = \"Expected maps to be of shape (N, H, W, 3); got %r\"",
            "raise ValueError(msg % repr(maps.shape))",
            "elif isinstance(maps, list):"
        ]
    },
    {
        "number": 8563,
        "comments": "",
        "commit_message": "Fixed failing test for statistical einsum (#6073)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def einsum(",
            "*operands: Union[tf.Tensor, tf.Variable],",
            "out: Optional[Union[tf.Tensor, tf.Variable]] = None,",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "+    dtype = _get_promoted_type_of_operands(operands)",
            "operands = (tf.cast(operand, tf.float32) for operand in operands)",
            "-    return tf.einsum(equation, *operands)",
            "+    return tf.cast(tf.einsum(equation, *operands), dtype)"
        ]
    },
    {
        "number": 8566,
        "comments": "",
        "commit_message": "allow integer device for BatchEncoding (#9271)\n\nFixes #9244\n\nCo-authored-by: Jethro Kuan <jethro.kuan@bytedance.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class BatchEncoding(UserDict):",
            "# This check catches things like APEX blindly calling \"to\" on all inputs to a module",
            "# Otherwise it passes the casts down and casts the LongTensor containing the token idxs",
            "# into a HalfTensor",
            "-        if isinstance(device, str) or isinstance(device, torch.device):",
            "+        if isinstance(device, str) or isinstance(device, torch.device) or isinstance(device, int):",
            "self.data = {k: v.to(device=device) for k, v in self.data.items()}",
            "else:",
            "logger.warning("
        ]
    },
    {
        "number": 8568,
        "comments": "",
        "commit_message": "fix bug in gradient computation for tail adaptive loss (#1805)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TraceTailAdaptive_ELBO(Trace_ELBO):",
            "check_fully_reparametrized(site)",
            "",
            "# rank the particles according to p/q",
            "-        log_pq = torch.logsumexp(log_p - log_q, dim=0)",
            "+        log_pq = log_p - log_q",
            "rank = torch.argsort(log_pq, descending=False)",
            "rank = torch.index_select(torch.arange(self.num_particles) + 1, -1, rank).type_as(log_pq)"
        ]
    },
    {
        "number": 8574,
        "comments": "",
        "commit_message": "use tf.gfile in saver (fix #345)\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class ModelSaver(Callback):",
            "self.var_collections = var_collections",
            "if checkpoint_dir is None:",
            "checkpoint_dir = logger.LOG_DIR",
            "-        assert os.path.isdir(checkpoint_dir), checkpoint_dir",
            "+        assert tf.gfile.IsDirectory(checkpoint_dir), checkpoint_dir",
            "self.checkpoint_dir = checkpoint_dir",
            "",
            "def _setup_graph(self):"
        ]
    },
    {
        "number": 8578,
        "comments": "",
        "commit_message": "Tensorboard model visualization bug fix (#2758)\n\nThis fix should allow for visualizing YOLOv5 model graphs correctly in Tensorboard by uncommenting line 335 in train.py:\n```python\n                    if tb_writer:\n                        tb_writer.add_graph(torch.jit.trace(model, imgs, strict=False), [])  # add model graph\n```\n\nThe problem was that the detect() layer checks the input size to adapt the grid if required, and tracing does not seem to like this shape check (even if the shape is fine and no grid recomputation is required). The following will warn:\nhttps://github.com/ultralytics/yolov5/blob/0cae7576a9241110157cd154fc2237e703c2719e/train.py#L335\n\nSolution is below. This is a YOLOv5s model displayed in TensorBoard. You can see the Detect() layer merging the 3 layers into a single output for example, and everything appears to work and visualize correctly.\n```python\ntb_writer.add_graph(torch.jit.trace(model, imgs, strict=False), [])\n```\n<img width=\"893\" alt=\"Screenshot 2021-04-11 at 01 10 09\" src=\"https://user-images.githubusercontent.com/26833433/114286928-349bd600-9a63-11eb-941f-7139ee6cd602.png\">\n",
        "label": "",
        "answer": "no",
        "change": [
            "def train(hyp, opt, device, tb_writer=None):",
            "Thread(target=plot_images, args=(imgs, targets, paths, f), daemon=True).start()",
            "# if tb_writer:",
            "#     tb_writer.add_image(f, result, dataformats='HWC', global_step=epoch)",
            "-                    #     tb_writer.add_graph(model, imgs)  # add model to tensorboard",
            "+                    #     tb_writer.add_graph(torch.jit.trace(model, imgs, strict=False), [])  # add model graph",
            "elif plots and ni == 10 and wandb_logger.wandb:",
            "wandb_logger.log({\"Mosaics\": [wandb_logger.wandb.Image(str(x), caption=x.name) for x in",
            "save_dir.glob('train*.jpg') if x.exists()]})"
        ]
    },
    {
        "number": 8580,
        "comments": "",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestMaskedLayerNorm(AllenNlpTestCase):",
            "mask_n = np.array([[1, 1, 0], [1, 1, 1]])",
            "",
            "x = torch.from_numpy(x_n).float()",
            "-        mask = torch.from_numpy(mask_n)",
            "+        mask = torch.from_numpy(mask_n).bool()",
            "",
            "layer_norm = MaskedLayerNorm(7, gamma0=0.2)",
            "normed_x = layer_norm(x, mask)"
        ]
    },
    {
        "number": 8585,
        "comments": "",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class MultiHeadSelfAttention(Seq2SeqEncoder):",
            "",
            "batch_size, timesteps, _ = inputs.size()",
            "if mask is None:",
            "-            mask = inputs.new_ones(batch_size, timesteps)",
            "+            mask = inputs.new_ones(batch_size, timesteps).bool()",
            "",
            "# Shape (batch_size, timesteps, 2 * attention_dim + values_dim)",
            "combined_projection = self._combined_projection(inputs)"
        ]
    },
    {
        "number": 8591,
        "comments": "",
        "commit_message": "Drop torch 1.6 testing (#10390)\n\n* Drop torch 1.6 support\n\n* Drop 1.6 support\n\n* Update CHANGELOG\n\n* Fixes\n\n* Split change\n\n* Undo change\n\n* 1.7 -> 1.7.1\n\nhttps://github.com/pytorch/pytorch/issues/47354\n\n* Force trigger nightly\n\n* Update .github/workflows/events-nightly.yml\n\nCo-authored-by: Aki Nitta <nitta@akihironitta.com>\n\n* Revert 1.7.1 change - try wildcard\n\n* Update adjust versions and test it\n\n* Undo test changes\n\n* Revert \"Undo test changes\"\n\nThis reverts commit 3a6acadd115e86f02d83a788f1978372ab6764f3.\n\n* Update CHANGELOG.md\n\nCo-authored-by: Aki Nitta <nitta@akihironitta.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class AcceleratorConnector:",
            "self.deterministic = deterministic",
            "if _TORCH_GREATER_EQUAL_1_8:",
            "torch.use_deterministic_algorithms(deterministic)",
            "-        elif _TORCH_GREATER_EQUAL_1_7:",
            "+        else:",
            "torch.set_deterministic(deterministic)",
            "-        else:  # the minimum version Lightning supports is PyTorch 1.6",
            "-            torch._set_deterministic(deterministic)",
            "if deterministic:",
            "# fixing non-deterministic part of horovod",
            "# https://github.com/PyTorchLightning/pytorch-lightning/pull/1572/files#r420279383"
        ]
    },
    {
        "number": 8598,
        "comments": "",
        "commit_message": "fixed linting and tests\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "for i in range(split.size(0) - 1):",
            "model.conv1.reset_parameters()",
            "model.conv2.reset_parameters()",
            "model.fc1.reset_parameters()",
            "-        times = []",
            "for epoch in range(1, 301):",
            "train(epoch)",
            "-        times = torch.FloatTensor(times)",
            "acc = test(epoch, test_loader, ' Test Accuracy')",
            "accs_single.append(acc)",
            "accs.append(accs_single)"
        ]
    },
    {
        "number": 8602,
        "comments": "",
        "commit_message": "fix ctc_loss API (#215)\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Model(ModelDesc):",
            "W_init=tf.truncated_normal_initializer(stddev=0.01))",
            "logits = tf.reshape(logits, (BATCH, -1, NR_CLASS))",
            "",
            "-        loss = tf.nn.ctc_loss(logits, label, seqlen, time_major=False)",
            "+        loss = tf.nn.ctc_loss(label, logits, seqlen, time_major=False)",
            "",
            "self.cost = tf.reduce_mean(loss, name='cost')"
        ]
    },
    {
        "number": 8604,
        "comments": "",
        "commit_message": "remove numpy import (#1116)\n\n* remove numpy import\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* undo and add setup develop in gh actions\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* remove create_checkerboard\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestGradient:",
            "None, None, :, :",
            "]",
            "assert_allclose(",
            "-            gradient(tensor, torch.ones_like(structural_element), structuring_element=structural_element), expected,",
            "-            atol=1e-3, rtol=1e-3",
            "+            gradient(tensor, torch.ones_like(structural_element), structuring_element=structural_element),",
            "+            expected,",
            "+            atol=1e-3,",
            "+            rtol=1e-3,",
            ")",
            "",
            "def test_exception(self, device, dtype):"
        ]
    },
    {
        "number": 8608,
        "comments": "",
        "commit_message": "Fix issue for \"AttributeError: module 'tensorflow._api.v2.image' has no attribute 'resize_with_crop_or_pad'\" (#1049)\n\nThe API 'resize_with_crop_or_pad()' seems only for TF 1.0.\nFor TF 2.0.0-alpha0, the API should be resize_with_pad(). Modify Line 125, then this example can work fine.\n",
        "label": "",
        "answer": "no",
        "change": [
            "def _map_fn_train(img, target):",
            "",
            "def _map_fn_test(img, target):",
            "# 1. Crop the central [height, width] of the image.",
            "-    img = tf.image.resize_with_crop_or_pad(img, 24, 24)",
            "+    img = tf.image.resize_with_pad(img, 24, 24)",
            "# 2. Subtract off the mean and divide by the variance of the pixels.",
            "img = tf.image.per_image_standardization(img)",
            "img = tf.reshape(img, (24, 24, 3))"
        ]
    },
    {
        "number": 8609,
        "comments": "",
        "commit_message": "breaking changes in independent act mode, other fixes\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Dropout(Layer):",
            "pass_tensors=dropout",
            ")",
            "",
            "-        skip_dropout = tf.math.logical_not(x=Module.retrieve_tensor(name='optimization'))",
            "+        skip_dropout = tf.math.logical_not(x=Module.retrieve_tensor(name='deterministic'))",
            "zero = tf.constant(value=0.0, dtype=util.tf_dtype(dtype='float'))",
            "skip_dropout = tf.math.logical_or(x=skip_dropout, y=tf.math.equal(x=rate, y=zero))",
            "return self.cond(pred=skip_dropout, true_fn=no_dropout, false_fn=apply_dropout)"
        ]
    },
    {
        "number": 8617,
        "comments": "",
        "commit_message": "samplified-example-code (#385)\n\n* Update tutorial_mlp_dropout2.py\n\n* Update tutorial_mlp_dropout1.py\n\n* Update tutorial_mnist_simple.py\n\n* Update tutorial_mnist_float16.py\n\n* fix format\n\n* fix format\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "sess = tf.InteractiveSession()",
            "batch_size = 128",
            "",
            "x = tf.placeholder(LayersConfig.tf_dtype, shape=[batch_size, 28, 28, 1])",
            "-y_ = tf.placeholder(",
            "-    tf.int64, shape=[",
            "-        batch_size,",
            "-    ])",
            "+y_ = tf.placeholder(tf.int64, shape=[batch_size])",
            "",
            "",
            "def model(x, is_train=True, reuse=False):"
        ]
    },
    {
        "number": 8618,
        "comments": "",
        "commit_message": "fix import_user_module (#3144)\n\nSummary:\n## What does this PR do?\nAvoid throwing ValueError when attempting to load a user defined module from common.user_dir that has the same module name and same module path as some loaded module. This occurs when a job is preempted and restarts using submitit_slurm\n\nX-link: https://github.com/fairinternal/fairseq-py/pull/3144\n\nReviewed By: Abdel-rahmanMohamed\n\nDifferential Revision: D34521450\n\nPulled By: wnhsu\n\nfbshipit-source-id: eed00d4238a66dc524eee400a55ad2c011e1543c\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class SequenceGenerator(nn.Module):",
            "cum_unfin.append(prev)",
            "cum_fin_tensor = torch.tensor(cum_unfin, dtype=torch.int).to(bbsz_idx)",
            "",
            "-        unfin_idx = torch.div(bbsz_idx, beam_size, rounding_mode='trunc')",
            "+        unfin_idx = torch.div(bbsz_idx, beam_size, rounding_mode=\"trunc\")",
            "sent = unfin_idx + torch.index_select(cum_fin_tensor, 0, unfin_idx)",
            "",
            "# Create a set of \"{sent}{unfin_idx}\", where"
        ]
    },
    {
        "number": 8621,
        "comments": "",
        "commit_message": "fix loading jit and onnx embeddings\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TransformerEmbeddings(TransformerBaseEmbeddings):",
            "if self.token_embedding:",
            "assert word_ids is not None",
            "assert token_lengths is not None",
            "-            all_token_embeddings = torch.zeros(",
            "-                word_ids.shape[0], token_lengths.max(), self.embedding_length_internal, device=flair.device  # type: ignore",
            "+            all_token_embeddings = torch.zeros(  # type: ignore",
            "+                word_ids.shape[0], token_lengths.max(), self.embedding_length_internal, device=flair.device",
            ")",
            "true_tensor = torch.ones_like(word_ids[:, :1], dtype=torch.bool)",
            "if self.subtoken_pooling == \"first\":"
        ]
    },
    {
        "number": 8622,
        "comments": "",
        "commit_message": "[MaskRCNN] bugfix on data loading\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def crop_and_resize(image, boxes, box_ind, crop_size, pad_border=True):",
            "boxes = transform_fpcoor_for_tf(boxes, image_shape, [crop_size, crop_size])",
            "image = tf.transpose(image, [0, 2, 3, 1])   # 1hwc",
            "ret = tf.image.crop_and_resize(",
            "-        image, boxes, box_ind,",
            "+        image, boxes, tf.to_int32(box_ind),",
            "crop_size=[crop_size, crop_size])",
            "ret = tf.transpose(ret, [0, 3, 1, 2])   # ncss",
            "return ret"
        ]
    },
    {
        "number": 8623,
        "comments": "",
        "commit_message": "Fix linting issues\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "var.support_native_out = True",
            "# ------#",
            "",
            "",
            "-def einsum(equation: str, *operands: torch.Tensor, out: Optional[torch.Tensor] = None) -> torch.Tensor:",
            "+def einsum(",
            "+    equation: str, *operands: torch.Tensor, out: Optional[torch.Tensor] = None",
            "+) -> torch.Tensor:",
            "return torch.einsum(equation, *operands)"
        ]
    },
    {
        "number": 8630,
        "comments": "",
        "commit_message": "ci: Redo `format.sh --all` script & backfill lint fixes (#9956)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def from_importance_weights(log_rhos,",
            "if clip_pg_rho_threshold is not None:",
            "clip_pg_rho_threshold.shape.assert_has_rank(0)",
            "",
            "-    with tf1.name_scope(name, values=[",
            "-        log_rhos, discounts, rewards, values, bootstrap_value",
            "-    ]):",
            "+    with tf1.name_scope(",
            "+            name,",
            "+            values=[log_rhos, discounts, rewards, values, bootstrap_value]):",
            "rhos = tf.math.exp(log_rhos)",
            "if clip_rho_threshold is not None:",
            "clipped_rhos = tf.minimum(",
            "clip_rho_threshold, rhos, name=\"clipped_rhos\")",
            "",
            "-            tf1.summary.histogram(",
            "-                    \"clipped_rhos_1000\", tf.minimum(1000.0, rhos))",
            "+            tf1.summary.histogram(\"clipped_rhos_1000\", tf.minimum(",
            "+                1000.0, rhos))",
            "tf1.summary.scalar(",
            "\"num_of_clipped_rhos\",",
            "tf.reduce_sum("
        ]
    },
    {
        "number": 8634,
        "comments": "",
        "commit_message": "[BUG] remove float casting in image_to_tensor (#497)\n\n* remove float casting when onverting to tensor\n\n* test dtype is persistent\n\n* fixl linter issues\n",
        "label": "",
        "answer": "no",
        "change": [
            "def image_to_tensor(image: Union[np.ndarray, Image.Image], keepdim: bool = True)",
            "\"Input size must be a two, three or four dimensional array\")",
            "",
            "input_shape = image.shape",
            "-    tensor: torch.Tensor = torch.from_numpy(image).to(torch.float)",
            "+    tensor: torch.Tensor = torch.from_numpy(image)",
            "",
            "if len(input_shape) == 2:",
            "# (H, W) -> (1, H, W)"
        ]
    },
    {
        "number": 8641,
        "comments": "",
        "commit_message": "Fix _module_available to detect horovod.torch properly (#12377)\n\n\nCo-authored-by: Adrian W\u00e4lchli <aedu.waelchli@gmail.com>\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\nCo-authored-by: Jirka <jirka.borovec@seznam.cz>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class AcceleratorConnector:",
            "hvd.init()",
            "if isinstance(self.accelerator, GPUAccelerator):",
            "# Horovod assigns one local GPU per process",
            "-            self._parallel_devices = list(range(hvd.local_size()))",
            "+            self._parallel_devices = [torch.device(f\"cuda:{i}\") for i in range(hvd.local_size())]",
            "else:",
            "self._parallel_devices = [torch.device(\"cpu\")] * hvd.local_size()"
        ]
    },
    {
        "number": 8642,
        "comments": "",
        "commit_message": "Allow not specifying feature cols other than `predictions`/`references` in `Metric.compute` (#3824)\n\n* Update SARI metric\n\n* Fix Metric.compute\n\n* Style fix\n\n* Add test\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Sari(datasets.Metric):",
            "inputs_description=_KWARGS_DESCRIPTION,",
            "features=datasets.Features(",
            "{",
            "+                    \"sources\": datasets.Value(\"string\", id=\"sequence\"),",
            "\"predictions\": datasets.Value(\"string\", id=\"sequence\"),",
            "\"references\": datasets.Sequence(datasets.Value(\"string\", id=\"sequence\"), id=\"references\"),",
            "}"
        ]
    },
    {
        "number": 8647,
        "comments": "",
        "commit_message": "[rllib] Fixes typo in RolloutWorker.__init__ (#17583)\n\nFixes the typo in RolloutWorker.__init__, closes #17582\n",
        "label": "",
        "answer": "no",
        "change": [
            "class RolloutWorker(ParallelIteratorWorker):",
            "# Not all Operations support this.",
            "torch.use_deterministic_algorithms(True)",
            "else:",
            "-                        torch.set_determinstic(True)",
            "+                        torch.set_deterministic(True)",
            "# This is only for Convolution no problem.",
            "torch.backends.cudnn.deterministic = True",
            "# Tf2.x."
        ]
    },
    {
        "number": 8648,
        "comments": "",
        "commit_message": "refactor of the optimization interface (#212)\n\n* initial commit\n\n* optim.py\n\n* basic thing working\n\n* got some tests to pass\n\n* fixedguidemodel test happy\n\n* more comments, default implementation for loss_and_grads, ...\n\n* change import\n\n* got a lot more tests to pass; a tiny bit inference test refactoring\n\n* flake8\n\n* fix more tests\n\n* made conjugate model tests faster; made example test pass; all tests pass now maybe?\n\n* made conjugate model tests faster; made example test pass; all tests pass now maybe?\n\n* xfail test_examples\n\n* xfail test_examples flake8\n\n* made tests/poutine/test_mapdata.py happy\n\n* fix test lambda test\n\n* more refactoring...\n\n* more refactoring...\n\n* progress on further rerefactoring\n\n* always more refactoring\n\n* linting\n\n* done with initial re-refactor?\n\n* fix bug\n\n* fl8\n\n* address tests\n\n*  make examples happy\n\n* fix tests. fixed everything?\n\n* final clean-up?\n\n* assert fix\n\n* sets not hashable error\n\n* exclusively doc string changes\n\n* batch rearrange\n\n* fix fake conflict\n\n* fl8\n\n* doc rebuild workaround\n\n* fix random module test (tags)\n\n* wrap loss\n\n* rebalance tests\n\n* whoops\n\n* fix more merge conflicts; fix test imports\n\n* fl8\n\n*  delete old tracegraph_kl_qp.py\n\n* travis workaround\n\n* better rng seeds/inits?\n\n* fix travis yml\n\n* these seeds/inits should be good\n\n* fix data\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def main():",
            "",
            "# get batch",
            "batch_data = mnist_data[batch_start:batch_end]",
            "-            epoch_loss += kl_optim.step(batch_data)",
            "+            epoch_loss += svi.step(batch_data)",
            "",
            "sample = model_sample()",
            "vis.image(batch_data[0].contiguous().view(28, 28).data.numpy())"
        ]
    },
    {
        "number": 8651,
        "comments": "",
        "commit_message": "Bug(?) fix\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class DistributedFusedAdam(torch.optim.Optimizer):",
            "",
            "def _pipeline_block_step(self, block_id):",
            "if self._new_params is None:",
            "-            self._new_params = torch.zeros_like(self._flat_grads,dtype=uint8 if self._e5m2_allgather else self._flat_grads.dtype)",
            "+            self._new_params = torch.zeros_like(self._flat_grads,dtype=torch.uint8 if self._e5m2_allgather else self._flat_grads.dtype)",
            "",
            "start = block_id * self._block_size",
            "end = start + self._block_size"
        ]
    },
    {
        "number": 8652,
        "comments": "",
        "commit_message": "Add SPOS docs and improve NAS doc structure (#1907)\n\n* darts mutator docs\n\n* fix docs\n\n* update\n\n* add docs for SPOS\n\n* index SPOS\n\n* restore workers\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Trainer(BaseTrainer):",
            "workers : int",
            "Number of workers used in data preprocessing.",
            "device : torch.device",
            "-            Device object. Either `torch.device(\"cuda\")` or torch.device(\"cpu\")`. When `None`, trainer will",
            "+            Device object. Either ``torch.device(\"cuda\")`` or ``torch.device(\"cpu\")``. When ``None``, trainer will",
            "automatic detects GPU and selects GPU first.",
            "log_frequency : int",
            "Number of mini-batches to log metrics."
        ]
    },
    {
        "number": 8656,
        "comments": "",
        "commit_message": "Fix imports\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def attn(x, scope, n_state, *, past, params, block_offset=0, train=False):",
            ")",
            "else:",
            "# HOWEVER, `attention` DOES NOT implement masking so we need to pass in `bias` on our own!",
            "-            a = mtf.transformer.attention.attention(",
            "+            a = mtf_transformer.attention.attention(",
            "q, k, v,",
            "memory_length_dim=dim_seq,",
            "key_dim=dim_embd,"
        ]
    },
    {
        "number": 8657,
        "comments": "",
        "commit_message": "improved global tensor handling, various other fixes\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Dropout(Layer):",
            ")",
            "",
            "update = Module.retrieve_tensor(name='update')",
            "-        return tf.cond(pred=update, true_fn=true_fn, false_fn=(lambda: x))",
            "+        return self.cond(pred=update, true_fn=true_fn, false_fn=(lambda: x))"
        ]
    },
    {
        "number": 8659,
        "comments": "",
        "commit_message": "Fixed out argument usage in eye torch (#7002)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def eye(",
            "elif 0 < k < n_cols:",
            "mat = torch.concat(",
            "[",
            "-                torch.zeros([n_rows, k], dtype=dtype, device=device, out=out),",
            "+                torch.zeros([n_rows, k], dtype=dtype, device=device),",
            "i[:, : n_cols - k],",
            "],",
            "1,",
            ")",
            "ret = torch.reshape(mat, reshape_dims).repeat(tile_dims)",
            "+        if out is not None:",
            "+            return ivy.inplace_update(out, ret)",
            "else:",
            "ret = torch.zeros(",
            "batch_shape + [n_rows, n_cols], dtype=dtype, device=device, out=out",
            ")",
            "-    if out is not None:",
            "-        return ivy.inplace_update(out, ret)",
            "return ret"
        ]
    },
    {
        "number": 8660,
        "comments": "",
        "commit_message": "fixed py3 errors\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class GaussianPyramidTests(TestCase):",
            "def test_elbo_reparameterized(self):",
            "for N in [3, 4, 5]:",
            "self.setup_pyramid(N)",
            "-            self.do_elbo_test(True, N * 4000 - 4000)",
            "+            self.do_elbo_test(True, N * 3000 - 3000)",
            "",
            "# def test_elbo_nonreparameterized(self): XXX to add",
            "#     self.do_elbo_test(False, 5000)"
        ]
    },
    {
        "number": 8673,
        "comments": "",
        "commit_message": "Check copies blackify (#10775)\n\n* Apply black before checking copies\n\n* Fix for class methods\n\n* Deal with lonely brackets\n\n* Remove debug and add forward changes\n\n* Separate copies and fix test\n\n* Add black as a test dependency\n",
        "label": "",
        "answer": "no",
        "change": [
            "class M2M100Decoder(M2M100PreTrainedModel):",
            "",
            "self.init_weights()",
            "",
            "-    # Copied from transformers.models.mbart.modeling_mbart.MBartDecoder.forward with MBart->M2M100",
            "def forward(",
            "self,",
            "input_ids=None,"
        ]
    },
    {
        "number": 8675,
        "comments": "",
        "commit_message": "fix for #6700\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class PersonalizedBase(Dataset):",
            "weight /= weight.mean()",
            "elif use_weight:",
            "#If an image does not have a alpha channel, add a ones weight map anyway so we can stack it later",
            "-                weight = torch.ones([channels] + latent_size)",
            "+                weight = torch.ones(latent_sample.shape)",
            "else:",
            "weight = None"
        ]
    },
    {
        "number": 8676,
        "comments": "",
        "commit_message": "enable black in the precommit (#1777)\n\n* enable black in the precommit\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add some fixes\n\n* update libface detection url\n\n* added url from kornia checkpoint\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def psnr(input: torch.Tensor, target: torch.Tensor, max_val: float) -> torch.Ten",
            "if input.shape != target.shape:",
            "raise TypeError(f\"Expected tensors of equal shapes, but got {input.shape} and {target.shape}\")",
            "",
            "-    return 10.0 * torch.log10(max_val ** 2 / mse(input, target, reduction='mean'))",
            "+    return 10.0 * torch.log10(max_val**2 / mse(input, target, reduction='mean'))"
        ]
    },
    {
        "number": 8682,
        "comments": "",
        "commit_message": "Update serving signatures and make sure we actually use them (#19034)\n\n* Override save() to use the serving signature as the default\n\n* Replace int32 with int64 in all our serving signatures\n\n* Remember one very important line so as not to break every test at once\n\n* Dtype fix for TFLED\n\n* dtype fix for shift_tokens_right in general\n\n* Dtype fixes in mBART and RAG\n\n* Fix dtypes for test_unpack_inputs\n\n* More dtype fixes\n\n* Yet more mBART + RAG dtype fixes\n\n* Yet more mBART + RAG dtype fixes\n\n* Add a check that the model actually has a serving method\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TFFunnelForMultipleChoice(TFFunnelPreTrainedModel, TFMultipleChoiceLoss):",
            "@tf.function(",
            "input_signature=[",
            "{",
            "-                \"input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"input_ids\"),",
            "+                \"input_ids\": tf.TensorSpec((None, None), tf.int64, name=\"input_ids\"),",
            "\"attention_mask\": tf.TensorSpec((None, None), tf.float32, name=\"attention_mask\"),",
            "-                \"token_type_ids\": tf.TensorSpec((None, None), tf.int32, name=\"token_type_ids\"),",
            "+                \"token_type_ids\": tf.TensorSpec((None, None), tf.int64, name=\"token_type_ids\"),",
            "}",
            "]",
            ")"
        ]
    },
    {
        "number": 8683,
        "comments": "",
        "commit_message": "Fixes wrong imports\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def main():",
            "# extract",
            "logging.info('backend = ' + args.backend)",
            "if args.backend == \"pytorch\":",
            "-        from tts.pytorch.tts_pytorch import decode",
            "+        fromespnet.lmpytorch.tts_pytorch import decode",
            "decode(args)",
            "else:",
            "raise NotImplementedError(\"Only pytorch is supported.\")"
        ]
    },
    {
        "number": 8684,
        "comments": "",
        "commit_message": "quick fix\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "with tf.Graph().as_default():",
            "return x",
            "",
            "net = dnn(X)",
            "-    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(net, Y))",
            "+    loss = tf.reduce_mean(",
            "+        tf.nn.softmax_cross_entropy_with_logits(logits=net, labels=Y))",
            "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)",
            "accuracy = tf.reduce_mean(",
            "tf.cast(tf.equal(tf.argmax(net, 1), tf.argmax(Y, 1)), tf.float32),"
        ]
    },
    {
        "number": 8686,
        "comments": "",
        "commit_message": "Change attention mask dtype to be bool. Fix #1119\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TransfoXLModel(TransfoXLPreTrainedModel):",
            "else:",
            "mask_shift_len = qlen",
            "dec_attn_mask = (torch.triu(all_ones, 1+mlen)",
            "-                    + torch.tril(all_ones, -mask_shift_len)).byte()[:, :, None] # -1",
            "+                    + torch.tril(all_ones, -mask_shift_len)).bool()[:, :, None] # -1",
            "else:",
            "dec_attn_mask = torch.triu(",
            "-                word_emb.new_ones(qlen, klen), diagonal=1+mlen).byte()[:,:,None]",
            "+                word_emb.new_ones(qlen, klen), diagonal=1+mlen).bool()[:,:,None]",
            "",
            "hids = []",
            "attentions = []"
        ]
    },
    {
        "number": 8688,
        "comments": "",
        "commit_message": "Fix DETR integration test (#12734)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class DetrModelIntegrationTests(unittest.TestCase):",
            "expected_slice_masks = torch.tensor(",
            "[[-7.7558, -10.8788, -11.9797], [-11.8881, -16.4329, -17.7451], [-14.7316, -19.7383, -20.3004]]",
            ").to(torch_device)",
            "-        self.assertTrue(torch.allclose(outputs.pred_masks[0, 0, :3, :3], expected_slice_masks, atol=1e-4))",
            "+        self.assertTrue(torch.allclose(outputs.pred_masks[0, 0, :3, :3], expected_slice_masks, atol=1e-3))"
        ]
    },
    {
        "number": 8693,
        "comments": "",
        "commit_message": "fix best score on wrong device in EarlyStopping callback (#8295)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class EarlyStopping(Callback):",
            "f\" {self.monitor} = {current} {self.order_dict[self.mode]} {self.divergence_threshold}.\"",
            "\" Signaling Trainer to stop.\"",
            ")",
            "-        elif self.monitor_op(current - self.min_delta, self.best_score.to(trainer.lightning_module.device)):",
            "+        elif self.monitor_op(current - self.min_delta, self.best_score.to(current.device)):",
            "should_stop = False",
            "reason = self._improvement_message(current)",
            "self.best_score = current"
        ]
    },
    {
        "number": 8698,
        "comments": "",
        "commit_message": "Fixed failing test for linalg tensordot (#4636)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def tensordot(",
            "out: Optional[Union[tf.Tensor, tf.Variable]] = None,",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "# find type to promote to",
            "-    dtype = tf.experimental.numpy.promote_types(x1.dtype, x2.dtype)",
            "+    dtype = ivy.as_native_dtype(ivy.promote_types(x1.dtype, x2.dtype))",
            "",
            "# type casting to float32 which is acceptable for tf.tensordot",
            "x1, x2 = tf.cast(x1, tf.float32), tf.cast(x2, tf.float32)"
        ]
    },
    {
        "number": 8699,
        "comments": "",
        "commit_message": "Fix keras docstrings\n\nPiperOrigin-RevId: 424275818\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def log_cosh(y_true, y_pred):",
            ">>> x = y_pred - y_true",
            ">>> assert np.allclose(",
            "...     loss.numpy(),",
            "-  ...     np.mean(x + np.log(np.exp(-2. * x) + 1.) - math_ops.log(2.), axis=-1),",
            "+  ...     np.mean(x + np.log(np.exp(-2. * x) + 1.) - tf.math.log(2.), axis=-1),",
            "...     atol=1e-5)",
            "",
            "Args:"
        ]
    },
    {
        "number": 8705,
        "comments": "",
        "commit_message": "Fix a bug about variable spelling errors\n\nThe variable subsampling_factors in class CustomConverterMulEnc was  incorrectly  written as subsamping_factors , resulting in inconsistency.\n",
        "label": "",
        "answer": "no",
        "change": [
            "class CustomConverterMulEnc(object):",
            "ys = batch[0][-1]",
            "",
            "# perform subsampling",
            "-        if np.sum(self.subsamping_factors) > self.num_encs:",
            "+        if np.sum(self.subsampling_factors) > self.num_encs:",
            "xs_list = [",
            "[x[:: self.subsampling_factors[i], :] for x in xs_list[i]]",
            "for i in range(self.num_encs)"
        ]
    },
    {
        "number": 8706,
        "comments": "",
        "commit_message": "fix the import path of msda (#5338)\n\n* fix the import path of msda\n\n* fix the import path of msda\n",
        "label": "",
        "answer": "no",
        "change": [
            "class DeformableDetrTransformer(Transformer):",
            "nn.init.xavier_uniform_(p)",
            "for m in self.modules():",
            "if isinstance(m, MultiScaleDeformableAttention):",
            "-                m.init_weight()",
            "+                m.init_weights()",
            "if not self.as_two_stage:",
            "xavier_init(self.reference_points, distribution='uniform', bias=0.)",
            "normal_(self.level_embeds)"
        ]
    },
    {
        "number": 8707,
        "comments": "",
        "commit_message": "[torch] remove deprecated uint8 in favor of bool (#21384)\n\n* uint8 -> bool\n\n* fix copies\n\n* style\n\n* update test modeling commen when checking attention buffers\n\n* style\n\n* use logical not on random mask instead of subtraction with 1\n\n* remove torch uint8\n\n* quality\n\n* remove modified modeling utils\n\n* Update based on review\n\nCo-authored-by: sgugger <sylvain.gugger@gmail.com>\n\n---------\n\nCo-authored-by: sgugger <sylvain.gugger@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def filter_logits(logits, top_k=0, top_p=0.0, filter_value=-float(\"Inf\")):",
            "sorted_indices_to_remove[..., 0] = 0",
            "",
            "# indices_to_remove = sorted_indices[sorted_indices_to_remove]",
            "-        indices_to_remove = torch.zeros_like(logits, dtype=torch.uint8).scatter_(",
            "+        indices_to_remove = torch.zeros_like(logits, dtype=torch.bool).scatter_(",
            "dim=-1, index=sorted_indices, src=sorted_indices_to_remove",
            ")",
            "logits[indices_to_remove] = filter_value"
        ]
    },
    {
        "number": 8709,
        "comments": "",
        "commit_message": "Fixed a bug with the vocoder's inference demo\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "if __name__ == '__main__':",
            "torch.save({'step': step, 'model_state': model.state_dict()}, model_fpath)",
            "print('<saved>')",
            "",
            "-    optimiser = optim.Adam(model.parameters())",
            "-    train(model, optimiser, epochs=60, batch_size=128, classes=2**bits,",
            "+    optimizer = optim.Adam(model.parameters())",
            "+    train(model, optimizer, epochs=60, batch_size=64, classes=2 ** bits,",
            "seq_len=seq_len, step=step, lr=1e-4)",
            "",
            "\\ No newline at end of file"
        ]
    },
    {
        "number": 8711,
        "comments": "",
        "commit_message": "Layer API Refactoring (#675)\n\n* Layer API Refactoring\n\n* private method decorator added\n\n* Input Layers added\n\n* Lazy Import on ROI pooling\n\n* @zsdonghao change proposal added\n\n* UTF-8 header added\n\n* gitignore updated\n\n* Python2 error correction\n\n* Changelog Updated\n\n* Python Headers Uniformized + Codacy Error Fix + LSTMStateTuple missing import\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class ROIPoolingLayer(Layer):",
            "",
            "logging.info(\"ROIPoolingLayer %s: (%d, %d)\" % (name, pool_height, pool_width))",
            "",
            "-        self.inputs = prev_layer.outputs",
            "-",
            "self.outputs = roi_pooling(self.inputs, rois, pool_height, pool_width)",
            "",
            "-        self.all_layers.append(self.outputs)",
            "+        self._add_layers(self.outputs)"
        ]
    },
    {
        "number": 8713,
        "comments": "",
        "commit_message": "fixed problems with q_model internals, q_nstep_model disc cum reward, base_agent_test excludes in test_multi, evolutionary test params\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Optimizer(object):",
            "The optimization operation.",
            "\"\"\"",
            "deltas = self.step(time=time, variables=variables, **kwargs)",
            "-        # deltas[0] = tf.Print(deltas[0], (deltas[0],))",
            "with tf.control_dependencies(control_inputs=deltas):",
            "return tf.no_op()"
        ]
    },
    {
        "number": 8717,
        "comments": "",
        "commit_message": "Fix `DegreeScaler` to not divide by zero (#6877)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class DegreeScalerAggregation(Aggregation):",
            "elif scaler == 'amplification':",
            "out_scaler = out * (torch.log(deg + 1) / self.avg_deg_log)",
            "elif scaler == 'attenuation':",
            "-                out_scaler = out * (self.avg_deg_log / torch.log(deg + 1))",
            "+                # Clamp minimum degree to one to avoid dividing by zero:",
            "+                out_scaler = out * (self.avg_deg_log /",
            "+                                    torch.log(deg.clamp(min=1) + 1))",
            "elif scaler == 'linear':",
            "out_scaler = out * (deg / self.avg_deg_lin)",
            "elif scaler == 'inverse_linear':",
            "-                # Clamps minimum degree into one to avoid dividing by zero",
            "-                out_scaler = out * (self.avg_deg_lin / deg.clamp(1))",
            "+                # Clamp minimum degree to one to avoid dividing by zero:",
            "+                out_scaler = out * (self.avg_deg_lin / deg.clamp(min=1))",
            "else:",
            "raise ValueError(f\"Unknown scaler '{scaler}'\")",
            "outs.append(out_scaler)"
        ]
    },
    {
        "number": 8718,
        "comments": "",
        "commit_message": "Fixed module resolution for tf.keras optimizers and added unit tests (#1935)\n\nSigned-off-by: Travis Addair <taddair@uber.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def broadcast(backend, value, root_rank, name):",
            "return _eval(backend, hvd.broadcast(tf.constant(value, name=name), root_rank))",
            "",
            "",
            "-def load_model(keras, wrap_optimizer, filepath, custom_optimizers, custom_objects):",
            "+def load_model(keras, wrap_optimizer, optimizer_modules, filepath, custom_optimizers, custom_objects):",
            "horovod_objects = {",
            "subclass.__name__.lower(): wrap_optimizer(subclass)",
            "for subclass in keras.optimizers.Optimizer.__subclasses__()",
            "-        if subclass.__module__ == keras.optimizers.Optimizer.__module__",
            "+        if subclass.__module__ in optimizer_modules",
            "}",
            "",
            "if custom_optimizers is not None:"
        ]
    },
    {
        "number": 8719,
        "comments": "",
        "commit_message": "small fixes, replacing as_str arg with as_native, and removing redundant setting of the default value.\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "from ivy.container import Container",
            "",
            "",
            "def variable(x):",
            "-    with tf.device(\"/\" + ivy.dev(x, as_str=True).upper()):",
            "+    with ivy.dev(x, as_native=True):",
            "return tf.Variable(x, trainable=True)"
        ]
    },
    {
        "number": 8726,
        "comments": "",
        "commit_message": "Revert \"Fix mtf broadcast error when lowering\"\n\nThis reverts commit 8b9631fc5a06149fb6fc5465305217bf5ae31c3e.\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def expand_tile(value, newdim):",
            "print(value)",
            "print('############')",
            "",
            "-    return mtf.broadcast(mtf_expand_dims(value, newdim, 0),",
            "+    return mtf.broadcast(mtf_expand_dims(value, 'dummy_batch', 0),",
            "[newdim] + value.shape.dims)  # shape.dims gets us a list which we need in order to concat"
        ]
    },
    {
        "number": 8727,
        "comments": "",
        "commit_message": "fix test - reduce metric\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_metric_are_properly_reduced(tmpdir):",
            "trainer.fit(model)",
            "",
            "assert trainer.callback_metrics[\"val_acc\"] == 8 / 32.",
            "-    assert \"train_acc\" in trainer.callback_metrics",
            "+    assert \"train_loss\" in trainer.callback_metrics"
        ]
    },
    {
        "number": 8729,
        "comments": "",
        "commit_message": "Quick fix :) (#606)\n\n* Changing the name\n\n* style + quality\n\n* update doc and logo\n\n* clean up\n\n* circle-CI on the branche for now\n\n* fix daily dialog dataset\n\n* fix urls\n\nCo-authored-by: Quentin Lhoest <lhoest.q@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class ObjectInfo:",
            "",
            "def __repr__(self):",
            "single_line_description = self.description.replace(\"\\n\", \"\") if self.description is not None else \"\"",
            "-        return f\"nlp.ObjectInfo(\\n\\tid='{self.id}',\\n\\tdescription='{single_line_description}',\\n\\tfiles={self.siblings}\\n)\"",
            "+        return f\"datasets.ObjectInfo(\\n\\tid='{self.id}',\\n\\tdescription='{single_line_description}',\\n\\tfiles={self.siblings}\\n)\"",
            "",
            "",
            "class HfApi:"
        ]
    },
    {
        "number": 8738,
        "comments": "",
        "commit_message": "fix the embedding averaging in the entity linker\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class EntityLinker(flair.nn.DefaultClassifier[Sentence, Span]):",
            ")",
            "",
            "def emb_mean(self, span, embedding_names):",
            "-        return torch.mean(torch.cat([token.get_embedding(embedding_names) for token in span], 0), 0)",
            "+        return torch.mean(torch.stack([token.get_embedding(embedding_names) for token in span], 0), 0)",
            "",
            "def _get_data_points_from_sentence(self, sentence: Sentence) -> List[Span]:",
            "return sentence.get_spans(self.label_type)"
        ]
    },
    {
        "number": 8743,
        "comments": "",
        "commit_message": "Add test to ensure models can take int64 inputs (#17210)\n\n* Add test to ensure models can take int64 inputs\n\n* is_integer is an attribute, not a method\n\n* Fix test when some inputs aren't tensors\n\n* Add casts to blenderbot and blenderbot-small\n\n* Add casts to the other failing models\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TFAdaptiveSoftmaxMask(tf.keras.layers.Layer):",
            "@staticmethod",
            "def _gather_logprob(logprob, target):",
            "lp_size = shape_list(logprob)",
            "-        r = tf.range(lp_size[0])",
            "+        r = tf.range(lp_size[0], dtype=target.dtype)",
            "idx = tf.stack([r, target], 1)",
            "return tf.gather_nd(logprob, idx)"
        ]
    },
    {
        "number": 8744,
        "comments": "",
        "commit_message": "Fix `spots` argument processing for approximate option price under Heston.\n\nPiperOrigin-RevId: 353640676\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def european_option_price(*,",
            "sigmas = tf.convert_to_tensor(sigmas, dtype=dtype, name='sigmas')",
            "rhos = tf.convert_to_tensor(rhos, dtype=dtype, name='rhos')",
            "variances = tf.convert_to_tensor(variances, dtype=dtype, name='variances')",
            "-    forwards = tf.convert_to_tensor(forwards, dtype=dtype, name='forwards')",
            "",
            "if discount_factors is not None:",
            "discount_factors = tf.convert_to_tensor("
        ]
    },
    {
        "number": 8745,
        "comments": "",
        "commit_message": "Fix actor_critic example (#301)\n\n* smooth_l1_loss now requires shapes to match\n * once scalars are enabled we must torch.stack() instead of\n   torch.cat() a list of scalars\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def finish_episode():",
            "rewards = torch.Tensor(rewards)",
            "rewards = (rewards - rewards.mean()) / (rewards.std() + np.finfo(np.float32).eps)",
            "for (log_prob, value), r in zip(saved_actions, rewards):",
            "-        reward = r - value.data[0, 0]",
            "+        reward = r - value.data[0]",
            "policy_losses.append(-log_prob * reward)",
            "value_losses.append(F.smooth_l1_loss(value, Variable(torch.Tensor([r]))))",
            "optimizer.zero_grad()",
            "-    loss = torch.cat(policy_losses).sum() + torch.cat(value_losses).sum()",
            "+    loss = torch.stack(policy_losses).sum() + torch.stack(value_losses).sum()",
            "loss.backward()",
            "optimizer.step()",
            "del model.rewards[:]"
        ]
    },
    {
        "number": 8746,
        "comments": "",
        "commit_message": "Inline `_make_grid()` meshgrid (#9170)\n\n* Inline _make_grid() meshgrid \n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Detect(nn.Module):",
            "t = self.anchors[i].dtype",
            "shape = 1, self.na, ny, nx, 2  # grid shape",
            "y, x = torch.arange(ny, device=d, dtype=t), torch.arange(nx, device=d, dtype=t)",
            "-        if torch_1_10:  # torch>=1.10.0 meshgrid workaround for torch>=0.7 compatibility",
            "-            yv, xv = torch.meshgrid(y, x, indexing='ij')",
            "-        else:",
            "-            yv, xv = torch.meshgrid(y, x)",
            "+        yv, xv = torch.meshgrid(y, x, indexing='ij') if torch_1_10 else torch.meshgrid(y, x)  # torch>=0.7 compatibility",
            "grid = torch.stack((xv, yv), 2).expand(shape) - 0.5  # add grid offset, i.e. y = 2.0 * x - 0.5",
            "anchor_grid = (self.anchors[i] * self.stride[i]).view((1, self.na, 1, 1, 2)).expand(shape)",
            "return grid, anchor_grid"
        ]
    },
    {
        "number": 8747,
        "comments": "",
        "commit_message": "deprecate enable_pl_optimizer as it is not restored properly (#5244)\n\n* update\n\n* clean test\n\n* still in progress\n\n* udpdate test\n\n* update\n\n* update\n\n* resolve flake\n\n* add test for zero_grad\n\n* update\n\n* works without accumulated_grad\n\n* update\n\n* update\n\n* resolve amp\n\n* revert back to True\n\n* update\n\n* clean tests\n\n* cleaned out\n\n* typo\n\n* update test\n\n* git repare bug\n\n* remove print\n\n* udpate\n\n* Fix formatting/optimizer imports\n\n* Refactor the test for cleanliness\n\n* Add vanilla model to the test, better var names\n\n* Fixed var names, let's clean up these mock tests\n\n* repare test\n\n* update test\n\n* resolve flake8\n\n* add manual_optimization\n\n* update tests\n\n* resolve flake8\n\n* add random accumulate_grad_batches\n\n* improve test\n\n* Update tests/trainer/optimization/test_parity_automatic_optimization.py\n\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\n\n* Update tests/trainer/optimization/test_parity_automatic_optimization.py\n\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\n\n* update\n\n* clean tests\n\n* correct bug\n\n* Apply suggestions from code review\n\n* format\n\n* adress comments\n\n* update on comments\n\n* wip\n\n* typo\n\n* depreceate enable_pl_optimizer\n\n* resolve latest bugs\n\n* update\n\n* resolve merge\n\n* add comment\n\n* Update pytorch_lightning/core/lightning.py\n\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\n\n* Update tests/deprecated_api/test_remove_1-3.py\n\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\n\n* Update pytorch_lightning/trainer/connectors/optimizer_connector.py\n\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\n\n* Update pytorch_lightning/trainer/trainer.py\n\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\n\n* Update pytorch_lightning/trainer/trainer.py\n\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\n\n* Update tests/trainer/optimization/test_parity_automatic_optimization.py\n\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\n\n* update on comments\n\n* update restore\n\n* add a property\n\n* remove setstate as not needed anymore\n\n* update test\n\n* provide optimizer to on_before_zero_grad\n\n* update on comments\n\n* update on comments\n\n* Update pytorch_lightning/trainer/trainer.py\n\nCo-authored-by: Adrian W\u00e4lchli <aedu.waelchli@gmail.com>\n\n* Update tests/trainer/optimization/test_parity_automatic_optimization.py\n\nCo-authored-by: Adrian W\u00e4lchli <aedu.waelchli@gmail.com>\n\n* Update tests/trainer/optimization/test_parity_automatic_optimization.py\n\nCo-authored-by: Adrian W\u00e4lchli <aedu.waelchli@gmail.com>\n\n* Update tests/trainer/optimization/test_parity_automatic_optimization.py\n\nCo-authored-by: Adrian W\u00e4lchli <aedu.waelchli@gmail.com>\n\n* mofidy import\n\n* update changelog\n\n* resolve flake8\n\n* update\n\n* update\n\n* clean doc\n\nCo-authored-by: SeanNaren <sean@grid.ai>\nCo-authored-by: Ubuntu <ubuntu@ip-172-31-62-109.ec2.internal>\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\nCo-authored-by: Jirka Borovec <jirka.borovec@seznam.cz>\nCo-authored-by: Adrian W\u00e4lchli <aedu.waelchli@gmail.com>\nCo-authored-by: Sean Naren <sean.narenthiran@gmail.com>\n\n(cherry picked from commit f2e99d617f05ec65fded81ccc6d0d59807c47573)\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def register_optimizers(ctx, model):",
            "model.trainer.optimizers = optimizers",
            "model.trainer.lr_schedulers = lr_schedulers",
            "model.trainer.optimizer_frequencies = optimizer_frequencies",
            "-    model.trainer.convert_to_lightning_optimizers()",
            "",
            "",
            "def run_optimizer(ctx, model):"
        ]
    },
    {
        "number": 8748,
        "comments": "",
        "commit_message": "Update keras RNG logic to use tf.random.Generator if possible.\n\nThis change also update the RNG behavior for initializer. The seeded initializer will no longer produce same random value across multiple calls. Instead, it will produce different value, and multiple initializer created with same seed will produce same sequences. This change will the make the seeded initializer behavior align between v1 and v2.\n\nKeras was using stateful RNG op in various place when seed is not provided. The recommended approach in v2 is using tf.random.Generator which can be treat as a variable (seed) with stateless RNG op. This change make sure we use this new approach when seed is provided in v2, and also leave a flag to enforce the new approach, which has not been turn on yet. The new approach will be turned on when all the internal tests are fixed. V1 graph mode, the behavior is not change.\n\nPiperOrigin-RevId: 392092094\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def _get_random_features_initializer(initializer, shape):",
            "random_features_initializer = initializer",
            "if isinstance(initializer, str):",
            "if initializer.lower() == 'gaussian':",
            "-      random_features_initializer = tf.compat.v1.random_normal_initializer(",
            "-          stddev=1.0)",
            "+      random_features_initializer = initializers.RandomNormal(stddev=1.0)",
            "elif initializer.lower() == 'laplacian':",
            "-      random_features_initializer = tf.compat.v1.constant_initializer(",
            "+      random_features_initializer = initializers.Constant(",
            "_get_cauchy_samples(loc=0.0, scale=1.0, shape=shape))",
            "",
            "else:"
        ]
    },
    {
        "number": 8753,
        "comments": "",
        "commit_message": "retro env improvement, some fixes\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Deltafier(PreprocessingLayer):",
            "value=tf.constant(value=True, dtype=util.tf_dtype(dtype='bool')), read_value=False",
            ")",
            "with tf.control_dependencies(control_inputs=(assignment,)):",
            "-                return tf.concat(values=(tf.zeros_like(tensor=x[:1]), x[1:] - x[:-1]), axis=0)",
            "+                return tf.concat(values=(tf.zeros_like(tensor=x[:1]), x[1:] - x[:-1]), axis=0)  # dtype=util.tf_dtype(dtype='???'))",
            "",
            "def later_delta():",
            "return x - tf.concat(values=(self.previous, x[:-1]), axis=0)"
        ]
    },
    {
        "number": 8760,
        "comments": "",
        "commit_message": "fix test_ldm_text2img_fast\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class PipelineTesterMixin(unittest.TestCase):",
            "image_slice = image[0, -1, -3:, -3:].cpu()",
            "",
            "assert image.shape == (1, 3, 256, 256)",
            "-        expected_slice = torch.rensor([0.3163, 0.8670, 0.6465, 0.1865, 0.6291, 0.5139, 0.2824, 0.3723, 0.4344])",
            "+        expected_slice = torch.tensor([0.3163, 0.8670, 0.6465, 0.1865, 0.6291, 0.5139, 0.2824, 0.3723, 0.4344])",
            "assert (image_slice.flatten() - expected_slice).abs().max() < 1e-2",
            "",
            "@slow"
        ]
    },
    {
        "number": 8761,
        "comments": "",
        "commit_message": "Quick fix :) (#606)\n\n* Changing the name\n\n* style + quality\n\n* update doc and logo\n\n* clean up\n\n* circle-CI on the branche for now\n\n* fix daily dialog dataset\n\n* fix urls\n\nCo-authored-by: Quentin Lhoest <lhoest.q@gmail.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Reddit(nlp.GeneratorBasedBuilder):",
            "\"\"\"Returns SplitGenerators.\"\"\"",
            "dl_path = dl_manager.download_and_extract(_URL)",
            "return [",
            "-            nlp.SplitGenerator(",
            "-                name=nlp.Split.TRAIN,",
            "+            datasets.SplitGenerator(",
            "+                name=datasets.Split.TRAIN,",
            "gen_kwargs={\"path\": os.path.join(dl_path, \"corpus-webis-tldr-17.json\")},",
            ")",
            "]"
        ]
    },
    {
        "number": 8762,
        "comments": "",
        "commit_message": "fix unpooling with none shape\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def UnPooling2x2ZeroFilled(x):",
            "out_size = [-1, sh[1] * 2, sh[2] * 2, sh[3]]",
            "return tf.reshape(out, out_size)",
            "else:",
            "-        sh = tf.shape(x)",
            "-        ret = tf.reshape(out, tf.pack([-1, sh[1] * 2, sh[2] * 2, sh[3]]))",
            "+        shv = tf.shape(x)",
            "+        ret = tf.reshape(out, tf.pack([-1, shv[1] * 2, shv[2] * 2, sh[3]]))",
            "ret.set_shape([None, None, None, sh[3]])",
            "return ret"
        ]
    },
    {
        "number": 8763,
        "comments": "",
        "commit_message": "Ast refactor (#4941)\n\n* Ast refactor that includes:\n* property support + action for properties\n* static methods\n* ast refactor\n* globals support\n* settable properties\n\n* removing props\n\n* removing protobuf\n\n* protobug changes\n\n* reverting protobuf\n\n* protoc changes\n\n* Some fixes\n\n* Added sy_serialize back for situations where serialize is taken\n\n* Add copy method back\n\n* Restored change to send method which requires tags, description\n\n* More fixes\n\n* Lib ast fix\n\n* Type hint and tests fix\n\nSigned-off-by: George-Cristian Muraru <murarugeorgec@gmail.com>\n\n* Add util lost file\n\n* Remove type hint\n\n* docs\n\n* removing protobuf\n\n* proto removal\n\n* mypy fixes\n\n* pytest hack\n\n* mypy fix\n\n* skipping duet test\n\n* fixing mypy err\n\n* Disable python 3.9 tests because torchcsprng==0.1.4 not available yet\n\n* Linting and small fixes\n\n- Adding missing break to loopback=True _server_exchange\n- Renamed exception variable in logging to prevent shadowing\n- Added if not None check to Metaclass to satisfy linter\n- Replaced Logger code with template and added print=True kwarg\n- isort and pre_commit.sh checks\n\n* Reverting back to python 3.6 compatible logging code\n\nCo-authored-by: George Muraru <murarugeorgec@gmail.com>\nCo-authored-by: Madhava Jay <me@madhavajay.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_path_cache() -> None:",
            "",
            "refs: TypeList[TypeType] = []",
            "for path in conv2d_paths:",
            "-        klass = sy.lib_ast(path, return_callable=True, obj_type=th.nn.Conv2d)",
            "+        klass = sy.lib_ast.query(path, obj_type=th.nn.Conv2d)",
            "assert klass == sy.lib_ast.torch.nn.Conv2d",
            "assert klass.name == \"Conv2d\"",
            "assert klass.path_and_name == short_fqn"
        ]
    },
    {
        "number": 8765,
        "comments": "",
        "commit_message": "fix bn performance\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class SyncMultiGPUTrainer(MultiGPUTrainer,",
            "super(SyncMultiGPUTrainer, self)._setup()",
            "grad_list = MultiGPUTrainer._multi_tower_grads(",
            "self.config.tower, lambda: self._get_cost_and_grad()[1])",
            "+",
            "+        # debug tower performance:",
            "+        #ops = [k[0] for k in grad_list[1]] + [k[0] for k in grad_list[0]]",
            "+        #self.train_op = tf.group(*ops)",
            "+        #return",
            "+",
            "grads = SyncMultiGPUTrainer._average_grads(grad_list)",
            "grads = apply_grad_processors(grads, self.model.get_gradient_processor())"
        ]
    },
    {
        "number": 8766,
        "comments": "",
        "commit_message": "[Datasets] Autodetect dataset parallelism based on available resources and data size (#25883)\n\nThis PR defaults the parallelism of Dataset reads to `-1`. The parallelism is determined according to the following rule in this case:\n- The number of available CPUs is estimated. If in a placement group, the number of CPUs in the cluster is scaled by the size of the placement group compared to the cluster size. If not in a placement group, this is the number of CPUs in the cluster. If the estimated CPUs is less than 8, it is set to 8.\n- The parallelism is set to the estimated number of CPUs multiplied by 2.\n- The in-memory data size is estimated. If the parallelism would create in-memory blocks larger than the target block size (512MiB), the parallelism is increased until the blocks are < 512MiB in size.\n\nThese rules fix two common user problems:\n1. Insufficient parallelism in a large cluster, or too much parallelism on a small cluster.\n2. Overly large block sizes leading to OOMs when processing a single block.\n\nTODO:\n- [x] Unit tests\n- [x] Docs update\n\nSupercedes part of: https://github.com/ray-project/ray/pull/25708\n\nCo-authored-by: Ubuntu <ubuntu@ip-172-31-32-136.us-west-2.compute.internal>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_zip_arrow(ray_start_regular_shared):",
            "def test_batch_tensors(ray_start_regular_shared):",
            "import torch",
            "",
            "-    ds = ray.data.from_items([torch.tensor([0, 0]) for _ in range(40)])",
            "+    ds = ray.data.from_items([torch.tensor([0, 0]) for _ in range(40)], parallelism=40)",
            "res = \"Dataset(num_blocks=40, num_rows=40, schema=<class 'torch.Tensor'>)\"",
            "assert str(ds) == res, str(ds)",
            "with pytest.raises(pa.lib.ArrowInvalid):"
        ]
    },
    {
        "number": 8771,
        "comments": "",
        "commit_message": "TensorFlow Version 1 Compatibility Fix (#1538)\n\n* tf v1 compat fix\n\n* fix lint error for unused module\n",
        "label": "",
        "answer": "no",
        "change": [
            "class EvalCallback(Callback):",
            "self.dataflow = get_eval_dataflow(self._eval_dataset,",
            "shard=hvd.local_rank(), num_shards=hvd.local_size())",
            "",
            "-            self.barrier = hvd.allreduce(tf.random_normal(shape=[1]))",
            "+            self.barrier = hvd.allreduce(tfv1.random_normal(shape=[1]))",
            "",
            "def _build_predictor(self, idx):",
            "return self.trainer.get_predictor(self._in_names, self._out_names, device=idx)"
        ]
    },
    {
        "number": 8774,
        "comments": "",
        "commit_message": "fixed conv2d\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def dense_layer(x, size, l2_regularization=0.0):",
            "",
            "def conv2d_layer(x, size, l2_regularization=0.0, window=3, stride=1):",
            "with tf.variable_scope('conv2d'):",
            "-        filters = tf.Variable(initial_value=tf.random_normal(shape=(window, window, x.get_shape()[2].value, size), stddev=sqrt(2.0 / size))),",
            "+        filters = tf.Variable(initial_value=tf.random_normal(shape=(window, window, x.get_shape()[3].value, size), stddev=sqrt(2.0 / size)))",
            "+",
            "if l2_regularization > 0.0:",
            "tf.losses.add_loss(l2_regularization * tf.nn.l2_loss(t=filters))"
        ]
    },
    {
        "number": 8779,
        "comments": "",
        "commit_message": "upgrade to pytorch 0.4.0 (#1126)\n\n* bump pytorch to 0.4 + fix sanitize\n\n* remove check for Variable in block_orthogonal\n\n* rename parameter split_size=\n\n* remove checks for Variable\n\n* fix more tests\n\n* fixes\n\n* get tests to pass\n\n* fix warnings\n\n* get rid of some of the Variables\n\n* more tests passing\n\n* more elimination of variables\n\n* finish removing all Variables\n\n* pylint and such\n\n* a few fixes\n\n* move torch.no_grad into model.forward_on_instances\n\n* more pytorch 0.4 changes\n\n* detach() -> data\n\n* pylint\n\n* fix bad tensor creation\n\n* fix types\n\n* remove print statement\n\n* more 0.4 goodness\n\n* factor out is_tensor\n\n* add no_grad to elmo command\n\n* cleanup\n\n* remove TODO\n\n* address PR feedback\n\n* replace all() with item()\n\n* more cleanup\n\n* further cleanup\n\n* remove Variable\n\n* really fix merge conflict\n\n* fix pylint\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestMultiHeadedSimilarityFunction(AllenNlpTestCase):",
            "similarity = MultiHeadedSimilarity(num_heads=3, tensor_1_dim=6)",
            "similarity._tensor_1_projection = Parameter(torch.eye(6))",
            "similarity._tensor_2_projection = Parameter(torch.eye(6))",
            "-        a_vectors = Variable(torch.FloatTensor([[[[1, 1, -1, -1, 0, 1], [-2, 5, 9, -1, 3, 4]]]]))",
            "-        b_vectors = Variable(torch.FloatTensor([[[[1, 1, 1, 0, 2, 5], [0, 1, -1, -7, 1, 2]]]]))",
            "+        a_vectors = torch.FloatTensor([[[[1, 1, -1, -1, 0, 1], [-2, 5, 9, -1, 3, 4]]]])",
            "+        b_vectors = torch.FloatTensor([[[[1, 1, 1, 0, 2, 5], [0, 1, -1, -7, 1, 2]]]])",
            "result = similarity(a_vectors, b_vectors).data.numpy()",
            "assert result.shape == (1, 1, 2, 3)",
            "assert_almost_equal(result, [[[[2, -1, 5], [5, -2, 11]]]])"
        ]
    },
    {
        "number": 8784,
        "comments": "",
        "commit_message": "bug fix\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def text_to_seqvec(text, CONFIG, use_cuda):",
            "def numpy_to_torch(np_array, dtype, cuda=False):",
            "if np_array is None:",
            "return None",
            "-    tensor = torch.Tensor(np_array, dtype=dtype)",
            "+    tensor = torch.as_tensor(np_array, dtype=dtype)",
            "if cuda:",
            "return tensor.cuda()",
            "return tensor"
        ]
    },
    {
        "number": 8787,
        "comments": "",
        "commit_message": "[RLlib] Tf2x preparation; part 2 (upgrading `try_import_tf()`). (#9136)\n\n* WIP.\n\n* Fixes.\n\n* LINT.\n\n* WIP.\n\n* WIP.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* WIP.\n\n* Fixes.\n\n* Test\n\n* Fix.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* LINT.\n",
        "label": "",
        "answer": "no",
        "change": [
            "class DeprecatedCustomLossModelV1(Model):",
            "",
            "def _build_layers_v2(self, input_dict, num_outputs, options):",
            "self.obs_in = input_dict[\"obs\"]",
            "-        with tf.variable_scope(\"shared\", reuse=tf.AUTO_REUSE):",
            "+        with tf1.variable_scope(\"shared\", reuse=tf1.AUTO_REUSE):",
            "self.fcnet = FullyConnectedNetwork(input_dict, self.obs_space,",
            "self.action_space, num_outputs,",
            "options)"
        ]
    },
    {
        "number": 8789,
        "comments": "",
        "commit_message": "fix tests for PyTorch 1.10\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_pdn_conv():",
            "",
            "t = '(Tensor, SparseTensor, OptTensor) -> Tensor'",
            "jit = torch.jit.script(conv.jittable(t))",
            "-    assert torch.allclose(jit(x, adj.t()), out)",
            "+    assert torch.allclose(jit(x, adj.t()), out, atol=1e-6)",
            "",
            "",
            "def test_pdn_conv_with_sparse_node_input_feature():"
        ]
    },
    {
        "number": 8790,
        "comments": "",
        "commit_message": "fix many pylint issues.\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class SpatialTransformer2dAffineLayer(Layer):",
            "else:",
            "from tensorflow.python.ops import array_ops",
            "batch_size = array_ops.shape(self.inputs)[0]",
            "-            size = self.inputs.get_shape().as_list()",
            "+",
            "n_channels = self.inputs.get_shape().as_list()[-1]",
            "# logging.info(self.outputs)",
            "self.outputs = tf.reshape(self.outputs, shape=[batch_size, out_size[0], out_size[1], n_channels])"
        ]
    },
    {
        "number": 8791,
        "comments": "",
        "commit_message": "fix little bug, the correct tf.concat api first parameter is dim, not values\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def reshape(incoming, new_shape, name=\"Reshape\"):",
            "with tf.name_scope(name) as scope:",
            "inference = incoming",
            "if isinstance(inference, list):",
            "-            inference = tf.concat(inference, 0)",
            "+            inference = tf.concat(0, inference)",
            "inference = tf.cast(inference, tf.float32)",
            "inference = tf.reshape(inference, shape=new_shape)"
        ]
    },
    {
        "number": 8794,
        "comments": "",
        "commit_message": "Fix when _stable_1d_sort to work when n >= N (#6177)\n\n* Fix when _stable_1d_sort to work when n >= N\n\n* Apply suggestions\n\nCo-authored-by: Carlos Mocholi <carlossmocholi@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestAUC(MetricTester):",
            "])",
            "def test_auc(x, y, expected):",
            "# Test Area Under Curve (AUC) computation",
            "-    assert auc(torch.tensor(x), torch.tensor(y)) == expected",
            "+    assert auc(torch.tensor(x), torch.tensor(y), reorder=True) == expected"
        ]
    },
    {
        "number": 8800,
        "comments": "",
        "commit_message": "Replace swish with silu (#8166)\n\n* Replace swish with silu\n\n* revert nn.silu to nn.swish due to older version\n\n* simplify optimized silu conditional and fix format\n\n* Update activations.py\n\n* Update activations_tf.py\n\n* Update modeling_flax_utils.py\n\n* Update modeling_openai.py\n\n* add swish testcase\n\n* add pytorch swish testcase\n\n* Add more robust python version check\n\n* more formatting fixes\n\nCo-authored-by: TFUsers <TFUsers@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "ACT2FN = {",
            "\"gelu\": tf.keras.layers.Activation(gelu),",
            "\"relu\": tf.keras.activations.relu,",
            "\"swish\": tf.keras.activations.swish,",
            "+    \"silu\": tf.keras.activations.swish,",
            "\"gelu_new\": tf.keras.layers.Activation(gelu_new),",
            "\"mish\": tf.keras.layers.Activation(mish),",
            "\"tanh\": tf.keras.activations.tanh,"
        ]
    },
    {
        "number": 8801,
        "comments": "",
        "commit_message": "Fix typo when checking nans\n",
        "label": "",
        "answer": "no",
        "change": [
            "def train_model(",
            "tag = tag.replace('/', '.')",
            "if not (torch.isinf(value) | torch.isnan(value)).any():",
            "histograms['Weights/' + tag] = wandb.Histogram(value.data.cpu())",
            "-                            if not (torch.isinf(value.grad) | torch.isnan(value)).any():",
            "+                            if not (torch.isinf(value.grad) | torch.isnan(value.grad)).any():",
            "histograms['Gradients/' + tag] = wandb.Histogram(value.grad.data.cpu())",
            "",
            "val_score = evaluate(model, val_loader, device, amp)"
        ]
    },
    {
        "number": 8803,
        "comments": "",
        "commit_message": "fixed TF seed problem, improved seed unittest\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Agent(object):",
            "assert isinstance(seed, int)",
            "random.seed(a=seed)",
            "np.random.seed(seed=seed)",
            "-            tf.random.set_random_seed(seed=seed)",
            "",
            "# States/actions specification",
            "self.states_spec = util.valid_values_spec("
        ]
    },
    {
        "number": 8804,
        "comments": "",
        "commit_message": "Llff & blender convention fix\n\nSummary: Images were coming out in the wrong format.\n\nReviewed By: shapovalov\n\nDifferential Revision: D37291278\n\nfbshipit-source-id: c10871c37dd186982e7abf2071ac66ed583df2e6\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class LlffDatasetMapProvider(SingleSceneDatasetMapProviderBase):",
            "i_split = (i_train, i_test, i_test)",
            "H, W, focal = hwf",
            "H, W = int(H), int(W)",
            "-        images = torch.from_numpy(images)",
            "+        images = torch.from_numpy(images).permute(0, 3, 1, 2)",
            "poses = torch.from_numpy(poses)",
            "",
            "# pyre-ignore[16]"
        ]
    },
    {
        "number": 8805,
        "comments": "",
        "commit_message": "[Testing] Fix LINT/sphinx errors. (#8874)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "torch, nn = try_import_torch()",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "-class FullyConnectedNetwork(TorchModelV2):",
            "+class FullyConnectedNetwork(TorchModelV2, nn.Module):",
            "\"\"\"Generic fully connected network.\"\"\"",
            "",
            "def __init__(self, obs_space, action_space, num_outputs, model_config,",
            "name):",
            "TorchModelV2.__init__(self, obs_space, action_space, num_outputs,",
            "model_config, name)",
            "+        nn.Module.__init__(self)",
            "",
            "activation = get_activation_fn(",
            "model_config.get(\"fcnet_activation\"), framework=\"torch\")"
        ]
    },
    {
        "number": 8810,
        "comments": "",
        "commit_message": "type hints and small fixes in conversions\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def slogdet(",
            "/,",
            ") -> NamedTuple:",
            "results = NamedTuple(",
            "-        \"slogdet\",",
            "-        [(\"sign\", torch.Tensor), (\"logabsdet\", torch.Tensor)]",
            "+        \"slogdet\", [(\"sign\", torch.Tensor), (\"logabsdet\", torch.Tensor)]",
            ")",
            "sign, logabsdet = torch.linalg.slogdet(x)",
            "return results(sign, logabsdet)"
        ]
    },
    {
        "number": 8814,
        "comments": "",
        "commit_message": "fix linear layer into tag space predictions\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class ViterbiDecoder:",
            "self.start_tag = tag_dictionary.get_idx_for_item(START_TAG)",
            "self.stop_tag = tag_dictionary.get_idx_for_item(STOP_TAG)",
            "",
            "-    def decode(self, features: torch.Tensor, lengths: torch.Tensor) -> List:",
            "+    def decode(self, features_tuple: tuple) -> List:",
            "\"\"\"",
            "Decoding function returning the most likely sequence of tags.",
            ":param features: CRF scores from CRF forward method in shape (batch size, seq len, tagset size, tagset size)",
            ":param lengths: lengths tuple containing sorted lengths and indices from unsorted list",
            ":return: decoded sequences",
            "\"\"\"",
            "+        features, lengths = features_tuple",
            "+",
            "tags = []",
            "batch_size = features.size(0)",
            "seq_len = features.size(1)"
        ]
    },
    {
        "number": 8815,
        "comments": "",
        "commit_message": "Optiimzation (#355)\n\n* constant folding into kernels\n\n* that opt worth it?\n\n* fix mypy\n\n* ast one kernel\n\n* save 2 lines in conv kernel\n\n* debug print kernel count\n\n* cl debugging\n\n* early realize inputs\n\n* refactor Device\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TorchBuffer(torch.Tensor):",
            "def custompad(x, padding): return torch.nn.functional.pad(x, [item for sublist in padding[::-1] for item in sublist])",
            "",
            "@staticmethod",
            "-  def fromCPU(data):",
            "-    return TorchBuffer(torch.from_numpy(data).requires_grad_(False)).to(device)",
            "-  def toCPU(x):",
            "-    return x.cpu().numpy()",
            "+  def fromCPU(data): return TorchBuffer(torch.from_numpy(data).requires_grad_(False)).to(device)",
            "+  def toCPU(x): return x.cpu().numpy()",
            "",
            "unary_op, binary_op, reduce_op, movement_op = CPUBuffer.unary_op, CPUBuffer.binary_op, CPUBuffer.reduce_op, CPUBuffer.movement_op"
        ]
    },
    {
        "number": 8823,
        "comments": "",
        "commit_message": "[FasterRCNN] remove unused arg; fix bug in decode bbox\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def decode_bbox_target(box_predictions, anchors):",
            "xbyb = box_pred_txty * waha + xaya",
            "x1y1 = xbyb - wbhb * 0.5",
            "x2y2 = xbyb + wbhb * 0.5    # (...)x1x2",
            "-    out = tf.concat([x1y1, x2y2], axis=1)",
            "+    out = tf.concat([x1y1, x2y2], axis=-2)",
            "return tf.reshape(out, orig_shape)"
        ]
    },
    {
        "number": 8824,
        "comments": "",
        "commit_message": "upgrade detection / some v2 compat fix\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def _replace_global_by_local(kwargs):",
            "if 'collections' in kwargs:",
            "collections = kwargs['collections']",
            "if not collections:",
            "-        collections = {tf.GraphKeys.GLOBAL_VARIABLES}",
            "+        collections = {tfv1.GraphKeys.GLOBAL_VARIABLES}",
            "else:",
            "collections = set(collections.copy())",
            "-    collections.remove(tf.GraphKeys.GLOBAL_VARIABLES)",
            "-    collections.add(tf.GraphKeys.LOCAL_VARIABLES)",
            "+    collections.remove(tfv1.GraphKeys.GLOBAL_VARIABLES)",
            "+    collections.add(tfv1.GraphKeys.LOCAL_VARIABLES)",
            "kwargs['collections'] = list(collections)"
        ]
    }
]