[
    {
        "commit_hash": "5b8b1c9419fca922ae4e670d1d6c798f833f0268",
        "index": "ef11f99..0286aab 100644",
        "commit_message": "Time Series Forecaster task (#1050)\n\n* Initial Commit for final Time Series Forecaster task\n\n* Code for Time Series Forecaster task\n\n* Completed changes for Time Series forecaster task\n\n* Fix for failing tests\n\n* Fix for output shape test for TimeseriesForecaster\n\n* Fix for test of fit and predict timeseriesforecaster\n\n* Moved StructuredData Functionality to a Mixin\n\n* Fixed Typo\n\n* Fixed typo in SupervisedStructuredDatapipeline\n",
        "file": "autokeras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "models",
        "change": [
            "class AutoModel(object):",
            "if isinstance(input_node, input_module.StructuredDataInput):",
            "middle_nodes.append(hypermodels.StructuredDataBlock()(input_node))",
            "if isinstance(input_node, input_module.TimeseriesInput):",
            "-                middle_nodes.append(hypermodels.TimeSeriesBlock()(input_node))",
            "+                middle_nodes.append(hypermodels.TimeseriesBlock()(input_node))",
            "",
            "# Merge the middle nodes.",
            "if len(middle_nodes) > 1:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=TimeSeriesBlock), value='TimeseriesBlock')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 5775,
        "neg_line": [
            "-middle_nodes.append(hypermodels.TimeSeriesBlock()(input_node))"
        ],
        "pos_line": [
            "+middle_nodes.append(hypermodels.TimeseriesBlock()(input_node))"
        ],
        "core_change": "-middle_nodes.append(hypermodels.TimeSeriesBlock()(input_node)) +middle_nodes.append(hypermodels.TimeseriesBlock()(input_node))",
        "core_API": "append"
    },
    {
        "commit_hash": "1a57d314195ab40517f4d6835e71e34bece05fa6",
        "index": "3a2b4ca2..1ec7d2ae 100644",
        "commit_message": "Numerically stabilize ProjectedNormal.log_prob() via erfc (#3071)\n\n* Numerically stabilize ProjectedNormal.log_prob() via logaddexp\n\n* Fix conceptual error, now this NANs\n\n* Switch to erfc\n\n* Strengthen tests\n\n* lint\n\n* Strengthen test, clamp harder\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_gof(continuous_dist):",
            "num_samples = 50000",
            "for i in range(continuous_dist.get_num_test_data()):",
            "d = Dist(**continuous_dist.get_dist_params(i))",
            "-        samples = d.sample(torch.Size([num_samples]))",
            "+        with torch.random.fork_rng():",
            "+            samples = d.sample(torch.Size([num_samples]))",
            "with xfail_if_not_implemented():",
            "probs = d.log_prob(samples).exp()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('with_statement', None), position=4, insert_id=671594)",
            "Insert(target_node=IN(type=with_statement), node=('with', 'with'), position=0, insert_id=671595)",
            "Insert(target_node=IN(type=with_statement), node=('with_clause', None), position=1, insert_id=671596)",
            "Insert(target_node=IN(type=with_statement), node=(':', ':'), position=2, insert_id=671597)",
            "Insert(target_node=IN(type=with_statement), node=('block', None), position=3, insert_id=671598)",
            "Insert(target_node=IN(type=with_clause), node=('with_item', None), position=0, insert_id=671599)",
            "Move(target_node=IN(type=block), node=ASTNode(type=expression_statement), position=0)",
            "Insert(target_node=IN(type=with_item), node=('call', None), position=0, insert_id=671600)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=671601)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=671602)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=671603)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=671604)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'fork_rng'), position=2, insert_id=671605)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=671606)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=671607)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=671608)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=671609)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'random'), position=2, insert_id=671610)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 18,
        "number": 5781,
        "neg_line": [
            "-samples = d.sample(torch.Size([num_samples]))"
        ],
        "pos_line": [
            "+with torch.random.fork_rng():",
            "+samples = d.sample(torch.Size([num_samples]))"
        ],
        "core_change": "-samples = d.sample(torch.Size([num_samples])) +with torch.random.fork_rng(): +samples = d.sample(torch.Size([num_samples]))",
        "core_API": "get_num_test_data"
    },
    {
        "commit_hash": "864e54602c9cd8c1c1ebfca288647a5db58dcfa1",
        "index": "5fb195b32..e01c8c606 100644",
        "commit_message": "fixed test\n\n",
        "file": "PySyft.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestChainTensor(TestCase):",
            "x.get()",
            "x.child = x.child.child",
            "",
            "-        target = sy._PlusIsMinusTensor().on(torch.FloatTensor([1, 1]))",
            "+        # target = sy._PlusIsMinusTensor().on(torch.FloatTensor([1, 1]))",
            "+        target = torch.FloatTensor([1, 1])",
            "assert torch.equal(x.grad.data, target)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=IN(type=root), node=ASTNode(type=call), position=0)",
            "Delete(target_node=ASTNode(type=identifier, text=sy))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=_PlusIsMinusTensor))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=on))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 15,
        "number": 5782,
        "neg_line": [
            "-target = sy._PlusIsMinusTensor().on(torch.FloatTensor([1, 1]))"
        ],
        "pos_line": [
            "+# target = sy._PlusIsMinusTensor().on(torch.FloatTensor([1, 1]))",
            "+target = torch.FloatTensor([1, 1])"
        ],
        "core_change": "-target = sy._PlusIsMinusTensor().on(torch.FloatTensor([1, 1])) +# target = sy._PlusIsMinusTensor().on(torch.FloatTensor([1, 1])) +target = torch.FloatTensor([1, 1])",
        "core_API": "get"
    },
    {
        "commit_hash": "dc05cf05b9e6b492e7a408a4e9358f206f3263f5",
        "index": "e272392be0..6a71048984 100644",
        "commit_message": "fix syntax error\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_torch_leaky_relu(",
            "num_positional_args,",
            "as_variable,",
            "with_out,",
            "-    native_array",
            "+    native_array,",
            "fw,",
            "alpha,",
            "):",
            "input_dtype, x = dtype_and_x",
            "-",
            "helpers.test_frontend_function(",
            "input_dtypes=input_dtype,",
            "as_variable_flags=as_variable,"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=ASTNode(type=,, text=,), node=ASTNode(type=parameters), position=8)",
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=2, insert_id=330836)",
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=9, insert_id=330837)",
            "Move(target_node=ASTNode(type=parameters), node=ASTNode(type=identifier, text=fw), position=10)",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 5783,
        "neg_line": [
            "-native_array",
            "-"
        ],
        "pos_line": [
            "+native_array,"
        ],
        "core_change": "-native_array +native_array, -",
        "core_API": "test_frontend_function"
    },
    {
        "commit_hash": "56a3d0592fe2c2027ddefd940105abaa57f257f8",
        "index": "7e43bf1b..67536514 100755",
        "commit_message": "Fix TRPO natural-gradient/linesearch optimizer bug\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class NaturalGradient(Optimizer):",
            "#     tf.math.reduce_sum(input_tensor=(loss_grad * delta))",
            "#     for loss_grad, delta in zip(loss_gradients, estimated_deltas.values())",
            "# ])",
            "-                return estimated_deltas.fmap(function=tf_util.identity)",
            "+                return [tf_util.identity(input=delta) for delta in estimated_deltas.values()]",
            "",
            "if self.only_positive_updates:",
            "# Natural gradient step only works if constant > 0 (epsilon to avoid zero division)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=return_statement), node=('list_comprehension', None), position=1, insert_id=2221941)",
            "Insert(target_node=IN(type=list_comprehension), node=('[', '['), position=0, insert_id=2221942)",
            "Move(target_node=IN(type=list_comprehension), node=ASTNode(type=call), position=1)",
            "Insert(target_node=IN(type=list_comprehension), node=('for_in_clause', None), position=2, insert_id=2221943)",
            "Insert(target_node=IN(type=list_comprehension), node=(']', ']'), position=3, insert_id=2221944)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=for_in_clause), node=('for', 'for'), position=0, insert_id=2221945)",
            "Insert(target_node=IN(type=for_in_clause), node=('identifier', 'delta'), position=1, insert_id=2221946)",
            "Insert(target_node=IN(type=for_in_clause), node=('in', 'in'), position=2, insert_id=2221947)",
            "Insert(target_node=IN(type=for_in_clause), node=('call', None), position=3, insert_id=2221948)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2221949)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2221950)",
            "Update(target_node=ASTNode(type=identifier, text=function), value='input')",
            "Insert(target_node=ASTNode(type=keyword_argument), node=('identifier', 'delta'), position=2, insert_id=2221951)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'estimated_deltas'), position=0, insert_id=2221952)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2221953)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'values'), position=2, insert_id=2221954)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2221955)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=2221956)",
            "Delete(target_node=ASTNode(type=identifier, text=estimated_deltas))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=fmap))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 23,
        "number": 5785,
        "neg_line": [
            "-return estimated_deltas.fmap(function=tf_util.identity)"
        ],
        "pos_line": [
            "+return [tf_util.identity(input=delta) for delta in estimated_deltas.values()]"
        ],
        "core_change": "-return estimated_deltas.fmap(function=tf_util.identity) +return [tf_util.identity(input=delta) for delta in estimated_deltas.values()]",
        "core_API": "reduce_sum"
    },
    {
        "commit_hash": "98d47ff27df24cae9744912959e5b6f150337650",
        "index": "41058107..59f095aa 100644",
        "commit_message": "TF 2.0 compatibility, fixed pretrain issue\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TrackableNestedDict(NestedDict, tf.python.training.tracking.tracking.AutoT",
            "super().__setattr__(name, value)",
            "",
            "def __setitem__(self, key, value):",
            "-        value = tf.python.training.tracking.data_structures.sticky_attribute_assignment(",
            "-            trackable=self, value=value, name=key",
            "-        )",
            "+        value = sticky_attribute_assignment(trackable=self, value=value, name=key)",
            "super().__setitem__(key, value)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=sticky_attribute_assignment), position=0)",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=python))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=training))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=tracking))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=data_structures))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 16,
        "number": 5786,
        "neg_line": [
            "-value = tf.python.training.tracking.data_structures.sticky_attribute_assignment(",
            "-trackable=self, value=value, name=key",
            "-)"
        ],
        "pos_line": [
            "+value = sticky_attribute_assignment(trackable=self, value=value, name=key)"
        ],
        "core_change": "-value = tf.python.training.tracking.data_structures.sticky_attribute_assignment( -trackable=self, value=value, name=key -) +value = sticky_attribute_assignment(trackable=self, value=value, name=key)",
        "core_API": "sticky_attribute_assignment"
    },
    {
        "commit_hash": "a8dff34d14822637ec7048385c24358ddebbb9b9",
        "index": "4820eb392c..52023b180b 100644",
        "commit_message": "try to fix nanmean by isolate the casting of input dtype out for nanmean of torch backend\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def nanmean(",
            "dtype: Optional[torch.dtype] = None,",
            "out: Optional[torch.Tensor] = None,",
            ") -> torch.Tensor:",
            "-    return torch.nanmean(a, dim=axis, keepdim=keepdims, dtype=dtype, out=out)",
            "+    input = a.to(dtype)",
            "+    return torch.nanmean(input, dim=axis, keepdim=keepdims, out=out)",
            "",
            "",
            "nanmean.support_native_out = True"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=block), node=('expression_statement', None), position=0, insert_id=265391)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=265392)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'input'), position=0, insert_id=265393)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=265394)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=265395)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=265396)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=265397)",
            "Update(target_node=ASTNode(type=identifier, text=a), value='input')",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'a'), position=0, insert_id=265398)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=265399)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to'), position=2, insert_id=265400)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=265401)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'dtype'), position=1, insert_id=265402)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=265403)",
            "Delete(target_node=ASTNode(type=identifier, text=dtype))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=dtype))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 19,
        "number": 5787,
        "neg_line": [
            "-return torch.nanmean(a, dim=axis, keepdim=keepdims, dtype=dtype, out=out)"
        ],
        "pos_line": [
            "+input = a.to(dtype)",
            "+return torch.nanmean(input, dim=axis, keepdim=keepdims, out=out)"
        ],
        "core_change": "-return torch.nanmean(a, dim=axis, keepdim=keepdims, dtype=dtype, out=out) +input = a.to(dtype) +return torch.nanmean(input, dim=axis, keepdim=keepdims, out=out)",
        "core_API": "nanmean"
    },
    {
        "commit_hash": "129350f37c96be5717d9c195b701da57692fd24e",
        "index": "98b24731..2ea67c7d 100644",
        "commit_message": "fixed problem with multi-state/action networks and scoping\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Beta(Distribution):",
            "self.max_value = max_value",
            "action_size = util.prod(self.shape)",
            "",
            "-        with tf.name_scope(name=scope):",
            "-            self.alpha = Linear(size=action_size, bias=alpha, scope='alpha')",
            "-            self.beta = Linear(size=action_size, bias=beta, scope='beta')",
            "+        self.alpha = Linear(size=action_size, bias=alpha, scope='alpha')",
            "+        self.beta = Linear(size=action_size, bias=beta, scope='beta')",
            "",
            "super(Beta, self).__init__(scope, summary_labels)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=module), node=ASTNode(type=expression_statement), position=3)",
            "Move(target_node=ASTNode(type=module), node=ASTNode(type=expression_statement), position=4)",
            "Delete(target_node=ASTNode(type=with, text=with))",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=name_scope))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=name))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=scope))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=with_item))",
            "Delete(target_node=ASTNode(type=with_clause))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=block))",
            "Delete(target_node=ASTNode(type=with_statement))"
        ],
        "plus_line": 2,
        "minus_line": 3,
        "AST_diff_line": 20,
        "number": 5789,
        "neg_line": [
            "-with tf.name_scope(name=scope):",
            "-self.alpha = Linear(size=action_size, bias=alpha, scope='alpha')",
            "-self.beta = Linear(size=action_size, bias=beta, scope='beta')"
        ],
        "pos_line": [
            "+self.alpha = Linear(size=action_size, bias=alpha, scope='alpha')",
            "+self.beta = Linear(size=action_size, bias=beta, scope='beta')"
        ],
        "core_change": "-with tf.name_scope(name=scope): -self.alpha = Linear(size=action_size, bias=alpha, scope='alpha') -self.beta = Linear(size=action_size, bias=beta, scope='beta') +self.alpha = Linear(size=action_size, bias=alpha, scope='alpha') +self.beta = Linear(size=action_size, bias=beta, scope='beta')",
        "core_API": "prod"
    },
    {
        "commit_hash": "45a5b28774f4eba2f29cb1ba6a853c6402d1fa67",
        "index": "71e94794a..aa07ffca5 100644",
        "commit_message": "fixed flake8\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def get_layer(l_name, library=torch.nn):",
            "",
            "Returns:",
            "layer_handler (object): handler for the requested layer e.g. (torch.nn.ELU)",
            "+",
            "\"\"\"",
            "+",
            "all_torch_layers = [x for x in dir(torch.nn)]",
            "match = [x for x in all_torch_layers if l_name.lower() == x.lower()]",
            "if len(match) == 0:"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 0,
        "minus_line": 0,
        "AST_diff_line": 17,
        "number": 5795,
        "neg_line": [],
        "pos_line": [
            "+",
            "+"
        ],
        "core_change": "+ +",
        "core_API": "lower"
    },
    {
        "commit_hash": "2e7e4280aa6f380a4e3afad6524295a17901c56c",
        "index": "4f202800b..509d7250b 100755",
        "commit_message": "Traced models serialization and torchscripting fix (#17206)\n\n* Fix torch.jit.script and pickling issues\n\n* Fix get_attr issues\n\n* Fix import in function\n\n* Fix GPT-J and T5 tracing for torch=1.11\n\n* Gate graph surgery on torch version\n\n* Modeling minor changes to enable TorchScripting\n\n* Model serialization / deserialization test\n\n* Remove _assert_is_none users\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DecisionTransformerGPT2Attention(nn.Module):",
            "if not self.is_cross_attention:",
            "# if only \"normal\" attention layer implements causal mask",
            "query_length, key_length = query.size(-2), key.size(-2)",
            "-            causal_mask = self.bias[:, :, key_length - query_length : key_length, :key_length].bool()",
            "+            causal_mask = self.bias[:, :, key_length - query_length : key_length, :key_length].to(torch.bool)",
            "attn_weights = torch.where(causal_mask, attn_weights, self.masked_bias.to(attn_weights.dtype))",
            "",
            "if attention_mask is not None:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=bool), value='to')",
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=1, insert_id=1199115)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1199116)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1199117)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'bool'), position=2, insert_id=1199118)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 5799,
        "neg_line": [
            "-causal_mask = self.bias[:, :, key_length - query_length : key_length, :key_length].bool()"
        ],
        "pos_line": [
            "+causal_mask = self.bias[:, :, key_length - query_length : key_length, :key_length].to(torch.bool)"
        ],
        "core_change": "-causal_mask = self.bias[:, :, key_length - query_length : key_length, :key_length].bool() +causal_mask = self.bias[:, :, key_length - query_length : key_length, :key_length].to(torch.bool)",
        "core_API": "size"
    },
    {
        "commit_hash": "5e3c72842da2e2f660c93d7478752443b96848b9",
        "index": "7058de8de..74c923e1c 100644",
        "commit_message": "bugfix on model name\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "models",
        "change": [
            "from .modeling_tf_roberta import (",
            "logger = logging.getLogger(__name__)",
            "",
            "TF_CAMEMBERT_PRETRAINED_MODEL_ARCHIVE_MAP = {",
            "-    # \"camembert-base\": \"https://s3.amazonaws.com/models.huggingface.co/bert/camembert-base-tf_model.h5\"",
            "}"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 5800,
        "neg_line": [
            "-# \"camembert-base\": \"https://s3.amazonaws.com/models.huggingface.co/bert/camembert-base-tf_model.h5\""
        ],
        "pos_line": [],
        "core_change": "-# \"camembert-base\": \"https://s3.amazonaws.com/models.huggingface.co/bert/camembert-base-tf_model.h5\"",
        "core_API": "getLogger"
    },
    {
        "commit_hash": "7ea781c9a030fb2d49574d8d324e562e58dd49fb",
        "index": "0ad369d3b..23f525167 100644",
        "commit_message": "CI fix3\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SISNRLoss(TimeDomainLoss):",
            "# s_target = <s', s>s / ||s||^2",
            "pair_wise_dot = torch.sum(s_estimate * s_target, dim=1, keepdim=True)  # [B, 1]",
            "s_target_energy = (",
            "-            torch.sum(s_target**2, dim=1, keepdim=True) + self.eps",
            "+            torch.sum(s_target ** 2, dim=1, keepdim=True) + self.eps",
            ")  # [B, 1]",
            "pair_wise_proj = pair_wise_dot * s_target / s_target_energy  # [B, T]",
            "# e_noise = s' - s_target",
            "e_noise = s_estimate - pair_wise_proj  # [B, T]",
            "",
            "# SI-SNR = 10 * log_10(||s_target||^2 / ||e_noise||^2)",
            "-        pair_wise_si_snr = torch.sum(pair_wise_proj**2, dim=1) / (",
            "-            torch.sum(e_noise**2, dim=1) + self.eps",
            "+        pair_wise_si_snr = torch.sum(pair_wise_proj ** 2, dim=1) / (",
            "+            torch.sum(e_noise ** 2, dim=1) + self.eps",
            ")",
            "pair_wise_si_snr = 10 * torch.log10(pair_wise_si_snr + self.eps)  # [B]"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 17,
        "number": 5802,
        "neg_line": [
            "-torch.sum(s_target**2, dim=1, keepdim=True) + self.eps",
            "-pair_wise_si_snr = torch.sum(pair_wise_proj**2, dim=1) / (",
            "-torch.sum(e_noise**2, dim=1) + self.eps"
        ],
        "pos_line": [
            "+torch.sum(s_target ** 2, dim=1, keepdim=True) + self.eps",
            "+pair_wise_si_snr = torch.sum(pair_wise_proj ** 2, dim=1) / (",
            "+torch.sum(e_noise ** 2, dim=1) + self.eps"
        ],
        "core_change": "-torch.sum(s_target**2, dim=1, keepdim=True) + self.eps +torch.sum(s_target ** 2, dim=1, keepdim=True) + self.eps -pair_wise_si_snr = torch.sum(pair_wise_proj**2, dim=1) / ( -torch.sum(e_noise**2, dim=1) + self.eps +pair_wise_si_snr = torch.sum(pair_wise_proj ** 2, dim=1) / ( +torch.sum(e_noise ** 2, dim=1) + self.eps",
        "core_API": "sum"
    },
    {
        "commit_hash": "1417978cd49181fd08837e7722c34dd5c8c113e3",
        "index": "fdb589d70..46b382d7a 100644",
        "commit_message": "[SequenceFeatureExtractor] Rewrite padding logic from pure python to numpy (#13650)\n\n* Test np padding\n\n* Pass feature extraction tests\n\n* Update type hints\n\n* Fix flaky integration tests\n\n* Try a more stable waveform\n\n* Add to_numpy jax support\n\n* int32 attention masks\n\n* Refactor normalization tests\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class SequenceFeatureExtractionTestMixin(FeatureExtractionSavingTestMixin):",
            "input_np = feat_extract.pad(processed_features, padding=\"longest\", return_tensors=\"np\")[input_name]",
            "input_tf = feat_extract.pad(processed_features, padding=\"longest\", return_tensors=\"tf\")[input_name]",
            "",
            "-        self.assertTrue(abs(input_np.astype(np.float32).sum() - input_tf.numpy().sum()) < 1e-2)",
            "+        self.assertTrue(abs(input_np.astype(np.float32).sum() - input_tf.numpy().astype(np.float32).sum()) < 1e-2)",
            "",
            "def test_attention_mask(self):",
            "feat_dict = self.feat_extract_dict"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2369128)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=2369129)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2369130)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'astype'), position=2, insert_id=2369131)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2369132)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=2369133)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2369134)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'np'), position=0, insert_id=2369135)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2369136)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'float32'), position=2, insert_id=2369137)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 5803,
        "neg_line": [
            "-self.assertTrue(abs(input_np.astype(np.float32).sum() - input_tf.numpy().sum()) < 1e-2)"
        ],
        "pos_line": [
            "+self.assertTrue(abs(input_np.astype(np.float32).sum() - input_tf.numpy().astype(np.float32).sum()) < 1e-2)"
        ],
        "core_change": "-self.assertTrue(abs(input_np.astype(np.float32).sum() - input_tf.numpy().sum()) < 1e-2) +self.assertTrue(abs(input_np.astype(np.float32).sum() - input_tf.numpy().astype(np.float32).sum()) < 1e-2)",
        "core_API": "pad"
    },
    {
        "commit_hash": "41458065513f86774beb73d50eef61b95c9941f5",
        "index": "adaad0d074..68f894b1da 100644",
        "commit_message": "fixed NumPy and PyTorch implementations of iinfo.\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def finfo(type: Union[torch.dtype, str, torch.Tensor]) -> Finfo:",
            "",
            "",
            "def iinfo(type: Union[torch.dtype, str, torch.Tensor]) -> torch.iinfo:",
            "+    if isinstance(type, torch.Tensor):",
            "+        type = type.dtype",
            "return torch.iinfo(ivy.as_native_dtype(type))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('block', None), position=0, insert_id=350864)",
            "Insert(target_node=IN(type=block), node=('if_statement', None), position=0, insert_id=350865)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=350866)",
            "Insert(target_node=IN(type=if_statement), node=('call', None), position=1, insert_id=350867)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=350868)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=350869)",
            "Insert(target_node=IN(type=call), node=('identifier', 'isinstance'), position=0, insert_id=350870)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=350871)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=350872)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=350873)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'type'), position=1, insert_id=350874)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=350875)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=3, insert_id=350876)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=350877)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=350878)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=350879)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=350880)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Tensor'), position=2, insert_id=350881)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'type'), position=0, insert_id=350882)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=350883)",
            "Insert(target_node=IN(type=assignment), node=('attribute', None), position=2, insert_id=350884)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'type'), position=0, insert_id=350885)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=350886)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dtype'), position=2, insert_id=350887)",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 2,
        "minus_line": 0,
        "AST_diff_line": 25,
        "number": 5805,
        "neg_line": [],
        "pos_line": [
            "+if isinstance(type, torch.Tensor):",
            "+type = type.dtype"
        ],
        "core_change": "+if isinstance(type, torch.Tensor): +type = type.dtype",
        "core_API": "iinfo"
    },
    {
        "commit_hash": "2a5d05b5621be7cc1485564407f4617a3e45d50b",
        "index": "3f4ff354e..032e78731 100644",
        "commit_message": "Fix tpu spawn plugin test (#11131)\n\n\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class BoringModelTPU(BoringModel):",
            "@pl_multi_process_test",
            "def test_model_tpu_one_core():",
            "\"\"\"Tests if device/debug flag is set correctely when training and after teardown for TPUSpawnPlugin.\"\"\"",
            "-    trainer = Trainer(tpu_cores=1, fast_dev_run=True, plugin=TPUSpawnPlugin(debug=True))",
            "+    trainer = Trainer(tpu_cores=1, fast_dev_run=True, strategy=TPUSpawnPlugin(debug=True))",
            "# assert training type plugin attributes for device setting",
            "assert isinstance(trainer.training_type_plugin, TPUSpawnPlugin)",
            "assert not trainer.training_type_plugin.on_gpu",
            "assert trainer.training_type_plugin.on_tpu",
            "-    assert trainer.training_type_plugin.root_device == torch.device(\"xla\")",
            "+    assert trainer.training_type_plugin.root_device == torch.device(\"xla\", index=1)",
            "model = BoringModelTPU()",
            "trainer.fit(model)",
            "assert \"PT_XLA_DEBUG\" not in os.environ"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=516853)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=516854)",
            "Update(target_node=ASTNode(type=identifier, text=plugin), value='strategy')",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'index'), position=0, insert_id=516855)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=516856)",
            "Insert(target_node=IN(type=keyword_argument), node=('integer', '1'), position=2, insert_id=516857)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 6,
        "number": 5807,
        "neg_line": [
            "-trainer = Trainer(tpu_cores=1, fast_dev_run=True, plugin=TPUSpawnPlugin(debug=True))",
            "-assert trainer.training_type_plugin.root_device == torch.device(\"xla\")"
        ],
        "pos_line": [
            "+trainer = Trainer(tpu_cores=1, fast_dev_run=True, strategy=TPUSpawnPlugin(debug=True))",
            "+assert trainer.training_type_plugin.root_device == torch.device(\"xla\", index=1)"
        ],
        "core_change": "-trainer = Trainer(tpu_cores=1, fast_dev_run=True, plugin=TPUSpawnPlugin(debug=True)) +trainer = Trainer(tpu_cores=1, fast_dev_run=True, strategy=TPUSpawnPlugin(debug=True)) -assert trainer.training_type_plugin.root_device == torch.device(\"xla\") +assert trainer.training_type_plugin.root_device == torch.device(\"xla\", index=1)",
        "core_API": "device"
    },
    {
        "commit_hash": "923a842e9cf3a9e767ac7fe86c5962a3cba1a566",
        "index": "1248e1cd2..a559cb5f8 100644",
        "commit_message": "Fix import from torch.distributed when distributed not available (#16658)\n\n\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_import_pytorch_lightning_with_torch_dist_unavailable():",
            "code = dedent(",
            "\"\"\"",
            "import torch",
            "-        torch.distributed.is_available = lambda: False  # pretend torch.distributed not available",
            "+",
            "+        # pretend torch.distributed not available",
            "+        for name in list(torch.distributed.__dict__.keys()):",
            "+            if not name.startswith(\"__\"):",
            "+                delattr(torch.distributed, name)",
            "+",
            "+        torch.distributed.is_available = lambda: False",
            "+",
            "import lightning.pytorch",
            "\"\"\"",
            ")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"\"\"\nimport torch\n        torch.distributed.is_available = lambda: False  # pretend torch.distributed not available\nimport lightning.pytorch\n\"\"\"), value='\"\"\"\\nimport torch\\n\\n        # pretend torch.distributed not available\\n        for name in list(torch.distributed.__dict__.keys()):\\n            if not name.startswith(\"__\"):\\n                delattr(torch.distributed, name)\\n\\n        torch.distributed.is_available = lambda: False\\n\\nimport lightning.pytorch\\n\"\"\"')"
        ],
        "plus_line": 5,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 5812,
        "neg_line": [
            "-torch.distributed.is_available = lambda: False  # pretend torch.distributed not available"
        ],
        "pos_line": [
            "+",
            "+# pretend torch.distributed not available",
            "+for name in list(torch.distributed.__dict__.keys()):",
            "+if not name.startswith(\"__\"):",
            "+delattr(torch.distributed, name)",
            "+",
            "+torch.distributed.is_available = lambda: False",
            "+"
        ],
        "core_change": "-torch.distributed.is_available = lambda: False  # pretend torch.distributed not available + +# pretend torch.distributed not available +for name in list(torch.distributed.__dict__.keys()): +if not name.startswith(\"__\"): +delattr(torch.distributed, name) + +torch.distributed.is_available = lambda: False +",
        "core_API": "keys"
    },
    {
        "commit_hash": "8167dddaa3bdaf8ca7fa7e23b04ac265e971e902",
        "index": "11602a7a9..fcca10a4c 100644",
        "commit_message": "Fix code based on pep8\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def recog(args):",
            "",
            "if rnnlm is not None:",
            "rnnlm = lm_pytorch.ClassifierWithState(",
            "-                            extlm_pytorch.MultiLevelLM(word_rnnlm.predictor,",
            "-                                    rnnlm.predictor, word_dict, char_dict))",
            "+                extlm_pytorch.MultiLevelLM(word_rnnlm.predictor,",
            "+                                           rnnlm.predictor, word_dict, char_dict))",
            "else:",
            "rnnlm = lm_pytorch.ClassifierWithState(",
            "-                            extlm_pytorch.LookAheadWordLM(word_rnnlm.predictor,",
            "-                                    word_dict, char_dict))",
            "+                extlm_pytorch.LookAheadWordLM(word_rnnlm.predictor,",
            "+                                              word_dict, char_dict))",
            "",
            "# read json data",
            "with open(args.recog_json, 'rb') as f:"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 4,
        "minus_line": 4,
        "AST_diff_line": 17,
        "number": 5816,
        "neg_line": [
            "-extlm_pytorch.MultiLevelLM(word_rnnlm.predictor,",
            "-rnnlm.predictor, word_dict, char_dict))",
            "-extlm_pytorch.LookAheadWordLM(word_rnnlm.predictor,",
            "-word_dict, char_dict))"
        ],
        "pos_line": [
            "+extlm_pytorch.MultiLevelLM(word_rnnlm.predictor,",
            "+rnnlm.predictor, word_dict, char_dict))",
            "+extlm_pytorch.LookAheadWordLM(word_rnnlm.predictor,",
            "+word_dict, char_dict))"
        ],
        "core_change": "-extlm_pytorch.MultiLevelLM(word_rnnlm.predictor, -rnnlm.predictor, word_dict, char_dict)) +extlm_pytorch.MultiLevelLM(word_rnnlm.predictor, +rnnlm.predictor, word_dict, char_dict)) -extlm_pytorch.LookAheadWordLM(word_rnnlm.predictor, -word_dict, char_dict)) +extlm_pytorch.LookAheadWordLM(word_rnnlm.predictor, +word_dict, char_dict))",
        "core_API": "ClassifierWithState"
    },
    {
        "commit_hash": "05cda7e842ebe2ff9b5f45a087a161266357ac86",
        "index": "3439a2b315..7ffe17cb97 100644",
        "commit_message": "lint fixes\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def svd(",
            "",
            "",
            "def outer(",
            "-    x1: torch.Tensor,",
            "-    x2: torch.Tensor,",
            "-    out: Optional[torch.Tensor] = None",
            "+    x1: torch.Tensor, x2: torch.Tensor, out: Optional[torch.Tensor] = None",
            ") -> torch.Tensor:",
            "ret = torch.outer(x1, x2, out=out)",
            "return ret"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 17,
        "number": 5818,
        "neg_line": [
            "-x1: torch.Tensor,",
            "-x2: torch.Tensor,",
            "-out: Optional[torch.Tensor] = None"
        ],
        "pos_line": [
            "+x1: torch.Tensor, x2: torch.Tensor, out: Optional[torch.Tensor] = None"
        ],
        "core_change": "-x1: torch.Tensor, -x2: torch.Tensor, -out: Optional[torch.Tensor] = None +x1: torch.Tensor, x2: torch.Tensor, out: Optional[torch.Tensor] = None",
        "core_API": "outer"
    },
    {
        "commit_hash": "e795641a57b0a899e31e1b1d538be3095664fb35",
        "index": "42c94681..53750c6e 100644",
        "commit_message": "Fixed non-4-dim input error for sequential (#1146)\n\n* Fixed non-4-dim input error for sequential\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestAugmentationSequential:",
            "data_keys=[\"input\"],",
            "random_apply=random_apply,",
            "return_transform=return_transform,",
            "+            same_on_batch=same_on_batch,",
            ")",
            "out = aug(inp)",
            "if aug.return_label:",
            "out, label = out",
            "if return_transform and isinstance(out, (tuple, list)):",
            "out = out[0]",
            "-        assert out.shape == inp.shape",
            "+        assert out.shape[-3:] == inp.shape[-3:]",
            "reproducibility_test(inp, aug)",
            "",
            "@pytest.mark.parametrize('random_apply', [1, (2, 2), (1, 2), (2,), 10, True, False])"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=comparison_operator), node=('subscript', None), position=0, insert_id=421288)",
            "Insert(target_node=ASTNode(type=comparison_operator), node=('subscript', None), position=3, insert_id=421289)",
            "Insert(target_node=ASTNode(type=assignment), node=('assignment', None), position=2, insert_id=421290)",
            "Move(target_node=IN(type=subscript), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=421291)",
            "Insert(target_node=IN(type=subscript), node=('slice', None), position=2, insert_id=421292)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=421293)",
            "Move(target_node=IN(type=subscript), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=421294)",
            "Insert(target_node=IN(type=subscript), node=('slice', None), position=2, insert_id=421295)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=421296)",
            "Insert(target_node=IN(type=assignment), node=('pattern_list', None), position=0, insert_id=421297)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=421298)",
            "Insert(target_node=IN(type=assignment), node=('expression_list', None), position=2, insert_id=421299)",
            "Insert(target_node=IN(type=slice), node=('unary_operator', '-3'), position=0, insert_id=421300)",
            "Insert(target_node=IN(type=slice), node=(':', ':'), position=1, insert_id=421301)",
            "Insert(target_node=IN(type=slice), node=('unary_operator', '-3'), position=0, insert_id=421302)",
            "Insert(target_node=IN(type=slice), node=(':', ':'), position=1, insert_id=421303)",
            "Move(target_node=IN(type=pattern_list), node=ASTNode(type=identifier, text=return_transform), position=0)",
            "Move(target_node=IN(type=pattern_list), node=ASTNode(type=,, text=,), position=1)",
            "Insert(target_node=IN(type=pattern_list), node=('identifier', 'same_on_batch'), position=2, insert_id=421304)",
            "Insert(target_node=IN(type=expression_list), node=('identifier', 'same_on_batch'), position=0, insert_id=421305)",
            "Insert(target_node=IN(type=expression_list), node=(',', ','), position=1, insert_id=421306)",
            "Delete(target_node=ASTNode(type=expression_list))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 24,
        "number": 5819,
        "neg_line": [
            "-assert out.shape == inp.shape"
        ],
        "pos_line": [
            "+same_on_batch=same_on_batch,",
            "+assert out.shape[-3:] == inp.shape[-3:]"
        ],
        "core_change": "+same_on_batch=same_on_batch, -assert out.shape == inp.shape +assert out.shape[-3:] == inp.shape[-3:]",
        "core_API": "parametrize"
    },
    {
        "commit_hash": "bd60fe1a4cbfe0ac03d4c45edd6587d8de55840d",
        "index": "cf5ced78..e9c061d1 100644",
        "commit_message": "fixes ssim_loss average (#850)\n\n\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def ssim_loss(img1: torch.Tensor, img2: torch.Tensor, window_size: int,",
            "ssim_map: torch.Tensor = ssim(img1, img2, window_size, max_val, eps)",
            "",
            "# compute and reduce the loss",
            "-    loss = torch.clamp(1. - ssim_map, min=0, max=1) / 2.",
            "+    loss = torch.clamp((1. - ssim_map) / 2, min=0, max=1)",
            "",
            "if reduction == \"mean\":",
            "loss = torch.mean(loss)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=call), position=13)",
            "Insert(target_node=ASTNode(type=argument_list), node=('binary_operator', None), position=1, insert_id=430468)",
            "Insert(target_node=IN(type=binary_operator), node=('parenthesized_expression', None), position=0, insert_id=430469)",
            "Insert(target_node=IN(type=binary_operator), node=('/', '/'), position=1, insert_id=430470)",
            "Insert(target_node=IN(type=binary_operator), node=('integer', '2'), position=2, insert_id=430471)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('(', '('), position=0, insert_id=430472)",
            "Move(target_node=IN(type=parenthesized_expression), node=ASTNode(type=binary_operator), position=1)",
            "Insert(target_node=IN(type=parenthesized_expression), node=(')', ')'), position=2, insert_id=430473)",
            "Delete(target_node=ASTNode(type=/, text=/))",
            "Delete(target_node=ASTNode(type=float, text=2.))",
            "Delete(target_node=ASTNode(type=binary_operator))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 5820,
        "neg_line": [
            "-loss = torch.clamp(1. - ssim_map, min=0, max=1) / 2."
        ],
        "pos_line": [
            "+loss = torch.clamp((1. - ssim_map) / 2, min=0, max=1)"
        ],
        "core_change": "-loss = torch.clamp(1. - ssim_map, min=0, max=1) / 2. +loss = torch.clamp((1. - ssim_map) / 2, min=0, max=1)",
        "core_API": "clamp"
    },
    {
        "commit_hash": "8ae1c0520c668b39e193c58548f285130f1f1755",
        "index": "80b2a470f..f49191c12 100644",
        "commit_message": "Fix compute_q_noisy_max tensor of tensors\n\n",
        "file": "PySyft.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def compute_q_noisy_max_torch(counts, noise_eps):",
            "",
            "if type(counts) != torch.tensor:",
            "",
            "-        counts = torch.tensor(counts, dtype=torch.float)",
            "+        counts = torch.tensor(tensors_to_literals(counts), dtype=torch.float)",
            "",
            "_, winner = counts.max(0)",
            "counts_normalized = noise_eps * ("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=1, insert_id=829397)",
            "Insert(target_node=IN(type=call), node=('identifier', 'tensors_to_literals'), position=0, insert_id=829398)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=829399)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=829400)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=counts), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=829401)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 5824,
        "neg_line": [
            "-counts = torch.tensor(counts, dtype=torch.float)"
        ],
        "pos_line": [
            "+counts = torch.tensor(tensors_to_literals(counts), dtype=torch.float)"
        ],
        "core_change": "-counts = torch.tensor(counts, dtype=torch.float) +counts = torch.tensor(tensors_to_literals(counts), dtype=torch.float)",
        "core_API": "tensor"
    },
    {
        "commit_hash": "848f53c9c8cd971e584b2ca5cd3c9d94745e0130",
        "index": "2572b71e..a79b97e1 100644",
        "commit_message": "Fix CI out of memory & add PyTorch1.9 Python3.9 unit tests (#5862)\n\n* fix ci\n\n* fix ci\n\n* fix ci\n\n* fix ci\n\n* fix ci\n\n* fix ci\n\n* fix ci\n\n* fix ci\n\n* fix ci\n\n* fix ci\n\n* try to fix CI\n\n* try to fix CI\n\n* try to fix CI\n\n* try to fix CI\n\n* fix ci\n\n* Use docker to skip CUDA installation in CI\n\n* fix mmcv install\n\n* delete pt1.9\n\n* fix ci\n\n* delete py3.9\n\n* fix ci\n\n* fix albu\n\n* fix albu\n\n* skip mmpycocotools\n\n* pycocotools\n\n* pycocotools\n\n* env\n\n* env\n\n* env\n\n* env\n\n* env\n\n* env\n\n* env\n\n* skip some large model\n\n* fix tracking\n\n* reduce resnet channels\n\n* reduce resnset channels\n\n* reduce resnet channels\n\n* reduce csp darknet channels\n\n* reduce tracking channels\n\n* fix CE loss\n\n* enable cpu test\n\n* add cuda10.2 and torch1.9 and python3.9\n\n* fix python3.9 env\n\n* fix python3.9 env\n\n* fix python3.9 env\n\n* fix python3.9 env\n\n* fix apt install\n\n* fix py3.9 ppa\n\n* fix py3.9 ppa\n\n* fix py3.9\n\n* fix py3.9\n\n* fix py3.9\n\n* fix python dev\n\n* fix python dev\n\n* add cmake\n\n* update onnxruntime\n\n* update codecov\n\n* update CI\n\n* fix mmcv cuda version\n",
        "file": "mmdetection.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_trident_resnet_backbone():",
            "TridentResNet(50, num_stages=4, **tridentresnet_config)",
            "",
            "model = TridentResNet(50, num_stages=3, **tridentresnet_config)",
            "-    model.init_weights()",
            "model.train()",
            "",
            "-    imgs = torch.randn(1, 3, 224, 224)",
            "+    imgs = torch.randn(1, 3, 32, 32)",
            "feat = model(imgs)",
            "assert len(feat) == 1",
            "-    assert feat[0].shape == torch.Size([3, 1024, 14, 14])",
            "+    assert feat[0].shape == torch.Size([3, 1024, 2, 2])"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=integer, text=224), value='32')",
            "Update(target_node=ASTNode(type=integer, text=224), value='32')",
            "Update(target_node=ASTNode(type=integer, text=14), value='2')",
            "Update(target_node=ASTNode(type=integer, text=14), value='2')",
            "Delete(target_node=ASTNode(type=identifier, text=model))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=init_weights))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 2,
        "minus_line": 3,
        "AST_diff_line": 13,
        "number": 5828,
        "neg_line": [
            "-model.init_weights()",
            "-imgs = torch.randn(1, 3, 224, 224)",
            "-assert feat[0].shape == torch.Size([3, 1024, 14, 14])"
        ],
        "pos_line": [
            "+imgs = torch.randn(1, 3, 32, 32)",
            "+assert feat[0].shape == torch.Size([3, 1024, 2, 2])"
        ],
        "core_change": "-model.init_weights() -imgs = torch.randn(1, 3, 224, 224) +imgs = torch.randn(1, 3, 32, 32) -assert feat[0].shape == torch.Size([3, 1024, 14, 14]) +assert feat[0].shape == torch.Size([3, 1024, 2, 2])",
        "core_API": "init_weights"
    },
    {
        "commit_hash": "0abf9710053520a7c9217aeb6afc0d279f7ef69d",
        "index": "a233c337..b0f5c976 100644",
        "commit_message": "fix seed in `GraphNorm` tests (#6642)\n\nsolves https://github.com/pyg-team/pytorch_geometric/issues/6622\n\n---------\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "from torch_geometric.testing import is_full_test",
            "",
            "",
            "def test_graph_norm():",
            "+    torch.manual_seed(42)",
            "x = torch.randn(200, 16)",
            "batch = torch.arange(4).view(-1, 1).repeat(1, 50).view(-1)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('block', None), position=0, insert_id=938758)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=938759)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=938760)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=938761)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=938762)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=938763)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=938764)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'manual_seed'), position=2, insert_id=938765)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=938766)",
            "Insert(target_node=IN(type=argument_list), node=('integer', '42'), position=1, insert_id=938767)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=938768)",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 12,
        "number": 5829,
        "neg_line": [],
        "pos_line": [
            "+torch.manual_seed(42)"
        ],
        "core_change": "+torch.manual_seed(42)",
        "core_API": "manual_seed"
    },
    {
        "commit_hash": "b5e2b183af5e40e33a4dc7659e697d137259d56e",
        "index": "976135dd7..8cbd2c6fd 100644",
        "commit_message": "Doc styler examples (#14953)\n\n* Fix bad examples\n\n* Add black formatting to style_doc\n\n* Use first nonempty line\n\n* Put it at the right place\n\n* Don't add spaces to empty lines\n\n* Better templates\n\n* Deal with triple quotes in docstrings\n\n* Result of style_doc\n\n* Enable mdx treatment and fix code examples in MDXs\n\n* Result of doc styler on doc source files\n\n* Last fixes\n\n* Break copy from\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFHubertForCTC(TFHubertPreTrainedModel):",
            ">>> processor = Wav2Vec2Processor.from_pretrained(\"facebook/hubert-base-960h\")",
            ">>> model = TFHubertForCTC.from_pretrained(\"facebook/hubert-base-960h\")",
            "",
            "+",
            ">>> def map_to_array(batch):",
            "...     speech, _ = sf.read(batch[\"file\"])",
            "...     batch[\"speech\"] = speech",
            "...     return batch",
            "",
            "+",
            ">>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")",
            ">>> ds = ds.map(map_to_array)",
            "",
            "-        >>> input_values = processor(ds[\"speech\"][0], return_tensors=\"tf\").input_values # Batch size 1",
            "-        >>> logits = model(input_values).logits >>> predicted_ids = tf.argmax(logits, axis=-1)",
            "+        >>> input_values = processor(ds[\"speech\"][0], return_tensors=\"tf\").input_values  # Batch size 1",
            "+        >>> logits = model(input_values).logits",
            "+        >>> predicted_ids = tf.argmax(logits, axis=-1)",
            "",
            ">>> transcription = processor.decode(predicted_ids[0])"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=module), position=19)",
            "Insert(target_node=ASTNode(type=module), node=('ERROR', None), position=1, insert_id=2367965)",
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=20, insert_id=2367966)",
            "Insert(target_node=ASTNode(type=module), node=('ERROR', None), position=21, insert_id=2367967)",
            "Insert(target_node=IN(type=ERROR), node=('>>', '>>'), position=0, insert_id=2367968)",
            "Insert(target_node=IN(type=ERROR), node=('>', '>'), position=1, insert_id=2367969)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=2367970)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=>>, text=>>), position=0)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=>, text=>), position=1)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=identifier, text=logits), position=0)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type==, text==), position=1)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=attribute), position=2)",
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=identifier, text=predicted_ids), position=0)",
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type==, text==), position=1)",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=binary_operator))",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=>>, text=>>))",
            "Delete(target_node=ASTNode(type=>, text=>))",
            "Delete(target_node=ASTNode(type=ERROR))"
        ],
        "plus_line": 3,
        "minus_line": 2,
        "AST_diff_line": 20,
        "number": 5833,
        "neg_line": [
            "->>> input_values = processor(ds[\"speech\"][0], return_tensors=\"tf\").input_values # Batch size 1",
            "->>> logits = model(input_values).logits >>> predicted_ids = tf.argmax(logits, axis=-1)"
        ],
        "pos_line": [
            "+",
            "+",
            "+>>> input_values = processor(ds[\"speech\"][0], return_tensors=\"tf\").input_values  # Batch size 1",
            "+>>> logits = model(input_values).logits",
            "+>>> predicted_ids = tf.argmax(logits, axis=-1)"
        ],
        "core_change": "+ + ->>> input_values = processor(ds[\"speech\"][0], return_tensors=\"tf\").input_values # Batch size 1 ->>> logits = model(input_values).logits >>> predicted_ids = tf.argmax(logits, axis=-1) +>>> input_values = processor(ds[\"speech\"][0], return_tensors=\"tf\").input_values  # Batch size 1 +>>> logits = model(input_values).logits +>>> predicted_ids = tf.argmax(logits, axis=-1)",
        "core_API": "from_pretrained"
    },
    {
        "commit_hash": "3dd0b8c186a2f7a15790b65d16ac73dd49cccbda",
        "index": "0bb2323f4..c2694eabf 100644",
        "commit_message": "fix metric name to work with default earlystopping (#628)\n\n\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "Minimal example",
            "",
            "def test_end(self, outputs):",
            "# OPTIONAL",
            "-            avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()",
            "-            return {'avg_test_loss': avg_loss}",
            "+            test_loss_mean = torch.stack([x['test_loss'] for x in outputs]).mean()",
            "+            return {'test_loss': test_loss_mean}",
            "",
            "def configure_optimizers(self):",
            "# REQUIRED"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=avg_loss), value='test_loss_mean')",
            "Update(target_node=ASTNode(type=string, text='avg_test_loss'), value=\"'test_loss'\")",
            "Update(target_node=ASTNode(type=identifier, text=avg_loss), value='test_loss_mean')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 3,
        "number": 5837,
        "neg_line": [
            "-avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()",
            "-return {'avg_test_loss': avg_loss}"
        ],
        "pos_line": [
            "+test_loss_mean = torch.stack([x['test_loss'] for x in outputs]).mean()",
            "+return {'test_loss': test_loss_mean}"
        ],
        "core_change": "-avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean() -return {'avg_test_loss': avg_loss} +test_loss_mean = torch.stack([x['test_loss'] for x in outputs]).mean() +return {'test_loss': test_loss_mean}",
        "core_API": "stack"
    },
    {
        "commit_hash": "2322eb8e2f9765cb73f59b324cc46a0e9cfe803f",
        "index": "0b7a81aa3..ec034a5fa 100644",
        "commit_message": "Update serving signatures and make sure we actually use them (#19034)\n\n* Override save() to use the serving signature as the default\n\n* Replace int32 with int64 in all our serving signatures\n\n* Remember one very important line so as not to break every test at once\n\n* Dtype fix for TFLED\n\n* dtype fix for shift_tokens_right in general\n\n* Dtype fixes in mBART and RAG\n\n* Fix dtypes for test_unpack_inputs\n\n* More dtype fixes\n\n* Yet more mBART + RAG dtype fixes\n\n* Yet more mBART + RAG dtype fixes\n\n* Add a check that the model actually has a serving method\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def shift_tokens_right(input_ids: tf.Tensor, pad_token_id: int):",
            "if pad_token_id is None:",
            "raise ValueError(\"self.model.config.pad_token_id has to be defined.\")",
            "# replace possible -100 values in labels by `pad_token_id`",
            "-    input_ids = tf.where(input_ids == -100, tf.fill(shape_list(input_ids), pad_token_id), input_ids)",
            "+    input_ids = tf.where(",
            "+        input_ids == -100, tf.fill(shape_list(input_ids), tf.cast(pad_token_id, input_ids.dtype)), input_ids",
            "+    )",
            "language_id_index = (",
            "tf.reduce_sum(tf.cast(tf.math.not_equal(input_ids, pad_token_id), dtype=input_ids.dtype), axis=-1) - 1",
            ")",
            "-    language_id_index = tf.stack([tf.range(shape_list(input_ids)[0]), language_id_index], axis=-1)",
            "+    language_id_index = tf.stack(",
            "+        [tf.range(shape_list(input_ids)[0], dtype=input_ids.dtype), language_id_index], axis=-1",
            "+    )",
            "languages_ids = tf.gather_nd(input_ids, language_id_index)",
            "",
            "shifted_input_ids = tf.concat([tf.expand_dims(languages_ids, axis=-1), input_ids[:, :-1]], axis=-1)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=3, insert_id=2361483)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=4, insert_id=2361484)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2361485)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2361486)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=2361487)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=2361488)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2361489)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2361490)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cast'), position=2, insert_id=2361491)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2361492)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=pad_token_id), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=2361493)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=3, insert_id=2361494)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=), text=)), position=4)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=2361495)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2361496)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=2361497)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'input_ids'), position=0, insert_id=2361498)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2361499)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dtype'), position=2, insert_id=2361500)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'input_ids'), position=0, insert_id=2361501)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2361502)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dtype'), position=2, insert_id=2361503)"
        ],
        "plus_line": 6,
        "minus_line": 2,
        "AST_diff_line": 23,
        "number": 5838,
        "neg_line": [
            "-input_ids = tf.where(input_ids == -100, tf.fill(shape_list(input_ids), pad_token_id), input_ids)",
            "-language_id_index = tf.stack([tf.range(shape_list(input_ids)[0]), language_id_index], axis=-1)"
        ],
        "pos_line": [
            "+input_ids = tf.where(",
            "+input_ids == -100, tf.fill(shape_list(input_ids), tf.cast(pad_token_id, input_ids.dtype)), input_ids",
            "+)",
            "+language_id_index = tf.stack(",
            "+[tf.range(shape_list(input_ids)[0], dtype=input_ids.dtype), language_id_index], axis=-1",
            "+)"
        ],
        "core_change": "-input_ids = tf.where(input_ids == -100, tf.fill(shape_list(input_ids), pad_token_id), input_ids) +input_ids = tf.where( +input_ids == -100, tf.fill(shape_list(input_ids), tf.cast(pad_token_id, input_ids.dtype)), input_ids +) -language_id_index = tf.stack([tf.range(shape_list(input_ids)[0]), language_id_index], axis=-1) +language_id_index = tf.stack( +[tf.range(shape_list(input_ids)[0], dtype=input_ids.dtype), language_id_index], axis=-1 +)",
        "core_API": "where"
    },
    {
        "commit_hash": "6dd8fccac0b366782b5319af6236d2d337a25b42",
        "index": "c3bced3..0453a22 100644",
        "commit_message": "Add support for Torchscript export of IntentSlotOutputLayer and CRF (#1146)\n\nSummary:\nPull Request resolved: https://github.com/facebookresearch/pytext/pull/1146\n\nThis diff does the following:\n\n1. Modifies `IntentSlotOutputLayer`, `WordTaggingOutputLayer` and `CRFOutputLayer` for torchscript export.\n\n1. Makes CRF implementation torchscriptable\n1. Fixes `predict` method of `NewTask` to make sure it passes model context as well to `get_pred`\n1. Fixes return type of the `forward` method of the decoder to return tuples of tensors instead of lists of tensors.\n\nReviewed By: liaimi\n\nDifferential Revision: D18565235\n\nfbshipit-source-id: 80836351c96b53f0650fa05ba4b4ab78b866899a\n\n",
        "file": "pytext.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class IntentSlotModelDecoder(DecoderBase):",
            "dense = dense.unsqueeze(1).repeat(1, word_input_shape[1], 1)",
            "x_w = torch.cat((x_w, dense), 2)",
            "",
            "-        return [logit_d, self.word_decoder(x_w)]",
            "+        return logit_d, self.word_decoder(x_w)",
            "",
            "def get_decoder(self) -> List[nn.Module]:",
            "\"\"\"Returns the document and word decoder modules."
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=return_statement), node=('expression_list', None), position=1, insert_id=878486)",
            "Move(target_node=IN(type=expression_list), node=ASTNode(type=identifier, text=logit_d), position=0)",
            "Move(target_node=IN(type=expression_list), node=ASTNode(type=,, text=,), position=1)",
            "Move(target_node=IN(type=expression_list), node=ASTNode(type=call), position=2)",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=list))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 5840,
        "neg_line": [
            "-return [logit_d, self.word_decoder(x_w)]"
        ],
        "pos_line": [
            "+return logit_d, self.word_decoder(x_w)"
        ],
        "core_change": "-return [logit_d, self.word_decoder(x_w)] +return logit_d, self.word_decoder(x_w)",
        "core_API": "unsqueeze"
    },
    {
        "commit_hash": "1f0670e5e8a3239b5f99384811ac7628fed685e2",
        "index": "973e3728..0a78a110 100755",
        "commit_message": "fix scripts on new config\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "with tf.Graph().as_default() as G:",
            "",
            "if args.output_type == 'label':",
            "for r in res:",
            "-            print r.argsort()[-top:][::-1]",
            "+            print r[0].argsort(axis=1)[:,-args.top:][:,::-1]",
            "elif args.output_type == 'label_prob':",
            "raise NotImplementedError",
            "elif args.output_type == 'raw':"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=subscript), node=('slice', None), position=2, insert_id=2321714)",
            "Insert(target_node=ASTNode(type=subscript), node=(',', ','), position=3, insert_id=2321715)",
            "Insert(target_node=ASTNode(type=subscript), node=('slice', None), position=2, insert_id=2321716)",
            "Insert(target_node=ASTNode(type=subscript), node=(',', ','), position=3, insert_id=2321717)",
            "Insert(target_node=IN(type=slice), node=(':', ':'), position=0, insert_id=2321718)",
            "Insert(target_node=IN(type=slice), node=(':', ':'), position=0, insert_id=2321719)",
            "Insert(target_node=ASTNode(type=attribute), node=('subscript', None), position=0, insert_id=2321720)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=1, insert_id=2321721)",
            "Insert(target_node=ASTNode(type=unary_operator), node=('attribute', None), position=1, insert_id=2321722)",
            "Move(target_node=IN(type=subscript), node=ASTNode(type=identifier, text=r), position=0)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=2321723)",
            "Insert(target_node=IN(type=subscript), node=('integer', '0'), position=2, insert_id=2321724)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=2321725)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'axis'), position=0, insert_id=2321726)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2321727)",
            "Insert(target_node=IN(type=keyword_argument), node=('integer', '1'), position=2, insert_id=2321728)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'args'), position=0, insert_id=2321729)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2321730)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=top), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 19,
        "number": 5842,
        "neg_line": [
            "-print r.argsort()[-top:][::-1]"
        ],
        "pos_line": [
            "+print r[0].argsort(axis=1)[:,-args.top:][:,::-1]"
        ],
        "core_change": "-print r.argsort()[-top:][::-1] +print r[0].argsort(axis=1)[:,-args.top:][:,::-1]",
        "core_API": "Graph"
    },
    {
        "commit_hash": "d98a384cb0ec56e5e83d22d75a536052e21305a0",
        "index": "398d385aa..b8d9348da 100644",
        "commit_message": "fix bug in prepare inputs for language generation for xlm for effective batch_size > 1\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class XLMWithLMHeadModel(XLMPreTrainedModel):",
            "mask_token_id = self.config.mask_token_id",
            "lang_id = self.config.lang_id",
            "",
            "-        mask_token = torch.full((1, 1), mask_token_id, dtype=torch.long, device=input_ids.device)",
            "+        effective_batch_size = input_ids.shape[0]",
            "+        mask_token = torch.full((effective_batch_size, 1), mask_token_id, dtype=torch.long, device=input_ids.device)",
            "input_ids = torch.cat([input_ids, mask_token], dim=1)",
            "if lang_id is not None:",
            "langs = torch.full_like(input_ids, lang_id)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=1243368)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1243369)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'effective_batch_size'), position=0, insert_id=1243370)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1243371)",
            "Insert(target_node=IN(type=assignment), node=('subscript', None), position=2, insert_id=1243372)",
            "Insert(target_node=IN(type=subscript), node=('attribute', None), position=0, insert_id=1243373)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=1243374)",
            "Insert(target_node=IN(type=subscript), node=('integer', '0'), position=2, insert_id=1243375)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=1243376)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'input_ids'), position=0, insert_id=1243377)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1243378)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'shape'), position=2, insert_id=1243379)",
            "Insert(target_node=ASTNode(type=tuple), node=('identifier', 'effective_batch_size'), position=1, insert_id=1243380)",
            "Delete(target_node=ASTNode(type=integer, text=1))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 14,
        "number": 5845,
        "neg_line": [
            "-mask_token = torch.full((1, 1), mask_token_id, dtype=torch.long, device=input_ids.device)"
        ],
        "pos_line": [
            "+effective_batch_size = input_ids.shape[0]",
            "+mask_token = torch.full((effective_batch_size, 1), mask_token_id, dtype=torch.long, device=input_ids.device)"
        ],
        "core_change": "-mask_token = torch.full((1, 1), mask_token_id, dtype=torch.long, device=input_ids.device) +effective_batch_size = input_ids.shape[0] +mask_token = torch.full((effective_batch_size, 1), mask_token_id, dtype=torch.long, device=input_ids.device)",
        "core_API": "full"
    },
    {
        "commit_hash": "48ae7217e482a1a3624d6e5380c972a653cacfaf",
        "index": "760b177c..717f7326 100644",
        "commit_message": "Fix TensorFlow RNN backwards support. (#3662)\n\n\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def rnn(step_function, inputs, initial_states,",
            "",
            "if mask is not None:",
            "if go_backwards:",
            "-                mask = tf.reverse(mask, [True] + [False] * (ndim - 1))",
            "+                mask = tf.reverse(mask, [True] + [False] * (ndim - 2))",
            "",
            "# Transpose not supported by bool tensor types, hence round-trip to uint8.",
            "mask = tf.cast(mask, tf.uint8)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=integer, text=1), value='2')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 5848,
        "neg_line": [
            "-mask = tf.reverse(mask, [True] + [False] * (ndim - 1))"
        ],
        "pos_line": [
            "+mask = tf.reverse(mask, [True] + [False] * (ndim - 2))"
        ],
        "core_change": "-mask = tf.reverse(mask, [True] + [False] * (ndim - 1)) +mask = tf.reverse(mask, [True] + [False] * (ndim - 2))",
        "core_API": "reverse"
    },
    {
        "commit_hash": "26e5aea90bfcac5558ee01679504e12e4e662687",
        "index": "d8a366ad9..f83c0c0b9 100644",
        "commit_message": "Tracing from inside the placeholders (#3303)\n\n* draft tracing from placeholders\n\n* trace from placeholder hook\n\n* add role attribute to placeholder\n\n* use tracing from placeholders to build Plan\n\n* rename hook functions\n\n* add a toggle_tracing method on Plan\n\n* change State.read() so that it returns placeholders\n\n* add tracing of hooked functions\n\n* find first arg placeholder instead of using first arg when tracing\n\n* fix import issues\n\n* start experimenting with wrapper and contextmanager around frameworks\n\n* Add tracing for module-level functions (like `torch.rand`)\n\n* black\n\n* use framework kwarg only conditionally\n\n* wrap only module functions\n\n* add torch in arguments for test plan made with the decorator\n\n* fix state attributes not being placeholders on build\n\n* transform parameters to placeholders on the fly at build time\n\n* fix unwrap on Placeholder.instantiate()\n\n* Fix `Placeholder.expected_shape` after instantiation\n\nInstantiating with a new tensor should also set the `expected_shape`.\n\n* handle actions that don't return tensors in plans\n\n* implement function to create already instantiated placeholder\n\n* test create_from\n\n* add test for functions as torch.split\n\n* handle functions returning tuples\n\n* remove wrapper case when instantiate placeholders\n\n* clean State.tensors()\n\n* fix state bugs\n\n* fix bug __setattr__ torchscript\n\n* remove previous tracing implementation\n\n* noqa in test\n\n* use pragma: no cover\n\n* move tracing logic to separate file\n\n* remove comment\n\n* update protobuf serde for Role\n\nCo-authored-by: Karl Higley <kmhigley@gmail.com>\n",
        "file": "PySyft.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TorchHook(FrameworkHook):",
            "if \"native_tensor\" not in dir(hook_self.torch):",
            "hook_self.torch.native_tensor = hook_self.torch.tensor",
            "",
            "-        @tracer(func_name=\"torch.tensor\")",
            "def new_tensor(*args, owner=None, id=None, register=True, **kwargs):",
            "current_tensor = hook_self.torch.native_tensor(*args, **kwargs)",
            "_apply_args(hook_self, current_tensor, owner, id)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=ASTNode(type=module), node=ASTNode(type=function_definition), position=3)",
            "Delete(target_node=ASTNode(type=@, text=@))",
            "Delete(target_node=ASTNode(type=identifier, text=tracer))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=func_name))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=string, text=\"torch.tensor\"))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=decorator))",
            "Delete(target_node=ASTNode(type=decorated_definition))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 13,
        "number": 5857,
        "neg_line": [
            "-@tracer(func_name=\"torch.tensor\")"
        ],
        "pos_line": [],
        "core_change": "-@tracer(func_name=\"torch.tensor\")",
        "core_API": "native_tensor"
    },
    {
        "commit_hash": "8e861e75ed9afee06a046c24539fad5535e5c578",
        "index": "7e5d1f31..ba40f512 100644",
        "commit_message": "pytorch 1.0 (#2165)\n\n* fixes\n\n* allenlp\n\n* fix bug\n\n* reduce=False -> reduction='none'\n\n* fix _sparse_mask\n\n* remove print statement\n\n* fix more tests\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class GraphParser(Model):",
            "\"arc representation dim\", \"arc feedforward output dim\")",
            "",
            "self._unlabelled_f1 = F1Measure(positive_label=1)",
            "-        self._arc_loss = torch.nn.BCEWithLogitsLoss(reduce=False)",
            "-        self._tag_loss = torch.nn.CrossEntropyLoss(reduce=False)",
            "+        self._arc_loss = torch.nn.BCEWithLogitsLoss(reduction='none')",
            "+        self._tag_loss = torch.nn.CrossEntropyLoss(reduction='none')",
            "initializer(self)",
            "",
            "@overrides"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=reduce), value='reduction')",
            "Insert(target_node=ASTNode(type=keyword_argument), node=('string', \"'none'\"), position=2, insert_id=31992)",
            "Update(target_node=ASTNode(type=identifier, text=reduce), value='reduction')",
            "Insert(target_node=ASTNode(type=keyword_argument), node=('string', \"'none'\"), position=2, insert_id=31993)",
            "Delete(target_node=ASTNode(type=false, text=False))",
            "Delete(target_node=ASTNode(type=false, text=False))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 6,
        "number": 5860,
        "neg_line": [
            "-self._arc_loss = torch.nn.BCEWithLogitsLoss(reduce=False)",
            "-self._tag_loss = torch.nn.CrossEntropyLoss(reduce=False)"
        ],
        "pos_line": [
            "+self._arc_loss = torch.nn.BCEWithLogitsLoss(reduction='none')",
            "+self._tag_loss = torch.nn.CrossEntropyLoss(reduction='none')"
        ],
        "core_change": "-self._arc_loss = torch.nn.BCEWithLogitsLoss(reduce=False) -self._tag_loss = torch.nn.CrossEntropyLoss(reduce=False) +self._arc_loss = torch.nn.BCEWithLogitsLoss(reduction='none') +self._tag_loss = torch.nn.CrossEntropyLoss(reduction='none')",
        "core_API": "BCEWithLogitsLoss"
    },
    {
        "commit_hash": "9c6a02f2645db0043166c286cf01480ce1213c25",
        "index": "7e45137e..fe00de09 100644",
        "commit_message": "fix tests\n\n",
        "file": "flair.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestTransformerWordEmbeddings(BaseEmbeddingsTest):",
            "\"Berlin to Germany is like Vienna to Austria\",",
            "]",
            "",
            "-        normal_sentences = [",
            "-            Sentence(text) for text in texts",
            "-        ]",
            "-        onnx_sentences = [",
            "-            Sentence(text) for text in texts",
            "-        ]",
            "+        normal_sentences = [Sentence(text) for text in texts]",
            "+        onnx_sentences = [Sentence(text) for text in texts]",
            "",
            "embeddings = TransformerWordEmbeddings(\"distilbert-base-uncased\")",
            "results_base_path.mkdir(exist_ok=True, parents=True)"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 6,
        "AST_diff_line": 17,
        "number": 5862,
        "neg_line": [
            "-normal_sentences = [",
            "-Sentence(text) for text in texts",
            "-]",
            "-onnx_sentences = [",
            "-Sentence(text) for text in texts",
            "-]"
        ],
        "pos_line": [
            "+normal_sentences = [Sentence(text) for text in texts]",
            "+onnx_sentences = [Sentence(text) for text in texts]"
        ],
        "core_change": "-normal_sentences = [ -Sentence(text) for text in texts -] -onnx_sentences = [ -Sentence(text) for text in texts -] +normal_sentences = [Sentence(text) for text in texts] +onnx_sentences = [Sentence(text) for text in texts]",
        "core_API": "mkdir"
    },
    {
        "commit_hash": "b45cdffee0fd4ade451fc9d20ba411ed2b6c4d1a",
        "index": "bbdb33f1be..5a77bae511 100644",
        "commit_message": "Fixed small issue with reshape\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def reshape(",
            "shape: Union[ivy.NativeShape, Sequence[int]],",
            "*,",
            "copy: Optional[bool] = None,",
            "-    out: Optional[torch.Tensor] = None,",
            "order: Optional[str] = \"C\",",
            "+    out: Optional[torch.Tensor] = None,",
            ") -> torch.Tensor:",
            "ivy.assertions.check_elem_in_list(order, [\"C\", \"F\"])",
            "if copy:"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=typed_default_parameter), node=ASTNode(type=parameters), position=8)",
            "Move(target_node=ASTNode(type=typed_default_parameter), node=ASTNode(type=parameters), position=7)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 5866,
        "neg_line": [
            "-out: Optional[torch.Tensor] = None,"
        ],
        "pos_line": [
            "+out: Optional[torch.Tensor] = None,"
        ],
        "core_change": "-out: Optional[torch.Tensor] = None, +out: Optional[torch.Tensor] = None,",
        "core_API": "check_elem_in_list"
    },
    {
        "commit_hash": "7cbeb6c281b8f4c3ac6193a49dd003129ef18c92",
        "index": "ca380217..764b6c66 100644",
        "commit_message": "Display activation functions as modules. (#4045)\n\n* Display activations as modules.\n\n* Fix tests and make changes in doc.\n\n* Fix parameter type.\n\n* Fix lambda based activation name displaying.\n\n* Fix formatting.\n\nCo-authored-by: Dirk Groeneveld <dirkg@allenai.org>\nCo-authored-by: Evan Pete Walsh <epwalsh10@gmail.com>\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class MockSeq2SeqEncoder(Seq2SeqEncoder):",
            "def _make_feedforward(input_dim, output_dim):",
            "return FeedForwardEncoder(",
            "FeedForward(",
            "-            input_dim=input_dim, num_layers=1, activations=torch.relu, hidden_dims=output_dim",
            "+            input_dim=input_dim, num_layers=1, activations=torch.nn.ReLU(), hidden_dims=output_dim",
            ")",
            ")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=keyword_argument), node=('call', None), position=2, insert_id=18387)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=18388)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=18389)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=18390)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'ReLU'), position=2, insert_id=18391)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=18392)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=18393)",
            "Update(target_node=ASTNode(type=identifier, text=relu), value='nn')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 5867,
        "neg_line": [
            "-input_dim=input_dim, num_layers=1, activations=torch.relu, hidden_dims=output_dim"
        ],
        "pos_line": [
            "+input_dim=input_dim, num_layers=1, activations=torch.nn.ReLU(), hidden_dims=output_dim"
        ],
        "core_change": "-input_dim=input_dim, num_layers=1, activations=torch.relu, hidden_dims=output_dim +input_dim=input_dim, num_layers=1, activations=torch.nn.ReLU(), hidden_dims=output_dim",
        "core_API": "ReLU"
    },
    {
        "commit_hash": "c7fd1d9fb18862318b133c9474d37c5085850070",
        "index": "f93bec5d..f1120798 100755",
        "commit_message": "fix deprecations about casting & initializers in tf1.13\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Model(ModelDesc):",
            "isTrain = get_current_tower_context().is_training",
            "if isTrain:",
            "# beam search is too slow to run in training",
            "-            predictions = tf.to_int32(",
            "-                tf.nn.ctc_greedy_decoder(logits, seqlen)[0][0])",
            "+            predictions = tf.cast(",
            "+                tf.nn.ctc_greedy_decoder(logits, seqlen)[0][0], tf.int32)",
            "else:",
            "-            predictions = tf.to_int32(",
            "-                tf.nn.ctc_beam_search_decoder(logits, seqlen)[0][0])",
            "+            predictions = tf.cast(",
            "+                tf.nn.ctc_beam_search_decoder(logits, seqlen)[0][0], tf.int32)",
            "err = tf.edit_distance(predictions, label, normalize=True)",
            "err.set_shape([None])",
            "err = tf.reduce_mean(err, name='error')"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=to_int32), value='cast')",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=2278848)",
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=3, insert_id=2278849)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2278850)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2278851)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'int32'), position=2, insert_id=2278852)",
            "Update(target_node=ASTNode(type=identifier, text=to_int32), value='cast')",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=2278853)",
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=3, insert_id=2278854)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2278855)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2278856)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'int32'), position=2, insert_id=2278857)"
        ],
        "plus_line": 4,
        "minus_line": 4,
        "AST_diff_line": 12,
        "number": 5870,
        "neg_line": [
            "-predictions = tf.to_int32(",
            "-tf.nn.ctc_greedy_decoder(logits, seqlen)[0][0])",
            "-predictions = tf.to_int32(",
            "-tf.nn.ctc_beam_search_decoder(logits, seqlen)[0][0])"
        ],
        "pos_line": [
            "+predictions = tf.cast(",
            "+tf.nn.ctc_greedy_decoder(logits, seqlen)[0][0], tf.int32)",
            "+predictions = tf.cast(",
            "+tf.nn.ctc_beam_search_decoder(logits, seqlen)[0][0], tf.int32)"
        ],
        "core_change": "-predictions = tf.to_int32( -tf.nn.ctc_greedy_decoder(logits, seqlen)[0][0]) +predictions = tf.cast( +tf.nn.ctc_greedy_decoder(logits, seqlen)[0][0], tf.int32) -predictions = tf.to_int32( -tf.nn.ctc_beam_search_decoder(logits, seqlen)[0][0]) +predictions = tf.cast( +tf.nn.ctc_beam_search_decoder(logits, seqlen)[0][0], tf.int32)",
        "core_API": "to_int32"
    },
    {
        "commit_hash": "fbac2569827558227e1e2cca71f837f5a3baa3b9",
        "index": "bf812dfb7..022801e7d 100644",
        "commit_message": "[sgd] Add benchmarks (#7454)\n\n* Init fp16\n\n* fp16 and schedulers\n\n* scheduler linking and fp16\n\n* to fp16\n\n* loss scaling and documentation\n\n* more documentation\n\n* add tests, refactor config\n\n* moredocs\n\n* more docs\n\n* fix logo, add test mode, add fp16 flag\n\n* fix tests\n\n* fix scheduler\n\n* fix apex\n\n* improve safety\n\n* fix tests\n\n* fix tests\n\n* remove pin memory default\n\n* rm\n\n* fix\n\n* Update doc/examples/doc_code/raysgd_torch_signatures.py\n\n* fix\n\n* migrate changes from other PR\n\n* ok thanks\n\n* pass\n\n* signatures\n\n* lint'\n\n* Update python/ray/experimental/sgd/pytorch/utils.py\n\n* Apply suggestions from code review\n\nCo-Authored-By: Edward Oakes <ed.nmi.oakes@gmail.com>\n\n* should address most comments\n\n* comments\n\n* fix this ci\n\n* first_pass\n\n* add overrides\n\n* override\n\n* fixing up operators\n\n* format\n\n* sgd\n\n* constants\n\n* rm\n\n* revert\n\n* save\n\n* failures\n\n* fixes\n\n* trainer\n\n* run test\n\n* operator\n\n* code\n\n* op\n\n* ok done\n\n* operator\n\n* sgd test fixes\n\n* ok\n\n* trainer\n\n* format\n\n* Apply suggestions from code review\n\nCo-Authored-By: Edward Oakes <ed.nmi.oakes@gmail.com>\n\n* Update doc/source/raysgd/raysgd_pytorch.rst\n\n* docstring\n\n* dcgan\n\n* doc\n\n* commits\n\n* nit\n\n* testing\n\n* revert\n\n* Start renaming pytorch to torch\n\n* Rename PyTorchTrainer to TorchTrainer\n\n* Rename PyTorch runners to Torch runners\n\n* Finish renaming API\n\n* Rename to torch in tests\n\n* Finish renaming docs + tests\n\n* Run format + fix DeprecationWarning\n\n* fix\n\n* move tests up\n\n* benchmarks\n\n* rename\n\n* remove some args\n\n* better metrics output\n\n* fix up the benchmark\n\n* benchmark-yaml\n\n* horovod-benchmark\n\n* benchmarks\n\n* Remove benchmark code for cleanups\n\n* benchmark-code\n\n* nits\n\n* benchmark yamls\n\n* benchmark yaml\n\n* ok\n\n* ok\n\n* ok\n\n* benchmark\n\n* nit\n\n* finish_bench\n\n* makedatacreator\n\n* relax\n\n* metrics\n\n* autosetsampler\n\n* profile\n\n* movements\n\n* OK\n\n* smoothen\n\n* fix\n\n* nitdocs\n\n* loss\n\n* envflag\n\n* comments\n\n* nit\n\n* format\n\n* visible\n\n* images\n\n* move_images\n\n* fix\n\n* rernder\n\n* rrender\n\n* rest\n\n* multgpu\n\n* fix\n\n* nit\n\n* finish\n\n* extrra\n\n* setup\n\n* revert\n\nCo-authored-by: Edward Oakes <ed.nmi.oakes@gmail.com>\nCo-authored-by: Maksim Smolin <maximsmol@gmail.com>\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "cuda",
        "change": [
            "class TorchRunner:",
            "else:",
            "self.criterion = self.loss_creator(self.config)",
            "",
            "-        if torch.cuda.is_available() and hasattr(\"cuda\", self.criterion):",
            "+        if torch.cuda.is_available() and hasattr(self.criterion, \"cuda\"):",
            "self.criterion = self.criterion.cuda()",
            "",
            "def _create_schedulers_if_available(self):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=3, insert_id=1870768)",
            "Insert(target_node=ASTNode(type=argument_list), node=('string', '\"cuda\"'), position=4, insert_id=1870769)",
            "Delete(target_node=ASTNode(type=string, text=\"cuda\"))",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 5871,
        "neg_line": [
            "-if torch.cuda.is_available() and hasattr(\"cuda\", self.criterion):"
        ],
        "pos_line": [
            "+if torch.cuda.is_available() and hasattr(self.criterion, \"cuda\"):"
        ],
        "core_change": "-if torch.cuda.is_available() and hasattr(\"cuda\", self.criterion): +if torch.cuda.is_available() and hasattr(self.criterion, \"cuda\"):",
        "core_API": "loss_creator"
    },
    {
        "commit_hash": "62e3d1dbdc88af9ed8afb37f7e8ecc011f5486a3",
        "index": "2c8aded0..648c8c07 100644",
        "commit_message": "Scope TokenIndexer output by indexer name (#3597)\n\n* Indexer tests are now passing, at least\n\n* Fixed some masking issues, and padding keys for one config file\n\n* TextFieldEmbedder tests pass\n\n* Fixing fixtures, some more tests passing\n\n* Fixed weird ordering bug\n\n* All TokenEmbedder tests passing?\n\n* Update fixtures\n\n* Fix more hard-coded references\n\n* fix field tests\n\n* fix dataset reader tests\n\n* Fix iterator tests\n\n* More tests passing\n\n* fix hotflip and some other tests\n\n* more tests\n\n* more test fixes\n\n* more tests\n\n* most tests passing; I think the remaining ones are spacy model changes\n\n* hard-code POS tag test\n\n* last test, I think\n\n* black\n\n* updated black\n\n* flake8\n\n* mypy\n\n* black again\n\n* fix training configs\n\n* remove reference to embedder_to_indexer_map\n\n* Other fixes from PR comments\n\n* fix breakage from incorrect merge during rebase\n\n* flake, some docstring formatting\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SpanConstituencyParser(Model):",
            "@overrides",
            "def forward(",
            "self,  # type: ignore",
            "-        tokens: Dict[str, torch.LongTensor],",
            "+        tokens: TextFieldTensors,",
            "spans: torch.LongTensor,",
            "metadata: List[Dict[str, Any]],",
            "-        pos_tags: Dict[str, torch.LongTensor] = None,",
            "+        pos_tags: TextFieldTensors = None,",
            "span_labels: torch.LongTensor = None,",
            ") -> Dict[str, torch.Tensor]:",
            "",
            "\"\"\"",
            "# Parameters",
            "",
            "-        tokens : Dict[str, torch.LongTensor], required",
            "+        tokens : TextFieldTensors, required",
            "The output of ``TextField.as_array()``, which should typically be passed directly to a",
            "``TextFieldEmbedder``. This output is a dictionary mapping keys to ``TokenIndexer``",
            "tensors.  At its most basic, using a ``SingleIdTokenIndexer`` this is : ``{\"tokens\":"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=Dict), value='TextFieldTensors')",
            "Move(target_node=ASTNode(type=type), node=ASTNode(type=identifier, text=Dict), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=Dict), value='TextFieldTensors')",
            "Move(target_node=ASTNode(type=type), node=ASTNode(type=identifier, text=Dict), position=0)",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=identifier, text=str))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=LongTensor))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=identifier, text=str))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=LongTensor))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 22,
        "number": 5873,
        "neg_line": [
            "-tokens: Dict[str, torch.LongTensor],",
            "-pos_tags: Dict[str, torch.LongTensor] = None,",
            "-tokens : Dict[str, torch.LongTensor], required"
        ],
        "pos_line": [
            "+tokens: TextFieldTensors,",
            "+pos_tags: TextFieldTensors = None,",
            "+tokens : TextFieldTensors, required"
        ],
        "core_change": "-tokens: Dict[str, torch.LongTensor], +tokens: TextFieldTensors, -pos_tags: Dict[str, torch.LongTensor] = None, +pos_tags: TextFieldTensors = None, -tokens : Dict[str, torch.LongTensor], required +tokens : TextFieldTensors, required",
        "core_API": "as_array"
    },
    {
        "commit_hash": "60455784fdba862b90eb48d725a113d1dcbb52f2",
        "index": "54c8d8b85..5fb0717b0 100644",
        "commit_message": "fixed docstring\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Conv1dLinear(torch.nn.Module):",
            "\"\"\"Calculate forward propagation.",
            "",
            "Args:",
            "-            x (torch.Tensor): Batch of input tensors (B, ..., in_chans).",
            "+            x (torch.Tensor): Batch of input tensors (B, T, in_chans).",
            "",
            "Returns:",
            "-            torch.Tensor: Batch of output tensors (B, `*`, hidden_chans).",
            "+            torch.Tensor: Batch of output tensors (B, T, hidden_chans).",
            "",
            "\"\"\"",
            "x = torch.relu(self.w_1(x.transpose(-1, 1))).transpose(-1, 1)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"\"\"Calculate forward propagation.\n\nArgs:\n            x (torch.Tensor): Batch of input tensors (B, ..., in_chans).\n\nReturns:\n            torch.Tensor: Batch of output tensors (B, `*`, hidden_chans).\n\n\"\"\"), value='\"\"\"Calculate forward propagation.\\n\\nArgs:\\n            x (torch.Tensor): Batch of input tensors (B, T, in_chans).\\n\\nReturns:\\n            torch.Tensor: Batch of output tensors (B, T, hidden_chans).\\n\\n\"\"\"')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 1,
        "number": 5876,
        "neg_line": [
            "-x (torch.Tensor): Batch of input tensors (B, ..., in_chans).",
            "-torch.Tensor: Batch of output tensors (B, `*`, hidden_chans)."
        ],
        "pos_line": [
            "+x (torch.Tensor): Batch of input tensors (B, T, in_chans).",
            "+torch.Tensor: Batch of output tensors (B, T, hidden_chans)."
        ],
        "core_change": "-x (torch.Tensor): Batch of input tensors (B, ..., in_chans). +x (torch.Tensor): Batch of input tensors (B, T, in_chans). -torch.Tensor: Batch of output tensors (B, `*`, hidden_chans). +torch.Tensor: Batch of output tensors (B, T, hidden_chans).",
        "core_API": "relu"
    },
    {
        "commit_hash": "9bda1eb5bb6f644be3112e942541ac9f0fa12a8c",
        "index": "ffd06ffa..bbf6c943 100644",
        "commit_message": "sample points device fix\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SamplePoints(object):",
            "sample = torch.multinomial(prob, self.num, replacement=True)",
            "face = face[:, sample]",
            "",
            "-        frac = torch.rand(self.num, 2)",
            "+        frac = torch.rand(self.num, 2, device=pos.device)",
            "mask = frac.sum(dim=-1) > 1",
            "frac[mask] = 1 - frac[mask]"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=1051576)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=1051577)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=1051578)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1051579)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=1051580)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'pos'), position=0, insert_id=1051581)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1051582)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=1051583)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 5878,
        "neg_line": [
            "-frac = torch.rand(self.num, 2)"
        ],
        "pos_line": [
            "+frac = torch.rand(self.num, 2, device=pos.device)"
        ],
        "core_change": "-frac = torch.rand(self.num, 2) +frac = torch.rand(self.num, 2, device=pos.device)",
        "core_API": "multinomial"
    },
    {
        "commit_hash": "ece762443e4269e50cc1a348838e63e4a499575d",
        "index": "06581e732..6cfb0bb62 100644",
        "commit_message": "Fix incorrect comments about atten mask for pytorch backend (#18728)\n\n* fix incorrect comments about atten mask\n\n* typo\n\n* Update for CodeGen\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class CodeGenModel(CodeGenPreTrainedModel):",
            "",
            "# Since attention_mask is 1.0 for positions we want to attend and 0.0 for",
            "# masked positions, this operation will create a tensor which is 0.0 for",
            "-            # positions we want to attend and -10000.0 for masked positions.",
            "+            # positions we want to attend and the dtype's smallest value for masked positions.",
            "# Since we are adding it to the raw scores before the softmax, this is",
            "# effectively the same as removing these entirely.",
            "attention_mask = attention_mask.to(dtype=self.dtype)  # fp16 compatibility",
            "-            attention_mask = (1.0 - attention_mask) * -10000.0",
            "+            attention_mask = (1.0 - attention_mask) * torch.finfo(self.dtype).min",
            "",
            "# Prepare head mask if needed",
            "# 1.0 in head_mask indicate we keep the head"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=binary_operator), node=('attribute', None), position=2, insert_id=1190142)",
            "Insert(target_node=IN(type=attribute), node=('call', None), position=0, insert_id=1190143)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1190144)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'min'), position=2, insert_id=1190145)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1190146)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1190147)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1190148)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1190149)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'finfo'), position=2, insert_id=1190150)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1190151)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=1190152)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1190153)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=1190154)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1190155)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dtype'), position=2, insert_id=1190156)",
            "Delete(target_node=ASTNode(type=-, text=-))",
            "Delete(target_node=ASTNode(type=float, text=10000.0))",
            "Delete(target_node=ASTNode(type=unary_operator))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 18,
        "number": 5882,
        "neg_line": [
            "-# positions we want to attend and -10000.0 for masked positions.",
            "-attention_mask = (1.0 - attention_mask) * -10000.0"
        ],
        "pos_line": [
            "+# positions we want to attend and the dtype's smallest value for masked positions.",
            "+attention_mask = (1.0 - attention_mask) * torch.finfo(self.dtype).min"
        ],
        "core_change": "-# positions we want to attend and -10000.0 for masked positions. +# positions we want to attend and the dtype's smallest value for masked positions. -attention_mask = (1.0 - attention_mask) * -10000.0 +attention_mask = (1.0 - attention_mask) * torch.finfo(self.dtype).min",
        "core_API": "to"
    },
    {
        "commit_hash": "4a7b9508690d7d5b96faebe82a7c5bc164b87d22",
        "index": "b1a5d4c..a9d8b6e 100644",
        "commit_message": "Add exeption for jit C++ lib runtime error (#396)\n\n* Add exption for \"tuple appears in op that does not forward tuples\" runtime error\n\n* Fix unit tests\n\n* Remove redundant comment\n\n",
        "file": "tensorboardX.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class PytorchGraphTest(unittest.TestCase):",
            "class myLinear(torch.nn.Module):",
            "def __init__(self):",
            "super(myLinear, self).__init__()",
            "-                self.l = torch.nn.Linear(3, 5)",
            "+                self.linear = torch.nn.Linear(3, 5)",
            "",
            "def forward(self, x):",
            "-                return self.l(x)",
            "+                return self.linear(x)",
            "",
            "with SummaryWriter(comment='LinearModel') as w:",
            "w.add_graph(myLinear(), dummy_input, True)",
            "",
            "def test_wrong_input_size(self):",
            "print('expect error here:')",
            "-        with self.assertRaises(RuntimeError) as e_info:",
            "+        with self.assertRaises(TypeError):",
            "dummy_input = torch.rand(1, 9)",
            "model = torch.nn.Linear(3, 5)",
            "with SummaryWriter(comment='expect_error') as w:"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=l), value='linear')",
            "Move(target_node=ASTNode(type=with_item), node=ASTNode(type=call), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=l), value='linear')",
            "Update(target_node=ASTNode(type=identifier, text=RuntimeError), value='TypeError')",
            "Delete(target_node=ASTNode(type=as, text=as))",
            "Delete(target_node=ASTNode(type=identifier, text=e_info))",
            "Delete(target_node=ASTNode(type=as_pattern_target))",
            "Delete(target_node=ASTNode(type=as_pattern))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 8,
        "number": 5883,
        "neg_line": [
            "-self.l = torch.nn.Linear(3, 5)",
            "-return self.l(x)",
            "-with self.assertRaises(RuntimeError) as e_info:"
        ],
        "pos_line": [
            "+self.linear = torch.nn.Linear(3, 5)",
            "+return self.linear(x)",
            "+with self.assertRaises(TypeError):"
        ],
        "core_change": "-self.l = torch.nn.Linear(3, 5) +self.linear = torch.nn.Linear(3, 5) -return self.l(x) +return self.linear(x) -with self.assertRaises(RuntimeError) as e_info: +with self.assertRaises(TypeError):",
        "core_API": "Linear"
    },
    {
        "commit_hash": "ea63f95f0712dc2c21c3a62e41584765694c23f3",
        "index": "4a21a7d..0d7c80e 100644",
        "commit_message": "Fix for multi-GPU training.\n\nAnd cleaner path split in nucleus sample.\n",
        "file": "Mask_RCNN.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class MaskRCNN():",
            "# TODO: can this be optimized to avoid duplicating the anchors?",
            "anchors = np.broadcast_to(anchors, (config.BATCH_SIZE,) + anchors.shape)",
            "# A hack to get around Keras's bad support for constants",
            "-            anchors = KL.Lambda(lambda x: tf.constant(anchors), name=\"anchors\")(input_image)",
            "+            anchors = KL.Lambda(lambda x: tf.Variable(anchors), name=\"anchors\")(input_image)",
            "else:",
            "anchors = input_anchors"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=constant), value='Variable')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 5886,
        "neg_line": [
            "-anchors = KL.Lambda(lambda x: tf.constant(anchors), name=\"anchors\")(input_image)"
        ],
        "pos_line": [
            "+anchors = KL.Lambda(lambda x: tf.Variable(anchors), name=\"anchors\")(input_image)"
        ],
        "core_change": "-anchors = KL.Lambda(lambda x: tf.constant(anchors), name=\"anchors\")(input_image) +anchors = KL.Lambda(lambda x: tf.Variable(anchors), name=\"anchors\")(input_image)",
        "core_API": "broadcast_to"
    },
    {
        "commit_hash": "c51dc4f92755c67a83f3fc8a0bd6b3e64df199e4",
        "index": "13160f77c..5562efa28 100644",
        "commit_message": "[torch] remove deprecated uint8 in favor of bool (#21384)\n\n* uint8 -> bool\n\n* fix copies\n\n* style\n\n* update test modeling commen when checking attention buffers\n\n* style\n\n* use logical not on random mask instead of subtraction with 1\n\n* remove torch uint8\n\n* quality\n\n* remove modified modeling utils\n\n* Update based on review\n\nCo-authored-by: sgugger <sylvain.gugger@gmail.com>\n\n---------\n\nCo-authored-by: sgugger <sylvain.gugger@gmail.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def convert_megatron_checkpoint(sd_megatron, config):",
            "",
            "pf = \"model.language_model.encoder.layers.\"",
            "for i in range(layers):",
            "-        causal_mask = torch.tril(torch.ones((n_positions, n_positions), dtype=torch.uint8))",
            "+        causal_mask = torch.tril(torch.ones((n_positions, n_positions), dtype=torch.bool))",
            "causal_mask = causal_mask.view(1, 1, n_positions, n_positions)",
            "sd_hf[f\"transformer.h.{i}.attn.bias\"] = causal_mask",
            "sd_hf[f\"transformer.h.{i}.attn.masked_bias\"] = torch.tensor(-1e4, dtype=torch.bfloat16)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=uint8), value='bool')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 5893,
        "neg_line": [
            "-causal_mask = torch.tril(torch.ones((n_positions, n_positions), dtype=torch.uint8))"
        ],
        "pos_line": [
            "+causal_mask = torch.tril(torch.ones((n_positions, n_positions), dtype=torch.bool))"
        ],
        "core_change": "-causal_mask = torch.tril(torch.ones((n_positions, n_positions), dtype=torch.uint8)) +causal_mask = torch.tril(torch.ones((n_positions, n_positions), dtype=torch.bool))",
        "core_API": "tril"
    },
    {
        "commit_hash": "9336086ab5d232cccd9512333518cf4299528882",
        "index": "6e5d3c4c8..a8ae72d0b 100644",
        "commit_message": "prepare_seq2seq_batch makes labels/ decoder_input_ids made later. (#6654)\n\n* broken test\n\n* batch parity\n\n* tests pass\n\n* boom boom\n\n* boom boom\n\n* split out bart tokenizer tests\n\n* fix tests\n\n* boom boom\n\n* Fixed dataset bug\n\n* Fix marian\n\n* Undo extra\n\n* Get marian working\n\n* Fix t5 tok tests\n\n* Test passing\n\n* Cleanup\n\n* better assert msg\n\n* require torch\n\n* Fix mbart tests\n\n* undo extra decoder_attn_mask change\n\n* Fix import\n\n* pegasus tokenizer can ignore src_lang kwargs\n\n* unused kwarg test cov\n\n* boom boom\n\n* add todo for pegasus issue\n\n* cover one word translation edge case\n\n* Cleanup\n\n* doc\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class T5PreTrainedModel(PreTrainedModel):",
            "# replace possible -100 values in labels by `pad_token_id`",
            "shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)",
            "",
            "-        assert torch.all(shifted_input_ids >= 0).item(), \"Verify that `labels` has only positive values and -100\"",
            "+        assert torch.all(shifted_input_ids >= 0).item(), \"Verify that `shifted_input_ids` has only positive values\"",
            "",
            "return shifted_input_ids"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"Verify that `labels` has only positive values and -100\"), value='\"Verify that `shifted_input_ids` has only positive values\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 5899,
        "neg_line": [
            "-assert torch.all(shifted_input_ids >= 0).item(), \"Verify that `labels` has only positive values and -100\""
        ],
        "pos_line": [
            "+assert torch.all(shifted_input_ids >= 0).item(), \"Verify that `shifted_input_ids` has only positive values\""
        ],
        "core_change": "-assert torch.all(shifted_input_ids >= 0).item(), \"Verify that `labels` has only positive values and -100\" +assert torch.all(shifted_input_ids >= 0).item(), \"Verify that `shifted_input_ids` has only positive values\"",
        "core_API": "masked_fill_"
    },
    {
        "commit_hash": "ccd258d0956da89b1feca96448eff8e4969d405a",
        "index": "52e8fc74..b9cfff0a 100644",
        "commit_message": "fixes for half precision in imgwrp (#1723)\n\n\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def invert_affine_transform(matrix: torch.Tensor) -> torch.Tensor:",
            "raise ValueError(f\"Input matrix must be a Bx2x3 tensor. Got {matrix.shape}\")",
            "",
            "matrix_tmp: torch.Tensor = convert_affinematrix_to_homography(matrix)",
            "-    matrix_inv: torch.Tensor = torch.inverse(matrix_tmp)",
            "+    matrix_inv: torch.Tensor = _torch_inverse_cast(matrix_tmp)",
            "",
            "return matrix_inv[..., :2, :3]"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=torch), value='_torch_inverse_cast')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=torch), position=0)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=inverse))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 5900,
        "neg_line": [
            "-matrix_inv: torch.Tensor = torch.inverse(matrix_tmp)"
        ],
        "pos_line": [
            "+matrix_inv: torch.Tensor = _torch_inverse_cast(matrix_tmp)"
        ],
        "core_change": "-matrix_inv: torch.Tensor = torch.inverse(matrix_tmp) +matrix_inv: torch.Tensor = _torch_inverse_cast(matrix_tmp)",
        "core_API": "inverse"
    },
    {
        "commit_hash": "2e1c4db1263f5202a4d88bb27fae1984e09054d7",
        "index": "25ba31b16..659a49d9c 100644",
        "commit_message": "Fixed formatting of 1 file\n\n",
        "file": "PySyft.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_lstm(workers):",
            "lstm = nn.LSTM(3, 3)",
            "lstm.send(bob)",
            "inputs = torch.randn(5, 1, 3).send(bob)",
            "-    hidden = (torch.randn(1, 1, 3).send(bob),",
            "-        torch.randn(1, 1, 3).send(bob), )  # clean out hidden state",
            "+    hidden = (",
            "+        torch.randn(1, 1, 3).send(bob),",
            "+        torch.randn(1, 1, 3).send(bob),",
            "+    )  # clean out hidden state",
            "out, hidden = lstm(inputs, hidden)",
            "assert out.shape == torch.Size([5, 1, 3])"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 4,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 5907,
        "neg_line": [
            "-hidden = (torch.randn(1, 1, 3).send(bob),",
            "-torch.randn(1, 1, 3).send(bob), )  # clean out hidden state"
        ],
        "pos_line": [
            "+hidden = (",
            "+torch.randn(1, 1, 3).send(bob),",
            "+torch.randn(1, 1, 3).send(bob),",
            "+)  # clean out hidden state"
        ],
        "core_change": "-hidden = (torch.randn(1, 1, 3).send(bob), -torch.randn(1, 1, 3).send(bob), )  # clean out hidden state +hidden = ( +torch.randn(1, 1, 3).send(bob), +torch.randn(1, 1, 3).send(bob), +)  # clean out hidden state",
        "core_API": "LSTM"
    },
    {
        "commit_hash": "16f0b7d72c6d4e122957392c342b074aa2c5c519",
        "index": "17ab62e4a..9c81937f4 100644",
        "commit_message": "Update ONNX docs (#14904)\n\n* Remove docs for deprecated ONNX export\n\n* Tidy up the CLI help messages\n\n* Revamp ONNX docs\n\n* Update auto-config table\n\n* Use DistilBERT as example for consistency\n\n* Wrap up first pass at ONNX docs\n\n* Fix table check\n\n* Add tweaks and introduction\n\n* Add cross-ref\n\n* Fix missing import\n\n* Fix style\n\n* Add permalinks to ONNX configs\n\n* Clarify role of OrderedDict\n\n* Update docs/source/serialization.mdx\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n\n* Add doctest syntax to code blocks\n\n* Remove permalinks\n\n* Revert \"Remove permalinks\"\n\nThis reverts commit 099701daf0db27823457867938efdb2d4f22a7c1.\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "models",
        "change": [
            "def get_onnx_model_list():",
            "",
            "",
            "def check_onnx_model_list(overwrite=False):",
            "-    \"\"\"Check the model list in the serialization.rst is consistent with the state of the lib and maybe `overwrite`.\"\"\"",
            "+    \"\"\"Check the model list in the serialization.mdx is consistent with the state of the lib and maybe `overwrite`.\"\"\"",
            "current_list, start_index, end_index, lines = _find_text_in_file(",
            "filename=os.path.join(PATH_TO_DOCS, \"serialization.mdx\"),",
            "start_prompt=\"<!--This table is automatically generated by make style, do not fill manually!-->\",",
            "-        end_prompt=\"This conversion is handled with the PyTorch version of models \",",
            "+        end_prompt=\"The ONNX conversion is supported for the PyTorch versions of the models.\",",
            ")",
            "new_list = get_onnx_model_list()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"\"\"Check the model list in the serialization.rst is consistent with the state of the lib and maybe `overwrite`.\"\"\"), value='\"\"\"Check the model list in the serialization.mdx is consistent with the state of the lib and maybe `overwrite`.\"\"\"')",
            "Update(target_node=ASTNode(type=string, text=\"This conversion is handled with the PyTorch version of models \"), value='\"The ONNX conversion is supported for the PyTorch versions of the models.\"')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 5909,
        "neg_line": [
            "-\"\"\"Check the model list in the serialization.rst is consistent with the state of the lib and maybe `overwrite`.\"\"\"",
            "-end_prompt=\"This conversion is handled with the PyTorch version of models \","
        ],
        "pos_line": [
            "+\"\"\"Check the model list in the serialization.mdx is consistent with the state of the lib and maybe `overwrite`.\"\"\"",
            "+end_prompt=\"The ONNX conversion is supported for the PyTorch versions of the models.\","
        ],
        "core_change": "-\"\"\"Check the model list in the serialization.rst is consistent with the state of the lib and maybe `overwrite`.\"\"\" +\"\"\"Check the model list in the serialization.mdx is consistent with the state of the lib and maybe `overwrite`.\"\"\" -end_prompt=\"This conversion is handled with the PyTorch version of models \", +end_prompt=\"The ONNX conversion is supported for the PyTorch versions of the models.\",",
        "core_API": "join"
    },
    {
        "commit_hash": "5a34b5559895c687a78c532f94824fcd73de765d",
        "index": "3de64981..345684ca 100644",
        "commit_message": "Deprecate LKJCorrCholesky distribution (#2771)\n\n* Deprecate LKJCorrCholesky distribution\n\n* fix tests; add FutureWarning\n\n* Fix failing examples/lkj.py\n\n* Fix deprecation warning in bart example\n\n* Address review comment\n\nCo-authored-by: Neeraj Pradhan <neerajprad@fb.com>\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_empty_model_error():",
            "",
            "def test_unpack_latent():",
            "def model():",
            "-        return pyro.sample('x', dist.LKJCorrCholesky(2, torch.tensor(1.)))",
            "+        return pyro.sample('x', dist.LKJCholesky(2, torch.tensor(1.)))",
            "",
            "guide = AutoDiagonalNormal(model)",
            "assert guide()['x'].shape == model().shape"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=LKJCorrCholesky), value='LKJCholesky')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 5915,
        "neg_line": [
            "-return pyro.sample('x', dist.LKJCorrCholesky(2, torch.tensor(1.)))"
        ],
        "pos_line": [
            "+return pyro.sample('x', dist.LKJCholesky(2, torch.tensor(1.)))"
        ],
        "core_change": "-return pyro.sample('x', dist.LKJCorrCholesky(2, torch.tensor(1.))) +return pyro.sample('x', dist.LKJCholesky(2, torch.tensor(1.)))",
        "core_API": "sample"
    },
    {
        "commit_hash": "4b06cbf728ff49054f3e35f74d32c66b873eb834",
        "index": "2d5ac689..254039b9 100644",
        "commit_message": "Fix gradient bug for `distance_transform` (#1584)\n\n* Fix gradient bug for distance_transform.\n\n* Move signal_ones outside the loop && Add another grad check for distance_transform.\n\n* Move signal_ones outside the loop && Add another grad check for distance_transform.\n\nMove signal_ones outside the loop && Add another grad check for distance_transform.\n\n* Removing try/catch block of distance_transform grad test.\n\nCo-authored-by: guangpan.cd <guangpan.cd@alibaba-inc.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def distance_transform(",
            "",
            "offset: int = i * kernel_size // 2",
            "out += (offset + cdt) * mask",
            "-        boundary[mask == 1] = 1",
            "+        boundary = torch.where(mask == 1, signal_ones, boundary)",
            "",
            "return out"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=ERROR), node=('identifier', 'return'), position=4, insert_id=406958)",
            "Insert(target_node=ASTNode(type=binary_operator), node=('call', None), position=2, insert_id=406959)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=406960)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=406961)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=mask), position=0)",
            "Insert(target_node=IN(type=attribute), node=('ERROR', None), position=1, insert_id=406962)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=2, insert_id=406963)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'where'), position=3, insert_id=406964)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=406965)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=comparison_operator), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=406966)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'signal_ones'), position=3, insert_id=406967)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=4, insert_id=406968)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'boundary'), position=5, insert_id=406969)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=6, insert_id=406970)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=identifier, text=boundary), position=0)",
            "Insert(target_node=IN(type=ERROR), node=('=', '='), position=1, insert_id=406971)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'torch'), position=2, insert_id=406972)",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=return, text=return))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 25,
        "number": 5917,
        "neg_line": [
            "-boundary[mask == 1] = 1"
        ],
        "pos_line": [
            "+boundary = torch.where(mask == 1, signal_ones, boundary)"
        ],
        "core_change": "-boundary[mask == 1] = 1 +boundary = torch.where(mask == 1, signal_ones, boundary)",
        "core_API": "where"
    },
    {
        "commit_hash": "4ad831ac851a6e68dbf18b3b395af7a30bcac115",
        "index": "e395ded4..2b498084 100755",
        "commit_message": "[FasterRCNN] fix rgb/bgr in visualization (fix #519)\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Model(ModelDesc):",
            "fg_inds_wrt_sample = tf.reshape(tf.where(rcnn_labels > 0), [-1])   # fg inds w.r.t all samples",
            "fg_sampled_boxes = tf.gather(rcnn_sampled_boxes, fg_inds_wrt_sample)",
            "",
            "-            # TODO move to models",
            "with tf.name_scope('fg_sample_patch_viz'):",
            "fg_sampled_patches = crop_and_resize(",
            "image, fg_sampled_boxes,",
            "tf.zeros_like(fg_inds_wrt_sample, dtype=tf.int32), 300)",
            "fg_sampled_patches = tf.transpose(fg_sampled_patches, [0, 2, 3, 1])",
            "+                fg_sampled_patches = tf.reverse(fg_sampled_patches, axis=-1)  # BGR->RGB",
            "tf.summary.image('viz', fg_sampled_patches, max_outputs=30)",
            "",
            "matched_gt_boxes = tf.gather(gt_boxes, fg_inds_wrt_gt)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=6, insert_id=2292545)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=2292546)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'fg_sampled_patches'), position=0, insert_id=2292547)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=2292548)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=2292549)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2292550)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2292551)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2292552)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2292553)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'reverse'), position=2, insert_id=2292554)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2292555)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'fg_sampled_patches'), position=1, insert_id=2292556)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=2292557)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=3, insert_id=2292558)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=2292559)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'axis'), position=0, insert_id=2292560)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2292561)",
            "Insert(target_node=IN(type=keyword_argument), node=('unary_operator', '-1'), position=2, insert_id=2292562)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 18,
        "number": 5918,
        "neg_line": [
            "-# TODO move to models"
        ],
        "pos_line": [
            "+fg_sampled_patches = tf.reverse(fg_sampled_patches, axis=-1)  # BGR->RGB"
        ],
        "core_change": "-# TODO move to models +fg_sampled_patches = tf.reverse(fg_sampled_patches, axis=-1)  # BGR->RGB",
        "core_API": "reshape"
    },
    {
        "commit_hash": "c4c532d25e012dbe6ab1ac14bca75e53e0acc621",
        "index": "6cdace38..5d0dbf6b 100644",
        "commit_message": "pylint -> flake8 (#3288)\n\n* pylint\n\n* update pylint\n\n* undo a lot of the raise / else\n\n* add bound on typed-ast\n\n* first mypy fixes\n\n* new flag\n\n* fix mypy errors\n\n* requirements.txt\n\n* pylint -> flake8\n\n* mypy 0.720 -> mypy 0.730\n\n* add back erroneously removed initial newline\n\n* remove .pylintrc\n\n* remove pylintrc from Dockerfile\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class IntraSentenceAttentionEncoder(Seq2SeqEncoder):",
            "return False",
            "",
            "@overrides",
            "-    def forward(self, tokens: torch.Tensor, mask: torch.Tensor):  # pylint: disable=arguments-differ",
            "+    def forward(self, tokens: torch.Tensor, mask: torch.Tensor):",
            "batch_size, sequence_length, _ = tokens.size()",
            "# Shape: (batch_size, sequence_length, sequence_length)",
            "similarity_matrix = self._matrix_attention(tokens, tokens)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 5920,
        "neg_line": [
            "-def forward(self, tokens: torch.Tensor, mask: torch.Tensor):  # pylint: disable=arguments-differ"
        ],
        "pos_line": [
            "+def forward(self, tokens: torch.Tensor, mask: torch.Tensor):"
        ],
        "core_change": "-def forward(self, tokens: torch.Tensor, mask: torch.Tensor):  # pylint: disable=arguments-differ +def forward(self, tokens: torch.Tensor, mask: torch.Tensor):",
        "core_API": "size"
    },
    {
        "commit_hash": "f282991db0ca822a8b942a36013d0ff830ae1c93",
        "index": "4212e20b8..e35a9ab0f 100644",
        "commit_message": "Fix src and test code\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def recog(args):",
            "word_dict = rnnlm_args.char_list_dict",
            "char_dict = {x: i for i, x in enumerate(train_args.char_list)}",
            "word_rnnlm = lm_pytorch.ClassifierWithState(lm_pytorch.RNNLM(",
            "-            len(word_dict), rnnlm_args.unit))",
            "+            len(word_dict), rnnlm_args.layers, rnnlm_args.units))",
            "torch_load(args.word_rnnlm, word_rnnlm)",
            "word_rnnlm.eval()"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=179776)",
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=5, insert_id=179777)",
            "Update(target_node=ASTNode(type=identifier, text=unit), value='layers')",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'rnnlm_args'), position=0, insert_id=179778)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=179779)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'units'), position=2, insert_id=179780)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 5923,
        "neg_line": [
            "-len(word_dict), rnnlm_args.unit))"
        ],
        "pos_line": [
            "+len(word_dict), rnnlm_args.layers, rnnlm_args.units))"
        ],
        "core_change": "-len(word_dict), rnnlm_args.unit)) +len(word_dict), rnnlm_args.layers, rnnlm_args.units))",
        "core_API": "ClassifierWithState"
    },
    {
        "commit_hash": "77534faf1a43447bf66b243dbdee792baca7d6c6",
        "index": "88a0ea42..cc1bf1f6 100644",
        "commit_message": "summaries updated and fixed\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Activation(Layer):",
            "x = tf.nn.leaky_relu(features=x, alpha=0.2)  # alpha argument???",
            "",
            "elif self.nonlinearity == 'none':",
            "-            x = tf.identity(input=x)",
            "+            pass",
            "",
            "elif self.nonlinearity == 'relu':",
            "x = tf.nn.relu(features=x)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('pass_statement', None), position=3, insert_id=2232978)",
            "Insert(target_node=ASTNode(type=ERROR), node=(':', ':'), position=2, insert_id=2232979)",
            "Insert(target_node=IN(type=pass_statement), node=('pass', 'pass'), position=0, insert_id=2232980)",
            "Move(target_node=ASTNode(type=comparison_operator), node=ASTNode(type=string, text='none'), position=2)",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=identifier, text=x))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=identity))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=input))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=x))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 21,
        "number": 5925,
        "neg_line": [
            "-x = tf.identity(input=x)"
        ],
        "pos_line": [
            "+pass"
        ],
        "core_change": "-x = tf.identity(input=x) +pass",
        "core_API": "leaky_relu"
    },
    {
        "commit_hash": "2e0b961a0e3233eba39f9ea473cb4f56dcdf3bd8",
        "index": "226f08fe..a71e7801 100644",
        "commit_message": "fix import_user_module (#3144)\n\nSummary:\n## What does this PR do?\nAvoid throwing ValueError when attempting to load a user defined module from common.user_dir that has the same module name and same module path as some loaded module. This occurs when a job is preempted and restarts using submitit_slurm\n\nX-link: https://github.com/fairinternal/fairseq-py/pull/3144\n\nReviewed By: Abdel-rahmanMohamed\n\nDifferential Revision: D34521450\n\nPulled By: wnhsu\n\nfbshipit-source-id: eed00d4238a66dc524eee400a55ad2c011e1543c\n\n",
        "file": "fairseq.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class BeamSearch(Search):",
            "scores_buf = top_prediction[0]",
            "indices_buf = top_prediction[1]",
            "# Project back into relative indices and beams",
            "-        beams_buf = torch.div(indices_buf, vocab_size, rounding_mode='trunc')",
            "+        beams_buf = torch.div(indices_buf, vocab_size, rounding_mode=\"trunc\")",
            "indices_buf = indices_buf.fmod(vocab_size)",
            "",
            "# At this point, beams_buf and indices_buf are single-dim and contain relative indices"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text='trunc'), value='\"trunc\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 5926,
        "neg_line": [
            "-beams_buf = torch.div(indices_buf, vocab_size, rounding_mode='trunc')"
        ],
        "pos_line": [
            "+beams_buf = torch.div(indices_buf, vocab_size, rounding_mode=\"trunc\")"
        ],
        "core_change": "-beams_buf = torch.div(indices_buf, vocab_size, rounding_mode='trunc') +beams_buf = torch.div(indices_buf, vocab_size, rounding_mode=\"trunc\")",
        "core_API": "div"
    },
    {
        "commit_hash": "f2f5331d00a32d97ea4f4e5306c8414d16e1f23d",
        "index": "f0e6f71c..690053bc 100644",
        "commit_message": "Standardize on `use_causal_mask` in `call()` and mark `causal` argument in constructor as deprecated.\n\nAlso remove testing of causal masks from `layer_correctness` since not needed and changing arguments in call doesn't fit easily into this test fixture.\n\nPiperOrigin-RevId: 459791640\n\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "keras",
        "change": [
            "class AdditiveAttentionTest(tf.test.TestCase, parameterized.TestCase):",
            "q = tf.cast(tf.random.uniform((2, 3, 4), seed=1), \"float16\")",
            "v = tf.cast(tf.random.uniform((2, 3, 4), seed=2), \"float16\")",
            "k = tf.cast(tf.random.uniform((2, 3, 4), seed=3), \"float16\")",
            "-            layer = keras.layers.AdditiveAttention(causal=True)",
            "-            _ = layer([q, v, k])",
            "+            layer = keras.layers.AdditiveAttention()",
            "+            _ = layer([q, v, k], use_causal_mask=True)",
            "",
            "",
            "if __name__ == \"__main__\":"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=2505924)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=2505925)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'use_causal_mask'), position=0, insert_id=2505926)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2505927)",
            "Insert(target_node=IN(type=keyword_argument), node=('true', 'True'), position=2, insert_id=2505928)",
            "Delete(target_node=ASTNode(type=identifier, text=causal))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=true, text=True))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 9,
        "number": 5927,
        "neg_line": [
            "-layer = keras.layers.AdditiveAttention(causal=True)",
            "-_ = layer([q, v, k])"
        ],
        "pos_line": [
            "+layer = keras.layers.AdditiveAttention()",
            "+_ = layer([q, v, k], use_causal_mask=True)"
        ],
        "core_change": "-layer = keras.layers.AdditiveAttention(causal=True) -_ = layer([q, v, k]) +layer = keras.layers.AdditiveAttention() +_ = layer([q, v, k], use_causal_mask=True)",
        "core_API": "cast"
    },
    {
        "commit_hash": "a82ad3a617b6c8192688c022c7df3fde71fe91f7",
        "index": "fd8c0829..820d5a0c 100644",
        "commit_message": "Fix tensorflow_backend  deprecation warning (#9488)\n\nWARNING:tensorflow:From .../lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3148: calling l2_normalize (from tensorflow.python.ops.nn_impl) with dim is deprecated and will be removed in a future version.\nInstructions for updating:\ndim is deprecated, use axis instead\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def l2_normalize(x, axis=None):",
            "# Returns",
            "A tensor.",
            "\"\"\"",
            "-    return tf.nn.l2_normalize(x, dim=axis)",
            "+    return tf.nn.l2_normalize(x, axis=axis)",
            "",
            "",
            "def in_top_k(predictions, targets, k):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=dim), value='axis')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 5928,
        "neg_line": [
            "-return tf.nn.l2_normalize(x, dim=axis)"
        ],
        "pos_line": [
            "+return tf.nn.l2_normalize(x, axis=axis)"
        ],
        "core_change": "-return tf.nn.l2_normalize(x, dim=axis) +return tf.nn.l2_normalize(x, axis=axis)",
        "core_API": "l2_normalize"
    },
    {
        "commit_hash": "5c87753e88b681ef71688b73c68f84eea456d53c",
        "index": "f6385747..c8ad410d 100644",
        "commit_message": "glow-tts fix for saving inverse weight\n\n",
        "file": "TTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class InvConvNear(nn.Module):",
            "return z, logdet",
            "",
            "def store_inverse(self):",
            "-        self.weight_inv = torch.inverse(",
            "+        weight_inv = torch.inverse(",
            "self.weight.float()).to(dtype=self.weight.dtype)",
            "+        self.weight_inv = nn.Parameter(weight_inv, requires_grad=False)",
            "",
            "",
            "class CouplingBlock(nn.Module):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=block), node=('expression_statement', None), position=1, insert_id=1554759)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1554760)",
            "Insert(target_node=ASTNode(type=assignment), node=('identifier', 'weight_inv'), position=0, insert_id=1554761)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1554762)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=1554763)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1554764)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1554765)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'nn'), position=0, insert_id=1554766)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1554767)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Parameter'), position=2, insert_id=1554768)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1554769)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'weight_inv'), position=1, insert_id=1554770)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=1554771)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1554772)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=1554773)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'requires_grad'), position=0, insert_id=1554774)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1554775)",
            "Insert(target_node=IN(type=keyword_argument), node=('false', 'False'), position=2, insert_id=1554776)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 19,
        "number": 5932,
        "neg_line": [
            "-self.weight_inv = torch.inverse("
        ],
        "pos_line": [
            "+weight_inv = torch.inverse(",
            "+self.weight_inv = nn.Parameter(weight_inv, requires_grad=False)"
        ],
        "core_change": "-self.weight_inv = torch.inverse( +weight_inv = torch.inverse( +self.weight_inv = nn.Parameter(weight_inv, requires_grad=False)",
        "core_API": "inverse"
    },
    {
        "commit_hash": "03c1c9c83771274f07eda4e7e84c7eeff2b3414b",
        "index": "2fffb4fb..2f93717a 100644",
        "commit_message": "Fix PDE grid batching\n\nPiperOrigin-RevId: 468659748\n\n",
        "file": "tf-quant-finance.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class ParabolicEquationStepperTest(tf.test.TestCase, parameterized.TestCase):",
            "boundary_conditions=[(None, upper_boundary_fn)])[0]",
            "",
            "true_values = tf.math.exp(final_t + grid[0])",
            "-    print('est_values: ', est_values)",
            "self.assertAllClose(",
            "est_values, true_values, atol=1e-2, rtol=1e-2)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=print))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=string, text='est_values: '))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=est_values))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 5935,
        "neg_line": [
            "-print('est_values: ', est_values)"
        ],
        "pos_line": [],
        "core_change": "-print('est_values: ', est_values)",
        "core_API": "exp"
    },
    {
        "commit_hash": "e85855f2c408f65a4aaf5d15baab6ca90fd26050",
        "index": "164dc7432..d1650d41a 100644",
        "commit_message": "Fix ALBERT exports with pretraining + sp classifier; Fix naming for ALBERT TF models\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "models",
        "change": [
            "import logging",
            "logger = logging.getLogger(__name__)",
            "",
            "TF_ALBERT_PRETRAINED_MODEL_ARCHIVE_MAP = {",
            "-    'albert-base-v1': \"https://s3.amazonaws.com/models.huggingface.co/bert/albert-base-tf_model.h5\",",
            "-    'albert-large-v1': \"https://s3.amazonaws.com/models.huggingface.co/bert/albert-large-tf_model.h5\",",
            "-    'albert-xlarge-v1': \"https://s3.amazonaws.com/models.huggingface.co/bert/albert-xlarge-tf_model.h5\",",
            "-    'albert-xxlarge-v1': \"https://s3.amazonaws.com/models.huggingface.co/bert/albert-xxlarge-tf_model.h5\",",
            "+    'albert-base-v1': \"https://s3.amazonaws.com/models.huggingface.co/bert/albert-base-v1-tf_model.h5\",",
            "+    'albert-large-v1': \"https://s3.amazonaws.com/models.huggingface.co/bert/albert-large-v1-tf_model.h5\",",
            "+    'albert-xlarge-v1': \"https://s3.amazonaws.com/models.huggingface.co/bert/albert-xlarge-v1-tf_model.h5\",",
            "+    'albert-xxlarge-v1': \"https://s3.amazonaws.com/models.huggingface.co/bert/albert-xxlarge-v1-tf_model.h5\",",
            "'albert-base-v2': \"https://s3.amazonaws.com/models.huggingface.co/bert/albert-base-v2-tf_model.h5\",",
            "'albert-large-v2': \"https://s3.amazonaws.com/models.huggingface.co/bert/albert-large-v2-tf_model.h5\",",
            "'albert-xlarge-v2': \"https://s3.amazonaws.com/models.huggingface.co/bert/albert-xlarge-v2-tf_model.h5\","
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"https://s3.amazonaws.com/models.huggingface.co/bert/albert-base-tf_model.h5\"), value='\"https://s3.amazonaws.com/models.huggingface.co/bert/albert-base-v1-tf_model.h5\"')",
            "Update(target_node=ASTNode(type=string, text=\"https://s3.amazonaws.com/models.huggingface.co/bert/albert-large-tf_model.h5\"), value='\"https://s3.amazonaws.com/models.huggingface.co/bert/albert-large-v1-tf_model.h5\"')",
            "Update(target_node=ASTNode(type=string, text=\"https://s3.amazonaws.com/models.huggingface.co/bert/albert-xlarge-tf_model.h5\"), value='\"https://s3.amazonaws.com/models.huggingface.co/bert/albert-xlarge-v1-tf_model.h5\"')",
            "Update(target_node=ASTNode(type=string, text=\"https://s3.amazonaws.com/models.huggingface.co/bert/albert-xxlarge-tf_model.h5\"), value='\"https://s3.amazonaws.com/models.huggingface.co/bert/albert-xxlarge-v1-tf_model.h5\"')"
        ],
        "plus_line": 4,
        "minus_line": 4,
        "AST_diff_line": 4,
        "number": 5937,
        "neg_line": [
            "-'albert-base-v1': \"https://s3.amazonaws.com/models.huggingface.co/bert/albert-base-tf_model.h5\",",
            "-'albert-large-v1': \"https://s3.amazonaws.com/models.huggingface.co/bert/albert-large-tf_model.h5\",",
            "-'albert-xlarge-v1': \"https://s3.amazonaws.com/models.huggingface.co/bert/albert-xlarge-tf_model.h5\",",
            "-'albert-xxlarge-v1': \"https://s3.amazonaws.com/models.huggingface.co/bert/albert-xxlarge-tf_model.h5\","
        ],
        "pos_line": [
            "+'albert-base-v1': \"https://s3.amazonaws.com/models.huggingface.co/bert/albert-base-v1-tf_model.h5\",",
            "+'albert-large-v1': \"https://s3.amazonaws.com/models.huggingface.co/bert/albert-large-v1-tf_model.h5\",",
            "+'albert-xlarge-v1': \"https://s3.amazonaws.com/models.huggingface.co/bert/albert-xlarge-v1-tf_model.h5\",",
            "+'albert-xxlarge-v1': \"https://s3.amazonaws.com/models.huggingface.co/bert/albert-xxlarge-v1-tf_model.h5\","
        ],
        "core_change": "-'albert-base-v1': \"https://s3.amazonaws.com/models.huggingface.co/bert/albert-base-tf_model.h5\", -'albert-large-v1': \"https://s3.amazonaws.com/models.huggingface.co/bert/albert-large-tf_model.h5\", -'albert-xlarge-v1': \"https://s3.amazonaws.com/models.huggingface.co/bert/albert-xlarge-tf_model.h5\", -'albert-xxlarge-v1': \"https://s3.amazonaws.com/models.huggingface.co/bert/albert-xxlarge-tf_model.h5\", +'albert-base-v1': \"https://s3.amazonaws.com/models.huggingface.co/bert/albert-base-v1-tf_model.h5\", +'albert-large-v1': \"https://s3.amazonaws.com/models.huggingface.co/bert/albert-large-v1-tf_model.h5\", +'albert-xlarge-v1': \"https://s3.amazonaws.com/models.huggingface.co/bert/albert-xlarge-v1-tf_model.h5\", +'albert-xxlarge-v1': \"https://s3.amazonaws.com/models.huggingface.co/bert/albert-xxlarge-v1-tf_model.h5\",",
        "core_API": "getLogger"
    },
    {
        "commit_hash": "584d63964324f974d833b16cb91300d4accd89db",
        "index": "2c678c6e38..b8c8c52cf9 100644",
        "commit_message": "Fixed failing test for searching where (#4744)\n\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def where(",
            "out: Optional[torch.Tensor] = None,",
            ") -> torch.Tensor:",
            "x1, x2 = ivy.promote_types_of_inputs(x1, x2)",
            "-    return torch.where(condition, x1, x2)",
            "+    return torch.where(condition, x1, x2).to(dtype=x1.dtype)",
            "",
            "",
            "# Extra #"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=324029)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=324030)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=324031)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to'), position=2, insert_id=324032)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=324033)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=1, insert_id=324034)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=324035)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=324036)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=324037)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=324038)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'x1'), position=0, insert_id=324039)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=324040)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dtype'), position=2, insert_id=324041)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 14,
        "number": 5945,
        "neg_line": [
            "-return torch.where(condition, x1, x2)"
        ],
        "pos_line": [
            "+return torch.where(condition, x1, x2).to(dtype=x1.dtype)"
        ],
        "core_change": "-return torch.where(condition, x1, x2) +return torch.where(condition, x1, x2).to(dtype=x1.dtype)",
        "core_API": "promote_types_of_inputs"
    },
    {
        "commit_hash": "1abaf6275bbf718f369929b091a9c7bb949b386c",
        "index": "8a9f8452..004a6287 100644",
        "commit_message": "[Compression v2] bugfix & improvement (#4307)\n\n\n",
        "file": "nni.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SlimPruner(BasicPruner):",
            "def patched_criterion(input_tensor: Tensor, target: Tensor):",
            "sum_l1 = 0",
            "for _, wrapper in self.get_modules_wrapper().items():",
            "-                sum_l1 += torch.norm(wrapper.module.weight.data, p=1)",
            "+                sum_l1 += torch.norm(wrapper.module.weight, p=1)",
            "return criterion(input_tensor, target) + self._scale * sum_l1",
            "return patched_criterion"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=IN(type=root), node=ASTNode(type=attribute), position=0)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=data))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 3,
        "number": 5946,
        "neg_line": [
            "-sum_l1 += torch.norm(wrapper.module.weight.data, p=1)"
        ],
        "pos_line": [
            "+sum_l1 += torch.norm(wrapper.module.weight, p=1)"
        ],
        "core_change": "-sum_l1 += torch.norm(wrapper.module.weight.data, p=1) +sum_l1 += torch.norm(wrapper.module.weight, p=1)",
        "core_API": "get_modules_wrapper"
    },
    {
        "commit_hash": "38f7461df3fe51308a62a81e4a0e7770a38d7125",
        "index": "27fe1cadf..1daf1bf32 100644",
        "commit_message": "[TFT5, Cache] Add cache to TFT5 (#3772)\n\n* correct gpt2 test inputs\n\n* make style\n\n* delete modeling_gpt2 change in test file\n\n* translate from pytorch\n\n* correct tests\n\n* fix conflicts\n\n* fix conflicts\n\n* fix conflicts\n\n* fix conflicts\n\n* make tensorflow t5 caching work\n\n* make style\n\n* clean reorder cache\n\n* remove unnecessary spaces\n\n* fix test\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class T5ModelTest(ModelTesterMixin, unittest.TestCase):",
            "def create_t5_and_check_t5_generate_with_past_key_value_states(",
            "self, config, input_ids, decoder_input_ids, attention_mask, decoder_attention_mask, lm_labels,",
            "):",
            "-            config.num_layers = 1",
            "model = T5ForConditionalGeneration(config=config)",
            "model.to(torch_device)",
            "model.eval()"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('block', ''), position=0, insert_id=1240214)",
            "Delete(target_node=ASTNode(type=identifier, text=config))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=num_layers))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=block))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 5949,
        "neg_line": [
            "-config.num_layers = 1"
        ],
        "pos_line": [],
        "core_change": "-config.num_layers = 1",
        "core_API": "to"
    },
    {
        "commit_hash": "01b293b84fd2b1eeb80a01afe44d341d9c9c6486",
        "index": "1fe93274..d5074ff9 100644",
        "commit_message": "0.4.0 fixes\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_add_self_loops():",
            "",
            "",
            "def test_remove_self_loops():",
            "-    row = torch.LongTensor([1, 0, 1, 0, 2, 1])",
            "-    col = torch.LongTensor([0, 1, 1, 1, 2, 0])",
            "+    row = torch.tensor([1, 0, 1, 0, 2, 1])",
            "+    col = torch.tensor([0, 1, 1, 1, 2, 0])",
            "expected_output = [[1, 0, 0, 1], [0, 1, 1, 0]]",
            "",
            "output = remove_self_loops(torch.stack([row, col], dim=0))"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=LongTensor), value='tensor')",
            "Update(target_node=ASTNode(type=identifier, text=LongTensor), value='tensor')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 5954,
        "neg_line": [
            "-row = torch.LongTensor([1, 0, 1, 0, 2, 1])",
            "-col = torch.LongTensor([0, 1, 1, 1, 2, 0])"
        ],
        "pos_line": [
            "+row = torch.tensor([1, 0, 1, 0, 2, 1])",
            "+col = torch.tensor([0, 1, 1, 1, 2, 0])"
        ],
        "core_change": "-row = torch.LongTensor([1, 0, 1, 0, 2, 1]) -col = torch.LongTensor([0, 1, 1, 1, 2, 0]) +row = torch.tensor([1, 0, 1, 0, 2, 1]) +col = torch.tensor([0, 1, 1, 1, 2, 0])",
        "core_API": "LongTensor"
    },
    {
        "commit_hash": "fdc907675050cb9e3aa887bf89d6c92519178349",
        "index": "dbc1f0e4..23275ce3 100755",
        "commit_message": "fix use of deprecated TF functions.\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Model(ModelDesc):",
            "if not USE_SLIM:",
            "# Use a regex to find parameters to apply weight decay.",
            "# Here we apply a weight decay on all W (weight matrix) of all fc layers",
            "-            wd_cost = tf.mul(1e-5,",
            "-                             regularize_cost('fc.*/W', tf.nn.l2_loss),",
            "-                             name='regularize_loss')",
            "+            wd_cost = tf.multiply(1e-5,",
            "+                                  regularize_cost('fc.*/W', tf.nn.l2_loss),",
            "+                                  name='regularize_loss')",
            "self.cost = tf.add_n([wd_cost, cost], name='total_cost')",
            "summary.add_moving_summary(cost, wd_cost, self.cost)",
            "else:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=mul), value='multiply')"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 1,
        "number": 5963,
        "neg_line": [
            "-wd_cost = tf.mul(1e-5,",
            "-regularize_cost('fc.*/W', tf.nn.l2_loss),",
            "-name='regularize_loss')"
        ],
        "pos_line": [
            "+wd_cost = tf.multiply(1e-5,",
            "+regularize_cost('fc.*/W', tf.nn.l2_loss),",
            "+name='regularize_loss')"
        ],
        "core_change": "-wd_cost = tf.mul(1e-5, -regularize_cost('fc.*/W', tf.nn.l2_loss), -name='regularize_loss') +wd_cost = tf.multiply(1e-5, +regularize_cost('fc.*/W', tf.nn.l2_loss), +name='regularize_loss')",
        "core_API": "mul"
    },
    {
        "commit_hash": "51efde54a907c85495263a980d253dcdb3e75209",
        "index": "84f329fea..a92adb4c5 100644",
        "commit_message": "cos fix\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class WarmupCosineSchedule(LRSchedule):",
            "return progress / self.warmup",
            "else:",
            "progress = (progress - self.warmup) / (1 - self.warmup)   # progress after warmup",
            "-            return 0.5 * (1. + torch.cos(math.pi * self.cycles * 2 * progress))",
            "+            return 0.5 * (1. + math.cos(math.pi * self.cycles * 2 * progress))",
            "",
            "",
            "class WarmupConstantSchedule(LRSchedule):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=torch), value='math')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 5964,
        "neg_line": [
            "-return 0.5 * (1. + torch.cos(math.pi * self.cycles * 2 * progress))"
        ],
        "pos_line": [
            "+return 0.5 * (1. + math.cos(math.pi * self.cycles * 2 * progress))"
        ],
        "core_change": "-return 0.5 * (1. + torch.cos(math.pi * self.cycles * 2 * progress)) +return 0.5 * (1. + math.cos(math.pi * self.cycles * 2 * progress))",
        "core_API": "cos"
    },
    {
        "commit_hash": "9bc8bbd77e1d00ccb4417e2beacf60eab67323b4",
        "index": "59d664bee6..f129142a09 100644",
        "commit_message": "applied fix to matmul for tf backend for 1-D case\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def matmul(",
            "ret = tf.constant(0)",
            "else:",
            "",
            "-            ret = tf.math.multiply(x1, x2)[0]",
            "+            ret = tf.reduce_sum(tf.math.multiply(x1, x2))",
            "ret = tf.cast(ret, dtype=dtype_from)  # return ret",
            "",
            "else:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('call', None), position=0, insert_id=2006728)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2006729)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2006730)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2006731)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2006732)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'reduce_sum'), position=2, insert_id=2006733)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2006734)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=call), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2006735)",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=integer, text=0))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 13,
        "number": 5968,
        "neg_line": [
            "-ret = tf.math.multiply(x1, x2)[0]"
        ],
        "pos_line": [
            "+ret = tf.reduce_sum(tf.math.multiply(x1, x2))"
        ],
        "core_change": "-ret = tf.math.multiply(x1, x2)[0] +ret = tf.reduce_sum(tf.math.multiply(x1, x2))",
        "core_API": "constant"
    },
    {
        "commit_hash": "5593b6f772c75ae016580e2579c2af9f86b19a39",
        "index": "7d303e6ed..e62ddb90f 100644",
        "commit_message": "Merge pull request #7872 from PyTorchLightning/refactor/logger-poc-changes\n\nRandom fixes for logger connector PoC\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_early_stopping_patience_train(",
            "callbacks=[early_stop_callback],",
            "num_sanity_val_steps=0,",
            "max_epochs=10,",
            "+        progress_bar_refresh_rate=0,",
            ")",
            "trainer.fit(model)",
            "assert trainer.current_epoch == expected_stop_epoch"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=parameters), node=('default_parameter', None), position=7, insert_id=533341)",
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=8, insert_id=533342)",
            "Insert(target_node=IN(type=default_parameter), node=('identifier', 'progress_bar_refresh_rate'), position=0, insert_id=533343)",
            "Insert(target_node=IN(type=default_parameter), node=('=', '='), position=1, insert_id=533344)",
            "Insert(target_node=IN(type=default_parameter), node=('integer', '0'), position=2, insert_id=533345)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 5,
        "number": 5969,
        "neg_line": [],
        "pos_line": [
            "+progress_bar_refresh_rate=0,"
        ],
        "core_change": "+progress_bar_refresh_rate=0,",
        "core_API": "fit"
    },
    {
        "commit_hash": "2f8dc11438f4af8bd244504efdbb48664e27e84c",
        "index": "4198f9b3ca..8b006df556 100644",
        "commit_message": "fix `fn_name` to `fn_tree` in `test_frontend_function` (#2991)\n\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_permute(",
            "native_array_flags=native_array,",
            "fw=fw,",
            "frontend=\"torch\",",
            "-        fn_name=\"permute\",",
            "+        fn_tree=\"permute\",",
            "input=np.asarray(value, dtype=dtype),",
            "dims=axis,",
            ")"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=fn_name), value='fn_tree')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 5971,
        "neg_line": [
            "-fn_name=\"permute\","
        ],
        "pos_line": [
            "+fn_tree=\"permute\","
        ],
        "core_change": "-fn_name=\"permute\", +fn_tree=\"permute\",",
        "core_API": "asarray"
    },
    {
        "commit_hash": "edbea7c4f0d8b58aa52d67e351424c0a9d59ec35",
        "index": "87573b1..dcf44b5 100644",
        "commit_message": "Fix colab tfds version\n\nPiperOrigin-RevId: 310566035\n\n",
        "file": "hub.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "\"def get_example_data(dataset, num_examples, **data_kw):\\n\",",
            "\"  \\\"\\\"\\\"Show example data\\\"\\\"\\\"\\n\",",
            "\"  with tf.Session() as sess:\\n\",",
            "-        \"    it = dataset.get_data(**data_kw).take(num_examples).map(dataset.preprocess_fn).batch(num_examples).make_one_shot_iterator().get_next()\\n\",",
            "+        \"    batched_ds = dataset.get_data(**data_kw).take(num_examples).map(dataset.preprocess_fn).batch(num_examples)\\n\",",
            "+        \"    it = tf.data.make_one_shot_iterator(batched_ds).get_next()\\n\",",
            "\"    data = sess.run(it)\\n\",",
            "\"  return data\\n\",",
            "\"\\n\","
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=4, insert_id=1948906)",
            "Update(target_node=ASTNode(type=string, text=\"    it = dataset.get_data(**data_kw).take(num_examples).map(dataset.preprocess_fn).batch(num_examples).make_one_shot_iterator().get_next()\\n\"), value='\"    batched_ds = dataset.get_data(**data_kw).take(num_examples).map(dataset.preprocess_fn).batch(num_examples)\\\\n\"')",
            "Insert(target_node=IN(type=expression_statement), node=('string', '\"    it = tf.data.make_one_shot_iterator(batched_ds).get_next()\\\\n\"'), position=0, insert_id=1948907)",
            "Insert(target_node=IN(type=expression_statement), node=(',', ','), position=1, insert_id=1948908)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 5974,
        "neg_line": [
            "-\"    it = dataset.get_data(**data_kw).take(num_examples).map(dataset.preprocess_fn).batch(num_examples).make_one_shot_iterator().get_next()\\n\","
        ],
        "pos_line": [
            "+\"    batched_ds = dataset.get_data(**data_kw).take(num_examples).map(dataset.preprocess_fn).batch(num_examples)\\n\",",
            "+\"    it = tf.data.make_one_shot_iterator(batched_ds).get_next()\\n\","
        ],
        "core_change": "-\"    it = dataset.get_data(**data_kw).take(num_examples).map(dataset.preprocess_fn).batch(num_examples).make_one_shot_iterator().get_next()\\n\", +\"    batched_ds = dataset.get_data(**data_kw).take(num_examples).map(dataset.preprocess_fn).batch(num_examples)\\n\", +\"    it = tf.data.make_one_shot_iterator(batched_ds).get_next()\\n\",",
        "core_API": "Session"
    },
    {
        "commit_hash": "76a1c67d87efcfc8656c0228c9f9f0ac184421b2",
        "index": "0561a6d40..b66981b99 100644",
        "commit_message": "rename logging -> loggers (#767)\n\n* move logging >> loggers\n\n* add warning\n\n* fix tests\n\n* logging alias\n\n* formatting\n\n* formatting\n\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "cuda",
        "change": [
            "class TrainerDDPMixin(ABC):",
            "gpu_str = ','.join([str(x) for x in data_parallel_device_ids])",
            "os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu_str",
            "",
            "-        logging.info(f'VISIBLE GPUS: {os.environ[\"CUDA_VISIBLE_DEVICES\"]}')",
            "+        log.info(f'VISIBLE GPUS: {os.environ[\"CUDA_VISIBLE_DEVICES\"]}')",
            "",
            "def ddp_train(self, gpu_idx, model):",
            "\"\"\""
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=logging), value='log')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 5981,
        "neg_line": [
            "-logging.info(f'VISIBLE GPUS: {os.environ[\"CUDA_VISIBLE_DEVICES\"]}')"
        ],
        "pos_line": [
            "+log.info(f'VISIBLE GPUS: {os.environ[\"CUDA_VISIBLE_DEVICES\"]}')"
        ],
        "core_change": "-logging.info(f'VISIBLE GPUS: {os.environ[\"CUDA_VISIBLE_DEVICES\"]}') +log.info(f'VISIBLE GPUS: {os.environ[\"CUDA_VISIBLE_DEVICES\"]}')",
        "core_API": "info"
    },
    {
        "commit_hash": "a37bdeb245159de2076e389ef7ec1c6c686b5424",
        "index": "d66dfa2..9d0e29e 100644",
        "commit_message": "Fix Pytext Documentation Build on CircleCI (#1596)\n\nSummary:\nPull Request resolved: https://github.com/facebookresearch/pytext/pull/1596\n\nBugfixes:\n- dependency errors: import/fix transformers installation, mock module for accelerators\n- rename bert_classification model inputs to differentiate config paths\n- fix adabelief docstring spacing so that Sphinx can parse it correctly\n- manually generate ExportConfig similarly to PytextConfig\n\nReviewed By: mikekgfb\n\nDifferential Revision: D26229331\n\nfbshipit-source-id: 2ecc72aa0b95b8a787cc475b40fe79fb4d0868a1\n\n",
        "file": "pytext.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "html_static_path = [\"_static\"]",
            "# autodoc_mock_imports = ['scipy', 'tensorboardX']",
            "",
            "# Manually mocking out the libraries to prevent requiring these modules",
            "-MOCK_MODULES = [\"scipy\", \"scipy.special\", \"torch.utils.tensorboard\"]",
            "+MOCK_MODULES = [",
            "+    \"scipy\",",
            "+    \"scipy.special\",",
            "+    \"torch.utils.tensorboard\",",
            "+    \"accelerators.pytorch.lib.glow_decorator\",",
            "+    \"pytext.PreprocessingMap.ttypes\",",
            "+]",
            "for mod_name in MOCK_MODULES:",
            "sys.modules[mod_name] = mock.Mock()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=,, text=,), node=ASTNode(type=list), position=5)",
            "Insert(target_node=ASTNode(type=list), node=(',', ','), position=2, insert_id=869955)",
            "Insert(target_node=ASTNode(type=list), node=(',', ','), position=7, insert_id=869956)",
            "Insert(target_node=ASTNode(type=list), node=('string', '\"accelerators.pytorch.lib.glow_decorator\"'), position=8, insert_id=869957)",
            "Insert(target_node=ASTNode(type=list), node=(',', ','), position=9, insert_id=869958)",
            "Insert(target_node=ASTNode(type=list), node=('string', '\"pytext.PreprocessingMap.ttypes\"'), position=10, insert_id=869959)"
        ],
        "plus_line": 5,
        "minus_line": 0,
        "AST_diff_line": 6,
        "number": 5984,
        "neg_line": [
            "-MOCK_MODULES = [\"scipy\", \"scipy.special\", \"torch.utils.tensorboard\"]"
        ],
        "pos_line": [
            "+MOCK_MODULES = [",
            "+\"scipy\",",
            "+\"scipy.special\",",
            "+\"torch.utils.tensorboard\",",
            "+\"accelerators.pytorch.lib.glow_decorator\",",
            "+\"pytext.PreprocessingMap.ttypes\",",
            "+]"
        ],
        "core_change": "-MOCK_MODULES = [\"scipy\", \"scipy.special\", \"torch.utils.tensorboard\"] +MOCK_MODULES = [ +\"scipy\", +\"scipy.special\", +\"torch.utils.tensorboard\", +\"accelerators.pytorch.lib.glow_decorator\", +\"pytext.PreprocessingMap.ttypes\", +]",
        "core_API": "Mock"
    },
    {
        "commit_hash": "737b1f467e3610691f6f7ca4debb94a9d6219ad1",
        "index": "74784624..57821793 100644",
        "commit_message": "Full GraphGym + PyG integration (#3076)\n\n* Full GraphGym + PyG integration\n\n* fix torch.allclose\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "import torch.nn as nn",
            "",
            "",
            "def init_weights(m):",
            "-    \"\"\"Performs weight initialization.\"\"\"",
            "+    r\"\"\"",
            "+    Performs weight initialization",
            "+",
            "+    Args:",
            "+        m (nn.Module): PyTorch module",
            "+",
            "+    \"\"\"",
            "if isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):",
            "m.weight.data.fill_(1.0)",
            "m.bias.data.zero_()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"\"\"Performs weight initialization.\"\"\"), value='r\"\"\"\\n    Performs weight initialization\\n\\n    Args:\\n        m (nn.Module): PyTorch module\\n\\n    \"\"\"')"
        ],
        "plus_line": 5,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 5989,
        "neg_line": [
            "-\"\"\"Performs weight initialization.\"\"\""
        ],
        "pos_line": [
            "+r\"\"\"",
            "+Performs weight initialization",
            "+",
            "+Args:",
            "+m (nn.Module): PyTorch module",
            "+",
            "+\"\"\""
        ],
        "core_change": "-\"\"\"Performs weight initialization.\"\"\" +r\"\"\" +Performs weight initialization + +Args: +m (nn.Module): PyTorch module + +\"\"\"",
        "core_API": "fill_"
    },
    {
        "commit_hash": "c683ac83f735797b812dcd9b4c58c8753acda922",
        "index": "c03dbf173b..d662b2840b 100644",
        "commit_message": "lintfixbot: Auto-commit fixed lint errors in codebase\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def where(",
            "# ----- #",
            "",
            "",
            "-def argwhere(",
            "-        x: torch.Tensor,",
            "-        /,",
            "-        *,",
            "-        out: Optional[torch.Tensor] = None",
            "-) -> torch.Tensor:",
            "+def argwhere(x: torch.Tensor, /, *, out: Optional[torch.Tensor] = None) -> torch.Tensor:",
            "return torch.argwhere(x)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('return_statement', None), position=1, insert_id=267570)",
            "Move(target_node=IN(type=return_statement), node=ASTNode(type=return, text=return), position=0)",
            "Move(target_node=IN(type=return_statement), node=ASTNode(type=call), position=1)",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 0,
        "minus_line": 4,
        "AST_diff_line": 5,
        "number": 5991,
        "neg_line": [
            "-def argwhere(",
            "-x: torch.Tensor,",
            "-/,",
            "-*,",
            "-out: Optional[torch.Tensor] = None",
            "-) -> torch.Tensor:"
        ],
        "pos_line": [
            "+def argwhere(x: torch.Tensor, /, *, out: Optional[torch.Tensor] = None) -> torch.Tensor:"
        ],
        "core_change": "-def argwhere( -x: torch.Tensor, -/, -*, -out: Optional[torch.Tensor] = None -) -> torch.Tensor: +def argwhere(x: torch.Tensor, /, *, out: Optional[torch.Tensor] = None) -> torch.Tensor:",
        "core_API": "argwhere"
    },
    {
        "commit_hash": "fec3112cbacf19cd211109b1bc0b33a502595aca",
        "index": "b8f05d11..ed8a6468 100644",
        "commit_message": "Update to PyTorch 1.9.0 (#2887)\n\n* Update to PyTorch 1.9.0\n\n* Update torch.cholesky, torch.symeig\n\n* Fix torch.tensordot, torch.qr, funsor dependency\n\n* Fix torch import, AutoGuideList usage\n\n* Ignore bug in PyTorch jit\n\n* Ignore tracer warnings\n\n* Replace torch.solve with torch.linalg.solve\n\n* Xfail vectorized markov funsor tests\n\n* Update funsor version\n\n* Switch MNIST mirrors\n\n* Pin pillow version\n\n* Bump torchvision version\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_precision_to_scale_tril(batch_shape, event_shape):",
            "x = torch.randn(batch_shape + event_shape + event_shape)",
            "precision = x.matmul(x.transpose(-2, -1))",
            "actual = precision_to_scale_tril(precision)",
            "-    expected = precision.inverse().cholesky()",
            "+    expected = torch.linalg.cholesky(precision.inverse())",
            "assert_close(actual, expected)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=677203)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=677204)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=677205)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=677206)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cholesky'), position=2, insert_id=677207)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=677208)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=call), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=677209)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=677210)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=677211)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'linalg'), position=2, insert_id=677212)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=cholesky))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 5992,
        "neg_line": [
            "-expected = precision.inverse().cholesky()"
        ],
        "pos_line": [
            "+expected = torch.linalg.cholesky(precision.inverse())"
        ],
        "core_change": "-expected = precision.inverse().cholesky() +expected = torch.linalg.cholesky(precision.inverse())",
        "core_API": "randn"
    },
    {
        "commit_hash": "5c11c805f204f8b869a48d0f9908de53a1d36f1f",
        "index": "0f1046f3..d9a8cf2b 100644",
        "commit_message": "Fix/find homography dlt iterated gradcheck fail (#1032)\n\n* Fixing #795: find_homography_dlt_iterated sometimes fails\n\n* Fixing #795: omitting gradcheck if torch.version < 1.7\n\n* Fixing #795: omitting gradcheck if torch.version < 1.7 with skipif\n\n* Fixing #795: omitting gradcheck if torch.version < 1.7 with skipif and flake8\n\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestFindHomographyDLTIter:",
            "kornia.transform_points(dst_homo_src, points_src), points_dst, rtol=1e-3, atol=1e-4)",
            "",
            "@pytest.mark.grad",
            "+    @pytest.mark.skipif(torch.__version__ < '1.7', reason=\"pytorch bug of incopatible types: #33546 fixed in v1.7\")",
            "def test_gradcheck(self, device):",
            "",
            "# Save initial seed"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=decorated_definition), node=('decorator', None), position=1, insert_id=424092)",
            "Insert(target_node=IN(type=decorator), node=('@', '@'), position=0, insert_id=424093)",
            "Insert(target_node=IN(type=decorator), node=('call', None), position=1, insert_id=424094)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=424095)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=424096)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=424097)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=424098)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'skipif'), position=2, insert_id=424099)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=424100)",
            "Insert(target_node=IN(type=argument_list), node=('comparison_operator', None), position=1, insert_id=424101)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=424102)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=3, insert_id=424103)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=424104)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'pytest'), position=0, insert_id=424105)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=424106)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'mark'), position=2, insert_id=424107)",
            "Insert(target_node=IN(type=comparison_operator), node=('attribute', None), position=0, insert_id=424108)",
            "Insert(target_node=IN(type=comparison_operator), node=('<', '<'), position=1, insert_id=424109)",
            "Insert(target_node=IN(type=comparison_operator), node=('string', \"'1.7'\"), position=2, insert_id=424110)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'reason'), position=0, insert_id=424111)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=424112)",
            "Insert(target_node=IN(type=keyword_argument), node=('string', '\"pytorch bug of incopatible types: #33546 fixed in v1.7\"'), position=2, insert_id=424113)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=424114)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=424115)",
            "Insert(target_node=IN(type=attribute), node=('identifier', '__version__'), position=2, insert_id=424116)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 25,
        "number": 5995,
        "neg_line": [],
        "pos_line": [
            "+@pytest.mark.skipif(torch.__version__ < '1.7', reason=\"pytorch bug of incopatible types: #33546 fixed in v1.7\")"
        ],
        "core_change": "+@pytest.mark.skipif(torch.__version__ < '1.7', reason=\"pytorch bug of incopatible types: #33546 fixed in v1.7\")",
        "core_API": "transform_points"
    },
    {
        "commit_hash": "f4a057201876fd29872d722390e2e13d2a4127ab",
        "index": "ad5fcc4b..b502fcc4 100644",
        "commit_message": "Test documentation (#511)\n\n* Activation Cleaning Docstring Test\n\n* Requirements Pinned with range to insure tested versions are used. Range are used to prevent updating requirements all the time.\n\n* setup.cfg file added with PEP8 configuration\n\n* activation.py refactored\n\n* docstring fixed - ready for documentation unittest\n\n* Yapf correction for max_line_length: 120\n\n* test yapf refactored\n\n* test documentation added\n\n* Missing requirement added: sphinx\n\n* Allow test on documentation to pass on warning\n\n* Fix travis dependencies install\n\n* Travis install script fixed\n\n* Travis install command fixed\n\n* Requirements conflict solved\n\n* Yapf Style modified and merged in file \"setup.cfg\"\n\n* Yapf Confiuguration Updated\n\n* Code Refactored with new YAPF formating style\n\n* Code Refactored with new YAPF formating style\n\n* Code Refactored with new YAPF formating style\n\n* shorten codes\n\n* Various Cleaning\n\n* Trailing Slashes removed\n\n* Test Recurrent Fixed\n\n* Line Width Fix\n\n* docs requirements updated\n\n* fix example docs style\n\n* Codacy Issue Fixed\n\n* Merge Errors fixed\n\n* YAPF Style Applied\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class ReconLayer(DenseLayer):",
            "# ce = cost.cross_entropy(y, x_recon)                                               # <haodong>: list , list , Error (only be used for softmax output)",
            "# ce = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y, x_recon))          # <haodong>: list , list , Error (only be used for softmax output)",
            "# ce = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(y, x_recon))   # <haodong>: list , index , Error (only be used for softmax output)",
            "-        L2_w = tf.contrib.layers.l2_regularizer(lambda_l2_w)(self.train_params[0]) \\",
            "-                + tf.contrib.layers.l2_regularizer(lambda_l2_w)(self.train_params[2])           # faster than the code below",
            "+        L2_w = tf.contrib.layers.l2_regularizer(lambda_l2_w)(",
            "+            self.train_params[0]",
            "+        ) + tf.contrib.layers.l2_regularizer(lambda_l2_w)(self.train_params[2])  # faster than the code below",
            "# L2_w = lambda_l2_w * tf.reduce_mean(tf.square(self.train_params[0])) + lambda_l2_w * tf.reduce_mean( tf.square(self.train_params[2]))",
            "",
            "# DropNeuro"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 3,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 6000,
        "neg_line": [
            "-L2_w = tf.contrib.layers.l2_regularizer(lambda_l2_w)(self.train_params[0]) \\",
            "-+ tf.contrib.layers.l2_regularizer(lambda_l2_w)(self.train_params[2])           # faster than the code below"
        ],
        "pos_line": [
            "+L2_w = tf.contrib.layers.l2_regularizer(lambda_l2_w)(",
            "+self.train_params[0]",
            "+) + tf.contrib.layers.l2_regularizer(lambda_l2_w)(self.train_params[2])  # faster than the code below"
        ],
        "core_change": "-L2_w = tf.contrib.layers.l2_regularizer(lambda_l2_w)(self.train_params[0]) \\ -+ tf.contrib.layers.l2_regularizer(lambda_l2_w)(self.train_params[2])           # faster than the code below +L2_w = tf.contrib.layers.l2_regularizer(lambda_l2_w)( +self.train_params[0] +) + tf.contrib.layers.l2_regularizer(lambda_l2_w)(self.train_params[2])  # faster than the code below",
        "core_API": "cross_entropy"
    },
    {
        "commit_hash": "0ba7472da92a89af9aca84de5b01a228d18340a2",
        "index": "7c5c91965..b91e675b1 100644",
        "commit_message": "[Testing] Fix LINT/sphinx errors. (#8874)\n\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class RecurrentNetwork(TorchModelV2):",
            "raise NotImplementedError(\"You must implement this for an RNN model\")",
            "",
            "",
            "-class LSTMWrapper(RecurrentNetwork):",
            "+class LSTMWrapper(RecurrentNetwork, nn.Module):",
            "\"\"\"An LSTM wrapper serving as an interface for ModelV2s that set use_lstm.",
            "\"\"\"",
            "",
            "def __init__(self, obs_space, action_space, num_outputs, model_config,",
            "name):",
            "",
            "-        super(LSTMWrapper, self).__init__(obs_space, action_space, None,",
            "-                                          model_config, name)",
            "+        nn.Module.__init__(self)",
            "+        super().__init__(obs_space, action_space, None, model_config, name)",
            "",
            "self.cell_size = model_config[\"lstm_cell_size\"]",
            "self.lstm = nn.LSTM(self.num_outputs, self.cell_size, batch_first=True)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1506416)",
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=3, insert_id=1506417)",
            "Insert(target_node=ASTNode(type=block), node=('expression_statement', None), position=0, insert_id=1506418)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'nn'), position=0, insert_id=1506419)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1506420)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Module'), position=2, insert_id=1506421)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=1506422)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1506423)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1506424)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1506425)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1506426)",
            "Insert(target_node=IN(type=attribute), node=('identifier', '__init__'), position=2, insert_id=1506427)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1506428)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'self'), position=1, insert_id=1506429)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1506430)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'nn'), position=0, insert_id=1506431)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1506432)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Module'), position=2, insert_id=1506433)",
            "Delete(target_node=ASTNode(type=identifier, text=LSTMWrapper))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=self))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 21,
        "number": 6001,
        "neg_line": [
            "-class LSTMWrapper(RecurrentNetwork):",
            "-super(LSTMWrapper, self).__init__(obs_space, action_space, None,",
            "-model_config, name)"
        ],
        "pos_line": [
            "+class LSTMWrapper(RecurrentNetwork, nn.Module):",
            "+nn.Module.__init__(self)",
            "+super().__init__(obs_space, action_space, None, model_config, name)"
        ],
        "core_change": "-class LSTMWrapper(RecurrentNetwork): +class LSTMWrapper(RecurrentNetwork, nn.Module): -super(LSTMWrapper, self).__init__(obs_space, action_space, None, -model_config, name) +nn.Module.__init__(self) +super().__init__(obs_space, action_space, None, model_config, name)",
        "core_API": "__init__"
    },
    {
        "commit_hash": "d052478479033c62fcbdea9a268810739f411048",
        "index": "e6a7bede..fef833f4 100644",
        "commit_message": "fixed self-loops in pointnet\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class PPFConv(MessagePassing):",
            "for use in message passing in bipartite graphs.",
            "edge_index (LongTensor): The edge indices.",
            "\"\"\"",
            "-        if torch.is_tensor(x):",
            "+        if torch.is_tensor(pos):  # Add self-loops for symmetric adjacencies.",
            "edge_index, _ = remove_self_loops(edge_index)",
            "edge_index = add_self_loops(edge_index, num_nodes=x.size(0))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=x), value='pos')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 6003,
        "neg_line": [
            "-if torch.is_tensor(x):"
        ],
        "pos_line": [
            "+if torch.is_tensor(pos):  # Add self-loops for symmetric adjacencies."
        ],
        "core_change": "-if torch.is_tensor(x): +if torch.is_tensor(pos):  # Add self-loops for symmetric adjacencies.",
        "core_API": "is_tensor"
    },
    {
        "commit_hash": "a979a3f34eebbea7e6ffa7506aaf83aa6c47e7a1",
        "index": "a55b9db6..6a622f44 100755",
        "commit_message": "fix image summary\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Model(ModelDesc):",
            "fake_output = tf.image.grayscale_to_rgb(fake_output)",
            "viz = (tf.concat(2, [input, output, fake_output]) + 1.0) * 128.0",
            "viz = tf.cast(tf.clip_by_value(viz, 0, 255), tf.uint8, name='viz')",
            "-        tf.image_summary('gen', viz, max_images=max(30, BATCH))",
            "+        tf.image_summary('gen', viz, max_outputs=max(30, BATCH))",
            "",
            "all_vars = tf.trainable_variables()",
            "self.g_vars = [v for v in all_vars if v.name.startswith('gen/')]"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=max_images), value='max_outputs')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 6005,
        "neg_line": [
            "-tf.image_summary('gen', viz, max_images=max(30, BATCH))"
        ],
        "pos_line": [
            "+tf.image_summary('gen', viz, max_outputs=max(30, BATCH))"
        ],
        "core_change": "-tf.image_summary('gen', viz, max_images=max(30, BATCH)) +tf.image_summary('gen', viz, max_outputs=max(30, BATCH))",
        "core_API": "grayscale_to_rgb"
    },
    {
        "commit_hash": "95b6d985ffb231d9a2047c039c95660913063b9d",
        "index": "49ed47b5..ded8e431 100644",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class GatedCnnEncoder(Seq2SeqEncoder):",
            "",
            "self._return_all_layers = return_all_layers",
            "",
            "-    def forward(self, token_embeddings: torch.Tensor, mask: torch.Tensor):",
            "+    def forward(self, token_embeddings: torch.Tensor, mask: torch.BoolTensor):",
            "",
            "# Convolutions need transposed input",
            "transposed_embeddings = torch.transpose(token_embeddings, 1, 2)",
            "",
            "# We need to broadcast the mask to feature dimension,",
            "# and to use masked_fill_ we need the inverse of the mask.",
            "-        mask_for_fill = (1 - mask).unsqueeze(1).to(dtype=torch.bool)",
            "+        mask_for_fill = ~mask.unsqueeze(1)",
            "",
            "if self._return_all_layers:",
            "# outputs will be [[all forward layers], [all backward layers]]"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('unary_operator', None), position=2, insert_id=19590)",
            "Insert(target_node=IN(type=unary_operator), node=('~', '~'), position=0, insert_id=19591)",
            "Move(target_node=IN(type=unary_operator), node=ASTNode(type=call), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=Tensor), value='BoolTensor')",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=mask), position=0)",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=-, text=-))",
            "Delete(target_node=ASTNode(type=binary_operator))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=parenthesized_expression))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=to))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=dtype))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=bool))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 25,
        "number": 6006,
        "neg_line": [
            "-def forward(self, token_embeddings: torch.Tensor, mask: torch.Tensor):",
            "-mask_for_fill = (1 - mask).unsqueeze(1).to(dtype=torch.bool)"
        ],
        "pos_line": [
            "+def forward(self, token_embeddings: torch.Tensor, mask: torch.BoolTensor):",
            "+mask_for_fill = ~mask.unsqueeze(1)"
        ],
        "core_change": "-def forward(self, token_embeddings: torch.Tensor, mask: torch.Tensor): +def forward(self, token_embeddings: torch.Tensor, mask: torch.BoolTensor): -mask_for_fill = (1 - mask).unsqueeze(1).to(dtype=torch.bool) +mask_for_fill = ~mask.unsqueeze(1)",
        "core_API": "transpose"
    },
    {
        "commit_hash": "fa3fde7d7290c1a2989dca4ab8a01d046634678f",
        "index": "1d69bca..55347a3 100644",
        "commit_message": "fix bug\n\n",
        "file": "imagen-pytorch.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Imagen(BaseGaussianDiffusion):",
            "device = next(self.parameters()).device",
            "",
            "lowres_sample_noise_level = default(lowres_sample_noise_level, self.lowres_sample_noise_level)",
            "-        lowres_noise_times = torch.full((batch_size,), lowres_sample_noise_level, device = device, dtype = torch.long)",
            "+        lowres_noise_times = torch.full((batch_size,), int(lowres_sample_noise_level * self.num_timesteps), device = device, dtype = torch.long)",
            "",
            "for unet_number, unet, channel, image_size, learned_variance in tqdm(zip(range(1, len(self.unets) + 1), self.unets, self.sample_channels, self.image_sizes, self.learned_variance)):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=3, insert_id=255295)",
            "Insert(target_node=IN(type=call), node=('identifier', 'int'), position=0, insert_id=255296)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=255297)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=255298)",
            "Insert(target_node=IN(type=argument_list), node=('binary_operator', None), position=1, insert_id=255299)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=255300)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=identifier, text=lowres_sample_noise_level), position=0)",
            "Insert(target_node=IN(type=binary_operator), node=('*', '*'), position=1, insert_id=255301)",
            "Insert(target_node=IN(type=binary_operator), node=('attribute', None), position=2, insert_id=255302)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=255303)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=255304)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'num_timesteps'), position=2, insert_id=255305)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 12,
        "number": 6008,
        "neg_line": [
            "-lowres_noise_times = torch.full((batch_size,), lowres_sample_noise_level, device = device, dtype = torch.long)"
        ],
        "pos_line": [
            "+lowres_noise_times = torch.full((batch_size,), int(lowres_sample_noise_level * self.num_timesteps), device = device, dtype = torch.long)"
        ],
        "core_change": "-lowres_noise_times = torch.full((batch_size,), lowres_sample_noise_level, device = device, dtype = torch.long) +lowres_noise_times = torch.full((batch_size,), int(lowres_sample_noise_level * self.num_timesteps), device = device, dtype = torch.long)",
        "core_API": "parameters"
    },
    {
        "commit_hash": "1eb8c833141b919aa4e60918dfecbace03a4fe68",
        "index": "75a5a9045f..bd0fdf018d 100644",
        "commit_message": "[rllib] Initial RLLib documentation (#969)\n\n* initial documentation for RLLib\n\n* more RL documentation\n\n* fix linting\n\n* fix comments\n\n* update\n\n* fix\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "from ray.rllib.models.misc import (conv2d, linear, flatten,",
            "normc_initializer)",
            "from ray.rllib.models.model import Model",
            "",
            "-use_tf100_api = (distutils.version.LooseVersion(tf.VERSION) >=",
            "-                 distutils.version.LooseVersion(\"1.0.0\"))",
            "-",
            "",
            "class LSTM(Model):",
            "# TODO(rliaw): Add LSTM code for other algorithms",
            "def _init(self, inputs, num_outputs, options):",
            "+        use_tf100_api = (distutils.version.LooseVersion(tf.VERSION) >=",
            "+                         distutils.version.LooseVersion(\"1.0.0\"))",
            "+",
            "self.x = x = inputs",
            "for i in range(4):",
            "x = tf.nn.elu(conv2d(x, 32, \"l{}\".format(i + 1), [3, 3], [2, 2]))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('function_definition', None), position=4, insert_id=2155583)",
            "Insert(target_node=IN(type=function_definition), node=('def', 'def'), position=0, insert_id=2155584)",
            "Insert(target_node=IN(type=function_definition), node=('identifier', '_init'), position=1, insert_id=2155585)",
            "Move(target_node=IN(type=function_definition), node=ASTNode(type=parameters), position=2)",
            "Insert(target_node=IN(type=function_definition), node=(':', ':'), position=3, insert_id=2155586)",
            "Insert(target_node=IN(type=function_definition), node=('block', None), position=4, insert_id=2155587)",
            "Move(target_node=IN(type=block), node=ASTNode(type=expression_statement), position=0)",
            "Delete(target_node=ASTNode(type=def, text=def))",
            "Delete(target_node=ASTNode(type=identifier, text=_init))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=block, text=))",
            "Delete(target_node=ASTNode(type=function_definition))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 12,
        "number": 6009,
        "neg_line": [
            "-use_tf100_api = (distutils.version.LooseVersion(tf.VERSION) >=",
            "-distutils.version.LooseVersion(\"1.0.0\"))",
            "-"
        ],
        "pos_line": [
            "+use_tf100_api = (distutils.version.LooseVersion(tf.VERSION) >=",
            "+distutils.version.LooseVersion(\"1.0.0\"))",
            "+"
        ],
        "core_change": "-use_tf100_api = (distutils.version.LooseVersion(tf.VERSION) >= -distutils.version.LooseVersion(\"1.0.0\")) - +use_tf100_api = (distutils.version.LooseVersion(tf.VERSION) >= +distutils.version.LooseVersion(\"1.0.0\")) +",
        "core_API": "LooseVersion"
    },
    {
        "commit_hash": "47d5c244706ab01859a3931f66066407a9d5efbb",
        "index": "f59393db..c177c902 100755",
        "commit_message": "Summaries completely changed, distributed mode incompletely changed, various fixes\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Categorical(Distribution):",
            "definite = tf.argmax(input=logits, axis=-1)",
            "",
            "# Non-deterministic: sample action using Gumbel distribution",
            "-        uniform = tf.random_uniform(shape=tf.shape(input=logits), minval=util.epsilon, maxval=(1.0 - util.epsilon))",
            "-        gumbel_distribution = -tf.log(x=-tf.log(x=uniform))",
            "+        uniform_distribution = tf.random_uniform(",
            "+            shape=tf.shape(input=logits),",
            "+            minval=util.epsilon,",
            "+            maxval=(1.0 - util.epsilon)",
            "+        )",
            "+        gumbel_distribution = -tf.log(x=-tf.log(x=uniform_distribution))",
            "sampled = tf.argmax(input=(logits + gumbel_distribution), axis=-1)",
            "",
            "return tf.where(condition=deterministic, x=definite, y=sampled)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=uniform), value='uniform_distribution')",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2242605)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2242606)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2242607)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'log'), position=2, insert_id=2242608)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=uniform), value='uniform_distribution')",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=log))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 6,
        "minus_line": 2,
        "AST_diff_line": 11,
        "number": 6013,
        "neg_line": [
            "-uniform = tf.random_uniform(shape=tf.shape(input=logits), minval=util.epsilon, maxval=(1.0 - util.epsilon))",
            "-gumbel_distribution = -tf.log(x=-tf.log(x=uniform))"
        ],
        "pos_line": [
            "+uniform_distribution = tf.random_uniform(",
            "+shape=tf.shape(input=logits),",
            "+minval=util.epsilon,",
            "+maxval=(1.0 - util.epsilon)",
            "+)",
            "+gumbel_distribution = -tf.log(x=-tf.log(x=uniform_distribution))"
        ],
        "core_change": "-uniform = tf.random_uniform(shape=tf.shape(input=logits), minval=util.epsilon, maxval=(1.0 - util.epsilon)) -gumbel_distribution = -tf.log(x=-tf.log(x=uniform)) +uniform_distribution = tf.random_uniform( +shape=tf.shape(input=logits), +minval=util.epsilon, +maxval=(1.0 - util.epsilon) +) +gumbel_distribution = -tf.log(x=-tf.log(x=uniform_distribution))",
        "core_API": "argmax"
    },
    {
        "commit_hash": "0703d79606fa74da25a6dfbae95c583c8a609caf",
        "index": "e9fd4674..93ebc2a4 100644",
        "commit_message": "Improvements in the Documentation of Backend (#5767)\n\n* Fix and improvements to the `backend` documentation\n\nImproved Preamble of the `backend.md` template:\n- fixed a typo\n- Added few notes that makes the documentation more self explanatory\n- Made all code examples running by Copy&Paste\n\nAligned the format of the  `backend()` function\n\nFixed docstring of `set_image_dim_ordering()` function\n\n* Fixed a Typo in %USERPROFILE% env name for Window Users\n\n* Added `_variable` so not to get a different value every time\n\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "keras",
        "change": [
            "else:",
            "def backend():",
            "\"\"\"Publicly accessible method",
            "for determining the current backend.",
            "+",
            "+    # Returns",
            "+        String, the name of the backend Keras is currently using.",
            "+",
            "+    # Example",
            "+    ```python",
            "+        >>> keras.backend.backend()",
            "+        'tensorflow'",
            "+    ```",
            "\"\"\"",
            "return _BACKEND"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"\"\"Publicly accessible method\nfor determining the current backend.\n\"\"\"), value='\"\"\"Publicly accessible method\\nfor determining the current backend.\\n\\n    # Returns\\n        String, the name of the backend Keras is currently using.\\n\\n    # Example\\n    ```python\\n        >>> keras.backend.backend()\\n        \\'tensorflow\\'\\n    ```\\n\"\"\"')"
        ],
        "plus_line": 7,
        "minus_line": 0,
        "AST_diff_line": 1,
        "number": 6014,
        "neg_line": [],
        "pos_line": [
            "+",
            "+# Returns",
            "+String, the name of the backend Keras is currently using.",
            "+",
            "+# Example",
            "+```python",
            "+>>> keras.backend.backend()",
            "+'tensorflow'",
            "+```"
        ],
        "core_change": "+ +# Returns +String, the name of the backend Keras is currently using. + +# Example +```python +>>> keras.backend.backend() +'tensorflow' +```",
        "core_API": "backend"
    },
    {
        "commit_hash": "9b62ef3704e6517ac17c9221a3f2c402d18ce5e0",
        "index": "64d8dab..2f2e0e7 100644",
        "commit_message": "fix typos (#5)\n\n\n",
        "file": "tensorflow-wavenet.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class WaveNet(object):",
            "",
            "# We skip connections from the outputs of each layer, adding them",
            "# all up here.",
            "-            total = sum(out)",
            "+            total = sum(outputs)",
            "transformed1 = tf.nn.relu(total)",
            "conv1 = tf.nn.conv2d(transformed1, w1, [1] * 4, padding=\"SAME\")",
            "transformed2 = tf.nn.relu(conv1)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=out), value='outputs')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 6019,
        "neg_line": [
            "-total = sum(out)"
        ],
        "pos_line": [
            "+total = sum(outputs)"
        ],
        "core_change": "-total = sum(out) +total = sum(outputs)",
        "core_API": "relu"
    },
    {
        "commit_hash": "82258b47dcf6e12a9b9f2aec68ea18fb3fd3d9fc",
        "index": "5296c080b..ff9cd322b 100644",
        "commit_message": "Refactor download (#4384)\n\n* Refactor download\n\n* Fix textvqa dataset\n\n* Move download module one level up\n\n* Fix test mock fsspec\n\n* Fix docs\n\n* Fix import of DownloadConfig\n\n* Refactor mock_download_manager\n\n* Refactor DownloadConfig\n\n* Fix docs\n\n* Add sections to docs of builder classes\n\n* Use relative imports\n\n* Fix path\n\n* Fix import\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class Textvqa(datasets.GeneratorBasedBuilder):",
            "citation=_CITATION,",
            ")",
            "",
            "-    def _split_generators(self, dl_manager: datasets.utils.DownloadManager):",
            "+    def _split_generators(self, dl_manager):",
            "downloaded_files = dl_manager.download_and_extract(_URLS)",
            "return [",
            "datasets.SplitGenerator("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=parameters), node=ASTNode(type=identifier, text=dl_manager), position=3)",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=identifier, text=datasets))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=utils))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=DownloadManager))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=type))",
            "Delete(target_node=ASTNode(type=typed_parameter))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 6024,
        "neg_line": [
            "-def _split_generators(self, dl_manager: datasets.utils.DownloadManager):"
        ],
        "pos_line": [
            "+def _split_generators(self, dl_manager):"
        ],
        "core_change": "-def _split_generators(self, dl_manager: datasets.utils.DownloadManager): +def _split_generators(self, dl_manager):",
        "core_API": "download_and_extract"
    },
    {
        "commit_hash": "6eea694f99e25eb611a930d409ce01fb2e71ac59",
        "index": "e620ea560b..dca17e21ff 100644",
        "commit_message": "fix `all` and `any` to be consistent across backends (#4539)\n\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def any(",
            "keepdims: bool = False,",
            "out: Optional[torch.Tensor] = None,",
            ") -> torch.Tensor:",
            "-    x = ivy.asarray(x).type(torch.bool)",
            "+    x = torch.as_tensor(x).type(torch.bool)",
            "if axis is None:",
            "num_dims = len(x.shape)",
            "axis = list(range(num_dims))"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=ivy), value='torch')",
            "Update(target_node=ASTNode(type=identifier, text=asarray), value='as_tensor')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 6026,
        "neg_line": [
            "-x = ivy.asarray(x).type(torch.bool)"
        ],
        "pos_line": [
            "+x = torch.as_tensor(x).type(torch.bool)"
        ],
        "core_change": "-x = ivy.asarray(x).type(torch.bool) +x = torch.as_tensor(x).type(torch.bool)",
        "core_API": "asarray"
    },
    {
        "commit_hash": "02fb71f730f6e600edbf27f58cea86209cb5dc55",
        "index": "6dcf959b..229b4b7c 100755",
        "commit_message": "NAF added, regularization loss handling improved, various fixes and formatting\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Optimizer(tf.train.Optimizer):",
            "raise NotImplementedError",
            "",
            "def minimize(self, time, variables, **kwargs):",
            "-        diffs = self.fn_step(time=time, variables=variables, **kwargs)",
            "+        diffs = self.step(time=time, variables=variables, **kwargs)",
            "# diffs[0] = tf.Print(diffs[0], (diffs[0],))",
            "with tf.control_dependencies(control_inputs=diffs):",
            "return tf.no_op()"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=fn_step), value='step')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 6028,
        "neg_line": [
            "-diffs = self.fn_step(time=time, variables=variables, **kwargs)"
        ],
        "pos_line": [
            "+diffs = self.step(time=time, variables=variables, **kwargs)"
        ],
        "core_change": "-diffs = self.fn_step(time=time, variables=variables, **kwargs) +diffs = self.step(time=time, variables=variables, **kwargs)",
        "core_API": "fn_step"
    },
    {
        "commit_hash": "ec27313be242b4131354490e9dbed365ed7b9be9",
        "index": "717d625f6..72eeb197e 100644",
        "commit_message": "Fix batch size extraction when set by the user in `LightningModule.log` (#10408)\n\nCo-authored-by: Carlos Mocholi <carlossmocholi@gmail.com>\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_loops_state_dict_structure():",
            "\"is_last_batch\": False,",
            "},",
            "\"_results\": {",
            "+                \"batch\": None,",
            "+                \"batch_size\": None,",
            "\"training\": False,",
            "-                \"_batch_size\": torch.tensor(1),",
            "\"device\": None,",
            "\"items\": {},",
            "},"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Move(target_node=ASTNode(type=pair), node=ASTNode(type=dictionary), position=2)",
            "Insert(target_node=ASTNode(type=dictionary), node=('pair', None), position=1, insert_id=520115)",
            "Insert(target_node=ASTNode(type=dictionary), node=('pair', None), position=4, insert_id=520116)",
            "Insert(target_node=ASTNode(type=dictionary), node=(',', ','), position=5, insert_id=520117)",
            "Insert(target_node=IN(type=pair), node=('string', '\"batch\"'), position=0, insert_id=520118)",
            "Insert(target_node=IN(type=pair), node=(':', ':'), position=1, insert_id=520119)",
            "Insert(target_node=IN(type=pair), node=('none', 'None'), position=2, insert_id=520120)",
            "Update(target_node=ASTNode(type=string, text=\"_batch_size\"), value='\"batch_size\"')",
            "Move(target_node=IN(type=pair), node=ASTNode(type=string, text=\"_batch_size\"), position=0)",
            "Move(target_node=IN(type=pair), node=ASTNode(type=:, text=:), position=1)",
            "Insert(target_node=IN(type=pair), node=('none', 'None'), position=2, insert_id=520121)",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=tensor))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=pair))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 21,
        "number": 6029,
        "neg_line": [
            "-\"_batch_size\": torch.tensor(1),"
        ],
        "pos_line": [
            "+\"batch\": None,",
            "+\"batch_size\": None,"
        ],
        "core_change": "+\"batch\": None, +\"batch_size\": None, -\"_batch_size\": torch.tensor(1),",
        "core_API": "tensor"
    },
    {
        "commit_hash": "671030eb86037c4d12ddd8519f767f1ee3927db5",
        "index": "eb7b060..133cb08 100644",
        "commit_message": "Fix error related to cpu only machines\n",
        "file": "PyTorch-YOLOv3.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def load_model(model_path, weights_path=None):",
            "if weights_path:",
            "if weights_path.endswith(\".pth\"):",
            "# Load checkpoint weights",
            "-            model.load_state_dict(torch.load(weights_path))",
            "+            model.load_state_dict(torch.load(weights_path, map_location=device))",
            "else:",
            "# Load darknet weights",
            "model.load_darknet_weights(weights_path)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=905331)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=905332)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'map_location'), position=0, insert_id=905333)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=905334)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=2, insert_id=905335)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 6032,
        "neg_line": [
            "-model.load_state_dict(torch.load(weights_path))"
        ],
        "pos_line": [
            "+model.load_state_dict(torch.load(weights_path, map_location=device))"
        ],
        "core_change": "-model.load_state_dict(torch.load(weights_path)) +model.load_state_dict(torch.load(weights_path, map_location=device))",
        "core_API": "endswith"
    },
    {
        "commit_hash": "f44fcdd2690b94c7bede47a89aa8763882297462",
        "index": "a61bf48..2481cc0 100644",
        "commit_message": "style(models): fix style error (#1566)\n\nstyle(models): fix style error\n",
        "file": "YOLOX.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class YOLOXHead(nn.Module):",
            "grids = torch.cat(grids, dim=1).type(dtype)",
            "strides = torch.cat(strides, dim=1).type(dtype)",
            "",
            "-        outputs = torch.cat([(outputs[..., 0:2] + grids) * strides, torch.exp(outputs[..., 2:4]) * strides, outputs[..., 4:]], dim=-1)",
            "+        outputs = torch.cat([",
            "+            (outputs[..., 0:2] + grids) * strides,",
            "+            torch.exp(outputs[..., 2:4]) * strides,",
            "+            outputs[..., 4:]",
            "+        ], dim=-1)",
            "return outputs",
            "",
            "def get_losses("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 5,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 6033,
        "neg_line": [
            "-outputs = torch.cat([(outputs[..., 0:2] + grids) * strides, torch.exp(outputs[..., 2:4]) * strides, outputs[..., 4:]], dim=-1)"
        ],
        "pos_line": [
            "+outputs = torch.cat([",
            "+(outputs[..., 0:2] + grids) * strides,",
            "+torch.exp(outputs[..., 2:4]) * strides,",
            "+outputs[..., 4:]",
            "+], dim=-1)"
        ],
        "core_change": "-outputs = torch.cat([(outputs[..., 0:2] + grids) * strides, torch.exp(outputs[..., 2:4]) * strides, outputs[..., 4:]], dim=-1) +outputs = torch.cat([ +(outputs[..., 0:2] + grids) * strides, +torch.exp(outputs[..., 2:4]) * strides, +outputs[..., 4:] +], dim=-1)",
        "core_API": "cat"
    },
    {
        "commit_hash": "091568ec36900509b424ae3121a64b46fe5a836e",
        "index": "4192653c..b71361f5 100644",
        "commit_message": "fix DistributedTrainer (fix #505)\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class SimplePredictBuilder(GraphBuilder):",
            "self._ns_name, self._device))",
            "",
            "with tf.device(self._device), \\",
            "-                self._maybe_open_vs(), \\",
            "TowerContext(",
            "self._ns_name, is_training=False, vs_name=self._vs_name):",
            "inputs = input.get_input_tensors()"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=_maybe_open_vs))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=with_item))",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 6034,
        "neg_line": [
            "-self._maybe_open_vs(), \\"
        ],
        "pos_line": [],
        "core_change": "-self._maybe_open_vs(), \\",
        "core_API": "device"
    },
    {
        "commit_hash": "d9316bf8bc742dfa7635cf0cb2d5fea0cd1b0d00",
        "index": "7e8af27e..5f86993c 100644",
        "commit_message": "Fix mutable proj_out weight in the Attention layer (#73)\n\n* Catch unused params in DDP\n\n* Fix proj_out, add test\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class AttentionBlock(nn.Module):",
            "a = torch.einsum(\"bts,bcs->bct\", weight, v)",
            "h = a.reshape(bs, -1, length)",
            "",
            "-        h = self.proj_out(h)",
            "+        h = self.proj(h)",
            "h = h.reshape(b, c, *spatial)",
            "",
            "result = x + h"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=proj_out), value='proj')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 6035,
        "neg_line": [
            "-h = self.proj_out(h)"
        ],
        "pos_line": [
            "+h = self.proj(h)"
        ],
        "core_change": "-h = self.proj_out(h) +h = self.proj(h)",
        "core_API": "einsum"
    },
    {
        "commit_hash": "dcd5cf92bd094600b687a0a26103e3afa75e55c3",
        "index": "f459cefd82..d1723c1a59 100644",
        "commit_message": "fixed failing core.general unit tests.\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def linspace(start, stop, num, axis=None, dev=None):",
            "if axis is None:",
            "axis = -1",
            "dev = default_device(dev)",
            "-    with _tf.device('/' + dev.upper()):",
            "+    with _tf.device(ivy.dev_from_str(dev)):",
            "return _tf.linspace(start, stop, num, axis=axis)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=call), position=1)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=2, insert_id=2034770)",
            "Insert(target_node=ASTNode(type=attribute), node=('identifier', 'ivy'), position=0, insert_id=2034771)",
            "Update(target_node=ASTNode(type=identifier, text=upper), value='dev_from_str')",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=dev), position=1)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=), text=)), position=2)",
            "Delete(target_node=ASTNode(type=string, text='/'))",
            "Delete(target_node=ASTNode(type=+, text=+))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=binary_operator))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 6037,
        "neg_line": [
            "-with _tf.device('/' + dev.upper()):"
        ],
        "pos_line": [
            "+with _tf.device(ivy.dev_from_str(dev)):"
        ],
        "core_change": "-with _tf.device('/' + dev.upper()): +with _tf.device(ivy.dev_from_str(dev)):",
        "core_API": "device"
    },
    {
        "commit_hash": "3b6a6a23df9c8d4dc765e78ede3f5b43e8359f11",
        "index": "768dedc9c6..fb63a7c63b 100644",
        "commit_message": "fix tensorflow asarray inplace updating its inputs\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def asarray(",
            "",
            "dtype = ivy.as_ivy_dtype(ivy.default_dtype(dtype=dtype, item=obj))",
            "return tf.convert_to_tensor(",
            "-                    ivy.nested_map(obj, lambda x: tf.cast(x, dtype)),",
            "+                    ivy.nested_map(obj, lambda x: tf.cast(x, dtype), shallow=False),",
            "dtype=dtype,",
            ")",
            "else:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=1957731)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=1957732)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'shallow'), position=0, insert_id=1957733)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1957734)",
            "Insert(target_node=IN(type=keyword_argument), node=('false', 'False'), position=2, insert_id=1957735)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 6038,
        "neg_line": [
            "-ivy.nested_map(obj, lambda x: tf.cast(x, dtype)),"
        ],
        "pos_line": [
            "+ivy.nested_map(obj, lambda x: tf.cast(x, dtype), shallow=False),"
        ],
        "core_change": "-ivy.nested_map(obj, lambda x: tf.cast(x, dtype)), +ivy.nested_map(obj, lambda x: tf.cast(x, dtype), shallow=False),",
        "core_API": "as_ivy_dtype"
    },
    {
        "commit_hash": "15c72be444fab0d1ba6879d097399b12f6a2a8b0",
        "index": "c856ffe..4f5f161 100644",
        "commit_message": "Fix coordinate system conventions in renderer\n\nSummary:\n## Updates\n\n- Defined the world and camera coordinates according to this figure. The world coordinates are defined as having +Y up, +X left and +Z in.\n\n{F230888499}\n\n- Removed all flipping from blending functions.\n- Updated the rasterizer to return images with +Y up and +X left.\n- Updated all the mesh rasterizer tests\n    - The expected values are now defined in terms of the default +Y up, +X left\n    - Added tests where the triangles in the meshes are non symmetrical so that it is clear which direction +X and +Y are\n\n## Questions:\n- Should we have **scene settings** instead of raster settings?\n    - To be more correct we should be [z clipping in the rasterizer based on the far/near clipping planes](https://github.com/ShichenLiu/SoftRas/blob/master/soft_renderer/cuda/soft_rasterize_cuda_kernel.cu#L400) - these values are also required in the blending functions so should we make these scene level parameters and have a scene settings tuple which is available to the rasterizer and shader?\n\nReviewed By: gkioxari\n\nDifferential Revision: D20208604\n\nfbshipit-source-id: 55787301b1bffa0afa9618f0a0886cc681da51f3\n\n",
        "file": "pytorch3d.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def camera_position_from_spherical_angles(",
            "azim = math.pi / 180.0 * azim",
            "x = dist * torch.cos(elev) * torch.sin(azim)",
            "y = dist * torch.sin(elev)",
            "-    z = -dist * torch.cos(elev) * torch.cos(azim)",
            "+    z = dist * torch.cos(elev) * torch.cos(azim)",
            "camera_position = torch.stack([x, y, z], dim=1)",
            "if camera_position.dim() == 0:",
            "camera_position = camera_position.view(1, -1)  # add batch dim."
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=IN(type=root), node=ASTNode(type=binary_operator), position=0)",
            "Insert(target_node=ASTNode(type=binary_operator), node=('binary_operator', None), position=0, insert_id=929078)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=binary_operator), position=0)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=ERROR), position=1)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=*, text=*), position=2)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=call), position=3)",
            "Insert(target_node=ASTNode(type=ERROR), node=('identifier', 'dist'), position=2, insert_id=929079)",
            "Delete(target_node=ASTNode(type=-, text=-))",
            "Delete(target_node=ASTNode(type=identifier, text=dist))",
            "Delete(target_node=ASTNode(type=binary_operator))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 6039,
        "neg_line": [
            "-z = -dist * torch.cos(elev) * torch.cos(azim)"
        ],
        "pos_line": [
            "+z = dist * torch.cos(elev) * torch.cos(azim)"
        ],
        "core_change": "-z = -dist * torch.cos(elev) * torch.cos(azim) +z = dist * torch.cos(elev) * torch.cos(azim)",
        "core_API": "cos"
    },
    {
        "commit_hash": "613856f54ff203cbb3e3d7d9657e914a8d4aa31f",
        "index": "2fb2f4666..ee92fd460 100644",
        "commit_message": "Bug fix: espnet/espnet2/torch_utils/recursive_op\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def recursive_average(obj, weight: torch.Tensor, distributed: bool = False):",
            "obj = recursive_sum(obj, weight, distributed)",
            "weight = weight.sum()",
            "if distributed:",
            "-        torch.distributed.all_reduce(weight, op=torch.distributed.ReduceOP.SUM)",
            "+        torch.distributed.all_reduce(weight, op=torch.distributed.reduce_op.SUM)",
            "# Normalize weight to be sum-to-1",
            "obj = recursive_divide(obj, weight)",
            "return obj, weight"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=ReduceOP), value='reduce_op')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 6040,
        "neg_line": [
            "-torch.distributed.all_reduce(weight, op=torch.distributed.ReduceOP.SUM)"
        ],
        "pos_line": [
            "+torch.distributed.all_reduce(weight, op=torch.distributed.reduce_op.SUM)"
        ],
        "core_change": "-torch.distributed.all_reduce(weight, op=torch.distributed.ReduceOP.SUM) +torch.distributed.all_reduce(weight, op=torch.distributed.reduce_op.SUM)",
        "core_API": "sum"
    },
    {
        "commit_hash": "9639a45f1e1a33071d53f709d2fb705d41c8f22f",
        "index": "7aee752a..a029fc51 100644",
        "commit_message": "Augmentation Base Refactor (#2117)\n\n* refactor\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Updated augmentation base.\n\n* Removed label & start apply/inverse\n\n* Finished augmentaion base refactor\n\n* container refactoring\n\n* Added missing files\n\n* Added ops\n\n* Update sequential ops\n\n* Almost there\n\n* Fixed computation matrix computation\n\n* Fixed randomcrop\n\n* Fixed erasing\n\n* almost almost\n\n* finished\n\n* Added missing file\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Bug and typing fix\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Added list typing\n\n* fixed test base\n\n* Fixed typing\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* bug fix\n\n* Update kornia/augmentation/_2d/geometric/crop.py\n\nCo-authored-by: João Gustavo A. Amorim <joaogustavoamorim@gmail.com>\n\n* Fixed build-docs\n\n* Fixed bfloat16 issue on torch1.13.1\n\n* Revert the last commit\n\n* Fixed typing\n\n* Fixed typos\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Typo fix\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: João Gustavo A. Amorim <joaogustavoamorim@gmail.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def bbox_to_mask(boxes: torch.Tensor, width: int, height: int) -> torch.Tensor:",
            "\"\"\"",
            "validate_bbox(boxes)",
            "# zero padding the surroudings",
            "-    mask = torch.zeros((len(boxes), height + 2, width + 2), dtype=torch.float, device=boxes.device)",
            "+    mask = torch.zeros((len(boxes), height + 2, width + 2), dtype=boxes.dtype, device=boxes.device)",
            "# push all points one pixel off",
            "# in order to zero-out the fully filled rows or columns",
            "box_i = (boxes + 1).long()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=torch), value='boxes')",
            "Update(target_node=ASTNode(type=identifier, text=float), value='dtype')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 6041,
        "neg_line": [
            "-mask = torch.zeros((len(boxes), height + 2, width + 2), dtype=torch.float, device=boxes.device)"
        ],
        "pos_line": [
            "+mask = torch.zeros((len(boxes), height + 2, width + 2), dtype=boxes.dtype, device=boxes.device)"
        ],
        "core_change": "-mask = torch.zeros((len(boxes), height + 2, width + 2), dtype=torch.float, device=boxes.device) +mask = torch.zeros((len(boxes), height + 2, width + 2), dtype=boxes.dtype, device=boxes.device)",
        "core_API": "zeros"
    },
    {
        "commit_hash": "455c6390938a5c737fa63e78396cedae41e4e87e",
        "index": "7ab6de32b..8dffb131a 100644",
        "commit_message": "CDN urls (#4030)\n\n* [file_utils] use_cdn + documentation\n\n* Move to cdn. urls for weights\n\n* [urls] Hotfix for bert-base-japanese\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "models",
        "change": [
            "from .tokenization_utils import BatchEncoding",
            "logger = logging.getLogger(__name__)",
            "",
            "TF_GPT2_PRETRAINED_MODEL_ARCHIVE_MAP = {",
            "-    \"gpt2\": \"https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-tf_model.h5\",",
            "-    \"gpt2-medium\": \"https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-medium-tf_model.h5\",",
            "-    \"gpt2-large\": \"https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-large-tf_model.h5\",",
            "-    \"gpt2-xl\": \"https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-xl-tf_model.h5\",",
            "-    \"distilgpt2\": \"https://s3.amazonaws.com/models.huggingface.co/bert/distilgpt2-tf_model.h5\",",
            "+    \"gpt2\": \"https://cdn.huggingface.co/gpt2-tf_model.h5\",",
            "+    \"gpt2-medium\": \"https://cdn.huggingface.co/gpt2-medium-tf_model.h5\",",
            "+    \"gpt2-large\": \"https://cdn.huggingface.co/gpt2-large-tf_model.h5\",",
            "+    \"gpt2-xl\": \"https://cdn.huggingface.co/gpt2-xl-tf_model.h5\",",
            "+    \"distilgpt2\": \"https://cdn.huggingface.co/distilgpt2-tf_model.h5\",",
            "}"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=,, text=,), node=ASTNode(type=dictionary), position=9)",
            "Insert(target_node=ASTNode(type=dictionary), node=(',', ','), position=2, insert_id=2689789)",
            "Insert(target_node=ASTNode(type=pair), node=(':', ':'), position=1, insert_id=2689790)",
            "Update(target_node=ASTNode(type=string, text=\"https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-tf_model.h5\"), value='\"https://cdn.huggingface.co/gpt2-tf_model.h5\"')",
            "Update(target_node=ASTNode(type=string, text=\"https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-medium-tf_model.h5\"), value='\"https://cdn.huggingface.co/gpt2-medium-tf_model.h5\"')",
            "Update(target_node=ASTNode(type=string, text=\"https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-large-tf_model.h5\"), value='\"https://cdn.huggingface.co/gpt2-large-tf_model.h5\"')",
            "Update(target_node=ASTNode(type=string, text=\"https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-xl-tf_model.h5\"), value='\"https://cdn.huggingface.co/gpt2-xl-tf_model.h5\"')",
            "Move(target_node=ASTNode(type=pair), node=ASTNode(type=:, text=:), position=1)",
            "Update(target_node=ASTNode(type=string, text=\"https://s3.amazonaws.com/models.huggingface.co/bert/distilgpt2-tf_model.h5\"), value='\"https://cdn.huggingface.co/distilgpt2-tf_model.h5\"')",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 5,
        "minus_line": 5,
        "AST_diff_line": 11,
        "number": 6045,
        "neg_line": [
            "-\"gpt2\": \"https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-tf_model.h5\",",
            "-\"gpt2-medium\": \"https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-medium-tf_model.h5\",",
            "-\"gpt2-large\": \"https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-large-tf_model.h5\",",
            "-\"gpt2-xl\": \"https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-xl-tf_model.h5\",",
            "-\"distilgpt2\": \"https://s3.amazonaws.com/models.huggingface.co/bert/distilgpt2-tf_model.h5\","
        ],
        "pos_line": [
            "+\"gpt2\": \"https://cdn.huggingface.co/gpt2-tf_model.h5\",",
            "+\"gpt2-medium\": \"https://cdn.huggingface.co/gpt2-medium-tf_model.h5\",",
            "+\"gpt2-large\": \"https://cdn.huggingface.co/gpt2-large-tf_model.h5\",",
            "+\"gpt2-xl\": \"https://cdn.huggingface.co/gpt2-xl-tf_model.h5\",",
            "+\"distilgpt2\": \"https://cdn.huggingface.co/distilgpt2-tf_model.h5\","
        ],
        "core_change": "-\"gpt2\": \"https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-tf_model.h5\", -\"gpt2-medium\": \"https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-medium-tf_model.h5\", -\"gpt2-large\": \"https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-large-tf_model.h5\", -\"gpt2-xl\": \"https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-xl-tf_model.h5\", -\"distilgpt2\": \"https://s3.amazonaws.com/models.huggingface.co/bert/distilgpt2-tf_model.h5\", +\"gpt2\": \"https://cdn.huggingface.co/gpt2-tf_model.h5\", +\"gpt2-medium\": \"https://cdn.huggingface.co/gpt2-medium-tf_model.h5\", +\"gpt2-large\": \"https://cdn.huggingface.co/gpt2-large-tf_model.h5\", +\"gpt2-xl\": \"https://cdn.huggingface.co/gpt2-xl-tf_model.h5\", +\"distilgpt2\": \"https://cdn.huggingface.co/distilgpt2-tf_model.h5\",",
        "core_API": "getLogger"
    },
    {
        "commit_hash": "426f8a8d15449c5e78efc4604e2b0fd4a8577fb8",
        "index": "2aa442279..2a11f12a0 100644",
        "commit_message": "[tune] Fix tutorial training on GPU (#12914)\n\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "space = {",
            "hyperopt_search = HyperOptSearch(space, metric=\"mean_accuracy\", mode=\"max\")",
            "",
            "analysis = tune.run(train_mnist, num_samples=10, search_alg=hyperopt_search)",
            "+",
            "+# To enable GPUs, use this instead:",
            "+# analysis = tune.run(",
            "+#     train_mnist, config=search_space, resources_per_trial={'gpu': 1})",
            "+",
            "# __run_searchalg_end__",
            "",
            "# __run_analysis_begin__"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 0,
        "minus_line": 0,
        "AST_diff_line": 17,
        "number": 6050,
        "neg_line": [],
        "pos_line": [
            "+",
            "+# To enable GPUs, use this instead:",
            "+# analysis = tune.run(",
            "+#     train_mnist, config=search_space, resources_per_trial={'gpu': 1})",
            "+"
        ],
        "core_change": "+ +# To enable GPUs, use this instead: +# analysis = tune.run( +#     train_mnist, config=search_space, resources_per_trial={'gpu': 1}) +",
        "core_API": "run"
    },
    {
        "commit_hash": "74e4a6e9da114e7c84a58b48ef0bae19bdef3074",
        "index": "4903d89..dce4465 100644",
        "commit_message": "Fix issues with tests\n\n",
        "file": "nebullvm.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_onnxruntime(",
            "res_orig = tuple(model(*inputs_example))",
            "assert all(",
            "[",
            "-                    torch.allclose(res_tensor, res_orig_tensor, rtol=1e-03)",
            "+                    torch.allclose(res_tensor, res_orig_tensor, rtol=2e-01)",
            "for (res_tensor, res_orig_tensor) in zip(res, res_orig)",
            "]",
            ")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=float, text=1e-03), value='2e-01')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 6054,
        "neg_line": [
            "-torch.allclose(res_tensor, res_orig_tensor, rtol=1e-03)"
        ],
        "pos_line": [
            "+torch.allclose(res_tensor, res_orig_tensor, rtol=2e-01)"
        ],
        "core_change": "-torch.allclose(res_tensor, res_orig_tensor, rtol=1e-03) +torch.allclose(res_tensor, res_orig_tensor, rtol=2e-01)",
        "core_API": "allclose"
    },
    {
        "commit_hash": "d4c2bcf8a385b54809b3d14026e1fc26622b9132",
        "index": "fe1ed4ca..88751312 100644",
        "commit_message": "fix nois in ldm\n\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class LatentDiffusion(DiffusionPipeline):",
            "# 3. optionally sample variance",
            "variance = 0",
            "if eta > 0:",
            "-                noise = self.noise_scheduler.sample_noise(image.shape, device=image.device, generator=generator)",
            "+                noise = torch.randn(image.shape, generator=generator, device=image.device)",
            "variance = self.noise_scheduler.get_variance(t, num_inference_steps).sqrt() * eta * noise",
            "",
            "# 4. set current image to prev_image: x_t -> x_t-1"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Move(target_node=ASTNode(type=keyword_argument), node=ASTNode(type=argument_list), position=3)",
            "Update(target_node=ASTNode(type=identifier, text=self), value='torch')",
            "Update(target_node=ASTNode(type=identifier, text=noise_scheduler), value='randn')",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=6, insert_id=113276)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=sample_noise))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 6055,
        "neg_line": [
            "-noise = self.noise_scheduler.sample_noise(image.shape, device=image.device, generator=generator)"
        ],
        "pos_line": [
            "+noise = torch.randn(image.shape, generator=generator, device=image.device)"
        ],
        "core_change": "-noise = self.noise_scheduler.sample_noise(image.shape, device=image.device, generator=generator) +noise = torch.randn(image.shape, generator=generator, device=image.device)",
        "core_API": "sample_noise"
    },
    {
        "commit_hash": "6a7ca50cf580f48beea5435859b36956b6a6c636",
        "index": "ac8cd820..899740b4 100644",
        "commit_message": "fix bugs\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TGN(torch.nn.Module):",
            ")",
            "",
            "self.register_buffer('memory', torch.empty(num_nodes, memory_dim))",
            "-        self.register_buffer('last_update', torch.empty(num_nodes))",
            "+        self.register_buffer('last_update',",
            "+                             torch.empty(num_nodes, dtype=torch.long))",
            "self.previous_events = None",
            "",
            "self.reset_parameters()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1011388)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1011389)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=1011390)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1011391)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=1011392)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1011393)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1011394)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'long'), position=2, insert_id=1011395)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 6059,
        "neg_line": [
            "-self.register_buffer('last_update', torch.empty(num_nodes))"
        ],
        "pos_line": [
            "+self.register_buffer('last_update',",
            "+torch.empty(num_nodes, dtype=torch.long))"
        ],
        "core_change": "-self.register_buffer('last_update', torch.empty(num_nodes)) +self.register_buffer('last_update', +torch.empty(num_nodes, dtype=torch.long))",
        "core_API": "register_buffer"
    },
    {
        "commit_hash": "78057f3ad8f69998cfa8b8c88fe51c5943f67e41",
        "index": "cb2374813..bc410c961 100644",
        "commit_message": "Fix batch processing in enh_inference\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def config_file(tmp_path: Path):",
            "",
            "",
            "@pytest.mark.execution_timeout(5)",
            "+@pytest.mark.parametrize(\"batch_size\", [1, 2])",
            "@pytest.mark.parametrize(",
            "\"input_size, segment_size, hop_size\", [(16000, None, None), (35000, 2.4, 0.8)]",
            ")",
            "-def test_SeparateSpeech(config_file, input_size, segment_size, hop_size):",
            "+def test_SeparateSpeech(config_file, batch_size, input_size, segment_size, hop_size):",
            "if not is_torch_1_2_plus:",
            "pytest.skip(\"Pytorch Version Under 1.2 is not supported for Enh task\")",
            "",
            "separate_speech = SeparateSpeech(",
            "enh_train_config=config_file, segment_size=segment_size, hop_size=hop_size",
            ")",
            "-    wav = torch.rand(1, input_size)",
            "+    wav = torch.rand(batch_size, input_size)",
            "separate_speech(wav, fs=8000)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=decorated_definition), node=('decorator', None), position=1, insert_id=142589)",
            "Insert(target_node=IN(type=decorator), node=('@', '@'), position=0, insert_id=142590)",
            "Insert(target_node=IN(type=decorator), node=('call', None), position=1, insert_id=142591)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=142592)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=142593)",
            "Insert(target_node=ASTNode(type=parameters), node=('identifier', 'batch_size'), position=3, insert_id=142594)",
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=4, insert_id=142595)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=142596)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=142597)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'parametrize'), position=2, insert_id=142598)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=142599)",
            "Insert(target_node=IN(type=argument_list), node=('string', '\"batch_size\"'), position=1, insert_id=142600)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=142601)",
            "Insert(target_node=IN(type=argument_list), node=('list', None), position=3, insert_id=142602)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=142603)",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'batch_size'), position=1, insert_id=142604)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'pytest'), position=0, insert_id=142605)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=142606)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'mark'), position=2, insert_id=142607)",
            "Insert(target_node=IN(type=list), node=('[', '['), position=0, insert_id=142608)",
            "Insert(target_node=IN(type=list), node=('integer', '1'), position=1, insert_id=142609)",
            "Insert(target_node=IN(type=list), node=(',', ','), position=2, insert_id=142610)",
            "Insert(target_node=IN(type=list), node=('integer', '2'), position=3, insert_id=142611)",
            "Insert(target_node=IN(type=list), node=(']', ']'), position=4, insert_id=142612)",
            "Delete(target_node=ASTNode(type=integer, text=1))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 25,
        "number": 6062,
        "neg_line": [
            "-def test_SeparateSpeech(config_file, input_size, segment_size, hop_size):",
            "-wav = torch.rand(1, input_size)"
        ],
        "pos_line": [
            "+@pytest.mark.parametrize(\"batch_size\", [1, 2])",
            "+def test_SeparateSpeech(config_file, batch_size, input_size, segment_size, hop_size):",
            "+wav = torch.rand(batch_size, input_size)"
        ],
        "core_change": "+@pytest.mark.parametrize(\"batch_size\", [1, 2]) -def test_SeparateSpeech(config_file, input_size, segment_size, hop_size): +def test_SeparateSpeech(config_file, batch_size, input_size, segment_size, hop_size): -wav = torch.rand(1, input_size) +wav = torch.rand(batch_size, input_size)",
        "core_API": "execution_timeout"
    },
    {
        "commit_hash": "f8798c1c13bece96a0fd92d9b7e7bd69576fa34d",
        "index": "71a47fe8..77489be2 100644",
        "commit_message": "enable `disallow_incomplete_defs` on mypy (#2094)\n\n* enable `disallow_incomplete_defs` on mypy\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix `blur_pool2d` doc\n\n* finish v1: works on torch 1.13.1\n\n- Remove JIT support for Boxes3D\n\n* rip off the np typing\n\n* replace `Size` with `Tuple[int, ...]` on augs\n\n* add `Dtype` to kornia.filters.kernels\n\n* minor fix after rebase\n\n* Remove old torch from typing CI\n\n---------\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "def batched_forward(",
            "if st >= end:",
            "continue",
            "out_list.append(model_dev(data[st:end].to(device), **kwargs))",
            "-        out = torch.cat(out_list, dim=0)",
            "+        out = concatenate(out_list, 0)",
            "return out.to(data.device)",
            "return model(data, **kwargs)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=torch), value='concatenate')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=torch), position=0)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=integer, text=0), position=3)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=cat))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=identifier, text=dim))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 6071,
        "neg_line": [
            "-out = torch.cat(out_list, dim=0)"
        ],
        "pos_line": [
            "+out = concatenate(out_list, 0)"
        ],
        "core_change": "-out = torch.cat(out_list, dim=0) +out = concatenate(out_list, 0)",
        "core_API": "append"
    },
    {
        "commit_hash": "68badf3b9497b27ede89cb586770c3b14aba31de",
        "index": "d287c172..d0675823 100644",
        "commit_message": "Zca inverse transform fix (#695)\n\n* fix inv transform\n\n* update zca test\n\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def zca_mean(inp: torch.Tensor, dim: int = 0,",
            "",
            "T_inv: Optional[torch.Tensor] = None",
            "if return_inverse:",
            "-        T_inv = (U).mm(torch.sqrt(S) * U.t())",
            "+        T_inv = (U).mm(torch.sqrt(S + eps) * U.t())",
            "",
            "return T, mean, T_inv"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('binary_operator', None), position=1, insert_id=436182)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=identifier, text=S), position=0)",
            "Insert(target_node=IN(type=binary_operator), node=('+', '+'), position=1, insert_id=436183)",
            "Insert(target_node=IN(type=binary_operator), node=('identifier', 'eps'), position=2, insert_id=436184)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 6077,
        "neg_line": [
            "-T_inv = (U).mm(torch.sqrt(S) * U.t())"
        ],
        "pos_line": [
            "+T_inv = (U).mm(torch.sqrt(S + eps) * U.t())"
        ],
        "core_change": "-T_inv = (U).mm(torch.sqrt(S) * U.t()) +T_inv = (U).mm(torch.sqrt(S + eps) * U.t())",
        "core_API": "sqrt"
    },
    {
        "commit_hash": "9000ad931722064fb2efe38649389a5154538ce4",
        "index": "2106c94d..4e3a1c6f 100644",
        "commit_message": "various improvements and fixes\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Model(Module):",
            "# at most one terminal",
            "assertions.append(",
            "tf.debugging.assert_less_equal(",
            "-                x=tf.count_nonzero(input_tensor=terminal, dtype=util.tf_dtype(dtype='int')),",
            "+                x=tf.math.count_nonzero(input_tensor=terminal, dtype=util.tf_dtype(dtype='int')),",
            "y=tf.constant(value=1, dtype=util.tf_dtype(dtype='int'))",
            ")",
            ")",
            "# if terminal, last timestep in batch",
            "assertions.append(",
            "-            tf.debugging.assert_equal(x=tf.reduce_any(input_tensor=terminal), y=terminal[-1])",
            "+            tf.debugging.assert_equal(x=tf.math.reduce_any(input_tensor=terminal), y=terminal[-1])",
            ")",
            "",
            "# Set global tensors"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2230214)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'assertions'), position=0, insert_id=2230215)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2230216)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'append'), position=2, insert_id=2230217)",
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=2230218)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=2230219)",
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=2230220)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=2230221)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=tf), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'math'), position=2, insert_id=2230222)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=tf), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'math'), position=2, insert_id=2230223)",
            "Delete(target_node=ASTNode(type=identifier, text=assertions))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=append))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 19,
        "number": 6078,
        "neg_line": [
            "-x=tf.count_nonzero(input_tensor=terminal, dtype=util.tf_dtype(dtype='int')),",
            "-tf.debugging.assert_equal(x=tf.reduce_any(input_tensor=terminal), y=terminal[-1])"
        ],
        "pos_line": [
            "+x=tf.math.count_nonzero(input_tensor=terminal, dtype=util.tf_dtype(dtype='int')),",
            "+tf.debugging.assert_equal(x=tf.math.reduce_any(input_tensor=terminal), y=terminal[-1])"
        ],
        "core_change": "-x=tf.count_nonzero(input_tensor=terminal, dtype=util.tf_dtype(dtype='int')), +x=tf.math.count_nonzero(input_tensor=terminal, dtype=util.tf_dtype(dtype='int')), -tf.debugging.assert_equal(x=tf.reduce_any(input_tensor=terminal), y=terminal[-1]) +tf.debugging.assert_equal(x=tf.math.reduce_any(input_tensor=terminal), y=terminal[-1])",
        "core_API": "append"
    },
    {
        "commit_hash": "ca52108c2b89694e82713e1b4ca5c28902bc5435",
        "index": "2da323c..814b695 100644",
        "commit_message": "Fix some model support functions\n\n",
        "file": "pytorch-image-models.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class EfficientFormer(nn.Module):",
            "def get_classifier(self):",
            "return self.head, self.head_dist",
            "",
            "-    def reset_classifier(self, num_classes, global_pool=None, distillation=None):",
            "+    def reset_classifier(self, num_classes, global_pool=None):",
            "self.num_classes = num_classes",
            "if global_pool is not None:",
            "self.global_pool = global_pool",
            "self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()",
            "-        if self.dist:",
            "-            self.head_dist = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()",
            "+        self.head_dist = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()",
            "",
            "@torch.jit.ignore",
            "def set_distilled_training(self, enable=True):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=module), node=ASTNode(type=expression_statement), position=8)",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=distillation))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=none, text=None))",
            "Delete(target_node=ASTNode(type=default_parameter))",
            "Delete(target_node=ASTNode(type=if, text=if))",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=dist))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=block))",
            "Delete(target_node=ASTNode(type=if_statement))"
        ],
        "plus_line": 2,
        "minus_line": 3,
        "AST_diff_line": 14,
        "number": 6080,
        "neg_line": [
            "-def reset_classifier(self, num_classes, global_pool=None, distillation=None):",
            "-if self.dist:",
            "-self.head_dist = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()"
        ],
        "pos_line": [
            "+def reset_classifier(self, num_classes, global_pool=None):",
            "+self.head_dist = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()"
        ],
        "core_change": "-def reset_classifier(self, num_classes, global_pool=None, distillation=None): +def reset_classifier(self, num_classes, global_pool=None): -if self.dist: -self.head_dist = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity() +self.head_dist = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()",
        "core_API": "Linear"
    },
    {
        "commit_hash": "2bcc1050f076e8f8fcbc2ede262e7950bf7fc9d2",
        "index": "4342c8a2..166abc1f 100644",
        "commit_message": "fix arange dtype\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def grid_index(height, width, device=None):",
            "",
            "",
            "def grid_pos(height, width, dtype=None, device=None):",
            "+    dtype = torch.float if dtype is None else dtype",
            "x = torch.arange(width, dtype=dtype, device=device)",
            "y = (height - 1) - torch.arange(height, dtype=dtype, device=device)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('block', None), position=0, insert_id=1055245)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=1055246)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1055247)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'dtype'), position=0, insert_id=1055248)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1055249)",
            "Insert(target_node=IN(type=assignment), node=('conditional_expression', None), position=2, insert_id=1055250)",
            "Insert(target_node=IN(type=conditional_expression), node=('attribute', None), position=0, insert_id=1055251)",
            "Insert(target_node=IN(type=conditional_expression), node=('if', 'if'), position=1, insert_id=1055252)",
            "Insert(target_node=IN(type=conditional_expression), node=('comparison_operator', None), position=2, insert_id=1055253)",
            "Insert(target_node=IN(type=conditional_expression), node=('else', 'else'), position=3, insert_id=1055254)",
            "Insert(target_node=IN(type=conditional_expression), node=('identifier', 'dtype'), position=4, insert_id=1055255)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1055256)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1055257)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'float'), position=2, insert_id=1055258)",
            "Insert(target_node=IN(type=comparison_operator), node=('identifier', 'dtype'), position=0, insert_id=1055259)",
            "Insert(target_node=IN(type=comparison_operator), node=('is', 'is'), position=1, insert_id=1055260)",
            "Insert(target_node=IN(type=comparison_operator), node=('none', 'None'), position=2, insert_id=1055261)",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 18,
        "number": 6081,
        "neg_line": [],
        "pos_line": [
            "+dtype = torch.float if dtype is None else dtype"
        ],
        "core_change": "+dtype = torch.float if dtype is None else dtype",
        "core_API": "arange"
    },
    {
        "commit_hash": "95b6d985ffb231d9a2047c039c95660913063b9d",
        "index": "0945b652..dfc76177 100644",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestEndpointSpanExtractor:",
            "# for both the forward and backward directions.",
            "extractor = EndpointSpanExtractor(8, \"x,y\", use_exclusive_start_indices=True)",
            "indices = torch.LongTensor([[[1, 3], [2, 4]], [[0, 2], [0, 1]]])",
            "-        sequence_mask = torch.LongTensor([[1, 1, 1, 1, 1], [1, 1, 1, 0, 0]])",
            "+        sequence_mask = torch.BoolTensor(",
            "+            [[True, True, True, True, True], [True, True, True, False, False]]",
            "+        )",
            "",
            "span_representations = extractor(sequence_tensor, indices, sequence_mask=sequence_mask)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=LongTensor), value='BoolTensor')",
            "Insert(target_node=ASTNode(type=list), node=(',', ','), position=2, insert_id=19925)",
            "Insert(target_node=ASTNode(type=list), node=('true', 'True'), position=1, insert_id=19926)",
            "Insert(target_node=ASTNode(type=list), node=('true', 'True'), position=4, insert_id=19927)",
            "Insert(target_node=ASTNode(type=list), node=('true', 'True'), position=6, insert_id=19928)",
            "Insert(target_node=ASTNode(type=list), node=('true', 'True'), position=9, insert_id=19929)",
            "Insert(target_node=ASTNode(type=list), node=('true', 'True'), position=11, insert_id=19930)",
            "Insert(target_node=ASTNode(type=list), node=('true', 'True'), position=1, insert_id=19931)",
            "Insert(target_node=ASTNode(type=list), node=('true', 'True'), position=4, insert_id=19932)",
            "Insert(target_node=ASTNode(type=list), node=('true', 'True'), position=6, insert_id=19933)",
            "Insert(target_node=ASTNode(type=list), node=('false', 'False'), position=8, insert_id=19934)",
            "Move(target_node=ASTNode(type=list), node=ASTNode(type=,, text=,), position=9)",
            "Insert(target_node=ASTNode(type=list), node=('false', 'False'), position=10, insert_id=19935)",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=integer, text=0))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=integer, text=0))"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 24,
        "number": 6083,
        "neg_line": [
            "-sequence_mask = torch.LongTensor([[1, 1, 1, 1, 1], [1, 1, 1, 0, 0]])"
        ],
        "pos_line": [
            "+sequence_mask = torch.BoolTensor(",
            "+[[True, True, True, True, True], [True, True, True, False, False]]",
            "+)"
        ],
        "core_change": "-sequence_mask = torch.LongTensor([[1, 1, 1, 1, 1], [1, 1, 1, 0, 0]]) +sequence_mask = torch.BoolTensor( +[[True, True, True, True, True], [True, True, True, False, False]] +)",
        "core_API": "LongTensor"
    },
    {
        "commit_hash": "b41a0440333380953add96d69f0d56a15821ecf0",
        "index": "45019d83..f5db2fb9 100644",
        "commit_message": "Use attention_mask in pretrained transformers (#3560)\n\n* Add attention_mask to PretrainedTransformerIndexer\n\n* Add attention_mask to PretrainedTransformerEmbedder\n\n* delete mask_padding_with_zero\n\n* set model_type for xlnet\n\n* Rename attention_mask -> mask\n\n* Fix PretrainedTransformerEmbedder\n\n* Add end-to-end test\n\nCo-authored-by: Dirk Groeneveld <groeneveld@gmail.com>\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class PretrainedTransformerEmbedder(TokenEmbedder):",
            "def get_output_dim(self):",
            "return self.output_dim",
            "",
            "-    def forward(self, token_ids: torch.LongTensor) -> torch.Tensor:  # type: ignore",
            "+    def forward(",
            "+        self, token_ids: torch.LongTensor, attention_mask: torch.LongTensor",
            "+    ) -> torch.Tensor:  # type: ignore",
            "",
            "-        return self.transformer_model(token_ids)[0]",
            "+        return self.transformer_model(input_ids=token_ids, attention_mask=attention_mask)[0]"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=4, insert_id=23003)",
            "Insert(target_node=ASTNode(type=parameters), node=('typed_parameter', None), position=5, insert_id=23004)",
            "Insert(target_node=IN(type=typed_parameter), node=('identifier', 'attention_mask'), position=0, insert_id=23005)",
            "Insert(target_node=IN(type=typed_parameter), node=(':', ':'), position=1, insert_id=23006)",
            "Insert(target_node=IN(type=typed_parameter), node=('type', None), position=2, insert_id=23007)",
            "Insert(target_node=IN(type=type), node=('attribute', None), position=0, insert_id=23008)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=23009)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=23010)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'LongTensor'), position=2, insert_id=23011)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=1, insert_id=23012)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=23013)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=23014)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'input_ids'), position=0, insert_id=23015)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=23016)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=identifier, text=token_ids), position=2)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'attention_mask'), position=0, insert_id=23017)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=23018)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'attention_mask'), position=2, insert_id=23019)"
        ],
        "plus_line": 4,
        "minus_line": 2,
        "AST_diff_line": 18,
        "number": 6084,
        "neg_line": [
            "-def forward(self, token_ids: torch.LongTensor) -> torch.Tensor:  # type: ignore",
            "-return self.transformer_model(token_ids)[0]"
        ],
        "pos_line": [
            "+def forward(",
            "+self, token_ids: torch.LongTensor, attention_mask: torch.LongTensor",
            "+) -> torch.Tensor:  # type: ignore",
            "+return self.transformer_model(input_ids=token_ids, attention_mask=attention_mask)[0]"
        ],
        "core_change": "-def forward(self, token_ids: torch.LongTensor) -> torch.Tensor:  # type: ignore +def forward( +self, token_ids: torch.LongTensor, attention_mask: torch.LongTensor +) -> torch.Tensor:  # type: ignore -return self.transformer_model(token_ids)[0] +return self.transformer_model(input_ids=token_ids, attention_mask=attention_mask)[0]",
        "core_API": "transformer_model"
    },
    {
        "commit_hash": "e7298b5d387118f55e51b89d29892ebe4b43fd73",
        "index": "f63fdc694..4f942f9b3 100644",
        "commit_message": "fix parallel devices return type & add copyright (#6215)\n\n\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class AcceleratorConnector(object):",
            "return len(gpus)",
            "",
            "@property",
            "-    def parallel_devices(self) -> Union[List[torch.device], int]:",
            "+    def parallel_devices(self) -> List[Union[torch.device, int]]:",
            "if self.on_gpu:",
            "devices = [torch.device(\"cuda\", i) for i in self.parallel_device_ids]",
            "elif self.on_tpu:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=Union), value='List')",
            "Insert(target_node=ASTNode(type=subscript), node=('subscript', None), position=2, insert_id=545246)",
            "Insert(target_node=ASTNode(type=subscript), node=(']', ']'), position=3, insert_id=545247)",
            "Update(target_node=ASTNode(type=identifier, text=List), value='Union')",
            "Move(target_node=IN(type=subscript), node=ASTNode(type=identifier, text=List), position=0)",
            "Move(target_node=IN(type=subscript), node=ASTNode(type=[, text=[), position=1)",
            "Move(target_node=IN(type=subscript), node=ASTNode(type=attribute), position=2)",
            "Move(target_node=IN(type=subscript), node=ASTNode(type=,, text=,), position=3)",
            "Move(target_node=IN(type=subscript), node=ASTNode(type=identifier, text=int), position=4)",
            "Move(target_node=IN(type=subscript), node=ASTNode(type=], text=]), position=5)",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 12,
        "number": 6085,
        "neg_line": [
            "-def parallel_devices(self) -> Union[List[torch.device], int]:"
        ],
        "pos_line": [
            "+def parallel_devices(self) -> List[Union[torch.device, int]]:"
        ],
        "core_change": "-def parallel_devices(self) -> Union[List[torch.device], int]: +def parallel_devices(self) -> List[Union[torch.device, int]]:",
        "core_API": "device"
    },
    {
        "commit_hash": "7258ba0b313cdf56f7f357fa7a0e289a9cb35de3",
        "index": "21c8c87..b45b391 100644",
        "commit_message": "🔧 Fix #70\n\n",
        "file": "TensorFlowTTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def main():",
            "mel_outputs, post_mel_outputs, stop_outputs, alignment_historys = tacotron2.inference(",
            "charactor,",
            "char_length,",
            "-            speaker_ids=tf.zeros(shape=[tf.shape(charactor)[0]]),",
            "+            speaker_ids=tf.zeros(shape=[tf.shape(charactor)[0]], dtype=tf.int32),",
            ")",
            "",
            "# convert to numpy"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=2215825)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=2215826)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=2215827)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2215828)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=2215829)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2215830)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2215831)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'int32'), position=2, insert_id=2215832)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 6088,
        "neg_line": [
            "-speaker_ids=tf.zeros(shape=[tf.shape(charactor)[0]]),"
        ],
        "pos_line": [
            "+speaker_ids=tf.zeros(shape=[tf.shape(charactor)[0]], dtype=tf.int32),"
        ],
        "core_change": "-speaker_ids=tf.zeros(shape=[tf.shape(charactor)[0]]), +speaker_ids=tf.zeros(shape=[tf.shape(charactor)[0]], dtype=tf.int32),",
        "core_API": "inference"
    },
    {
        "commit_hash": "2b32895c3178a412a701aba09dbad821d8acbd1a",
        "index": "e40c09dd..d5e08d47 100644",
        "commit_message": "fix cuda test issues\n\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestTransformLAFs:",
            "# projected should be equal as initial",
            "assert_allclose(lafs_src, lafs_dst_to_src)",
            "",
            "-    def test_gradcheck(self):",
            "+    def test_gradcheck(self, device):",
            "# generate input data",
            "batch_size, num_points, num_dims = 2, 3, 2",
            "eye_size = 3",
            "-        points_src = torch.rand(batch_size, num_points, 2, 3)",
            "-        dst_homo_src = utils.create_random_homography(batch_size, eye_size)",
            "+        points_src = torch.rand(batch_size, num_points, 2, 3).to(device)",
            "+        dst_homo_src = utils.create_random_homography(batch_size, eye_size).to(device)",
            "# evaluate function gradient",
            "points_src = utils.tensor_to_gradcheck_var(points_src)  # to var",
            "dst_homo_src = utils.tensor_to_gradcheck_var(dst_homo_src)  # to var"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=2, insert_id=451115)",
            "Insert(target_node=ASTNode(type=parameters), node=('identifier', 'device'), position=3, insert_id=451116)",
            "Insert(target_node=ASTNode(type=assignment), node=('call', None), position=2, insert_id=451117)",
            "Insert(target_node=ASTNode(type=assignment), node=('call', None), position=2, insert_id=451118)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=451119)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=451120)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=451121)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=451122)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=451123)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to'), position=2, insert_id=451124)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=451125)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'device'), position=1, insert_id=451126)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=451127)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=451128)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to'), position=2, insert_id=451129)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=451130)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'device'), position=1, insert_id=451131)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=451132)"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 20,
        "number": 6089,
        "neg_line": [
            "-def test_gradcheck(self):",
            "-points_src = torch.rand(batch_size, num_points, 2, 3)",
            "-dst_homo_src = utils.create_random_homography(batch_size, eye_size)"
        ],
        "pos_line": [
            "+def test_gradcheck(self, device):",
            "+points_src = torch.rand(batch_size, num_points, 2, 3).to(device)",
            "+dst_homo_src = utils.create_random_homography(batch_size, eye_size).to(device)"
        ],
        "core_change": "-def test_gradcheck(self): +def test_gradcheck(self, device): -points_src = torch.rand(batch_size, num_points, 2, 3) -dst_homo_src = utils.create_random_homography(batch_size, eye_size) +points_src = torch.rand(batch_size, num_points, 2, 3).to(device) +dst_homo_src = utils.create_random_homography(batch_size, eye_size).to(device)",
        "core_API": "rand"
    },
    {
        "commit_hash": "8c9de8349ff4498c83f72fdcf5e432a6975d9722",
        "index": "34a64489..cb2b8886 100644",
        "commit_message": "Formatting fixes (#12090)\n\n\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "keras",
        "change": [
            "def predict_generator(model, generator,",
            "warnings.warn(",
            "UserWarning('Using a generator with `use_multiprocessing=True`'",
            "' and multiple workers may duplicate your data.'",
            "-                        ' Please consider using the`keras.utils.Sequence'",
            "+                        ' Please consider using the `keras.utils.Sequence'",
            "' class.'))",
            "if steps is None:",
            "if use_sequence_api:"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=' Please consider using the`keras.utils.Sequence'), value=\"' Please consider using the `keras.utils.Sequence'\")"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 6098,
        "neg_line": [
            "-' Please consider using the`keras.utils.Sequence'"
        ],
        "pos_line": [
            "+' Please consider using the `keras.utils.Sequence'"
        ],
        "core_change": "-' Please consider using the`keras.utils.Sequence' +' Please consider using the `keras.utils.Sequence'",
        "core_API": "warn"
    },
    {
        "commit_hash": "4ca93ba1e2bf6ca328ae236b2e6b25a873ab879c",
        "index": "54eb5b4aac..5d738190a1 100644",
        "commit_message": "small fix for `dev` in torch (#2232)\n\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "torch_scatter = None",
            "def dev(x: torch.Tensor, as_native: bool = False) -> Union[ivy.Device, torch.device]:",
            "dv = x.device",
            "if as_native:",
            "-        return torch.device(dv.type.replace(\"gpu\", \"cuda\"))",
            "+        if isinstance(dv, torch.device):",
            "+            dv = dv.type",
            "+        return torch.device(dv.replace(\"gpu\", \"cuda\"))",
            "return as_ivy_dev(dv)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=block), node=('if_statement', None), position=0, insert_id=347163)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=347164)",
            "Insert(target_node=IN(type=if_statement), node=('call', None), position=1, insert_id=347165)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=347166)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=347167)",
            "Insert(target_node=IN(type=call), node=('identifier', 'isinstance'), position=0, insert_id=347168)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=347169)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=347170)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=347171)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'dv'), position=1, insert_id=347172)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=347173)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=3, insert_id=347174)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=347175)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=347176)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=347177)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=347178)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=347179)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'dv'), position=0, insert_id=347180)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=347181)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=attribute), position=2)",
            "Insert(target_node=ASTNode(type=attribute), node=('identifier', 'dv'), position=0, insert_id=347182)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=347183)",
            "Delete(target_node=ASTNode(type=., text=.))"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 23,
        "number": 6099,
        "neg_line": [
            "-return torch.device(dv.type.replace(\"gpu\", \"cuda\"))"
        ],
        "pos_line": [
            "+if isinstance(dv, torch.device):",
            "+dv = dv.type",
            "+return torch.device(dv.replace(\"gpu\", \"cuda\"))"
        ],
        "core_change": "-return torch.device(dv.type.replace(\"gpu\", \"cuda\")) +if isinstance(dv, torch.device): +dv = dv.type +return torch.device(dv.replace(\"gpu\", \"cuda\"))",
        "core_API": "device"
    },
    {
        "commit_hash": "35fe3213ef5b1c566c4d73a9d90b0c2aace04f88",
        "index": "bff488c2..701b60d6 100644",
        "commit_message": "RANSAC improvements (#1435)\n\n- Added LU solver for RANSAC-homography, it speeds-up the minimal-solver part x10\n- Now find_fundamental weights is optional, as it was for homography\n- Fix test precision and made one xFail test to be compulsory\n- Fixed tests for fundamental matrix, as one have to have >= 8 points (also added to docstring)\n- updates deprecated torch.svd to torch.linalg.svd\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def _nullspace(A):",
            "",
            "Return the smallest singular value and the corresponding vector.",
            "\"\"\"",
            "-    _, s, vh = torch.svd(A)",
            "-    return s[..., -1], vh[..., -1]",
            "+    _, s, v = _torch_svd_cast(A)",
            "+    return s[..., -1], v[..., -1]",
            "",
            "",
            "def projections_from_fundamental(F_mat: torch.Tensor) -> torch.Tensor:"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=vh), value='v')",
            "Update(target_node=ASTNode(type=identifier, text=torch), value='_torch_svd_cast')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=torch), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=vh), value='v')",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=svd))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 7,
        "number": 6101,
        "neg_line": [
            "-_, s, vh = torch.svd(A)",
            "-return s[..., -1], vh[..., -1]"
        ],
        "pos_line": [
            "+_, s, v = _torch_svd_cast(A)",
            "+return s[..., -1], v[..., -1]"
        ],
        "core_change": "-_, s, vh = torch.svd(A) -return s[..., -1], vh[..., -1] +_, s, v = _torch_svd_cast(A) +return s[..., -1], v[..., -1]",
        "core_API": "svd"
    },
    {
        "commit_hash": "c05ace221339102abb1c8b0933b9cbfd51ddd4a3",
        "index": "4f8f57164..b94a59cf0 100644",
        "commit_message": "Use f-strings in the dataset scripts (#3291)\n\n* Finishes #3257\n\nUsed f-strings to format the .py files in the dataset folder\n\n* Fix style\n\n* Fix hkcancor dataset\n\nCo-authored-by: Mario Šaško <mario@huggingface.co>\nCo-authored-by: mariosasko <mariosasko777@gmail.com>\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class JigsawUnintendedBias(datasets.GeneratorBasedBuilder):",
            "",
            "if not os.path.exists(data_dir):",
            "raise FileNotFoundError(",
            "-                \"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('jigsaw_unintended_bias', data_dir=...)`. Manual download instructions: {}\".format(",
            "-                    data_dir, self.manual_download_instructions",
            "-                )",
            "+                f\"{data_dir} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('jigsaw_unintended_bias', data_dir=...)`. Manual download instructions: {self.manual_download_instructions}\"",
            ")",
            "",
            "return ["
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('string', 'f\"{data_dir} does not exist. Make sure you insert a manual dir via `datasets.load_dataset(\\'jigsaw_unintended_bias\\', data_dir=...)`. Manual download instructions: {self.manual_download_instructions}\"'), position=1, insert_id=1781604)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=), text=)), position=2)",
            "Delete(target_node=ASTNode(type=string, text=\"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('jigsaw_unintended_bias', data_dir=...)`. Manual download instructions: {}\"))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=format))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=data_dir))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=manual_download_instructions))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=), text=)))"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 16,
        "number": 6103,
        "neg_line": [
            "-\"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('jigsaw_unintended_bias', data_dir=...)`. Manual download instructions: {}\".format(",
            "-data_dir, self.manual_download_instructions",
            "-)"
        ],
        "pos_line": [
            "+f\"{data_dir} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('jigsaw_unintended_bias', data_dir=...)`. Manual download instructions: {self.manual_download_instructions}\""
        ],
        "core_change": "-\"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('jigsaw_unintended_bias', data_dir=...)`. Manual download instructions: {}\".format( -data_dir, self.manual_download_instructions -) +f\"{data_dir} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('jigsaw_unintended_bias', data_dir=...)`. Manual download instructions: {self.manual_download_instructions}\"",
        "core_API": "exists"
    },
    {
        "commit_hash": "9b6385488617ee99188b60c0525cbe4ec7e8f2a0",
        "index": "2b0db402..14807e3e 100644",
        "commit_message": "Improve reproduceability 2/3 (#1906)\n\n* [Repro] Correct reproducability\n\n* up\n\n* up\n\n* uP\n\n* up\n\n* need better image\n\n* allow conversion from no state dict checkpoints\n\n* up\n\n* up\n\n* up\n\n* up\n\n* check tensors\n\n* check tensors\n\n* check tensors\n\n* check tensors\n\n* next try\n\n* up\n\n* up\n\n* better name\n\n* up\n\n* up\n\n* Apply suggestions from code review\n\n* correct more\n\n* up\n\n* replace all torch randn\n\n* fix\n\n* correct\n\n* correct\n\n* finish\n\n* fix more\n\n* up\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class AudioDiffusionPipeline(DiffusionPipeline):",
            "input_dims = self.get_input_dims()",
            "self.mel.set_resolution(x_res=input_dims[1], y_res=input_dims[0])",
            "if noise is None:",
            "-            noise = torch.randn(",
            "+            noise = randn_tensor(",
            "(",
            "batch_size,",
            "self.unet.in_channels,"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=torch), value='randn_tensor')",
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=identifier, text=torch), position=8)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=randn))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 6107,
        "neg_line": [
            "-noise = torch.randn("
        ],
        "pos_line": [
            "+noise = randn_tensor("
        ],
        "core_change": "-noise = torch.randn( +noise = randn_tensor(",
        "core_API": "get_input_dims"
    },
    {
        "commit_hash": "123eee9d8cada4470a78ccdab4ac396b1f245926",
        "index": "cfc41e7d..a06528de 100644",
        "commit_message": "fix linter errors\n\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Tester(unittest.TestCase):",
            "# create transformation (rotation)",
            "M = torch.tensor([[",
            "[torch.cos(alpha), -torch.sin(alpha), 0.],",
            "-            [torch.sin(alpha),  torch.cos(alpha), 0.],",
            "-            [              0.,                0., 1.],",
            "+            [torch.sin(alpha), torch.cos(alpha), 0.],",
            "+            [0., 0., 1.],",
            "]])  # Bx3x3",
            "M = utils.tensor_to_gradcheck_var(M, requires_grad=False)  # to var"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 6108,
        "neg_line": [
            "-[torch.sin(alpha),  torch.cos(alpha), 0.],",
            "-[              0.,                0., 1.],"
        ],
        "pos_line": [
            "+[torch.sin(alpha), torch.cos(alpha), 0.],",
            "+[0., 0., 1.],"
        ],
        "core_change": "-[torch.sin(alpha),  torch.cos(alpha), 0.], -[              0.,                0., 1.], +[torch.sin(alpha), torch.cos(alpha), 0.], +[0., 0., 1.],",
        "core_API": "tensor"
    },
    {
        "commit_hash": "3e22cdabc140f6fa5f443f71c2069d3472e97652",
        "index": "7b4f1969..a6a9be71 100644",
        "commit_message": "fix doc\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def to_hetero_with_bases(module: Module, metadata: Metadata, num_bases: int,",
            "import torch",
            "from torch_geometric.nn import SAGEConv, to_hetero_with_bases",
            "",
            "-        Net(torch.nn.Module):",
            "+        class GNN(torch.nn.Module):",
            "def __init__(self):",
            "-                self.conv1 = SAGEConv(16, 16)",
            "-                self.conv2 = SAGEConv(16, 16)",
            "+                self.conv1 = SAGEConv((16, 16), 32)",
            "+                self.conv2 = SAGEConv((32, 32), 32)",
            "",
            "def forward(self, x, edge_index):",
            "x = self.conv1(x, edge_index).relu()",
            "x = self.conv2(x, edge_index).relu()",
            "return x",
            "",
            "-        model = Net()",
            "+        model = GNN()",
            "",
            "node_types = ['paper', 'author']",
            "edge_types = ["
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=Net), value='class')",
            "Insert(target_node=ASTNode(type=ERROR), node=('identifier', 'GNN'), position=2, insert_id=999994)",
            "Update(target_node=ASTNode(type=identifier, text=Net), value='GNN')",
            "Insert(target_node=ASTNode(type=argument_list), node=('tuple', None), position=1, insert_id=999995)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=999996)",
            "Insert(target_node=ASTNode(type=argument_list), node=('integer', '32'), position=3, insert_id=999997)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=4, insert_id=999998)",
            "Insert(target_node=ASTNode(type=argument_list), node=('tuple', None), position=1, insert_id=999999)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1000000)",
            "Insert(target_node=ASTNode(type=argument_list), node=('integer', '32'), position=3, insert_id=1000001)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=4, insert_id=1000002)",
            "Insert(target_node=IN(type=tuple), node=('(', '('), position=0, insert_id=1000003)",
            "Move(target_node=IN(type=tuple), node=ASTNode(type=integer, text=16), position=1)",
            "Move(target_node=IN(type=tuple), node=ASTNode(type=,, text=,), position=2)",
            "Move(target_node=IN(type=tuple), node=ASTNode(type=integer, text=16), position=3)",
            "Move(target_node=IN(type=tuple), node=ASTNode(type=), text=)), position=4)",
            "Insert(target_node=IN(type=tuple), node=('(', '('), position=0, insert_id=1000004)",
            "Insert(target_node=IN(type=tuple), node=('integer', '32'), position=1, insert_id=1000005)",
            "Move(target_node=IN(type=tuple), node=ASTNode(type=,, text=,), position=2)",
            "Update(target_node=ASTNode(type=integer, text=16), value='32')",
            "Move(target_node=IN(type=tuple), node=ASTNode(type=integer, text=16), position=3)",
            "Move(target_node=IN(type=tuple), node=ASTNode(type=), text=)), position=4)",
            "Delete(target_node=ASTNode(type=integer, text=16))"
        ],
        "plus_line": 4,
        "minus_line": 4,
        "AST_diff_line": 23,
        "number": 6109,
        "neg_line": [
            "-Net(torch.nn.Module):",
            "-self.conv1 = SAGEConv(16, 16)",
            "-self.conv2 = SAGEConv(16, 16)",
            "-model = Net()"
        ],
        "pos_line": [
            "+class GNN(torch.nn.Module):",
            "+self.conv1 = SAGEConv((16, 16), 32)",
            "+self.conv2 = SAGEConv((32, 32), 32)",
            "+model = GNN()"
        ],
        "core_change": "-Net(torch.nn.Module): +class GNN(torch.nn.Module): -self.conv1 = SAGEConv(16, 16) -self.conv2 = SAGEConv(16, 16) +self.conv1 = SAGEConv((16, 16), 32) +self.conv2 = SAGEConv((32, 32), 32) -model = Net() +model = GNN()",
        "core_API": "conv1"
    },
    {
        "commit_hash": "a21515775936b0a2d5c1b8859770ca109b37b3e6",
        "index": "884d7ab808..d7676ca360 100644",
        "commit_message": "fix median of torch as torch.median's behavior is different from expected, so replace it with torch.quantile\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def median(",
            "temp = input",
            "if hasattr(axis, \"__iter__\"):",
            "for dim in axis:",
            "-            temp = torch.median(",
            "+            temp = torch.quantile(",
            "temp,",
            "+                0.5,",
            "dim=dim,",
            "keepdim=keepdims,",
            ")[0]",
            "-        return input",
            "+        return temp",
            "else:",
            "-        return torch.median(",
            "+        return torch.quantile(",
            "input,",
            "+            0.5,",
            "dim=axis,",
            "keepdim=keepdims,",
            ")[0]"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=input), value='temp')",
            "Update(target_node=ASTNode(type=identifier, text=median), value='quantile')",
            "Insert(target_node=ASTNode(type=argument_list), node=('float', '0.5'), position=3, insert_id=265408)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=265409)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=keyword_argument), position=7)",
            "Update(target_node=ASTNode(type=identifier, text=median), value='quantile')",
            "Insert(target_node=ASTNode(type=argument_list), node=('float', '0.5'), position=3, insert_id=265410)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=265411)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=7, insert_id=265412)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'keepdim'), position=0, insert_id=265413)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=265414)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'keepdims'), position=2, insert_id=265415)",
            "Delete(target_node=ASTNode(type=identifier, text=keepdim))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=keepdims))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 5,
        "minus_line": 3,
        "AST_diff_line": 16,
        "number": 6112,
        "neg_line": [
            "-temp = torch.median(",
            "-return input",
            "-return torch.median("
        ],
        "pos_line": [
            "+temp = torch.quantile(",
            "+0.5,",
            "+return temp",
            "+return torch.quantile(",
            "+0.5,"
        ],
        "core_change": "-temp = torch.median( +temp = torch.quantile( +0.5, -return input +return temp -return torch.median( +return torch.quantile( +0.5,",
        "core_API": "median"
    },
    {
        "commit_hash": "820251a00d2c612066f393de652b9223f85271e9",
        "index": "94151535..326c69e8 100644",
        "commit_message": "Small fix.\n\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def in_top_k(predictions, targets, k):",
            "`output[i]` is `True` if `predictions[i, targets[i]]` is within top-`k`",
            "values of `predictions[i]`.",
            "\"\"\"",
            "-    return tf_math_ops.in_top_k(tf.cast(predictions, 'float32'),",
            "-                                tf.cast(targets, 'int32'),",
            "-                                k)",
            "+    return tf.nn.in_top_k(tf.cast(predictions, 'float32'),",
            "+                          tf.cast(targets, 'int32'),",
            "+                          k)",
            "",
            "",
            "# CONVOLUTIONS"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=2110621)",
            "Update(target_node=ASTNode(type=identifier, text=tf_math_ops), value='tf')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=tf_math_ops), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2110622)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'nn'), position=2, insert_id=2110623)"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 5,
        "number": 6119,
        "neg_line": [
            "-return tf_math_ops.in_top_k(tf.cast(predictions, 'float32'),",
            "-tf.cast(targets, 'int32'),",
            "-k)"
        ],
        "pos_line": [
            "+return tf.nn.in_top_k(tf.cast(predictions, 'float32'),",
            "+tf.cast(targets, 'int32'),",
            "+k)"
        ],
        "core_change": "-return tf_math_ops.in_top_k(tf.cast(predictions, 'float32'), -tf.cast(targets, 'int32'), -k) +return tf.nn.in_top_k(tf.cast(predictions, 'float32'), +tf.cast(targets, 'int32'), +k)",
        "core_API": "in_top_k"
    },
    {
        "commit_hash": "b7045b1938cb58506a6f8dc8274c971f630d9105",
        "index": "b49ed77f..f186fb69 100644",
        "commit_message": "fix buffer transfer bug (#2045)\n\n\n",
        "file": "nni.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class AGP_Pruner(Pruner):",
            "if epoch > 0:",
            "self.now_epoch = epoch",
            "for wrapper in self.get_modules_wrapper():",
            "-                wrapper.registered_buffers['if_calculated'].copy_(torch.tensor(0)) # pylint: disable=not-callable",
            "+                wrapper.if_calculated.copy_(torch.tensor(0)) # pylint: disable=not-callable",
            "",
            "class SlimPruner(Pruner):",
            "\"\"\""
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=registered_buffers), value='if_calculated')",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=string, text='if_calculated'))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 6120,
        "neg_line": [
            "-wrapper.registered_buffers['if_calculated'].copy_(torch.tensor(0)) # pylint: disable=not-callable"
        ],
        "pos_line": [
            "+wrapper.if_calculated.copy_(torch.tensor(0)) # pylint: disable=not-callable"
        ],
        "core_change": "-wrapper.registered_buffers['if_calculated'].copy_(torch.tensor(0)) # pylint: disable=not-callable +wrapper.if_calculated.copy_(torch.tensor(0)) # pylint: disable=not-callable",
        "core_API": "get_modules_wrapper"
    },
    {
        "commit_hash": "991731edf0fecf017de8e64d92345252a7a5bc0b",
        "index": "08892aa0..8f6e6c2f 100644",
        "commit_message": "Fix failing CUDA tests on dev (#1277)\n\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_welford_dense(n_samples, dim_size):",
            "samples.append(sample)",
            "w.update(sample)",
            "",
            "-    sample_cov = np.cov(torch.stack(samples).data.numpy(), bias=False, rowvar=False)",
            "-    estimates = w.get_covariance(regularize=False).data.numpy()",
            "+    sample_cov = np.cov(torch.stack(samples).data.cpu().numpy(), bias=False, rowvar=False)",
            "+    estimates = w.get_covariance(regularize=False).data.cpu().numpy()",
            "assert_equal(estimates, sample_cov)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('call', None), position=2, insert_id=738292)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=738293)",
            "Move(target_node=IN(type=call), node=ASTNode(type=argument_list), position=1)",
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=1, insert_id=738294)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=738295)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'numpy'), position=2, insert_id=738296)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=738297)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=738298)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=argument_list), position=1)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=738299)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'numpy'), position=2, insert_id=738300)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=738301)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=738302)",
            "Update(target_node=ASTNode(type=identifier, text=numpy), value='cpu')",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=738303)",
            "Update(target_node=ASTNode(type=identifier, text=numpy), value='cpu')",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=738304)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=738305)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 20,
        "number": 6121,
        "neg_line": [
            "-sample_cov = np.cov(torch.stack(samples).data.numpy(), bias=False, rowvar=False)",
            "-estimates = w.get_covariance(regularize=False).data.numpy()"
        ],
        "pos_line": [
            "+sample_cov = np.cov(torch.stack(samples).data.cpu().numpy(), bias=False, rowvar=False)",
            "+estimates = w.get_covariance(regularize=False).data.cpu().numpy()"
        ],
        "core_change": "-sample_cov = np.cov(torch.stack(samples).data.numpy(), bias=False, rowvar=False) -estimates = w.get_covariance(regularize=False).data.numpy() +sample_cov = np.cov(torch.stack(samples).data.cpu().numpy(), bias=False, rowvar=False) +estimates = w.get_covariance(regularize=False).data.cpu().numpy()",
        "core_API": "append"
    },
    {
        "commit_hash": "2de47a11683658e4a5fb70b8e98e8c99983605ec",
        "index": "4b5c8d26..2b089c78 100644",
        "commit_message": "fix mean_iou batch2 unit test\n\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def mean_iou(",
            "# TODO: is it possible to vectorize this ?",
            "# iterate over classes",
            "for class_id in range(num_classes):",
            "-        tp: torch.Tensor = conf_mat[..., class_id, class_id].float()",
            "+        tp: torch.Tensor = conf_mat[..., None, class_id, class_id]",
            "total = torch.sum(conf_mat[..., class_id, :], dim=-1, keepdim=True) + \\",
            "torch.sum(conf_mat[..., :, class_id], dim=-1, keepdim=True)",
            "iou_val: torch.Tensor = tp / (total.float() - tp + 1e-6)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=subscript), position=4)",
            "Insert(target_node=ASTNode(type=subscript), node=('none', 'None'), position=4, insert_id=473607)",
            "Insert(target_node=ASTNode(type=subscript), node=(',', ','), position=5, insert_id=473608)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=float))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 6133,
        "neg_line": [
            "-tp: torch.Tensor = conf_mat[..., class_id, class_id].float()"
        ],
        "pos_line": [
            "+tp: torch.Tensor = conf_mat[..., None, class_id, class_id]"
        ],
        "core_change": "-tp: torch.Tensor = conf_mat[..., class_id, class_id].float() +tp: torch.Tensor = conf_mat[..., None, class_id, class_id]",
        "core_API": "sum"
    },
    {
        "commit_hash": "7d9ffe6d4ecbadd688eb3ebebb6c6c370281820f",
        "index": "938e19e..127adad 100644",
        "commit_message": "weight_decay fix\n\n",
        "file": "yolov3.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def train():",
            "torch.save(chkpt, wdir + 'backup%g.pt' % epoch)",
            "",
            "# Delete checkpoint",
            "-            del chkpt  # end epoch -------------------------------------------------------------------------------------",
            "+            del chkpt",
            "+",
            "+        # end epoch ----------------------------------------------------------------------------------------------------",
            "",
            "# Report time",
            "plot_results()  # save as results.png"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 6135,
        "neg_line": [
            "-del chkpt  # end epoch -------------------------------------------------------------------------------------"
        ],
        "pos_line": [
            "+del chkpt",
            "+",
            "+# end epoch ----------------------------------------------------------------------------------------------------"
        ],
        "core_change": "-del chkpt  # end epoch ------------------------------------------------------------------------------------- +del chkpt + +# end epoch ----------------------------------------------------------------------------------------------------",
        "core_API": "save"
    },
    {
        "commit_hash": "84cf269e409563052fe87320ca3cc88d31e15152",
        "index": "6e825fa2..9b72e5f8 100644",
        "commit_message": "Fix AutoRegNN with default CUDA tensor (#1308)\n\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class AutoRegressiveNNTests(TestCase):",
            "# NOTE: the hidden dimension must be greater than the input_dim for the",
            "# masks to be well-defined!",
            "hidden_dim = input_dim * 5",
            "-                        permutation = torch.randperm(input_dim)",
            "+                        permutation = torch.randperm(input_dim, device='cpu')",
            "self._test_masks(",
            "input_dim,",
            "observed_dim,"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=737515)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=737516)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=737517)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=737518)",
            "Insert(target_node=IN(type=keyword_argument), node=('string', \"'cpu'\"), position=2, insert_id=737519)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 6144,
        "neg_line": [
            "-permutation = torch.randperm(input_dim)"
        ],
        "pos_line": [
            "+permutation = torch.randperm(input_dim, device='cpu')"
        ],
        "core_change": "-permutation = torch.randperm(input_dim) +permutation = torch.randperm(input_dim, device='cpu')",
        "core_API": "randperm"
    },
    {
        "commit_hash": "7db18ce0c28c0b7c93e9a61af55433f2d88d4213",
        "index": "cdde691a..13196128 100644",
        "commit_message": "fix mypy errors\n\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def mean_iou(",
            "torch.Tensor: a tensor representing the mean intersection-over union",
            "with shape :math:`(B, K)` where K is the number of classes.",
            "\"\"\"",
            "-    if not torch.is_tensor(input) and input.dtype is not torch.uint64:",
            "+    if not torch.is_tensor(input) and input.dtype is not torch.int64:",
            "raise TypeError(\"Input input type is not a torch.Tensor with \"",
            "\"torch.int64 dtype. Got {}\".format(type(input)))",
            "-    if not torch.is_tensor(target) and target.dtype is not torch.uint64:",
            "+    if not torch.is_tensor(target) and target.dtype is not torch.int64:",
            "raise TypeError(\"Input target type is not a torch.Tensor with \"",
            "\"torch.int64 dtype. Got {}\".format(type(target)))",
            "if not input.shape == target.shape:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"\"\"\n    if not torch.is_tensor(input) and input.dtype is not torch.uint64:\nraise TypeError(\"Input input type is not a torch.Tensor with \"\n\"torch.int64 dtype. Got {}\".format(type(input)))\n    if not torch.is_tensor(target) and target.dtype is not torch.uint64:\nraise TypeError(\"Input target type is not a torch.Tensor with \"\n\"torch.int64 dtype. Got {}\"), value='\"\"\"\\n    if not torch.is_tensor(input) and input.dtype is not torch.int64:\\nraise TypeError(\"Input input type is not a torch.Tensor with \"\\n\"torch.int64 dtype. Got {}\".format(type(input)))\\n    if not torch.is_tensor(target) and target.dtype is not torch.int64:\\nraise TypeError(\"Input target type is not a torch.Tensor with \"\\n\"torch.int64 dtype. Got {}\"')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 1,
        "number": 6149,
        "neg_line": [
            "-if not torch.is_tensor(input) and input.dtype is not torch.uint64:",
            "-if not torch.is_tensor(target) and target.dtype is not torch.uint64:"
        ],
        "pos_line": [
            "+if not torch.is_tensor(input) and input.dtype is not torch.int64:",
            "+if not torch.is_tensor(target) and target.dtype is not torch.int64:"
        ],
        "core_change": "-if not torch.is_tensor(input) and input.dtype is not torch.uint64: +if not torch.is_tensor(input) and input.dtype is not torch.int64: -if not torch.is_tensor(target) and target.dtype is not torch.uint64: +if not torch.is_tensor(target) and target.dtype is not torch.int64:",
        "core_API": "is_tensor"
    },
    {
        "commit_hash": "4699eea6ab471eba2adf7d7ac0a5121d3f67c9cf",
        "index": "edb277ff..ee8f65df 100644",
        "commit_message": "bugfix\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test(epoch, loader, string):",
            "for data in loader:",
            "adj, slice = data['adj']['content'], data['adj']['slice'][:, 0]",
            "input, target = data['input'], data['target']",
            "-        input = torch.cat([input, input.new(input.size(0)).fill_(1)], dim=1)",
            "+        input = torch.cat([input, input.new(input.size(0), 1).fill_(1)], dim=1)",
            "num_examples += target.size(0)",
            "",
            "if torch.cuda.is_available():"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1087146)",
            "Insert(target_node=ASTNode(type=argument_list), node=('integer', '1'), position=3, insert_id=1087147)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 6151,
        "neg_line": [
            "-input = torch.cat([input, input.new(input.size(0)).fill_(1)], dim=1)"
        ],
        "pos_line": [
            "+input = torch.cat([input, input.new(input.size(0), 1).fill_(1)], dim=1)"
        ],
        "core_change": "-input = torch.cat([input, input.new(input.size(0)).fill_(1)], dim=1) +input = torch.cat([input, input.new(input.size(0), 1).fill_(1)], dim=1)",
        "core_API": "cat"
    },
    {
        "commit_hash": "504da45e699cae4f8e62c6fa39ab7d504706d356",
        "index": "8d188a735..9ec27bd68 100644",
        "commit_message": "fix(rllib): allow explore=False with tuple action distributions (#10443)\n\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class StochasticSampling(Exploration):",
            "logp = action_dist.sampled_action_logp()",
            "else:",
            "action = action_dist.deterministic_sample()",
            "-            logp = torch.zeros((action.size()[0], ), dtype=torch.float32)",
            "+            logp = torch.zeros_like(action_dist.sampled_action_logp())",
            "return action, logp"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=zeros), value='zeros_like')",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=call), position=1)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=), text=)), position=2)",
            "Update(target_node=ASTNode(type=identifier, text=action), value='action_dist')",
            "Update(target_node=ASTNode(type=identifier, text=size), value='sampled_action_logp')",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=integer, text=0))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=tuple))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=dtype))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=float32))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=), text=)))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 21,
        "number": 6153,
        "neg_line": [
            "-logp = torch.zeros((action.size()[0], ), dtype=torch.float32)"
        ],
        "pos_line": [
            "+logp = torch.zeros_like(action_dist.sampled_action_logp())"
        ],
        "core_change": "-logp = torch.zeros((action.size()[0], ), dtype=torch.float32) +logp = torch.zeros_like(action_dist.sampled_action_logp())",
        "core_API": "sampled_action_logp"
    },
    {
        "commit_hash": "8542437679b4bdef0d206b850a3e7bb1b3d5cd82",
        "index": "84818b055..8da60d7d5 100644",
        "commit_message": "docs: ✏️ fix TranslationVariableLanguages example (#4731)\n\n\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class TranslationVariableLanguages:",
            "",
            "```python",
            ">>> # At construction time:",
            "-    >>> datasets.features.Translation(languages=['en', 'fr', 'de'])",
            "+    >>> datasets.features.TranslationVariableLanguages(languages=['en', 'fr', 'de'])",
            ">>> # During data generation:",
            ">>> yield {",
            "...         'en': 'the cat',"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=Translation), value='TranslationVariableLanguages')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 6162,
        "neg_line": [
            "->>> datasets.features.Translation(languages=['en', 'fr', 'de'])"
        ],
        "pos_line": [
            "+>>> datasets.features.TranslationVariableLanguages(languages=['en', 'fr', 'de'])"
        ],
        "core_change": "->>> datasets.features.Translation(languages=['en', 'fr', 'de']) +>>> datasets.features.TranslationVariableLanguages(languages=['en', 'fr', 'de'])",
        "core_API": "Translation"
    },
    {
        "commit_hash": "bf2f9542e05802c19319ef42c299605eff503b64",
        "index": "cc36193b..0378543e 100644",
        "commit_message": "Fix memory issue of MCMC for multiple chains (#1956)\n\n\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "if __name__ == \"__main__\":",
            "help=\"run this example in GPU\")",
            "args = parser.parse_args()",
            "",
            "+    # work around the error \"CUDA error: initialization error\" when arg.cuda is False",
            "+    # see https://github.com/pytorch/pytorch/issues/2517",
            "+    if six.PY3:",
            "+        torch.multiprocessing.set_start_method(\"spawn\")",
            "pyro.set_rng_seed(args.rng_seed)",
            "# Enable validation checks",
            "pyro.enable_validation(__debug__)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=2, insert_id=718473)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=718474)",
            "Insert(target_node=IN(type=if_statement), node=('attribute', None), position=1, insert_id=718475)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=718476)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=718477)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'six'), position=0, insert_id=718478)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=718479)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'PY3'), position=2, insert_id=718480)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=718481)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=718482)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=718483)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=718484)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=718485)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=718486)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'set_start_method'), position=2, insert_id=718487)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=718488)",
            "Insert(target_node=IN(type=argument_list), node=('string', '\"spawn\"'), position=1, insert_id=718489)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=718490)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=718491)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=718492)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'multiprocessing'), position=2, insert_id=718493)"
        ],
        "plus_line": 4,
        "minus_line": 0,
        "AST_diff_line": 21,
        "number": 6164,
        "neg_line": [],
        "pos_line": [
            "+# work around the error \"CUDA error: initialization error\" when arg.cuda is False",
            "+# see https://github.com/pytorch/pytorch/issues/2517",
            "+if six.PY3:",
            "+torch.multiprocessing.set_start_method(\"spawn\")"
        ],
        "core_change": "+# work around the error \"CUDA error: initialization error\" when arg.cuda is False +# see https://github.com/pytorch/pytorch/issues/2517 +if six.PY3: +torch.multiprocessing.set_start_method(\"spawn\")",
        "core_API": "parse_args"
    },
    {
        "commit_hash": "ed63b7ee9e68ba120893aaaaca0d4525a56ed484",
        "index": "db6cea02..da08ee52 100644",
        "commit_message": "upgrade to pytorch 0.4.0 (#1126)\n\n* bump pytorch to 0.4 + fix sanitize\n\n* remove check for Variable in block_orthogonal\n\n* rename parameter split_size=\n\n* remove checks for Variable\n\n* fix more tests\n\n* fixes\n\n* get tests to pass\n\n* fix warnings\n\n* get rid of some of the Variables\n\n* more tests passing\n\n* more elimination of variables\n\n* finish removing all Variables\n\n* pylint and such\n\n* a few fixes\n\n* move torch.no_grad into model.forward_on_instances\n\n* more pytorch 0.4 changes\n\n* detach() -> data\n\n* pylint\n\n* fix bad tensor creation\n\n* fix types\n\n* remove print statement\n\n* more 0.4 goodness\n\n* factor out is_tensor\n\n* add no_grad to elmo command\n\n* cleanup\n\n* remove TODO\n\n* address PR feedback\n\n* replace all() with item()\n\n* more cleanup\n\n* further cleanup\n\n* remove Variable\n\n* really fix merge conflict\n\n* fix pylint\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestIntraSentenceAttentionEncoder(AllenNlpTestCase):",
            "similarity_function=similarity,",
            "num_attention_heads=3,",
            "combination=\"1+2\")",
            "-        input_tensor = Variable(torch.from_numpy(numpy.random.rand(4, 6, 24))).float()",
            "+        input_tensor = torch.from_numpy(numpy.random.rand(4, 6, 24)).float()",
            "encoder_output = encoder(input_tensor, None)",
            "assert list(encoder_output.size()) == [4, 6, 24]"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=Variable))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 6165,
        "neg_line": [
            "-input_tensor = Variable(torch.from_numpy(numpy.random.rand(4, 6, 24))).float()"
        ],
        "pos_line": [
            "+input_tensor = torch.from_numpy(numpy.random.rand(4, 6, 24)).float()"
        ],
        "core_change": "-input_tensor = Variable(torch.from_numpy(numpy.random.rand(4, 6, 24))).float() +input_tensor = torch.from_numpy(numpy.random.rand(4, 6, 24)).float()",
        "core_API": "from_numpy"
    },
    {
        "commit_hash": "052d11729ea9d00ae6b424dc660a0bf43453ee96",
        "index": "c5655d8..484443a 100755",
        "commit_message": "fix Error in calc iou\n\n",
        "file": "tensorflow-yolov3.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class yolov3(object):",
            "# caculate iou between true boxes and pred boxes",
            "intersect_xy1 = tf.maximum(true_box_xy - true_box_wh / 2.0,",
            "pred_box_xy - pred_box_xy / 2.0)",
            "-        intersect_xy2 = tf.maximum(true_box_xy + true_box_wh / 2.0,",
            "+        intersect_xy2 = tf.minimum(true_box_xy + true_box_wh / 2.0,",
            "pred_box_xy + pred_box_wh / 2.0)",
            "intersect_wh = tf.maximum(intersect_xy2 - intersect_xy1, 0.)",
            "intersect_area = intersect_wh[..., 0] * intersect_wh[..., 1]"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=maximum), value='minimum')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 6167,
        "neg_line": [
            "-intersect_xy2 = tf.maximum(true_box_xy + true_box_wh / 2.0,"
        ],
        "pos_line": [
            "+intersect_xy2 = tf.minimum(true_box_xy + true_box_wh / 2.0,"
        ],
        "core_change": "-intersect_xy2 = tf.maximum(true_box_xy + true_box_wh / 2.0, +intersect_xy2 = tf.minimum(true_box_xy + true_box_wh / 2.0,",
        "core_API": "maximum"
    },
    {
        "commit_hash": "1251072f46f895dd79f29c0470af737fdce3738b",
        "index": "becaa1172..0481b0ae8 100644",
        "commit_message": "Fix SEW-D implementation differences (#14191)\n\n* Fix SEW-D\n\n* Update tests\n\n* isort\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SEWDModelIntegrationTest(unittest.TestCase):",
            ")",
            "expected_output_sum = 54201.0469",
            "",
            "-        self.assertTrue(torch.allclose(outputs[:, :4, :4], expected_outputs_first, atol=5e-3))",
            "-        self.assertTrue(torch.allclose(outputs[:, -4:, -4:], expected_outputs_last, atol=5e-3))",
            "-        self.assertTrue(abs(outputs.sum() - expected_output_sum) < 5)",
            "+        self.assertTrue(torch.allclose(outputs[:, :4, :4], expected_outputs_first, atol=1e-3))",
            "+        self.assertTrue(torch.allclose(outputs[:, -4:, -4:], expected_outputs_last, atol=1e-3))",
            "+        self.assertTrue(abs(outputs.sum() - expected_output_sum) < 1)",
            "",
            "def test_inference_ctc_batched(self):",
            "model = SEWDForCTC.from_pretrained(\"asapp/sew-d-tiny-100k-ft-ls100h\").to(torch_device)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1209990)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=1209991)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1209992)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'assertTrue'), position=2, insert_id=1209993)",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1209994)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Update(target_node=ASTNode(type=integer, text=5), value='1')",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1209995)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1209996)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'allclose'), position=2, insert_id=1209997)",
            "Update(target_node=ASTNode(type=float, text=5e-3), value='1e-3')",
            "Update(target_node=ASTNode(type=float, text=5e-3), value='1e-3')",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=allclose))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=assertTrue))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 21,
        "number": 6170,
        "neg_line": [
            "-self.assertTrue(torch.allclose(outputs[:, :4, :4], expected_outputs_first, atol=5e-3))",
            "-self.assertTrue(torch.allclose(outputs[:, -4:, -4:], expected_outputs_last, atol=5e-3))",
            "-self.assertTrue(abs(outputs.sum() - expected_output_sum) < 5)"
        ],
        "pos_line": [
            "+self.assertTrue(torch.allclose(outputs[:, :4, :4], expected_outputs_first, atol=1e-3))",
            "+self.assertTrue(torch.allclose(outputs[:, -4:, -4:], expected_outputs_last, atol=1e-3))",
            "+self.assertTrue(abs(outputs.sum() - expected_output_sum) < 1)"
        ],
        "core_change": "-self.assertTrue(torch.allclose(outputs[:, :4, :4], expected_outputs_first, atol=5e-3)) -self.assertTrue(torch.allclose(outputs[:, -4:, -4:], expected_outputs_last, atol=5e-3)) -self.assertTrue(abs(outputs.sum() - expected_output_sum) < 5) +self.assertTrue(torch.allclose(outputs[:, :4, :4], expected_outputs_first, atol=1e-3)) +self.assertTrue(torch.allclose(outputs[:, -4:, -4:], expected_outputs_last, atol=1e-3)) +self.assertTrue(abs(outputs.sum() - expected_output_sum) < 1)",
        "core_API": "assertTrue"
    },
    {
        "commit_hash": "3a3782c39fc2e75f53515660b866e7cc501b7704",
        "index": "b6cb20830..581569f0e 100644",
        "commit_message": "[rllib] Fix LSTM regression on truncated sequences and add regression test (#2898)\n\n* fix\n\n* add test\n\n* yapf\n\n* yapf\n\n* fix space\n\n* Oops that should be lstm: True\n\n* Update cartpole_lstm.py\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class LSTM(Model):",
            "self.state_in = [c_in, h_in]",
            "",
            "# Setup LSTM outputs",
            "+        state_in = rnn.LSTMStateTuple(c_in, h_in)",
            "lstm_out, lstm_state = tf.nn.dynamic_rnn(",
            "lstm,",
            "last_layer,",
            "+            initial_state=state_in,",
            "sequence_length=self.seq_lens,",
            "time_major=False,",
            "dtype=tf.float32)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=2, insert_id=1509010)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1509011)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'state_in'), position=0, insert_id=1509012)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1509013)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=1509014)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1509015)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1509016)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'rnn'), position=0, insert_id=1509017)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1509018)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'LSTMStateTuple'), position=2, insert_id=1509019)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1509020)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'c_in'), position=1, insert_id=1509021)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=1509022)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'h_in'), position=3, insert_id=1509023)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=1509024)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=1509025)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=6, insert_id=1509026)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'initial_state'), position=0, insert_id=1509027)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1509028)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'state_in'), position=2, insert_id=1509029)"
        ],
        "plus_line": 2,
        "minus_line": 0,
        "AST_diff_line": 20,
        "number": 6172,
        "neg_line": [],
        "pos_line": [
            "+state_in = rnn.LSTMStateTuple(c_in, h_in)",
            "+initial_state=state_in,"
        ],
        "core_change": "+state_in = rnn.LSTMStateTuple(c_in, h_in) +initial_state=state_in,",
        "core_API": "LSTMStateTuple"
    },
    {
        "commit_hash": "8892d3c0e0cbf51493b16079355109e0d6a7306b",
        "index": "ede922b..d074e1f 100644",
        "commit_message": "Update lstm_crf_layer.py\n\nfix bug for https://github.com/macanv/BERT-BiLSTM-CRF-NER/issues/159 and https://github.com/macanv/BERT-BiLSTM-CRF-NER/issues/254\n",
        "file": "BERT-BiLSTM-CRF-NER.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class BLSTM_CRF(object):",
            "b = tf.get_variable(\"b\", shape=[self.hidden_unit], dtype=tf.float32,",
            "initializer=tf.zeros_initializer())",
            "output = tf.reshape(lstm_outputs, shape=[-1, self.hidden_unit * 2])",
            "-                hidden = tf.tanh(tf.nn.xw_plus_b(output, W, b))",
            "+                hidden = tf.nn.xw_plus_b(output, W, b)",
            "",
            "# project to score of tags",
            "with tf.variable_scope(\"logits\"):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=IN(type=root), node=ASTNode(type=call), position=0)",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=tanh))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 6174,
        "neg_line": [
            "-hidden = tf.tanh(tf.nn.xw_plus_b(output, W, b))"
        ],
        "pos_line": [
            "+hidden = tf.nn.xw_plus_b(output, W, b)"
        ],
        "core_change": "-hidden = tf.tanh(tf.nn.xw_plus_b(output, W, b)) +hidden = tf.nn.xw_plus_b(output, W, b)",
        "core_API": "get_variable"
    },
    {
        "commit_hash": "fbd5aa4a9a01c7d97d16465178f73e77400e02ef",
        "index": "0434b02..9202077 100644",
        "commit_message": "fix loss\n\n",
        "file": "darkflow.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def loss(net):",
            "confs = tf.mul(best_box, _confs)",
            "",
            "# take care of the weight terms",
            "-    weight_con = snoob*(1.-best_box) + sconf*best_box",
            "+    weight_con = snoob * (1. - confs) + sconf * confs",
            "conid = tf.mul(_conid, weight_con)",
            "-    weight_coo = tf.concat(3, 4 * [tf.expand_dims(best_box, -1)])",
            "+    weight_coo = tf.concat(3, 4 * [tf.expand_dims(confs, -1)])",
            "cooid = tf.mul(_cooid, scoor * weight_coo)",
            "proid = sprob * _proid"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=best_box), value='confs')",
            "Update(target_node=ASTNode(type=identifier, text=best_box), value='confs')",
            "Update(target_node=ASTNode(type=identifier, text=best_box), value='confs')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 3,
        "number": 6178,
        "neg_line": [
            "-weight_con = snoob*(1.-best_box) + sconf*best_box",
            "-weight_coo = tf.concat(3, 4 * [tf.expand_dims(best_box, -1)])"
        ],
        "pos_line": [
            "+weight_con = snoob * (1. - confs) + sconf * confs",
            "+weight_coo = tf.concat(3, 4 * [tf.expand_dims(confs, -1)])"
        ],
        "core_change": "-weight_con = snoob*(1.-best_box) + sconf*best_box +weight_con = snoob * (1. - confs) + sconf * confs -weight_coo = tf.concat(3, 4 * [tf.expand_dims(best_box, -1)]) +weight_coo = tf.concat(3, 4 * [tf.expand_dims(confs, -1)])",
        "core_API": "mul"
    },
    {
        "commit_hash": "7482178162b779506a54538f2cf2565c8b88c597",
        "index": "78d001e3..f5a92fac 100644",
        "commit_message": "default fast model loading 🔥 (#1115)\n\n* make accelerate hard dep\n\n* default fast init\n\n* move params to cpu when device map is None\n\n* handle device_map=None\n\n* handle torch < 1.9\n\n* remove device_map=\"auto\"\n\n* style\n\n* add accelerate in torch extra\n\n* remove accelerate from extras[\"test\"]\n\n* raise an error if torch is available but not accelerate\n\n* update installation docs\n\n* Apply suggestions from code review\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* improve defautl loading speed even further, allow disabling fats loading\n\n* address review comments\n\n* adapt the tests\n\n* fix test_stable_diffusion_fast_load\n\n* fix test_read_init\n\n* temp fix for dummy checks\n\n* Trigger Build\n\n* Apply suggestions from code review\n\nCo-authored-by: Anton Lozhkov <anton@huggingface.co>\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\nCo-authored-by: Anton Lozhkov <anton@huggingface.co>\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class StableDiffusionImg2ImgPipelineIntegrationTests(unittest.TestCase):",
            "init_image = init_image.resize((768, 512))",
            "",
            "pipe = StableDiffusionImg2ImgPipeline.from_pretrained(",
            "-            \"CompVis/stable-diffusion-v1-4\", revision=\"fp16\", torch_dtype=torch.float16, device_map=\"auto\"",
            "+            \"CompVis/stable-diffusion-v1-4\",",
            "+            revision=\"fp16\",",
            "+            torch_dtype=torch.float16,",
            ")",
            "pipe.to(torch_device)",
            "pipe.set_progress_bar_config(disable=None)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=device_map))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=string, text=\"auto\"))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 6181,
        "neg_line": [
            "-\"CompVis/stable-diffusion-v1-4\", revision=\"fp16\", torch_dtype=torch.float16, device_map=\"auto\""
        ],
        "pos_line": [
            "+\"CompVis/stable-diffusion-v1-4\",",
            "+revision=\"fp16\",",
            "+torch_dtype=torch.float16,"
        ],
        "core_change": "-\"CompVis/stable-diffusion-v1-4\", revision=\"fp16\", torch_dtype=torch.float16, device_map=\"auto\" +\"CompVis/stable-diffusion-v1-4\", +revision=\"fp16\", +torch_dtype=torch.float16,",
        "core_API": "resize"
    },
    {
        "commit_hash": "6b4e0d472fe4abfdb21cc93d05e7deabe957a3f4",
        "index": "9ff4adb..8d7b488 100644",
        "commit_message": "Fix keras 2.1.6 error in parser.\n\n",
        "file": "MMdnn.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "keras",
        "change": [
            "class Keras2Parser(Parser):",
            "# load model files into Keras graph",
            "if isinstance(model, _string_types):",
            "try:",
            "-                from _keras.applications.mobilenet import relu6",
            "-                from _keras.applications.mobilenet import DepthwiseConv2D",
            "+                from keras.applications.mobilenet import relu6",
            "+                from keras.applications.mobilenet import DepthwiseConv2D",
            "model = _keras.models.load_model(",
            "model,",
            "custom_objects={"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=_keras), value='keras')",
            "Update(target_node=ASTNode(type=identifier, text=_keras), value='keras')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 6183,
        "neg_line": [
            "-from _keras.applications.mobilenet import relu6",
            "-from _keras.applications.mobilenet import DepthwiseConv2D"
        ],
        "pos_line": [
            "+from keras.applications.mobilenet import relu6",
            "+from keras.applications.mobilenet import DepthwiseConv2D"
        ],
        "core_change": "-from _keras.applications.mobilenet import relu6 -from _keras.applications.mobilenet import DepthwiseConv2D +from keras.applications.mobilenet import relu6 +from keras.applications.mobilenet import DepthwiseConv2D",
        "core_API": "load_model"
    },
    {
        "commit_hash": "34fb9c48350e314b13e3c685cded9b57efbe1b08",
        "index": "91de34ec21..d269125c72 100644",
        "commit_message": "fix lint and docstring failures\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def nonzero(x: torch.Tensor) -> Tuple[torch.Tensor]:",
            "return torch.nonzero(x, as_tuple=True)",
            "",
            "",
            "-def where(",
            "-    condition: torch.Tensor,",
            "-    x1: torch.Tensor,",
            "-    x2: torch.Tensor",
            "-) -> torch.Tensor:",
            "+def where(condition: torch.Tensor, x1: torch.Tensor, x2: torch.Tensor) -> torch.Tensor:",
            "promoted_type = torch.promote_types(x1.dtype, x2.dtype)",
            "x1 = x1.to(promoted_type)",
            "x2 = x2.to(promoted_type)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 0,
        "minus_line": 3,
        "AST_diff_line": 17,
        "number": 6184,
        "neg_line": [
            "-def where(",
            "-condition: torch.Tensor,",
            "-x1: torch.Tensor,",
            "-x2: torch.Tensor",
            "-) -> torch.Tensor:"
        ],
        "pos_line": [
            "+def where(condition: torch.Tensor, x1: torch.Tensor, x2: torch.Tensor) -> torch.Tensor:"
        ],
        "core_change": "-def where( -condition: torch.Tensor, -x1: torch.Tensor, -x2: torch.Tensor -) -> torch.Tensor: +def where(condition: torch.Tensor, x1: torch.Tensor, x2: torch.Tensor) -> torch.Tensor:",
        "core_API": "nonzero"
    },
    {
        "commit_hash": "82a059d89d5f5ebc77c6a3f0b8ac4072a3979104",
        "index": "e66214ff..02eaa586 100755",
        "commit_message": "breaking: unified complex_network and network, fixed synchronization optimizer problem\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Synchronization(Optimizer):",
            "",
            "last_sync = tf.get_variable(",
            "name='last-sync',",
            "-            dtype=tf.int32,",
            "-            initializer=(-self.sync_frequency),",
            "+            dtype=tf.int64,",
            "+            initializer=tf.constant_initializer(value=(-self.sync_frequency), dtype=tf.int64),",
            "trainable=False",
            ")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=2234716)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'initializer'), position=0, insert_id=2234717)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2234718)",
            "Insert(target_node=IN(type=keyword_argument), node=('call', None), position=2, insert_id=2234719)",
            "Update(target_node=ASTNode(type=identifier, text=int32), value='int64')",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2234720)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2234721)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2234722)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2234723)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'constant_initializer'), position=2, insert_id=2234724)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2234725)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=keyword_argument), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=2234726)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=3, insert_id=2234727)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=2234728)",
            "Update(target_node=ASTNode(type=identifier, text=initializer), value='value')",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=2234729)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2234730)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=2234731)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2234732)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2234733)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'int64'), position=2, insert_id=2234734)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 22,
        "number": 6186,
        "neg_line": [
            "-dtype=tf.int32,",
            "-initializer=(-self.sync_frequency),"
        ],
        "pos_line": [
            "+dtype=tf.int64,",
            "+initializer=tf.constant_initializer(value=(-self.sync_frequency), dtype=tf.int64),"
        ],
        "core_change": "-dtype=tf.int32, -initializer=(-self.sync_frequency), +dtype=tf.int64, +initializer=tf.constant_initializer(value=(-self.sync_frequency), dtype=tf.int64),",
        "core_API": "get_variable"
    },
    {
        "commit_hash": "9b1d0907486af6ee16e9e614839afbbe9d583a37",
        "index": "03895189..f9d44e85 100644",
        "commit_message": "fix bn performance\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class SingleCostFeedfreeTrainer(FeedfreeTrainer):",
            "cost_var = self.model.get_cost()",
            "# GATE_NONE faster?",
            "grads = self.config.optimizer.compute_gradients(",
            "-                cost_var, gate_gradients=0)",
            "+                cost_var,",
            "+                gate_gradients=tf.train.Optimizer.GATE_NONE,",
            "+                colocate_gradients_with_ops=False)",
            "add_moving_summary(cost_var)",
            "return cost_var, grads"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=keyword_argument), node=ASTNode(type=argument_list), position=2)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=2308563)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=2308564)",
            "Insert(target_node=ASTNode(type=keyword_argument), node=('=', '='), position=1, insert_id=2308565)",
            "Insert(target_node=ASTNode(type=keyword_argument), node=('attribute', None), position=2, insert_id=2308566)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'colocate_gradients_with_ops'), position=0, insert_id=2308567)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type==, text==), position=1)",
            "Insert(target_node=IN(type=keyword_argument), node=('false', 'False'), position=2, insert_id=2308568)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2308569)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2308570)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'GATE_NONE'), position=2, insert_id=2308571)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2308572)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2308573)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Optimizer'), position=2, insert_id=2308574)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2308575)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2308576)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'train'), position=2, insert_id=2308577)",
            "Delete(target_node=ASTNode(type=integer, text=0))"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 18,
        "number": 6187,
        "neg_line": [
            "-cost_var, gate_gradients=0)"
        ],
        "pos_line": [
            "+cost_var,",
            "+gate_gradients=tf.train.Optimizer.GATE_NONE,",
            "+colocate_gradients_with_ops=False)"
        ],
        "core_change": "-cost_var, gate_gradients=0) +cost_var, +gate_gradients=tf.train.Optimizer.GATE_NONE, +colocate_gradients_with_ops=False)",
        "core_API": "get_cost"
    },
    {
        "commit_hash": "d25221c8f281521bde2d12d9bdc9c4089ce3e305",
        "index": "e02e437..559988f 100644",
        "commit_message": "Fix batch normaliziation (1-x)\n\n",
        "file": "PyTorch-YOLOv3.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "def create_modules(module_defs):",
            ")",
            "if bn:",
            "modules.add_module(f\"batch_norm_{module_i}\",",
            "-                                   nn.BatchNorm2d(filters, momentum=0.9, eps=1e-5))",
            "+                                   nn.BatchNorm2d(filters, momentum=0.1, eps=1e-5))",
            "if module_def[\"activation\"] == \"leaky\":",
            "modules.add_module(f\"leaky_{module_i}\", nn.LeakyReLU(0.1))",
            "if module_def[\"activation\"] == \"mish\":"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=float, text=0.9), value='0.1')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 6189,
        "neg_line": [
            "-nn.BatchNorm2d(filters, momentum=0.9, eps=1e-5))"
        ],
        "pos_line": [
            "+nn.BatchNorm2d(filters, momentum=0.1, eps=1e-5))"
        ],
        "core_change": "-nn.BatchNorm2d(filters, momentum=0.9, eps=1e-5)) +nn.BatchNorm2d(filters, momentum=0.1, eps=1e-5))",
        "core_API": "add_module"
    },
    {
        "commit_hash": "ba4d30c42e0702bd894c36777d7d2c0adf74516c",
        "index": "797cbad38..759b31dce 100644",
        "commit_message": "Module namespace cleanup for v2.0 (#3875)\n\n* Imports cleaning\n\n* Small change\n\n* Remove unused methods\n\n* Small fix\n\n* Additional fix\n\n* Final fix\n\n* Fix benchmark test\n\n* Fix benchmark test #2\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "def test_table_to_pandas(dtype, dummy_value):",
            "features = datasets.Features({\"foo\": datasets.Array2D(dtype=dtype, shape=(2, 2))})",
            "dataset = datasets.Dataset.from_dict({\"foo\": [[[dummy_value] * 2] * 2]}, features=features)",
            "df = dataset._data.to_pandas()",
            "-    assert type(df.foo.dtype) == datasets.features.PandasArrayExtensionDtype",
            "+    assert type(df.foo.dtype) == PandasArrayExtensionDtype",
            "arr = df.foo.to_numpy()",
            "np.testing.assert_equal(arr, np.array([[[dummy_value] * 2] * 2], dtype=np.dtype(dtype)))"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=comparison_operator), node=ASTNode(type=identifier, text=PandasArrayExtensionDtype), position=2)",
            "Delete(target_node=ASTNode(type=identifier, text=datasets))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=features))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 6191,
        "neg_line": [
            "-assert type(df.foo.dtype) == datasets.features.PandasArrayExtensionDtype"
        ],
        "pos_line": [
            "+assert type(df.foo.dtype) == PandasArrayExtensionDtype"
        ],
        "core_change": "-assert type(df.foo.dtype) == datasets.features.PandasArrayExtensionDtype +assert type(df.foo.dtype) == PandasArrayExtensionDtype",
        "core_API": "Features"
    },
    {
        "commit_hash": "09a081ee422b85ab15f91554e05dce5c10e88c49",
        "index": "7c7967f..1a2ff19 100644",
        "commit_message": "fixed indendentation\n\n",
        "file": "gpt-neo.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def attn(x, scope, n_state, *, past, params, train=False):",
            "",
            "def split_heads(x, last_dim):",
            "with tf.variable_scope('split_heads'):",
            "-        # From [batch, sequence, features] to [batch, heads, sequence, features_per_head]",
            "-        # heads is split out of features!",
            "-        x = mtf.reshape(x, [dim_batch, dim_seq, dim_heads, last_dim], name=\"split_heads_reshape\")",
            "-        x = mtf.transpose(x, [dim_batch, dim_heads, dim_seq, last_dim], name=\"split_heads_transpose\")",
            "+            # From [batch, sequence, features] to [batch, heads, sequence, features_per_head]",
            "+            # heads is split out of features!",
            "+            x = mtf.reshape(x, [dim_batch, dim_seq, dim_heads, last_dim], name=\"split_heads_reshape\")",
            "+            x = mtf.transpose(x, [dim_batch, dim_heads, dim_seq, last_dim], name=\"split_heads_transpose\")",
            "return x",
            "",
            "def merge_heads(x):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 4,
        "minus_line": 4,
        "AST_diff_line": 17,
        "number": 6194,
        "neg_line": [
            "-# From [batch, sequence, features] to [batch, heads, sequence, features_per_head]",
            "-# heads is split out of features!",
            "-x = mtf.reshape(x, [dim_batch, dim_seq, dim_heads, last_dim], name=\"split_heads_reshape\")",
            "-x = mtf.transpose(x, [dim_batch, dim_heads, dim_seq, last_dim], name=\"split_heads_transpose\")"
        ],
        "pos_line": [
            "+# From [batch, sequence, features] to [batch, heads, sequence, features_per_head]",
            "+# heads is split out of features!",
            "+x = mtf.reshape(x, [dim_batch, dim_seq, dim_heads, last_dim], name=\"split_heads_reshape\")",
            "+x = mtf.transpose(x, [dim_batch, dim_heads, dim_seq, last_dim], name=\"split_heads_transpose\")"
        ],
        "core_change": "-# From [batch, sequence, features] to [batch, heads, sequence, features_per_head] -# heads is split out of features! -x = mtf.reshape(x, [dim_batch, dim_seq, dim_heads, last_dim], name=\"split_heads_reshape\") -x = mtf.transpose(x, [dim_batch, dim_heads, dim_seq, last_dim], name=\"split_heads_transpose\") +# From [batch, sequence, features] to [batch, heads, sequence, features_per_head] +# heads is split out of features! +x = mtf.reshape(x, [dim_batch, dim_seq, dim_heads, last_dim], name=\"split_heads_reshape\") +x = mtf.transpose(x, [dim_batch, dim_heads, dim_seq, last_dim], name=\"split_heads_transpose\")",
        "core_API": "variable_scope"
    },
    {
        "commit_hash": "5e61f27c9edbdeeb7e513ceabd977960b65996bf",
        "index": "da54114f..dd5b289a 100644",
        "commit_message": "Issue #689 fix - documentation refactoring (#691)\n\n* Documentation Refactoring\n\n* tl.visualize warning fixed\n\n* moving mean initializer moved in the arguments\n\n* Refactored Changelog\n\n* Docs Restructured with Subsection Title\n\n* https://github.com/zsdonghao/tensorlayer/ replaced by https://github.com/tensorlayer/tensorlayer/\n\n* Get Involved and Contribution Refactored\n\n* Redirections Fixed\n\n* get_involved added\n\n* Hao added\n\n* TL Layer Doc refactoring\n\n* Update contributing.rst\n\n* Core Team Updated\n\n* super resolution layers moved to convolution folder\n\n* YAPF Correction\n\n* Layer Documentation Refactored\n\n* Core Team Updated\n\n* Adding Travis TF 1.9.0rc0\n\n* Fangde Added to core team\n\n* Contributors page updated\n\n* Decorator Deprecated added\n\n* Changelog Updated\n\n* Update requirements.txt\n\n* Python 2 Bug Fix\n\n* Missing  Import fixed\n\n* YAPF Error Fix\n\n* 1.8.6rc6 released\n\n* Update README.rst\n\n* Setup Warning Fixed\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class BatchNormLayer(Layer):",
            "gamma = None",
            "",
            "# 2.",
            "-            moving_mean_init = tf.zeros_initializer()",
            "",
            "moving_mean = tf.get_variable(",
            "'moving_mean', params_shape, initializer=moving_mean_init, dtype=LayersConfig.tf_dtype, trainable=False"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=moving_mean_init))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=zeros_initializer))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 12,
        "number": 6195,
        "neg_line": [
            "-moving_mean_init = tf.zeros_initializer()"
        ],
        "pos_line": [],
        "core_change": "-moving_mean_init = tf.zeros_initializer()",
        "core_API": "zeros_initializer"
    },
    {
        "commit_hash": "68704e2e5b708d45d1f40ef7c14e5a4be68df3d1",
        "index": "18a19170..59790cf3 100644",
        "commit_message": "move test case to confusion_matrix_test.py\nfix dtype error in update_state()\n\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def _update_confusion_matrix_variables_optimized(",
            "y_pred)",
            "if not multi_label:",
            "label_weights = tf.reshape(label_weights, [-1])",
            "-  weights = tf.multiply(sample_weights, label_weights)",
            "+  weights = tf.cast(tf.multiply(sample_weights, label_weights), y_true.dtype)",
            "",
            "# We shouldn't need this, but in case there are predict value that is out of",
            "# the range of [0.0, 1.0]"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2057420)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=2057421)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2057422)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2057423)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cast'), position=2, insert_id=2057424)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2057425)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=call), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=2057426)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=3, insert_id=2057427)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=2057428)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'y_true'), position=0, insert_id=2057429)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2057430)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dtype'), position=2, insert_id=2057431)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 13,
        "number": 6196,
        "neg_line": [
            "-weights = tf.multiply(sample_weights, label_weights)"
        ],
        "pos_line": [
            "+weights = tf.cast(tf.multiply(sample_weights, label_weights), y_true.dtype)"
        ],
        "core_change": "-weights = tf.multiply(sample_weights, label_weights) +weights = tf.cast(tf.multiply(sample_weights, label_weights), y_true.dtype)",
        "core_API": "reshape"
    },
    {
        "commit_hash": "95b6d985ffb231d9a2047c039c95660913063b9d",
        "index": "c1062520..abb88b60 100644",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Attention(torch.nn.Module, Registrable):",
            "",
            "@overrides",
            "def forward(",
            "-        self, vector: torch.Tensor, matrix: torch.Tensor, matrix_mask: torch.Tensor = None",
            "+        self, vector: torch.Tensor, matrix: torch.Tensor, matrix_mask: torch.BoolTensor = None",
            ") -> torch.Tensor:",
            "similarities = self._forward_internal(vector, matrix)",
            "if self._normalize:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=Tensor), value='BoolTensor')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 6197,
        "neg_line": [
            "-self, vector: torch.Tensor, matrix: torch.Tensor, matrix_mask: torch.Tensor = None"
        ],
        "pos_line": [
            "+self, vector: torch.Tensor, matrix: torch.Tensor, matrix_mask: torch.BoolTensor = None"
        ],
        "core_change": "-self, vector: torch.Tensor, matrix: torch.Tensor, matrix_mask: torch.Tensor = None +self, vector: torch.Tensor, matrix: torch.Tensor, matrix_mask: torch.BoolTensor = None",
        "core_API": "_forward_internal"
    },
    {
        "commit_hash": "f30fc7d71c93ecc28115b9eca2c2d680bd061d09",
        "index": "5b926625..0a1f4491 100644",
        "commit_message": "Fix MultiheadAttention and torch hub\n\nSummary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/895\n\nReviewed By: akinh\n\nDifferential Revision: D18246479\n\nPulled By: myleott\n\nfbshipit-source-id: a610f1e4943619d32a523601a572fb09cdc5638d\n\n",
        "file": "fairseq.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class MultiheadAttention(nn.Module):",
            "",
            "for key, value in items_to_add.items():",
            "state_dict[key] = value",
            "-",
            "-        return state_dict"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=return, text=return))",
            "Delete(target_node=ASTNode(type=identifier, text=state_dict))",
            "Delete(target_node=ASTNode(type=return_statement))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 3,
        "number": 6200,
        "neg_line": [
            "-",
            "-return state_dict"
        ],
        "pos_line": [],
        "core_change": "- -return state_dict",
        "core_API": "items"
    },
    {
        "commit_hash": "bcd7cd0d4d00e8cc1ef66b964810ab7020b4f893",
        "index": "e0fc2d4c5e..e2736b5605 100644",
        "commit_message": "Type promotion fixes (#2620)\n\n* casting fixes\n\n* lint fixes\n\n* changes\n\n* more changes\n\n* lint fixes\n\n* Fixed matrix_Rank\n\n* update matrix_rank return dtype\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def matrix_rank(",
            "x = tf.expand_dims(x, 0)",
            "x, rtol = ivy.promote_types_of_inputs(x, rtol)",
            "ret = tf.linalg.matrix_rank(x, rtol)",
            "-    ret = tf.cast(ret, ivy.default_float_dtype(as_native=True))",
            "+    ret = tf.cast(ret, ivy.default_int_dtype(as_native=True))",
            "return ret"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=default_float_dtype), value='default_int_dtype')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 6201,
        "neg_line": [
            "-ret = tf.cast(ret, ivy.default_float_dtype(as_native=True))"
        ],
        "pos_line": [
            "+ret = tf.cast(ret, ivy.default_int_dtype(as_native=True))"
        ],
        "core_change": "-ret = tf.cast(ret, ivy.default_float_dtype(as_native=True)) +ret = tf.cast(ret, ivy.default_int_dtype(as_native=True))",
        "core_API": "expand_dims"
    },
    {
        "commit_hash": "89d124945add51b4218cf0a9028a3966cc9dfd47",
        "index": "a9bf4a6c..2a2c2031 100644",
        "commit_message": "ONNX supervised inpainting (#906)\n\n* ONNX supervised inpainting\n\n* sync with the torch pipeline\n\n* fix concat\n\n* update ref values\n\n* back to 8 steps\n\n* type fix\n\n* make fix-copies\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def convert_models(model_path: str, output_path: str, opset: int):",
            "unet_path = output_path / \"unet\" / \"model.onnx\"",
            "onnx_export(",
            "pipeline.unet,",
            "-        model_args=(torch.randn(2, 4, 64, 64), torch.LongTensor([0, 1]), torch.randn(2, 77, 768), False),",
            "+        model_args=(",
            "+            torch.randn(2, pipeline.unet.in_channels, 64, 64),",
            "+            torch.LongTensor([0, 1]),",
            "+            torch.randn(2, 77, 768),",
            "+            False,",
            "+        ),",
            "output_path=unet_path,",
            "ordered_input_names=[\"sample\", \"timestep\", \"encoder_hidden_states\", \"return_dict\"],",
            "output_names=[\"out_sample\"],  # has to be different from \"sample\" for correct tracing"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=tuple), node=(',', ','), position=8, insert_id=102301)",
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=3, insert_id=102302)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=102303)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=102304)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'in_channels'), position=2, insert_id=102305)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'pipeline'), position=0, insert_id=102306)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=102307)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'unet'), position=2, insert_id=102308)",
            "Delete(target_node=ASTNode(type=integer, text=4))"
        ],
        "plus_line": 6,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 6202,
        "neg_line": [
            "-model_args=(torch.randn(2, 4, 64, 64), torch.LongTensor([0, 1]), torch.randn(2, 77, 768), False),"
        ],
        "pos_line": [
            "+model_args=(",
            "+torch.randn(2, pipeline.unet.in_channels, 64, 64),",
            "+torch.LongTensor([0, 1]),",
            "+torch.randn(2, 77, 768),",
            "+False,",
            "+),"
        ],
        "core_change": "-model_args=(torch.randn(2, 4, 64, 64), torch.LongTensor([0, 1]), torch.randn(2, 77, 768), False), +model_args=( +torch.randn(2, pipeline.unet.in_channels, 64, 64), +torch.LongTensor([0, 1]), +torch.randn(2, 77, 768), +False, +),",
        "core_API": "randn"
    },
    {
        "commit_hash": "bd7068f635da74083efc0502837306fa56b86393",
        "index": "d8c6ca7..919dabe 100644",
        "commit_message": "fix tests hopefully\n\n",
        "file": "tinygrad.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "from tinygrad.llops.ops_cpu import unary_op, binary_op, reduce_op, movement_op",
            "",
            "from tinygrad.ops import ProcessingOps",
            "",
            "-def processing_op(op,x,w,ret,C):",
            "+def processing_op(ctx,op,x,w,out_shape,C):",
            "assert op == ProcessingOps.CONV, f\"{op} isn't supported\"",
            "-  ret[:] = torch.conv2d(x, w, stride=(C.ys, C.xs), groups=C.groups, dilation=(C.dy, C.dx), padding=(C.py, C.px))",
            "+  return torch.conv2d(x, w, stride=(C.ys, C.xs), groups=C.groups, dilation=(C.dy, C.dx), padding=(C.py, C.px))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('return_statement', None), position=4, insert_id=1165334)",
            "Insert(target_node=IN(type=return_statement), node=('return', 'return'), position=0, insert_id=1165335)",
            "Move(target_node=IN(type=return_statement), node=ASTNode(type=call), position=1)",
            "Insert(target_node=ASTNode(type=parameters), node=('identifier', 'ctx'), position=1, insert_id=1165336)",
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=2, insert_id=1165337)",
            "Update(target_node=ASTNode(type=identifier, text=ret), value='out_shape')",
            "Delete(target_node=ASTNode(type=identifier, text=ret))",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=slice))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 15,
        "number": 6206,
        "neg_line": [
            "-def processing_op(op,x,w,ret,C):",
            "-ret[:] = torch.conv2d(x, w, stride=(C.ys, C.xs), groups=C.groups, dilation=(C.dy, C.dx), padding=(C.py, C.px))"
        ],
        "pos_line": [
            "+def processing_op(ctx,op,x,w,out_shape,C):",
            "+return torch.conv2d(x, w, stride=(C.ys, C.xs), groups=C.groups, dilation=(C.dy, C.dx), padding=(C.py, C.px))"
        ],
        "core_change": "-def processing_op(op,x,w,ret,C): +def processing_op(ctx,op,x,w,out_shape,C): -ret[:] = torch.conv2d(x, w, stride=(C.ys, C.xs), groups=C.groups, dilation=(C.dy, C.dx), padding=(C.py, C.px)) +return torch.conv2d(x, w, stride=(C.ys, C.xs), groups=C.groups, dilation=(C.dy, C.dx), padding=(C.py, C.px))",
        "core_API": "conv2d"
    },
    {
        "commit_hash": "732dc8bb459cb885b8e081d6c723845dca9d79a2",
        "index": "9012a34a..2ef6d603 100755",
        "commit_message": "Changed memory and multithreaded test, added documentation, fixed memory retrieve_sequences problems\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Dropout(Layer):",
            "return tf.cond(",
            "pred=update,",
            "true_fn=(lambda: tf.nn.dropout(x=x, keep_prob=(1.0 - self.rate))),",
            "-            false_fn=(lambda: x)",
            "+            false_fn=(lambda: tf.identity(input=x))",
            ")"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=lambda), node=('call', None), position=2, insert_id=2235671)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2235672)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2235673)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2235674)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2235675)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'identity'), position=2, insert_id=2235676)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2235677)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=1, insert_id=2235678)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2235679)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'input'), position=0, insert_id=2235680)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2235681)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=identifier, text=x), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 12,
        "number": 6207,
        "neg_line": [
            "-false_fn=(lambda: x)"
        ],
        "pos_line": [
            "+false_fn=(lambda: tf.identity(input=x))"
        ],
        "core_change": "-false_fn=(lambda: x) +false_fn=(lambda: tf.identity(input=x))",
        "core_API": "cond"
    },
    {
        "commit_hash": "37c5c8e0b68445536d1b3eabd2ab1ffe4d87b61a",
        "index": "9c9483d..1a090e3 100644",
        "commit_message": "Linter, deprecated type()\n\nSummary: Run linter after recent changes. Fix long comment in knn.h which clang-format has reflowed badly. Add crude test that code doesn't call deprecated `.type()` or `.data()`.\n\nReviewed By: nikhilaravi\n\nDifferential Revision: D20692935\n\nfbshipit-source-id: 28ce0308adae79a870cb41a810b7cf8744f41ab8\n\n",
        "file": "pytorch3d.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestKNN(unittest.TestCase):",
            "",
            "def test_knn_vs_python_cuda(self):",
            "\"\"\" Test CUDA output vs PyTorch implementation \"\"\"",
            "-        device = torch.device('cuda')",
            "+        device = torch.device(\"cuda\")",
            "Ns = [1, 4]",
            "Ds = [2, 3, 8]",
            "P1s = [1, 8, 64, 128, 1001]"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text='cuda'), value='\"cuda\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 6208,
        "neg_line": [
            "-device = torch.device('cuda')"
        ],
        "pos_line": [
            "+device = torch.device(\"cuda\")"
        ],
        "core_change": "-device = torch.device('cuda') +device = torch.device(\"cuda\")",
        "core_API": "device"
    },
    {
        "commit_hash": "9f2d590aba3f206a166837014b68274541f68a1f",
        "index": "b7c785f..0111c4f 100644",
        "commit_message": "Fix for TensorFlow v0.12 (tf.inv was removed)\n\n",
        "file": "gcn.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def sparse_dropout(x, keep_prob, noise_shape):",
            "random_tensor += tf.random_uniform(noise_shape)",
            "dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)",
            "pre_out = tf.sparse_retain(x, dropout_mask)",
            "-    return pre_out * tf.inv(keep_prob)",
            "+    return pre_out * (1./keep_prob)",
            "",
            "",
            "def dot(x, y, sparse=False):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=binary_operator), node=('parenthesized_expression', None), position=2, insert_id=1931969)",
            "Move(target_node=IN(type=parenthesized_expression), node=ASTNode(type=(, text=(), position=0)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('binary_operator', None), position=1, insert_id=1931970)",
            "Move(target_node=IN(type=parenthesized_expression), node=ASTNode(type=), text=)), position=2)",
            "Insert(target_node=IN(type=binary_operator), node=('float', '1.'), position=0, insert_id=1931971)",
            "Insert(target_node=IN(type=binary_operator), node=('/', '/'), position=1, insert_id=1931972)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=identifier, text=keep_prob), position=2)",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=inv))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 13,
        "number": 6212,
        "neg_line": [
            "-return pre_out * tf.inv(keep_prob)"
        ],
        "pos_line": [
            "+return pre_out * (1./keep_prob)"
        ],
        "core_change": "-return pre_out * tf.inv(keep_prob) +return pre_out * (1./keep_prob)",
        "core_API": "random_uniform"
    },
    {
        "commit_hash": "2fb219a658a9d1abb385a4175bc94543b218e17f",
        "index": "616c9f45bd..645aa4cf67 100644",
        "commit_message": "[Ray RLlib] Fix tree import (#7662)\n\n* Rollback.\n\n* Fix import tree error by adding meaningful error and replacing by tf.nest wherever possible.\n\n* LINT.\n\n* LINT.\n\n* Fix.\n\n* Fix log-likelihood test case failing on travis.\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def do_test_log_likelihood(run,",
            "prev_reward_batch=np.array([prev_r]))",
            "check(np.exp(logp), expected_prob, atol=0.2)",
            "",
            "+        if eager_ctx:",
            "+            eager_ctx.__exit__(None, None, None)",
            "+",
            "",
            "class TestComputeLogLikelihood(unittest.TestCase):",
            "def test_dqn(self):"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=ERROR), node=('if', 'if'), position=4, insert_id=2146935)",
            "Insert(target_node=ASTNode(type=ERROR), node=('identifier', 'eager_ctx'), position=5, insert_id=2146936)",
            "Insert(target_node=ASTNode(type=ERROR), node=(':', ':'), position=6, insert_id=2146937)",
            "Insert(target_node=ASTNode(type=ERROR), node=('identifier', 'eager_ctx'), position=7, insert_id=2146938)",
            "Insert(target_node=ASTNode(type=ERROR), node=('.', '.'), position=8, insert_id=2146939)",
            "Insert(target_node=ASTNode(type=ERROR), node=('identifier', '__exit__'), position=9, insert_id=2146940)",
            "Insert(target_node=ASTNode(type=ERROR), node=('parameters', None), position=10, insert_id=2146941)",
            "Insert(target_node=ASTNode(type=ERROR), node=('class', 'class'), position=11, insert_id=2146942)",
            "Insert(target_node=IN(type=parameters), node=('(', '('), position=0, insert_id=2146943)",
            "Insert(target_node=IN(type=parameters), node=('identifier', 'None'), position=1, insert_id=2146944)",
            "Insert(target_node=IN(type=parameters), node=(',', ','), position=2, insert_id=2146945)",
            "Insert(target_node=IN(type=parameters), node=('identifier', 'None'), position=3, insert_id=2146946)",
            "Insert(target_node=IN(type=parameters), node=(',', ','), position=4, insert_id=2146947)",
            "Insert(target_node=IN(type=parameters), node=('identifier', 'None'), position=5, insert_id=2146948)",
            "Insert(target_node=IN(type=parameters), node=(')', ')'), position=6, insert_id=2146949)",
            "Delete(target_node=ASTNode(type=identifier, text=class))"
        ],
        "plus_line": 2,
        "minus_line": 0,
        "AST_diff_line": 16,
        "number": 6216,
        "neg_line": [],
        "pos_line": [
            "+if eager_ctx:",
            "+eager_ctx.__exit__(None, None, None)",
            "+"
        ],
        "core_change": "+if eager_ctx: +eager_ctx.__exit__(None, None, None) +",
        "core_API": "array"
    },
    {
        "commit_hash": "4eb0d8fe6a407793ff6cdff4eba53eaecddfaecd",
        "index": "63a36b7..8472a97 100644",
        "commit_message": "fix: ignore update net1 in train2\n\n",
        "file": "deep-voice-conversion.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def train(args, logdir1, logdir2):",
            "# )",
            "",
            "session_inits = []",
            "-    ckpt2 = args.ckpt if args.ckpt else tf.train.latest_checkpoint(logdir2)",
            "+    ckpt2 = '{}/{}'.format(logdir2, args.ckpt) if args.ckpt else tf.train.latest_checkpoint(logdir2)",
            "if ckpt2:",
            "session_inits.append(SaverRestore(ckpt2))",
            "ckpt1 = tf.train.latest_checkpoint(logdir1)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=conditional_expression), node=('call', None), position=0, insert_id=1919882)",
            "Insert(target_node=ASTNode(type=conditional_expression), node=('attribute', None), position=3, insert_id=1919883)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1919884)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1919885)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'args'), position=0, insert_id=1919886)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1919887)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'ckpt'), position=2, insert_id=1919888)",
            "Insert(target_node=IN(type=attribute), node=('string', \"'{}/{}'\"), position=0, insert_id=1919889)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1919890)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'format'), position=2, insert_id=1919891)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1919892)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'logdir2'), position=1, insert_id=1919893)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=1919894)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=attribute), position=3)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=1919895)",
            "Delete(target_node=ASTNode(type=identifier, text=args))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=ckpt))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 19,
        "number": 6218,
        "neg_line": [
            "-ckpt2 = args.ckpt if args.ckpt else tf.train.latest_checkpoint(logdir2)"
        ],
        "pos_line": [
            "+ckpt2 = '{}/{}'.format(logdir2, args.ckpt) if args.ckpt else tf.train.latest_checkpoint(logdir2)"
        ],
        "core_change": "-ckpt2 = args.ckpt if args.ckpt else tf.train.latest_checkpoint(logdir2) +ckpt2 = '{}/{}'.format(logdir2, args.ckpt) if args.ckpt else tf.train.latest_checkpoint(logdir2)",
        "core_API": "latest_checkpoint"
    },
    {
        "commit_hash": "a24fdf2d1b36e699d8f4e3efd33b7b78d6a02e7e",
        "index": "d5ea68b4..226f08fe 100644",
        "commit_message": "Fixing and error related to Floor Division (#4221)\n\nSummary:\n# Before submitting\n\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\n- [ ] Did you make sure to update the docs?\n- [ ] Did you write any new necessary tests?\n\n## What does this PR do?\nFixes https://github.com/pytorch/fairseq/issues/4058\nWhile using the library the following warnings are shown which sometimes hinder the workflow. The warnings are\n\n`<USER_PATH>/fairseq/search.py:140: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n  beams_buf = indices_buf // vocab_size`\n\n`<USER_PATH>/fairseq/sequence_generator.py:666: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n  unfin_idx = bbsz_idx // beam_size`\n\nThe methodology was simple, instead of using the `//`, it was replaced by `torch.div(arg1, arg2, rounding_mode='trunc')` and the variable alues do not change for both before and after, just the warning is resolved.\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding �\nYes, I did! Thanks!\n\nPull Request resolved: https://github.com/pytorch/fairseq/pull/4221\n\nReviewed By: arbabu123\n\nDifferential Revision: D34538147\n\nPulled By: alexeib\n\nfbshipit-source-id: 143897a249129a163b6a30ba9b5cf5595ef42330\n\n",
        "file": "fairseq.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class BeamSearch(Search):",
            "scores_buf = top_prediction[0]",
            "indices_buf = top_prediction[1]",
            "# Project back into relative indices and beams",
            "-        beams_buf = indices_buf // vocab_size",
            "+        beams_buf = torch.div(indices_buf, vocab_size, rounding_mode='trunc')",
            "indices_buf = indices_buf.fmod(vocab_size)",
            "",
            "# At this point, beams_buf and indices_buf are single-dim and contain relative indices"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('call', None), position=2, insert_id=204060)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=204061)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=204062)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=204063)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=204064)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'div'), position=2, insert_id=204065)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=204066)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=indices_buf), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=204067)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=vocab_size), position=3)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=4, insert_id=204068)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=5, insert_id=204069)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=6, insert_id=204070)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'rounding_mode'), position=0, insert_id=204071)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=204072)",
            "Insert(target_node=IN(type=keyword_argument), node=('string', \"'trunc'\"), position=2, insert_id=204073)",
            "Delete(target_node=ASTNode(type=//, text=//))",
            "Delete(target_node=ASTNode(type=binary_operator))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 18,
        "number": 6219,
        "neg_line": [
            "-beams_buf = indices_buf // vocab_size"
        ],
        "pos_line": [
            "+beams_buf = torch.div(indices_buf, vocab_size, rounding_mode='trunc')"
        ],
        "core_change": "-beams_buf = indices_buf // vocab_size +beams_buf = torch.div(indices_buf, vocab_size, rounding_mode='trunc')",
        "core_API": "div"
    },
    {
        "commit_hash": "b8ff56896ccbd27a54035a90a3bc278a44541a74",
        "index": "ba5f83282..25333de0e 100644",
        "commit_message": "Fix bug of multi-gpu training in lm finetuning\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def main():",
            "global_step += 1",
            "",
            "# Save a trained model",
            "-        if args.do_train and ( n_gpu > 1 and torch.distributed.get_rank() == 0  or n_gpu <=1):",
            "+        if args.do_train and (args.local_rank == -1 or torch.distributed.get_rank() == 0):",
            "logger.info(\"** ** * Saving fine - tuned model ** ** * \")",
            "model.save_pretrained(args.output_dir)",
            "tokenizer.save_pretrained(args.output_dir)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=boolean_operator), node=('or', 'or'), position=1, insert_id=1247874)",
            "Insert(target_node=ASTNode(type=comparison_operator), node=('attribute', None), position=0, insert_id=1247875)",
            "Insert(target_node=ASTNode(type=comparison_operator), node=('==', '=='), position=1, insert_id=1247876)",
            "Insert(target_node=ASTNode(type=comparison_operator), node=('unary_operator', '-1'), position=2, insert_id=1247877)",
            "Update(target_node=ASTNode(type=identifier, text=n_gpu), value='args')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=n_gpu), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1247878)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'local_rank'), position=2, insert_id=1247879)",
            "Delete(target_node=ASTNode(type=>, text=>))",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=and, text=and))",
            "Delete(target_node=ASTNode(type=or, text=or))",
            "Delete(target_node=ASTNode(type=identifier, text=n_gpu))",
            "Delete(target_node=ASTNode(type=<=, text=<=))",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=comparison_operator))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 16,
        "number": 6221,
        "neg_line": [
            "-if args.do_train and ( n_gpu > 1 and torch.distributed.get_rank() == 0  or n_gpu <=1):"
        ],
        "pos_line": [
            "+if args.do_train and (args.local_rank == -1 or torch.distributed.get_rank() == 0):"
        ],
        "core_change": "-if args.do_train and ( n_gpu > 1 and torch.distributed.get_rank() == 0  or n_gpu <=1): +if args.do_train and (args.local_rank == -1 or torch.distributed.get_rank() == 0):",
        "core_API": "get_rank"
    },
    {
        "commit_hash": "304c2d06dc7ab25d7e9bb236e2f514f94bbeb4cb",
        "index": "3ad5cef8..b680381d 100644",
        "commit_message": "Fix clone_model to consider input_tensors\n\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "keras",
        "change": [
            "class TestModelCloning(keras_parameterized.TestCase):",
            "run_eagerly=testing_utils.should_run_eagerly())",
            "new_model.train_on_batch([val_a, val_b], val_out)",
            "",
            "+    # New model should use provided input tensors",
            "+    self.assertListEqual(new_model.inputs, new_input_tensors)",
            "+",
            "# On top of new, non-Keras tensors",
            "if not tf.executing_eagerly():",
            "# TODO(b/121277734):Skip Eager contexts, as Input() layers raise an error"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=2, insert_id=2520861)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=2520862)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2520863)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2520864)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=2520865)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2520866)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'assertListEqual'), position=2, insert_id=2520867)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2520868)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=2520869)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=2520870)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'new_input_tensors'), position=3, insert_id=2520871)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=2520872)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'new_model'), position=0, insert_id=2520873)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2520874)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'inputs'), position=2, insert_id=2520875)"
        ],
        "plus_line": 2,
        "minus_line": 0,
        "AST_diff_line": 15,
        "number": 6224,
        "neg_line": [],
        "pos_line": [
            "+# New model should use provided input tensors",
            "+self.assertListEqual(new_model.inputs, new_input_tensors)",
            "+"
        ],
        "core_change": "+# New model should use provided input tensors +self.assertListEqual(new_model.inputs, new_input_tensors) +",
        "core_API": "should_run_eagerly"
    },
    {
        "commit_hash": "da2f1325014a158be36d62bdd20293ef97e574f4",
        "index": "bbc73896..89e0dd2a 100644",
        "commit_message": "enable black in the precommit (#1777)\n\n* enable black in the precommit\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add some fixes\n\n* update libface detection url\n\n* added url from kornia checkpoint\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def cart2pol(x: torch.Tensor, y: torch.Tensor, eps: float = 1.0e-8) -> Tuple[tor",
            "if not (isinstance(x, torch.Tensor) & isinstance(y, torch.Tensor)):",
            "raise TypeError(f\"Input type is not a torch.Tensor. Got {type(x)}, {type(y)}\")",
            "",
            "-    rho = torch.sqrt(x ** 2 + y ** 2 + eps)",
            "+    rho = torch.sqrt(x**2 + y**2 + eps)",
            "phi = torch.atan2(y, x)",
            "return rho, phi"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 6225,
        "neg_line": [
            "-rho = torch.sqrt(x ** 2 + y ** 2 + eps)"
        ],
        "pos_line": [
            "+rho = torch.sqrt(x**2 + y**2 + eps)"
        ],
        "core_change": "-rho = torch.sqrt(x ** 2 + y ** 2 + eps) +rho = torch.sqrt(x**2 + y**2 + eps)",
        "core_API": "sqrt"
    },
    {
        "commit_hash": "34263ab196cbfab65f2afa1150e6cf6db5ffd8b6",
        "index": "612ba5763..19eb4eb87 100644",
        "commit_message": "rnn->encoders, removes _th suffix, same filenames for chainer and pytorch, modify tests to not rely on _th\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_train_acc():",
            "pytest.importorskip(\"torch\")",
            "import torch",
            "",
            "-    from espnet.nets.pytorch.e2e_asr_th import pad_list",
            "-    from espnet.nets.pytorch.e2e_asr_th import th_accuracy",
            "+    from espnet.nets.pytorch.e2e_asr import pad_list",
            "+    from espnet.nets.pytorch.e2e_asr import th_accuracy",
            "",
            "n_out = 7",
            "_eos = n_out - 1"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=e2e_asr_th), value='e2e_asr')",
            "Update(target_node=ASTNode(type=identifier, text=e2e_asr_th), value='e2e_asr')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 6228,
        "neg_line": [
            "-from espnet.nets.pytorch.e2e_asr_th import pad_list",
            "-from espnet.nets.pytorch.e2e_asr_th import th_accuracy"
        ],
        "pos_line": [
            "+from espnet.nets.pytorch.e2e_asr import pad_list",
            "+from espnet.nets.pytorch.e2e_asr import th_accuracy"
        ],
        "core_change": "-from espnet.nets.pytorch.e2e_asr_th import pad_list -from espnet.nets.pytorch.e2e_asr_th import th_accuracy +from espnet.nets.pytorch.e2e_asr import pad_list +from espnet.nets.pytorch.e2e_asr import th_accuracy",
        "core_API": "importorskip"
    },
    {
        "commit_hash": "c820514e445db2bc06b5593b0a808c2066157101",
        "index": "d533c1ebf..1756de4da 100644",
        "commit_message": "fixed indententation\n\n",
        "file": "PySyft.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def spdz_mul(x, y, workers, mod=field):",
            "delta = delta.broadcast(workers)",
            "epsilon = epsilon.broadcast(workers)",
            "",
            "-    z = torch.fmod((c",
            "-                    + torch.fmod((delta * b), mod)",
            "-                    + torch.fmod((epsilon * a), mod)",
            "-                    ), mod)",
            "-",
            "+    z = torch.fmod(",
            "+        (c + torch.fmod((delta * b), mod) + torch.fmod((epsilon * a), mod)), mod",
            "+    )",
            "",
            "z.child.public_add_(epsilon_delta)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 3,
        "minus_line": 4,
        "AST_diff_line": 17,
        "number": 6229,
        "neg_line": [
            "-z = torch.fmod((c",
            "-+ torch.fmod((delta * b), mod)",
            "-+ torch.fmod((epsilon * a), mod)",
            "-), mod)",
            "-"
        ],
        "pos_line": [
            "+z = torch.fmod(",
            "+(c + torch.fmod((delta * b), mod) + torch.fmod((epsilon * a), mod)), mod",
            "+)"
        ],
        "core_change": "-z = torch.fmod((c -+ torch.fmod((delta * b), mod) -+ torch.fmod((epsilon * a), mod) -), mod) - +z = torch.fmod( +(c + torch.fmod((delta * b), mod) + torch.fmod((epsilon * a), mod)), mod +)",
        "core_API": "broadcast"
    },
    {
        "commit_hash": "d08bc09495a289519a774e881f399cf864178f9b",
        "index": "847ef4e6..dfe30c7b 100644",
        "commit_message": "Fix unused variable found (#1250)\n\n* Fix Unused variable found\n\n* more fixes\n\n* Update kornia/enhance/histogram.py\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestPatchSequential:",
            "input = torch.randn(*shape, device=device, dtype=dtype)",
            "out = seq(input)",
            "if seq.return_label:",
            "-            out, label = out",
            "+            out, _ = out",
            "assert out.shape[-3:] == input.shape[-3:]",
            "",
            "reproducibility_test(input, seq)"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=label), value='_')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 6230,
        "neg_line": [
            "-out, label = out"
        ],
        "pos_line": [
            "+out, _ = out"
        ],
        "core_change": "-out, label = out +out, _ = out",
        "core_API": "randn"
    },
    {
        "commit_hash": "526f0c886c097da6f15b40498c0ba29b98a09f03",
        "index": "e99aa3ea..377aa4e6 100755",
        "commit_message": "Fix log stability in logits.\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Categorical(Distribution):",
            "self.probabilities = tf.nn.softmax(logits=self.logits, dim=-1)",
            "",
            "# \"normalized\" logits",
            "-        self.logits = tf.log(x=self.probabilities)",
            "+        self.logits = tf.log(x=self.probabilities + util.epsilon)",
            "",
            "# General distribution values",
            "self.distribution = (self.logits,)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=keyword_argument), node=('binary_operator', None), position=2, insert_id=2243614)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=binary_operator), node=('+', '+'), position=1, insert_id=2243615)",
            "Insert(target_node=IN(type=binary_operator), node=('attribute', None), position=2, insert_id=2243616)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'util'), position=0, insert_id=2243617)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2243618)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'epsilon'), position=2, insert_id=2243619)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 6234,
        "neg_line": [
            "-self.logits = tf.log(x=self.probabilities)"
        ],
        "pos_line": [
            "+self.logits = tf.log(x=self.probabilities + util.epsilon)"
        ],
        "core_change": "-self.logits = tf.log(x=self.probabilities) +self.logits = tf.log(x=self.probabilities + util.epsilon)",
        "core_API": "softmax"
    },
    {
        "commit_hash": "88dbbfb2d649c7fc7227c80354751502936c01ff",
        "index": "f37c2d77c..0d5df863e 100644",
        "commit_message": "Fix make fix-copies with type annotations (#13586)\n\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def _compute_mask_indices(",
            "tf.ones_like(spec_aug_mask_idxs), spec_aug_mask_idxs, spec_aug_mask.shape",
            ")",
            "",
            "-    return tf.cast(spec_aug_mask, tf.float32)",
            "+    return spec_aug_mask",
            "",
            "",
            "# Copied from transformers.models.bart.modeling_tf_bart._expand_mask"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=return, text=return), position=3)",
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=identifier, text=spec_aug_mask), position=4)",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=cast))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=float32))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=return_statement))",
            "Delete(target_node=ASTNode(type=ERROR, text=))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 6235,
        "neg_line": [
            "-return tf.cast(spec_aug_mask, tf.float32)"
        ],
        "pos_line": [
            "+return spec_aug_mask"
        ],
        "core_change": "-return tf.cast(spec_aug_mask, tf.float32) +return spec_aug_mask",
        "core_API": "ones_like"
    },
    {
        "commit_hash": "a979a3f34eebbea7e6ffa7506aaf83aa6c47e7a1",
        "index": "f8ddd7e3..e4622620 100755",
        "commit_message": "fix image summary\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def get_config():",
            "optimizer=tf.train.AdamOptimizer(lr, beta1=0.5, epsilon=1e-3),",
            "callbacks=Callbacks([",
            "StatPrinter(), ModelSaver(),",
            "-            ScheduledHyperParamSetter('learning_rate', [(200, 1e-4)])",
            "]),",
            "session_config=get_default_sess_config(0.5),",
            "model=Model(),",
            "step_per_epoch=300,",
            "-        max_epoch=300,",
            "+        max_epoch=200,",
            ")",
            "",
            "def sample(model_path):"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=integer, text=300), value='200')",
            "Move(target_node=ASTNode(type=list), node=ASTNode(type=], text=]), position=5)",
            "Delete(target_node=ASTNode(type=identifier, text=ScheduledHyperParamSetter))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=string, text='learning_rate'))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=integer, text=200))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=float, text=1e-4))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=tuple))",
            "Delete(target_node=ASTNode(type=list))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=], text=]))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 18,
        "number": 6241,
        "neg_line": [
            "-ScheduledHyperParamSetter('learning_rate', [(200, 1e-4)])",
            "-max_epoch=300,"
        ],
        "pos_line": [
            "+max_epoch=200,"
        ],
        "core_change": "-ScheduledHyperParamSetter('learning_rate', [(200, 1e-4)]) -max_epoch=300, +max_epoch=200,",
        "core_API": "AdamOptimizer"
    },
    {
        "commit_hash": "051311ff66e7b23bfcfc42bc514c969517323ce9",
        "index": "a0ce06ec5..7842b95b3 100644",
        "commit_message": "fix string (#18568)\n\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Pipeline(_ScikitCompat):",
            "elif device < 0:",
            "self.device = torch.device(\"cpu\")",
            "else:",
            "-                self.device = torch.device(\"cuda:{device}\")",
            "+                self.device = torch.device(f\"cuda:{device}\")",
            "else:",
            "self.device = device",
            "self.binary_output = binary_output"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"cuda:{device}\"), value='f\"cuda:{device}\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 6242,
        "neg_line": [
            "-self.device = torch.device(\"cuda:{device}\")"
        ],
        "pos_line": [
            "+self.device = torch.device(f\"cuda:{device}\")"
        ],
        "core_change": "-self.device = torch.device(\"cuda:{device}\") +self.device = torch.device(f\"cuda:{device}\")",
        "core_API": "device"
    },
    {
        "commit_hash": "c9042dc2adbb635aeca407c10cf492a6eb14d772",
        "index": "daef2a4..a4bc3ca 100755",
        "commit_message": "Improved non-latin `Annotator()` plotting (#7488)\n\n* Improved non-latin labels Annotator plotting\n\nMay resolve https://github.com/ultralytics/yolov5/issues/7460\n\n* Update train.py\n\n* Update train.py\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add progress arg\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "yolov5.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def check_file(file, suffix=''):",
            "return files[0]  # return file",
            "",
            "",
            "-def check_font(font=FONT):",
            "+def check_font(font=FONT, progress=False):",
            "# Download font to CONFIG_DIR if necessary",
            "font = Path(font)",
            "if not font.exists() and not (CONFIG_DIR / font.name).exists():",
            "url = \"https://ultralytics.com/assets/\" + font.name",
            "LOGGER.info(f'Downloading {url} to {CONFIG_DIR / font.name}...')",
            "-        torch.hub.download_url_to_file(url, str(font), progress=False)",
            "+        torch.hub.download_url_to_file(url, str(font), progress=progress)",
            "",
            "",
            "def check_dataset(data, autodownload=True):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=2, insert_id=1294810)",
            "Insert(target_node=ASTNode(type=parameters), node=('default_parameter', None), position=3, insert_id=1294811)",
            "Insert(target_node=IN(type=default_parameter), node=('identifier', 'progress'), position=0, insert_id=1294812)",
            "Insert(target_node=IN(type=default_parameter), node=('=', '='), position=1, insert_id=1294813)",
            "Insert(target_node=IN(type=default_parameter), node=('false', 'False'), position=2, insert_id=1294814)",
            "Insert(target_node=ASTNode(type=keyword_argument), node=('identifier', 'progress'), position=2, insert_id=1294815)",
            "Delete(target_node=ASTNode(type=false, text=False))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 6245,
        "neg_line": [
            "-def check_font(font=FONT):",
            "-torch.hub.download_url_to_file(url, str(font), progress=False)"
        ],
        "pos_line": [
            "+def check_font(font=FONT, progress=False):",
            "+torch.hub.download_url_to_file(url, str(font), progress=progress)"
        ],
        "core_change": "-def check_font(font=FONT): +def check_font(font=FONT, progress=False): -torch.hub.download_url_to_file(url, str(font), progress=False) +torch.hub.download_url_to_file(url, str(font), progress=progress)",
        "core_API": "exists"
    },
    {
        "commit_hash": "c4a7f16b67d5893abc16a8e019cf524115b9e78e",
        "index": "ecb33aed..c751d002 100755",
        "commit_message": "bug fix on resnet\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Model(ModelDesc):",
            "# weight decay on all W of fc layers",
            "wd_w = tf.train.exponential_decay(1e-4, get_global_step_var(),",
            "200000, 0.7, True)",
            "-        wd_w = wd_w / tf.get_default_graph().get_tensor_by_name('learning_rate')",
            "wd_cost = tf.mul(wd_w, regularize_cost('.*/W', tf.nn.l2_loss), name='l2_regularize_loss')",
            "add_moving_summary(loss, wd_cost)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=wd_w))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=wd_w))",
            "Delete(target_node=ASTNode(type=/, text=/))",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=get_default_graph))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=get_tensor_by_name))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=string, text='learning_rate'))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=binary_operator))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 23,
        "number": 6247,
        "neg_line": [
            "-wd_w = wd_w / tf.get_default_graph().get_tensor_by_name('learning_rate')"
        ],
        "pos_line": [],
        "core_change": "-wd_w = wd_w / tf.get_default_graph().get_tensor_by_name('learning_rate')",
        "core_API": "exponential_decay"
    },
    {
        "commit_hash": "69c4c78d0d7e203098aad11db5039d0702c1bb06",
        "index": "0a7599517..eca6b6b45 100644",
        "commit_message": "Identified/Fixed Several Bugs in Keras Worker (#87)\n\n* revamped keras interface into an initial client\n\n* updated notebook\n\n* added initial torch service\n\n* initial service for torch\n\n* initial service for torch\n\n* improved logging a bit\n\n* typo\n\n* typo\n\n* missing numpy import\n\n* another bugfix in fit\n\n",
        "file": "PySyft.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "\"execution_count\": 127,",
            "\"metadata\": {},",
            "\"outputs\": [],",
            "-   \"source\": [",
            "-    \"c = a.add(b).get()\"",
            "-   ]",
            "+   \"source\": []",
            "},",
            "{",
            "\"cell_type\": \"code\","
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=4, insert_id=865843)",
            "Move(target_node=IN(type=expression_statement), node=ASTNode(type=string, text=\"source\"), position=0)",
            "Insert(target_node=IN(type=expression_statement), node=('ERROR', None), position=1, insert_id=865844)",
            "Move(target_node=IN(type=expression_statement), node=ASTNode(type=,, text=,), position=2)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=:, text=:), position=0)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=[, text=[), position=1)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=], text=]), position=2)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=}, text=}), position=3)",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=string, text=\"c = a.add(b).get()\"))",
            "Delete(target_node=ASTNode(type=subscript))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 12,
        "number": 6248,
        "neg_line": [
            "-\"source\": [",
            "-\"c = a.add(b).get()\"",
            "-]"
        ],
        "pos_line": [
            "+\"source\": []"
        ],
        "core_change": "-\"source\": [ -\"c = a.add(b).get()\" -] +\"source\": []",
        "core_API": "add"
    },
    {
        "commit_hash": "c1528acd46fddcf02955f1922f594c3bb713ef16",
        "index": "e0d383d..47ee6ae 100644",
        "commit_message": "fix feature maps in Nest, thanks to @MarkYangjiayi\n\n",
        "file": "vit-pytorch.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class NesT(nn.Module):",
            "Aggregate(dim_in, dim_out) if not is_last else nn.Identity()",
            "]))",
            "",
            "+",
            "self.mlp_head = nn.Sequential(",
            "-            LayerNorm(dim),",
            "+            LayerNorm(last_dim),",
            "Reduce('b c h w -> b c', 'mean'),",
            "-            nn.Linear(dim, num_classes)",
            "+            nn.Linear(last_dim, num_classes)",
            ")",
            "",
            "def forward(self, img):"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=dim), value='last_dim')",
            "Update(target_node=ASTNode(type=identifier, text=dim), value='last_dim')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 6250,
        "neg_line": [
            "-LayerNorm(dim),",
            "-nn.Linear(dim, num_classes)"
        ],
        "pos_line": [
            "+",
            "+LayerNorm(last_dim),",
            "+nn.Linear(last_dim, num_classes)"
        ],
        "core_change": "+ -LayerNorm(dim), +LayerNorm(last_dim), -nn.Linear(dim, num_classes) +nn.Linear(last_dim, num_classes)",
        "core_API": "Identity"
    },
    {
        "commit_hash": "ae03c42dd6cd9c493814f149abff0324fadca190",
        "index": "8c8eafba3c..47ff12ebda 100644",
        "commit_message": "Fixed inconsistent action placeholder (#5213)\n\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class MARWILPolicy(MARWILPostprocessing, TFPolicy):",
            "self.output_actions = action_dist.sample()",
            "",
            "# Training inputs",
            "-        self.act_t = tf.placeholder(tf.int32, [None], name=\"action\")",
            "+        self.act_t = ModelCatalog.get_action_placeholder(action_space)",
            "self.cum_rew_t = tf.placeholder(tf.float32, [None], name=\"reward\")",
            "",
            "# v network evaluation"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tf), value='ModelCatalog')",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=placeholder), value='get_action_placeholder')",
            "Update(target_node=ASTNode(type=identifier, text=tf), value='action_space')",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=tf), position=1)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=int32))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=none, text=None))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=list))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=name))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=string, text=\"action\"))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 18,
        "number": 6251,
        "neg_line": [
            "-self.act_t = tf.placeholder(tf.int32, [None], name=\"action\")"
        ],
        "pos_line": [
            "+self.act_t = ModelCatalog.get_action_placeholder(action_space)"
        ],
        "core_change": "-self.act_t = tf.placeholder(tf.int32, [None], name=\"action\") +self.act_t = ModelCatalog.get_action_placeholder(action_space)",
        "core_API": "sample"
    },
    {
        "commit_hash": "b2ee110c853c5effdd8d21f50a8437485bafb285",
        "index": "a3ddea81..bacd0e8a 100644",
        "commit_message": "Bugfixes (#1164)\n\nSummary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1164\n\nReviewed By: ngoyal2707\n\nDifferential Revision: D21373232\n\nPulled By: myleott\n\nfbshipit-source-id: f31c65c6f2ebd9a603099e0cbe9e32c47585f50d\n\n",
        "file": "fairseq.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DummyMaskedLMTask(FairseqTask):",
            "'id': 1,",
            "'net_input': {",
            "'src_tokens': torch.stack([self.dummy_src for _ in range(bsz)]),",
            "-                    'src_lengths': torch.full((bsz, ), self.args.tokens_per_sample),",
            "+                    'src_lengths': torch.full(",
            "+                        (bsz, ), self.args.tokens_per_sample, dtype=torch.long",
            "+                    ),",
            "},",
            "'target': torch.stack([self.dummy_tgt for _ in range(bsz)]),",
            "'nsentences': bsz,"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=213082)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=213083)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=213084)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=213085)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=213086)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=213087)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=213088)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'long'), position=2, insert_id=213089)"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 6257,
        "neg_line": [
            "-'src_lengths': torch.full((bsz, ), self.args.tokens_per_sample),"
        ],
        "pos_line": [
            "+'src_lengths': torch.full(",
            "+(bsz, ), self.args.tokens_per_sample, dtype=torch.long",
            "+),"
        ],
        "core_change": "-'src_lengths': torch.full((bsz, ), self.args.tokens_per_sample), +'src_lengths': torch.full( +(bsz, ), self.args.tokens_per_sample, dtype=torch.long +),",
        "core_API": "stack"
    },
    {
        "commit_hash": "983163494663e24b611f1ba8d5d47a3edc00e2e5",
        "index": "89ea982f..789169d2 100644",
        "commit_message": "Misc fixes (#2448)\n\nSummary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/2448\n\nReviewed By: ngoyal2707\n\nDifferential Revision: D23011193\n\nPulled By: myleott\n\nfbshipit-source-id: 1a29481707108e4465aca78ec1581fb79f05efba\n\n",
        "file": "fairseq.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class NanDetector:",
            "def _detect(self, tensor, name, backward):",
            "err = None",
            "if (",
            "-            tensor.numel() >= 2",
            "-        ):  # single value tensors (like the loss) will not provide much info",
            "+            torch.is_floating_point(tensor)",
            "+            # single value tensors (like the loss) will not provide much info",
            "+            and tensor.numel() >= 2",
            "+        ):",
            "with torch.no_grad():",
            "if torch.isnan(tensor).any():",
            "err = \"NaN\""
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=parenthesized_expression), node=('boolean_operator', None), position=1, insert_id=211698)",
            "Insert(target_node=IN(type=boolean_operator), node=('call', None), position=0, insert_id=211699)",
            "Insert(target_node=IN(type=boolean_operator), node=('and', 'and'), position=1, insert_id=211700)",
            "Move(target_node=IN(type=boolean_operator), node=ASTNode(type=comparison_operator), position=2)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=211701)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=211702)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=211703)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=211704)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'is_floating_point'), position=2, insert_id=211705)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=211706)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'tensor'), position=1, insert_id=211707)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=211708)"
        ],
        "plus_line": 4,
        "minus_line": 2,
        "AST_diff_line": 12,
        "number": 6260,
        "neg_line": [
            "-tensor.numel() >= 2",
            "-):  # single value tensors (like the loss) will not provide much info"
        ],
        "pos_line": [
            "+torch.is_floating_point(tensor)",
            "+# single value tensors (like the loss) will not provide much info",
            "+and tensor.numel() >= 2",
            "+):"
        ],
        "core_change": "-tensor.numel() >= 2 -):  # single value tensors (like the loss) will not provide much info +torch.is_floating_point(tensor) +# single value tensors (like the loss) will not provide much info +and tensor.numel() >= 2 +):",
        "core_API": "numel"
    },
    {
        "commit_hash": "d4e12751495d339643718e99ba98fe92532a29aa",
        "index": "48433997..dbb96f7a 100644",
        "commit_message": "fix flow direction for meta example\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class MetaLayer(torch.nn.Module):",
            "# u: [B, F_u]",
            "# batch: [N] with max entry B - 1.",
            "row, col = edge_index",
            "-                out = torch.cat([x[col], edge_attr], dim=1)",
            "+                out = torch.cat([x[row], edge_attr], dim=1)",
            "out = self.node_mlp_1(out)",
            "-                out = scatter_mean(out, row, dim=0, dim_size=x.size(0))",
            "+                out = scatter_mean(out, col, dim=0, dim_size=x.size(0))",
            "out = torch.cat([x, out, u[batch]], dim=1)",
            "return self.node_mlp_2(out)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=row), value='col')",
            "Update(target_node=ASTNode(type=identifier, text=col), value='row')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 6262,
        "neg_line": [
            "-out = torch.cat([x[col], edge_attr], dim=1)",
            "-out = scatter_mean(out, row, dim=0, dim_size=x.size(0))"
        ],
        "pos_line": [
            "+out = torch.cat([x[row], edge_attr], dim=1)",
            "+out = scatter_mean(out, col, dim=0, dim_size=x.size(0))"
        ],
        "core_change": "-out = torch.cat([x[col], edge_attr], dim=1) +out = torch.cat([x[row], edge_attr], dim=1) -out = scatter_mean(out, row, dim=0, dim_size=x.size(0)) +out = scatter_mean(out, col, dim=0, dim_size=x.size(0))",
        "core_API": "cat"
    },
    {
        "commit_hash": "32a47cfbb02c31b691aee04764fd6c8eb34b1a3f",
        "index": "fd6e6afe9..8ab813fe9 100644",
        "commit_message": "fix: hot fix of tf_model & removal of unused code\n\n",
        "file": "DeepPavlov.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFModel(metaclass=TfModelMeta):",
            "print('\\n:: Model saved to {} \\n'.format(self._model_path.as_posix()))",
            "",
            "def get_checkpoint_state(self):",
            "-        return tf.train.get_checkpoint_state(self._model_path.as_posix())",
            "+        return tf.train.get_checkpoint_state(self._model_path.parent)",
            "",
            "def load(self):",
            "\"\"\""
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=IN(type=root), node=ASTNode(type=argument_list), position=0)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=attribute), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=as_posix), value='parent')",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 6269,
        "neg_line": [
            "-return tf.train.get_checkpoint_state(self._model_path.as_posix())"
        ],
        "pos_line": [
            "+return tf.train.get_checkpoint_state(self._model_path.parent)"
        ],
        "core_change": "-return tf.train.get_checkpoint_state(self._model_path.as_posix()) +return tf.train.get_checkpoint_state(self._model_path.parent)",
        "core_API": "as_posix"
    },
    {
        "commit_hash": "4dbe7ed0de30146e6a10ab28ecda0b0fe48f6a4d",
        "index": "e5c62b0e..e3a5ff3c 100644",
        "commit_message": "Fix all-zero duration case for GlowTTS\n\n",
        "file": "TTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class GlowTTS(BaseTTS):",
            "o_mean, o_log_scale, o_dur_log, x_mask = self.encoder(x, x_lengths, g=g)",
            "# compute output durations",
            "w = (torch.exp(o_dur_log) - 1) * x_mask * self.length_scale",
            "-        w_ceil = torch.ceil(w)",
            "+        w_ceil = torch.clamp_min(torch.ceil(w), 1)",
            "y_lengths = torch.clamp_min(torch.sum(w_ceil, [1, 2]), 1).long()",
            "y_max_length = None",
            "# compute masks"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1255772)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=1255773)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1255774)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1255775)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'clamp_min'), position=2, insert_id=1255776)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1255777)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=call), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=1255778)",
            "Insert(target_node=IN(type=argument_list), node=('integer', '1'), position=3, insert_id=1255779)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=1255780)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 6270,
        "neg_line": [
            "-w_ceil = torch.ceil(w)"
        ],
        "pos_line": [
            "+w_ceil = torch.clamp_min(torch.ceil(w), 1)"
        ],
        "core_change": "-w_ceil = torch.ceil(w) +w_ceil = torch.clamp_min(torch.ceil(w), 1)",
        "core_API": "encoder"
    },
    {
        "commit_hash": "c51dc4f92755c67a83f3fc8a0bd6b3e64df199e4",
        "index": "094b2d33f..e97091c35 100644",
        "commit_message": "[torch] remove deprecated uint8 in favor of bool (#21384)\n\n* uint8 -> bool\n\n* fix copies\n\n* style\n\n* update test modeling commen when checking attention buffers\n\n* style\n\n* use logical not on random mask instead of subtraction with 1\n\n* remove torch uint8\n\n* quality\n\n* remove modified modeling utils\n\n* Update based on review\n\nCo-authored-by: sgugger <sylvain.gugger@gmail.com>\n\n---------\n\nCo-authored-by: sgugger <sylvain.gugger@gmail.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TransfoXLModel(TransfoXLPreTrainedModel):",
            "mask_shift_len = qlen",
            "dec_attn_mask = (torch.triu(all_ones, 1 + mlen) + torch.tril(all_ones, -mask_shift_len))[:, :, None]  # -1",
            "else:",
            "-            dec_attn_mask = torch.triu(word_emb.new_ones((qlen, klen), dtype=torch.uint8), diagonal=1 + mlen)[",
            "+            dec_attn_mask = torch.triu(word_emb.new_ones((qlen, klen), dtype=torch.bool), diagonal=1 + mlen)[",
            ":, :, None",
            "]"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=uint8), value='bool')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 6272,
        "neg_line": [
            "-dec_attn_mask = torch.triu(word_emb.new_ones((qlen, klen), dtype=torch.uint8), diagonal=1 + mlen)["
        ],
        "pos_line": [
            "+dec_attn_mask = torch.triu(word_emb.new_ones((qlen, klen), dtype=torch.bool), diagonal=1 + mlen)["
        ],
        "core_change": "-dec_attn_mask = torch.triu(word_emb.new_ones((qlen, klen), dtype=torch.uint8), diagonal=1 + mlen)[ +dec_attn_mask = torch.triu(word_emb.new_ones((qlen, klen), dtype=torch.bool), diagonal=1 + mlen)[",
        "core_API": "triu"
    },
    {
        "commit_hash": "f72f17d0e324cfa813592f35e71ad51588dc57fe",
        "index": "2790ef25..df4fa3a1 100644",
        "commit_message": "quickfix batch shape\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Categorical(Distribution):",
            "def log_prob(self, dist, actions):",
            "prob = dist['policy_output']",
            "",
            "-        return tf.log(tf.reduce_sum(tf.multiply(prob, actions), [1]) + self.epsilon)",
            "+        return tf.log(tf.reduce_sum(tf.multiply(prob, actions), [2]) + self.epsilon)",
            "",
            "def fixed_kl(self, dist):",
            "\"\"\""
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=integer, text=1), value='2')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 6274,
        "neg_line": [
            "-return tf.log(tf.reduce_sum(tf.multiply(prob, actions), [1]) + self.epsilon)"
        ],
        "pos_line": [
            "+return tf.log(tf.reduce_sum(tf.multiply(prob, actions), [2]) + self.epsilon)"
        ],
        "core_change": "-return tf.log(tf.reduce_sum(tf.multiply(prob, actions), [1]) + self.epsilon) +return tf.log(tf.reduce_sum(tf.multiply(prob, actions), [2]) + self.epsilon)",
        "core_API": "log"
    },
    {
        "commit_hash": "16932d0b566d8a0fbdb041402e5f5aad5108d2e4",
        "index": "5bf2584..bda3274 100644",
        "commit_message": "fix for issue #209\n\n",
        "file": "tensorflow-wavenet.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def mu_law_encode(audio, quantization_channels):",
            "with tf.name_scope('encode'):",
            "mu = quantization_channels - 1",
            "# Perform mu-law companding transformation (ITU-T, 1988).",
            "-        magnitude = tf.log(1 + mu * tf.abs(audio)) / tf.log(1. + mu)",
            "+        # Minimum operation is here to deal with rare large amplitudes caused by resampling.",
            "+        magnitude = tf.log(1 + mu * tf.minimum(tf.abs(audio), 1.0)) / tf.log(1. + mu)",
            "signal = tf.sign(audio) * magnitude",
            "# Quantize signal to the specified number of levels.",
            "return tf.cast((signal + 1) / 2 * mu + 0.5, tf.int32)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2207229)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=2207230)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2207231)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2207232)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'minimum'), position=2, insert_id=2207233)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2207234)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=call), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=2207235)",
            "Insert(target_node=IN(type=argument_list), node=('float', '1.0'), position=3, insert_id=2207236)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=2207237)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 6277,
        "neg_line": [
            "-magnitude = tf.log(1 + mu * tf.abs(audio)) / tf.log(1. + mu)"
        ],
        "pos_line": [
            "+# Minimum operation is here to deal with rare large amplitudes caused by resampling.",
            "+magnitude = tf.log(1 + mu * tf.minimum(tf.abs(audio), 1.0)) / tf.log(1. + mu)"
        ],
        "core_change": "-magnitude = tf.log(1 + mu * tf.abs(audio)) / tf.log(1. + mu) +# Minimum operation is here to deal with rare large amplitudes caused by resampling. +magnitude = tf.log(1 + mu * tf.minimum(tf.abs(audio), 1.0)) / tf.log(1. + mu)",
        "core_API": "name_scope"
    },
    {
        "commit_hash": "f17c111d9b33c07a3a5d0fc00651d0ccfe33aa2a",
        "index": "dc8086fd..ec7f09a2 100644",
        "commit_message": "fix a bug in tutorial 1\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Model():",
            "# convert inputs to tensor if it is originally not",
            "if isinstance(inputs, list):",
            "for idx in range(len(inputs)):",
            "-                if isinstance(inputs[idx], np.ndarray):",
            "-                    inputs[idx] = tf.convert_to_tensor(inputs[idx])",
            "+                inputs[idx] = tf.convert_to_tensor(inputs[idx])",
            "elif isinstance(inputs, np.ndarray):",
            "inputs = tf.convert_to_tensor(inputs)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=if, text=if))",
            "Delete(target_node=ASTNode(type=identifier, text=isinstance))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=inputs))",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=identifier, text=idx))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=np))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=ndarray))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=if_statement))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 18,
        "number": 6282,
        "neg_line": [
            "-if isinstance(inputs[idx], np.ndarray):",
            "-inputs[idx] = tf.convert_to_tensor(inputs[idx])"
        ],
        "pos_line": [
            "+inputs[idx] = tf.convert_to_tensor(inputs[idx])"
        ],
        "core_change": "-if isinstance(inputs[idx], np.ndarray): -inputs[idx] = tf.convert_to_tensor(inputs[idx]) +inputs[idx] = tf.convert_to_tensor(inputs[idx])",
        "core_API": "convert_to_tensor"
    },
    {
        "commit_hash": "ff0db800cca34f3177f6e4e629aed7935a4ddb79",
        "index": "e791acc99..1c8fb7dd7 100644",
        "commit_message": "fixed\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def decode(args):",
            "def _convert_att_ws_to_durations(att_ws):",
            "# get the most diagonal attention among the all of the sournce attentions for transformer",
            "if len(att_ws.shape) == 4:",
            "-            att_ws = torch.cat([att_w for att_w in att_ws], axis=0)  # (#heads * #layers, L, T)",
            "+            att_ws = torch.cat([att_w for att_w in att_ws], dim=0)  # (#heads * #layers, L, T)",
            "diagonal_scores = att_ws.max(dim=-1)[0].mean(dim=-1).mean(dim=0)  # (#heads * #layers,)",
            "diagonal_head_idx = diagonal_scores.argmax()",
            "att_ws = att_ws[diagonal_head_idx]  # (L, T)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=axis), value='dim')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 6288,
        "neg_line": [
            "-att_ws = torch.cat([att_w for att_w in att_ws], axis=0)  # (#heads * #layers, L, T)"
        ],
        "pos_line": [
            "+att_ws = torch.cat([att_w for att_w in att_ws], dim=0)  # (#heads * #layers, L, T)"
        ],
        "core_change": "-att_ws = torch.cat([att_w for att_w in att_ws], axis=0)  # (#heads * #layers, L, T) +att_ws = torch.cat([att_w for att_w in att_ws], dim=0)  # (#heads * #layers, L, T)",
        "core_API": "cat"
    },
    {
        "commit_hash": "1bab0a17a9139ba95841fee3c716346a2ca05ed7",
        "index": "79c0cf7c1..173d1f8a0 100644",
        "commit_message": "Fix torch bfloat import version (#9089)\n\n\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class AMPTestModel(BoringModel):",
            "16,",
            "pytest.param(",
            "\"bf16\",",
            "-            marks=pytest.mark.skipif(not _TORCH_GREATER_EQUAL_1_10, reason=\"torch.bfloat16 not available\"),",
            "+            marks=pytest.mark.skipif(not _TORCH_BFLOAT_AVAILABLE, reason=\"torch.bfloat16 not available\"),",
            "),",
            "],",
            ")"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=_TORCH_GREATER_EQUAL_1_10), value='_TORCH_BFLOAT_AVAILABLE')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 6294,
        "neg_line": [
            "-marks=pytest.mark.skipif(not _TORCH_GREATER_EQUAL_1_10, reason=\"torch.bfloat16 not available\"),"
        ],
        "pos_line": [
            "+marks=pytest.mark.skipif(not _TORCH_BFLOAT_AVAILABLE, reason=\"torch.bfloat16 not available\"),"
        ],
        "core_change": "-marks=pytest.mark.skipif(not _TORCH_GREATER_EQUAL_1_10, reason=\"torch.bfloat16 not available\"), +marks=pytest.mark.skipif(not _TORCH_BFLOAT_AVAILABLE, reason=\"torch.bfloat16 not available\"),",
        "core_API": "param"
    },
    {
        "commit_hash": "a1f110c2626a0197cca28c56ded4347c80cdfbcd",
        "index": "0a8f83d7..bac34709 100644",
        "commit_message": "quick fix for dropout_node when training=False (#5513)\n\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def dropout_node(edge_index: Tensor, p: float = 0.5,",
            "num_nodes = maybe_num_nodes(edge_index, num_nodes)",
            "",
            "if not training or p == 0.0:",
            "-        node_mask = edge_index.new_zeros(num_nodes, dtype=torch.bool)",
            "-        edge_mask = edge_index.new_zeros(edge_index.size(1), dtype=torch.bool)",
            "+        node_mask = edge_index.new_ones(num_nodes, dtype=torch.bool)",
            "+        edge_mask = edge_index.new_ones(edge_index.size(1), dtype=torch.bool)",
            "return edge_index, edge_mask, node_mask",
            "",
            "prob = torch.rand(num_nodes, device=edge_index.device)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=new_zeros), value='new_ones')",
            "Update(target_node=ASTNode(type=identifier, text=new_zeros), value='new_ones')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 6306,
        "neg_line": [
            "-node_mask = edge_index.new_zeros(num_nodes, dtype=torch.bool)",
            "-edge_mask = edge_index.new_zeros(edge_index.size(1), dtype=torch.bool)"
        ],
        "pos_line": [
            "+node_mask = edge_index.new_ones(num_nodes, dtype=torch.bool)",
            "+edge_mask = edge_index.new_ones(edge_index.size(1), dtype=torch.bool)"
        ],
        "core_change": "-node_mask = edge_index.new_zeros(num_nodes, dtype=torch.bool) -edge_mask = edge_index.new_zeros(edge_index.size(1), dtype=torch.bool) +node_mask = edge_index.new_ones(num_nodes, dtype=torch.bool) +edge_mask = edge_index.new_ones(edge_index.size(1), dtype=torch.bool)",
        "core_API": "new_zeros"
    },
    {
        "commit_hash": "fc00d58f810553a177400e441b903263db96c1e1",
        "index": "08a91657..aeb19e4d 100644",
        "commit_message": "Fix typo in docstring for `DenseFeatures`\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class DenseFeatures(dense_features.DenseFeatures):",
            "```python",
            "price = tf.feature_column.numeric_column('price')",
            "keywords_embedded = tf.feature_column.embedding_column(",
            "-      tf.feature_column.categorical_column_with_hash_bucket(\"keywords\", 10K),",
            "+      tf.feature_column.categorical_column_with_hash_bucket(\"keywords\", 10000),",
            "dimensions=16)",
            "columns = [price, keywords_embedded, ...]",
            "feature_layer = tf.keras.layers.DenseFeatures(columns)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=integer, text=10), value='10000')",
            "Delete(target_node=ASTNode(type=identifier, text=K))",
            "Delete(target_node=ASTNode(type=ERROR))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 3,
        "number": 6309,
        "neg_line": [
            "-tf.feature_column.categorical_column_with_hash_bucket(\"keywords\", 10K),"
        ],
        "pos_line": [
            "+tf.feature_column.categorical_column_with_hash_bucket(\"keywords\", 10000),"
        ],
        "core_change": "-tf.feature_column.categorical_column_with_hash_bucket(\"keywords\", 10K), +tf.feature_column.categorical_column_with_hash_bucket(\"keywords\", 10000),",
        "core_API": "numeric_column"
    },
    {
        "commit_hash": "8b6bba54a707a718149f11c2b0631e6ccd715fba",
        "index": "f40d5278c..53969ef93 100644",
        "commit_message": "Fix `ViTMSNForImageClassification` doctest (#19275)\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ViTMSNForImageClassification(ViTMSNPreTrainedModel):",
            ">>> from PIL import Image",
            ">>> import requests",
            "",
            "-        >>> torch.manual_seed(2)",
            "+        >>> torch.manual_seed(2)  # doctest: +IGNORE_RESULT",
            "",
            ">>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"",
            ">>> image = Image.open(requests.get(url, stream=True).raw)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 6310,
        "neg_line": [
            "->>> torch.manual_seed(2)"
        ],
        "pos_line": [
            "+>>> torch.manual_seed(2)  # doctest: +IGNORE_RESULT"
        ],
        "core_change": "->>> torch.manual_seed(2) +>>> torch.manual_seed(2)  # doctest: +IGNORE_RESULT",
        "core_API": "manual_seed"
    },
    {
        "commit_hash": "8dfdda49c8644599c563c2f7db74927bcc1932aa",
        "index": "ad38aed66..e13ff4fab 100644",
        "commit_message": "fixing loss (#1420)\n\nCo-authored-by: holly1238 <77758406+holly1238@users.noreply.github.com>\n",
        "file": "tutorials.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "def optimize_model():",
            "expected_state_action_values = (next_state_values * GAMMA) + reward_batch",
            "",
            "# Compute Huber loss",
            "-    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))",
            "+    criterion = nn.SmoothL1Loss()",
            "+    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))",
            "",
            "# Optimize the model",
            "optimizer.zero_grad()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=2, insert_id=1559772)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1559773)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'criterion'), position=0, insert_id=1559774)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1559775)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=1559776)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1559777)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1559778)",
            "Update(target_node=ASTNode(type=identifier, text=F), value='criterion')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=F), position=0)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'nn'), position=0, insert_id=1559779)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1559780)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'SmoothL1Loss'), position=2, insert_id=1559781)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1559782)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1559783)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=smooth_l1_loss))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 6311,
        "neg_line": [
            "-loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))"
        ],
        "pos_line": [
            "+criterion = nn.SmoothL1Loss()",
            "+loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))"
        ],
        "core_change": "-loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1)) +criterion = nn.SmoothL1Loss() +loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))",
        "core_API": "smooth_l1_loss"
    },
    {
        "commit_hash": "fb6a2941b90494c18fdb995e11315c15b5b7738e",
        "index": "8f086240..dfa670a2 100644",
        "commit_message": "Fix typos\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "keras",
        "change": [
            "def MusicTaggerCRNN(weights='msd', input_tensor=None,",
            "else:",
            "# Load weights",
            "if K.image_dim_ordering() == 'tf':",
            "-            raise RuntimeError(\"Please set image_dim_ordering == 'th'.\"",
            "-                               \"You can set it at ~/.keras/keras.json\")",
            "+            raise RuntimeError('Please set `image_dim_ordering` to \"th\".'",
            "+                               'You can set it at `~/.keras/keras.json`.')",
            "",
            "if K._BACKEND == 'theano':",
            "weights_path = get_file('music_tagger_crnn_weights_theano.h5',"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('concatenated_string', None), position=0, insert_id=2542059)",
            "Insert(target_node=IN(type=concatenated_string), node=('string', '\\'Please set `image_dim_ordering` to \"th\".\\''), position=0, insert_id=2542060)",
            "Insert(target_node=IN(type=concatenated_string), node=('string', \"'You can set it at `~/.keras/keras.json`.'\"), position=1, insert_id=2542061)",
            "Delete(target_node=ASTNode(type=string, text=\"Please set image_dim_ordering == 'th'.\"))",
            "Delete(target_node=ASTNode(type=string, text=\"You can set it at ~/.keras/keras.json\"))",
            "Delete(target_node=ASTNode(type=concatenated_string))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 6,
        "number": 6315,
        "neg_line": [
            "-raise RuntimeError(\"Please set image_dim_ordering == 'th'.\"",
            "-\"You can set it at ~/.keras/keras.json\")"
        ],
        "pos_line": [
            "+raise RuntimeError('Please set `image_dim_ordering` to \"th\".'",
            "+'You can set it at `~/.keras/keras.json`.')"
        ],
        "core_change": "-raise RuntimeError(\"Please set image_dim_ordering == 'th'.\" -\"You can set it at ~/.keras/keras.json\") +raise RuntimeError('Please set `image_dim_ordering` to \"th\".' +'You can set it at `~/.keras/keras.json`.')",
        "core_API": "image_dim_ordering"
    },
    {
        "commit_hash": "d215aa258e77f43b6c36950bc9a484d5d849eb4f",
        "index": "54b0f21c..b5eb811c 100644",
        "commit_message": "fix torch.nonzero usage in pytorch 1.5 (#2602)\n\n\n",
        "file": "mmdetection.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class GARPNHead(GuidedAnchorHead):",
            "if cfg.min_bbox_size > 0:",
            "w = proposals[:, 2] - proposals[:, 0]",
            "h = proposals[:, 3] - proposals[:, 1]",
            "-                valid_inds = torch.nonzero((w >= cfg.min_bbox_size) &",
            "-                                           (h >= cfg.min_bbox_size)).squeeze()",
            "+                valid_inds = torch.nonzero(",
            "+                    (w >= cfg.min_bbox_size) & (h >= cfg.min_bbox_size),",
            "+                    as_tuple=False).squeeze()",
            "proposals = proposals[valid_inds, :]",
            "scores = scores[valid_inds]",
            "proposals = torch.cat([proposals, scores.unsqueeze(-1)], dim=-1)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=638783)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=638784)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'as_tuple'), position=0, insert_id=638785)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=638786)",
            "Insert(target_node=IN(type=keyword_argument), node=('false', 'False'), position=2, insert_id=638787)"
        ],
        "plus_line": 3,
        "minus_line": 2,
        "AST_diff_line": 5,
        "number": 6316,
        "neg_line": [
            "-valid_inds = torch.nonzero((w >= cfg.min_bbox_size) &",
            "-(h >= cfg.min_bbox_size)).squeeze()"
        ],
        "pos_line": [
            "+valid_inds = torch.nonzero(",
            "+(w >= cfg.min_bbox_size) & (h >= cfg.min_bbox_size),",
            "+as_tuple=False).squeeze()"
        ],
        "core_change": "-valid_inds = torch.nonzero((w >= cfg.min_bbox_size) & -(h >= cfg.min_bbox_size)).squeeze() +valid_inds = torch.nonzero( +(w >= cfg.min_bbox_size) & (h >= cfg.min_bbox_size), +as_tuple=False).squeeze()",
        "core_API": "nonzero"
    },
    {
        "commit_hash": "d460a531a9d26256cab17538407fe771049712b3",
        "index": "480025c3..107dc2a3 100644",
        "commit_message": "YOLACT (#3456)\n\n* YOLACT (not the final commit, so please don't review)\n\n* Implementation notes for YOLACT\n\n* Modify yolact_r50.py\n\n* Update YOLACT configs\n\n* Add YOLACT README\n\n* Adjust according to the first round of review\n\n* Handle the corner case when gt_mask is empty\n\n* Add unit tests for YOLACT heads\n\n* Add unit tests for YOLACT forward\n\n* Adjust according to Kai's comments, except for gt_mask filtering\n\n* Change use_OHEM to use_ohem in unit tests\n\n* Update config in YOLACT unit tests\n\n* merge bbox overlap and batch bbox overlap\n\n* fix unit test\n\n* refactor some codes and fix some bugs\n\n* fix isort\n\n* fix ci\n\n* update model link\n\n* fix cfg path\n\n* support batch test\n\n* Make get_seg_masks faster\n\n* fix format\n\n* update readme\n\nCo-authored-by: Cao Yuhang <yhcao6@gmail.com>\n",
        "file": "mmdetection.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_max_iou_assigner_with_empty_gt():",
            "[5, 5, 15, 15],",
            "[32, 32, 38, 42],",
            "])",
            "-    gt_bboxes = torch.FloatTensor([])",
            "+    gt_bboxes = torch.empty(0, 4)",
            "assign_result = self.assign(bboxes, gt_bboxes)",
            "",
            "expected_gt_inds = torch.LongTensor([0, 0, 0, 0])"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=FloatTensor), value='empty')",
            "Insert(target_node=ASTNode(type=argument_list), node=('integer', '0'), position=1, insert_id=633997)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=633998)",
            "Insert(target_node=ASTNode(type=argument_list), node=('integer', '4'), position=3, insert_id=633999)",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=list))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 6321,
        "neg_line": [
            "-gt_bboxes = torch.FloatTensor([])"
        ],
        "pos_line": [
            "+gt_bboxes = torch.empty(0, 4)"
        ],
        "core_change": "-gt_bboxes = torch.FloatTensor([]) +gt_bboxes = torch.empty(0, 4)",
        "core_API": "FloatTensor"
    },
    {
        "commit_hash": "4adad1ca217fa5154937946350e0a8d1a05b410d",
        "index": "9975f6b7..71a95abf 100644",
        "commit_message": "Changes I need for the TransformerQA model (#3646)\n\n* Adds a vocab class for transformer vocabularies\n\n* Tix fypo\n\n* Remove special token stuff from vocab\n\n* Include type ids in the output if we have them\n\n* Use correct padding value even if the padding token id isn't 0\n\n* The tokenizer knows about special tokens\n\n* Typo\n\n* Don't complain about too many tensors if we are going to use those tensors\n\n* Support token type ids in the pretrained transformer wrapper\n\n* Adds the threaded generator\n\n* Fixed type annotations for token\n\n* Fix problem introduced with the merge\n\n* Productivity through formatting\n\n* Removes test we no longer need\n\n* Productivity through formatting\n\n* Better documentation\n\n* Add helpful documentation\n\n* Remove unused import\n\n* Fix tests\n\n* Fix another test\n\n* Make mypy happy\n\n* Remove the special token functions\n\nNow that `tokenizer` is public, we don't need them like this.\n\n* Fix flake8\n\n* Fixes the treatment of type_ids in the mismatched embedder and indexer\n\n* Productivity through formatting\n\n* Don't save the padding value anymore\n\n* Adds test for different padding values\n\n* Productivity through formatting\n\n* Brendan doesn't like threaded_generator\n\n* Better comments\n\n* Remove unused imports\n\n* Provide a clear error message when your type ids are too big\n\n* Adds code coverage test\n\n* Mark flaky test as flaky\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def get_text_field_mask(",
            "",
            "TODO(joelgrus): can we change this?",
            "NOTE: Our functions for generating masks create torch.LongTensors, because using",
            "-    torch.ByteTensors  makes it easy to run into overflow errors",
            "+    torch.ByteTensors makes it easy to run into overflow errors",
            "when doing mask manipulation, such as summing to get the lengths of sequences - see below.",
            ">>> mask = torch.ones([260]).byte()",
            ">>> mask.sum() # equals 260."
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 6323,
        "neg_line": [
            "-torch.ByteTensors  makes it easy to run into overflow errors"
        ],
        "pos_line": [
            "+torch.ByteTensors makes it easy to run into overflow errors"
        ],
        "core_change": "-torch.ByteTensors  makes it easy to run into overflow errors +torch.ByteTensors makes it easy to run into overflow errors",
        "core_API": "ones"
    },
    {
        "commit_hash": "758ed3332b219dd3529a1d3639fa30aa4954e0f3",
        "index": "b4953b6e5..3c224555d 100644",
        "commit_message": "Transformers fast import part 2 (#9446)\n\n* Main init work\n\n* Add version\n\n* Change from absolute to relative imports\n\n* Fix imports\n\n* One more typo\n\n* More typos\n\n* Styling\n\n* Make quality script pass\n\n* Add necessary replace in template\n\n* Fix typos\n\n* Spaces are ignored in replace for some reason\n\n* Forgot one models.\n\n* Fixes for import\n\nCo-authored-by: LysandreJik <lysandre.debut@reseau.eseo.fr>\n\n* Add documentation\n\n* Styling\n\nCo-authored-by: LysandreJik <lysandre.debut@reseau.eseo.fr>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "models",
        "change": [
            "class DownloadCommand(BaseTransformersCLICommand):",
            "self._force = force",
            "",
            "def run(self):",
            "-        from transformers import AutoModel, AutoTokenizer",
            "+        from ..models.auto import AutoModel, AutoTokenizer",
            "",
            "AutoModel.from_pretrained(self._model, cache_dir=self._cache, force_download=self._force)",
            "AutoTokenizer.from_pretrained(self._model, cache_dir=self._cache, force_download=self._force)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=import_from_statement), node=('relative_import', None), position=1, insert_id=2688443)",
            "Insert(target_node=IN(type=relative_import), node=('import_prefix', None), position=0, insert_id=2688444)",
            "Move(target_node=IN(type=relative_import), node=ASTNode(type=dotted_name), position=1)",
            "Insert(target_node=IN(type=import_prefix), node=('.', '.'), position=0, insert_id=2688445)",
            "Insert(target_node=IN(type=import_prefix), node=('.', '.'), position=1, insert_id=2688446)",
            "Update(target_node=ASTNode(type=identifier, text=transformers), value='models')",
            "Insert(target_node=ASTNode(type=dotted_name), node=('.', '.'), position=1, insert_id=2688447)",
            "Insert(target_node=ASTNode(type=dotted_name), node=('identifier', 'auto'), position=2, insert_id=2688448)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 6327,
        "neg_line": [
            "-from transformers import AutoModel, AutoTokenizer"
        ],
        "pos_line": [
            "+from ..models.auto import AutoModel, AutoTokenizer"
        ],
        "core_change": "-from transformers import AutoModel, AutoTokenizer +from ..models.auto import AutoModel, AutoTokenizer",
        "core_API": "from_pretrained"
    },
    {
        "commit_hash": "6dae981a53dae0f7264b06d115f2bfa35354a090",
        "index": "2de864ddf..9c0f77af0 100644",
        "commit_message": "fix problem of dataparallel\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "def train(args):",
            "elif ngpu > 1:",
            "gpu_id = range(ngpu)",
            "logging.info('gpu id: ' + str(gpu_id))",
            "-        model = torch.nn.DataParallel(model, device_ids=gpu_id)",
            "+        model = DataParallel(model, device_ids=gpu_id)",
            "model.cuda()",
            "logging.info('batch size is automatically increased (%d -> %d)' % (",
            "args.batch_size, args.batch_size * args.ngpu))"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=DataParallel), position=0)",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=nn))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 6328,
        "neg_line": [
            "-model = torch.nn.DataParallel(model, device_ids=gpu_id)"
        ],
        "pos_line": [
            "+model = DataParallel(model, device_ids=gpu_id)"
        ],
        "core_change": "-model = torch.nn.DataParallel(model, device_ids=gpu_id) +model = DataParallel(model, device_ids=gpu_id)",
        "core_API": "info"
    },
    {
        "commit_hash": "f67892ea963640389c9838eb716336afbb4f3826",
        "index": "963b80079..960860953 100644",
        "commit_message": "CI: yesqa (#8564)\n\n* add yesqa\n* fix flake8\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "autograd",
        "change": [
            "class PyTorchProfiler(BaseProfiler):",
            "# close profiler if it is already opened. might happen if 2 profilers",
            "# are created and the first one did not call `describe`",
            "try:",
            "-                torch.autograd._disable_profiler()  # noqa",
            "+                torch.autograd._disable_profiler()",
            "except (AttributeError, RuntimeError):",
            "pass"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 6333,
        "neg_line": [
            "-torch.autograd._disable_profiler()  # noqa"
        ],
        "pos_line": [
            "+torch.autograd._disable_profiler()"
        ],
        "core_change": "-torch.autograd._disable_profiler()  # noqa +torch.autograd._disable_profiler()",
        "core_API": "_disable_profiler"
    },
    {
        "commit_hash": "98291f0d33b704d1e4d14197278f2996ac8f4b4a",
        "index": "716f9da1..64314c58 100644",
        "commit_message": "[WIP] - Documentation Cleaning and Coding Style (#516)\n\n* Activation Cleaning Docstring Test\n\n* Requirements Pinned with range to insure tested versions are used. Range are used to prevent updating requirements all the time.\n\n* setup.cfg file added with PEP8 configuration\n\n* activation.py refactored\n\n* docstring fixed - ready for documentation unittest\n\n* Yapf correction for max_line_length: 120\n\n* test yapf refactored\n\n* Requirements conflict solved\n\n* Yapf Style modified and merged in file \"setup.cfg\"\n\n* Yapf Confiuguration Updated\n\n* Code Refactored with new YAPF formating style\n\n* Code Refactored with new YAPF formating style\n\n* Code Refactored with new YAPF formating style\n\n* tl.layers.pooling YAPF reformat\n\n* yapf updated\n\n* gitignore updated\n\n* YAPF Style Fixing Attempt\n\n* Space Error Fix\n\n* Style Correction\n\n* Assertion Codacy Errors Corrected\n\n* Error Fix\n\n* Assertion Refactored\n\n* YAPF Style Applied to Master\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tensorflow",
        "change": [
            "class PadLayer(Layer):",
            "self.inputs = prev_layer.outputs",
            "",
            "if padding is None:",
            "-            raise Exception(\"padding should be a Tensor of type int32. see https://www.tensorflow.org/api_docs/python/tf/pad\")",
            "+            raise Exception(",
            "+                \"padding should be a Tensor of type int32. see https://www.tensorflow.org/api_docs/python/tf/pad\"",
            "+            )",
            "",
            "self.outputs = tf.pad(self.inputs, paddings=padding, mode=mode, name=name)",
            "self.all_layers.append(self.outputs)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 6341,
        "neg_line": [
            "-raise Exception(\"padding should be a Tensor of type int32. see https://www.tensorflow.org/api_docs/python/tf/pad\")"
        ],
        "pos_line": [
            "+raise Exception(",
            "+\"padding should be a Tensor of type int32. see https://www.tensorflow.org/api_docs/python/tf/pad\"",
            "+)"
        ],
        "core_change": "-raise Exception(\"padding should be a Tensor of type int32. see https://www.tensorflow.org/api_docs/python/tf/pad\") +raise Exception( +\"padding should be a Tensor of type int32. see https://www.tensorflow.org/api_docs/python/tf/pad\" +)",
        "core_API": "pad"
    },
    {
        "commit_hash": "ad39271ae8518afaa4c4831bd1ddccc3bc86eab6",
        "index": "3d35d2c4a..2137b0018 100644",
        "commit_message": "Fix FP16 and attention masks in FunnelTransformer (#7374)\n\n* Fix #7371\n\n* Fix training\n\n* Fix test values\n\n* Apply the fix to TF as well\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFFunnelRelMultiheadAttention(tf.keras.layers.Layer):",
            "attn_score = tf.cast(attn_score, tf.float32)",
            "# perform masking",
            "if attention_mask is not None:",
            "-            attn_score = attn_score - INF * tf.cast(attention_mask[:, None, None], tf.float32)",
            "+            attn_score = attn_score - INF * (1 - tf.cast(attention_mask[:, None, None], tf.float32))",
            "# attention probability",
            "attn_prob = tf.nn.softmax(attn_score, axis=-1)",
            "if dtype != tf.float32:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=binary_operator), node=('parenthesized_expression', None), position=2, insert_id=2380001)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('(', '('), position=0, insert_id=2380002)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('binary_operator', None), position=1, insert_id=2380003)",
            "Insert(target_node=IN(type=parenthesized_expression), node=(')', ')'), position=2, insert_id=2380004)",
            "Insert(target_node=IN(type=binary_operator), node=('integer', '1'), position=0, insert_id=2380005)",
            "Insert(target_node=IN(type=binary_operator), node=('-', '-'), position=1, insert_id=2380006)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=call), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 6347,
        "neg_line": [
            "-attn_score = attn_score - INF * tf.cast(attention_mask[:, None, None], tf.float32)"
        ],
        "pos_line": [
            "+attn_score = attn_score - INF * (1 - tf.cast(attention_mask[:, None, None], tf.float32))"
        ],
        "core_change": "-attn_score = attn_score - INF * tf.cast(attention_mask[:, None, None], tf.float32) +attn_score = attn_score - INF * (1 - tf.cast(attention_mask[:, None, None], tf.float32))",
        "core_API": "cast"
    },
    {
        "commit_hash": "f6521366df57f8469850f38164629bb9d5fc8978",
        "index": "194920c4..cbc4dab2 100644",
        "commit_message": "Fix fail test (#1896)\n\n* fix OOM\n\n* Union is not supported still\n\n\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestRandomEqualizeAlternative(CommonTests):",
            "",
            "with pytest.raises(ValueError):",
            "self._create_augmentation_from_params(p=1.0)(",
            "-                torch.ones((1, 3, 4, 5) * 200, device=self.device, dtype=self.dtype)",
            "+                torch.ones((1, 3, 4, 5) * 3, device=self.device, dtype=self.dtype)",
            ")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=integer, text=200), value='3')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 6348,
        "neg_line": [
            "-torch.ones((1, 3, 4, 5) * 200, device=self.device, dtype=self.dtype)"
        ],
        "pos_line": [
            "+torch.ones((1, 3, 4, 5) * 3, device=self.device, dtype=self.dtype)"
        ],
        "core_change": "-torch.ones((1, 3, 4, 5) * 200, device=self.device, dtype=self.dtype) +torch.ones((1, 3, 4, 5) * 3, device=self.device, dtype=self.dtype)",
        "core_API": "raises"
    },
    {
        "commit_hash": "679d9a79c072c04b5ac15bc9b64285f6e881fccc",
        "index": "4cdd1c3a..d0a64608 100644",
        "commit_message": "unittest bug fix\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Layer_Convolution_1D_Test(CustomTestCase):",
            "cls.input_layer = Input(cls.inputs_shape, name='input_layer')",
            "",
            "cls.dense = tl.layers.Dense(",
            "-            n_units=100, act=tf.nn.relu, in_channels=200, name='dense'",
            "+            n_units=100, act=tf.nn.relu, in_channels=200",
            ")(cls.input_layer)",
            "",
            "cls.noiselayer = tl.layers.GaussianNoise("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=name))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=string, text='dense'))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 6351,
        "neg_line": [
            "-n_units=100, act=tf.nn.relu, in_channels=200, name='dense'"
        ],
        "pos_line": [
            "+n_units=100, act=tf.nn.relu, in_channels=200"
        ],
        "core_change": "-n_units=100, act=tf.nn.relu, in_channels=200, name='dense' +n_units=100, act=tf.nn.relu, in_channels=200",
        "core_API": "Dense"
    },
    {
        "commit_hash": "d215aa258e77f43b6c36950bc9a484d5d849eb4f",
        "index": "b563e9a4..b47ad3dd 100644",
        "commit_message": "fix torch.nonzero usage in pytorch 1.5 (#2602)\n\n\n",
        "file": "mmdetection.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class InstanceBalancedPosSampler(RandomSampler):",
            "num_per_gt = int(round(num_expected / float(num_gts)) + 1)",
            "sampled_inds = []",
            "for i in unique_gt_inds:",
            "-                inds = torch.nonzero(assign_result.gt_inds == i.item())",
            "+                inds = torch.nonzero(",
            "+                    assign_result.gt_inds == i.item(), as_tuple=False)",
            "if inds.numel() != 0:",
            "inds = inds.squeeze(1)",
            "else:"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=638720)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=638721)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'as_tuple'), position=0, insert_id=638722)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=638723)",
            "Insert(target_node=IN(type=keyword_argument), node=('false', 'False'), position=2, insert_id=638724)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 6352,
        "neg_line": [
            "-inds = torch.nonzero(assign_result.gt_inds == i.item())"
        ],
        "pos_line": [
            "+inds = torch.nonzero(",
            "+assign_result.gt_inds == i.item(), as_tuple=False)"
        ],
        "core_change": "-inds = torch.nonzero(assign_result.gt_inds == i.item()) +inds = torch.nonzero( +assign_result.gt_inds == i.item(), as_tuple=False)",
        "core_API": "nonzero"
    },
    {
        "commit_hash": "c4c532d25e012dbe6ab1ac14bca75e53e0acc621",
        "index": "a83775c9..93ee61ca 100644",
        "commit_message": "pylint -> flake8 (#3288)\n\n* pylint\n\n* update pylint\n\n* undo a lot of the raise / else\n\n* add bound on typed-ast\n\n* first mypy fixes\n\n* new flag\n\n* fix mypy errors\n\n* requirements.txt\n\n* pylint -> flake8\n\n* mypy 0.720 -> mypy 0.730\n\n* add back erroneously removed initial newline\n\n* remove .pylintrc\n\n* remove pylintrc from Dockerfile\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class BagOfEmbeddingsEncoder(Seq2VecEncoder):",
            "def get_output_dim(self) -> int:",
            "return self._embedding_dim",
            "",
            "-    def forward(self, tokens: torch.Tensor, mask: torch.Tensor = None):  #pylint: disable=arguments-differ",
            "+    def forward(self, tokens: torch.Tensor, mask: torch.Tensor = None):",
            "if mask is not None:",
            "tokens = tokens * mask.unsqueeze(-1).float()"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 6355,
        "neg_line": [
            "-def forward(self, tokens: torch.Tensor, mask: torch.Tensor = None):  #pylint: disable=arguments-differ"
        ],
        "pos_line": [
            "+def forward(self, tokens: torch.Tensor, mask: torch.Tensor = None):"
        ],
        "core_change": "-def forward(self, tokens: torch.Tensor, mask: torch.Tensor = None):  #pylint: disable=arguments-differ +def forward(self, tokens: torch.Tensor, mask: torch.Tensor = None):",
        "core_API": "unsqueeze"
    },
    {
        "commit_hash": "542e00f270b9aa2fa78056e758954e3d4c365325",
        "index": "2489a711..47b83e89 100755",
        "commit_message": "small fix\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "with tf.Graph().as_default() as G:",
            "init = sessinit.ParamRestore(np.load(args.model).item())",
            "else:",
            "init = sessinit.SaverRestore(args.model)",
            "-    sess = tf.Session()",
            "+    sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))",
            "init.init(sess)",
            "",
            "# dump ..."
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=1, insert_id=2312545)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=2, insert_id=2312546)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'config'), position=0, insert_id=2312547)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2312548)",
            "Insert(target_node=IN(type=keyword_argument), node=('call', None), position=2, insert_id=2312549)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2312550)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2312551)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2312552)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2312553)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'ConfigProto'), position=2, insert_id=2312554)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2312555)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=1, insert_id=2312556)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=), text=)), position=2)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'allow_soft_placement'), position=0, insert_id=2312557)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2312558)",
            "Insert(target_node=IN(type=keyword_argument), node=('true', 'True'), position=2, insert_id=2312559)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 16,
        "number": 6356,
        "neg_line": [
            "-sess = tf.Session()"
        ],
        "pos_line": [
            "+sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))"
        ],
        "core_change": "-sess = tf.Session() +sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))",
        "core_API": "Graph"
    },
    {
        "commit_hash": "b34be039f9e5e0ca48166f14b849eac089949c68",
        "index": "4b4c90d0..9898bb7e 100644",
        "commit_message": "Karras VE, DDIM and DDPM flax schedulers (#508)\n\n* beta never changes removed from state\n\n* fix typos in docs\n\n* removed unused var\n\n* initial ddim flax scheduler\n\n* import\n\n* added dummy objects\n\n* fix style\n\n* fix typo\n\n* docs\n\n* fix typo in comment\n\n* set return type\n\n* added flax ddom\n\n* fix style\n\n* remake\n\n* pass PRNG key as argument and split before use\n\n* fix doc string\n\n* use config\n\n* added flax Karras VE scheduler\n\n* make style\n\n* fix dummy\n\n* fix ndarray type annotation\n\n* replace returns a new state\n\n* added lms_discrete scheduler\n\n* use self.config\n\n* add_noise needs state\n\n* use config\n\n* use config\n\n* docstring\n\n* added flax score sde ve\n\n* fix imports\n\n* fix typos\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class KarrasVeScheduler(SchedulerMixin, ConfigMixin):",
            "",
            "TODO Args:",
            "\"\"\"",
            "-        if self.s_min <= sigma <= self.s_max:",
            "-            gamma = min(self.s_churn / self.num_inference_steps, 2**0.5 - 1)",
            "+        if self.config.s_min <= sigma <= self.config.s_max:",
            "+            gamma = min(self.config.s_churn / self.num_inference_steps, 2**0.5 - 1)",
            "else:",
            "gamma = 0",
            "",
            "# sample eps ~ N(0, S_noise^2 * I)",
            "-        eps = self.s_noise * torch.randn(sample.shape, generator=generator).to(sample.device)",
            "+        eps = self.config.s_noise * torch.randn(sample.shape, generator=generator).to(sample.device)",
            "sigma_hat = sigma + gamma * sigma",
            "sample_hat = sample + ((sigma_hat**2 - sigma**2) ** 0.5 * eps)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=104603)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=104604)",
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=104605)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=104606)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=self), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'config'), position=2, insert_id=104607)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=self), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'config'), position=2, insert_id=104608)",
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=104609)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=104610)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=self), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'config'), position=2, insert_id=104611)",
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=104612)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=104613)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=self), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'config'), position=2, insert_id=104614)"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 20,
        "number": 6363,
        "neg_line": [
            "-if self.s_min <= sigma <= self.s_max:",
            "-gamma = min(self.s_churn / self.num_inference_steps, 2**0.5 - 1)",
            "-eps = self.s_noise * torch.randn(sample.shape, generator=generator).to(sample.device)"
        ],
        "pos_line": [
            "+if self.config.s_min <= sigma <= self.config.s_max:",
            "+gamma = min(self.config.s_churn / self.num_inference_steps, 2**0.5 - 1)",
            "+eps = self.config.s_noise * torch.randn(sample.shape, generator=generator).to(sample.device)"
        ],
        "core_change": "-if self.s_min <= sigma <= self.s_max: -gamma = min(self.s_churn / self.num_inference_steps, 2**0.5 - 1) +if self.config.s_min <= sigma <= self.config.s_max: +gamma = min(self.config.s_churn / self.num_inference_steps, 2**0.5 - 1) -eps = self.s_noise * torch.randn(sample.shape, generator=generator).to(sample.device) +eps = self.config.s_noise * torch.randn(sample.shape, generator=generator).to(sample.device)",
        "core_API": "randn"
    },
    {
        "commit_hash": "a43b1e94eeb552ad1d0353591695ee08448bff71",
        "index": "3cc1a47..a4a6e3c 100644",
        "commit_message": "Stability improvements (#99)\n\n* bugfix & torch fx implementation\n\n* update torch tensorrt installation\n\n* fix sparseml colab\n\n* fix sparseml training on gpu\n\n* restored lost fixes\n\n* fixed torch_tensorrt install\n\n* fix quantization\n\n* improved tests & bugfix\n\n* prevent deepsparse installation for arm cpu\n\n* bugfix tvm & improved tests\n\n* added tests for tensorflow and onnx\n\n* fix test torchscript\n\n* fix tensorrt static quant\n\n* update notebooks\n\n* fix deepsparse bugs and implemented tests\n\n* add test for sparseml compressor\n\n* bug fixes on tensorflow backend and added tests\n\n* add version limit to tensorflow due to protobuffers 2.x not being supported from tf2onnx\n\n* update onnx version\n\n* remove numpy update\n\n* restored cpu tests\n\n* add python 3.10 in cpu tests\n\n* limit tensorflow gpu usage\n\n* fix python 3.10\n\n* improved tests\n\n* add warmup in model latency computation & add original model latency\n\n* fix pytorch tensorrt for transformers models\n\n* fixed bugs on onnx model handling\n\n* added onnx simplifier to fix tensorrt in onnx pipeline\n\n* fix deepsparse support to NO_COMPILER_INSTALLATION flag\n\n* fix model to onnx conversion problem and tensorrt issue with static quantization\n\n* add valerio citation code\n\n* added readme to notebooks folder\n\n* added tensorflow and onnx notebooks\n\n* style fix\n\n* fix tensor RT bug with static quantization when using new version of polygraphy and update pytorch resnet50 notebook\n\n* fix huggingface bug when passing tokenizer to optimize_model\n\n* updated notebooks readme and bugfix\n\n* minor fixes & added pruning with intel neural compressor\n\n* fixes and added test for intel pruning\n\n* fixes & added neural compressor quantization\n\n* fix test intel pruning compressor\n\n* added tests for neural compressor optimizer & bug fixes\n\n* removed transformers from requirements\n\n* changed openvino dynamic shape\n\n* computing latency using different data\n\n* bugfix openvino\n\n* make onnxsim optional\n\n* install onnx_sim only on intel machines\n\n* added bf16 and dynamic_quantization to neural_compressor\n\n* check output of compiled models\n\n* internal fixes\n\n* fix when no optimized model is found\n\nCo-authored-by: Valerio Sofi <v.sofi@nebuly.ai>\n",
        "file": "nebullvm.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def create_model_inputs_tf(",
            "def run_tf_model(",
            "model: tf.Module, input_tensors: Tuple[tf.Tensor]",
            ") -> Tuple[tf.Tensor]:",
            "-    pred = model.predict(*input_tensors)",
            "+    pred = model.predict(input_tensors)",
            "if isinstance(pred, tf.Module) and pred is not None:",
            "pred = (pred,)",
            "return pred"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=input_tensors), position=1)",
            "Delete(target_node=ASTNode(type=*, text=*))",
            "Delete(target_node=ASTNode(type=list_splat))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 3,
        "number": 6365,
        "neg_line": [
            "-pred = model.predict(*input_tensors)"
        ],
        "pos_line": [
            "+pred = model.predict(input_tensors)"
        ],
        "core_change": "-pred = model.predict(*input_tensors) +pred = model.predict(input_tensors)",
        "core_API": "predict"
    },
    {
        "commit_hash": "aa50652aa82b746eaff271f5bfd9cccaeaa8bd82",
        "index": "23b7fae3e..37b128158 100644",
        "commit_message": "Fix dropout\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class Decoder(torch.nn.Module):",
            "self.decoder = torch.nn.ModuleList()",
            "self.dropout_dec = torch.nn.ModuleList()",
            "self.decoder += [torch.nn.LSTMCell(dunits + eprojs, dunits)]",
            "+        self.dropout_dec += [torch.nn.Dropout(p=dropout)]",
            "for _ in six.moves.range(1, self.dlayers):",
            "self.decoder += [torch.nn.LSTMCell(dunits, dunits)]",
            "self.dropout_dec += [torch.nn.Dropout(p=dropout)]"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=4, insert_id=1343373)",
            "Insert(target_node=IN(type=expression_statement), node=('augmented_assignment', None), position=0, insert_id=1343374)",
            "Insert(target_node=IN(type=augmented_assignment), node=('attribute', None), position=0, insert_id=1343375)",
            "Insert(target_node=IN(type=augmented_assignment), node=('+=', '+='), position=1, insert_id=1343376)",
            "Insert(target_node=IN(type=augmented_assignment), node=('list', None), position=2, insert_id=1343377)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=1343378)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1343379)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dropout_dec'), position=2, insert_id=1343380)",
            "Insert(target_node=IN(type=list), node=('[', '['), position=0, insert_id=1343381)",
            "Insert(target_node=IN(type=list), node=('call', None), position=1, insert_id=1343382)",
            "Insert(target_node=IN(type=list), node=(']', ']'), position=2, insert_id=1343383)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1343384)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1343385)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1343386)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1343387)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Dropout'), position=2, insert_id=1343388)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1343389)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=1, insert_id=1343390)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1343391)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1343392)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1343393)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'nn'), position=2, insert_id=1343394)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'p'), position=0, insert_id=1343395)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1343396)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dropout'), position=2, insert_id=1343397)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 25,
        "number": 6366,
        "neg_line": [],
        "pos_line": [
            "+self.dropout_dec += [torch.nn.Dropout(p=dropout)]"
        ],
        "core_change": "+self.dropout_dec += [torch.nn.Dropout(p=dropout)]",
        "core_API": "ModuleList"
    },
    {
        "commit_hash": "8a4d81d496be62c58b0bfa125c3f8b6e5e6eff0b",
        "index": "f13c808f3b..2dba94755e 100644",
        "commit_message": "lintfixbot: Auto-commit fixed lint errors in codebase\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def xlogy(",
            "return torch.xlogy(x, y, out=out)",
            "",
            "",
            "-def real(x: Union[torch.Tensor],",
            "-         /,",
            "-         *,",
            "-         out: Optional[torch.Tensor] = None) -> torch.Tensor:",
            "+def real(",
            "+    x: Union[torch.Tensor], /, *, out: Optional[torch.Tensor] = None",
            "+) -> torch.Tensor:",
            "return torch.real(x)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=parameters), node=('ERROR', None), position=1, insert_id=274462)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=identifier, text=return), position=0)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=identifier, text=torch), position=1)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=., text=.), position=2)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=identifier, text=xlogy), position=3)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=tuple_pattern), position=4)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=identifier, text=def), position=5)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=identifier, text=real), position=6)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=(, text=(), position=7)",
            "Insert(target_node=IN(type=ERROR), node=('ERROR', None), position=8, insert_id=274463)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=subscript), position=9)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=,, text=,), position=10)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=identifier, text=x), position=0)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=:, text=:), position=1)",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=type))",
            "Delete(target_node=ASTNode(type=typed_parameter))"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 17,
        "number": 6370,
        "neg_line": [
            "-def real(x: Union[torch.Tensor],",
            "-/,",
            "-*,",
            "-out: Optional[torch.Tensor] = None) -> torch.Tensor:"
        ],
        "pos_line": [
            "+def real(",
            "+x: Union[torch.Tensor], /, *, out: Optional[torch.Tensor] = None",
            "+) -> torch.Tensor:"
        ],
        "core_change": "-def real(x: Union[torch.Tensor], -/, -*, -out: Optional[torch.Tensor] = None) -> torch.Tensor: +def real( +x: Union[torch.Tensor], /, *, out: Optional[torch.Tensor] = None +) -> torch.Tensor:",
        "core_API": "xlogy"
    },
    {
        "commit_hash": "867a5efa3523fd2bd2fb7705e293f80f4bb15299",
        "index": "7a2d0670..1d193b5d 100644",
        "commit_message": "Release 1.8.6rc0 (#660)\n\n* Release 1.8.6rc0\n\n* Update CHANGELOG.md\n\n* Typo Fix\n\n* Changelog Updated\n\n* tl_logging more readable\n\n* Typo Fixed\n\n* https enforced for badges\n\n* RTD badges pointing to Latest Commit\n\n* Decorators API Refactored\n\n* extra_requires `all`, `all_cpu` and `all_gpu` added\n\n* Error fix\n\n* YAPF Formating Correction\n\n* Test for private method decorator added\n\n* Test Logging Verbosity Fixed to Debug when runned individually\n\n* YAPF corrections applied\n\n* Changelog Added\n\n* Changelog updated\n\n* PR number changed\n\n* Changelog updated\n\n* Update .travis.yml\n\n* Update CONTRIBUTING.md\n\n* Update CONTRIBUTING.md\n\n* Update CHANGELOG.md\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Layer_DeformableConvolution_Test(unittest.TestCase):",
            "",
            "",
            "if __name__ == '__main__':",
            "-    # tl.logging.set_verbosity(tl.logging.INFO)",
            "+",
            "+    tf.logging.set_verbosity(tf.logging.DEBUG)",
            "tl.logging.set_verbosity(tl.logging.DEBUG)",
            "",
            "unittest.main()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('block', None), position=0, insert_id=2262372)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=2262373)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=2262374)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2262375)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2262376)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2262377)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2262378)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'set_verbosity'), position=2, insert_id=2262379)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2262380)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=2262381)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2262382)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2262383)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2262384)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'logging'), position=2, insert_id=2262385)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2262386)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2262387)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'DEBUG'), position=2, insert_id=2262388)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2262389)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2262390)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'logging'), position=2, insert_id=2262391)",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 21,
        "number": 6372,
        "neg_line": [
            "-# tl.logging.set_verbosity(tl.logging.INFO)"
        ],
        "pos_line": [
            "+",
            "+tf.logging.set_verbosity(tf.logging.DEBUG)"
        ],
        "core_change": "-# tl.logging.set_verbosity(tl.logging.INFO) + +tf.logging.set_verbosity(tf.logging.DEBUG)",
        "core_API": "set_verbosity"
    },
    {
        "commit_hash": "d3c3e722d69627d6334d7ef8faaced7df3103174",
        "index": "23e550f51..e763a63d6 100755",
        "commit_message": "[FLAX] Minor fixes in CLM example (#12914)\n\n* readme: fix retrieval of vocab size for flax clm example\n\n* examples: fix flax clm example when using training/evaluation files\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "def main():",
            "extension = \"text\"",
            "dataset = load_dataset(extension, data_files=data_files, cache_dir=model_args.cache_dir)",
            "",
            "-        if \"validation\" not in datasets.keys():",
            "-            datasets[\"validation\"] = load_dataset(",
            "+        if \"validation\" not in dataset.keys():",
            "+            dataset[\"validation\"] = load_dataset(",
            "extension,",
            "data_files=data_files,",
            "split=f\"train[:{data_args.validation_split_percentage}%]\",",
            "cache_dir=model_args.cache_dir,",
            ")",
            "-            datasets[\"train\"] = load_dataset(",
            "+            dataset[\"train\"] = load_dataset(",
            "extension,",
            "data_files=data_files,",
            "split=f\"train[{data_args.validation_split_percentage}%:]\","
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=datasets), value='dataset')",
            "Update(target_node=ASTNode(type=identifier, text=datasets), value='dataset')",
            "Update(target_node=ASTNode(type=identifier, text=datasets), value='dataset')"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 3,
        "number": 6374,
        "neg_line": [
            "-if \"validation\" not in datasets.keys():",
            "-datasets[\"validation\"] = load_dataset(",
            "-datasets[\"train\"] = load_dataset("
        ],
        "pos_line": [
            "+if \"validation\" not in dataset.keys():",
            "+dataset[\"validation\"] = load_dataset(",
            "+dataset[\"train\"] = load_dataset("
        ],
        "core_change": "-if \"validation\" not in datasets.keys(): -datasets[\"validation\"] = load_dataset( +if \"validation\" not in dataset.keys(): +dataset[\"validation\"] = load_dataset( -datasets[\"train\"] = load_dataset( +dataset[\"train\"] = load_dataset(",
        "core_API": "keys"
    },
    {
        "commit_hash": "8525c548f29d466253cdb5d4b07d80b99477dc2c",
        "index": "84dfaa6d..752345b3 100644",
        "commit_message": "Added container operation weights and OneOf documentation (#1443)\n\n* Added OneOf documentation\n\n* Added random apply weights\n\n* Updated docs\n\n* Added tests\n\n* doctest fix\n\n* doctest fix\n\n* Apply suggestions from code review\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class VideoSequential(ImageSequential):",
            "data_format: str = \"BTCHW\",",
            "same_on_frame: bool = True,",
            "random_apply: Union[int, bool, Tuple[int, int]] = False,",
            "+        random_apply_weights: Optional[List[float]] = None,",
            ") -> None:",
            "-        super().__init__(*args, same_on_batch=None, return_transform=None, keepdim=None, random_apply=random_apply)",
            "+        super().__init__(",
            "+            *args, same_on_batch=None, return_transform=None, keepdim=None, random_apply=random_apply,",
            "+            random_apply_weights=random_apply_weights",
            "+        )",
            "self.same_on_frame = same_on_frame",
            "self.data_format = data_format.upper()",
            "if self.data_format not in [\"BCTHW\", \"BTCHW\"]:"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=4, insert_id=412153)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=412154)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'random_apply_weights'), position=0, insert_id=412155)",
            "Insert(target_node=IN(type=assignment), node=(':', ':'), position=1, insert_id=412156)",
            "Insert(target_node=IN(type=assignment), node=('type', None), position=2, insert_id=412157)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=3, insert_id=412158)",
            "Insert(target_node=IN(type=assignment), node=('expression_list', None), position=4, insert_id=412159)",
            "Insert(target_node=IN(type=type), node=('subscript', None), position=0, insert_id=412160)",
            "Insert(target_node=IN(type=expression_list), node=('none', 'None'), position=0, insert_id=412161)",
            "Insert(target_node=IN(type=expression_list), node=(',', ','), position=1, insert_id=412162)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=10, insert_id=412163)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=11, insert_id=412164)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'Optional'), position=0, insert_id=412165)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=412166)",
            "Insert(target_node=IN(type=subscript), node=('subscript', None), position=2, insert_id=412167)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=412168)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'random_apply_weights'), position=0, insert_id=412169)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=412170)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'random_apply_weights'), position=2, insert_id=412171)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'List'), position=0, insert_id=412172)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=412173)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'float'), position=2, insert_id=412174)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=412175)"
        ],
        "plus_line": 5,
        "minus_line": 1,
        "AST_diff_line": 23,
        "number": 6376,
        "neg_line": [
            "-super().__init__(*args, same_on_batch=None, return_transform=None, keepdim=None, random_apply=random_apply)"
        ],
        "pos_line": [
            "+random_apply_weights: Optional[List[float]] = None,",
            "+super().__init__(",
            "+*args, same_on_batch=None, return_transform=None, keepdim=None, random_apply=random_apply,",
            "+random_apply_weights=random_apply_weights",
            "+)"
        ],
        "core_change": "+random_apply_weights: Optional[List[float]] = None, -super().__init__(*args, same_on_batch=None, return_transform=None, keepdim=None, random_apply=random_apply) +super().__init__( +*args, same_on_batch=None, return_transform=None, keepdim=None, random_apply=random_apply, +random_apply_weights=random_apply_weights +)",
        "core_API": "upper"
    },
    {
        "commit_hash": "33adab2b91697b3e78af618a21ab9f1176281165",
        "index": "9bd99b25d..eff54f71e 100644",
        "commit_message": "Fix albert example\n\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "models",
        "change": [
            "class PreTrainedModel(nn.Module):",
            "model = BertModel.from_pretrained('./tf_model/my_tf_checkpoint.ckpt.index', from_tf=True, config=config)",
            "",
            "\"\"\"",
            "-        if pretrained_model_name_or_path is not None and (",
            "-                \"albert\" in pretrained_model_name_or_path and \"v2\" in pretrained_model_name_or_path):",
            "-            logger.warning(\"There is currently an upstream reproducibility issue with ALBERT v2 models. Please see \" +",
            "-                           \"https://github.com/google-research/google-research/issues/119 for more information.\")",
            "-",
            "config = kwargs.pop('config', None)",
            "state_dict = kwargs.pop('state_dict', None)",
            "cache_dir = kwargs.pop('cache_dir', None)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=), text=)))"
        ],
        "plus_line": 0,
        "minus_line": 4,
        "AST_diff_line": 1,
        "number": 6380,
        "neg_line": [
            "-if pretrained_model_name_or_path is not None and (",
            "-\"albert\" in pretrained_model_name_or_path and \"v2\" in pretrained_model_name_or_path):",
            "-logger.warning(\"There is currently an upstream reproducibility issue with ALBERT v2 models. Please see \" +",
            "-\"https://github.com/google-research/google-research/issues/119 for more information.\")",
            "-"
        ],
        "pos_line": [],
        "core_change": "-if pretrained_model_name_or_path is not None and ( -\"albert\" in pretrained_model_name_or_path and \"v2\" in pretrained_model_name_or_path): -logger.warning(\"There is currently an upstream reproducibility issue with ALBERT v2 models. Please see \" + -\"https://github.com/google-research/google-research/issues/119 for more information.\") -",
        "core_API": "from_pretrained"
    },
    {
        "commit_hash": "50703c2637fa7f82e8a8367270d5d7c37755baef",
        "index": "bc3fcac..a2e714a 100644",
        "commit_message": "Fix some dimensions\n\n",
        "file": "gpt-neo.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def model(X, params, mesh, labels=None, past=None, scope='model', reuse=False, t",
            "# wpe has shape [ctx, embd]",
            "# positions_for would have shape [batch, seq]",
            "# h has shape [batch, seq, embd]",
            "-        zerodim = mtf.Dimension('singleton', 0)",
            "",
            "-        h = mtf.gather(wte, X, zerodim) + mtf.gather(wpe, positions_for(X, past_length, batch_dim), zerodim)",
            "+        h = mtf.gather(wte, X, vocab_dim) + mtf.gather(wpe, positions_for(X, past_length, batch_dim), vocab_dim)",
            "",
            "# Transformer",
            "presents = []"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=zerodim), value='h')",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=mtf), position=0)",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=zerodim), value='vocab_dim')",
            "Update(target_node=ASTNode(type=identifier, text=zerodim), value='vocab_dim')",
            "Delete(target_node=ASTNode(type=identifier, text=Dimension))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=string, text='singleton'))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=integer, text=0))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=identifier, text=h))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=mtf))",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=., text=.))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 19,
        "number": 6382,
        "neg_line": [
            "-zerodim = mtf.Dimension('singleton', 0)",
            "-h = mtf.gather(wte, X, zerodim) + mtf.gather(wpe, positions_for(X, past_length, batch_dim), zerodim)"
        ],
        "pos_line": [
            "+h = mtf.gather(wte, X, vocab_dim) + mtf.gather(wpe, positions_for(X, past_length, batch_dim), vocab_dim)"
        ],
        "core_change": "-zerodim = mtf.Dimension('singleton', 0) -h = mtf.gather(wte, X, zerodim) + mtf.gather(wpe, positions_for(X, past_length, batch_dim), zerodim) +h = mtf.gather(wte, X, vocab_dim) + mtf.gather(wpe, positions_for(X, past_length, batch_dim), vocab_dim)",
        "core_API": "Dimension"
    },
    {
        "commit_hash": "698a0e0f1ed95d20116dc51aa9c6a7ed48446deb",
        "index": "7a9f008a..954de224 100644",
        "commit_message": "Adding new export for Triton (#2078)\n\n* Adding new export for triton.  Fixes for load model for neuropod export, add output dict format\n\n* Adding test for triton.  Fix to cast int to string for os.path.join.  Added annotation for neurpod\n\n* Minor tweaks to config.pbtxt output\n\n* Remove logger that is not being used\n\n* Restrict torchmetrics<0.9 and whylogs<1.0 until compatibility fixed\n\n* Update export_triton to return model path, and added docstrings\n\n* Update api to return both model path and config path\n\nCo-authored-by: Travis Addair <tgaddair@gmail.com>\n",
        "file": "ludwig.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def _get_output_dicts(config: Dict[str, Any]) -> str:",
            "results = []",
            "for feature in config[\"output_features\"]:",
            "name = feature[NAME]",
            "-        results.append(\"{\" + f'\"{name}\": results[\"{name}\"][\"predictions\"]' + \"}\")",
            "-    return \", \".join(results)",
            "+        results.append(f'\"{name}\": results[\"{name}\"][\"predictions\"]')",
            "+    return \"{\" + \", \".join(results) + \"}\"",
            "",
            "",
            "def generate_neuropod_torchscript(model: LudwigModel):"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=return_statement), node=('binary_operator', None), position=1, insert_id=603234)",
            "Insert(target_node=IN(type=binary_operator), node=('binary_operator', None), position=0, insert_id=603235)",
            "Insert(target_node=IN(type=binary_operator), node=('+', '+'), position=1, insert_id=603236)",
            "Insert(target_node=IN(type=binary_operator), node=('string', '\"}\"'), position=2, insert_id=603237)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=string, text=f'\"{name}\": results[\"{name}\"][\"predictions\"]'), position=1)",
            "Insert(target_node=IN(type=binary_operator), node=('string', '\"{\"'), position=0, insert_id=603238)",
            "Insert(target_node=IN(type=binary_operator), node=('+', '+'), position=1, insert_id=603239)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=call), position=2)",
            "Delete(target_node=ASTNode(type=string, text=\"{\"))",
            "Delete(target_node=ASTNode(type=+, text=+))",
            "Delete(target_node=ASTNode(type=binary_operator))",
            "Delete(target_node=ASTNode(type=+, text=+))",
            "Delete(target_node=ASTNode(type=string, text=\"}\"))",
            "Delete(target_node=ASTNode(type=binary_operator))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 14,
        "number": 6384,
        "neg_line": [
            "-results.append(\"{\" + f'\"{name}\": results[\"{name}\"][\"predictions\"]' + \"}\")",
            "-return \", \".join(results)"
        ],
        "pos_line": [
            "+results.append(f'\"{name}\": results[\"{name}\"][\"predictions\"]')",
            "+return \"{\" + \", \".join(results) + \"}\""
        ],
        "core_change": "-results.append(\"{\" + f'\"{name}\": results[\"{name}\"][\"predictions\"]' + \"}\") -return \", \".join(results) +results.append(f'\"{name}\": results[\"{name}\"][\"predictions\"]') +return \"{\" + \", \".join(results) + \"}\"",
        "core_API": "append"
    },
    {
        "commit_hash": "50cfdd03b296df0951a0c463cc7b208a0e2982e6",
        "index": "2cdbf30..70845d9 100644",
        "commit_message": "fix issues for pytorch<1.9.0.\n\n",
        "file": "CodeFormer.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Detect(nn.Module):",
            "",
            "@staticmethod",
            "def _make_grid(nx=20, ny=20):",
            "-        yv, xv = torch.meshgrid([torch.arange(ny), torch.arange(nx)], indexing=\"ij\")",
            "+        # yv, xv = torch.meshgrid([torch.arange(ny), torch.arange(nx)], indexing=\"ij\") # for pytorch>=1.10",
            "+        yv, xv = torch.meshgrid([torch.arange(ny), torch.arange(nx)])",
            "return torch.stack((xv, yv), 2).view((1, 1, ny, nx, 2)).float()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=indexing))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=string, text=\"ij\"))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 6386,
        "neg_line": [
            "-yv, xv = torch.meshgrid([torch.arange(ny), torch.arange(nx)], indexing=\"ij\")"
        ],
        "pos_line": [
            "+# yv, xv = torch.meshgrid([torch.arange(ny), torch.arange(nx)], indexing=\"ij\") # for pytorch>=1.10",
            "+yv, xv = torch.meshgrid([torch.arange(ny), torch.arange(nx)])"
        ],
        "core_change": "-yv, xv = torch.meshgrid([torch.arange(ny), torch.arange(nx)], indexing=\"ij\") +# yv, xv = torch.meshgrid([torch.arange(ny), torch.arange(nx)], indexing=\"ij\") # for pytorch>=1.10 +yv, xv = torch.meshgrid([torch.arange(ny), torch.arange(nx)])",
        "core_API": "meshgrid"
    },
    {
        "commit_hash": "a3eb618014a93390b2506d53ebbba11574ba98d3",
        "index": "9b65e16f..7d0e2dc6 100644",
        "commit_message": "fix mypy\n\n",
        "file": "flair.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class CRF(torch.nn.Module):",
            "self.transitions.detach()[:, tag_dictionary.get_idx_for_item(STOP_TAG)] = -10000",
            "self.to(flair.device)",
            "",
            "-    def forward(self, features: torch.tensor) -> torch.tensor:",
            "+    def forward(self, features: torch.Tensor) -> torch.Tensor:",
            "\"\"\"",
            "Forward propagation of Conditional Random Field.",
            ":param features: output from RNN / Linear layer in shape (batch size, seq len, hidden size)",
            ":return: CRF scores (emission scores for each token + transitions prob from previous state) in",
            "shape (batch_size, seq len, tagset size, tagset size)",
            "\"\"\"",
            "-        batch_size = features.size(0)",
            "-        seq_len = features.size(1)",
            "+        batch_size, seq_len, _, _ = features.size()",
            "",
            "emission_scores = features",
            "emission_scores = emission_scores.unsqueeze(-1).expand(batch_size, seq_len, self.tagset_size, self.tagset_size)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('pattern_list', None), position=0, insert_id=234717)",
            "Update(target_node=ASTNode(type=identifier, text=tensor), value='Tensor')",
            "Move(target_node=IN(type=pattern_list), node=ASTNode(type=identifier, text=batch_size), position=0)",
            "Insert(target_node=IN(type=pattern_list), node=(',', ','), position=1, insert_id=234718)",
            "Insert(target_node=IN(type=pattern_list), node=('identifier', 'seq_len'), position=2, insert_id=234719)",
            "Insert(target_node=IN(type=pattern_list), node=(',', ','), position=3, insert_id=234720)",
            "Insert(target_node=IN(type=pattern_list), node=('identifier', '_'), position=4, insert_id=234721)",
            "Insert(target_node=IN(type=pattern_list), node=(',', ','), position=5, insert_id=234722)",
            "Insert(target_node=IN(type=pattern_list), node=('identifier', '_'), position=6, insert_id=234723)",
            "Update(target_node=ASTNode(type=identifier, text=tensor), value='Tensor')",
            "Delete(target_node=ASTNode(type=integer, text=0))",
            "Delete(target_node=ASTNode(type=identifier, text=seq_len))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=features))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=size))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 2,
        "minus_line": 3,
        "AST_diff_line": 24,
        "number": 6389,
        "neg_line": [
            "-def forward(self, features: torch.tensor) -> torch.tensor:",
            "-batch_size = features.size(0)",
            "-seq_len = features.size(1)"
        ],
        "pos_line": [
            "+def forward(self, features: torch.Tensor) -> torch.Tensor:",
            "+batch_size, seq_len, _, _ = features.size()"
        ],
        "core_change": "-def forward(self, features: torch.tensor) -> torch.tensor: +def forward(self, features: torch.Tensor) -> torch.Tensor: -batch_size = features.size(0) -seq_len = features.size(1) +batch_size, seq_len, _, _ = features.size()",
        "core_API": "detach"
    },
    {
        "commit_hash": "2452492f35e5122b675e77108385d9853411900c",
        "index": "f2c69e0a..2a7eca1d 100644",
        "commit_message": "fix batched_index_select (#2765)\n\n* fix batched_index_select\n\n* fix batched_index_select\n\n* add unit test\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def flatten_and_batch_shift_indices(indices: torch.Tensor,",
            "offset_indices : ``torch.LongTensor``",
            "\"\"\"",
            "# Shape: (batch_size)",
            "+    if torch.max(indices) >= sequence_length or torch.min(indices) < 0:",
            "+        raise ConfigurationError(f\"All elements in indices should be in range (0, {sequence_length - 1})\")",
            "offsets = get_range_vector(indices.size(0), get_device_of(indices)) * sequence_length",
            "for _ in range(len(indices.size()) - 1):",
            "offsets = offsets.unsqueeze(1)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=ERROR), node=(')', ')'), position=4, insert_id=26382)"
        ],
        "plus_line": 2,
        "minus_line": 0,
        "AST_diff_line": 1,
        "number": 6394,
        "neg_line": [],
        "pos_line": [
            "+if torch.max(indices) >= sequence_length or torch.min(indices) < 0:",
            "+raise ConfigurationError(f\"All elements in indices should be in range (0, {sequence_length - 1})\")"
        ],
        "core_change": "+if torch.max(indices) >= sequence_length or torch.min(indices) < 0: +raise ConfigurationError(f\"All elements in indices should be in range (0, {sequence_length - 1})\")",
        "core_API": "max"
    },
    {
        "commit_hash": "8c4c4d7f046a5191fa22067aaf236f7e47a1c2e7",
        "index": "3a7f59a1..fdfe19ee 100644",
        "commit_message": "TRPO working, other fixes and improvements\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Beta(Distribution):",
            "",
            "sampled = beta_sample / tf.maximum(x=(alpha_sample + beta_sample), y=epsilon)",
            "",
            "-        sampled = tf.where(condition=(temperature < epsilon), x=definite, y=sampled)",
            "+        action = tf.where(condition=(temperature < epsilon), x=definite, y=sampled)",
            "",
            "min_value = tf_util.constant(value=self.action_spec.min_value, dtype='float')",
            "max_value = tf_util.constant(value=self.action_spec.max_value, dtype='float')",
            "",
            "with tf.control_dependencies(control_inputs=dependencies):",
            "-            return min_value + (max_value - min_value) * sampled",
            "+            return min_value + (max_value - min_value) * action",
            "",
            "@tf_function(num_args=2)",
            "def log_probability(self, *, parameters, action):"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=sampled), value='action')",
            "Update(target_node=ASTNode(type=identifier, text=sampled), value='action')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 6400,
        "neg_line": [
            "-sampled = tf.where(condition=(temperature < epsilon), x=definite, y=sampled)",
            "-return min_value + (max_value - min_value) * sampled"
        ],
        "pos_line": [
            "+action = tf.where(condition=(temperature < epsilon), x=definite, y=sampled)",
            "+return min_value + (max_value - min_value) * action"
        ],
        "core_change": "-sampled = tf.where(condition=(temperature < epsilon), x=definite, y=sampled) +action = tf.where(condition=(temperature < epsilon), x=definite, y=sampled) -return min_value + (max_value - min_value) * sampled +return min_value + (max_value - min_value) * action",
        "core_API": "maximum"
    },
    {
        "commit_hash": "a7dabfb3d1d0c3fbbb710933c2edd9d044b72977",
        "index": "c1c65a5e4..50c792fbc 100644",
        "commit_message": "Fix TF s2s models (#9478)\n\n* Fix Seq2Seq models for serving\n\n* Apply style\n\n* Fix lonfgormer\n\n* Fix mBart/Pegasus/Blenderbot\n\n* Apply style\n\n* Add a main intermediate layer\n\n* Apply style\n\n* Remove import\n\n* Apply tf.function to Longformer\n\n* Fix utils check_copy\n\n* Update S2S template\n\n* Fix BART + Blenderbot\n\n* Fix BlenderbotSmall\n\n* Fix BlenderbotSmall\n\n* Fix BlenderbotSmall\n\n* Fix MBart\n\n* Fix Marian\n\n* Fix Pegasus + template\n\n* Apply style\n\n* Fix common attributes test\n\n* Forgot to fix the LED test\n\n* Apply Patrick's comment on LED Decoder\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFT5ForConditionalGeneration(TFT5PreTrainedModel, TFCausalLanguageModeling",
            ")",
            "",
            "def serving_output(self, output):",
            "-        pkv = (tf.convert_to_tensor(output.past_key_values[1:]) if self.config.use_cache else None,)",
            "+        pkv = tf.convert_to_tensor(output.past_key_values[1:]) if self.config.use_cache else None",
            "dec_hs = tf.convert_to_tensor(output.decoder_hidden_states) if self.config.output_hidden_states else None",
            "dec_attns = tf.convert_to_tensor(output.decoder_attentions) if self.config.output_attentions else None",
            "enc_hs = tf.convert_to_tensor(output.encoder_hidden_states) if self.config.output_hidden_states else None"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=conditional_expression), position=2)",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=tuple))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 6401,
        "neg_line": [
            "-pkv = (tf.convert_to_tensor(output.past_key_values[1:]) if self.config.use_cache else None,)"
        ],
        "pos_line": [
            "+pkv = tf.convert_to_tensor(output.past_key_values[1:]) if self.config.use_cache else None"
        ],
        "core_change": "-pkv = (tf.convert_to_tensor(output.past_key_values[1:]) if self.config.use_cache else None,) +pkv = tf.convert_to_tensor(output.past_key_values[1:]) if self.config.use_cache else None",
        "core_API": "convert_to_tensor"
    },
    {
        "commit_hash": "ee8c795db99651eb59defb20a82c84d41b1939b0",
        "index": "9227f0dba2..fbdc992ac8 100644",
        "commit_message": "fix diff of torch backend so not casting None to tensor\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def diff(",
            "append: Optional[Union[torch.Tensor, int, float, list, tuple]] = None,",
            ") -> torch.Tensor:",
            "x = x if type(x) == torch.Tensor else torch.Tensor(x)",
            "-    prepend = prepend if type(prepend) == torch.Tensor else torch.Tensor(prepend)",
            "-    append = append if type(append) == torch.Tensor else torch.Tensor(append)",
            "+    prepend = prepend if type(prepend) == torch.Tensor or prepend == None else torch.Tensor(prepend)",
            "+    append = append if type(append) == torch.Tensor or append == None else torch.Tensor(append)",
            "return torch.diff(x, n=n, dim=axis, prepend=prepend, append=append)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=conditional_expression), node=('boolean_operator', None), position=2, insert_id=279482)",
            "Insert(target_node=ASTNode(type=conditional_expression), node=('boolean_operator', None), position=2, insert_id=279483)",
            "Move(target_node=IN(type=boolean_operator), node=ASTNode(type=comparison_operator), position=0)",
            "Insert(target_node=IN(type=boolean_operator), node=('or', 'or'), position=1, insert_id=279484)",
            "Insert(target_node=IN(type=boolean_operator), node=('comparison_operator', None), position=2, insert_id=279485)",
            "Move(target_node=IN(type=boolean_operator), node=ASTNode(type=comparison_operator), position=0)",
            "Insert(target_node=IN(type=boolean_operator), node=('or', 'or'), position=1, insert_id=279486)",
            "Insert(target_node=IN(type=boolean_operator), node=('comparison_operator', None), position=2, insert_id=279487)",
            "Insert(target_node=IN(type=comparison_operator), node=('identifier', 'prepend'), position=0, insert_id=279488)",
            "Insert(target_node=IN(type=comparison_operator), node=('==', '=='), position=1, insert_id=279489)",
            "Insert(target_node=IN(type=comparison_operator), node=('none', 'None'), position=2, insert_id=279490)",
            "Insert(target_node=IN(type=comparison_operator), node=('identifier', 'append'), position=0, insert_id=279491)",
            "Insert(target_node=IN(type=comparison_operator), node=('==', '=='), position=1, insert_id=279492)",
            "Insert(target_node=IN(type=comparison_operator), node=('none', 'None'), position=2, insert_id=279493)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 14,
        "number": 6402,
        "neg_line": [
            "-prepend = prepend if type(prepend) == torch.Tensor else torch.Tensor(prepend)",
            "-append = append if type(append) == torch.Tensor else torch.Tensor(append)"
        ],
        "pos_line": [
            "+prepend = prepend if type(prepend) == torch.Tensor or prepend == None else torch.Tensor(prepend)",
            "+append = append if type(append) == torch.Tensor or append == None else torch.Tensor(append)"
        ],
        "core_change": "-prepend = prepend if type(prepend) == torch.Tensor else torch.Tensor(prepend) -append = append if type(append) == torch.Tensor else torch.Tensor(append) +prepend = prepend if type(prepend) == torch.Tensor or prepend == None else torch.Tensor(prepend) +append = append if type(append) == torch.Tensor or append == None else torch.Tensor(append)",
        "core_API": "Tensor"
    },
    {
        "commit_hash": "081136341de620f83de1d3eb2524c159c3e46b4c",
        "index": "4354ab0e..4526f5f6 100644",
        "commit_message": "Fixed set explanations (#3160)\n\n\n",
        "file": "ludwig.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SetInputFeature(SetFeatureMixin, InputFeature):",
            "",
            "def forward(self, inputs):",
            "assert isinstance(inputs, torch.Tensor)",
            "-        assert inputs.dtype in [torch.bool, torch.int64]",
            "+        assert inputs.dtype in [torch.bool, torch.int64, torch.float32]",
            "",
            "encoder_output = self.encoder_obj(inputs)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=list), node=(',', ','), position=4, insert_id=595191)",
            "Insert(target_node=ASTNode(type=list), node=('attribute', None), position=5, insert_id=595192)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=595193)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=595194)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'float32'), position=2, insert_id=595195)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 6404,
        "neg_line": [
            "-assert inputs.dtype in [torch.bool, torch.int64]"
        ],
        "pos_line": [
            "+assert inputs.dtype in [torch.bool, torch.int64, torch.float32]"
        ],
        "core_change": "-assert inputs.dtype in [torch.bool, torch.int64] +assert inputs.dtype in [torch.bool, torch.int64, torch.float32]",
        "core_API": "encoder_obj"
    },
    {
        "commit_hash": "c51dc4f92755c67a83f3fc8a0bd6b3e64df199e4",
        "index": "21305de73..acd50d2be 100755",
        "commit_message": "[torch] remove deprecated uint8 in favor of bool (#21384)\n\n* uint8 -> bool\n\n* fix copies\n\n* style\n\n* update test modeling commen when checking attention buffers\n\n* style\n\n* use logical not on random mask instead of subtraction with 1\n\n* remove torch uint8\n\n* quality\n\n* remove modified modeling utils\n\n* Update based on review\n\nCo-authored-by: sgugger <sylvain.gugger@gmail.com>\n\n---------\n\nCo-authored-by: sgugger <sylvain.gugger@gmail.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ImageGPTAttention(nn.Module):",
            "if not self.is_cross_attention:",
            "# if only \"normal\" attention layer implements causal mask",
            "query_length, key_length = query.size(-2), key.size(-2)",
            "-            causal_mask = self.bias[:, :, key_length - query_length : key_length, :key_length].bool()",
            "+            causal_mask = self.bias[:, :, key_length - query_length : key_length, :key_length]",
            "mask_value = torch.finfo(attn_weights.dtype).min",
            "# Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.",
            "# Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=subscript), position=2)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=bool))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 6405,
        "neg_line": [
            "-causal_mask = self.bias[:, :, key_length - query_length : key_length, :key_length].bool()"
        ],
        "pos_line": [
            "+causal_mask = self.bias[:, :, key_length - query_length : key_length, :key_length]"
        ],
        "core_change": "-causal_mask = self.bias[:, :, key_length - query_length : key_length, :key_length].bool() +causal_mask = self.bias[:, :, key_length - query_length : key_length, :key_length]",
        "core_API": "size"
    },
    {
        "commit_hash": "184e0dabab151acd04be3e93c7321a90e10037ed",
        "index": "133487b456..6ef0331611 100644",
        "commit_message": "fix issue with torch sum where keepdims wasnt working when no axis is given\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def sum(",
            "return x.type(dtype)",
            "axis = tuple(axis) if isinstance(axis, list) else axis",
            "if axis is None:",
            "-        return torch.sum(input=x, dtype=dtype)",
            "+        return torch.sum(input=x, dim=(), dtype=dtype, keepdim=keepdims)",
            "return torch.sum(input=x, dim=axis, dtype=dtype, keepdim=keepdims)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=273302)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=273303)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=6, insert_id=273304)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=7, insert_id=273305)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dim'), position=0, insert_id=273306)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=273307)",
            "Insert(target_node=IN(type=keyword_argument), node=('tuple', None), position=2, insert_id=273308)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'keepdim'), position=0, insert_id=273309)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=273310)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'keepdims'), position=2, insert_id=273311)",
            "Insert(target_node=IN(type=tuple), node=('(', '('), position=0, insert_id=273312)",
            "Insert(target_node=IN(type=tuple), node=(')', ')'), position=1, insert_id=273313)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 12,
        "number": 6408,
        "neg_line": [
            "-return torch.sum(input=x, dtype=dtype)"
        ],
        "pos_line": [
            "+return torch.sum(input=x, dim=(), dtype=dtype, keepdim=keepdims)"
        ],
        "core_change": "-return torch.sum(input=x, dtype=dtype) +return torch.sum(input=x, dim=(), dtype=dtype, keepdim=keepdims)",
        "core_API": "type"
    },
    {
        "commit_hash": "95b6d985ffb231d9a2047c039c95660913063b9d",
        "index": "c93749b4..f346c84f 100644",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class EntropyTest(AllenNlpTestCase):",
            "logits = torch.tensor(",
            "[[1, 1, 1, 1], [10000, -10000, -10000, -1000]], dtype=torch.float, device=device",
            ")",
            "-        mask = torch.tensor([0, 1], device=device)",
            "+        mask = torch.BoolTensor([False, True], device=device)",
            "metric(logits, mask)",
            "assert metric.get_metric() == 0.0"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tensor), value='BoolTensor')",
            "Insert(target_node=ASTNode(type=list), node=('false', 'False'), position=1, insert_id=20063)",
            "Insert(target_node=ASTNode(type=list), node=('true', 'True'), position=4, insert_id=20064)",
            "Delete(target_node=ASTNode(type=integer, text=0))",
            "Delete(target_node=ASTNode(type=integer, text=1))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 6410,
        "neg_line": [
            "-mask = torch.tensor([0, 1], device=device)"
        ],
        "pos_line": [
            "+mask = torch.BoolTensor([False, True], device=device)"
        ],
        "core_change": "-mask = torch.tensor([0, 1], device=device) +mask = torch.BoolTensor([False, True], device=device)",
        "core_API": "tensor"
    },
    {
        "commit_hash": "da2f1325014a158be36d62bdd20293ef97e574f4",
        "index": "b694aceb..b651043b 100644",
        "commit_message": "enable black in the precommit (#1777)\n\n* enable black in the precommit\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add some fixes\n\n* update libface detection url\n\n* added url from kornia checkpoint\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class OriNet(nn.Module):",
            "# use torch.hub to load pretrained model",
            "if pretrained:",
            "storage_fcn: Callable = lambda storage, loc: storage",
            "-            pretrained_dict = torch.hub.load_state_dict_from_url(",
            "-                urls['orinet'], map_location=storage_fcn",
            "-            )",
            "+            pretrained_dict = torch.hub.load_state_dict_from_url(urls['orinet'], map_location=storage_fcn)",
            "self.load_state_dict(pretrained_dict['state_dict'], strict=False)",
            "self.eval()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 17,
        "number": 6412,
        "neg_line": [
            "-pretrained_dict = torch.hub.load_state_dict_from_url(",
            "-urls['orinet'], map_location=storage_fcn",
            "-)"
        ],
        "pos_line": [
            "+pretrained_dict = torch.hub.load_state_dict_from_url(urls['orinet'], map_location=storage_fcn)"
        ],
        "core_change": "-pretrained_dict = torch.hub.load_state_dict_from_url( -urls['orinet'], map_location=storage_fcn -) +pretrained_dict = torch.hub.load_state_dict_from_url(urls['orinet'], map_location=storage_fcn)",
        "core_API": "load_state_dict_from_url"
    },
    {
        "commit_hash": "06cd849538d9fdf8797e31e211871f4018cb3592",
        "index": "bd3d312a8..7c6d429a9 100644",
        "commit_message": "Allow loading checkpoints from urls (#1667)\n\n* allow loading checkpoints from urls\n\n* tmpdir_server fixture\n\n* test cases for loading checkpoints from url\n\n* dir => root_dir\n\n* default map_location to None\n\n* test case for resume_from_checkpoint\n\n* changelog\n\n* doc update\n\n* monkeypatch TORCH_HOME to avoid caching\n\n* Use a threading server with random ports so that it is easier to clean up\n\n* test fixes\n\n* pep8 fix\n\n* ThreadingHTTPServer support in 3.6\n\n* pep8 fix\n\n* fix changelog\n\n* separate tests for urls\n\n* typo\n\nCo-authored-by: Peter Yu <2057325+yukw777@users.noreply.github.com>\n\n* Apply suggestions from code review\n\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TrainerIOMixin(ABC):",
            "#     checkpoint = torch.load(checkpoint_path)",
            "# else:",
            "# load on CPU first",
            "-        checkpoint = torch.load(checkpoint_path, map_location=lambda storage, loc: storage)",
            "+        checkpoint = pl_load(checkpoint_path, map_location=lambda storage, loc: storage)",
            "",
            "# load model state",
            "model = self.get_model()"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=torch), value='pl_load')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=torch), position=0)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=load))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 6413,
        "neg_line": [
            "-checkpoint = torch.load(checkpoint_path, map_location=lambda storage, loc: storage)"
        ],
        "pos_line": [
            "+checkpoint = pl_load(checkpoint_path, map_location=lambda storage, loc: storage)"
        ],
        "core_change": "-checkpoint = torch.load(checkpoint_path, map_location=lambda storage, loc: storage) +checkpoint = pl_load(checkpoint_path, map_location=lambda storage, loc: storage)",
        "core_API": "load"
    },
    {
        "commit_hash": "d66f11c3782bf9e76d1f18fce5074ffe6673c3d2",
        "index": "9c3b0f3c..19345f79 100644",
        "commit_message": "Fix remaining GPU tests (#1603)\n\n\n",
        "file": "ludwig.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def run_api_experiment(input_features, output_features, dataset, **kwargs):",
            "loaded_state = loaded_model.model.state_dict()",
            "bcast_state = hvd.broadcast_object(loaded_state)",
            "for loaded, bcast in zip(loaded_state.values(), bcast_state.values()):",
            "-            assert np.allclose(loaded, bcast)",
            "+            assert torch.allclose(loaded, bcast)",
            "finally:",
            "if output_dir:",
            "shutil.rmtree(output_dir, ignore_errors=True)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=np), value='torch')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 6415,
        "neg_line": [
            "-assert np.allclose(loaded, bcast)"
        ],
        "pos_line": [
            "+assert torch.allclose(loaded, bcast)"
        ],
        "core_change": "-assert np.allclose(loaded, bcast) +assert torch.allclose(loaded, bcast)",
        "core_API": "state_dict"
    },
    {
        "commit_hash": "8595376da52c360bfbc3b3785c9863382350854d",
        "index": "bab35820..47059cee 100644",
        "commit_message": "Remove Re-definition found for builtin input function - Update tests (#2255)\n\n* change input to sample in test/feature folder\n\n* change input to sample in test/filters folder\n\n* change input to sample in test/morphology folder\n\n* change input to sample in test/geometry/subpix/test_spatial_softargmax.py\n\n* change input to sample in test/geometry/transfrom in test_imgwarp3d.py and test_pyramid.py\n\n* change input to input_org in test/geometry/transfrom in test_imgwarp.py\n\n* change input to inpt in test/grad_estimator/test_ste.py\n\n* change input to inpt and sample in test/enhance/ folder\n\n* change input to input_tensor in test/augmentation/test_augmentation_3d.py\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nCo-authored-by: oleksadobush <oleksandra.stasiuk@ucu.edu.ua>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestLaplacian(BaseTester):",
            "kernel_size = 3",
            "",
            "# evaluate function gradient",
            "-        input = torch.rand(batch_shape, device=device)",
            "-        input = tensor_to_gradcheck_var(input)",
            "-        self.gradcheck(laplacian, (input, kernel_size))",
            "+        sample = torch.rand(batch_shape, device=device)",
            "+        sample = tensor_to_gradcheck_var(sample)",
            "+        self.gradcheck(laplacian, (sample, kernel_size))",
            "",
            "def test_module(self, device, dtype):",
            "params = [3]"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=input), value='sample')",
            "Update(target_node=ASTNode(type=identifier, text=input), value='sample')",
            "Update(target_node=ASTNode(type=identifier, text=input), value='sample')",
            "Update(target_node=ASTNode(type=identifier, text=input), value='sample')"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 4,
        "number": 6421,
        "neg_line": [
            "-input = torch.rand(batch_shape, device=device)",
            "-input = tensor_to_gradcheck_var(input)",
            "-self.gradcheck(laplacian, (input, kernel_size))"
        ],
        "pos_line": [
            "+sample = torch.rand(batch_shape, device=device)",
            "+sample = tensor_to_gradcheck_var(sample)",
            "+self.gradcheck(laplacian, (sample, kernel_size))"
        ],
        "core_change": "-input = torch.rand(batch_shape, device=device) -input = tensor_to_gradcheck_var(input) -self.gradcheck(laplacian, (input, kernel_size)) +sample = torch.rand(batch_shape, device=device) +sample = tensor_to_gradcheck_var(sample) +self.gradcheck(laplacian, (sample, kernel_size))",
        "core_API": "rand"
    },
    {
        "commit_hash": "5ce464f1b16783a5d99ca824a81fab34281afc45",
        "index": "48a7bf19..49e2d692 100644",
        "commit_message": "Fix interpolation comments.\n\nPiperOrigin-RevId: 329882604\n\n",
        "file": "tf-quant-finance.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def interpolate(x_values,",
            "# dx = x_data[indices + 1] - x_data[indices]",
            "# dy = y_data[indices + 1] - y_data[indices]",
            "# indices is a tensor with different values per row/spline",
            "-    # Hence use a selection matrix with gather_nd",
            "def get_slice(x, encoding):",
            "if optimize_for_tpu:",
            "return tf.math.reduce_sum(tf.expand_dims(x, axis=-2) * encoding,"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 6423,
        "neg_line": [
            "-# Hence use a selection matrix with gather_nd"
        ],
        "pos_line": [],
        "core_change": "-# Hence use a selection matrix with gather_nd",
        "core_API": "reduce_sum"
    },
    {
        "commit_hash": "16f4cc9ff08f38debd0086a0a489b3e16e208c30",
        "index": "d5217c895..7abf987d5 100644",
        "commit_message": "Shubhamagarwal92 master (#1349)\n\n* SA: for #958: set torch cuda device when finding root\n\n* SA: for #958: removing root gpu hack in trainer/evaluation_loop\n\n* SA: setting torch cuda device\n\n* comment line too long\n\n* check if root gpu exists or available\n\n* Incorporating suggestions on #1094\n\n* since root gpu returns none instead of -1 for cpu\n\n* undo changes\n\n* fixed dp memory thing\n\nCo-authored-by: Shubham Agarwal <shubhamagarwal92@gmail.com>\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "cuda",
        "change": [
            "class TrainerDPMixin(ABC):",
            "if isinstance(device_ids, int):",
            "device_ids = list(range(device_ids))",
            "",
            "+        # set dp device",
            "+        torch.cuda.set_device(self.root_gpu)",
            "+",
            "model = LightningDataParallel(model, device_ids=device_ids)",
            "",
            "self.run_pretrain_routine(model)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=1849340)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=1849341)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1849342)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1849343)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1849344)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1849345)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'set_device'), position=2, insert_id=1849346)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1849347)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=1849348)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1849349)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1849350)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1849351)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cuda'), position=2, insert_id=1849352)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=1849353)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1849354)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'root_gpu'), position=2, insert_id=1849355)"
        ],
        "plus_line": 2,
        "minus_line": 0,
        "AST_diff_line": 16,
        "number": 6428,
        "neg_line": [],
        "pos_line": [
            "+# set dp device",
            "+torch.cuda.set_device(self.root_gpu)",
            "+"
        ],
        "core_change": "+# set dp device +torch.cuda.set_device(self.root_gpu) +",
        "core_API": "set_device"
    },
    {
        "commit_hash": "e49dca6efdb32b094fa8f7db89e5943aa64f13c8",
        "index": "7616d31..a60395a 100644",
        "commit_message": "fixing it to work properly in multi-node environment\n\n",
        "file": "apex.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "bn_opt = optim.SGD(bn.parameters(), lr=1.0)",
            "for group_num in (range(args.world_size//args.group_size)):",
            "group_ids = range(group_num*args.group_size, (group_num+1)*args.group_size)",
            "cur_group = torch.distributed.new_group(ranks=group_ids)",
            "-   if (args.local_rank//args.group_size == group_num):",
            "+   if (torch.distributed.get_rank()//args.group_size == group_num):",
            "group = cur_group",
            "",
            "sbn = apex.parallel.SyncBatchNorm(feature_size, process_group=group).cuda()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=binary_operator), node=('call', None), position=0, insert_id=55779)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=55780)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=55781)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=55782)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'get_rank'), position=2, insert_id=55783)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=55784)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=55785)",
            "Update(target_node=ASTNode(type=identifier, text=args), value='torch')",
            "Update(target_node=ASTNode(type=identifier, text=local_rank), value='distributed')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 6437,
        "neg_line": [
            "-if (args.local_rank//args.group_size == group_num):"
        ],
        "pos_line": [
            "+if (torch.distributed.get_rank()//args.group_size == group_num):"
        ],
        "core_change": "-if (args.local_rank//args.group_size == group_num): +if (torch.distributed.get_rank()//args.group_size == group_num):",
        "core_API": "SGD"
    },
    {
        "commit_hash": "ce8f42b12ecd522e3ef7251e0d5f5175075a3fb4",
        "index": "c98d263a..a84888f9 100644",
        "commit_message": "Fixes to enable dev branch to move to pytorch-1.2 (#2001)\n\n* Fixes to enable dev branch to move to pytorch-1.2\n\n* change name for published cpu wheels\n\n* fix failing unit tests\n\n* change docs conf\n\n* change tracking example\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "ARGS = {",
            "dist.Geometric: [0.5],",
            "dist.Independent: [dist.Normal(torch.zeros(2), torch.ones(2)), 1],",
            "dist.LowRankMultivariateNormal: [torch.zeros(2), torch.ones(2, 2), torch.ones(2)],",
            "-    dist.MaskedMixture: [torch.tensor([1, 0]).byte(), dist.Normal(0, 1), dist.Normal(0, 2)],",
            "+    dist.MaskedMixture: [torch.tensor([1, 0]).bool(), dist.Normal(0, 1), dist.Normal(0, 2)],",
            "dist.MixtureOfDiagNormals: [torch.ones(2, 3), torch.ones(2, 3), torch.ones(2)],",
            "dist.MixtureOfDiagNormalsSharedCovariance: [torch.ones(2, 3), torch.ones(3), torch.ones(2)],",
            "dist.Multinomial: [2, torch.ones(2)],"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=byte), value='bool')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 6438,
        "neg_line": [
            "-dist.MaskedMixture: [torch.tensor([1, 0]).byte(), dist.Normal(0, 1), dist.Normal(0, 2)],"
        ],
        "pos_line": [
            "+dist.MaskedMixture: [torch.tensor([1, 0]).bool(), dist.Normal(0, 1), dist.Normal(0, 2)],"
        ],
        "core_change": "-dist.MaskedMixture: [torch.tensor([1, 0]).byte(), dist.Normal(0, 1), dist.Normal(0, 2)], +dist.MaskedMixture: [torch.tensor([1, 0]).bool(), dist.Normal(0, 1), dist.Normal(0, 2)],",
        "core_API": "Normal"
    },
    {
        "commit_hash": "c05ace221339102abb1c8b0933b9cbfd51ddd4a3",
        "index": "b28c95a40..bc2fbfe36 100644",
        "commit_message": "Use f-strings in the dataset scripts (#3291)\n\n* Finishes #3257\n\nUsed f-strings to format the .py files in the dataset folder\n\n* Fix style\n\n* Fix hkcancor dataset\n\nCo-authored-by: Mario Šaško <mario@huggingface.co>\nCo-authored-by: mariosasko <mariosasko777@gmail.com>\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class RecipeNlg(datasets.GeneratorBasedBuilder):",
            "path_to_manual_file = os.path.abspath(os.path.expanduser(dl_manager.manual_dir))",
            "if not os.path.exists(path_to_manual_file):",
            "raise FileNotFoundError(",
            "-                \"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('recipe_nlg', data_dir=...)` that includes file name {}. Manual download instructions: {}\".format(",
            "-                    path_to_manual_file,",
            "-                    _FILENAME,",
            "-                    self.manual_download_instructions,",
            "-                )",
            "+                f\"{path_to_manual_file} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('recipe_nlg', data_dir=...)` that includes file name {_FILENAME}. Manual download instructions: {self.manual_download_instructions}\"",
            ")",
            "return [",
            "datasets.SplitGenerator("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('string', 'f\"{path_to_manual_file} does not exist. Make sure you insert a manual dir via `datasets.load_dataset(\\'recipe_nlg\\', data_dir=...)` that includes file name {_FILENAME}. Manual download instructions: {self.manual_download_instructions}\"'), position=1, insert_id=1781637)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=), text=)), position=2)",
            "Delete(target_node=ASTNode(type=string, text=\"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('recipe_nlg', data_dir=...)` that includes file name {}. Manual download instructions: {}\"))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=format))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=path_to_manual_file))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=_FILENAME))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=manual_download_instructions))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=), text=)))"
        ],
        "plus_line": 1,
        "minus_line": 5,
        "AST_diff_line": 19,
        "number": 6439,
        "neg_line": [
            "-\"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('recipe_nlg', data_dir=...)` that includes file name {}. Manual download instructions: {}\".format(",
            "-path_to_manual_file,",
            "-_FILENAME,",
            "-self.manual_download_instructions,",
            "-)"
        ],
        "pos_line": [
            "+f\"{path_to_manual_file} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('recipe_nlg', data_dir=...)` that includes file name {_FILENAME}. Manual download instructions: {self.manual_download_instructions}\""
        ],
        "core_change": "-\"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('recipe_nlg', data_dir=...)` that includes file name {}. Manual download instructions: {}\".format( -path_to_manual_file, -_FILENAME, -self.manual_download_instructions, -) +f\"{path_to_manual_file} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('recipe_nlg', data_dir=...)` that includes file name {_FILENAME}. Manual download instructions: {self.manual_download_instructions}\"",
        "core_API": "abspath"
    },
    {
        "commit_hash": "42054bc5633a4bef7886f780b368fca63960cf94",
        "index": "33486418..8278681d 100755",
        "commit_message": "fixed error in replay memory/retrieve episodes, added preliminary ddpg agent without target networks\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Replay(Queue):",
            "random_episode_indices = tf.random_uniform(shape=(n,), maxval=(self.episode_count + 1), dtype=tf.int32)",
            "starts = tf.gather(params=self.episode_indices, indices=random_episode_indices) + 1",
            "limits = tf.gather(params=self.episode_indices, indices=(random_episode_indices + 1))",
            "-        limits += tf.where(condition=(starts < limits), x=0, y=self.capacity)",
            "+        limits += tf.where(condition=(starts < limits), x=(0,), y=self.capacity)",
            "episodes = [tf.range(start=starts[n], limit=limits[n]) for k in range(n)]",
            "indices = tf.concat(values=episodes, axis=0) % self.capacity",
            "return self.retrieve_indices(indices=indices)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=keyword_argument), node=('tuple', None), position=2, insert_id=2236833)",
            "Insert(target_node=IN(type=tuple), node=('(', '('), position=0, insert_id=2236834)",
            "Move(target_node=IN(type=tuple), node=ASTNode(type=integer, text=0), position=1)",
            "Insert(target_node=IN(type=tuple), node=(',', ','), position=2, insert_id=2236835)",
            "Insert(target_node=IN(type=tuple), node=(')', ')'), position=3, insert_id=2236836)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 6446,
        "neg_line": [
            "-limits += tf.where(condition=(starts < limits), x=0, y=self.capacity)"
        ],
        "pos_line": [
            "+limits += tf.where(condition=(starts < limits), x=(0,), y=self.capacity)"
        ],
        "core_change": "-limits += tf.where(condition=(starts < limits), x=0, y=self.capacity) +limits += tf.where(condition=(starts < limits), x=(0,), y=self.capacity)",
        "core_API": "random_uniform"
    },
    {
        "commit_hash": "86cff17e7ef1b355e1ab4862ccd664ed4c856227",
        "index": "64121a239..c3772de18 100644",
        "commit_message": "[tune/raysgd] Tune API for TorchTrainer + Fix State Restoration (#7547)\n\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "import logging",
            "logger = logging.getLogger(__name__)",
            "",
            "TorchTrainer = None",
            "-TorchTrainable = None",
            "TrainingOperator = None",
            "+BaseTorchTrainable = None",
            "",
            "try:",
            "import torch  # noqa: F401",
            "",
            "-    from ray.util.sgd.torch.torch_trainer import (TorchTrainer, TorchTrainable)",
            "+    from ray.util.sgd.torch.torch_trainer import (TorchTrainer,",
            "+                                                  BaseTorchTrainable)",
            "",
            "from ray.util.sgd.torch.training_operator import TrainingOperator",
            "",
            "-    __all__ = [\"TorchTrainer\", \"TorchTrainable\", \"TrainingOperator\"]",
            "+    __all__ = [\"TorchTrainer\", \"BaseTorchTrainable\", \"TrainingOperator\"]",
            "except ImportError:",
            "logger.warning(\"PyTorch not found. TorchTrainer will not be available\")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=5, insert_id=1125867)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1125868)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'BaseTorchTrainable'), position=0, insert_id=1125869)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1125870)",
            "Insert(target_node=IN(type=assignment), node=('none', 'None'), position=2, insert_id=1125871)",
            "Update(target_node=ASTNode(type=identifier, text=TorchTrainable), value='BaseTorchTrainable')",
            "Update(target_node=ASTNode(type=string, text=\"TorchTrainable\"), value='\"BaseTorchTrainable\"')",
            "Delete(target_node=ASTNode(type=identifier, text=TorchTrainable))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=none, text=None))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 3,
        "minus_line": 2,
        "AST_diff_line": 12,
        "number": 6447,
        "neg_line": [
            "-TorchTrainable = None",
            "-from ray.util.sgd.torch.torch_trainer import (TorchTrainer, TorchTrainable)",
            "-__all__ = [\"TorchTrainer\", \"TorchTrainable\", \"TrainingOperator\"]"
        ],
        "pos_line": [
            "+BaseTorchTrainable = None",
            "+from ray.util.sgd.torch.torch_trainer import (TorchTrainer,",
            "+BaseTorchTrainable)",
            "+__all__ = [\"TorchTrainer\", \"BaseTorchTrainable\", \"TrainingOperator\"]"
        ],
        "core_change": "-TorchTrainable = None +BaseTorchTrainable = None -from ray.util.sgd.torch.torch_trainer import (TorchTrainer, TorchTrainable) +from ray.util.sgd.torch.torch_trainer import (TorchTrainer, +BaseTorchTrainable) -__all__ = [\"TorchTrainer\", \"TorchTrainable\", \"TrainingOperator\"] +__all__ = [\"TorchTrainer\", \"BaseTorchTrainable\", \"TrainingOperator\"]",
        "core_API": "getLogger"
    },
    {
        "commit_hash": "cdd2edd2874911206b799ee22f28351ef6cb443a",
        "index": "51e7668b4..37177546d 100644",
        "commit_message": "fix flake8 and refactor pre_beam\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class BeamSearch(torch.nn.Module):",
            "",
            "\"\"\"",
            "if self._do_pre_beam(scores):",
            "-            return torch.topk(scores[self.pre_beam_score_key], self.pre_beam_size)[1]",
            "+            return torch.topk(scores[self.pre_beam_score_key], self.pre_beam_size, dim=-1)[1]",
            "else:",
            "return torch.arange(self.n_vocab, device=device)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=163174)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=163175)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dim'), position=0, insert_id=163176)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=163177)",
            "Insert(target_node=IN(type=keyword_argument), node=('unary_operator', '-1'), position=2, insert_id=163178)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 6453,
        "neg_line": [
            "-return torch.topk(scores[self.pre_beam_score_key], self.pre_beam_size)[1]"
        ],
        "pos_line": [
            "+return torch.topk(scores[self.pre_beam_score_key], self.pre_beam_size, dim=-1)[1]"
        ],
        "core_change": "-return torch.topk(scores[self.pre_beam_score_key], self.pre_beam_size)[1] +return torch.topk(scores[self.pre_beam_score_key], self.pre_beam_size, dim=-1)[1]",
        "core_API": "_do_pre_beam"
    },
    {
        "commit_hash": "7d8b1665c8afe9279a723bcf2277dd55a0e10007",
        "index": "c2e7f561..b62004c8 100644",
        "commit_message": "Fix rand_segment edge case (input_len == seg_len - 1)\n\n",
        "file": "TTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def rand_segments(",
            "T = segment_size",
            "if _x_lenghts is None:",
            "_x_lenghts = T",
            "-    len_diff = _x_lenghts - segment_size + 1",
            "+    len_diff = _x_lenghts - segment_size",
            "if let_short_samples:",
            "_x_lenghts[len_diff < 0] = segment_size",
            "-        len_diff = _x_lenghts - segment_size + 1",
            "+        len_diff = _x_lenghts - segment_size",
            "else:",
            "assert all(",
            "len_diff > 0",
            "), f\" [!] At least one sample is shorter than the segment size ({segment_size}). \\n {_x_lenghts}\"",
            "-    segment_indices = (torch.rand([B]).type_as(x) * len_diff).long()",
            "-    ret = segment(x, segment_indices, segment_size)",
            "+    segment_indices = (torch.rand([B]).type_as(x) * (len_diff + 1)).long()",
            "+    ret = segment(x, segment_indices, segment_size, pad_short=pad_short)",
            "return ret, segment_indices"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=binary_operator), position=7)",
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=binary_operator), position=14)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=6, insert_id=1252611)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=7, insert_id=1252612)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'pad_short'), position=0, insert_id=1252613)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1252614)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'pad_short'), position=2, insert_id=1252615)",
            "Insert(target_node=ASTNode(type=binary_operator), node=('parenthesized_expression', None), position=2, insert_id=1252616)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('(', '('), position=0, insert_id=1252617)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('binary_operator', None), position=1, insert_id=1252618)",
            "Insert(target_node=IN(type=parenthesized_expression), node=(')', ')'), position=2, insert_id=1252619)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=identifier, text=len_diff), position=0)",
            "Insert(target_node=IN(type=binary_operator), node=('+', '+'), position=1, insert_id=1252620)",
            "Insert(target_node=IN(type=binary_operator), node=('integer', '1'), position=2, insert_id=1252621)",
            "Delete(target_node=ASTNode(type=+, text=+))",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=binary_operator))",
            "Delete(target_node=ASTNode(type=+, text=+))",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=binary_operator))"
        ],
        "plus_line": 4,
        "minus_line": 4,
        "AST_diff_line": 20,
        "number": 6455,
        "neg_line": [
            "-len_diff = _x_lenghts - segment_size + 1",
            "-len_diff = _x_lenghts - segment_size + 1",
            "-segment_indices = (torch.rand([B]).type_as(x) * len_diff).long()",
            "-ret = segment(x, segment_indices, segment_size)"
        ],
        "pos_line": [
            "+len_diff = _x_lenghts - segment_size",
            "+len_diff = _x_lenghts - segment_size",
            "+segment_indices = (torch.rand([B]).type_as(x) * (len_diff + 1)).long()",
            "+ret = segment(x, segment_indices, segment_size, pad_short=pad_short)"
        ],
        "core_change": "-len_diff = _x_lenghts - segment_size + 1 +len_diff = _x_lenghts - segment_size -len_diff = _x_lenghts - segment_size + 1 +len_diff = _x_lenghts - segment_size -segment_indices = (torch.rand([B]).type_as(x) * len_diff).long() -ret = segment(x, segment_indices, segment_size) +segment_indices = (torch.rand([B]).type_as(x) * (len_diff + 1)).long() +ret = segment(x, segment_indices, segment_size, pad_short=pad_short)",
        "core_API": "rand"
    },
    {
        "commit_hash": "132ee8a535359aef62eff3e264cba3cfe4ea921f",
        "index": "cdaad4ad..1b470d06 100644",
        "commit_message": "Extended api of LSTM encoder. (#2030)\n\nSummary:\n# Before submitting\n\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\n- [x] Did you make sure to update the docs?\n- [ ] Did you write any new necessary tests?\n\n## What does this PR do?\nFixes https://github.com/pytorch/fairseq/issues/1862.\nNobody responded to my issue. Nevertheless change is very small, therefore i think is doesn't need much discussion.\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding �\nPull Request resolved: https://github.com/pytorch/fairseq/pull/2030\n\nReviewed By: joshim5, ngoyal2707\n\nDifferential Revision: D21584250\n\nPulled By: myleott\n\nfbshipit-source-id: 28f0ccaca0df2860806178dbce02bcc12d7115d4\n\n",
        "file": "fairseq.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class LSTMEncoder(FairseqEncoder):",
            "x = x.transpose(0, 1)",
            "",
            "# pack embedded source tokens into a PackedSequence",
            "-        packed_x = nn.utils.rnn.pack_padded_sequence(x, src_lengths.data)",
            "+        packed_x = nn.utils.rnn.pack_padded_sequence(",
            "+            x, src_lengths.data, enforce_sorted=enforce_sorted",
            "+        )",
            "",
            "# apply LSTM",
            "if self.bidirectional:"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=1355802)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=1355803)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'enforce_sorted'), position=0, insert_id=1355804)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1355805)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'enforce_sorted'), position=2, insert_id=1355806)"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 6458,
        "neg_line": [
            "-packed_x = nn.utils.rnn.pack_padded_sequence(x, src_lengths.data)"
        ],
        "pos_line": [
            "+packed_x = nn.utils.rnn.pack_padded_sequence(",
            "+x, src_lengths.data, enforce_sorted=enforce_sorted",
            "+)"
        ],
        "core_change": "-packed_x = nn.utils.rnn.pack_padded_sequence(x, src_lengths.data) +packed_x = nn.utils.rnn.pack_padded_sequence( +x, src_lengths.data, enforce_sorted=enforce_sorted +)",
        "core_API": "transpose"
    },
    {
        "commit_hash": "94ba8ef8a62ed1d0e241368af6570a2b69af4608",
        "index": "9a941e30..d60910f0 100644",
        "commit_message": "Fix two warnings related to deprecated function names.\n\nPiperOrigin-RevId: 284760914\n\n",
        "file": "tf-quant-finance.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def find_interval_index(query_xs,",
            "# cap to last_index - 1.",
            "caps = last_index - tf.cast(should_cap, dtype=tf.dtypes.int32)",
            "",
            "-    return tf.where(last_interval_is_closed, tf.minimum(indices, caps), indices)",
            "+    return tf.compat.v1.where(last_interval_is_closed,",
            "+                              tf.minimum(indices, caps), indices)",
            "",
            "",
            "def _piecewise_constant_function(x, jump_locations, values, side='left'):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=2347379)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=2347380)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2347381)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2347382)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'v1'), position=2, insert_id=2347383)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=ERROR), position=1)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=2)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'compat'), position=3, insert_id=2347384)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 6459,
        "neg_line": [
            "-return tf.where(last_interval_is_closed, tf.minimum(indices, caps), indices)"
        ],
        "pos_line": [
            "+return tf.compat.v1.where(last_interval_is_closed,",
            "+tf.minimum(indices, caps), indices)"
        ],
        "core_change": "-return tf.where(last_interval_is_closed, tf.minimum(indices, caps), indices) +return tf.compat.v1.where(last_interval_is_closed, +tf.minimum(indices, caps), indices)",
        "core_API": "cast"
    },
    {
        "commit_hash": "82f9e2358c35c5bbff8714280174617ba29cf92b",
        "index": "5b7fe618..fe4cea41 100644",
        "commit_message": "Further fixes and improvements.\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def ndim(x):",
            "2",
            "```",
            "\"\"\"",
            "-    if is_sparse(x):",
            "-        return x._dims",
            "-",
            "dims = x.get_shape()._dims",
            "if dims is not None:",
            "return len(dims)"
        ],
        "hunk_index": 4,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=if, text=if))",
            "Delete(target_node=ASTNode(type=identifier, text=is_sparse))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=x))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=return, text=return))",
            "Delete(target_node=ASTNode(type=identifier, text=x))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=_dims))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=return_statement))",
            "Delete(target_node=ASTNode(type=block))",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=block, text=))",
            "Delete(target_node=ASTNode(type=if_statement))"
        ],
        "plus_line": 0,
        "minus_line": 2,
        "AST_diff_line": 18,
        "number": 6461,
        "neg_line": [
            "-if is_sparse(x):",
            "-return x._dims",
            "-"
        ],
        "pos_line": [],
        "core_change": "-if is_sparse(x): -return x._dims -",
        "core_API": "get_shape"
    },
    {
        "commit_hash": "5d4258fac5e6ceaa9c897f841cb737c56717a996",
        "index": "d0fb0e8..8b70a6f 100644",
        "commit_message": "Fix MixConv2d() remove shortcut + apply depthwise (#5410)\n\n\n",
        "file": "yolov5.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class BottleneckCSP(nn.Module):",
            "self.cv3 = nn.Conv2d(c_, c_, 1, 1, bias=False)",
            "self.cv4 = Conv(2 * c_, c2, 1, 1)",
            "self.bn = nn.BatchNorm2d(2 * c_)  # applied to cat(cv2, cv3)",
            "-        self.act = nn.LeakyReLU(0.1, inplace=True)",
            "+        self.act = nn.SiLU()",
            "self.m = nn.Sequential(*(Bottleneck(c_, c_, shortcut, g, e=1.0) for _ in range(n)))",
            "",
            "def forward(self, x):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=LeakyReLU), value='SiLU')",
            "Delete(target_node=ASTNode(type=float, text=0.1))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=inplace))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=true, text=True))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 6463,
        "neg_line": [
            "-self.act = nn.LeakyReLU(0.1, inplace=True)"
        ],
        "pos_line": [
            "+self.act = nn.SiLU()"
        ],
        "core_change": "-self.act = nn.LeakyReLU(0.1, inplace=True) +self.act = nn.SiLU()",
        "core_API": "Conv2d"
    },
    {
        "commit_hash": "87e8deadd563982672a1c5104a68d1a67f0cf765",
        "index": "c5cdd92..b9869df 100644",
        "commit_message": "zero-mAP fix remove `torch.empty()` forward pass in `.train()` mode (#9068)\n\n* Fix Zero Map Issue\n\nSigned-off-by: 0zppd <111682241+0zppd@users.noreply.github.com>\n\n* Update __init__.py\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\nSigned-off-by: 0zppd <111682241+0zppd@users.noreply.github.com>\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\nCo-authored-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n",
        "file": "yolov5.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def log_tensorboard_graph(tb, model, imgsz=(640, 640)):",
            "try:",
            "p = next(model.parameters())  # for device, type",
            "imgsz = (imgsz, imgsz) if isinstance(imgsz, int) else imgsz  # expand",
            "-        im = torch.empty((1, 3, *imgsz)).to(p.device).type_as(p)  # input image",
            "+        im = torch.zeros((1, 3, *imgsz)).to(p.device).type_as(p)  # input image (WARNING: must be zeros, not empty)",
            "with warnings.catch_warnings():",
            "warnings.simplefilter('ignore')  # suppress jit trace warning",
            "tb.add_graph(torch.jit.trace(de_parallel(model), im, strict=False), [])"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=empty), value='zeros')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 6467,
        "neg_line": [
            "-im = torch.empty((1, 3, *imgsz)).to(p.device).type_as(p)  # input image"
        ],
        "pos_line": [
            "+im = torch.zeros((1, 3, *imgsz)).to(p.device).type_as(p)  # input image (WARNING: must be zeros, not empty)"
        ],
        "core_change": "-im = torch.empty((1, 3, *imgsz)).to(p.device).type_as(p)  # input image +im = torch.zeros((1, 3, *imgsz)).to(p.device).type_as(p)  # input image (WARNING: must be zeros, not empty)",
        "core_API": "parameters"
    },
    {
        "commit_hash": "1a7e1c47cb502f19c693e992f0b8507a9154f0b2",
        "index": "258d60bbc9..365b897dcd 100644",
        "commit_message": "Added example for compute grads in ray tutorial (#238)\n\n* Added example for compute grads in ray\n\n* Added formatting\n\n* Removed need for placeholders in apply gradient\n\n* Streamlined examples\n\n* Fixed docs\n\n* Added formatting\n\n* Removed old references\n\n* Simplified code some\n\n* Addressed comments\n\n* Changes to first code block\n\n* Added test for training and updated code snippets\n\n* Formatting\n\n* Removed mean\n\n* Removed all mention of mean\n\n* Added comments\n\n* Added comments\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TensorFlowVariables(object):",
            "def set_weights(self, new_weights):",
            "\"\"\"Sets the weights to new_weights.\"\"\"",
            "self._check_sess()",
            "-    self.sess.run(self.assignment_nodes, feed_dict={self.assignment_placeholders[name]: value for (name, value) in new_weights.items()})",
            "+    self.sess.run(self.assignment_nodes, feed_dict={self.placeholders[name]: value for (name, value) in new_weights.items()})"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=assignment_placeholders), value='placeholders')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 6468,
        "neg_line": [
            "-self.sess.run(self.assignment_nodes, feed_dict={self.assignment_placeholders[name]: value for (name, value) in new_weights.items()})"
        ],
        "pos_line": [
            "+self.sess.run(self.assignment_nodes, feed_dict={self.placeholders[name]: value for (name, value) in new_weights.items()})"
        ],
        "core_change": "-self.sess.run(self.assignment_nodes, feed_dict={self.assignment_placeholders[name]: value for (name, value) in new_weights.items()}) +self.sess.run(self.assignment_nodes, feed_dict={self.placeholders[name]: value for (name, value) in new_weights.items()})",
        "core_API": "_check_sess"
    },
    {
        "commit_hash": "3dda5ac9fcf390d8b83eb855249360711707c11c",
        "index": "5abca5cf..9548ada7 100644",
        "commit_message": "for discussion: incorporate black code formatter (#3308)\n\n* setup files\n\n* run black\n\n* undo\n\n* update CONTRIBUTING.md\n\n* fix quotes in test_other_modules\n\n* make flake8 happy\n\n* set black to 100 characters per line\n\n* move type: ignore to where mypy wants them\n\n* more flake8\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def get_best_span(span_start_logits: torch.Tensor, span_end_logits: torch.Tensor",
            "span_log_probs = span_start_logits.unsqueeze(2) + span_end_logits.unsqueeze(1)",
            "# Only the upper triangle of the span matrix is valid; the lower triangle has entries where",
            "# the span ends before it starts.",
            "-    span_log_mask = torch.triu(torch.ones((passage_length, passage_length),",
            "-                                          device=device)).log()",
            "+    span_log_mask = torch.triu(torch.ones((passage_length, passage_length), device=device)).log()",
            "valid_span_log_probs = span_log_probs + span_log_mask",
            "",
            "# Here we take the span matrix and flatten it, then find the best span using argmax.  We"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 6470,
        "neg_line": [
            "-span_log_mask = torch.triu(torch.ones((passage_length, passage_length),",
            "-device=device)).log()"
        ],
        "pos_line": [
            "+span_log_mask = torch.triu(torch.ones((passage_length, passage_length), device=device)).log()"
        ],
        "core_change": "-span_log_mask = torch.triu(torch.ones((passage_length, passage_length), -device=device)).log() +span_log_mask = torch.triu(torch.ones((passage_length, passage_length), device=device)).log()",
        "core_API": "unsqueeze"
    },
    {
        "commit_hash": "ee6c0ace25c51d85e87b4a38db27371a2bc55c9f",
        "index": "69cae707..c605ef72 100755",
        "commit_message": "[layers] fix BiDynamicRNNLayer to avoid error in gradient updating\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class BiDynamicRNNLayer(Layer):",
            "",
            "print(\"     n_params : %d\" % (len(rnn_variables)))",
            "# Manage the outputs",
            "-            outputs = tf.concat(-1, outputs)",
            "+            outputs = tf.concat(2, outputs)",
            "if return_last:",
            "# [batch_size, 2 * n_hidden]",
            "self.outputs = advanced_indexing_op(outputs, sequence_length)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('integer', '2'), position=1, insert_id=2268945)",
            "Delete(target_node=ASTNode(type=unary_operator, text=-1))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 6476,
        "neg_line": [
            "-outputs = tf.concat(-1, outputs)"
        ],
        "pos_line": [
            "+outputs = tf.concat(2, outputs)"
        ],
        "core_change": "-outputs = tf.concat(-1, outputs) +outputs = tf.concat(2, outputs)",
        "core_API": "concat"
    },
    {
        "commit_hash": "4d95602d85222d4223436130d30f2fce567cfb5d",
        "index": "da92838..6c8964f 100755",
        "commit_message": "static padding fixed\n\n",
        "file": "EfficientNet-PyTorch.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class Conv2dStaticSamePadding(nn.Conv2d):",
            "pad_h = max((oh - 1) * self.stride[0] + (kh - 1) * self.dilation[0] + 1 - ih, 0)",
            "pad_w = max((ow - 1) * self.stride[1] + (kw - 1) * self.dilation[1] + 1 - iw, 0)",
            "if pad_h > 0 or pad_w > 0:",
            "-            self.static_padding = nn.ZeroPad2d((pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2))",
            "+            self.static_padding = nn.ZeroPad2d((pad_w - pad_w // 2, pad_w - pad_w // 2,",
            "+                                                pad_h - pad_h // 2, pad_h - pad_h // 2))",
            "else:",
            "self.static_padding = Identity()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=tuple), node=('binary_operator', None), position=1, insert_id=1331511)",
            "Insert(target_node=ASTNode(type=tuple), node=('binary_operator', None), position=6, insert_id=1331512)",
            "Insert(target_node=IN(type=binary_operator), node=('identifier', 'pad_w'), position=0, insert_id=1331513)",
            "Insert(target_node=IN(type=binary_operator), node=('-', '-'), position=1, insert_id=1331514)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=binary_operator), position=2)",
            "Insert(target_node=IN(type=binary_operator), node=('identifier', 'pad_h'), position=0, insert_id=1331515)",
            "Insert(target_node=IN(type=binary_operator), node=('-', '-'), position=1, insert_id=1331516)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=binary_operator), position=2)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 6479,
        "neg_line": [
            "-self.static_padding = nn.ZeroPad2d((pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2))"
        ],
        "pos_line": [
            "+self.static_padding = nn.ZeroPad2d((pad_w - pad_w // 2, pad_w - pad_w // 2,",
            "+pad_h - pad_h // 2, pad_h - pad_h // 2))"
        ],
        "core_change": "-self.static_padding = nn.ZeroPad2d((pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2)) +self.static_padding = nn.ZeroPad2d((pad_w - pad_w // 2, pad_w - pad_w // 2, +pad_h - pad_h // 2, pad_h - pad_h // 2))",
        "core_API": "ZeroPad2d"
    },
    {
        "commit_hash": "0eca74fa5f7bf82f3b93e3e38dd1d84cfedc5630",
        "index": "17f08da..c258515 100644",
        "commit_message": "lint fixes\n\nSummary:\nRan the linter.\nTODO: need to update the linter as per D21353065.\n\nReviewed By: bottler\n\nDifferential Revision: D21362270\n\nfbshipit-source-id: ad0e781de0a29f565ad25c43bc94a19b1828c020\n\n",
        "file": "pytorch3d.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestSamplePoints(TestCaseMixin, unittest.TestCase):",
            "x, y, z = samples[1, :].unbind(1)",
            "radius = torch.sqrt(x ** 2 + y ** 2 + z ** 2)",
            "",
            "-        self.assertClose(radius, torch.ones((num_samples)))",
            "+        self.assertClose(radius, torch.ones(num_samples))",
            "",
            "# Pyramid: points shoudl lie on one of the faces.",
            "pyramid_verts = samples[2, :]"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=num_samples), position=1)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=), text=)), position=2)",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=parenthesized_expression))",
            "Delete(target_node=ASTNode(type=), text=)))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 6486,
        "neg_line": [
            "-self.assertClose(radius, torch.ones((num_samples)))"
        ],
        "pos_line": [
            "+self.assertClose(radius, torch.ones(num_samples))"
        ],
        "core_change": "-self.assertClose(radius, torch.ones((num_samples))) +self.assertClose(radius, torch.ones(num_samples))",
        "core_API": "sqrt"
    },
    {
        "commit_hash": "738a9cd63554104635351ced21d6f5808c1b6072",
        "index": "3b8a049..5f75647 100644",
        "commit_message": "unbiased=False for torch.var_mean path of ConvNeXt LN. Fix #1090\n\n",
        "file": "pytorch-image-models.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class LayerNorm2d(nn.LayerNorm):",
            "return F.layer_norm(",
            "x.permute(0, 2, 3, 1), self.normalized_shape, self.weight, self.bias, self.eps).permute(0, 3, 1, 2)",
            "else:",
            "-            s, u = torch.var_mean(x, dim=1, keepdim=True)",
            "+            s, u = torch.var_mean(x, dim=1, unbiased=False, keepdim=True)",
            "x = (x - u) * torch.rsqrt(s + self.eps)",
            "x = x * self.weight[:, None, None] + self.bias[:, None, None]",
            "return x"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=894615)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=6, insert_id=894616)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'unbiased'), position=0, insert_id=894617)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=894618)",
            "Insert(target_node=IN(type=keyword_argument), node=('false', 'False'), position=2, insert_id=894619)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 6489,
        "neg_line": [
            "-s, u = torch.var_mean(x, dim=1, keepdim=True)"
        ],
        "pos_line": [
            "+s, u = torch.var_mean(x, dim=1, unbiased=False, keepdim=True)"
        ],
        "core_change": "-s, u = torch.var_mean(x, dim=1, keepdim=True) +s, u = torch.var_mean(x, dim=1, unbiased=False, keepdim=True)",
        "core_API": "layer_norm"
    },
    {
        "commit_hash": "96ebbe5ee9e52aa81da0a86eca48fb6df1222226",
        "index": "530aff2..9931fa8 100644",
        "commit_message": "Fix control_flow_ops (#380)\n\nTensorflow 0.11 removes the ability to access python.control_flow_ops\ntf.cond is (and was) the correct function\nREF: https://github.com/tensorflow/tensorflow/issues/4616\n",
        "file": "tflearn.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def batch_normalization(incoming, beta=0.0, gamma=1.0, epsilon=1e-5,",
            "",
            "# Retrieve variable managing training mode",
            "is_training = tflearn.get_training_mode()",
            "-        mean, var = tf.python.control_flow_ops.cond(",
            "+        mean, var = tf.cond(",
            "is_training, update_mean_var, lambda: (moving_mean, moving_variance))",
            "",
            "try:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=IN(type=root), node=ASTNode(type=attribute), position=0)",
            "Delete(target_node=ASTNode(type=identifier, text=python))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=control_flow_ops))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 6490,
        "neg_line": [
            "-mean, var = tf.python.control_flow_ops.cond("
        ],
        "pos_line": [
            "+mean, var = tf.cond("
        ],
        "core_change": "-mean, var = tf.python.control_flow_ops.cond( +mean, var = tf.cond(",
        "core_API": "get_training_mode"
    },
    {
        "commit_hash": "ad39271ae8518afaa4c4831bd1ddccc3bc86eab6",
        "index": "0197a8fec..6d1264785 100644",
        "commit_message": "Fix FP16 and attention masks in FunnelTransformer (#7374)\n\n* Fix #7371\n\n* Fix training\n\n* Fix test values\n\n* Apply the fix to TF as well\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class FunnelModelIntegrationTest(unittest.TestCase):",
            "inputs = tokenizer(\"Hello! I am the Funnel Transformer model.\", return_tensors=\"pt\")",
            "output = model(**inputs)[0]",
            "",
            "-        expected_output_sum = torch.tensor(235.7827)",
            "+        expected_output_sum = torch.tensor(235.7246)",
            "expected_output_mean = torch.tensor(0.0256)",
            "self.assertTrue(torch.allclose(output.sum(), expected_output_sum, atol=1e-4))",
            "self.assertTrue(torch.allclose(output.mean(), expected_output_mean, atol=1e-4))"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=float, text=235.7827), value='235.7246')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 6494,
        "neg_line": [
            "-expected_output_sum = torch.tensor(235.7827)"
        ],
        "pos_line": [
            "+expected_output_sum = torch.tensor(235.7246)"
        ],
        "core_change": "-expected_output_sum = torch.tensor(235.7827) +expected_output_sum = torch.tensor(235.7246)",
        "core_API": "tensor"
    },
    {
        "commit_hash": "39f3deb41a33bff86516a9c3373754fafe9cf949",
        "index": "3700f5aa4..e2d8a934e 100644",
        "commit_message": "MNIST Prototype Duet Notebooks (#4696)\n\n* added request_block support\n\n* ran black\n\n* fixed return types\n\n* Added loopback=True to launch_duet and join_duet\n\n- loopback automatically writes and reads the IDs to a shared tmp file\n- The purpose of this helper is to speed up development or local usage\n\n* Converted prototype MNIST notebook into Duet Notebooks\n\n- Added additional required ops for MNIST\n- Added matplotlib when displaying images for inference\n\n* ignoring bandit warning on /tmp folder\n\n- How else can we get a deterministic tmp folder between processes?\n\n* Fixed /tmp reference with tempfile.gettempdir()\n\n- Unique path on macOS seems to be deterministic between procs\n- This should now work on Windows as well\n- Tested on MNIST notebooks\n\n* Added tests for torch.Tensor.backward and torch.Tensor.__getitem__\n\n- Added requires_grad to Tensor since its legal to use requires_grad\n  without it being a nn.Parameter and required by the .backward test\n\n* Rebase off syft_0.3.0\n\n* Re-enabling Windows CI from syft_0.3.0\n\n* Renamed return types of SyNone to new _SyNone class\n\n- SyNone is the singleton and _SyNone is the constructor Class\n\n* Made improvements to get / request blocking mode\n\n- Added customizable timeout\n- Tested on MNIST notebooks\n\n* proto regen noise\n\n* Updated MNIST duet notebooks with blocking get requests\n\n- Added torchvision DataLoader pointers from remote side\n\n* Override __module__ for syft types (#4724)\n\n* Renamed examples/pytorch to examples/duet\n\n- Updated .gitignore for data download dirs\n\n* Added request_name and reason to blocking get request\n\n- Added more verbose output and UID in sy.VERBOSE mode\n\n* Added sy.Module which aims to provide similar functionality to nn.Module\n\n* Updated MNIST notebooks to use sy.Module\n\nCo-authored-by: Anubhav Raj Singh <anubhavraj.08@gmail.com>\nCo-authored-by: Jasopaum <jason.paumier@gmail.com>\n",
        "file": "PySyft.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def create_torch_ast() -> Globals:",
            "path=method, framework_reference=torch, return_type_name=return_type",
            ")",
            "else:",
            "-            print(f\"Skipping torch.{method} not supported in {TORCH_VERSION}\")",
            "+            pass",
            "+            # TODO: Replace with logging",
            "+            # print(f\"Skipping {method} not supported in {TORCH_VERSION}\")",
            "",
            "for klass in ast.classes:",
            "klass.create_pointer_class()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('pass_statement', None), position=3, insert_id=785649)",
            "Insert(target_node=IN(type=pass_statement), node=('pass', 'pass'), position=0, insert_id=785650)",
            "Delete(target_node=ASTNode(type=identifier, text=print))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=string, text=f\"Skipping torch.{method} not supported in {TORCH_VERSION}\"))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 6495,
        "neg_line": [
            "-print(f\"Skipping torch.{method} not supported in {TORCH_VERSION}\")"
        ],
        "pos_line": [
            "+pass",
            "+# TODO: Replace with logging",
            "+# print(f\"Skipping {method} not supported in {TORCH_VERSION}\")"
        ],
        "core_change": "-print(f\"Skipping torch.{method} not supported in {TORCH_VERSION}\") +pass +# TODO: Replace with logging +# print(f\"Skipping {method} not supported in {TORCH_VERSION}\")",
        "core_API": "create_pointer_class"
    },
    {
        "commit_hash": "ee950b503eeed5aca3747a4bcf2a40f624b743a0",
        "index": "95dce24f..77b45df8 100755",
        "commit_message": "fixed trpo\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFOptimizer(Optimizer):",
            "loss = fn_loss(**arguments)",
            "",
            "# The actual tensorflow minimize op.",
            "-        with tf.control_dependencies(control_inputs=(loss,)):",
            "-            # colocate_gradients_with_ops=True",
            "-            applied = self.optimizer.minimize(loss=loss, var_list=variables)",
            "+        applied = self.optimizer.minimize(loss=loss, var_list=variables)",
            "+        # colocate_gradients_with_ops=True",
            "",
            "# Return deltas after actually having change the variables.",
            "with tf.control_dependencies(control_inputs=(applied,)):"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=module), node=ASTNode(type=expression_statement), position=2)",
            "Delete(target_node=ASTNode(type=with, text=with))",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=control_dependencies))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=control_inputs))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=loss))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=tuple))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=with_item))",
            "Delete(target_node=ASTNode(type=with_clause))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=block))",
            "Delete(target_node=ASTNode(type=with_statement))"
        ],
        "plus_line": 2,
        "minus_line": 3,
        "AST_diff_line": 23,
        "number": 6496,
        "neg_line": [
            "-with tf.control_dependencies(control_inputs=(loss,)):",
            "-# colocate_gradients_with_ops=True",
            "-applied = self.optimizer.minimize(loss=loss, var_list=variables)"
        ],
        "pos_line": [
            "+applied = self.optimizer.minimize(loss=loss, var_list=variables)",
            "+# colocate_gradients_with_ops=True"
        ],
        "core_change": "-with tf.control_dependencies(control_inputs=(loss,)): -# colocate_gradients_with_ops=True -applied = self.optimizer.minimize(loss=loss, var_list=variables) +applied = self.optimizer.minimize(loss=loss, var_list=variables) +# colocate_gradients_with_ops=True",
        "core_API": "control_dependencies"
    },
    {
        "commit_hash": "7f1e63aed1252ba8145d4a66ce2272331d60cdae",
        "index": "9fe1829..d914dcb 100644",
        "commit_message": "Take care with single integers on gpu\n\nSummary:\nPytorch seems to be becoming stricter about integer tensors of shape `(1,)` on GPU and not allowing them to be used as `int`s. For example the following no longer works on pytorch master,\n    foo = torch.tensor([3, 5, 3], device=\"cuda:0\")\n    torch.arange(10) + foo[0]\nbecause this is the sum of tensors on different devices.\n\nHere fix tests which recently broke because of this.\n\nReviewed By: nikhilaravi\n\nDifferential Revision: D21929745\n\nfbshipit-source-id: 25374f70468d1c895372766f1a9dd61df0833957\n\n",
        "file": "pytorch3d.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "cuda",
        "change": [
            "class TestPointMeshDistance(TestCaseMixin, unittest.TestCase):",
            "start = faces_first_idx[i]",
            "end = faces_first_idx[i + 1] if i < N - 1 else faces_packed.shape[0]",
            "",
            "-            min_idx = idx_cuda.cpu()[start:end] - points_first_idx[i]",
            "+            min_idx = idx_cuda.cpu()[start:end] - points_first_idx[i].cpu()",
            "iidx = torch.arange(tris.shape[0], device=device)",
            "min_dist = dists_temp[iidx, min_idx]"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=binary_operator), node=('call', None), position=2, insert_id=1865148)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1865149)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1865150)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=subscript), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1865151)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cpu'), position=2, insert_id=1865152)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1865153)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1865154)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 6500,
        "neg_line": [
            "-min_idx = idx_cuda.cpu()[start:end] - points_first_idx[i]"
        ],
        "pos_line": [
            "+min_idx = idx_cuda.cpu()[start:end] - points_first_idx[i].cpu()"
        ],
        "core_change": "-min_idx = idx_cuda.cpu()[start:end] - points_first_idx[i] +min_idx = idx_cuda.cpu()[start:end] - points_first_idx[i].cpu()",
        "core_API": "cpu"
    },
    {
        "commit_hash": "da10de8466c001dceca328dac12751abb71c65eb",
        "index": "cd8bc4bc2..2a8f81cd3 100644",
        "commit_message": "fix bug with padding mask + add corresponding test\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def build_lm_labels(sequence, pad_token):",
            "def build_mask(sequence, pad_token):",
            "\"\"\" Builds the mask. The attention mechanism will only attend to positions",
            "with value 1. \"\"\"",
            "-    mask = sequence.clone()",
            "-    mask[mask != pad_token] = 1",
            "-    mask[mask == pad_token] = 0",
            "+    mask = torch.ones_like(sequence)",
            "+    idx_pad_tokens = (sequence == pad_token)",
            "+    mask[idx_pad_tokens] = 0",
            "return mask"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=mask), value='idx_pad_tokens')",
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=identifier, text=mask), position=0)",
            "Insert(target_node=ASTNode(type=assignment), node=('=', '='), position=1, insert_id=1245539)",
            "Insert(target_node=ASTNode(type=assignment), node=('parenthesized_expression', None), position=2, insert_id=1245540)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('(', '('), position=0, insert_id=1245541)",
            "Move(target_node=IN(type=parenthesized_expression), node=ASTNode(type=comparison_operator), position=1)",
            "Insert(target_node=IN(type=parenthesized_expression), node=(')', ')'), position=2, insert_id=1245542)",
            "Update(target_node=ASTNode(type=identifier, text=mask), value='idx_pad_tokens')",
            "Move(target_node=ASTNode(type=subscript), node=ASTNode(type=identifier, text=mask), position=2)",
            "Update(target_node=ASTNode(type=identifier, text=sequence), value='torch')",
            "Update(target_node=ASTNode(type=identifier, text=clone), value='ones_like')",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'sequence'), position=1, insert_id=1245543)",
            "Update(target_node=ASTNode(type=identifier, text=mask), value='sequence')",
            "Insert(target_node=ASTNode(type=comparison_operator), node=('==', '=='), position=1, insert_id=1245544)",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=!=, text=!=))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type===, text===))",
            "Delete(target_node=ASTNode(type=identifier, text=pad_token))",
            "Delete(target_node=ASTNode(type=comparison_operator))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 23,
        "number": 6504,
        "neg_line": [
            "-mask = sequence.clone()",
            "-mask[mask != pad_token] = 1",
            "-mask[mask == pad_token] = 0"
        ],
        "pos_line": [
            "+mask = torch.ones_like(sequence)",
            "+idx_pad_tokens = (sequence == pad_token)",
            "+mask[idx_pad_tokens] = 0"
        ],
        "core_change": "-mask = sequence.clone() -mask[mask != pad_token] = 1 -mask[mask == pad_token] = 0 +mask = torch.ones_like(sequence) +idx_pad_tokens = (sequence == pad_token) +mask[idx_pad_tokens] = 0",
        "core_API": "clone"
    },
    {
        "commit_hash": "96c4990165f8096da3de30954811b42b731a286d",
        "index": "8f4cae749..7c93ea16c 100644",
        "commit_message": "fix unused imports and style\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "keras",
        "change": [
            "class TFModelTesterMixin:",
            "for module_member_name in dir(module)",
            "if module_member_name.endswith(\"MainLayer\")",
            "for module_member in (getattr(module, module_member_name),)",
            "-            if isinstance(module_member, type) and tf.keras.layers.Layer in module_member.__bases__",
            "-            and getattr(module_member, '_keras_serializable', False)",
            "+            if isinstance(module_member, type)",
            "+            and tf.keras.layers.Layer in module_member.__bases__",
            "+            and getattr(module_member, \"_keras_serializable\", False)",
            ")",
            "for main_layer_class in tf_main_layer_classes:",
            "main_layer = main_layer_class(config)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=IN(type=root), node=ASTNode(type=ERROR), position=0)",
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=identifier, text=main_layer_class), position=4)",
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=(, text=(), position=5)",
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=identifier, text=config), position=6)",
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=), text=)), position=7)",
            "Update(target_node=ASTNode(type=string, text='_keras_serializable'), value='\"_keras_serializable\"')",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 3,
        "minus_line": 2,
        "AST_diff_line": 8,
        "number": 6505,
        "neg_line": [
            "-if isinstance(module_member, type) and tf.keras.layers.Layer in module_member.__bases__",
            "-and getattr(module_member, '_keras_serializable', False)"
        ],
        "pos_line": [
            "+if isinstance(module_member, type)",
            "+and tf.keras.layers.Layer in module_member.__bases__",
            "+and getattr(module_member, \"_keras_serializable\", False)"
        ],
        "core_change": "-if isinstance(module_member, type) and tf.keras.layers.Layer in module_member.__bases__ -and getattr(module_member, '_keras_serializable', False) +if isinstance(module_member, type) +and tf.keras.layers.Layer in module_member.__bases__ +and getattr(module_member, \"_keras_serializable\", False)",
        "core_API": "endswith"
    },
    {
        "commit_hash": "64103fb6beac8cc865945d3956266fd80b44f18f",
        "index": "8bb445e5d..555c5ea09 100644",
        "commit_message": "Fix TransfoXL (#9302)\n\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFTransfoXLMainLayer(tf.keras.layers.Layer):",
            "",
            "# There are `mlen + qlen` steps that can be cached into mems",
            "new_mems = []",
            "-        end_idx = mlen + max(0, qlen)",
            "-        beg_idx = max(0, end_idx - self.mem_len)",
            "+        end_idx = mlen + tf.math.maximum(0, qlen)",
            "+        beg_idx = tf.math.maximum(0, end_idx - tf.convert_to_tensor(self.mem_len))",
            "for i in range(len(hids)):",
            "",
            "cat = tf.concat([mems[i], hids[i]], axis=0)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2375508)",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2375509)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2375510)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2375511)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'maximum'), position=2, insert_id=2375512)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2375513)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2375514)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'maximum'), position=2, insert_id=2375515)",
            "Update(target_node=ASTNode(type=identifier, text=max), value='tf')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=max), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2375516)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'math'), position=2, insert_id=2375517)",
            "Insert(target_node=ASTNode(type=binary_operator), node=('call', None), position=2, insert_id=2375518)",
            "Update(target_node=ASTNode(type=identifier, text=max), value='tf')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=max), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2375519)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'math'), position=2, insert_id=2375520)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2375521)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2375522)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2375523)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2375524)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'convert_to_tensor'), position=2, insert_id=2375525)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2375526)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=attribute), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2375527)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 25,
        "number": 6508,
        "neg_line": [
            "-end_idx = mlen + max(0, qlen)",
            "-beg_idx = max(0, end_idx - self.mem_len)"
        ],
        "pos_line": [
            "+end_idx = mlen + tf.math.maximum(0, qlen)",
            "+beg_idx = tf.math.maximum(0, end_idx - tf.convert_to_tensor(self.mem_len))"
        ],
        "core_change": "-end_idx = mlen + max(0, qlen) -beg_idx = max(0, end_idx - self.mem_len) +end_idx = mlen + tf.math.maximum(0, qlen) +beg_idx = tf.math.maximum(0, end_idx - tf.convert_to_tensor(self.mem_len))",
        "core_API": "maximum"
    },
    {
        "commit_hash": "5a1aca713fb24d9c046c40b1cf280f55dc79b0c9",
        "index": "970d52c76..fc666ac1d 100644",
        "commit_message": "Update manual download URL in newsroom dataset (#3779)\n\n* Update manual download URL in newsroom dataset\n\n* Update homepage URL\n\n* Fix malformed dataset card\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class Newsroom(datasets.GeneratorBasedBuilder):",
            "description=_DESCRIPTION,",
            "features=datasets.Features(features),",
            "supervised_keys=(_DOCUMENT, _SUMMARY),",
            "-            homepage=\"http://lil.datasets.cornell.edu/newsroom/\",",
            "+            homepage=\"https://lil.nlp.cornell.edu/newsroom/index.html\",",
            "citation=_CITATION,",
            ")"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"http://lil.datasets.cornell.edu/newsroom/\"), value='\"https://lil.nlp.cornell.edu/newsroom/index.html\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 6510,
        "neg_line": [
            "-homepage=\"http://lil.datasets.cornell.edu/newsroom/\","
        ],
        "pos_line": [
            "+homepage=\"https://lil.nlp.cornell.edu/newsroom/index.html\","
        ],
        "core_change": "-homepage=\"http://lil.datasets.cornell.edu/newsroom/\", +homepage=\"https://lil.nlp.cornell.edu/newsroom/index.html\",",
        "core_API": "Features"
    },
    {
        "commit_hash": "6f6eef747ce2943d2b76d845b55220fd07fcf177",
        "index": "8289e01e..77963ad7 100644",
        "commit_message": "Fix Compatibility with Nvidia NGC Containers (#919)\n\nCheck if MPS backend is registered before calling is_available()\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "if is_torch_available():",
            ")",
            "",
            "if is_torch_higher_equal_than_1_12:",
            "-        torch_device = \"mps\" if torch.backends.mps.is_available() else torch_device",
            "+        # Some builds of torch 1.12 don't have the mps backend registered. See #892 for more details",
            "+        mps_backend_registered = hasattr(torch.backends, \"mps\")",
            "+        torch_device = \"mps\" if (mps_backend_registered and torch.backends.mps.is_available()) else torch_device",
            "",
            "",
            "def get_tests_dir(append_path=None):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=block), node=('expression_statement', None), position=0, insert_id=102278)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=102279)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'mps_backend_registered'), position=0, insert_id=102280)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=102281)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=102282)",
            "Insert(target_node=IN(type=call), node=('identifier', 'hasattr'), position=0, insert_id=102283)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=102284)",
            "Insert(target_node=ASTNode(type=conditional_expression), node=('parenthesized_expression', None), position=2, insert_id=102285)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=102286)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=102287)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=102288)",
            "Insert(target_node=IN(type=argument_list), node=('string', '\"mps\"'), position=3, insert_id=102289)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=102290)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('(', '('), position=0, insert_id=102291)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('boolean_operator', None), position=1, insert_id=102292)",
            "Insert(target_node=IN(type=parenthesized_expression), node=(')', ')'), position=2, insert_id=102293)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=102294)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=102295)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'backends'), position=2, insert_id=102296)",
            "Insert(target_node=IN(type=boolean_operator), node=('identifier', 'mps_backend_registered'), position=0, insert_id=102297)",
            "Insert(target_node=IN(type=boolean_operator), node=('and', 'and'), position=1, insert_id=102298)",
            "Move(target_node=IN(type=boolean_operator), node=ASTNode(type=call), position=2)"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 22,
        "number": 6511,
        "neg_line": [
            "-torch_device = \"mps\" if torch.backends.mps.is_available() else torch_device"
        ],
        "pos_line": [
            "+# Some builds of torch 1.12 don't have the mps backend registered. See #892 for more details",
            "+mps_backend_registered = hasattr(torch.backends, \"mps\")",
            "+torch_device = \"mps\" if (mps_backend_registered and torch.backends.mps.is_available()) else torch_device"
        ],
        "core_change": "-torch_device = \"mps\" if torch.backends.mps.is_available() else torch_device +# Some builds of torch 1.12 don't have the mps backend registered. See #892 for more details +mps_backend_registered = hasattr(torch.backends, \"mps\") +torch_device = \"mps\" if (mps_backend_registered and torch.backends.mps.is_available()) else torch_device",
        "core_API": "is_available"
    },
    {
        "commit_hash": "9f3a18b05391ad20420fcd7dbe8603b18394ec8c",
        "index": "6050faa13..35f391517 100644",
        "commit_message": "fixed global\n\n",
        "file": "PySyft.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "\"torch.Size([5])\"",
            "]",
            "},",
            "-     \"execution_count\": 10,",
            "+     \"execution_count\": 11,",
            "\"metadata\": {},",
            "\"output_type\": \"execute_result\"",
            "}"
        ],
        "hunk_index": 5,
        "AST_diff": [
            "Update(target_node=ASTNode(type=integer, text=10), value='11')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 6515,
        "neg_line": [
            "-\"execution_count\": 10,"
        ],
        "pos_line": [
            "+\"execution_count\": 11,"
        ],
        "core_change": "-\"execution_count\": 10, +\"execution_count\": 11,",
        "core_API": "Size"
    },
    {
        "commit_hash": "d2c82881aaa13202075a08a2b20eb9170401f2ca",
        "index": "b50576a21..aad83a243 100644",
        "commit_message": "fix for alphabet import\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ConvDecoder(AbsDecoder):",
            ")",
            "",
            "def forward(self, input: torch.Tensor, ilens: torch.Tensor):",
            "-        \"\"\"",
            "+        \"\"\"Forward.",
            "+",
            "Args:",
            "-            input (torch.Tensor): spectrum [Batch, T, F]",
            "-            ilens (torch.Tensor): input lengths [Batch]",
            "+        input (torch.Tensor): spectrum [Batch, T, F]",
            "+        ilens (torch.Tensor): input lengths [Batch]",
            "\"\"\"",
            "input = input.transpose(1, 2)",
            "batch_size = input.shape[0]"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"\"\"\nArgs:\n            input (torch.Tensor): spectrum [Batch, T, F]\n            ilens (torch.Tensor): input lengths [Batch]\n\"\"\"), value='\"\"\"Forward.\\n\\nArgs:\\n        input (torch.Tensor): spectrum [Batch, T, F]\\n        ilens (torch.Tensor): input lengths [Batch]\\n\"\"\"')"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 1,
        "number": 6516,
        "neg_line": [
            "-\"\"\"",
            "-input (torch.Tensor): spectrum [Batch, T, F]",
            "-ilens (torch.Tensor): input lengths [Batch]"
        ],
        "pos_line": [
            "+\"\"\"Forward.",
            "+",
            "+input (torch.Tensor): spectrum [Batch, T, F]",
            "+ilens (torch.Tensor): input lengths [Batch]"
        ],
        "core_change": "-\"\"\" +\"\"\"Forward. + -input (torch.Tensor): spectrum [Batch, T, F] -ilens (torch.Tensor): input lengths [Batch] +input (torch.Tensor): spectrum [Batch, T, F] +ilens (torch.Tensor): input lengths [Batch]",
        "core_API": "transpose"
    },
    {
        "commit_hash": "1f26bd7dc71098d9e7e390a5941113122ff8c35c",
        "index": "7dca2176..337634dd 100644",
        "commit_message": "Fix lint errors\n\n",
        "file": "deeplake.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tensorflow",
        "change": [
            "def from_tfds_to_path(tfds_dataset_name: str, split: str, hub_ds_path: str, batc",
            "return from_tfds(tfds_ds=tfds_ds, ds=ds)",
            "",
            "",
            "-def from_tfds(tfds_ds: (tensorflow.data.Dataset), ds: (Dataset)):",
            "+def from_tfds(tfds_ds: tensorflow.data.Dataset, ds: Dataset):",
            "+    \"\"\"Converts a tfds dataset to hub dataset",
            "+    Args:",
            "+        tfds_ds (tensorflow.data.Dataset): A tfds_dataset object.",
            "+        ds (Dataset) : A Hub dataset object where Tensor will be created.",
            "+    Returns:",
            "+        A hub dataset",
            "+    \"\"\"",
            "tfds_numpy = tfds.as_numpy(tfds_ds)  # Convert `tf.data.Dataset` to Python generator",
            "",
            "for ex in tqdm(tfds_numpy):"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=1, insert_id=2398855)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=2398856)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'tfds_numpy'), position=0, insert_id=2398857)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type==, text==), position=1)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=call), position=2)",
            "Insert(target_node=ASTNode(type=assignment), node=('ERROR', None), position=1, insert_id=2398858)",
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=:, text=:), position=2)",
            "Move(target_node=ASTNode(type=pattern_list), node=ASTNode(type=attribute), position=0)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=:, text=:), position=0)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=identifier, text=Dataset), position=1)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=), text=)), position=2)",
            "Insert(target_node=ASTNode(type=type), node=('string', '\"\"\"Converts a tfds dataset to hub dataset\\n    Args:\\n        tfds_ds (tensorflow.data.Dataset): A tfds_dataset object.\\n        ds (Dataset) : A Hub dataset object where Tensor will be created.\\n    Returns:\\n        A hub dataset\\n    \"\"\"'), position=0, insert_id=2398859)",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=tuple_pattern))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=parenthesized_expression))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=identifier, text=tfds_numpy))"
        ],
        "plus_line": 7,
        "minus_line": 0,
        "AST_diff_line": 20,
        "number": 6517,
        "neg_line": [
            "-def from_tfds(tfds_ds: (tensorflow.data.Dataset), ds: (Dataset)):"
        ],
        "pos_line": [
            "+def from_tfds(tfds_ds: tensorflow.data.Dataset, ds: Dataset):",
            "+\"\"\"Converts a tfds dataset to hub dataset",
            "+Args:",
            "+tfds_ds (tensorflow.data.Dataset): A tfds_dataset object.",
            "+ds (Dataset) : A Hub dataset object where Tensor will be created.",
            "+Returns:",
            "+A hub dataset",
            "+\"\"\""
        ],
        "core_change": "-def from_tfds(tfds_ds: (tensorflow.data.Dataset), ds: (Dataset)): +def from_tfds(tfds_ds: tensorflow.data.Dataset, ds: Dataset): +\"\"\"Converts a tfds dataset to hub dataset +Args: +tfds_ds (tensorflow.data.Dataset): A tfds_dataset object. +ds (Dataset) : A Hub dataset object where Tensor will be created. +Returns: +A hub dataset +\"\"\"",
        "core_API": "as_numpy"
    },
    {
        "commit_hash": "e0ac72b7bd2cbed43164b69308c2cf546e1a49a5",
        "index": "e79a7d599..f323b9091 100755",
        "commit_message": "Fix PerceiverMLP and test (#16405)\n\nCo-authored-by: Jaesun Park <jaesun.park1@navercorp.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class PerceiverMLP(nn.Module):",
            "self.intermediate_act_fn = ACT2FN[config.hidden_act]",
            "else:",
            "self.intermediate_act_fn = config.hidden_act",
            "-        self.dense2 = nn.Linear(input_size, input_size)",
            "+        self.dense2 = nn.Linear(widening_factor * input_size, input_size)",
            "",
            "def forward(self, hidden_states):",
            "hidden_states = self.dense1(hidden_states)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('binary_operator', None), position=1, insert_id=1535691)",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'input_size'), position=4, insert_id=1535692)",
            "Insert(target_node=IN(type=binary_operator), node=('identifier', 'widening_factor'), position=0, insert_id=1535693)",
            "Insert(target_node=IN(type=binary_operator), node=('*', '*'), position=1, insert_id=1535694)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=identifier, text=input_size), position=2)",
            "Delete(target_node=ASTNode(type=identifier, text=input_size))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 6518,
        "neg_line": [
            "-self.dense2 = nn.Linear(input_size, input_size)"
        ],
        "pos_line": [
            "+self.dense2 = nn.Linear(widening_factor * input_size, input_size)"
        ],
        "core_change": "-self.dense2 = nn.Linear(input_size, input_size) +self.dense2 = nn.Linear(widening_factor * input_size, input_size)",
        "core_API": "Linear"
    },
    {
        "commit_hash": "13e0ec397338985dfba03f0d759316cb5e3c680a",
        "index": "ac09a7a5..b11b6acb 100644",
        "commit_message": "Use NHWC in batch_norm when shape is 2d. fix #190\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def BatchNorm(x, use_local_stat=None, decay=0.9, epsilon=1e-5,",
            "return tf.identity(xn, name='output')",
            "",
            "",
            "+# TODO support NCHW",
            "@layer_register(log_shape=False)",
            "def BatchRenorm(x, rmax, dmax, decay=0.9, epsilon=1e-5,",
            "use_scale=True, use_bias=True):"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 0,
        "minus_line": 0,
        "AST_diff_line": 17,
        "number": 6519,
        "neg_line": [],
        "pos_line": [
            "+# TODO support NCHW"
        ],
        "core_change": "+# TODO support NCHW",
        "core_API": "identity"
    },
    {
        "commit_hash": "c0b2e80ab882878b41361eb14241eae381fe6da5",
        "index": "4d00f53a..9e25ea09 100644",
        "commit_message": "Refactor YOLOX (#6443)\n\n* Fix aug test error when the number of prediction bboxes is 0 (#6398)\n\n* Fix aug test error when the number of prediction bboxes is 0\n\n* test\n\n* test\n\n* fix lint\n\n* Support custom pin_memory and persistent_workers\n\n* [Docs] Chinese version of robustness_benchmarking.md (#6375)\n\n* Chinese version of robustness_benchmarking.md\n\n* Update docs_zh-CN/robustness_benchmarking.md\n\nCo-authored-by: RangiLyu <lyuchqi@gmail.com>\n\n* Update docs_zh-CN/robustness_benchmarking.md\n\nCo-authored-by: RangiLyu <lyuchqi@gmail.com>\n\n* Update docs_zh-CN/robustness_benchmarking.md\n\nCo-authored-by: RangiLyu <lyuchqi@gmail.com>\n\n* Update docs_zh-CN/robustness_benchmarking.md\n\nCo-authored-by: RangiLyu <lyuchqi@gmail.com>\n\n* Update docs_zh-CN/robustness_benchmarking.md\n\nCo-authored-by: RangiLyu <lyuchqi@gmail.com>\n\n* Update docs_zh-CN/robustness_benchmarking.md\n\nCo-authored-by: RangiLyu <lyuchqi@gmail.com>\n\n* Update robustness_benchmarking.md\n\n* Update robustness_benchmarking.md\n\n* Update robustness_benchmarking.md\n\n* Update robustness_benchmarking.md\n\n* Update robustness_benchmarking.md\n\n* Update robustness_benchmarking.md\n\nCo-authored-by: RangiLyu <lyuchqi@gmail.com>\n\n* update yolox_s\n\n* update yolox_s\n\n* support dynamic eval interval\n\n* fix some error\n\n* support ceph\n\n* fix none error\n\n* fix batch error\n\n* replace resize\n\n* fix comment\n\n* fix docstr\n\n* Update the link of checkpoints (#6460)\n\n* [Feature]: Support plot confusion matrix. (#6344)\n\n* remove pin_memory\n\n* update\n\n* fix unittest\n\n* update cfg\n\n* fix error\n\n* add unittest\n\n* [Fix] Fix SpatialReductionAttention in PVT. (#6488)\n\n* [Fix] Fix SpatialReductionAttention in PVT\n\n* Add warning\n\n* Save coco summarize print information to logger (#6505)\n\n* Fix type error in 2_new_data_mode (#6469)\n\n* Always map location to cpu when load checkpoint (#6405)\n\n* configs: update groie README (#6401)\n\nSigned-off-by: Leonardo Rossi <leonardo.rossi@unipr.it>\n\n* [Fix] fix config path in docs (#6396)\n\n* [Enchance] Set a random seed when the user does not set a seed. (#6457)\n\n* fix random seed bug\n\n* add comment\n\n* enchance random seed\n\n* rename\n\nCo-authored-by: Haobo Yuan <yuanhaobo@whu.edu.cn>\n\n* [BugFixed] fix wrong trunc_normal_init use (#6432)\n\n* fix wrong trunc_normal_init use\n\n* fix wrong trunc_normal_init use\n\n* fix #6446\n\nCo-authored-by: Uno Wu <st9007a@gmail.com>\nCo-authored-by: Leonardo Rossi <leonardo.rossi@unipr.it>\nCo-authored-by: BigDong <yudongwang@tju.edu.cn>\nCo-authored-by: Haian Huang(深度眸) <1286304229@qq.com>\nCo-authored-by: Haobo Yuan <yuanhaobo@whu.edu.cn>\nCo-authored-by: Shusheng Yang <shusheng.yang@qq.com>\n\n* bump version to v2.18.1 (#6510)\n\n* bump version to v2.18.1\n\n* Update changelog.md\n\n* add some comment\n\n* fix some comment\n\n* update readme\n\n* fix lint\n\n* add reduce mean\n\n* update\n\n* update readme\n\n* update params\n\nCo-authored-by: Cedric Luo <luochunhua1996@outlook.com>\nCo-authored-by: RangiLyu <lyuchqi@gmail.com>\nCo-authored-by: Guangchen Lin <347630870@qq.com>\nCo-authored-by: Andrea Panizza <8233615+AndreaPi@users.noreply.github.com>\nCo-authored-by: Uno Wu <st9007a@gmail.com>\nCo-authored-by: Leonardo Rossi <leonardo.rossi@unipr.it>\nCo-authored-by: BigDong <yudongwang@tju.edu.cn>\nCo-authored-by: Haobo Yuan <yuanhaobo@whu.edu.cn>\nCo-authored-by: Shusheng Yang <shusheng.yang@qq.com>\n",
        "file": "mmdetection.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SimOTAAssigner(BaseAssigner):",
            "def dynamic_k_matching(self, cost, pairwise_ious, num_gt, valid_mask):",
            "matching_matrix = torch.zeros_like(cost)",
            "# select candidate topk ious for dynamic-k calculation",
            "-        topk_ious, _ = torch.topk(pairwise_ious, self.candidate_topk, dim=0)",
            "+        candidate_topk = min(self.candidate_topk, pairwise_ious.size(0))",
            "+        topk_ious, _ = torch.topk(pairwise_ious, candidate_topk, dim=0)",
            "# calculate dynamic k for each gt",
            "dynamic_ks = torch.clamp(topk_ious.sum(0).int(), min=1)",
            "for gt_idx in range(num_gt):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=618941)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=618942)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'candidate_topk'), position=0, insert_id=618943)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=618944)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=618945)",
            "Insert(target_node=IN(type=call), node=('identifier', 'min'), position=0, insert_id=618946)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=618947)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=618948)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=attribute), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=618949)",
            "Insert(target_node=IN(type=argument_list), node=('call', None), position=3, insert_id=618950)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=618951)",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'candidate_topk'), position=3, insert_id=618952)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=618953)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=618954)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'pairwise_ious'), position=0, insert_id=618955)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=618956)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'size'), position=2, insert_id=618957)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=618958)",
            "Insert(target_node=IN(type=argument_list), node=('integer', '0'), position=1, insert_id=618959)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=618960)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 21,
        "number": 6526,
        "neg_line": [
            "-topk_ious, _ = torch.topk(pairwise_ious, self.candidate_topk, dim=0)"
        ],
        "pos_line": [
            "+candidate_topk = min(self.candidate_topk, pairwise_ious.size(0))",
            "+topk_ious, _ = torch.topk(pairwise_ious, candidate_topk, dim=0)"
        ],
        "core_change": "-topk_ious, _ = torch.topk(pairwise_ious, self.candidate_topk, dim=0) +candidate_topk = min(self.candidate_topk, pairwise_ious.size(0)) +topk_ious, _ = torch.topk(pairwise_ious, candidate_topk, dim=0)",
        "core_API": "zeros_like"
    },
    {
        "commit_hash": "5754fa7a961b4b6dd7651436bd29dd5712bc134f",
        "index": "9696778..01cbdd7 100644",
        "commit_message": "Fixes to Multihead Attention with LayerNorm and Dropout-Add (#860)\n\n\n",
        "file": "apex.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class EncdecMultiheadAttn(nn.Module):",
            "self.register_parameter('lyr_norm_beta_weights', None)",
            "self.lyr_nrm_gamma_weights = None",
            "self.lyr_nrm_beta_weights  = None",
            "-                self.lyr_nrm = torch.nn.LayerNorm(embed_dim)",
            "+                self.lyr_nrm = FusedLayerNorm(embed_dim)",
            "self.reset_parameters()",
            "-",
            "+",
            "if self.include_norm_add:",
            "if   impl == 'fast'    : self.attn_func = fast_encdec_attn_norm_add_func",
            "elif impl == 'default' : self.attn_func = encdec_attn_func"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=torch), value='FusedLayerNorm')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=torch), position=0)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=nn))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=LayerNorm))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 6528,
        "neg_line": [
            "-self.lyr_nrm = torch.nn.LayerNorm(embed_dim)",
            "-"
        ],
        "pos_line": [
            "+self.lyr_nrm = FusedLayerNorm(embed_dim)",
            "+"
        ],
        "core_change": "-self.lyr_nrm = torch.nn.LayerNorm(embed_dim) +self.lyr_nrm = FusedLayerNorm(embed_dim) - +",
        "core_API": "register_parameter"
    },
    {
        "commit_hash": "53ed4b0c7beed94f5caa176facb9a7ad151981ba",
        "index": "c012c273..9fe1bef6 100644",
        "commit_message": "Add test-cuda make target, and fix cuda bugs (#338)\n\n* Add test-cuda make target; fix cuda bugs\n\n* Fix more cuda bugs; distributions now passes\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class AutoRegressiveNN(nn.Module):",
            "",
            "if mask_encoding is None:",
            "# the dependency structure is chosen at random",
            "-            self.mask_encoding = 1 + torch.multinomial(torch.ones(input_dim - 1) / (input_dim - 1),",
            "+            self.mask_encoding = 1 + torch_multinomial(torch.ones(input_dim - 1) / (input_dim - 1),",
            "num_samples=hidden_dim, replacement=True)",
            "else:",
            "# the dependency structure is given by the user"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=torch), value='torch_multinomial')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=torch), position=0)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=multinomial))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 6529,
        "neg_line": [
            "-self.mask_encoding = 1 + torch.multinomial(torch.ones(input_dim - 1) / (input_dim - 1),"
        ],
        "pos_line": [
            "+self.mask_encoding = 1 + torch_multinomial(torch.ones(input_dim - 1) / (input_dim - 1),"
        ],
        "core_change": "-self.mask_encoding = 1 + torch.multinomial(torch.ones(input_dim - 1) / (input_dim - 1), +self.mask_encoding = 1 + torch_multinomial(torch.ones(input_dim - 1) / (input_dim - 1),",
        "core_API": "multinomial"
    },
    {
        "commit_hash": "a911f2ecf4eeff06ad4cfbf999f0ddf3e31f131e",
        "index": "97d8b8e8..4736f48d 100644",
        "commit_message": "refactor(internal): move model store to common store implementation (#2029)\n\n* delete models/store.py\n\n* feat(internal): add Model class\n\n* feat(internal): expose new model APIs\n\n* fix(internal): allow stores to be initialized with an FS\n\n* update internal modules to use new models\n\n* refactor(internal): refactor frameworks to use new modelstore\n\n* test: fix integration tests\n\n* fix(internal): fix SysPath lint\n\n* fix(internal): remove use of cached property\n\n* feat(internal): add bentoml.models.create\n\n* fix(internal): use shutil.move over os.rename\n\n* chore: patch\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\nci: don't use docker (#2045)\n\n* ci: don't use docker for checks\n* ci: remove docker from Makefile\n\nCo-authored-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\nchore: drop python 3.6 support (#2046)\n\nchore: enable pyright for testing (#2047)\n\nfix: revert&fix typing of di containers (#2044)\n\n* fix: revert&fix typing of di containers\n\n* addpend\n\n* fix: dependencies issues\n\nfix: type\n\nfix: tests\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix: CI for onnxmlir installation\n\nfix: onnxmlir tests and move to attrs\n\nfix(onnxmlir): activate env\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* chore: uses attr.define instead\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* chore: remove poetry\n\nfix: simple_di bug doesn't seem to work with attr.define\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix: flake8\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix: None check behaviour\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* revert\n\nCo-authored-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\nfix: schema typing (#2048)\n\nfix(internal): BentoInfo: don't try to parse creation time again (#2049)\n\n* fix: MLflow models\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix: onnxmlir\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix(wip): paddlepaddle\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix: paddle\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix: spacy\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix: tensorflow\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix: transformers\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* delete models/store.py\n\n* feat(internal): add Model class\n\n* feat(internal): expose new model APIs\n\n* fix(internal): allow stores to be initialized with an FS\n\n* update internal modules to use new models\n\n* refactor(internal): refactor frameworks to use new modelstore\n\n* test: fix integration tests\n\n* fix(internal): fix SysPath lint\n\n* fix(internal): remove use of cached property\n\n* feat(internal): add bentoml.models.create\n\n* fix(internal): use shutil.move over os.rename\n\n* chore: patch\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\nci: don't use docker (#2045)\n\n* ci: don't use docker for checks\n* ci: remove docker from Makefile\n\nCo-authored-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\nchore: drop python 3.6 support (#2046)\n\nchore: enable pyright for testing (#2047)\n\nfix: revert&fix typing of di containers (#2044)\n\n* fix: revert&fix typing of di containers\n\n* addpend\n\n* fix: dependencies issues\n\nfix: type\n\nfix: tests\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix: CI for onnxmlir installation\n\nfix: onnxmlir tests and move to attrs\n\nfix(onnxmlir): activate env\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* chore: uses attr.define instead\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* chore: remove poetry\n\nfix: simple_di bug doesn't seem to work with attr.define\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix: flake8\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix: None check behaviour\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* revert\n\nCo-authored-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\nfix: schema typing (#2048)\n\nfix(internal): BentoInfo: don't try to parse creation time again (#2049)\n\n* fix: MLflow models\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix: onnxmlir\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix(wip): paddlepaddle\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix: paddle\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix: spacy\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix: tensorflow\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix: transformers\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* chore: fix lint errors\n\n* Update test_onnx_impl.py\n\n* chore: ignore typing\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix: CI\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\nCo-authored-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n",
        "file": "BentoML.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "models",
        "change": [
            "logger = logging.getLogger(__name__)",
            "def list(  # pylint: disable=redefined-builtin",
            "tag: t.Optional[t.Union[Tag, str]] = None,",
            "_bento_store: \"BentoStore\" = Provide[BentoMLContainer.bento_store],",
            "-) -> t.List[SysPathBento]:",
            "+) -> \"t.List[SysPathBento]\":",
            "return _bento_store.list(tag)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('type', None), position=0, insert_id=2650907)",
            "Insert(target_node=IN(type=type), node=('string', '\"t.List[SysPathBento]\"'), position=0, insert_id=2650908)",
            "Delete(target_node=ASTNode(type=identifier, text=t))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=List))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=identifier, text=SysPathBento))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))",
            "Delete(target_node=ASTNode(type=type))"
        ],
        "plus_line": 0,
        "minus_line": 0,
        "AST_diff_line": 11,
        "number": 6534,
        "neg_line": [
            "-) -> t.List[SysPathBento]:"
        ],
        "pos_line": [
            "+) -> \"t.List[SysPathBento]\":"
        ],
        "core_change": "-) -> t.List[SysPathBento]: +) -> \"t.List[SysPathBento]\":",
        "core_API": "getLogger"
    },
    {
        "commit_hash": "038c439b5275dcf9674b07440207035509d26629",
        "index": "e4acd70c..7c2f9914 100755",
        "commit_message": "Graph summary, no assertions option, deprecated exception, fixes and improvements\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class NaturalGradient(Optimizer):",
            "]",
            "",
            "# delta' * grad(kldiv)",
            "-                multiply = functools.partial(tf_util.lift_indexedslices, tf.math.multiply)",
            "+                multiply = functools.partial(",
            "+                    tf_util.lift_indexedslices, tf.math.multiply,",
            "+                    with_assertions=self.config.create_tf_assertions",
            "+                )",
            "delta_kldiv_grads = tf.math.add_n(inputs=[",
            "tf.math.reduce_sum(input_tensor=multiply(delta, grad))",
            "for delta, grad in zip(deltas.values(), kldiv_grads)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=2224667)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=2224668)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'with_assertions'), position=0, insert_id=2224669)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2224670)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=2224671)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2224672)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2224673)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'create_tf_assertions'), position=2, insert_id=2224674)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=2224675)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2224676)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'config'), position=2, insert_id=2224677)"
        ],
        "plus_line": 4,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 6535,
        "neg_line": [
            "-multiply = functools.partial(tf_util.lift_indexedslices, tf.math.multiply)"
        ],
        "pos_line": [
            "+multiply = functools.partial(",
            "+tf_util.lift_indexedslices, tf.math.multiply,",
            "+with_assertions=self.config.create_tf_assertions",
            "+)"
        ],
        "core_change": "-multiply = functools.partial(tf_util.lift_indexedslices, tf.math.multiply) +multiply = functools.partial( +tf_util.lift_indexedslices, tf.math.multiply, +with_assertions=self.config.create_tf_assertions +)",
        "core_API": "partial"
    },
    {
        "commit_hash": "88cc1dfa41b02b3021bbfa26b30ed8859e2d4290",
        "index": "dfeb966..7f930df 100644",
        "commit_message": "Fix for issue with Torch Library - \"RuntimeError: some of the strides of a given numpy array are negative. This is currently not supported, but will be added in future releases.\"\n\n",
        "file": "face-alignment.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def detect(net, img, device):",
            "# Creates a batch of 1",
            "img = np.expand_dims(img, 0)",
            "",
            "-    img = torch.from_numpy(img).to(device, dtype=torch.float32)",
            "+    img = torch.from_numpy(img.copy()).to(device, dtype=torch.float32)",
            "",
            "return batch_detect(net, img, device)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=1, insert_id=199580)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=2, insert_id=199581)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=199582)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=199583)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=img), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=199584)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'copy'), position=2, insert_id=199585)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=199586)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=), text=)), position=1)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 6536,
        "neg_line": [
            "-img = torch.from_numpy(img).to(device, dtype=torch.float32)"
        ],
        "pos_line": [
            "+img = torch.from_numpy(img.copy()).to(device, dtype=torch.float32)"
        ],
        "core_change": "-img = torch.from_numpy(img).to(device, dtype=torch.float32) +img = torch.from_numpy(img.copy()).to(device, dtype=torch.float32)",
        "core_API": "expand_dims"
    },
    {
        "commit_hash": "c5b6ceea4a0a151ab78b9c065584bc416d1ee275",
        "index": "681c7099..01dd36c2 100644",
        "commit_message": "tl.layers API Refactoring and various modifications (#667)\n\n* Decorators API Refactored\n\n* extra_requires `all`, `all_cpu` and `all_gpu` added\n\n* Error fix\n\n* YAPF Formating Correction\n\n* Test for private method decorator added\n\n* Test Logging Verbosity Fixed to Debug when runned individually\n\n* YAPF corrections applied\n\n* Changelog Added\n\n* Changelog updated\n\n* PR number changed\n\n* First Refactoring Pass done\n\n* cleaning second pass\n\n* Refactoring 3rd pass\n\n* Refactoring 4th Pass\n\n* Code Error fix\n\n* YAPF Formating Fix\n\n* Arguments now using self\n\n* YAPF error correction\n\n* Bug Fix in Decorator\n\n* act name bug fix\n\n* Error Correction\n\n* YAPF formating fix\n\n* Useless tf.identity removed\n\n* Error Fix\n\n* Changelog Updated\n\n* Error fix in tl.activation\n\n* Documentation error fix\n\n* Lazy Import added\n\n* Import Refactoring with LazyImport when necessary\n\n* Changelog Updated\n\n* Gitter Removed\n\n* Fixed proposed by @zsdonghao\n\n* Documentation updated\n\n* Missing requirements added\n\n* Update to TensorLayer 1.8.6rc1\n\n* Requirements error fix\n\n* Docker Files updated\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Simple_MNIST_Test(CustomTestCase):",
            "# the softmax is implemented internally in tl.cost.cross_entropy(y, y_) to",
            "# speed up computation, so we use identity here.",
            "# see tf.nn.sparse_softmax_cross_entropy_with_logits()",
            "-        cls.network = tl.layers.DenseLayer(network, n_units=10, act=tf.identity, name='output')",
            "+        cls.network = tl.layers.DenseLayer(network, n_units=10, name='output')",
            "",
            "# define cost function and metric.",
            "y = cls.network.outputs"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=act))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=identity))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 6541,
        "neg_line": [
            "-cls.network = tl.layers.DenseLayer(network, n_units=10, act=tf.identity, name='output')"
        ],
        "pos_line": [
            "+cls.network = tl.layers.DenseLayer(network, n_units=10, name='output')"
        ],
        "core_change": "-cls.network = tl.layers.DenseLayer(network, n_units=10, act=tf.identity, name='output') +cls.network = tl.layers.DenseLayer(network, n_units=10, name='output')",
        "core_API": "cross_entropy"
    },
    {
        "commit_hash": "999846b2763b60f70df53f5844ef07857cf5f5aa",
        "index": "14ed331e..98b3959c 100644",
        "commit_message": "[MaskRCNN] Support empty training images (fix #1256,#1230)\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def unpackbits_masks(masks):",
            "unpacked = tf.bitwise.bitwise_and(tf.expand_dims(masks, -1), bits) > 0",
            "unpacked = tf.reshape(",
            "unpacked,",
            "-        tf.concat([tf.shape(masks)[:-1], [-1]], axis=0))",
            "+        tf.concat([tf.shape(masks)[:-1], [8 * tf.shape(masks)[-1]]], axis=0))",
            "return unpacked"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=list), node=('binary_operator', None), position=1, insert_id=2274617)",
            "Insert(target_node=ASTNode(type=list), node=(']', ']'), position=2, insert_id=2274618)",
            "Insert(target_node=IN(type=binary_operator), node=('integer', '8'), position=0, insert_id=2274619)",
            "Insert(target_node=IN(type=binary_operator), node=('*', '*'), position=1, insert_id=2274620)",
            "Insert(target_node=IN(type=binary_operator), node=('subscript', None), position=2, insert_id=2274621)",
            "Insert(target_node=IN(type=subscript), node=('call', None), position=0, insert_id=2274622)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=2274623)",
            "Move(target_node=IN(type=subscript), node=ASTNode(type=unary_operator, text=-1), position=2)",
            "Move(target_node=IN(type=subscript), node=ASTNode(type=], text=]), position=3)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2274624)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2274625)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2274626)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2274627)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'shape'), position=2, insert_id=2274628)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2274629)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'masks'), position=1, insert_id=2274630)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2274631)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 6546,
        "neg_line": [
            "-tf.concat([tf.shape(masks)[:-1], [-1]], axis=0))"
        ],
        "pos_line": [
            "+tf.concat([tf.shape(masks)[:-1], [8 * tf.shape(masks)[-1]]], axis=0))"
        ],
        "core_change": "-tf.concat([tf.shape(masks)[:-1], [-1]], axis=0)) +tf.concat([tf.shape(masks)[:-1], [8 * tf.shape(masks)[-1]]], axis=0))",
        "core_API": "bitwise_and"
    },
    {
        "commit_hash": "cdb7eeca466b139849aba76e0b467b1cba823b3c",
        "index": "6ce859659..a0a45b9dd 100644",
        "commit_message": "Fix `ConditionalDetrForSegmentation` doc example (#20531)\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "models",
        "change": [
            "class ConditionalDetrForSegmentation(ConditionalDetrPreTrainedModel):",
            "...     ConditionalDetrConfig,",
            "...     ConditionalDetrForSegmentation,",
            "... )",
            "-        >>> from transformers.models.conditional_detr.feature_extraction_conditional_detr import rgb_to_id",
            "+        >>> from transformers.image_transforms import rgb_to_id",
            "",
            ">>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"",
            ">>> image = Image.open(requests.get(url, stream=True).raw)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=models), value='image_transforms')",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=conditional_detr))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=feature_extraction_conditional_detr))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 6549,
        "neg_line": [
            "->>> from transformers.models.conditional_detr.feature_extraction_conditional_detr import rgb_to_id"
        ],
        "pos_line": [
            "+>>> from transformers.image_transforms import rgb_to_id"
        ],
        "core_change": "->>> from transformers.models.conditional_detr.feature_extraction_conditional_detr import rgb_to_id +>>> from transformers.image_transforms import rgb_to_id",
        "core_API": "open"
    },
    {
        "commit_hash": "b74d91f01737ff52a20759d2891db2a0e073dfac",
        "index": "4dd5323..a155f10 100644",
        "commit_message": "Fix bug in _take_channels (#148)\n\n\n",
        "file": "segmentation_models.pytorch.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def _take_channels(*xs, ignore_channels=None):",
            "return xs",
            "else:",
            "channels = [channel for channel in range(xs[0].shape[1]) if channel not in ignore_channels]",
            "-        xs = [torch.index_select(x, dim=1, index=torch.tensor(channels)) for x in xs]",
            "+        xs = [torch.index_select(x, dim=1, index=torch.tensor(channels).to(x.device)) for x in xs]",
            "return xs"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1131287)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=1131288)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1131289)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to'), position=2, insert_id=1131290)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1131291)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=1131292)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1131293)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'x'), position=0, insert_id=1131294)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1131295)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=1131296)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 6551,
        "neg_line": [
            "-xs = [torch.index_select(x, dim=1, index=torch.tensor(channels)) for x in xs]"
        ],
        "pos_line": [
            "+xs = [torch.index_select(x, dim=1, index=torch.tensor(channels).to(x.device)) for x in xs]"
        ],
        "core_change": "-xs = [torch.index_select(x, dim=1, index=torch.tensor(channels)) for x in xs] +xs = [torch.index_select(x, dim=1, index=torch.tensor(channels).to(x.device)) for x in xs]",
        "core_API": "index_select"
    },
    {
        "commit_hash": "efa5b04797ae0e881eef2d4c72f618fae066dd12",
        "index": "8f1d4972..e7dbf33d 100644",
        "commit_message": "Style fix\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "_MANUAL_VAR_INIT = False",
            "def clear_session():",
            "global _SESSION",
            "global _LEARNING_PHASE",
            "-    reset_default_graph()",
            "+    tf.reset_default_graph()",
            "reset_uids()",
            "_SESSION = None",
            "_LEARNING_PHASE = tf.placeholder(dtype='uint8', name='keras_learning_phase')"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2116691)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2116692)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2116693)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=reset_default_graph), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 6553,
        "neg_line": [
            "-reset_default_graph()"
        ],
        "pos_line": [
            "+tf.reset_default_graph()"
        ],
        "core_change": "-reset_default_graph() +tf.reset_default_graph()",
        "core_API": "reset_default_graph"
    },
    {
        "commit_hash": "8cb1ed1efe5c756991332f97949720d2cc98834c",
        "index": "3f497f4aa..4e8744fee 100644",
        "commit_message": "fixed positional_dropout_rate\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Transformer(TTSInterface, torch.nn.Module):",
            "]",
            "if self.use_scaled_pos_enc:",
            "report_keys += [",
            "-                {\"encoder_alpha\": self.encoder.embed[-2].alpha.data.item()},",
            "-                {\"decoder_alpha\": self.decoder.embed[-2].alpha.data.item()},",
            "+                {\"encoder_alpha\": self.encoder.embed[-1].alpha.data.item()},",
            "+                {\"decoder_alpha\": self.decoder.embed[-1].alpha.data.item()},",
            "]",
            "self.reporter.report(report_keys)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=173350)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=argument_list), position=1)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=173351)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=173352)",
            "Update(target_node=ASTNode(type=unary_operator, text=-2), value='-1')",
            "Update(target_node=ASTNode(type=unary_operator, text=-2), value='-1')",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 9,
        "number": 6559,
        "neg_line": [
            "-{\"encoder_alpha\": self.encoder.embed[-2].alpha.data.item()},",
            "-{\"decoder_alpha\": self.decoder.embed[-2].alpha.data.item()},"
        ],
        "pos_line": [
            "+{\"encoder_alpha\": self.encoder.embed[-1].alpha.data.item()},",
            "+{\"decoder_alpha\": self.decoder.embed[-1].alpha.data.item()},"
        ],
        "core_change": "-{\"encoder_alpha\": self.encoder.embed[-2].alpha.data.item()}, -{\"decoder_alpha\": self.decoder.embed[-2].alpha.data.item()}, +{\"encoder_alpha\": self.encoder.embed[-1].alpha.data.item()}, +{\"decoder_alpha\": self.decoder.embed[-1].alpha.data.item()},",
        "core_API": "item"
    },
    {
        "commit_hash": "34263ab196cbfab65f2afa1150e6cf6db5ffd8b6",
        "index": "201b2ebf4..27a3947d6 100755",
        "commit_message": "rnn->encoders, removes _th suffix, same filenames for chainer and pytorch, modify tests to not rely on _th\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def main():",
            "# extract",
            "logging.info('backend = ' + args.backend)",
            "if args.backend == \"pytorch\":",
            "-        from espnet.tts.pytorch.tts_pytorch import decode",
            "+        from espnet.tts.pytorch.tts import decode",
            "decode(args)",
            "else:",
            "raise NotImplementedError(\"Only pytorch is supported.\")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tts_pytorch), value='tts')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 6563,
        "neg_line": [
            "-from espnet.tts.pytorch.tts_pytorch import decode"
        ],
        "pos_line": [
            "+from espnet.tts.pytorch.tts import decode"
        ],
        "core_change": "-from espnet.tts.pytorch.tts_pytorch import decode +from espnet.tts.pytorch.tts import decode",
        "core_API": "info"
    },
    {
        "commit_hash": "f6f2435b91ed0bf5b4a2ac2f114a01b35b0e2f15",
        "index": "a7c9ba327..3952f4ea8 100644",
        "commit_message": "[SGD] Sgd v2 Dataset Integration (#17626)\n\n* wip\n\n* wip\n\n* wip\n\n* draft\n\n* disable tf autosharding\n\n* wip\n\n* wip\n\n* wip\n\n* wip\n\n* add example\n\n* wip\n\n* wip\n\n* wip\n\n* use dataset.split\n\n* add unit tests\n\n* add linear example\n\n* concatenate tensors and fix example\n\n* WIP tune example\n\n* add tensorflow example\n\n* wip\n\n* random_shuffle_each_window\n\n* fault tolerance test\n\n* GPU, examples, CI\n\n* formatting\n\n* fix\n\n* Update python/ray/util/sgd/v2/tests/test_trainer.py\n\nCo-authored-by: matthewdeng <matthew.j.deng@gmail.com>\n\n* wip\n\n* type hints\n\n* wip\n\n* update user guide\n\n* fix\n\n* fix immediate issues\n\n* update example\n\n* update\n\n* fix tune gpu test\n\n* fix resources for smoke test - 1 CPU for dataset tasks\n\n* update tests, docs, examples\n\n* Apply suggestions from code review\n\nCo-authored-by: Clark Zinzow <clarkzinzow@gmail.com>\n\n* address comments\n\n* add warning\n\n* fix tests\n\n* minor doc updates\n\n* update example in doc\n\n* configure tests\n\n* Update doc/source/raysgd/v2/user_guide.rst\n\nCo-authored-by: Clark Zinzow <clarkzinzow@gmail.com>\n\n* Update python/ray/data/dataset.py\n\nCo-authored-by: matthewdeng <matthew.j.deng@gmail.com>\n\n* fix docstring\n\nCo-authored-by: Matthew Deng <matthew.j.deng@gmail.com>\nCo-authored-by: matthewdeng <matt@anyscale.com>\nCo-authored-by: Clark Zinzow <clarkzinzow@gmail.com>\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_to_torch_feature_columns(ray_start_regular_shared):",
            "iterations = []",
            "",
            "for batch in iter(torchd):",
            "-        iterations.append(torch.cat((*batch[0], batch[1]), axis=1).numpy())",
            "+        iterations.append(torch.cat((batch[0], batch[1]), dim=1).numpy())",
            "combined_iterations = np.concatenate(iterations)",
            "assert np.array_equal(df.values, combined_iterations)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=tuple), node=ASTNode(type=subscript), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=axis), value='dim')",
            "Delete(target_node=ASTNode(type=*, text=*))",
            "Delete(target_node=ASTNode(type=list_splat))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 6566,
        "neg_line": [
            "-iterations.append(torch.cat((*batch[0], batch[1]), axis=1).numpy())"
        ],
        "pos_line": [
            "+iterations.append(torch.cat((batch[0], batch[1]), dim=1).numpy())"
        ],
        "core_change": "-iterations.append(torch.cat((*batch[0], batch[1]), axis=1).numpy()) +iterations.append(torch.cat((batch[0], batch[1]), dim=1).numpy())",
        "core_API": "append"
    },
    {
        "commit_hash": "ba4d30c42e0702bd894c36777d7d2c0adf74516c",
        "index": "001374881..66a2797fb 100644",
        "commit_message": "Module namespace cleanup for v2.0 (#3875)\n\n* Imports cleaning\n\n* Small change\n\n* Remove unused methods\n\n* Small fix\n\n* Additional fix\n\n* Final fix\n\n* Fix benchmark test\n\n* Fix benchmark test #2\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class DownloadManager:",
            "\"\"\"",
            "Ship the files using Beam FileSystems to the pipeline temp dir.",
            "\"\"\"",
            "-        from datasets.utils.beam_utils import upload_local_to_remote",
            "+        from .beam_utils import upload_local_to_remote",
            "",
            "remote_dir = pipeline._options.get_all_options().get(\"temp_location\")",
            "if remote_dir is None:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=import_from_statement), node=('relative_import', None), position=1, insert_id=1779714)",
            "Insert(target_node=IN(type=relative_import), node=('import_prefix', None), position=0, insert_id=1779715)",
            "Insert(target_node=IN(type=relative_import), node=('dotted_name', None), position=1, insert_id=1779716)",
            "Move(target_node=IN(type=import_prefix), node=ASTNode(type=., text=.), position=0)",
            "Move(target_node=IN(type=dotted_name), node=ASTNode(type=identifier, text=beam_utils), position=0)",
            "Delete(target_node=ASTNode(type=identifier, text=datasets))",
            "Delete(target_node=ASTNode(type=identifier, text=utils))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=dotted_name))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 6570,
        "neg_line": [
            "-from datasets.utils.beam_utils import upload_local_to_remote"
        ],
        "pos_line": [
            "+from .beam_utils import upload_local_to_remote"
        ],
        "core_change": "-from datasets.utils.beam_utils import upload_local_to_remote +from .beam_utils import upload_local_to_remote",
        "core_API": "get_all_options"
    },
    {
        "commit_hash": "f30ab10dc6743066cbfdfa23c8d1a7b8d6193b06",
        "index": "1c9ba57..07726d3 100644",
        "commit_message": "fix incorrect class loss calculation. cross entropy being averaged twice due to PyTorch defaults\n\n",
        "file": "PyTorch-YOLOv3.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class YOLOLayer(nn.Module):",
            "loss_conf = self.bce_loss(pred_conf[conf_mask_false], tconf[conf_mask_false]) + self.bce_loss(",
            "pred_conf[conf_mask_true], tconf[conf_mask_true]",
            ")",
            "-            loss_cls = (1 / nB) * self.ce_loss(pred_cls[mask], torch.argmax(tcls[mask], 1))",
            "+            loss_cls = self.ce_loss(pred_cls[mask], torch.argmax(tcls[mask], 1))",
            "loss = loss_x + loss_y + loss_w + loss_h + loss_conf + loss_cls",
            "",
            "return ("
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=call), position=2)",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=/, text=/))",
            "Delete(target_node=ASTNode(type=identifier, text=nB))",
            "Delete(target_node=ASTNode(type=binary_operator))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=parenthesized_expression))",
            "Delete(target_node=ASTNode(type=*, text=*))",
            "Delete(target_node=ASTNode(type=binary_operator))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 6573,
        "neg_line": [
            "-loss_cls = (1 / nB) * self.ce_loss(pred_cls[mask], torch.argmax(tcls[mask], 1))"
        ],
        "pos_line": [
            "+loss_cls = self.ce_loss(pred_cls[mask], torch.argmax(tcls[mask], 1))"
        ],
        "core_change": "-loss_cls = (1 / nB) * self.ce_loss(pred_cls[mask], torch.argmax(tcls[mask], 1)) +loss_cls = self.ce_loss(pred_cls[mask], torch.argmax(tcls[mask], 1))",
        "core_API": "bce_loss"
    },
    {
        "commit_hash": "caed77f155e506124629403a2e8e4a339d1b99a8",
        "index": "49bc0755d..1740d529c 100644",
        "commit_message": "Refactor `TorchElasticEnvironment.detect` to use `torch.distributed.is_torchelastic_launched` (#12376)\n\n* Refactor TorchElasticEnvironment.detect to use native utility from torch.distributed\n\n* fix version and tests\n\n* fix version\n\n* Update tests/accelerators/test_accelerator_connector.py\n\nCo-authored-by: Adrian Wälchli <aedu.waelchli@gmail.com>\n\nCo-authored-by: Adrian Wälchli <aedu.waelchli@gmail.com>\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TorchElasticEnvironment(ClusterEnvironment):",
            "@staticmethod",
            "def detect() -> bool:",
            "\"\"\"Returns ``True`` if the current process was launched using the torchelastic command.\"\"\"",
            "+        if _TORCH_GREATER_EQUAL_1_9_1:",
            "+            return torch.distributed.is_torchelastic_launched()",
            "required_env_vars = {\"RANK\", \"GROUP_RANK\", \"LOCAL_RANK\", \"LOCAL_WORLD_SIZE\"}",
            "return required_env_vars.issubset(os.environ.keys())"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=3, insert_id=511717)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=511718)",
            "Insert(target_node=IN(type=if_statement), node=('identifier', '_TORCH_GREATER_EQUAL_1_9_1'), position=1, insert_id=511719)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=511720)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=511721)",
            "Insert(target_node=IN(type=block), node=('return_statement', None), position=0, insert_id=511722)",
            "Insert(target_node=IN(type=return_statement), node=('return', 'return'), position=0, insert_id=511723)",
            "Insert(target_node=IN(type=return_statement), node=('call', None), position=1, insert_id=511724)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=511725)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=511726)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=511727)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=511728)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'is_torchelastic_launched'), position=2, insert_id=511729)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=511730)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=511731)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=511732)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=511733)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'distributed'), position=2, insert_id=511734)"
        ],
        "plus_line": 2,
        "minus_line": 0,
        "AST_diff_line": 18,
        "number": 6578,
        "neg_line": [],
        "pos_line": [
            "+if _TORCH_GREATER_EQUAL_1_9_1:",
            "+return torch.distributed.is_torchelastic_launched()"
        ],
        "core_change": "+if _TORCH_GREATER_EQUAL_1_9_1: +return torch.distributed.is_torchelastic_launched()",
        "core_API": "is_torchelastic_launched"
    },
    {
        "commit_hash": "ddde8ca1bf3d26e3274b5b452a480271a1446e34",
        "index": "d1a4538..8326f9a 100644",
        "commit_message": "fix cosine  bbeta schedule, thanks to @Zhengxinyang\n\n",
        "file": "DALLE2-pytorch.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def cosine_beta_schedule(timesteps, s = 0.008):",
            "as proposed in https://openreview.net/forum?id=-NEXDKk8gZ",
            "\"\"\"",
            "steps = timesteps + 1",
            "-    x = torch.linspace(0, steps, steps)",
            "-    alphas_cumprod = torch.cos(((x / steps) + s) / (1 + s) * torch.pi * 0.5) ** 2",
            "+    x = torch.linspace(0, timesteps, steps)",
            "+    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * torch.pi * 0.5) ** 2",
            "alphas_cumprod = alphas_cumprod / alphas_cumprod[0]",
            "betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])",
            "return torch.clip(betas, 0, 0.999)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=steps), value='timesteps')",
            "Update(target_node=ASTNode(type=identifier, text=steps), value='timesteps')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 6580,
        "neg_line": [
            "-x = torch.linspace(0, steps, steps)",
            "-alphas_cumprod = torch.cos(((x / steps) + s) / (1 + s) * torch.pi * 0.5) ** 2"
        ],
        "pos_line": [
            "+x = torch.linspace(0, timesteps, steps)",
            "+alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * torch.pi * 0.5) ** 2"
        ],
        "core_change": "-x = torch.linspace(0, steps, steps) -alphas_cumprod = torch.cos(((x / steps) + s) / (1 + s) * torch.pi * 0.5) ** 2 +x = torch.linspace(0, timesteps, steps) +alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * torch.pi * 0.5) ** 2",
        "core_API": "linspace"
    },
    {
        "commit_hash": "286dc19a4f7a1cbae3bb101094bc8e76805de454",
        "index": "a546b731f..942ddde04 100755",
        "commit_message": "Fix IterableDataset with __len__ in Trainer (#8095)\n\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Trainer:",
            "dataset.set_format(type=dataset.format[\"type\"], columns=columns)",
            "",
            "def _get_train_sampler(self) -> Optional[torch.utils.data.sampler.Sampler]:",
            "-        if not isinstance(self.train_dataset, collections.abc.Sized):",
            "+        if isinstance(self.train_dataset, torch.utils.data.IterableDataset) or not isinstance(",
            "+            self.train_dataset, collections.abc.Sized",
            "+        ):",
            "return None",
            "elif is_torch_tpu_available():",
            "return get_tpu_sampler(self.train_dataset)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=if_statement), node=('boolean_operator', None), position=1, insert_id=1231193)",
            "Insert(target_node=IN(type=boolean_operator), node=('call', None), position=0, insert_id=1231194)",
            "Insert(target_node=IN(type=boolean_operator), node=('or', 'or'), position=1, insert_id=1231195)",
            "Move(target_node=IN(type=boolean_operator), node=ASTNode(type=not_operator), position=2)",
            "Insert(target_node=IN(type=call), node=('identifier', 'isinstance'), position=0, insert_id=1231196)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1231197)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1231198)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=1231199)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=1231200)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=3, insert_id=1231201)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=1231202)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=1231203)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1231204)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'train_dataset'), position=2, insert_id=1231205)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1231206)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1231207)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'IterableDataset'), position=2, insert_id=1231208)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1231209)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1231210)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'data'), position=2, insert_id=1231211)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1231212)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1231213)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'utils'), position=2, insert_id=1231214)"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 23,
        "number": 6585,
        "neg_line": [
            "-if not isinstance(self.train_dataset, collections.abc.Sized):"
        ],
        "pos_line": [
            "+if isinstance(self.train_dataset, torch.utils.data.IterableDataset) or not isinstance(",
            "+self.train_dataset, collections.abc.Sized",
            "+):"
        ],
        "core_change": "-if not isinstance(self.train_dataset, collections.abc.Sized): +if isinstance(self.train_dataset, torch.utils.data.IterableDataset) or not isinstance( +self.train_dataset, collections.abc.Sized +):",
        "core_API": "set_format"
    },
    {
        "commit_hash": "41ba2f4535bb91c487b336fb395441078af2f517",
        "index": "3bba6600..90d46bdd 100644",
        "commit_message": "[Fix] Fix no attribute 'samples_per_gpu' bug in `auto_scale_lr` (#7862)\n\n* Fix\n\n* update docstring\n",
        "file": "mmdetection.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "dataloader",
        "change": [
            "def auto_scale_lr(cfg, distributed, logger):",
            "num_gpus = len(cfg.gpu_ids)",
            "",
            "# calculate the batch size",
            "-    batch_size = num_gpus * cfg.data.samples_per_gpu",
            "-    logger.info(f'You are using {num_gpus} GPU(s) '",
            "-                f'and {cfg.data.samples_per_gpu} samples per GPU. '",
            "-                f'Total batch size is {batch_size}.')",
            "+    samples_per_gpu = cfg.data.train_dataloader.samples_per_gpu",
            "+    batch_size = num_gpus * samples_per_gpu",
            "+    logger.info(f'Training with {num_gpus} GPU(s) with {samples_per_gpu} '",
            "+                f'samples per GPU. The total batch size is {batch_size}.')",
            "",
            "if batch_size != base_batch_size:",
            "# scale LR with"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=1771884)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1771885)",
            "Update(target_node=ASTNode(type=identifier, text=batch_size), value='samples_per_gpu')",
            "Insert(target_node=ASTNode(type=assignment), node=('attribute', None), position=2, insert_id=1771886)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'batch_size'), position=0, insert_id=1771887)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1771888)",
            "Insert(target_node=IN(type=assignment), node=('binary_operator', None), position=2, insert_id=1771889)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1771890)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'samples_per_gpu'), position=2, insert_id=1771891)",
            "Insert(target_node=IN(type=binary_operator), node=('identifier', 'num_gpus'), position=0, insert_id=1771892)",
            "Insert(target_node=IN(type=binary_operator), node=('*', '*'), position=1, insert_id=1771893)",
            "Insert(target_node=IN(type=binary_operator), node=('identifier', 'samples_per_gpu'), position=2, insert_id=1771894)",
            "Update(target_node=ASTNode(type=identifier, text=samples_per_gpu), value='train_dataloader')",
            "Update(target_node=ASTNode(type=string, text=f'You are using {num_gpus} GPU(s) '), value=\"f'Training with {num_gpus} GPU(s) with {samples_per_gpu} '\")",
            "Update(target_node=ASTNode(type=string, text=f'and {cfg.data.samples_per_gpu} samples per GPU. '), value=\"f'samples per GPU. The total batch size is {batch_size}.'\")",
            "Delete(target_node=ASTNode(type=identifier, text=num_gpus))",
            "Delete(target_node=ASTNode(type=*, text=*))",
            "Delete(target_node=ASTNode(type=binary_operator))",
            "Delete(target_node=ASTNode(type=string, text=f'Total batch size is {batch_size}.'))"
        ],
        "plus_line": 4,
        "minus_line": 4,
        "AST_diff_line": 20,
        "number": 6586,
        "neg_line": [
            "-batch_size = num_gpus * cfg.data.samples_per_gpu",
            "-logger.info(f'You are using {num_gpus} GPU(s) '",
            "-f'and {cfg.data.samples_per_gpu} samples per GPU. '",
            "-f'Total batch size is {batch_size}.')"
        ],
        "pos_line": [
            "+samples_per_gpu = cfg.data.train_dataloader.samples_per_gpu",
            "+batch_size = num_gpus * samples_per_gpu",
            "+logger.info(f'Training with {num_gpus} GPU(s) with {samples_per_gpu} '",
            "+f'samples per GPU. The total batch size is {batch_size}.')"
        ],
        "core_change": "-batch_size = num_gpus * cfg.data.samples_per_gpu -logger.info(f'You are using {num_gpus} GPU(s) ' -f'and {cfg.data.samples_per_gpu} samples per GPU. ' -f'Total batch size is {batch_size}.') +samples_per_gpu = cfg.data.train_dataloader.samples_per_gpu +batch_size = num_gpus * samples_per_gpu +logger.info(f'Training with {num_gpus} GPU(s) with {samples_per_gpu} ' +f'samples per GPU. The total batch size is {batch_size}.')",
        "core_API": "info"
    },
    {
        "commit_hash": "03acade785cde366cc2e7f26b623aef61d3861ba",
        "index": "96f83f02e..e959eced3 100644",
        "commit_message": "[Train] Strip \"module.\" from state dict (#30705)\n\nThis PR adds logic to automatically strip the \"module.\" prefix from a user-saved state dict in TorchCheckpoint, which is present if a user obtains the state dict from a DistributedDataParallel module directly. We already obtain the underlying module if a user saves the model object, so this merely makes the logic consistent.\n\nThis PR also edits our examples to remove instances where this operation was conducted in the example itself. This led to issues if train.torch.prepare_model was used with num_workers=1 (eg. on Google Colab), as the module was not wrapped around, thus leading to the .module attribute being missing.\n\nSigned-off-by: Antoni Baum <antoni.baum@protonmail.com>\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "\"\\n\",",
            "\"        # Checkpoint model after every epoch.\\n\",",
            "\"        state_dict = model.state_dict()\\n\",",
            "-    \"        consume_prefix_in_state_dict_if_present(state_dict, \\\"module.\\\")\\n\",",
            "\"        checkpoint = Checkpoint.from_dict(dict(model=state_dict))\\n\",",
            "\"        session.report({\\\"loss\\\": running_loss}, checkpoint=checkpoint)\"",
            "]"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=string, text=\"        consume_prefix_in_state_dict_if_present(state_dict, \\\"module.\\\")\\n\"))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 3,
        "number": 6588,
        "neg_line": [
            "-\"        consume_prefix_in_state_dict_if_present(state_dict, \\\"module.\\\")\\n\","
        ],
        "pos_line": [],
        "core_change": "-\"        consume_prefix_in_state_dict_if_present(state_dict, \\\"module.\\\")\\n\",",
        "core_API": "state_dict"
    },
    {
        "commit_hash": "dd3469ee312513f2b8a1b2e4e90e8eb95a6f3bbb",
        "index": "57963eb..8c1781e 100644",
        "commit_message": "bug fix using dataset for text input\n\n",
        "file": "autokeras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Adapter(serializable.Serializable):",
            "tf.data.Dataset. The converted dataset.",
            "\"\"\"",
            "if isinstance(dataset, np.ndarray):",
            "-            dataset = tf.data.Dataset.from_tensor_slices(",
            "-                dataset.astype(np.float32))",
            "+            dataset = tf.data.Dataset.from_tensor_slices(dataset)",
            "return data_utils.batch_dataset(dataset, self.batch_size)",
            "",
            "def fit(self, dataset):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=dataset), position=1)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=astype))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=np))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=float32))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 12,
        "number": 6589,
        "neg_line": [
            "-dataset = tf.data.Dataset.from_tensor_slices(",
            "-dataset.astype(np.float32))"
        ],
        "pos_line": [
            "+dataset = tf.data.Dataset.from_tensor_slices(dataset)"
        ],
        "core_change": "-dataset = tf.data.Dataset.from_tensor_slices( -dataset.astype(np.float32)) +dataset = tf.data.Dataset.from_tensor_slices(dataset)",
        "core_API": "from_tensor_slices"
    },
    {
        "commit_hash": "f889cd319c560b58369016d22b25e63edd5a282c",
        "index": "e006a86bd..12aca33c0 100644",
        "commit_message": "fix temp_seed for tensorflow 2.4.0\n\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tensorflow",
        "change": [
            "def temp_seed(seed: int, set_pytorch=False, set_tensorflow=False):",
            "if not tf.executing_eagerly():",
            "raise ValueError(\"Setting random seed for TensorFlow is only available in eager mode\")",
            "",
            "-        tf_context = tf.python.context.context()  # eager mode context",
            "+        tf_context = tfpy.context.context()  # eager mode context",
            "tf_seed = tf_context._seed",
            "tf_rng_initialized = hasattr(tf_context, \"_rng\")",
            "if tf_rng_initialized:"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tf), value='tfpy')",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=tf), position=0)",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Delete(target_node=ASTNode(type=identifier, text=python))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 6591,
        "neg_line": [
            "-tf_context = tf.python.context.context()  # eager mode context"
        ],
        "pos_line": [
            "+tf_context = tfpy.context.context()  # eager mode context"
        ],
        "core_change": "-tf_context = tf.python.context.context()  # eager mode context +tf_context = tfpy.context.context()  # eager mode context",
        "core_API": "executing_eagerly"
    },
    {
        "commit_hash": "dcdc63b95be066df01635b2b163721924a87318b",
        "index": "0af3c25a..a8a56271 100644",
        "commit_message": "Fix CUDA error on illegal memory access.\n\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def convert_points_from_homogeneous(",
            "# https://github.com/opencv/opencv/pull/14411/files",
            "mask: torch.Tensor = torch.abs(z_vec) > eps",
            "scale: torch.Tensor = torch.ones_like(z_vec).masked_scatter_(",
            "-        mask, torch.tensor(1.0) / z_vec[mask])",
            "+        mask, torch.tensor(1.0).to(points.device) / z_vec[mask])",
            "",
            "return scale * points[..., :-1]"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=447399)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=447400)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=447401)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to'), position=2, insert_id=447402)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=447403)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=447404)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=447405)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'points'), position=0, insert_id=447406)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=447407)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=447408)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 6594,
        "neg_line": [
            "-mask, torch.tensor(1.0) / z_vec[mask])"
        ],
        "pos_line": [
            "+mask, torch.tensor(1.0).to(points.device) / z_vec[mask])"
        ],
        "core_change": "-mask, torch.tensor(1.0) / z_vec[mask]) +mask, torch.tensor(1.0).to(points.device) / z_vec[mask])",
        "core_API": "abs"
    },
    {
        "commit_hash": "468b4d8ff2720568725d27831b8ff7a453e33b69",
        "index": "cdb632d..baca670 100644",
        "commit_message": "fix for issue #211\n\n",
        "file": "tensorflow-wavenet.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class WaveNetModel(object):",
            "",
            "# In this case, the number of global_embedding channels must be",
            "# equal to the the last dimension of the global_condition tensor.",
            "-            gc_batch_rank = len(global_condition.get_shape)",
            "+            gc_batch_rank = len(global_condition.get_shape())",
            "dims_match = (global_condition.get_shape()[gc_batch_rank - 1] ==",
            "self.global_condition_channels)",
            "if not dims_match:",
            "raise ValueError('Shape of global_condition {} does not'",
            "' match global_condition_channels {}.'.",
            "-                                 format(self.global_condition.get_shape(),",
            "+                                 format(global_condition.get_shape(),",
            "self.global_condition_channels))",
            "embedding = global_condition"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('call', None), position=2, insert_id=2207251)",
            "Move(target_node=IN(type=call), node=ASTNode(type=identifier, text=len), position=0)",
            "Move(target_node=IN(type=call), node=ASTNode(type=argument_list), position=1)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=call), position=1)",
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=1, insert_id=2207252)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=2, insert_id=2207253)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=argument_list), position=1)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2207254)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2207255)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'global_condition'), position=0, insert_id=2207256)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2207257)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'get_shape'), position=2, insert_id=2207258)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2207259)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=), text=)), position=1)",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=global_condition))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=get_shape))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 23,
        "number": 6596,
        "neg_line": [
            "-gc_batch_rank = len(global_condition.get_shape)",
            "-format(self.global_condition.get_shape(),"
        ],
        "pos_line": [
            "+gc_batch_rank = len(global_condition.get_shape())",
            "+format(global_condition.get_shape(),"
        ],
        "core_change": "-gc_batch_rank = len(global_condition.get_shape) +gc_batch_rank = len(global_condition.get_shape()) -format(self.global_condition.get_shape(), +format(global_condition.get_shape(),",
        "core_API": "get_shape"
    },
    {
        "commit_hash": "f8add5472995229aaf2f21f8aa1644e72b6a0bda",
        "index": "e62a6658..1290ddc1 100644",
        "commit_message": "Fix crash in mixed precision stateful RNNs.\n\nFixes https://github.com/keras-team/keras/issues/15140.\n\nThe issue was self.states is created as a float32 variable, which causes TypeErrors, so I cast self.states. Alternatively, self.states could be created as a float16 variable, which would simply the logic and have better performance, but this could break checkpoint compatibility between float32 and mixed precision.\n\nPiperOrigin-RevId: 390475709\n\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class LSTM(recurrent.DropoutRNNCellMixin, recurrent.LSTM):",
            "",
            "if self.stateful:",
            "updates = [",
            "-          tf.compat.v1.assign(self_state, state)",
            "+          tf.compat.v1.assign(self_state, tf.cast(state, self_state.dtype))",
            "for self_state, state in zip(self.states, states)",
            "]",
            "self.add_update(updates)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=3, insert_id=2082552)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=4, insert_id=2082553)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2082554)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2082555)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2082556)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2082557)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cast'), position=2, insert_id=2082558)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2082559)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=state), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=2082560)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=3, insert_id=2082561)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=), text=)), position=4)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self_state'), position=0, insert_id=2082562)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2082563)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dtype'), position=2, insert_id=2082564)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 15,
        "number": 6600,
        "neg_line": [
            "-tf.compat.v1.assign(self_state, state)"
        ],
        "pos_line": [
            "+tf.compat.v1.assign(self_state, tf.cast(state, self_state.dtype))"
        ],
        "core_change": "-tf.compat.v1.assign(self_state, state) +tf.compat.v1.assign(self_state, tf.cast(state, self_state.dtype))",
        "core_API": "assign"
    },
    {
        "commit_hash": "d274148105339d67d6c2da8517d0d692d41b5218",
        "index": "c93db553..1c9f6f61 100644",
        "commit_message": "RTD Compilation Fix (#663)\n\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tensorflow",
        "change": [
            "if 'TENSORLAYER_PACKAGE_BUILDING' not in os.environ:",
            "\" - `pip install --upgrade tensorflow-gpu`\"",
            ")",
            "",
            "-    if tensorflow.__version__ < \"1.6.0\":",
            "+    if tensorflow.__version__ < \"1.6.0\" and os.environ.get('READTHEDOCS', None) != 'True':",
            "raise RuntimeError(",
            "\"TensorLayer does not support Tensorflow version older than 1.6.0.\\n\"",
            "\"Please update Tensorflow with:\\n\""
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=if_statement), node=('boolean_operator', None), position=1, insert_id=2455573)",
            "Move(target_node=IN(type=boolean_operator), node=ASTNode(type=comparison_operator), position=0)",
            "Insert(target_node=IN(type=boolean_operator), node=('and', 'and'), position=1, insert_id=2455574)",
            "Insert(target_node=IN(type=boolean_operator), node=('comparison_operator', None), position=2, insert_id=2455575)",
            "Insert(target_node=IN(type=comparison_operator), node=('call', None), position=0, insert_id=2455576)",
            "Insert(target_node=IN(type=comparison_operator), node=('!=', '!='), position=1, insert_id=2455577)",
            "Insert(target_node=IN(type=comparison_operator), node=('string', \"'True'\"), position=2, insert_id=2455578)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2455579)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2455580)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2455581)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2455582)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'get'), position=2, insert_id=2455583)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2455584)",
            "Insert(target_node=IN(type=argument_list), node=('string', \"'READTHEDOCS'\"), position=1, insert_id=2455585)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=2455586)",
            "Insert(target_node=IN(type=argument_list), node=('none', 'None'), position=3, insert_id=2455587)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=2455588)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'os'), position=0, insert_id=2455589)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2455590)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'environ'), position=2, insert_id=2455591)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 20,
        "number": 6604,
        "neg_line": [
            "-if tensorflow.__version__ < \"1.6.0\":"
        ],
        "pos_line": [
            "+if tensorflow.__version__ < \"1.6.0\" and os.environ.get('READTHEDOCS', None) != 'True':"
        ],
        "core_change": "-if tensorflow.__version__ < \"1.6.0\": +if tensorflow.__version__ < \"1.6.0\" and os.environ.get('READTHEDOCS', None) != 'True':",
        "core_API": "get"
    },
    {
        "commit_hash": "22d6088205bc2a8d6abfdb9a9a6c0e9a6d7b79f9",
        "index": "d4cd1e8..374b308 100644",
        "commit_message": "speed-reproducibility fix #17\n\n",
        "file": "yolov5.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "import torch.nn.functional as F",
            "def init_seeds(seed=0):",
            "torch.manual_seed(seed)",
            "",
            "-    # Reduce randomness (may be slower on Tesla GPUs) # https://pytorch.org/docs/stable/notes/randomness.html",
            "-    if seed == 0:",
            "+    # Speed-reproducibility tradeoff https://pytorch.org/docs/stable/notes/randomness.html",
            "+    if seed == 0:  # slower, more reproducible",
            "+        cudnn.deterministic = True",
            "+        cudnn.benchmark = False",
            "+    else:  # faster, less reproducible",
            "cudnn.deterministic = False",
            "cudnn.benchmark = True"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 5,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 6606,
        "neg_line": [
            "-# Reduce randomness (may be slower on Tesla GPUs) # https://pytorch.org/docs/stable/notes/randomness.html",
            "-if seed == 0:"
        ],
        "pos_line": [
            "+# Speed-reproducibility tradeoff https://pytorch.org/docs/stable/notes/randomness.html",
            "+if seed == 0:  # slower, more reproducible",
            "+cudnn.deterministic = True",
            "+cudnn.benchmark = False",
            "+else:  # faster, less reproducible"
        ],
        "core_change": "-# Reduce randomness (may be slower on Tesla GPUs) # https://pytorch.org/docs/stable/notes/randomness.html -if seed == 0: +# Speed-reproducibility tradeoff https://pytorch.org/docs/stable/notes/randomness.html +if seed == 0:  # slower, more reproducible +cudnn.deterministic = True +cudnn.benchmark = False +else:  # faster, less reproducible",
        "core_API": "manual_seed"
    },
    {
        "commit_hash": "05192ffac48574831e47ab33b29d4de24b016b07",
        "index": "5de4a90..286c0b1 100644",
        "commit_message": "fix self conditioning shape in diffusion prior\n\n",
        "file": "DALLE2-pytorch.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DiffusionPriorNetwork(nn.Module):",
            "",
            "# setup self conditioning",
            "",
            "-        self_cond = None",
            "if self.self_cond:",
            "-            self_cond = default(self_cond, lambda: torch.zeros(batch, 1, self.dim, device = device, dtype = dtype))",
            "+            self_cond = default(self_cond, lambda: torch.zeros(batch, self.dim, device = device, dtype = dtype))",
            "+            self_cond = rearrange(self_cond, 'b d -> b 1 d')",
            "",
            "# in section 2.2, last paragraph",
            "# \"... consisting of encoded text, CLIP text embedding, diffusion timestep embedding, noised CLIP image embedding, final embedding for prediction\""
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=class_definition), node=('block', ''), position=4, insert_id=62873)",
            "Insert(target_node=ASTNode(type=block), node=('expression_statement', None), position=1, insert_id=62874)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=62875)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'self_cond'), position=0, insert_id=62876)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=62877)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=62878)",
            "Insert(target_node=IN(type=call), node=('identifier', 'rearrange'), position=0, insert_id=62879)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=62880)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=62881)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'self_cond'), position=1, insert_id=62882)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=62883)",
            "Insert(target_node=IN(type=argument_list), node=('string', \"'b d -> b 1 d'\"), position=3, insert_id=62884)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=62885)",
            "Delete(target_node=ASTNode(type=identifier, text=self_cond))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=none, text=None))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=block))",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 21,
        "number": 6607,
        "neg_line": [
            "-self_cond = None",
            "-self_cond = default(self_cond, lambda: torch.zeros(batch, 1, self.dim, device = device, dtype = dtype))"
        ],
        "pos_line": [
            "+self_cond = default(self_cond, lambda: torch.zeros(batch, self.dim, device = device, dtype = dtype))",
            "+self_cond = rearrange(self_cond, 'b d -> b 1 d')"
        ],
        "core_change": "-self_cond = None -self_cond = default(self_cond, lambda: torch.zeros(batch, 1, self.dim, device = device, dtype = dtype)) +self_cond = default(self_cond, lambda: torch.zeros(batch, self.dim, device = device, dtype = dtype)) +self_cond = rearrange(self_cond, 'b d -> b 1 d')",
        "core_API": "zeros"
    },
    {
        "commit_hash": "9559fb5302f437bce83b6d9092602b407f155193",
        "index": "3be22f91..19c52fa2 100644",
        "commit_message": "Fix issue calling TextVectorization on non-tensor input\n\ntf.keras.layers.TextVectorization(vocabulary=[\"foo\"])([\"foo\"]) would fail with an error.\nThis was due to checking rank of input during build. We should instead check rank during\ncall, after converting to tensor.\n\nPiperOrigin-RevId: 406160462\n\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class IntegerLookupSavingTest(keras_parameterized.TestCase,",
            "",
            "",
            "if __name__ == \"__main__\":",
            "+  # IntegerLookup is only exported as a TF2 API.",
            "+  tf.compat.v1.enable_v2_behavior()",
            "tf.test.main()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('block', None), position=0, insert_id=2076192)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=2076193)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=2076194)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2076195)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2076196)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2076197)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2076198)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'enable_v2_behavior'), position=2, insert_id=2076199)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2076200)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=2076201)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2076202)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2076203)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'v1'), position=2, insert_id=2076204)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2076205)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2076206)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'compat'), position=2, insert_id=2076207)",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 2,
        "minus_line": 0,
        "AST_diff_line": 17,
        "number": 6608,
        "neg_line": [],
        "pos_line": [
            "+# IntegerLookup is only exported as a TF2 API.",
            "+tf.compat.v1.enable_v2_behavior()"
        ],
        "core_change": "+# IntegerLookup is only exported as a TF2 API. +tf.compat.v1.enable_v2_behavior()",
        "core_API": "enable_v2_behavior"
    },
    {
        "commit_hash": "d7efcdf78ef3d0f15b0ac737f049ffc3c2f4fc87",
        "index": "d83a5cdf..91431f03 100644",
        "commit_message": "Added random param gen tests. Added device awareness for parameter generators. (#757)\n\n* Updated tests and docs for ColorJitter\n\n* Fixed perspective random gen and tests.\n\n* Added tests for randomAffineGen\n\n* Added tests for random rotation gen\n\n* Added random crop gen tests\n\n* Added random crop size gen tests\n\n* Added random rectangle gen tests\n\n* Added tests for center crop\n\n* Added tests for random motion blur gen\n\n* Added tests for random solarize gen\n\n* Added posterize gen tests\n\n* Added random sharpness gen tests\n\n* Fixed tests\n\n* Updated test_augmentation\n\n* Tests fix\n\n* Additional fix\n\n* Added tests for mix random gen\n\n* Added 3D perspective/affine tests\n\n* Added 3D rotation gen\n\n* Added the rest 3D augmentation tests\n\n* Some format fixes\n\n* Bug fixes\n\n* Fixed #761\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestRandomCutMix:",
            "expected = torch.tensor([[[[1., 0., 0., 1.],",
            "[1., 0., 0., 1.],",
            "[1., 1., 1., 1.]]],",
            "-                                 [[[1., 1., 0., 0.],",
            "-                                   [1., 1., 0., 0.],",
            "+                                 [[[0., 1., 1., 0.],",
            "+                                   [0., 1., 1., 0.],",
            "[0., 0., 0., 0.]]]], device=device, dtype=dtype)",
            "",
            "out_image, out_label = f(input, label)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=float, text=1.), value='0.')",
            "Update(target_node=ASTNode(type=float, text=0.), value='1.')",
            "Update(target_node=ASTNode(type=float, text=1.), value='0.')",
            "Update(target_node=ASTNode(type=float, text=0.), value='1.')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 4,
        "number": 6609,
        "neg_line": [
            "-[[[1., 1., 0., 0.],",
            "-[1., 1., 0., 0.],"
        ],
        "pos_line": [
            "+[[[0., 1., 1., 0.],",
            "+[0., 1., 1., 0.],"
        ],
        "core_change": "-[[[1., 1., 0., 0.], -[1., 1., 0., 0.], +[[[0., 1., 1., 0.], +[0., 1., 1., 0.],",
        "core_API": "tensor"
    },
    {
        "commit_hash": "db92d23866a083b6a8107f38c112df7fa52c60e3",
        "index": "f3733f21..613c4008 100644",
        "commit_message": "Remove type ignore from the codebase (#2030)\n\n* Remove type ignore from the codebase\n\n* undo `NamedTuple` property with `List[Self]`\n\n- This is causing `Segmentation fault (core dumped)` when running `mypy --cobertura-xml-report ./`\n\n* add TODO for next mypy release\n\n* fix typo\n\n* fix F401\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class MixAugmentationBaseV2(_BasicAugmentationBase):",
            "in_tensor: Tensor = input[in_tensor_idx]",
            "in_tensor = self.transform_tensor(in_tensor)",
            "self._params = self.forward_parameters(in_tensor.shape)",
            "-            self._params.update({\"dtype\": torch.tensor(DType.get(in_tensor.dtype).value)})",
            "+            self._params.update({\"dtype\": tensor(DType.get(in_tensor.dtype).value)})",
            "else:",
            "self._params = params",
            "",
            "-        outputs = []",
            "+        outputs: List[Tensor] = []",
            "for dcate, _input in zip(keys, input):",
            "output: Tensor",
            "if dcate == DataKey.INPUT:"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=(':', ':'), position=1, insert_id=394930)",
            "Insert(target_node=ASTNode(type=assignment), node=('type', None), position=2, insert_id=394931)",
            "Insert(target_node=IN(type=type), node=('subscript', None), position=0, insert_id=394932)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'List'), position=0, insert_id=394933)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=394934)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'Tensor'), position=2, insert_id=394935)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=394936)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=tensor), position=0)",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 11,
        "number": 6611,
        "neg_line": [
            "-self._params.update({\"dtype\": torch.tensor(DType.get(in_tensor.dtype).value)})",
            "-outputs = []"
        ],
        "pos_line": [
            "+self._params.update({\"dtype\": tensor(DType.get(in_tensor.dtype).value)})",
            "+outputs: List[Tensor] = []"
        ],
        "core_change": "-self._params.update({\"dtype\": torch.tensor(DType.get(in_tensor.dtype).value)}) +self._params.update({\"dtype\": tensor(DType.get(in_tensor.dtype).value)}) -outputs = [] +outputs: List[Tensor] = []",
        "core_API": "transform_tensor"
    },
    {
        "commit_hash": "c4c532d25e012dbe6ab1ac14bca75e53e0acc621",
        "index": "2a2f7c44..a00b0e53 100644",
        "commit_message": "pylint -> flake8 (#3288)\n\n* pylint\n\n* update pylint\n\n* undo a lot of the raise / else\n\n* add bound on typed-ast\n\n* first mypy fixes\n\n* new flag\n\n* fix mypy errors\n\n* requirements.txt\n\n* pylint -> flake8\n\n* mypy 0.720 -> mypy 0.730\n\n* add back erroneously removed initial newline\n\n* remove .pylintrc\n\n* remove pylintrc from Dockerfile\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TokenCharactersEncoder(TokenEmbedder):",
            "self._dropout = lambda x: x",
            "",
            "def get_output_dim(self) -> int:",
            "-        return self._encoder._module.get_output_dim()  # pylint: disable=protected-access",
            "+        return self._encoder._module.get_output_dim()",
            "",
            "-    def forward(self, token_characters: torch.Tensor) -> torch.Tensor:  # pylint: disable=arguments-differ",
            "+    def forward(self, token_characters: torch.Tensor) -> torch.Tensor:",
            "mask = (token_characters != 0).long()",
            "return self._dropout(self._encoder(self._embedding(token_characters), mask))",
            "",
            "# The setdefault requires a custom from_params",
            "@classmethod",
            "def from_params(cls, vocab: Vocabulary, params: Params) -> 'TokenCharactersEncoder':  # type: ignore",
            "-        # pylint: disable=arguments-differ",
            "+",
            "embedding_params: Params = params.pop(\"embedding\")",
            "# Embedding.from_params() uses \"tokens\" as the default namespace, but we need to change",
            "# that to be \"token_characters\" by default. If num_embeddings is present, set default namespace"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 3,
        "AST_diff_line": 17,
        "number": 6613,
        "neg_line": [
            "-return self._encoder._module.get_output_dim()  # pylint: disable=protected-access",
            "-def forward(self, token_characters: torch.Tensor) -> torch.Tensor:  # pylint: disable=arguments-differ",
            "-# pylint: disable=arguments-differ"
        ],
        "pos_line": [
            "+return self._encoder._module.get_output_dim()",
            "+def forward(self, token_characters: torch.Tensor) -> torch.Tensor:",
            "+"
        ],
        "core_change": "-return self._encoder._module.get_output_dim()  # pylint: disable=protected-access +return self._encoder._module.get_output_dim() -def forward(self, token_characters: torch.Tensor) -> torch.Tensor:  # pylint: disable=arguments-differ +def forward(self, token_characters: torch.Tensor) -> torch.Tensor: -# pylint: disable=arguments-differ +",
        "core_API": "get_output_dim"
    },
    {
        "commit_hash": "aa94eceea2c824106e9c78b9c4099732042a6986",
        "index": "6cdfc43c..d3abd400 100644",
        "commit_message": "fixed fake node bug\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class Net(nn.Module):",
            "data, _ = voxel_max_pool(",
            "data, 14, origin=0, fake_nodes=True, transform=transform)",
            "",
            "-        x = data.input.view(-1, 4 * 64)",
            "+        x = data.input.view(-1, 4 * 128)",
            "x = F.elu(self.fc1(x))",
            "x = F.dropout(x, training=self.training)",
            "x = self.fc2(x)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=integer, text=64), value='128')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 6615,
        "neg_line": [
            "-x = data.input.view(-1, 4 * 64)"
        ],
        "pos_line": [
            "+x = data.input.view(-1, 4 * 128)"
        ],
        "core_change": "-x = data.input.view(-1, 4 * 64) +x = data.input.view(-1, 4 * 128)",
        "core_API": "view"
    },
    {
        "commit_hash": "d215aa258e77f43b6c36950bc9a484d5d849eb4f",
        "index": "06c25a5c..d2979328 100644",
        "commit_message": "fix torch.nonzero usage in pytorch 1.5 (#2602)\n\n\n",
        "file": "mmdetection.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class MaskedConv2dFunction(Function):",
            "out_w = int(",
            "math.floor((features.size(3) + 2 * pad_w -",
            "(kernel_h - 1) - 1) / stride_w + 1))",
            "-        mask_inds = torch.nonzero(mask[0] > 0)",
            "+        mask_inds = torch.nonzero(mask[0] > 0, as_tuple=False)",
            "output = features.new_zeros(batch_size, out_channel, out_h, out_w)",
            "if mask_inds.numel() > 0:",
            "mask_h_idx = mask_inds[:, 0].contiguous()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=638825)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=638826)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'as_tuple'), position=0, insert_id=638827)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=638828)",
            "Insert(target_node=IN(type=keyword_argument), node=('false', 'False'), position=2, insert_id=638829)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 6617,
        "neg_line": [
            "-mask_inds = torch.nonzero(mask[0] > 0)"
        ],
        "pos_line": [
            "+mask_inds = torch.nonzero(mask[0] > 0, as_tuple=False)"
        ],
        "core_change": "-mask_inds = torch.nonzero(mask[0] > 0) +mask_inds = torch.nonzero(mask[0] > 0, as_tuple=False)",
        "core_API": "floor"
    },
    {
        "commit_hash": "c4c532d25e012dbe6ab1ac14bca75e53e0acc621",
        "index": "efcf690b..f812be82 100644",
        "commit_message": "pylint -> flake8 (#3288)\n\n* pylint\n\n* update pylint\n\n* undo a lot of the raise / else\n\n* add bound on typed-ast\n\n* first mypy fixes\n\n* new flag\n\n* fix mypy errors\n\n* requirements.txt\n\n* pylint -> flake8\n\n* mypy 0.720 -> mypy 0.730\n\n* add back erroneously removed initial newline\n\n* remove .pylintrc\n\n* remove pylintrc from Dockerfile\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class BertPooler(Seq2VecEncoder):",
            "def get_output_dim(self) -> int:",
            "return self._embedding_dim",
            "",
            "-    def forward(self, tokens: torch.Tensor, mask: torch.Tensor = None):  # pylint: disable=arguments-differ,unused-argument",
            "+    def forward(self, tokens: torch.Tensor, mask: torch.Tensor = None):",
            "pooled = self.pooler(tokens)",
            "pooled = self._dropout(pooled)",
            "return pooled"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 6619,
        "neg_line": [
            "-def forward(self, tokens: torch.Tensor, mask: torch.Tensor = None):  # pylint: disable=arguments-differ,unused-argument"
        ],
        "pos_line": [
            "+def forward(self, tokens: torch.Tensor, mask: torch.Tensor = None):"
        ],
        "core_change": "-def forward(self, tokens: torch.Tensor, mask: torch.Tensor = None):  # pylint: disable=arguments-differ,unused-argument +def forward(self, tokens: torch.Tensor, mask: torch.Tensor = None):",
        "core_API": "pooler"
    },
    {
        "commit_hash": "174dec6e1f6b470a008ab7e7defc44d14d8271b4",
        "index": "a85740526..e027d5359 100644",
        "commit_message": "feat: multilingual and english BERT-based models for classification and SQuAD (#742)\n\n* feat: add multi_squad_retr dataset\n\n* feat: add config file\n\n* feat: add multi_squad_ru_retr dataset\n\n* fix: paths and squad datareaders\n\n* fix: squad preprocess\n\n* feat: add context from squad rate sampling ratio\n\n* fix: dataset reader\n\n* fix: squad rate attr in squad_iterator\n\n* feat: add link to trained noans_ru model\n\n* feat: create odqa retr noans config, update top n for ru tfidf ranker\n\n* fix: upd metrics docs\n\n* feat: add bert for classification tasks\n\n* fix: remove squad model from registry\n\n* fix: add bert preprocessor\n\n* fix: upd names in configs\n\n* fix: set ru tfidf ranker top_n to 5\n\n* fix: set paragraphs false in odqa\n\n* fix: multilabel classification with bert\n\n* fix: bert dropout config names\n\n* feat: add AdamWeighDecay optimizer for bert\n\n* config for rusentiment bert 1m ckpt\n\n* chore: bert configs\n\n* feat: add bert model for squad\n\n* fix: squad bert config\n\n* feat: add loading pretrained bert for squad\n\n* feat: add multilingual bert for sbersquad\n\n* feat: add url arg to squad dataset reader\n\n* fix: bert answer postprocessor\n\n* feat&fix: add bert squad infer model and config fixes\n\n* chore: add squad bert infer config\n\n* feat: add batching to bert inference model\n\n* feat: add squad with rubert and odqa configs\n\n* fix: squad rubert config\n\n* chore: add ruodqa evaluation config\n\n* fix: squad iterator empty batch\n\n* chore: add one more bert based squad config\n\n* feat: add noans score and model trained on retr dump\n\n* chore: add inference config for bert on noans\n\n* feat: add bert_ner (with bugs for now)\n\n* fix: attribute assign in classifier\n\n* fix: Adam inited twice\n\n* fix: ner bert preprocessor\n\n* refactor&docs: add docs for bert preprocessor\n\n* feat: add bert_ner for inference\n\n* docs: add bert preprocessor call method docs\n\n* docs: bert squad model\n\n* feat: add parametrization of taken encoder layers\n\n* fix: rubert download link\n\n* docs: train_on_batch and call for bert squad model\n\n* docs: add docstring for BertSQuADInferModel\n\n* fix: sigmoid -> softmax output\n\n* docs: upd docstring\n\n* fix: list -> tuple default value\n\n* docs: add docstrings for bert classifier\n\n* chore: rm configs\n\n* feat: paraphraser config, model and scores\n\n* feat: rusentiment with multi-lingual bert\n\n* feat: rusentiment with multi-lingual bert\n\n* feat: insults kaggle on English BERT\n\n* chore: remove configs\n\n* docs: add ru squad results\n\n* feat: add bert models to tests\n\n* docs: upd docs\n\n* feat: upd configs and tests\n\n* feat: rm bert configs\n\n* refactor: rm debug logging\n\n* fix: paraphraser dataset reader\n\n* docs: update bert_ner docstring\n\n* fix: restore tests for squad noans model\n\n* fix: paraphraser tests\n\n* fix: add bert_dp requirement to squad_bert config\n\n* fix: download path for model files\n\n* fix: rusentiment tests\n\n* fix: import order\n\n* fix: remove extra tabulation\n\n* refactor: rename squad metrics\n\n* refactor: url argument in squad_dataset_reader has higher priority than dataset\n\n* refactor: remove process_event method from bert models\n\n* feat: add bert-base model for EN SQuAD\n\n* feat: add bert dstc2 model\n\n* fix: squad bert infer config\n\n* docs: add docs for NerPreprocessor\n\n* feat: give higher priority to url arg in multi_squad_dataset_reader\n\n* fix: bert NER preprocessor\n\n",
        "file": "DeepPavlov.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFModel(NNModel, metaclass=TfModelMeta):",
            "variables_to_train.extend(tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope_name))",
            "",
            "if optimizer is None:",
            "-                optimizer = tf.train.AdamOptimizer(learning_rate, **kwargs)",
            "+                optimizer = tf.train.AdamOptimizer",
            "",
            "# For batch norm it is necessary to update running averages",
            "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=attribute), position=2)",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=learning_rate))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=**, text=**))",
            "Delete(target_node=ASTNode(type=identifier, text=kwargs))",
            "Delete(target_node=ASTNode(type=dictionary_splat))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 6620,
        "neg_line": [
            "-optimizer = tf.train.AdamOptimizer(learning_rate, **kwargs)"
        ],
        "pos_line": [
            "+optimizer = tf.train.AdamOptimizer"
        ],
        "core_change": "-optimizer = tf.train.AdamOptimizer(learning_rate, **kwargs) +optimizer = tf.train.AdamOptimizer",
        "core_API": "extend"
    },
    {
        "commit_hash": "cbc241069fe6c6cba5de18f0777f469ad87401ae",
        "index": "b0b01451..7f7cd9cd 100644",
        "commit_message": "Unittest for Deformable Convolution Added + Issue Fix #572 (#573)\n\n* Unittest for Deformable Convolution Added\n\n* Issue #572 fixed\n\n* Changelog Refactored\n\n* Fix for PyUP and Tensorflow\n\n* Stale Bot Configuration Added\n\n* Danger Config Error Fix\n\n* Changelog Updated\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class DeformableConv2d(Layer):",
            "name='b_deformableconv2d', shape=(shape[-1]), initializer=b_init, dtype=LayersConfig.tf_dtype,",
            "**b_init_args",
            ")",
            "-                tf.reshape()",
            "+",
            "self.outputs = tf.reshape(",
            "tensor=act(tf.nn.conv3d(input_deform, W, strides=[1, 1, 1, 1, 1], padding='VALID', name=None) + b),",
            "shape=(tf.shape(self.inputs)[0], input_h, input_w, shape[-1])"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=), text=)), position=0)",
            "Insert(target_node=ASTNode(type=ERROR), node=('attribute', None), position=1, insert_id=2262888)",
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=attribute), position=5)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=self), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=outputs), position=2)",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=reshape))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 16,
        "number": 6623,
        "neg_line": [
            "-tf.reshape()"
        ],
        "pos_line": [
            "+"
        ],
        "core_change": "-tf.reshape() +",
        "core_API": "reshape"
    },
    {
        "commit_hash": "1e859f75070ea2c145850bddc61a0de82f69db61",
        "index": "4b55de79..593d48ac 100644",
        "commit_message": "Fix half of the tests that are failing in v1.\n\nMost of them are failing since the actual code are expected to run only in v2 (eg need eager/resource variable, or certain fix we added is only applied to the v2 code path).\n\nPiperOrigin-RevId: 394765626\n\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class ParameterServerCustomTrainingLoopTest(tf.test.TestCase):",
            "",
            "",
            "if __name__ == \"__main__\":",
            "-  tf.__internal__.distribute.multi_process_runner.test_main()",
            "+  if tf.__internal__.tf2.enabled():",
            "+    tf.__internal__.distribute.multi_process_runner.test_main()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=block), node=('if_statement', None), position=0, insert_id=2080185)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=2080186)",
            "Insert(target_node=IN(type=if_statement), node=('call', None), position=1, insert_id=2080187)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=2080188)",
            "Move(target_node=IN(type=if_statement), node=ASTNode(type=block), position=3)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2080189)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2080190)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2080191)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2080192)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'enabled'), position=2, insert_id=2080193)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2080194)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=2080195)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2080196)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2080197)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf2'), position=2, insert_id=2080198)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2080199)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2080200)",
            "Insert(target_node=IN(type=attribute), node=('identifier', '__internal__'), position=2, insert_id=2080201)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 18,
        "number": 6624,
        "neg_line": [
            "-tf.__internal__.distribute.multi_process_runner.test_main()"
        ],
        "pos_line": [
            "+if tf.__internal__.tf2.enabled():",
            "+tf.__internal__.distribute.multi_process_runner.test_main()"
        ],
        "core_change": "-tf.__internal__.distribute.multi_process_runner.test_main() +if tf.__internal__.tf2.enabled(): +tf.__internal__.distribute.multi_process_runner.test_main()",
        "core_API": "test_main"
    },
    {
        "commit_hash": "603026f0325339c6412e5c045b5149f351bd1778",
        "index": "6cf97123..3003d494 100755",
        "commit_message": "support for TF 1.13, improved while loops, fixed lstm problem\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFOptimizer(Optimizer):",
            "",
            "variables.extend(self.optimizer.variables())",
            "",
            "-        # variables.extend(",
            "-        #     self.optimizer._slots[slot][key] for slot in sorted(self.optimizer._slots)",
            "-        #     for key in sorted(self.optimizer._slots[slot])",
            "-        # )",
            "-",
            "-        # if isinstance(self.optimizer, (tf.train.AdamOptimizer, tf.contrib.opt.NadamOptimizer)):",
            "-        #     variables.extend(self.optimizer._get_beta_accumulators())",
            "-",
            "return variables"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 0,
        "minus_line": 6,
        "AST_diff_line": 17,
        "number": 6628,
        "neg_line": [
            "-# variables.extend(",
            "-#     self.optimizer._slots[slot][key] for slot in sorted(self.optimizer._slots)",
            "-#     for key in sorted(self.optimizer._slots[slot])",
            "-# )",
            "-",
            "-# if isinstance(self.optimizer, (tf.train.AdamOptimizer, tf.contrib.opt.NadamOptimizer)):",
            "-#     variables.extend(self.optimizer._get_beta_accumulators())",
            "-"
        ],
        "pos_line": [],
        "core_change": "-# variables.extend( -#     self.optimizer._slots[slot][key] for slot in sorted(self.optimizer._slots) -#     for key in sorted(self.optimizer._slots[slot]) -# ) - -# if isinstance(self.optimizer, (tf.train.AdamOptimizer, tf.contrib.opt.NadamOptimizer)): -#     variables.extend(self.optimizer._get_beta_accumulators()) -",
        "core_API": "extend"
    },
    {
        "commit_hash": "a8da3205f52527c363db5783283f25dd1721addd",
        "index": "b4ee6860..b0841c41 100644",
        "commit_message": "formatting, fixes, etc\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Normalize(Preprocessor):",
            "\"\"\"",
            "",
            "def __init__(self, scope='normalize', summary_labels=()):",
            "-        super(Normalize).__init__(scope, summary_labels)",
            "+        super(Normalize, self).__init__(scope=scope, summary_labels=summary_labels)",
            "",
            "def tf_process(self, tensor):",
            "# Min/max across every axis except batch dimension.",
            "-        min = tf.reduce_min(input_tensor=tensor, axis=np.arange(1, util.rank(tensor)))",
            "-        max = tf.reduce_max(input_tensor=tensor, axis=np.arange(1, util.rank(tensor)))",
            "+        min_value = tf.reduce_min(input_tensor=tensor, axis=np.arange(1, util.rank(tensor)))",
            "+        max_value = tf.reduce_max(input_tensor=tensor, axis=np.arange(1, util.rank(tensor)))",
            "",
            "-        return (tensor - min) / (max - min + util.epsilon)",
            "+        return (tensor - min_value) / (max_value - min_value + util.epsilon)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=min), value='min_value')",
            "Update(target_node=ASTNode(type=identifier, text=max), value='max_value')",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=1, insert_id=2238001)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=4, insert_id=2238002)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=identifier, text=scope), position=0)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2238003)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'scope'), position=2, insert_id=2238004)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=identifier, text=summary_labels), position=0)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2238005)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'summary_labels'), position=2, insert_id=2238006)",
            "Update(target_node=ASTNode(type=identifier, text=min), value='min_value')",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=2238007)",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'self'), position=3, insert_id=2238008)",
            "Update(target_node=ASTNode(type=identifier, text=max), value='max_value')",
            "Update(target_node=ASTNode(type=identifier, text=min), value='min_value')"
        ],
        "plus_line": 4,
        "minus_line": 4,
        "AST_diff_line": 15,
        "number": 6629,
        "neg_line": [
            "-super(Normalize).__init__(scope, summary_labels)",
            "-min = tf.reduce_min(input_tensor=tensor, axis=np.arange(1, util.rank(tensor)))",
            "-max = tf.reduce_max(input_tensor=tensor, axis=np.arange(1, util.rank(tensor)))",
            "-return (tensor - min) / (max - min + util.epsilon)"
        ],
        "pos_line": [
            "+super(Normalize, self).__init__(scope=scope, summary_labels=summary_labels)",
            "+min_value = tf.reduce_min(input_tensor=tensor, axis=np.arange(1, util.rank(tensor)))",
            "+max_value = tf.reduce_max(input_tensor=tensor, axis=np.arange(1, util.rank(tensor)))",
            "+return (tensor - min_value) / (max_value - min_value + util.epsilon)"
        ],
        "core_change": "-super(Normalize).__init__(scope, summary_labels) +super(Normalize, self).__init__(scope=scope, summary_labels=summary_labels) -min = tf.reduce_min(input_tensor=tensor, axis=np.arange(1, util.rank(tensor))) -max = tf.reduce_max(input_tensor=tensor, axis=np.arange(1, util.rank(tensor))) +min_value = tf.reduce_min(input_tensor=tensor, axis=np.arange(1, util.rank(tensor))) +max_value = tf.reduce_max(input_tensor=tensor, axis=np.arange(1, util.rank(tensor))) -return (tensor - min) / (max - min + util.epsilon) +return (tensor - min_value) / (max_value - min_value + util.epsilon)",
        "core_API": "reduce_min"
    },
    {
        "commit_hash": "a6ec1b7f086b76ce45ddeef8fdae079c0ddc68b8",
        "index": "a0ea7fef9c..2c657210be 100644",
        "commit_message": "fixed lint errors\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def random_uniform(",
            "rand_range = high - low",
            "if shape is None:",
            "shape = []",
            "-    return torch.rand(shape, device=default_device(device), dtype=dtype) * rand_range + low",
            "+    return (",
            "+        torch.rand(shape, device=default_device(device), dtype=dtype) * rand_range + low",
            "+    )",
            "",
            "",
            "def random_normal("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=ERROR), node=('ERROR', None), position=8, insert_id=351785)",
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=identifier, text=random_normal), position=9)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=list), position=0)",
            "Insert(target_node=IN(type=ERROR), node=('call', None), position=1, insert_id=351786)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=identifier, text=def), position=2)",
            "Move(target_node=IN(type=call), node=ASTNode(type=identifier, text=return), position=0)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=351787)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=351788)",
            "Insert(target_node=IN(type=argument_list), node=('binary_operator', None), position=1, insert_id=351789)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=351790)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=binary_operator), position=0)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=+, text=+), position=1)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=identifier, text=low), position=2)",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=torch), position=0)",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=ERROR))"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 16,
        "number": 6631,
        "neg_line": [
            "-return torch.rand(shape, device=default_device(device), dtype=dtype) * rand_range + low"
        ],
        "pos_line": [
            "+return (",
            "+torch.rand(shape, device=default_device(device), dtype=dtype) * rand_range + low",
            "+)"
        ],
        "core_change": "-return torch.rand(shape, device=default_device(device), dtype=dtype) * rand_range + low +return ( +torch.rand(shape, device=default_device(device), dtype=dtype) * rand_range + low +)",
        "core_API": "rand"
    },
    {
        "commit_hash": "de4159a318f9a4f2f474733306c00ac326e643a7",
        "index": "3e9802e20..95770c826 100644",
        "commit_message": "More TF int dtype fixes (#20384)\n\n* Add a test to ensure int dummy inputs are int64\n\n* Move the test into the existing int64 test and update a lot of existing dummies\n\n* Fix remaining dummies\n\n* Fix remaining dummies\n\n* Test for int64 serving sigs as well\n\n* Update core tests to use tf.int64\n\n* Add better messages to the assertions\n\n* Update all serving sigs to int64\n\n* More sneaky hiding tf.int32s\n\n* Add an optional int32 signature in save_pretrained\n\n* make fixup\n\n* Add Amy's suggestions\n\n* Switch all serving sigs back to tf.int32\n\n* Switch all dummies to tf.int32\n\n* Adjust tests to check for tf.int32 instead of tf.int64\n\n* Fix base dummy_inputs dtype\n\n* Start casting to tf.int32 in input_processing\n\n* Change dtype for unpack_inputs test\n\n* Add proper tf.int32 test\n\n* Make the alternate serving signature int64\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TF{{cookiecutter.camelcase_modelname}}ForMultipleChoice(TF{{cookiecutter.c",
            "Returns:",
            "tf.Tensor with dummy inputs",
            "\"\"\"",
            "-        return {\"input_ids\": tf.constant(MULTIPLE_CHOICE_DUMMY_INPUTS)}",
            "+        return {\"input_ids\": tf.constant(MULTIPLE_CHOICE_DUMMY_INPUTS, dtype=tf.int64)}",
            "",
            "@unpack_inputs",
            "@add_start_docstrings_to_model_forward({{cookiecutter.uppercase_modelname}}_INPUTS_DOCSTRING.format(\"batch_size, num_choices, sequence_length\"))"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 6632,
        "neg_line": [
            "-return {\"input_ids\": tf.constant(MULTIPLE_CHOICE_DUMMY_INPUTS)}"
        ],
        "pos_line": [
            "+return {\"input_ids\": tf.constant(MULTIPLE_CHOICE_DUMMY_INPUTS, dtype=tf.int64)}"
        ],
        "core_change": "-return {\"input_ids\": tf.constant(MULTIPLE_CHOICE_DUMMY_INPUTS)} +return {\"input_ids\": tf.constant(MULTIPLE_CHOICE_DUMMY_INPUTS, dtype=tf.int64)}",
        "core_API": "constant"
    },
    {
        "commit_hash": "5f4c6e830f603830117877b8990a0e65a2386aa6",
        "index": "b6183ca3e..659d8e1d3 100644",
        "commit_message": "Quick fix :) (#606)\n\n* Changing the name\n\n* style + quality\n\n* update doc and logo\n\n* clean up\n\n* circle-CI on the branche for now\n\n* fix daily dialog dataset\n\n* fix urls\n\nCo-authored-by: Quentin Lhoest <lhoest.q@gmail.com>\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class Emotion(nlp.GeneratorBasedBuilder):",
            "valid_path = dl_manager.download_and_extract(_VALIDATION_DOWNLOAD_URL)",
            "test_path = dl_manager.download_and_extract(_TEST_DOWNLOAD_URL)",
            "return [",
            "-            nlp.SplitGenerator(name=nlp.Split.TRAIN, gen_kwargs={\"filepath\": train_path}),",
            "-            nlp.SplitGenerator(name=nlp.Split.VALIDATION, gen_kwargs={\"filepath\": valid_path}),",
            "-            nlp.SplitGenerator(name=nlp.Split.TEST, gen_kwargs={\"filepath\": test_path}),",
            "+            datasets.SplitGenerator(name=datasets.Split.TRAIN, gen_kwargs={\"filepath\": train_path}),",
            "+            datasets.SplitGenerator(name=datasets.Split.VALIDATION, gen_kwargs={\"filepath\": valid_path}),",
            "+            datasets.SplitGenerator(name=datasets.Split.TEST, gen_kwargs={\"filepath\": test_path}),",
            "]",
            "",
            "def _generate_examples(self, filepath):"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=nlp), value='datasets')",
            "Update(target_node=ASTNode(type=identifier, text=nlp), value='datasets')",
            "Update(target_node=ASTNode(type=identifier, text=nlp), value='datasets')",
            "Update(target_node=ASTNode(type=identifier, text=nlp), value='datasets')",
            "Update(target_node=ASTNode(type=identifier, text=nlp), value='datasets')",
            "Update(target_node=ASTNode(type=identifier, text=nlp), value='datasets')"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 6,
        "number": 6633,
        "neg_line": [
            "-nlp.SplitGenerator(name=nlp.Split.TRAIN, gen_kwargs={\"filepath\": train_path}),",
            "-nlp.SplitGenerator(name=nlp.Split.VALIDATION, gen_kwargs={\"filepath\": valid_path}),",
            "-nlp.SplitGenerator(name=nlp.Split.TEST, gen_kwargs={\"filepath\": test_path}),"
        ],
        "pos_line": [
            "+datasets.SplitGenerator(name=datasets.Split.TRAIN, gen_kwargs={\"filepath\": train_path}),",
            "+datasets.SplitGenerator(name=datasets.Split.VALIDATION, gen_kwargs={\"filepath\": valid_path}),",
            "+datasets.SplitGenerator(name=datasets.Split.TEST, gen_kwargs={\"filepath\": test_path}),"
        ],
        "core_change": "-nlp.SplitGenerator(name=nlp.Split.TRAIN, gen_kwargs={\"filepath\": train_path}), -nlp.SplitGenerator(name=nlp.Split.VALIDATION, gen_kwargs={\"filepath\": valid_path}), -nlp.SplitGenerator(name=nlp.Split.TEST, gen_kwargs={\"filepath\": test_path}), +datasets.SplitGenerator(name=datasets.Split.TRAIN, gen_kwargs={\"filepath\": train_path}), +datasets.SplitGenerator(name=datasets.Split.VALIDATION, gen_kwargs={\"filepath\": valid_path}), +datasets.SplitGenerator(name=datasets.Split.TEST, gen_kwargs={\"filepath\": test_path}),",
        "core_API": "download_and_extract"
    },
    {
        "commit_hash": "43043ee4d5c672f7fbc22ac9559e8164731fd053",
        "index": "a8baf9bbff..0237772bcc 100644",
        "commit_message": "[RLlib] Tf2x preparation; part 2 (upgrading `try_import_tf()`). (#9136)\n\n* WIP.\n\n* Fixes.\n\n* LINT.\n\n* WIP.\n\n* WIP.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* WIP.\n\n* Fixes.\n\n* Test\n\n* Fix.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* LINT.\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def postprocess_trajectory(policy,",
            "",
            "def choose_optimizer(policy, config):",
            "if policy.config[\"opt_type\"] == \"adam\":",
            "-        return tf.train.AdamOptimizer(policy.cur_lr)",
            "+        return tf1.train.AdamOptimizer(policy.cur_lr)",
            "else:",
            "-        return tf.train.RMSPropOptimizer(policy.cur_lr, config[\"decay\"],",
            "-                                         config[\"momentum\"], config[\"epsilon\"])",
            "+        return tf1.train.RMSPropOptimizer(",
            "+            policy.cur_lr,",
            "+            config[\"decay\"], config[\"momentum\"], config[\"epsilon\"])",
            "",
            "",
            "def clip_gradients(policy, optimizer, loss):"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tf), value='tf1')",
            "Update(target_node=ASTNode(type=identifier, text=tf), value='tf1')"
        ],
        "plus_line": 4,
        "minus_line": 3,
        "AST_diff_line": 2,
        "number": 6637,
        "neg_line": [
            "-return tf.train.AdamOptimizer(policy.cur_lr)",
            "-return tf.train.RMSPropOptimizer(policy.cur_lr, config[\"decay\"],",
            "-config[\"momentum\"], config[\"epsilon\"])"
        ],
        "pos_line": [
            "+return tf1.train.AdamOptimizer(policy.cur_lr)",
            "+return tf1.train.RMSPropOptimizer(",
            "+policy.cur_lr,",
            "+config[\"decay\"], config[\"momentum\"], config[\"epsilon\"])"
        ],
        "core_change": "-return tf.train.AdamOptimizer(policy.cur_lr) +return tf1.train.AdamOptimizer(policy.cur_lr) -return tf.train.RMSPropOptimizer(policy.cur_lr, config[\"decay\"], -config[\"momentum\"], config[\"epsilon\"]) +return tf1.train.RMSPropOptimizer( +policy.cur_lr, +config[\"decay\"], config[\"momentum\"], config[\"epsilon\"])",
        "core_API": "AdamOptimizer"
    },
    {
        "commit_hash": "a1dd654ec3adb12c98870b9bc166fe710c7482d7",
        "index": "f45a070adb..89439df6f0 100644",
        "commit_message": "lintfixbot: Auto-commit fixed lint errors in codebase\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def real(",
            "",
            "",
            "def isposinf(",
            "-        x: Union[torch.Tensor],",
            "-        /,",
            "-        *,",
            "-        out: Optional[torch.Tensor] = None,",
            "+    x: Union[torch.Tensor],",
            "+    /,",
            "+    *,",
            "+    out: Optional[torch.Tensor] = None,",
            ") -> torch.Tensor:",
            "return torch.isposinf(x)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 4,
        "minus_line": 4,
        "AST_diff_line": 17,
        "number": 6638,
        "neg_line": [
            "-x: Union[torch.Tensor],",
            "-/,",
            "-*,",
            "-out: Optional[torch.Tensor] = None,"
        ],
        "pos_line": [
            "+x: Union[torch.Tensor],",
            "+/,",
            "+*,",
            "+out: Optional[torch.Tensor] = None,"
        ],
        "core_change": "-x: Union[torch.Tensor], -/, -*, -out: Optional[torch.Tensor] = None, +x: Union[torch.Tensor], +/, +*, +out: Optional[torch.Tensor] = None,",
        "core_API": "isposinf"
    },
    {
        "commit_hash": "53147ba006ea40c41bd3b47692fa78a2a6fefd02",
        "index": "2244e2a8..f42bdd5f 100644",
        "commit_message": "saver fix\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class ValueFunction(object):",
            "\"\"\"",
            "",
            "self.session = tf.Session()",
            "-        # TODO fix",
            "-        #self.saver = tf.train.Saver()",
            "+        self.saver = None",
            "",
            "def get_action(self, state):",
            "raise NotImplementedError"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=2249017)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=2249018)",
            "Insert(target_node=IN(type=assignment), node=('attribute', None), position=0, insert_id=2249019)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=2249020)",
            "Insert(target_node=IN(type=assignment), node=('none', 'None'), position=2, insert_id=2249021)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=2249022)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2249023)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'saver'), position=2, insert_id=2249024)"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 8,
        "number": 6640,
        "neg_line": [
            "-# TODO fix",
            "-#self.saver = tf.train.Saver()"
        ],
        "pos_line": [
            "+self.saver = None"
        ],
        "core_change": "-# TODO fix -#self.saver = tf.train.Saver() +self.saver = None",
        "core_API": "Session"
    },
    {
        "commit_hash": "51e64e508ad6ce0fd43244fc895bbfe2914f7812",
        "index": "3d6ad54c51..7c59756fdf 100644",
        "commit_message": "lintfixbot: Auto-commit fixed lint errors in codebase\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def unravel_index(",
            "for dim in reversed(shape):",
            "output.append(temp % dim)",
            "temp = temp // dim",
            "-    ret= tf.constant(reversed(output), dtype=tf.int32)",
            "+    ret = tf.constant(reversed(output), dtype=tf.int32)",
            "return tuple(ret)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 6641,
        "neg_line": [
            "-ret= tf.constant(reversed(output), dtype=tf.int32)"
        ],
        "pos_line": [
            "+ret = tf.constant(reversed(output), dtype=tf.int32)"
        ],
        "core_change": "-ret= tf.constant(reversed(output), dtype=tf.int32) +ret = tf.constant(reversed(output), dtype=tf.int32)",
        "core_API": "append"
    },
    {
        "commit_hash": "c05ace221339102abb1c8b0933b9cbfd51ddd4a3",
        "index": "52553cb74..50e3d4b43 100644",
        "commit_message": "Use f-strings in the dataset scripts (#3291)\n\n* Finishes #3257\n\nUsed f-strings to format the .py files in the dataset folder\n\n* Fix style\n\n* Fix hkcancor dataset\n\nCo-authored-by: Mario Šaško <mario@huggingface.co>\nCo-authored-by: mariosasko <mariosasko777@gmail.com>\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class TurkishShrinkedNER(datasets.GeneratorBasedBuilder):",
            "path_to_manual_file = os.path.abspath(os.path.expanduser(dl_manager.manual_dir))",
            "if not os.path.exists(path_to_manual_file):",
            "raise FileNotFoundError(",
            "-                \"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('turkish_shrinked_ner', data_dir=...)` that includes file name {}. Manual download instructions: {}\".format(",
            "-                    path_to_manual_file,",
            "-                    _FILENAME,",
            "-                    self.manual_download_instructions,",
            "-                )",
            "+                \"{path_to_manual_file} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('turkish_shrinked_ner', data_dir=...)` that includes file name {_FILENAME}. Manual download instructions: {self.manual_download_instructions}\"",
            ")",
            "return [",
            "datasets.SplitGenerator("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('string', '\"{path_to_manual_file} does not exist. Make sure you insert a manual dir via `datasets.load_dataset(\\'turkish_shrinked_ner\\', data_dir=...)` that includes file name {_FILENAME}. Manual download instructions: {self.manual_download_instructions}\"'), position=1, insert_id=1781661)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=), text=)), position=2)",
            "Delete(target_node=ASTNode(type=string, text=\"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('turkish_shrinked_ner', data_dir=...)` that includes file name {}. Manual download instructions: {}\"))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=format))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=path_to_manual_file))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=_FILENAME))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=manual_download_instructions))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=), text=)))"
        ],
        "plus_line": 1,
        "minus_line": 5,
        "AST_diff_line": 19,
        "number": 6644,
        "neg_line": [
            "-\"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('turkish_shrinked_ner', data_dir=...)` that includes file name {}. Manual download instructions: {}\".format(",
            "-path_to_manual_file,",
            "-_FILENAME,",
            "-self.manual_download_instructions,",
            "-)"
        ],
        "pos_line": [
            "+\"{path_to_manual_file} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('turkish_shrinked_ner', data_dir=...)` that includes file name {_FILENAME}. Manual download instructions: {self.manual_download_instructions}\""
        ],
        "core_change": "-\"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('turkish_shrinked_ner', data_dir=...)` that includes file name {}. Manual download instructions: {}\".format( -path_to_manual_file, -_FILENAME, -self.manual_download_instructions, -) +\"{path_to_manual_file} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('turkish_shrinked_ner', data_dir=...)` that includes file name {_FILENAME}. Manual download instructions: {self.manual_download_instructions}\"",
        "core_API": "abspath"
    },
    {
        "commit_hash": "a7e2d1116dae4cca782c38453660a977ac18ea65",
        "index": "7e6af3da0..827a05710 100644",
        "commit_message": "fix bugs in magnitude/spectrum loss for enh; add egs2/dirha/enh1 and egs2/reverb/enh1\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DNN_WPE(torch.nn.Module):",
            "",
            "# Averaging along the channel axis: (..., C, T) -> (..., T)",
            "power = power.mean(dim=-2)",
            "+            power = torch.clamp(power, min=1e-6)",
            "",
            "# enhanced: (..., C, T) -> (..., C, T)",
            "# NOTE(kamo): Calculate in double precision"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=2, insert_id=145454)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=145455)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'power'), position=0, insert_id=145456)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=145457)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=145458)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=145459)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=145460)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=145461)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=145462)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'clamp'), position=2, insert_id=145463)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=145464)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'power'), position=1, insert_id=145465)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=145466)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=3, insert_id=145467)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=145468)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'min'), position=0, insert_id=145469)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=145470)",
            "Insert(target_node=IN(type=keyword_argument), node=('float', '1e-6'), position=2, insert_id=145471)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 18,
        "number": 6648,
        "neg_line": [],
        "pos_line": [
            "+power = torch.clamp(power, min=1e-6)"
        ],
        "core_change": "+power = torch.clamp(power, min=1e-6)",
        "core_API": "mean"
    },
    {
        "commit_hash": "062f0947d25d87e7daee732a5951cf7ae96ea666",
        "index": "e061f7ee..0fbb88a9 100644",
        "commit_message": "py2 fixes\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def stack(sequence, horizontal=True, vertical=True):",
            "# Concat all indices and values to one new large sparse matrix.",
            "indices = torch.cat(indices, dim=1)",
            "values = torch.cat([mat._values() for mat in sequence])",
            "-    size = torch.Size([y_sum, x_sum, *sequence[0].size()[2:]])",
            "+    size = torch.Size([y_sum, x_sum, *(sequence[0].size()[2:])])",
            "slices = torch.LongTensor(slices)",
            "",
            "return torch.sparse.FloatTensor(indices, values, size), slices"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=list_splat), node=('parenthesized_expression', None), position=1, insert_id=1096852)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('(', '('), position=0, insert_id=1096853)",
            "Move(target_node=IN(type=parenthesized_expression), node=ASTNode(type=subscript), position=1)",
            "Insert(target_node=IN(type=parenthesized_expression), node=(')', ')'), position=2, insert_id=1096854)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 6650,
        "neg_line": [
            "-size = torch.Size([y_sum, x_sum, *sequence[0].size()[2:]])"
        ],
        "pos_line": [
            "+size = torch.Size([y_sum, x_sum, *(sequence[0].size()[2:])])"
        ],
        "core_change": "-size = torch.Size([y_sum, x_sum, *sequence[0].size()[2:]]) +size = torch.Size([y_sum, x_sum, *(sequence[0].size()[2:])])",
        "core_API": "cat"
    },
    {
        "commit_hash": "5c749c0e906e264458a72d3607f89cd8c30534c7",
        "index": "554854e4..ccef0380 100644",
        "commit_message": "minor fixes\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Categorical(Distribution):",
            "repeat=r))",
            "",
            "if r == 1:",
            "-            return iter([Variable(torch.Tensor([[i]])) for i in range(c)])",
            "+            return (Variable(torch.Tensor([[i]])) for i in range(c))",
            "return (Variable(torch.Tensor(list(x)).unsqueeze(1))",
            "for x in itertools.product(torch.arange(0, c),",
            "repeat=r))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=return_statement), node=('generator_expression', None), position=1, insert_id=772724)",
            "Move(target_node=IN(type=generator_expression), node=ASTNode(type=(, text=(), position=0)",
            "Move(target_node=IN(type=generator_expression), node=ASTNode(type=call), position=1)",
            "Move(target_node=IN(type=generator_expression), node=ASTNode(type=for_in_clause), position=2)",
            "Move(target_node=IN(type=generator_expression), node=ASTNode(type=), text=)), position=3)",
            "Delete(target_node=ASTNode(type=identifier, text=iter))",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=list_comprehension))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 6651,
        "neg_line": [
            "-return iter([Variable(torch.Tensor([[i]])) for i in range(c)])"
        ],
        "pos_line": [
            "+return (Variable(torch.Tensor([[i]])) for i in range(c))"
        ],
        "core_change": "-return iter([Variable(torch.Tensor([[i]])) for i in range(c)]) +return (Variable(torch.Tensor([[i]])) for i in range(c))",
        "core_API": "Tensor"
    },
    {
        "commit_hash": "c61c17e0637ca851437d49dabd04e40d791a6ddd",
        "index": "b0826bff..93442aaf 100644",
        "commit_message": "fix typo (#834)\n\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "\"metadata\": {},",
            "\"outputs\": [],",
            "\"source\": [",
            "-    \"# equivalent to pyro.condition(scale, data={\\\"measurement\\\": Variable(torch.ones(1))})\\n\",",
            "+    \"# equivalent to pyro.condition(scale, data={\\\"measurement\\\": Variable(torch.Tensor([9.5]))})\\n\",",
            "\"def scale_obs(guess):\\n\",",
            "\"    weight = pyro.sample(\\\"weight\\\", dist.Normal(guess, Variable(torch.ones(1))))\\n\",",
            "\"     # here we attach an observation measurement == 9.5\\n\","
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"# equivalent to pyro.condition(scale, data={\\\"measurement\\\": Variable(torch.ones(1))})\\n\"), value='\"# equivalent to pyro.condition(scale, data={\\\\\"measurement\\\\\": Variable(torch.Tensor([9.5]))})\\\\n\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 6653,
        "neg_line": [
            "-\"# equivalent to pyro.condition(scale, data={\\\"measurement\\\": Variable(torch.ones(1))})\\n\","
        ],
        "pos_line": [
            "+\"# equivalent to pyro.condition(scale, data={\\\"measurement\\\": Variable(torch.Tensor([9.5]))})\\n\","
        ],
        "core_change": "-\"# equivalent to pyro.condition(scale, data={\\\"measurement\\\": Variable(torch.ones(1))})\\n\", +\"# equivalent to pyro.condition(scale, data={\\\"measurement\\\": Variable(torch.Tensor([9.5]))})\\n\",",
        "core_API": "condition"
    },
    {
        "commit_hash": "72cad39854a7d9ebbd4d58994cefa966b0da8fc1",
        "index": "fe934ab..77549b0 100644",
        "commit_message": "Squeezenet reshape outputs fix (#10222)\n\n@AyushExel\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n",
        "file": "yolov5.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "def reshape_classifier_output(model, n=1000):",
            "elif nn.Conv2d in types:",
            "i = types.index(nn.Conv2d)  # nn.Conv2d index",
            "if m[i].out_channels != n:",
            "-                m[i] = nn.Conv2d(m[i].in_channels, n, m[i].kernel_size, m[i].stride, bias=m[i].bias)",
            "+                m[i] = nn.Conv2d(m[i].in_channels, n, m[i].kernel_size, m[i].stride, bias=m[i].bias is not None)",
            "",
            "",
            "@contextmanager"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=keyword_argument), node=('comparison_operator', None), position=2, insert_id=1568560)",
            "Move(target_node=IN(type=comparison_operator), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=comparison_operator), node=('is not', 'is'), position=1, insert_id=1568561)",
            "Insert(target_node=IN(type=comparison_operator), node=('is not', 'not'), position=2, insert_id=1568562)",
            "Insert(target_node=IN(type=comparison_operator), node=('none', 'None'), position=3, insert_id=1568563)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 6657,
        "neg_line": [
            "-m[i] = nn.Conv2d(m[i].in_channels, n, m[i].kernel_size, m[i].stride, bias=m[i].bias)"
        ],
        "pos_line": [
            "+m[i] = nn.Conv2d(m[i].in_channels, n, m[i].kernel_size, m[i].stride, bias=m[i].bias is not None)"
        ],
        "core_change": "-m[i] = nn.Conv2d(m[i].in_channels, n, m[i].kernel_size, m[i].stride, bias=m[i].bias) +m[i] = nn.Conv2d(m[i].in_channels, n, m[i].kernel_size, m[i].stride, bias=m[i].bias is not None)",
        "core_API": "index"
    },
    {
        "commit_hash": "29f83beb20f577448f69af4b7d7d4b28abc34304",
        "index": "becd2e9..e934f3d 100644",
        "commit_message": "fix a bug cause by torch.tensor()\n\n",
        "file": "deep-learning-for-image-processing.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DefaultBoxes(object):",
            "self.default_boxes.append((cx, cy, w, h))",
            "",
            "# 将default_boxes转为tensor格式",
            "-        self.dboxes = torch.tensor(self.default_boxes, dtype=torch.float32)  # 这里不转类型会报错",
            "+        self.dboxes = torch.as_tensor(self.default_boxes, dtype=torch.float32)  # 这里不转类型会报错",
            "self.dboxes.clamp_(min=0, max=1)  # 将坐标（x, y, w, h）都限制在0-1之间",
            "",
            "# For IoU calculation"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tensor), value='as_tensor')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 6672,
        "neg_line": [
            "-self.dboxes = torch.tensor(self.default_boxes, dtype=torch.float32)  # 这里不转类型会报错"
        ],
        "pos_line": [
            "+self.dboxes = torch.as_tensor(self.default_boxes, dtype=torch.float32)  # 这里不转类型会报错"
        ],
        "core_change": "-self.dboxes = torch.tensor(self.default_boxes, dtype=torch.float32)  # 这里不转类型会报错 +self.dboxes = torch.as_tensor(self.default_boxes, dtype=torch.float32)  # 这里不转类型会报错",
        "core_API": "append"
    },
    {
        "commit_hash": "d68afedb32fb5f3b632f67f2cbea2c89a145f0ad",
        "index": "2cbbbba..891f6be 100644",
        "commit_message": "Confusion matrix native image-space fix (#2046)\n\nMake sure the labels and predictions are equally scaled on confusion_matrix.process_batch\n",
        "file": "yolov5.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test(data,",
            "tbox = xywh2xyxy(labels[:, 1:5])",
            "scale_coords(img[si].shape[1:], tbox, shapes[si][0], shapes[si][1])  # native-space labels",
            "if plots:",
            "-                    confusion_matrix.process_batch(pred, torch.cat((labels[:, 0:1], tbox), 1))",
            "+                    confusion_matrix.process_batch(predn, torch.cat((labels[:, 0:1], tbox), 1))",
            "",
            "# Per target class",
            "for cls in torch.unique(tcls_tensor):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=pred), value='predn')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 6673,
        "neg_line": [
            "-confusion_matrix.process_batch(pred, torch.cat((labels[:, 0:1], tbox), 1))"
        ],
        "pos_line": [
            "+confusion_matrix.process_batch(predn, torch.cat((labels[:, 0:1], tbox), 1))"
        ],
        "core_change": "-confusion_matrix.process_batch(pred, torch.cat((labels[:, 0:1], tbox), 1)) +confusion_matrix.process_batch(predn, torch.cat((labels[:, 0:1], tbox), 1))",
        "core_API": "process_batch"
    },
    {
        "commit_hash": "c749fb1ab6a3965622a569fcd2afc285b787f083",
        "index": "719ded422b..39a3bdf493 100644",
        "commit_message": "lintfixbot: Auto-commit fixed lint errors in codebase\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def arange(",
            "stop = start",
            "if dtype is None:",
            "if isinstance(start, int) and isinstance(stop, int) and isinstance(step, int):",
            "-            return torch.arange(",
            "-                start, stop, step, dtype=torch.int64, device=device",
            "-            ).to(torch.int32)",
            "+            return torch.arange(start, stop, step, dtype=torch.int64, device=device).to(",
            "+                torch.int32",
            "+            )",
            "else:",
            "return torch.arange(start, stop, step, device=device)",
            "else:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 17,
        "number": 6681,
        "neg_line": [
            "-return torch.arange(",
            "-start, stop, step, dtype=torch.int64, device=device",
            "-).to(torch.int32)"
        ],
        "pos_line": [
            "+return torch.arange(start, stop, step, dtype=torch.int64, device=device).to(",
            "+torch.int32",
            "+)"
        ],
        "core_change": "-return torch.arange( -start, stop, step, dtype=torch.int64, device=device -).to(torch.int32) +return torch.arange(start, stop, step, dtype=torch.int64, device=device).to( +torch.int32 +)",
        "core_API": "arange"
    },
    {
        "commit_hash": "0b0bfee43f459e4c66b67a6a4a3fb4a9275b76b4",
        "index": "677cbc8b..3842d69f 100755",
        "commit_message": "fixed retrieve layer, added iterable callback option, simplified tensor spec handling\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class SubsamplingStep(MetaOptimizer):",
            "if some_argument:",
            "arguments_iter = iter(some_argument)",
            "some_argument = next(arguments_iter)",
            "-                elif some_argument is None or util.rank(some_argument) == 0:",
            "+                elif some_argument is None or util.rank(x=some_argument) == 0:",
            "# Non-batched argument",
            "some_argument = next(arguments_iter)",
            "else:"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=1, insert_id=2230526)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'x'), position=0, insert_id=2230527)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2230528)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=identifier, text=some_argument), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 6683,
        "neg_line": [
            "-elif some_argument is None or util.rank(some_argument) == 0:"
        ],
        "pos_line": [
            "+elif some_argument is None or util.rank(x=some_argument) == 0:"
        ],
        "core_change": "-elif some_argument is None or util.rank(some_argument) == 0: +elif some_argument is None or util.rank(x=some_argument) == 0:",
        "core_API": "rank"
    },
    {
        "commit_hash": "8595376da52c360bfbc3b3785c9863382350854d",
        "index": "37ba0a09..85a86d35 100644",
        "commit_message": "Remove Re-definition found for builtin input function - Update tests (#2255)\n\n* change input to sample in test/feature folder\n\n* change input to sample in test/filters folder\n\n* change input to sample in test/morphology folder\n\n* change input to sample in test/geometry/subpix/test_spatial_softargmax.py\n\n* change input to sample in test/geometry/transfrom in test_imgwarp3d.py and test_pyramid.py\n\n* change input to input_org in test/geometry/transfrom in test_imgwarp.py\n\n* change input to inpt in test/grad_estimator/test_ste.py\n\n* change input to inpt and sample in test/enhance/ folder\n\n* change input to input_tensor in test/augmentation/test_augmentation_3d.py\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nCo-authored-by: oleksadobush <oleksandra.stasiuk@ucu.edu.ua>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Testunsharp(BaseTester):",
            "",
            "def test_noncontiguous(self, device, dtype):",
            "batch_size = 3",
            "-        input = torch.rand(3, 5, 5, device=device, dtype=dtype).expand(batch_size, -1, -1, -1)",
            "+        inpt = torch.rand(3, 5, 5, device=device, dtype=dtype).expand(batch_size, -1, -1, -1)",
            "",
            "kernel_size = (3, 3)",
            "sigma = (1.5, 2.1)",
            "-        actual = unsharp_mask(input, kernel_size, sigma, \"replicate\")",
            "+        actual = unsharp_mask(inpt, kernel_size, sigma, \"replicate\")",
            "assert actual.is_contiguous()",
            "",
            "def test_gradcheck(self, device):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=input), value='inpt')",
            "Update(target_node=ASTNode(type=identifier, text=input), value='inpt')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 6687,
        "neg_line": [
            "-input = torch.rand(3, 5, 5, device=device, dtype=dtype).expand(batch_size, -1, -1, -1)",
            "-actual = unsharp_mask(input, kernel_size, sigma, \"replicate\")"
        ],
        "pos_line": [
            "+inpt = torch.rand(3, 5, 5, device=device, dtype=dtype).expand(batch_size, -1, -1, -1)",
            "+actual = unsharp_mask(inpt, kernel_size, sigma, \"replicate\")"
        ],
        "core_change": "-input = torch.rand(3, 5, 5, device=device, dtype=dtype).expand(batch_size, -1, -1, -1) +inpt = torch.rand(3, 5, 5, device=device, dtype=dtype).expand(batch_size, -1, -1, -1) -actual = unsharp_mask(input, kernel_size, sigma, \"replicate\") +actual = unsharp_mask(inpt, kernel_size, sigma, \"replicate\")",
        "core_API": "rand"
    },
    {
        "commit_hash": "c74dc58f8b38bb947b903d3f92efbd8d1cd675ce",
        "index": "66bb04f78..253e1a35f 100644",
        "commit_message": "[RLlib] Fix `use_lstm` flag for ModelV2 (w/o ModelV1 wrapping) and add it for PyTorch. (#8734)\n\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "from ray.rllib.utils import try_import_torch",
            "_, nn = try_import_torch()",
            "",
            "",
            "-class VisionNetwork(TorchModelV2, nn.Module):",
            "+class VisionNetwork(TorchModelV2):",
            "\"\"\"Generic vision network.\"\"\"",
            "",
            "def __init__(self, obs_space, action_space, num_outputs, model_config,",
            "name):",
            "TorchModelV2.__init__(self, obs_space, action_space, num_outputs,",
            "model_config, name)",
            "-        nn.Module.__init__(self)",
            "",
            "activation = get_activation_fn(",
            "model_config.get(\"conv_activation\"), framework=\"torch\")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=nn))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=Module))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=identifier, text=nn))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=Module))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=__init__))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 18,
        "number": 6688,
        "neg_line": [
            "-class VisionNetwork(TorchModelV2, nn.Module):",
            "-nn.Module.__init__(self)"
        ],
        "pos_line": [
            "+class VisionNetwork(TorchModelV2):"
        ],
        "core_change": "-class VisionNetwork(TorchModelV2, nn.Module): +class VisionNetwork(TorchModelV2): -nn.Module.__init__(self)",
        "core_API": "__init__"
    },
    {
        "commit_hash": "d669a74623f273f74213a88b5233964d1ab3ea08",
        "index": "9036b26..e442ed7 100644",
        "commit_message": "Detect.py supports running against a Triton container (#9228)\n\n* update coco128-seg comments\n\n* Enables detect.py to use Triton for inference\n\nTriton Inference Server is an open source inference serving software\nthat streamlines AI inferencing.\nhttps://github.com/triton-inference-server/server\n\nThe user can now provide a \"--triton-url\" argument to detect.py to use\na local or remote Triton server for inference.\nFor e.g., http://localhost:8000 will use http over port 8000\nand grpc://localhost:8001 will use grpc over port 8001.\nNote, it is not necessary to specify a weights file to use Triton.\n\nA Triton container can be created by first exporting the Yolov5 model\nto a Triton supported runtime. Onnx, Torchscript, TensorRT are\nsupported by both Triton and the export.py script.\n\nThe exported model can then be containerized via the OctoML CLI.\nSee https://github.com/octoml/octo-cli#getting-started for a guide.\n\n* added triton client to requirements\n\n* fixed support for TFSavedModels in Triton\n\n* reverted change\n\n* Test CoreML update\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Update ci-testing.yml\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Use pathlib\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Refacto DetectMultiBackend to directly accept triton url as --weights http://...\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Deploy category\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Update detect.py\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Update common.py\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Update common.py\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Update predict.py\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Update predict.py\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Update predict.py\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Update triton.py\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Update triton.py\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Add printout and requirements check\n\n* Cleanup\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* triton fixes\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fixed triton model query over grpc\n\n* Update check_requirements('tritonclient[all]')\n\n* group imports\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix likely remote URL bug\n\n* update comment\n\n* Update is_url()\n\n* Fix 2x download attempt on http://path/to/model.pt\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\nCo-authored-by: glennjocher <glenn.jocher@ultralytics.com>\nCo-authored-by: Gaz Iqbal <giqbal@octoml.ai>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "yolov5.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def run(",
            "",
            "def parse_opt():",
            "parser = argparse.ArgumentParser()",
            "-    parser.add_argument('--weights', nargs='+', type=str, default=ROOT / 'yolov5s.pt', help='model path(s)')",
            "+    parser.add_argument('--weights', nargs='+', type=str, default=ROOT / 'yolov5s.pt', help='model path or triton URL')",
            "parser.add_argument('--source', type=str, default=ROOT / 'data/images', help='file/dir/URL/glob/screen/0(webcam)')",
            "parser.add_argument('--data', type=str, default=ROOT / 'data/coco128.yaml', help='(optional) dataset.yaml path')",
            "parser.add_argument('--imgsz', '--img', '--img-size', nargs='+', type=int, default=[640], help='inference size h,w')"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text='model path(s)'), value=\"'model path or triton URL'\")"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 6693,
        "neg_line": [
            "-parser.add_argument('--weights', nargs='+', type=str, default=ROOT / 'yolov5s.pt', help='model path(s)')"
        ],
        "pos_line": [
            "+parser.add_argument('--weights', nargs='+', type=str, default=ROOT / 'yolov5s.pt', help='model path or triton URL')"
        ],
        "core_change": "-parser.add_argument('--weights', nargs='+', type=str, default=ROOT / 'yolov5s.pt', help='model path(s)') +parser.add_argument('--weights', nargs='+', type=str, default=ROOT / 'yolov5s.pt', help='model path or triton URL')",
        "core_API": "ArgumentParser"
    },
    {
        "commit_hash": "9e3627ae6a1e1be80bdac6799b36ae7c29f5ae41",
        "index": "ed058e53..54a479be 100644",
        "commit_message": "Fix mixed precision serialization of group convs\n\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Conv(Layer):",
            "inputs = tf.pad(inputs, self._compute_causal_padding(inputs))",
            "",
            "if self.groups > 1:",
            "-            outputs = self._jit_compiled_convolution_op(inputs, self.kernel)",
            "+            outputs = self._jit_compiled_convolution_op(",
            "+                inputs, tf.convert_to_tensor(self.kernel)",
            "+            )",
            "else:",
            "outputs = self.convolution_op(inputs, self.kernel)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=3, insert_id=2057109)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=4, insert_id=2057110)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2057111)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2057112)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2057113)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2057114)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'convert_to_tensor'), position=2, insert_id=2057115)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2057116)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=attribute), position=1)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=), text=)), position=2)"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 6695,
        "neg_line": [
            "-outputs = self._jit_compiled_convolution_op(inputs, self.kernel)"
        ],
        "pos_line": [
            "+outputs = self._jit_compiled_convolution_op(",
            "+inputs, tf.convert_to_tensor(self.kernel)",
            "+)"
        ],
        "core_change": "-outputs = self._jit_compiled_convolution_op(inputs, self.kernel) +outputs = self._jit_compiled_convolution_op( +inputs, tf.convert_to_tensor(self.kernel) +)",
        "core_API": "pad"
    },
    {
        "commit_hash": "07708793f20ec3a949ccab32cc4fe0c7272dcc4c",
        "index": "71d8c6deb..d61ee8f67 100644",
        "commit_message": "fix encoder outputs (#8368)\n\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFGenerationMixin:",
            "shape=(-1,),",
            ")",
            "# expand encoder_outputs",
            "-            encoder_outputs = (tf.gather(encoder_outputs[0], expanded_batch_idxs, axis=0), *encoder_outputs[1:])",
            "-",
            "+            encoder_outputs = (tf.gather(encoder_outputs[0], expanded_batch_idxs, axis=0),)",
            "else:",
            "encoder_outputs = None",
            "cur_len = shape_list(input_ids)[-1]"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=*, text=*))",
            "Delete(target_node=ASTNode(type=identifier, text=encoder_outputs))",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=slice))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))",
            "Delete(target_node=ASTNode(type=list_splat))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 6696,
        "neg_line": [
            "-encoder_outputs = (tf.gather(encoder_outputs[0], expanded_batch_idxs, axis=0), *encoder_outputs[1:])",
            "-"
        ],
        "pos_line": [
            "+encoder_outputs = (tf.gather(encoder_outputs[0], expanded_batch_idxs, axis=0),)"
        ],
        "core_change": "-encoder_outputs = (tf.gather(encoder_outputs[0], expanded_batch_idxs, axis=0), *encoder_outputs[1:]) - +encoder_outputs = (tf.gather(encoder_outputs[0], expanded_batch_idxs, axis=0),)",
        "core_API": "gather"
    },
    {
        "commit_hash": "8e66a12f62cdabf9e81ddb02be1fadf554672389",
        "index": "5ec9d72b..9a9a4fd3 100644",
        "commit_message": "fixes for exporter issue of bi-transformer model (#597)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/fairseq/pull/597\n\nPull Request resolved: https://github.com/facebookresearch/pytext/pull/424\n\nFixes two issues:\n1. the new Layernorm has issues in exporting\n2. fix tensorboard writing by using the \"RAW\" operator_export_type\n\nDifferential Revision: D14610694\n\nfbshipit-source-id: 1b859f54c571a90766128ab28539a9901375c3e6\n\n",
        "file": "fairseq.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "import torch",
            "",
            "",
            "-def LayerNorm(normalized_shape, eps=1e-5, elementwise_affine=True):",
            "-    if torch.cuda.is_available():",
            "+def LayerNorm(normalized_shape, eps=1e-5, elementwise_affine=True, export=False):",
            "+    if not export and torch.cuda.is_available():",
            "try:",
            "from apex.normalization import FusedLayerNorm",
            "return FusedLayerNorm(normalized_shape, eps, elementwise_affine)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=6, insert_id=219716)",
            "Insert(target_node=ASTNode(type=parameters), node=('default_parameter', None), position=7, insert_id=219717)",
            "Insert(target_node=IN(type=default_parameter), node=('identifier', 'export'), position=0, insert_id=219718)",
            "Insert(target_node=IN(type=default_parameter), node=('=', '='), position=1, insert_id=219719)",
            "Insert(target_node=IN(type=default_parameter), node=('false', 'False'), position=2, insert_id=219720)",
            "Insert(target_node=ASTNode(type=if_statement), node=('boolean_operator', None), position=1, insert_id=219721)",
            "Insert(target_node=IN(type=boolean_operator), node=('not_operator', None), position=0, insert_id=219722)",
            "Insert(target_node=IN(type=boolean_operator), node=('and', 'and'), position=1, insert_id=219723)",
            "Move(target_node=IN(type=boolean_operator), node=ASTNode(type=call), position=2)",
            "Insert(target_node=IN(type=not_operator), node=('not', 'not'), position=0, insert_id=219724)",
            "Insert(target_node=IN(type=not_operator), node=('identifier', 'export'), position=1, insert_id=219725)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 6697,
        "neg_line": [
            "-def LayerNorm(normalized_shape, eps=1e-5, elementwise_affine=True):",
            "-if torch.cuda.is_available():"
        ],
        "pos_line": [
            "+def LayerNorm(normalized_shape, eps=1e-5, elementwise_affine=True, export=False):",
            "+if not export and torch.cuda.is_available():"
        ],
        "core_change": "-def LayerNorm(normalized_shape, eps=1e-5, elementwise_affine=True): -if torch.cuda.is_available(): +def LayerNorm(normalized_shape, eps=1e-5, elementwise_affine=True, export=False): +if not export and torch.cuda.is_available():",
        "core_API": "is_available"
    },
    {
        "commit_hash": "903d9c791a7d05702772ef6a9bff01dcc5da9fa8",
        "index": "7c0f95e1..cc241c43 100644",
        "commit_message": "Fix for FloorDiv Function Warning (#1760)\n\n* Fix for Floor Function Warning\n\nFix for Floor Function Warning\n\n* Adding double quotes to fix formatting\n\nAdding double quotes to fix formatting\n\n* Update glow_tts.py\n\n* Update glow_tts.py\n",
        "file": "TTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class GlowTTS(BaseTTS):",
            "y = y[:, :, :y_max_length]",
            "if attn is not None:",
            "attn = attn[:, :, :, :y_max_length]",
            "-        y_lengths = (y_lengths // self.num_squeeze) * self.num_squeeze",
            "+        y_lengths = torch.div(y_lengths, self.num_squeeze, rounding_mode=\"floor\") * self.num_squeeze",
            "return y, y_lengths, y_max_length, attn",
            "",
            "def store_inverse(self):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=binary_operator), node=('call', None), position=0, insert_id=1252637)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1252638)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1252639)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1252640)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1252641)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'div'), position=2, insert_id=1252642)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=(, text=(), position=0)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=y_lengths), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=1252643)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=attribute), position=3)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=4, insert_id=1252644)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=5, insert_id=1252645)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=), text=)), position=6)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'rounding_mode'), position=0, insert_id=1252646)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1252647)",
            "Insert(target_node=IN(type=keyword_argument), node=('string', '\"floor\"'), position=2, insert_id=1252648)",
            "Delete(target_node=ASTNode(type=//, text=//))",
            "Delete(target_node=ASTNode(type=binary_operator))",
            "Delete(target_node=ASTNode(type=parenthesized_expression))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 19,
        "number": 6700,
        "neg_line": [
            "-y_lengths = (y_lengths // self.num_squeeze) * self.num_squeeze"
        ],
        "pos_line": [
            "+y_lengths = torch.div(y_lengths, self.num_squeeze, rounding_mode=\"floor\") * self.num_squeeze"
        ],
        "core_change": "-y_lengths = (y_lengths // self.num_squeeze) * self.num_squeeze +y_lengths = torch.div(y_lengths, self.num_squeeze, rounding_mode=\"floor\") * self.num_squeeze",
        "core_API": "div"
    },
    {
        "commit_hash": "0c13ea4949a9fa709a59681f6906eeb2f8047e42",
        "index": "163bf2fb..ba558aef 100644",
        "commit_message": "fix QAT ema issue and tensor type error (#3219)\n\n\n",
        "file": "nni.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class QuantGrad(torch.autograd.Function):",
            "raise ValueError(\"unrecognized QuantType.\")",
            "",
            "bits = QuantGrad.get_bits_length(wrapper.config, quant_type)",
            "-        qmin, qmax = torch.Tensor([0], device=tensor.device), torch.Tensor([(1 << bits) - 1], device=tensor.device)",
            "+        qmin, qmax = torch.Tensor([0]).to(device=tensor.device), torch.Tensor([(1 << bits)-1]).to(device=tensor.device)",
            "ctx.save_for_backward(tensor, wrapper.module.scale, wrapper.module.zero_point, qmin, qmax)",
            "return output"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=665369)",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=665370)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=665371)",
            "Insert(target_node=IN(type=attribute), node=('call', None), position=0, insert_id=665372)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=665373)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to'), position=2, insert_id=665374)",
            "Insert(target_node=IN(type=attribute), node=('call', None), position=0, insert_id=665375)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=665376)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to'), position=2, insert_id=665377)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=665378)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=keyword_argument), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=665379)",
            "Move(target_node=IN(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=665380)",
            "Move(target_node=IN(type=call), node=ASTNode(type=attribute), position=0)",
            "Move(target_node=IN(type=call), node=ASTNode(type=argument_list), position=1)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=665381)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=list), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=665382)",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 21,
        "number": 6706,
        "neg_line": [
            "-qmin, qmax = torch.Tensor([0], device=tensor.device), torch.Tensor([(1 << bits) - 1], device=tensor.device)"
        ],
        "pos_line": [
            "+qmin, qmax = torch.Tensor([0]).to(device=tensor.device), torch.Tensor([(1 << bits)-1]).to(device=tensor.device)"
        ],
        "core_change": "-qmin, qmax = torch.Tensor([0], device=tensor.device), torch.Tensor([(1 << bits) - 1], device=tensor.device) +qmin, qmax = torch.Tensor([0]).to(device=tensor.device), torch.Tensor([(1 << bits)-1]).to(device=tensor.device)",
        "core_API": "get_bits_length"
    },
    {
        "commit_hash": "34f69931f501e92d6044d1abccd1225207638f20",
        "index": "b84be69c..fe3f7c96 100644",
        "commit_message": "small fix in grayscale conversion and add opencv tests\n\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def rgb_to_grayscale(input: torch.Tensor) -> torch.Tensor:",
            ".format(input.shape))",
            "",
            "r, g, b = torch.chunk(input, chunks=3, dim=-3)",
            "-    gray: torch.Tensor = 0.299 * r + 0.587 * g + 0.110 * b",
            "+    gray: torch.Tensor = 0.299 * r + 0.587 * g + 0.114 * b",
            "return gray"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=float, text=0.110), value='0.114')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 6709,
        "neg_line": [
            "-gray: torch.Tensor = 0.299 * r + 0.587 * g + 0.110 * b"
        ],
        "pos_line": [
            "+gray: torch.Tensor = 0.299 * r + 0.587 * g + 0.114 * b"
        ],
        "core_change": "-gray: torch.Tensor = 0.299 * r + 0.587 * g + 0.110 * b +gray: torch.Tensor = 0.299 * r + 0.587 * g + 0.114 * b",
        "core_API": "chunk"
    },
    {
        "commit_hash": "24a43d5060adb1b523a018eeee17d5ca75b2b23a",
        "index": "f14d03c5..e9733f61 100644",
        "commit_message": "fix async test (#2820)\n\n* fix async test\n\n* minor fix\n",
        "file": "mmdetection.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class AsyncInferenceTestCase(AsyncTestCase):",
            "img_path = os.path.join(root_dir, 'demo/demo.jpg')",
            "bboxes, _ = await detector.apredict(img_path)",
            "self.assertTrue(bboxes)",
            "+            # asy inference detector will hack grad_enabled,",
            "+            # so restore here to avoid it to influence other tests",
            "+            torch.set_grad_enabled(ori_grad_enabled)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=4, insert_id=637608)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=637609)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=637610)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=637611)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=637612)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=637613)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'set_grad_enabled'), position=2, insert_id=637614)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=637615)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'ori_grad_enabled'), position=1, insert_id=637616)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=637617)"
        ],
        "plus_line": 3,
        "minus_line": 0,
        "AST_diff_line": 10,
        "number": 6710,
        "neg_line": [],
        "pos_line": [
            "+# asy inference detector will hack grad_enabled,",
            "+# so restore here to avoid it to influence other tests",
            "+torch.set_grad_enabled(ori_grad_enabled)"
        ],
        "core_change": "+# asy inference detector will hack grad_enabled, +# so restore here to avoid it to influence other tests +torch.set_grad_enabled(ori_grad_enabled)",
        "core_API": "join"
    },
    {
        "commit_hash": "613b3fbae8444d085a02531cb80fd9da10ae9b7e",
        "index": "620fd2b..fb72c73 100644",
        "commit_message": "Fix bug in dot-style attention\n\n",
        "file": "seq2seq.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def att_sum_bahdanau(v_att, keys, query):",
            "@function.Defun(tf.float32, tf.float32, func_name=\"att_sum_dot\", noinline=True)",
            "def att_sum_dot(keys, query):",
            "\"\"\"Calculates a batch- and timweise dot product\"\"\"",
            "-  return tf.reduce_sum(keys + tf.expand_dims(query, 1), [2])",
            "+  return tf.reduce_sum(keys * tf.expand_dims(query, 1), [2])",
            "",
            "",
            "class AttentionLayer(GraphModule):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=binary_operator), node=('*', '*'), position=1, insert_id=2166049)",
            "Delete(target_node=ASTNode(type=+, text=+))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 6711,
        "neg_line": [
            "-return tf.reduce_sum(keys + tf.expand_dims(query, 1), [2])"
        ],
        "pos_line": [
            "+return tf.reduce_sum(keys * tf.expand_dims(query, 1), [2])"
        ],
        "core_change": "-return tf.reduce_sum(keys + tf.expand_dims(query, 1), [2]) +return tf.reduce_sum(keys * tf.expand_dims(query, 1), [2])",
        "core_API": "Defun"
    },
    {
        "commit_hash": "a966834755378c6fdb92d1b7d0aebf818de7479f",
        "index": "f6d4c273..bc268820 100644",
        "commit_message": "[BugFix] fix compression bugs (#5140)\n\n\n",
        "file": "nni.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "optim",
        "change": [
            "def main():",
            "'op_names': ['features.6', 'features.9', 'features.13', 'features.16', 'features.20', 'classifier.2', 'classifier.5']",
            "}]",
            "",
            "-    quantizer = BNNQuantizer(model, configure_list)",
            "+    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)",
            "+    quantizer = BNNQuantizer(model, configure_list, optimizer)",
            "model = quantizer.compress()",
            "",
            "print('=' * 10 + 'train' + '=' * 10)",
            "-    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)",
            "best_top1 = 0",
            "for epoch in range(400):",
            "print('# Epoch {} #'.format(epoch))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=expression_statement), node=ASTNode(type=module), position=3)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=1602579)",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'optimizer'), position=5, insert_id=1602580)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 3,
        "number": 6714,
        "neg_line": [
            "-quantizer = BNNQuantizer(model, configure_list)",
            "-optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)"
        ],
        "pos_line": [
            "+optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)",
            "+quantizer = BNNQuantizer(model, configure_list, optimizer)"
        ],
        "core_change": "-quantizer = BNNQuantizer(model, configure_list) +optimizer = torch.optim.Adam(model.parameters(), lr=1e-2) +quantizer = BNNQuantizer(model, configure_list, optimizer) -optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)",
        "core_API": "Adam"
    },
    {
        "commit_hash": "f4781a0b27ffb3ea61ecd25b0b87305e0960304e",
        "index": "475ceef4..68342403 100644",
        "commit_message": "update expected results of slow tests (#268)\n\n* update expected results of slow tests\n\n* relax sum and mean tests\n\n* Print shapes when reporting exception\n\n* formatting\n\n* fix sentence\n\n* relax test_stable_diffusion_fast_ddim for gpu fp16\n\n* relax flakey tests on GPU\n\n* added comment on large tolerences\n\n* black\n\n* format\n\n* set scheduler seed\n\n* added generator\n\n* use np.isclose\n\n* set num_inference_steps to 50\n\n* fix dep. warning\n\n* update expected_slice\n\n* preprocess if image\n\n* updated expected results\n\n* updated expected from CI\n\n* pass generator to VAE\n\n* undo change back to orig\n\n* use orignal\n\n* revert back the expected on cpu\n\n* revert back values for CPU\n\n* more undo\n\n* update result after using gen\n\n* update mean\n\n* set generator for mps\n\n* update expected on CI server\n\n* undo\n\n* use new seed every time\n\n* cpu manual seed\n\n* reduce num_inference_steps\n\n* style\n\n* use generator for randn\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class StableDiffusionImg2ImgPipeline(DiffusionPipeline):",
            "",
            "self.scheduler.set_timesteps(num_inference_steps, **extra_set_kwargs)",
            "",
            "-        if not isinstance(init_image, torch.FloatTensor):",
            "+        if isinstance(init_image, PIL.Image.Image):",
            "init_image = preprocess(init_image)",
            "",
            "# encode the init image into latents and scale the latents"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=if_statement), node=ASTNode(type=call), position=1)",
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=3, insert_id=104834)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Image'), position=2, insert_id=104835)",
            "Update(target_node=ASTNode(type=identifier, text=torch), value='PIL')",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=104836)",
            "Update(target_node=ASTNode(type=identifier, text=FloatTensor), value='Image')",
            "Delete(target_node=ASTNode(type=not, text=not))",
            "Delete(target_node=ASTNode(type=not_operator))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 6716,
        "neg_line": [
            "-if not isinstance(init_image, torch.FloatTensor):"
        ],
        "pos_line": [
            "+if isinstance(init_image, PIL.Image.Image):"
        ],
        "core_change": "-if not isinstance(init_image, torch.FloatTensor): +if isinstance(init_image, PIL.Image.Image):",
        "core_API": "set_timesteps"
    },
    {
        "commit_hash": "62567e942b3d7c728d2b15a925eb34524803afec",
        "index": "731e8259a..96bf3a2f3 100644",
        "commit_message": "Fix flake8 (#1975)\n\n* Fix flake8\n\n* Rename k to line\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class GigaFrenConfig(datasets.BuilderConfig):",
            "",
            "",
            "class GigaFren(datasets.GeneratorBasedBuilder):",
            "-    BUILDER_CONFIGS = [GigaFrenConfig(description=f\"Translating en to fr \", version=datasets.Version(_VERSION))]",
            "+    BUILDER_CONFIGS = [GigaFrenConfig(description=\"Translating en to fr \", version=datasets.Version(_VERSION))]",
            "BUILDER_CONFIG_CLASS = GigaFrenConfig",
            "",
            "def _info(self):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=f\"Translating en to fr \"), value='\"Translating en to fr \"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 6717,
        "neg_line": [
            "-BUILDER_CONFIGS = [GigaFrenConfig(description=f\"Translating en to fr \", version=datasets.Version(_VERSION))]"
        ],
        "pos_line": [
            "+BUILDER_CONFIGS = [GigaFrenConfig(description=\"Translating en to fr \", version=datasets.Version(_VERSION))]"
        ],
        "core_change": "-BUILDER_CONFIGS = [GigaFrenConfig(description=f\"Translating en to fr \", version=datasets.Version(_VERSION))] +BUILDER_CONFIGS = [GigaFrenConfig(description=\"Translating en to fr \", version=datasets.Version(_VERSION))]",
        "core_API": "Version"
    },
    {
        "commit_hash": "1ea2239a6ea4ca8f4b1d314ccc7d7c2e03257b6d",
        "index": "e77ad345d..7ab8f437d 100644",
        "commit_message": "Better error message when using the wrong load_from_disk (#2437)\n\n* better error message when using the wrong load_from_disk\n\n* better message\n\n* fix\n\n* Update arrow_dataset.py\n\n* Update dataset_dict.py\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class Dataset(DatasetInfoMixin, IndexableMixin):",
            "- if `dataset_path` is a path of a dataset dict directory: a :class:`DatasetDict` with each split.",
            "\"\"\"",
            "# copies file from filesystem if it is remote filesystem to local filesystem and modifies dataset_path to temp directory containing local copies",
            "+        fs = fsspec.filesystem(\"file\") if fs is None else fs",
            "+        dataset_dict_json_path = Path(dataset_path, config.DATASETDICT_JSON_FILENAME).as_posix()",
            "+        dataset_info_path = Path(dataset_path, config.DATASET_INFO_FILENAME).as_posix()",
            "+        if not fs.isfile(dataset_info_path) and fs.isfile(dataset_dict_json_path):",
            "+            raise FileNotFoundError(",
            "+                f\"No such file or directory: '{dataset_info_path}'. Expected to load a Dataset object, but got a DatasetDict. Please use datasets.load_from_disk instead.\"",
            "+            )",
            "+",
            "if is_remote_filesystem(fs):",
            "src_dataset_path = extract_path_from_uri(dataset_path)",
            "tmp_dir = tempfile.TemporaryDirectory()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=ERROR), node=(')', ')'), position=1, insert_id=1784968)",
            "Delete(target_node=ASTNode(type=if, text=if))",
            "Delete(target_node=ASTNode(type=string, text=`dataset_path`))",
            "Delete(target_node=ASTNode(type=is, text=is))",
            "Delete(target_node=ASTNode(type=identifier, text=a))",
            "Delete(target_node=ASTNode(type=identifier, text=path))",
            "Delete(target_node=ASTNode(type=identifier, text=of))",
            "Delete(target_node=ASTNode(type=identifier, text=a))",
            "Delete(target_node=ASTNode(type=identifier, text=dataset))",
            "Delete(target_node=ASTNode(type=identifier, text=dict))",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=identifier, text=directory))",
            "Delete(target_node=ASTNode(type=comparison_operator))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=identifier, text=a))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=identifier, text=class))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=string, text=`DatasetDict`))",
            "Delete(target_node=ASTNode(type=identifier, text=with))",
            "Delete(target_node=ASTNode(type=identifier, text=each))",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=identifier, text=split))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=ERROR))"
        ],
        "plus_line": 7,
        "minus_line": 1,
        "AST_diff_line": 25,
        "number": 6720,
        "neg_line": [
            "-if `dataset_path` is a path of a dataset dict directory: a :class:`DatasetDict` with each split."
        ],
        "pos_line": [
            "+fs = fsspec.filesystem(\"file\") if fs is None else fs",
            "+dataset_dict_json_path = Path(dataset_path, config.DATASETDICT_JSON_FILENAME).as_posix()",
            "+dataset_info_path = Path(dataset_path, config.DATASET_INFO_FILENAME).as_posix()",
            "+if not fs.isfile(dataset_info_path) and fs.isfile(dataset_dict_json_path):",
            "+raise FileNotFoundError(",
            "+f\"No such file or directory: '{dataset_info_path}'. Expected to load a Dataset object, but got a DatasetDict. Please use datasets.load_from_disk instead.\"",
            "+)",
            "+"
        ],
        "core_change": "-if `dataset_path` is a path of a dataset dict directory: a :class:`DatasetDict` with each split. +fs = fsspec.filesystem(\"file\") if fs is None else fs +dataset_dict_json_path = Path(dataset_path, config.DATASETDICT_JSON_FILENAME).as_posix() +dataset_info_path = Path(dataset_path, config.DATASET_INFO_FILENAME).as_posix() +if not fs.isfile(dataset_info_path) and fs.isfile(dataset_dict_json_path): +raise FileNotFoundError( +f\"No such file or directory: '{dataset_info_path}'. Expected to load a Dataset object, but got a DatasetDict. Please use datasets.load_from_disk instead.\" +) +",
        "core_API": "filesystem"
    },
    {
        "commit_hash": "878226490718964caf6aea67ea9e0631dc8832fe",
        "index": "9f083c4df..92bad5e70 100644",
        "commit_message": "fixed a bug in the case of dlayers>2 and dtype=gru\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class Decoder(torch.nn.Module):",
            "self.dropout_dec += [torch.nn.Dropout(p=dropout)]",
            "for _ in six.moves.range(1, self.dlayers):",
            "self.decoder += [",
            "-                torch.nn.LSTMCell(dunits, dunits) if self.dtype == \"lstm\" else torch.nn.GRUCell(dunits + eprojs,",
            "-                                                                                                dunits)]",
            "+                torch.nn.LSTMCell(dunits, dunits) if self.dtype == \"lstm\" else torch.nn.GRUCell(dunits, dunits)]",
            "self.dropout_dec += [torch.nn.Dropout(p=dropout)]",
            "# NOTE: dropout is applied only for the vertical connections",
            "# see https://arxiv.org/pdf/1409.2329.pdf"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=dunits), position=1)",
            "Delete(target_node=ASTNode(type=+, text=+))",
            "Delete(target_node=ASTNode(type=identifier, text=eprojs))",
            "Delete(target_node=ASTNode(type=binary_operator))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 4,
        "number": 6725,
        "neg_line": [
            "-torch.nn.LSTMCell(dunits, dunits) if self.dtype == \"lstm\" else torch.nn.GRUCell(dunits + eprojs,",
            "-dunits)]"
        ],
        "pos_line": [
            "+torch.nn.LSTMCell(dunits, dunits) if self.dtype == \"lstm\" else torch.nn.GRUCell(dunits, dunits)]"
        ],
        "core_change": "-torch.nn.LSTMCell(dunits, dunits) if self.dtype == \"lstm\" else torch.nn.GRUCell(dunits + eprojs, -dunits)] +torch.nn.LSTMCell(dunits, dunits) if self.dtype == \"lstm\" else torch.nn.GRUCell(dunits, dunits)]",
        "core_API": "Dropout"
    },
    {
        "commit_hash": "f38aaa5be05104ddfc3452249eeb521f139ded99",
        "index": "59d44f16..7a3a8ef6 100644",
        "commit_message": "improved global tensor handling, various other fixes\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Lstm(TransformationBase):",
            "",
            "self.cell = tf.contrib.rnn.LSTMCell(num_units=self.lstm_size)",
            "# if self.lstm_dropout is not None:",
            "-        #     keep_prob = tf.cond(pred=update, true_fn=(lambda: 1.0 - self.lstm_dropout), false_fn=(lambda: 1.0))",
            "+        #     keep_prob = self.cond(pred=update, true_fn=(lambda: 1.0 - self.lstm_dropout), false_fn=(lambda: 1.0))",
            "#     self.lstm_cell = tf.contrib.rnn.DropoutWrapper(cell=self.lstm_cell, output_keep_prob=keep_prob)",
            "",
            "def tf_apply(self, x, sequence_length=None):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 6726,
        "neg_line": [
            "-#     keep_prob = tf.cond(pred=update, true_fn=(lambda: 1.0 - self.lstm_dropout), false_fn=(lambda: 1.0))"
        ],
        "pos_line": [
            "+#     keep_prob = self.cond(pred=update, true_fn=(lambda: 1.0 - self.lstm_dropout), false_fn=(lambda: 1.0))"
        ],
        "core_change": "-#     keep_prob = tf.cond(pred=update, true_fn=(lambda: 1.0 - self.lstm_dropout), false_fn=(lambda: 1.0)) +#     keep_prob = self.cond(pred=update, true_fn=(lambda: 1.0 - self.lstm_dropout), false_fn=(lambda: 1.0))",
        "core_API": "LSTMCell"
    },
    {
        "commit_hash": "8eafef3ff59bafebf10806c82563a64dbbd4d336",
        "index": "7bbb30e94..ab2e643fd 100644",
        "commit_message": "fix output\n\n",
        "file": "tutorials.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "\"source\": [",
            "\"import numpy as np\\n\",",
            "\"a = np.ones(5)\\n\",",
            "-    \"b = torch.DoubleTensor(a)\\n\",",
            "+    \"b = torch.from_numpy(a)\\n\",",
            "\"np.add(a, 1, out=a)\\n\",",
            "\"print(a)\\n\",",
            "\"print(b) # see how changing the np array changed the torch Tensor automatically\""
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"b = torch.DoubleTensor(a)\\n\"), value='\"b = torch.from_numpy(a)\\\\n\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 6729,
        "neg_line": [
            "-\"b = torch.DoubleTensor(a)\\n\","
        ],
        "pos_line": [
            "+\"b = torch.from_numpy(a)\\n\","
        ],
        "core_change": "-\"b = torch.DoubleTensor(a)\\n\", +\"b = torch.from_numpy(a)\\n\",",
        "core_API": "ones"
    },
    {
        "commit_hash": "7d3f3a404caa6ea2912cab15a7cc3031c19dd52b",
        "index": "e41842719..b4f72759c 100644",
        "commit_message": "Bug fix about multi gpu due to recent changings\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class E2E(torch.nn.Module):",
            "else:",
            "logging.warning('loss (=%f) is not correct', loss_data)",
            "",
            "-        return self.loss, loss_ctc, loss_att, acc, cer, wer",
            "+        # Note(kamo): In order to work with DataParallel, on pytorch==0.4,",
            "+        # the return value must be torch.CudaTensor, or tuple/list/dict of it.",
            "+        # Neither CPUTensor nor float/int value can be used",
            "+        # because NCCL communicates between GPU devices.",
            "+        return self.loss",
            "",
            "def recognize(self, x, recog_args, char_list, rnnlm=None):",
            "\"\"\"E2E beam search"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=return_statement), node=ASTNode(type=attribute), position=1)",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=loss_ctc))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=loss_att))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=acc))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=cer))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=wer))",
            "Delete(target_node=ASTNode(type=expression_list))"
        ],
        "plus_line": 5,
        "minus_line": 1,
        "AST_diff_line": 12,
        "number": 6734,
        "neg_line": [
            "-return self.loss, loss_ctc, loss_att, acc, cer, wer"
        ],
        "pos_line": [
            "+# Note(kamo): In order to work with DataParallel, on pytorch==0.4,",
            "+# the return value must be torch.CudaTensor, or tuple/list/dict of it.",
            "+# Neither CPUTensor nor float/int value can be used",
            "+# because NCCL communicates between GPU devices.",
            "+return self.loss"
        ],
        "core_change": "-return self.loss, loss_ctc, loss_att, acc, cer, wer +# Note(kamo): In order to work with DataParallel, on pytorch==0.4, +# the return value must be torch.CudaTensor, or tuple/list/dict of it. +# Neither CPUTensor nor float/int value can be used +# because NCCL communicates between GPU devices. +return self.loss",
        "core_API": "warning"
    },
    {
        "commit_hash": "1ec9f17b6ee580e9bb627100e44195462b141882",
        "index": "b27d151..dc206a1 100644",
        "commit_message": "add support for tensorflow1.15 (#818)\n\n* add support for tensorflow1.15\n\n* Fix typo in requirements/linux-py3.txt\n\nCo-authored-by: HX Lin <>\n",
        "file": "MMdnn.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def KitModel(weight_file = None):",
            "IR_node.name)",
            "return code",
            "",
            "-    def emit_Maxmum(self, IR_node):",
            "-        code = \"{:<15} = tf.maxmum({}, {}, name='{}')\".format(",
            "+    def emit_Maximum(self, IR_node):",
            "+        code = \"{:<15} = tf.maximum({}, {}, name='{}')\".format(",
            "IR_node.variable_name,",
            "self.parent_variable_name(IR_node),",
            "self.parent_variable_name(IR_node, [1]),"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=emit_Maxmum), value='emit_Maximum')",
            "Update(target_node=ASTNode(type=string, text=\"{:<15} = tf.maxmum({}, {}, name='{}')\"), value='\"{:<15} = tf.maximum({}, {}, name=\\'{}\\')\"')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 6737,
        "neg_line": [
            "-def emit_Maxmum(self, IR_node):",
            "-code = \"{:<15} = tf.maxmum({}, {}, name='{}')\".format("
        ],
        "pos_line": [
            "+def emit_Maximum(self, IR_node):",
            "+code = \"{:<15} = tf.maximum({}, {}, name='{}')\".format("
        ],
        "core_change": "-def emit_Maxmum(self, IR_node): -code = \"{:<15} = tf.maxmum({}, {}, name='{}')\".format( +def emit_Maximum(self, IR_node): +code = \"{:<15} = tf.maximum({}, {}, name='{}')\".format(",
        "core_API": "maxmum"
    },
    {
        "commit_hash": "42180bd929639c137706be99c93626da93a36f2e",
        "index": "395c145..6e42929 100644",
        "commit_message": "Forward/backward compatibility around pytorch 3aeb78, to fix #191\n\n",
        "file": "apex.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "cuda",
        "change": [
            "if \"--cuda_ext\" in sys.argv:",
            "'csrc/welford.cu']))",
            "ext_modules.append(",
            "CUDAExtension(name='fused_layer_norm_cuda',",
            "-                          sources=['apex/normalization/csrc/layer_norm_cuda.cpp',",
            "-                                   'apex/normalization/csrc/layer_norm_cuda_kernel.cu'],",
            "+                          sources=['csrc/layer_norm_cuda.cpp',",
            "+                                   'csrc/layer_norm_cuda_kernel.cu'],",
            "extra_compile_args={'cxx': ['-O3'] + version_ge_1_1,",
            "'nvcc':['-maxrregcount=50',",
            "'-O3',"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text='apex/normalization/csrc/layer_norm_cuda.cpp'), value=\"'csrc/layer_norm_cuda.cpp'\")",
            "Update(target_node=ASTNode(type=string, text='apex/normalization/csrc/layer_norm_cuda_kernel.cu'), value=\"'csrc/layer_norm_cuda_kernel.cu'\")"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 6739,
        "neg_line": [
            "-sources=['apex/normalization/csrc/layer_norm_cuda.cpp',",
            "-'apex/normalization/csrc/layer_norm_cuda_kernel.cu'],"
        ],
        "pos_line": [
            "+sources=['csrc/layer_norm_cuda.cpp',",
            "+'csrc/layer_norm_cuda_kernel.cu'],"
        ],
        "core_change": "-sources=['apex/normalization/csrc/layer_norm_cuda.cpp', -'apex/normalization/csrc/layer_norm_cuda_kernel.cu'], +sources=['csrc/layer_norm_cuda.cpp', +'csrc/layer_norm_cuda_kernel.cu'],",
        "core_API": "append"
    },
    {
        "commit_hash": "449b7b0c5b2408f97d2a316653379d09bafd57a5",
        "index": "4b2e15c4..e6ada0bd 100644",
        "commit_message": "fix named pipe open mode (#3183)\n\nCo-authored-by: liuzhe <zhe.liu@microsoft.com>\n",
        "file": "nni.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "else:",
            "",
            "def connect(self) -> BufferedIOBase:",
            "conn, _ = self._socket.accept()",
            "-            self.file = conn.makefile('w+b')",
            "+            self.file = conn.makefile('rwb')",
            "return self.file",
            "",
            "def close(self) -> None:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text='w+b'), value=\"'rwb'\")"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 6740,
        "neg_line": [
            "-self.file = conn.makefile('w+b')"
        ],
        "pos_line": [
            "+self.file = conn.makefile('rwb')"
        ],
        "core_change": "-self.file = conn.makefile('w+b') +self.file = conn.makefile('rwb')",
        "core_API": "accept"
    },
    {
        "commit_hash": "de4159a318f9a4f2f474733306c00ac326e643a7",
        "index": "16106ad30..05e7a2307 100644",
        "commit_message": "More TF int dtype fixes (#20384)\n\n* Add a test to ensure int dummy inputs are int64\n\n* Move the test into the existing int64 test and update a lot of existing dummies\n\n* Fix remaining dummies\n\n* Fix remaining dummies\n\n* Test for int64 serving sigs as well\n\n* Update core tests to use tf.int64\n\n* Add better messages to the assertions\n\n* Update all serving sigs to int64\n\n* More sneaky hiding tf.int32s\n\n* Add an optional int32 signature in save_pretrained\n\n* make fixup\n\n* Add Amy's suggestions\n\n* Switch all serving sigs back to tf.int32\n\n* Switch all dummies to tf.int32\n\n* Adjust tests to check for tf.int32 instead of tf.int64\n\n* Fix base dummy_inputs dtype\n\n* Start casting to tf.int32 in input_processing\n\n* Change dtype for unpack_inputs test\n\n* Add proper tf.int32 test\n\n* Make the alternate serving signature int64\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFMBartPreTrainedModel(TFPreTrainedModel):",
            "decoder_input_ids = tf.cast(tf.convert_to_tensor(DUMMY_INPUTS), tf.int32)",
            "dummy_inputs = {",
            "\"decoder_input_ids\": decoder_input_ids,",
            "-            \"attention_mask\": tf.math.not_equal(input_ids, pad_token),",
            "+            \"attention_mask\": tf.cast(input_ids != pad_token, tf.int32),",
            "\"input_ids\": input_ids,",
            "}",
            "return dummy_inputs"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=math), value='cast')",
            "Insert(target_node=ASTNode(type=argument_list), node=('comparison_operator', None), position=1, insert_id=2358056)",
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=4, insert_id=2358057)",
            "Move(target_node=IN(type=comparison_operator), node=ASTNode(type=identifier, text=input_ids), position=0)",
            "Insert(target_node=IN(type=comparison_operator), node=('!=', '!='), position=1, insert_id=2358058)",
            "Move(target_node=IN(type=comparison_operator), node=ASTNode(type=identifier, text=pad_token), position=2)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2358059)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2358060)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'int32'), position=2, insert_id=2358061)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=not_equal))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 13,
        "number": 6753,
        "neg_line": [
            "-\"attention_mask\": tf.math.not_equal(input_ids, pad_token),"
        ],
        "pos_line": [
            "+\"attention_mask\": tf.cast(input_ids != pad_token, tf.int32),"
        ],
        "core_change": "-\"attention_mask\": tf.math.not_equal(input_ids, pad_token), +\"attention_mask\": tf.cast(input_ids != pad_token, tf.int32),",
        "core_API": "cast"
    },
    {
        "commit_hash": "9067c6bdcdc177cad76b6b644a9174c4fda0a88f",
        "index": "8c75e1af..da6b5f99 100755",
        "commit_message": "fixed problems with multiple LSTMs, introduced scope argument for layers\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Categorical(Distribution):",
            "self.probabilities = tf.maximum(x=self.probabilities, y=util.epsilon)",
            "",
            "# \"Normalized\" logits",
            "-        self.logits = tf.log(x=self.probabilities + util.epsilon)",
            "+        self.logits = tf.log(x=self.probabilities)",
            "",
            "def sample(self):",
            "# Deterministic: maximum likelihood action"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=keyword_argument), node=ASTNode(type=attribute), position=2)",
            "Delete(target_node=ASTNode(type=+, text=+))",
            "Delete(target_node=ASTNode(type=identifier, text=util))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=epsilon))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=binary_operator))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 6755,
        "neg_line": [
            "-self.logits = tf.log(x=self.probabilities + util.epsilon)"
        ],
        "pos_line": [
            "+self.logits = tf.log(x=self.probabilities)"
        ],
        "core_change": "-self.logits = tf.log(x=self.probabilities + util.epsilon) +self.logits = tf.log(x=self.probabilities)",
        "core_API": "maximum"
    },
    {
        "commit_hash": "fbb778683d55cbba9422564f61cc149fcfdb1460",
        "index": "fc2b4434..db6ef1c6 100755",
        "commit_message": "fixed DQN LSTM problem, modified memories to support next_internals, added test for VPG with LSTM\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def lstm(x, size=None):",
            "",
            "with tf.variable_scope('lstm'):",
            "internal_input = tf.placeholder(dtype=tf.float32, shape=(None, 2, size))",
            "-        lstm = tf.contrib.rnn.LSTMCell(num_units=size)",
            "+        lstm_cell = tf.contrib.rnn.LSTMCell(num_units=size)",
            "c = internal_input[:, 0, :]",
            "h = internal_input[:, 1, :]",
            "state = tf.contrib.rnn.LSTMStateTuple(c=c, h=h)",
            "-        x, state = lstm(inputs=x, state=state)",
            "+        x, state = lstm_cell(inputs=x, state=state)",
            "",
            "internal_output = tf.stack(values=(state.c, state.h), axis=1)",
            "internal_init = np.zeros(shape=(2, size))"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=lstm), value='lstm_cell')",
            "Update(target_node=ASTNode(type=identifier, text=lstm), value='lstm_cell')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 6757,
        "neg_line": [
            "-lstm = tf.contrib.rnn.LSTMCell(num_units=size)",
            "-x, state = lstm(inputs=x, state=state)"
        ],
        "pos_line": [
            "+lstm_cell = tf.contrib.rnn.LSTMCell(num_units=size)",
            "+x, state = lstm_cell(inputs=x, state=state)"
        ],
        "core_change": "-lstm = tf.contrib.rnn.LSTMCell(num_units=size) +lstm_cell = tf.contrib.rnn.LSTMCell(num_units=size) -x, state = lstm(inputs=x, state=state) +x, state = lstm_cell(inputs=x, state=state)",
        "core_API": "variable_scope"
    },
    {
        "commit_hash": "e8b41cfeec0b70b692e1a042e6f1e542f1fe446d",
        "index": "98d63522..960280a6 100644",
        "commit_message": "Fix `DBLP` download, move `get_edge_index` to `testing` package (#6901)\n\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_graph_store():",
            "def test_graph_store_conversion():",
            "graph_store = MyGraphStore()",
            "",
            "-    coo = (row, col) = get_edge_index(100, 100, 300)",
            "+    coo = (row, col) = get_random_edge_index(100, 100, 300)",
            "adj = SparseTensor(row=row, col=col, sparse_sizes=(100, 100))",
            "csr, csc = adj.csr()[:2], adj.csc()[:2][::-1]"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=get_edge_index), value='get_random_edge_index')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 6759,
        "neg_line": [
            "-coo = (row, col) = get_edge_index(100, 100, 300)"
        ],
        "pos_line": [
            "+coo = (row, col) = get_random_edge_index(100, 100, 300)"
        ],
        "core_change": "-coo = (row, col) = get_edge_index(100, 100, 300) +coo = (row, col) = get_random_edge_index(100, 100, 300)",
        "core_API": "csr"
    },
    {
        "commit_hash": "9b6385488617ee99188b60c0525cbe4ec7e8f2a0",
        "index": "40e026e2..3043ee20 100644",
        "commit_message": "Improve reproduceability 2/3 (#1906)\n\n* [Repro] Correct reproducability\n\n* up\n\n* up\n\n* uP\n\n* up\n\n* need better image\n\n* allow conversion from no state dict checkpoints\n\n* up\n\n* up\n\n* up\n\n* up\n\n* check tensors\n\n* check tensors\n\n* check tensors\n\n* check tensors\n\n* next try\n\n* up\n\n* up\n\n* better name\n\n* up\n\n* up\n\n* Apply suggestions from code review\n\n* correct more\n\n* up\n\n* replace all torch randn\n\n* fix\n\n* correct\n\n* correct\n\n* finish\n\n* fix more\n\n* up\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class StableDiffusionInpaintPipelineLegacy(DiffusionPipeline):",
            "init_latents_orig = init_latents",
            "",
            "# add noise to latents using the timesteps",
            "-        noise = torch.randn(init_latents.shape, generator=generator, device=self.device, dtype=dtype)",
            "+        noise = randn_tensor(init_latents.shape, generator=generator, device=self.device, dtype=dtype)",
            "init_latents = self.scheduler.add_noise(init_latents, noise, timestep)",
            "latents = init_latents",
            "return latents, init_latents_orig, noise"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=torch), value='randn_tensor')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=torch), position=0)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=randn))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 6763,
        "neg_line": [
            "-noise = torch.randn(init_latents.shape, generator=generator, device=self.device, dtype=dtype)"
        ],
        "pos_line": [
            "+noise = randn_tensor(init_latents.shape, generator=generator, device=self.device, dtype=dtype)"
        ],
        "core_change": "-noise = torch.randn(init_latents.shape, generator=generator, device=self.device, dtype=dtype) +noise = randn_tensor(init_latents.shape, generator=generator, device=self.device, dtype=dtype)",
        "core_API": "randn"
    },
    {
        "commit_hash": "cea5f1f53876399dfaa0d37accdc527af7ca39af",
        "index": "3a979c25d..59f5e9975 100644",
        "commit_message": "Fix for `load_from_checkpoint` (#2776)\n\n* Fix.\n\n* Fix #2550: allow to load model from checkpoint if self.save_hyperparameters() was not called.\n\n* Fix? Cleaner way of not calling self.save_hyperparameters in EvalModelTemplate.\n\n* Fix? `_load_model_state` cleanup\n\n* Fix?\n\n* Fix #2550: allow to load model from checkpoint if self.save_hyperparameters() was not called.\n\n* Fix.\n\n* Fix? Cleaner way of not calling self.save_hyperparameters in EvalModelTemplate.\n\n* Fix? `_load_model_state` cleanup\n\n* Fixed side effect in `test_load_model_from_checkpoint_extra_args`.\n\n* Apply suggestions from code review\n\n* fix\n\n* try\n\n* fixed missing arg in evalmodel\n\n* fixed missing arg in evalmodel\n\n* fix\n\n* update\n\n* fix loading\n\n* add test\n\n* prune\n\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\nCo-authored-by: Jirka Borovec <jirka@pytorchlightning.ai>\nCo-authored-by: William Falcon <waf2107@columbia.edu>\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_load_model_from_checkpoint(tmpdir, model_template):",
            "for (old_name, old_p), (new_name, new_p) in zip(model.named_parameters(), pretrained_model.named_parameters()):",
            "assert torch.all(torch.eq(old_p, new_p)), 'loaded weights are not the same as the saved weights'",
            "",
            "+    # Check `test` on pretrained model:",
            "new_trainer = Trainer(**trainer_options)",
            "new_trainer.test(pretrained_model)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 17,
        "number": 6775,
        "neg_line": [],
        "pos_line": [
            "+# Check `test` on pretrained model:"
        ],
        "core_change": "+# Check `test` on pretrained model:",
        "core_API": "named_parameters"
    },
    {
        "commit_hash": "8b598bc7b88bbb30149ff955914efb2b64fbcbce",
        "index": "a8b0a8669e..08de4d8df7 100644",
        "commit_message": "lintfixbot: Auto-commit fixed lint errors in codebase\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def unique_all(",
            "decimal = tf.range(tf.size(inverse_indices)) / tf.size(inverse_indices)",
            "inv_sorted = tf.argsort(tf.cast(inverse_indices, dtype=decimal.dtype) + decimal)",
            "tot_counts = tf.concat(",
            "-            [tf.zeros((1,), dtype=counts.dtype), tf.cumsum(counts, axis=0)[:-1]], 0)",
            "+            [tf.zeros((1,), dtype=counts.dtype), tf.cumsum(counts, axis=0)[:-1]], 0",
            "+        )",
            "indices = inv_sorted.numpy()[tot_counts]",
            "",
            "return Results("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 6777,
        "neg_line": [
            "-[tf.zeros((1,), dtype=counts.dtype), tf.cumsum(counts, axis=0)[:-1]], 0)"
        ],
        "pos_line": [
            "+[tf.zeros((1,), dtype=counts.dtype), tf.cumsum(counts, axis=0)[:-1]], 0",
            "+)"
        ],
        "core_change": "-[tf.zeros((1,), dtype=counts.dtype), tf.cumsum(counts, axis=0)[:-1]], 0) +[tf.zeros((1,), dtype=counts.dtype), tf.cumsum(counts, axis=0)[:-1]], 0 +)",
        "core_API": "range"
    },
    {
        "commit_hash": "37030801ab3904fbe6de8df5a59b82d019665f7d",
        "index": "fd449f7d60..a9d8daed51 100644",
        "commit_message": "Fixed the matrix_rank function for numpy, jax and tensorflow (#4452)\n\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def matrix_rank(",
            "out: Optional[torch.Tensor] = None,",
            ") -> torch.Tensor:",
            "# ToDo: add support for default rtol value here, for the case where None is provided",
            "-    ret = torch.linalg.matrix_rank(x, atol=rtol, out=out)",
            "+    ret = torch.linalg.matrix_rank(x, rtol=rtol, out=out)",
            "return torch.tensor(ret, dtype=ivy.default_int_dtype(as_native=True))",
            "",
            "",
            "-matrix_rank.unsupported_dtypes = (\"float16\",)",
            "+matrix_rank.unsupported_dtypes = (",
            "+    \"float16\",",
            "+    \"bfloat16\",",
            "+)",
            "matrix_rank.support_native_out = True"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=tuple), node=('string', '\"bfloat16\"'), position=3, insert_id=327406)",
            "Insert(target_node=ASTNode(type=tuple), node=(',', ','), position=4, insert_id=327407)",
            "Update(target_node=ASTNode(type=identifier, text=atol), value='rtol')"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 3,
        "number": 6779,
        "neg_line": [
            "-ret = torch.linalg.matrix_rank(x, atol=rtol, out=out)",
            "-matrix_rank.unsupported_dtypes = (\"float16\",)"
        ],
        "pos_line": [
            "+ret = torch.linalg.matrix_rank(x, rtol=rtol, out=out)",
            "+matrix_rank.unsupported_dtypes = (",
            "+\"float16\",",
            "+\"bfloat16\",",
            "+)"
        ],
        "core_change": "-ret = torch.linalg.matrix_rank(x, atol=rtol, out=out) +ret = torch.linalg.matrix_rank(x, rtol=rtol, out=out) -matrix_rank.unsupported_dtypes = (\"float16\",) +matrix_rank.unsupported_dtypes = ( +\"float16\", +\"bfloat16\", +)",
        "core_API": "matrix_rank"
    },
    {
        "commit_hash": "8f8fe80d6c3852ac6c9e789aa6bde2db88ce50c9",
        "index": "918d9052..9f877618 100644",
        "commit_message": "FeedfreePredictor and example on ImageNet eval (fix #772)\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class NewSessionCreator(tf.train.ChiefSessionCreator):",
            "\"\"\"",
            "Args:",
            "target, graph, config: same as :meth:`Session.__init__()`.",
            "-            config: defaults to :func:`tfutils.get_default_sess_config()`",
            "+            config: a :class:`tf.ConfigProto` instance, defaults to :func:`tfutils.get_default_sess_config()`",
            "\"\"\"",
            "assert graph is None"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"\"\"\nArgs:\ntarget, graph, config: same as :meth:`Session.__init__()`.\n            config: defaults to :func:`tfutils.get_default_sess_config()`\n\"\"\"), value='\"\"\"\\nArgs:\\ntarget, graph, config: same as :meth:`Session.__init__()`.\\n            config: a :class:`tf.ConfigProto` instance, defaults to :func:`tfutils.get_default_sess_config()`\\n\"\"\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 6780,
        "neg_line": [
            "-config: defaults to :func:`tfutils.get_default_sess_config()`"
        ],
        "pos_line": [
            "+config: a :class:`tf.ConfigProto` instance, defaults to :func:`tfutils.get_default_sess_config()`"
        ],
        "core_change": "-config: defaults to :func:`tfutils.get_default_sess_config()` +config: a :class:`tf.ConfigProto` instance, defaults to :func:`tfutils.get_default_sess_config()`",
        "core_API": "__init__"
    },
    {
        "commit_hash": "3e050bdb4ca4b65ea39c8f70293b3b8ce1c40ed7",
        "index": "1732b0e..04cf619 100644",
        "commit_message": "Fix pylint errors\n\n",
        "file": "seq2seq.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class PipelineTest(tf.test.TestCase):",
            "tf.app.flags.FLAGS.output_dir = self.output_dir",
            "tf.app.flags.FLAGS.metrics = yaml.dump([",
            "\"log_perplexity\", \"bleu\", \"rouge_1/f_score\", \"rouge_l/f_score\"])",
            "-",
            "-    \"\"\"",
            "-      log_perplexity,bleu,rouge_1/f_score,rouge_l/f_score\"\"\"",
            "-    # tf.app.flags.FLAGS.train_source = sources_train.name",
            "-    # tf.app.flags.FLAGS.train_target = targets_train.name",
            "-    # tf.app.flags.FLAGS.vocab_source = vocab_source.name",
            "-    # tf.app.flags.FLAGS.vocab_target = vocab_target.name",
            "tf.app.flags.FLAGS.model = \"AttentionSeq2Seq\"",
            "tf.app.flags.FLAGS.model_params = \"\"\"",
            "attention.params:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=string, text=\"\"\"\n      log_perplexity,bleu,rouge_1/f_score,rouge_l/f_score\"\"\"))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 0,
        "minus_line": 6,
        "AST_diff_line": 2,
        "number": 6784,
        "neg_line": [
            "-",
            "-\"\"\"",
            "-log_perplexity,bleu,rouge_1/f_score,rouge_l/f_score\"\"\"",
            "-# tf.app.flags.FLAGS.train_source = sources_train.name",
            "-# tf.app.flags.FLAGS.train_target = targets_train.name",
            "-# tf.app.flags.FLAGS.vocab_source = vocab_source.name",
            "-# tf.app.flags.FLAGS.vocab_target = vocab_target.name"
        ],
        "pos_line": [],
        "core_change": "- -\"\"\" -log_perplexity,bleu,rouge_1/f_score,rouge_l/f_score\"\"\" -# tf.app.flags.FLAGS.train_source = sources_train.name -# tf.app.flags.FLAGS.train_target = targets_train.name -# tf.app.flags.FLAGS.vocab_source = vocab_source.name -# tf.app.flags.FLAGS.vocab_target = vocab_target.name",
        "core_API": "dump"
    },
    {
        "commit_hash": "ffecb4e3496379edf5ecae1483df5b7e0886c264",
        "index": "ab7deaa1..ef259b6f 100644",
        "commit_message": "Small fixes (#1215)\n\nSummary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1215\n\nReviewed By: ngoyal2707, msbaines\n\nDifferential Revision: D22514719\n\nPulled By: myleott\n\nfbshipit-source-id: 5f15ba501fd66af1eb49b5702aff940f06c3d91f\n\n",
        "file": "fairseq.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def _main(args, output_file):",
            "# Fix seed for stochastic decoding",
            "if args.seed is not None and not args.no_seed_provided:",
            "np.random.seed(args.seed)",
            "-        torch.manual_seed(args.seed)",
            "+        utils.set_torch_seed(args.seed)",
            "",
            "use_cuda = torch.cuda.is_available() and not args.cpu"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=torch), value='utils')",
            "Update(target_node=ASTNode(type=identifier, text=manual_seed), value='set_torch_seed')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 6797,
        "neg_line": [
            "-torch.manual_seed(args.seed)"
        ],
        "pos_line": [
            "+utils.set_torch_seed(args.seed)"
        ],
        "core_change": "-torch.manual_seed(args.seed) +utils.set_torch_seed(args.seed)",
        "core_API": "seed"
    },
    {
        "commit_hash": "e2e13229d997b93f5f0731e324f1497086fb25c5",
        "index": "c9964daf..4767b518 100644",
        "commit_message": "Beautified code, fixed docstrings, added future statements\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def get_flattened_gradient(loss, variables):",
            "\"\"\"",
            "gradients = tf.gradients(loss, variables)",
            "",
            "-    return tf.concat(axis=0, values=[tf.reshape(grad, [get_number_of_elements(v)])",
            "-                         for (v, grad) in zip(variables, gradients)])",
            "+    return tf.concat(axis=0, values=[tf.reshape(grad, [get_number_of_elements(v)]) for (v, grad) in zip(variables, gradients)])",
            "",
            "",
            "class FlatVarHelper(object):"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 6801,
        "neg_line": [
            "-return tf.concat(axis=0, values=[tf.reshape(grad, [get_number_of_elements(v)])",
            "-for (v, grad) in zip(variables, gradients)])"
        ],
        "pos_line": [
            "+return tf.concat(axis=0, values=[tf.reshape(grad, [get_number_of_elements(v)]) for (v, grad) in zip(variables, gradients)])"
        ],
        "core_change": "-return tf.concat(axis=0, values=[tf.reshape(grad, [get_number_of_elements(v)]) -for (v, grad) in zip(variables, gradients)]) +return tf.concat(axis=0, values=[tf.reshape(grad, [get_number_of_elements(v)]) for (v, grad) in zip(variables, gradients)])",
        "core_API": "gradients"
    },
    {
        "commit_hash": "dfe60bbeac955057a49bf6e0230d7d90ce1c56be",
        "index": "90f224a..9b8b4c2 100755",
        "commit_message": "fix FC_with_w\n\n",
        "file": "UGATIT.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def fully_connected_with_w(x, use_bias=True, sn=False, reuse=False, scope='linea",
            "if sn :",
            "w = tf.get_variable(\"kernel\", [channels, 1], tf.float32,",
            "initializer=weight_init, regularizer=weight_regularizer)",
            "+            w = spectral_norm(w)",
            "+",
            "if use_bias :",
            "bias = tf.get_variable(\"bias\", [1],",
            "initializer=tf.constant_initializer(0.0))",
            "",
            "-                x = tf.matmul(x, spectral_norm(w)) + bias",
            "+                x = tf.matmul(x, w) + bias",
            "else :",
            "-                x = tf.matmul(x, spectral_norm(w))",
            "+                x = tf.matmul(x, w)",
            "+",
            "else :",
            "x = tf.layers.dense(x, units=1, kernel_initializer=weight_init, kernel_regularizer=weight_regularizer, use_bias=use_bias)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=ERROR), node=('call', None), position=8, insert_id=2389776)",
            "Move(target_node=IN(type=call), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=call), node=('ERROR', None), position=1, insert_id=2389777)",
            "Move(target_node=IN(type=call), node=ASTNode(type=argument_list), position=2)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'w'), position=0, insert_id=2389778)",
            "Insert(target_node=IN(type=ERROR), node=('=', '='), position=1, insert_id=2389779)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'spectral_norm'), position=2, insert_id=2389780)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=w), position=3)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=), text=)), position=4)",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'w'), position=3, insert_id=2389781)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=4, insert_id=2389782)",
            "Delete(target_node=ASTNode(type=identifier, text=spectral_norm))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=identifier, text=spectral_norm))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=), text=)))"
        ],
        "plus_line": 3,
        "minus_line": 2,
        "AST_diff_line": 19,
        "number": 6803,
        "neg_line": [
            "-x = tf.matmul(x, spectral_norm(w)) + bias",
            "-x = tf.matmul(x, spectral_norm(w))"
        ],
        "pos_line": [
            "+w = spectral_norm(w)",
            "+",
            "+x = tf.matmul(x, w) + bias",
            "+x = tf.matmul(x, w)",
            "+"
        ],
        "core_change": "+w = spectral_norm(w) + -x = tf.matmul(x, spectral_norm(w)) + bias +x = tf.matmul(x, w) + bias -x = tf.matmul(x, spectral_norm(w)) +x = tf.matmul(x, w) +",
        "core_API": "get_variable"
    },
    {
        "commit_hash": "0219d279e30b5ac26e7a759047a3e9c7d67cf221",
        "index": "a566ca3..b4a2a7c 100644",
        "commit_message": "minor fix in blockwise sparsifier (#1130)\n\nSummary:\nPull Request resolved: https://github.com/facebookresearch/pytext/pull/1130\n\ncompute absolute value of the parameters\n\nReviewed By: arbabu123\n\nDifferential Revision: D18466312\n\nfbshipit-source-id: ff8546e5f4071e7a5be767071fdae9b363dba035\n\n",
        "file": "pytext.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class BlockwiseMagnitudeSparsifier(L0_projection_sparsifier):",
            "pre_mask=(pre_mask.transpose(1, 0) if pre_mask else None),",
            ").transpose(1, 0)",
            "padded_param = self._padding_into_full_blocks(param)",
            "-        block_l1norms = padded_param.reshape(-1, 1, self.block_size).sum(dim=2)",
            "+        block_l1norms = (",
            "+            torch.abs(padded_param).reshape(-1, 1, self.block_size).sum(dim=2)",
            "+        )",
            "max_num_blocks = self._num_blocks_kept(param)",
            "",
            "topk_threshold = ("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('parenthesized_expression', None), position=2, insert_id=879339)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('(', '('), position=0, insert_id=879340)",
            "Move(target_node=IN(type=parenthesized_expression), node=ASTNode(type=call), position=1)",
            "Insert(target_node=IN(type=parenthesized_expression), node=(')', ')'), position=2, insert_id=879341)",
            "Insert(target_node=ASTNode(type=attribute), node=('call', None), position=0, insert_id=879342)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=879343)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=879344)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=879345)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=879346)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'abs'), position=2, insert_id=879347)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=879348)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=padded_param), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=879349)"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 13,
        "number": 6804,
        "neg_line": [
            "-block_l1norms = padded_param.reshape(-1, 1, self.block_size).sum(dim=2)"
        ],
        "pos_line": [
            "+block_l1norms = (",
            "+torch.abs(padded_param).reshape(-1, 1, self.block_size).sum(dim=2)",
            "+)"
        ],
        "core_change": "-block_l1norms = padded_param.reshape(-1, 1, self.block_size).sum(dim=2) +block_l1norms = ( +torch.abs(padded_param).reshape(-1, 1, self.block_size).sum(dim=2) +)",
        "core_API": "transpose"
    },
    {
        "commit_hash": "76bb45964df1e62d1411b0a9e9fc673e9a791c9a",
        "index": "1aa0f3b37..61970ea30 100644",
        "commit_message": "Add Image feature (#3163)\n\n* Initial commit\n\n* Add basic decoding\n\n* Replace features.Audio with Audio\n\n* Add Image to package reference\n\n* Use np.array\n\n* Update error msg\n\n* Add mode and channel decoding\n\n* Fix return value\n\n* Finish decoding\n\n* Make CI happy\n\n* Some more fixes\n\n* Minor doc fix\n\n* Remove animated option\n\n* Pin version\n\n* Remove unused imports in setup.py\n\n* Add vision requirements to setup.py\n\n* Add initial tests\n\n* Delete other formats\n\n* Make Image feature hashable\n\n* Add more tests\n\n* Support numpy array in alter data check in TypedSequence\n\n* Fix TypedSequence converion\n\n* Finish tests\n\n* Update Image - add ImageExtensionType and supporting functions\n\n* Update encoding functions\n\n* Add support in TypedSequence for ImageExtensionType\n\n* Add tests\n\n* Remove unused import\n\n* Fix doc and style\n\n* Fix doc indentation\n\n* Improve comment\n\n* Return single image instead of dict\n\n* Return PIL Image and not dict\n\n* Encode dict\n\n* Update tests\n\n* Style\n\n* np.ndarray encoding/decoding\n\n* Minor improvements\n\n* PIL Image support in cast_to_python_objects\n\n* Test cast\n\n* Doc fix\n\n* Extension type fixes\n\n* Style\n\n* Use types_mapper in Dataset.to_pandas\n\n* Add pandas extension array for image type\n\n* Update tests\n\n* image type inference\n\n* Remvoe cast_to_python test after Quentin's change\n\n* Improve tests\n\n* Add storage type\n\n* Improve tests\n\n* Test map that returns np.ndarray\n\n* Rename functions\n\n* Add streaming test\n\n* Use image struct in all situations\n\n* Update src/datasets/features/image.py - encode_example type hint\n\nCo-authored-by: Quentin Lhoest <42851186+lhoestq@users.noreply.github.com>\n\n* Update src/datasets/features/image.py -list_image_compression_formats type hint\n\nCo-authored-by: Quentin Lhoest <42851186+lhoestq@users.noreply.github.com>\n\n* Support str in encode_objects_to_image_dicts\n\n* Update src/datasets/features/image.py - objects_to_list_of_image_dicts type hint\n\nCo-authored-by: Quentin Lhoest <42851186+lhoestq@users.noreply.github.com>\n\n* Style\n\nCo-authored-by: Quentin Lhoest <lhoest.q@gmail.com>\nCo-authored-by: Quentin Lhoest <42851186+lhoestq@users.noreply.github.com>\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class LJSpeech(datasets.GeneratorBasedBuilder):",
            "features=datasets.Features(",
            "{",
            "\"id\": datasets.Value(\"string\"),",
            "-                    \"audio\": datasets.features.Audio(sampling_rate=22050),",
            "+                    \"audio\": datasets.Audio(sampling_rate=22050),",
            "\"file\": datasets.Value(\"string\"),",
            "\"text\": datasets.Value(\"string\"),",
            "\"normalized_text\": datasets.Value(\"string\"),"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=features))",
            "Delete(target_node=ASTNode(type=., text=.))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 6807,
        "neg_line": [
            "-\"audio\": datasets.features.Audio(sampling_rate=22050),"
        ],
        "pos_line": [
            "+\"audio\": datasets.Audio(sampling_rate=22050),"
        ],
        "core_change": "-\"audio\": datasets.features.Audio(sampling_rate=22050), +\"audio\": datasets.Audio(sampling_rate=22050),",
        "core_API": "Features"
    },
    {
        "commit_hash": "3dbd7b0b33016ba8c492c786ad1ddcd053fc707c",
        "index": "1aa10733..081e316a 100644",
        "commit_message": "batch fix if items are only sparsely available\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Batch(Data):",
            "for i, data in enumerate(data_list):",
            "num_nodes = data.num_nodes",
            "batch.batch.append(torch.full((num_nodes, ), i, dtype=torch.long))",
            "-            for key in keys:",
            "+            for key in data.keys:",
            "item = data[key]",
            "item = item + cumsum if batch.cumsum(key, item) else item",
            "batch[key].append(item)",
            "cumsum += num_nodes",
            "",
            "for key in keys:",
            "-            batch[key] = torch.cat(batch[key], dim=data_list[0].cat_dim(key))",
            "+            batch[key] = torch.cat(",
            "+                batch[key], dim=data_list[0].cat_dim(key, batch[key][0]))",
            "batch.batch = torch.cat(batch.batch, dim=-1)",
            "return batch.contiguous()"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=for_statement), node=('attribute', None), position=3, insert_id=1067609)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'data'), position=0, insert_id=1067610)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1067611)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=keys), position=2)",
            "Insert(target_node=ASTNode(type=assignment), node=('subscript', None), position=0, insert_id=1067612)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'batch'), position=0, insert_id=1067613)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=1067614)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'key'), position=2, insert_id=1067615)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=1067616)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1067617)",
            "Insert(target_node=ASTNode(type=argument_list), node=('subscript', None), position=3, insert_id=1067618)",
            "Move(target_node=IN(type=subscript), node=ASTNode(type=subscript), position=0)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=1067619)",
            "Insert(target_node=IN(type=subscript), node=('integer', '0'), position=2, insert_id=1067620)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=1067621)"
        ],
        "plus_line": 3,
        "minus_line": 2,
        "AST_diff_line": 15,
        "number": 6809,
        "neg_line": [
            "-for key in keys:",
            "-batch[key] = torch.cat(batch[key], dim=data_list[0].cat_dim(key))"
        ],
        "pos_line": [
            "+for key in data.keys:",
            "+batch[key] = torch.cat(",
            "+batch[key], dim=data_list[0].cat_dim(key, batch[key][0]))"
        ],
        "core_change": "-for key in keys: +for key in data.keys: -batch[key] = torch.cat(batch[key], dim=data_list[0].cat_dim(key)) +batch[key] = torch.cat( +batch[key], dim=data_list[0].cat_dim(key, batch[key][0]))",
        "core_API": "append"
    },
    {
        "commit_hash": "ce8f42b12ecd522e3ef7251e0d5f5175075a3fb4",
        "index": "36a9635e..39ca9038 100644",
        "commit_message": "Fixes to enable dev branch to move to pytorch-1.2 (#2001)\n\n* Fixes to enable dev branch to move to pytorch-1.2\n\n* change name for published cpu wheels\n\n* fix failing unit tests\n\n* change docs conf\n\n* change tracking example\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def scale_and_mask(tensor, scale=1.0, mask=None):",
            ":param scale: a positive scale",
            ":type scale: torch.Tensor or number",
            ":param mask: an optional packed tensor mask",
            "-    :type mask: torch.ByteTensor or None",
            "+    :type mask: torch.BoolTensor or None",
            "\"\"\"",
            "if isinstance(scale, torch.Tensor) and scale.dim():",
            "raise NotImplementedError('non-scalar scale is not supported')"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=ByteTensor), value='BoolTensor')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 6810,
        "neg_line": [
            "-:type mask: torch.ByteTensor or None"
        ],
        "pos_line": [
            "+:type mask: torch.BoolTensor or None"
        ],
        "core_change": "-:type mask: torch.ByteTensor or None +:type mask: torch.BoolTensor or None",
        "core_API": "dim"
    },
    {
        "commit_hash": "81c260fb483e92c39195a1ca36e65bed33868157",
        "index": "56748bfc..9748ef0d 100644",
        "commit_message": "Fixed audio/bag/binary feature tests to work on GPU (#1600)\n\n* Fixed audio/bag/binary feature tests to work on GPU\n\n* Added timeseries feature test (#1601)\n\n* Removed device transfer calls from encoders to features\n",
        "file": "ludwig.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def audio_config():",
            "@pytest.mark.parametrize(\"encoder\", [\"rnn\", \"stacked_cnn\", \"parallel_cnn\", \"stacked_parallel_cnn\", \"rnn\", \"cnnrnn\"])",
            "def test_audio_input_feature(audio_config: Dict, encoder: str) -> None:",
            "audio_config.update({\"encoder\": encoder})",
            "-    audio_input_feature = AudioInputFeature(audio_config)",
            "-    audio_tensor = torch.randn([BATCH_SIZE, SEQ_SIZE, AUDIO_W_SIZE], dtype=torch.float32)",
            "+    audio_input_feature = AudioInputFeature(audio_config).to(DEVICE)",
            "+    audio_tensor = torch.randn([BATCH_SIZE, SEQ_SIZE, AUDIO_W_SIZE], dtype=torch.float32).to(DEVICE)",
            "encoder_output = audio_input_feature(audio_tensor)",
            "-    assert encoder_output[\"encoder_output\"].shape[1:] == audio_input_feature.encoder_obj.output_shape",
            "+    assert encoder_output[\"encoder_output\"].shape[1:] == audio_input_feature.output_shape"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('call', None), position=2, insert_id=608102)",
            "Insert(target_node=ASTNode(type=assignment), node=('call', None), position=2, insert_id=608103)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=608104)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=608105)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=608106)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=608107)",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=audio_input_feature), position=0)",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=608108)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to'), position=2, insert_id=608109)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=608110)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'DEVICE'), position=1, insert_id=608111)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=608112)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=608113)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to'), position=2, insert_id=608114)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=608115)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'DEVICE'), position=1, insert_id=608116)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=608117)",
            "Delete(target_node=ASTNode(type=identifier, text=encoder_obj))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 23,
        "number": 6811,
        "neg_line": [
            "-audio_input_feature = AudioInputFeature(audio_config)",
            "-audio_tensor = torch.randn([BATCH_SIZE, SEQ_SIZE, AUDIO_W_SIZE], dtype=torch.float32)",
            "-assert encoder_output[\"encoder_output\"].shape[1:] == audio_input_feature.encoder_obj.output_shape"
        ],
        "pos_line": [
            "+audio_input_feature = AudioInputFeature(audio_config).to(DEVICE)",
            "+audio_tensor = torch.randn([BATCH_SIZE, SEQ_SIZE, AUDIO_W_SIZE], dtype=torch.float32).to(DEVICE)",
            "+assert encoder_output[\"encoder_output\"].shape[1:] == audio_input_feature.output_shape"
        ],
        "core_change": "-audio_input_feature = AudioInputFeature(audio_config) -audio_tensor = torch.randn([BATCH_SIZE, SEQ_SIZE, AUDIO_W_SIZE], dtype=torch.float32) +audio_input_feature = AudioInputFeature(audio_config).to(DEVICE) +audio_tensor = torch.randn([BATCH_SIZE, SEQ_SIZE, AUDIO_W_SIZE], dtype=torch.float32).to(DEVICE) -assert encoder_output[\"encoder_output\"].shape[1:] == audio_input_feature.encoder_obj.output_shape +assert encoder_output[\"encoder_output\"].shape[1:] == audio_input_feature.output_shape",
        "core_API": "parametrize"
    },
    {
        "commit_hash": "1d73566e4e2fe9a4f5e799b80f8506a0f92768e3",
        "index": "0642f290..daa517b9 100644",
        "commit_message": "bugfix in GST\n\n",
        "file": "TTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def main(args):  # pylint: disable=redefined-outer-name",
            "print(\" > Partial model initialization.\")",
            "model_dict = model.state_dict()",
            "model_dict = set_init_dict(model_dict, checkpoint['model'], c)",
            "+            # torch.save(model_dict, os.path.join(OUT_PATH, 'state_dict.pt'))",
            "+            # print(\"State Dict saved for debug in: \", os.path.join(OUT_PATH, 'state_dict.pt'))",
            "model.load_state_dict(model_dict)",
            "del model_dict"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 0,
        "AST_diff_line": 17,
        "number": 6816,
        "neg_line": [],
        "pos_line": [
            "+# torch.save(model_dict, os.path.join(OUT_PATH, 'state_dict.pt'))",
            "+# print(\"State Dict saved for debug in: \", os.path.join(OUT_PATH, 'state_dict.pt'))"
        ],
        "core_change": "+# torch.save(model_dict, os.path.join(OUT_PATH, 'state_dict.pt')) +# print(\"State Dict saved for debug in: \", os.path.join(OUT_PATH, 'state_dict.pt'))",
        "core_API": "state_dict"
    },
    {
        "commit_hash": "0070252e186dba391147d72544498eae493dfca1",
        "index": "24555cb20..118047696 100644",
        "commit_message": "Move complex tensor related functions to espnet2/enh/layers/complex_utils.py; Refactor complex support code; fix unit tests\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class MaskEstimator(torch.nn.Module):",
            "",
            "# Calculate amplitude: (B, C, T, F) -> (B, C, T, F)",
            "if isinstance(xs, ComplexTensor) or (",
            "-            is_torch_1_8_plus and torch.is_complex(xs)",
            "+            is_torch_1_9_plus and torch.is_complex(xs)",
            "):",
            "xs = (xs.real ** 2 + xs.imag ** 2) ** 0.5",
            "# xs: (B, C, T, F) -> xs: (B * C, T, F)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=is_torch_1_8_plus), value='is_torch_1_9_plus')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 6819,
        "neg_line": [
            "-is_torch_1_8_plus and torch.is_complex(xs)"
        ],
        "pos_line": [
            "+is_torch_1_9_plus and torch.is_complex(xs)"
        ],
        "core_change": "-is_torch_1_8_plus and torch.is_complex(xs) +is_torch_1_9_plus and torch.is_complex(xs)",
        "core_API": "is_complex"
    },
    {
        "commit_hash": "23f889bd3b27990298d923c25104d000df232a17",
        "index": "da229e6b..14be0d89 100644",
        "commit_message": "Fix Batch Normalization inference behavior when virtual_batch_size is set\n\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class BatchNormalizationBase(Layer):",
            "offset += then_offset",
            "return (scale, offset)",
            "",
            "-        # Determine a boolean value for `training`: could be True, False, or",
            "-        # None.",
            "-        training_value = control_flow_util.constant_value(training)",
            "if training_value == False:  # noqa: E712",
            "mean, variance = self.moving_mean, self.moving_variance",
            "else:"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=training_value))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=control_flow_util))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=constant_value))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=training))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 0,
        "minus_line": 3,
        "AST_diff_line": 13,
        "number": 6820,
        "neg_line": [
            "-# Determine a boolean value for `training`: could be True, False, or",
            "-# None.",
            "-training_value = control_flow_util.constant_value(training)"
        ],
        "pos_line": [],
        "core_change": "-# Determine a boolean value for `training`: could be True, False, or -# None. -training_value = control_flow_util.constant_value(training)",
        "core_API": "constant_value"
    },
    {
        "commit_hash": "5e9ab08ed3b803599424a08ec574f60f6aefd6cc",
        "index": "7cbd73e28..5bff756cd 100644",
        "commit_message": "New termwarn, fixed tensorflow2 tensorboard with logging, history._process works, made keras backwards compatable, removed http log from cli\n\n",
        "file": "wandb.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Watcher(object):",
            "self._generator = directory_watcher.DirectoryWatcher(",
            "logdir,",
            "loader(save, namespace),",
            "-            io_wrapper.IsTensorFlowEventsFile)",
            "+            IsNewTensorFlowEventsFile)",
            "self._first_event_timestamp = None",
            "self._shutdown = False",
            "self._thread = threading.Thread(target=self._thread_body)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=io_wrapper), value='IsNewTensorFlowEventsFile')",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=io_wrapper), position=5)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=IsTensorFlowEventsFile))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 6823,
        "neg_line": [
            "-io_wrapper.IsTensorFlowEventsFile)"
        ],
        "pos_line": [
            "+IsNewTensorFlowEventsFile)"
        ],
        "core_change": "-io_wrapper.IsTensorFlowEventsFile) +IsNewTensorFlowEventsFile)",
        "core_API": "DirectoryWatcher"
    },
    {
        "commit_hash": "9c568294d09b8df60267007ede6966ef9e869ee8",
        "index": "fe379b6c..b1548748 100644",
        "commit_message": "fix linter error\n\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SSIM(nn.Module):",
            "channel: int) -> torch.Tensor:",
            "return F.conv2d(input, kernel, padding=self.padding, groups=channel)",
            "",
            "-    def forward(self, img1: torch.Tensor, img2: torch.Tensor) -> torch.Tensor:  # type: ignore",
            "+    def forward(  # type: ignore",
            "+            self,",
            "+            img1: torch.Tensor,",
            "+            img2: torch.Tensor) -> torch.Tensor:",
            "if not torch.is_tensor(img1):",
            "raise TypeError(\"Input img1 type is not a torch.Tensor. Got {}\"",
            ".format(type(img1)))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 4,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 6824,
        "neg_line": [
            "-def forward(self, img1: torch.Tensor, img2: torch.Tensor) -> torch.Tensor:  # type: ignore"
        ],
        "pos_line": [
            "+def forward(  # type: ignore",
            "+self,",
            "+img1: torch.Tensor,",
            "+img2: torch.Tensor) -> torch.Tensor:"
        ],
        "core_change": "-def forward(self, img1: torch.Tensor, img2: torch.Tensor) -> torch.Tensor:  # type: ignore +def forward(  # type: ignore +self, +img1: torch.Tensor, +img2: torch.Tensor) -> torch.Tensor:",
        "core_API": "conv2d"
    },
    {
        "commit_hash": "efcf911a7f65bbc2a10ceef1a260df989bdcfff6",
        "index": "5d582b8234..34370acc13 100644",
        "commit_message": "Fixed failing test for searching where (#4744)\n\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def where(",
            "out: Optional[Union[tf.Tensor, tf.Variable]] = None,",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "x1, x2 = ivy.promote_types_of_inputs(x1, x2)",
            "-    return tf.experimental.numpy.where(condition, x1, x2)",
            "+    return tf.cast(tf.experimental.numpy.where(condition, x1, x2), x1.dtype)",
            "",
            "",
            "# Extra #"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1992471)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=1992472)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=1992473)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1992474)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cast'), position=2, insert_id=1992475)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1992476)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=call), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=1992477)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=3, insert_id=1992478)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=1992479)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'x1'), position=0, insert_id=1992480)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1992481)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dtype'), position=2, insert_id=1992482)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 13,
        "number": 6832,
        "neg_line": [
            "-return tf.experimental.numpy.where(condition, x1, x2)"
        ],
        "pos_line": [
            "+return tf.cast(tf.experimental.numpy.where(condition, x1, x2), x1.dtype)"
        ],
        "core_change": "-return tf.experimental.numpy.where(condition, x1, x2) +return tf.cast(tf.experimental.numpy.where(condition, x1, x2), x1.dtype)",
        "core_API": "promote_types_of_inputs"
    },
    {
        "commit_hash": "3cfc48ae521d0ae8c091c9fdeeabad3f47e6ef31",
        "index": "5c13beb4a..380e43857 100644",
        "commit_message": "fix using gpu when builtin float16\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class CTC(torch.nn.Module):",
            "ys_hat = ys_hat.transpose(0, 1)",
            "if self.ctc_type == \"warpctc\" or dtype == torch.float16:",
            "# warpctc only supports float32",
            "+            # torch.ctc does not support float16 (#1751)",
            "ys_hat = ys_hat.to(dtype=torch.float32)",
            "-        else:",
            "+        if self.ctc_type == \"builtin\":",
            "# use GPU when using the cuDNN implementation",
            "ys_true = to_device(self, ys_true)",
            "self.loss = to_device(self, self.loss_fn(ys_hat, ys_true, hlens, olens)).to(dtype=dtype)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=4, insert_id=158016)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=158017)",
            "Insert(target_node=IN(type=if_statement), node=('comparison_operator', None), position=1, insert_id=158018)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=158019)",
            "Insert(target_node=IN(type=if_statement), node=('block', ''), position=3, insert_id=158020)",
            "Insert(target_node=IN(type=comparison_operator), node=('attribute', None), position=0, insert_id=158021)",
            "Insert(target_node=IN(type=comparison_operator), node=('==', '=='), position=1, insert_id=158022)",
            "Insert(target_node=IN(type=comparison_operator), node=('string', '\"builtin\"'), position=2, insert_id=158023)",
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=identifier, text=ys_true), position=0)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=158024)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=158025)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'ctc_type'), position=2, insert_id=158026)",
            "Delete(target_node=ASTNode(type=identifier, text=else))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=type))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 15,
        "number": 6833,
        "neg_line": [
            "-else:"
        ],
        "pos_line": [
            "+# torch.ctc does not support float16 (#1751)",
            "+if self.ctc_type == \"builtin\":"
        ],
        "core_change": "+# torch.ctc does not support float16 (#1751) -else: +if self.ctc_type == \"builtin\":",
        "core_API": "transpose"
    },
    {
        "commit_hash": "775b43afc419ce43ffcf7061f025f108cf5b303d",
        "index": "c62bd6dc..d9c453e9 100644",
        "commit_message": "bugfix\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "print('F', F.grad)",
            "i = torch.LongTensor([[0, 1], [2, 0]])",
            "v = torch.FloatTensor([3, 4])",
            "a = torch.sparse.FloatTensor(i, v, torch.Size([3, 3]))",
            "+print(a.to_dense())",
            "f = torch.FloatTensor([[1, 2], [3, 4], [5, 6]])",
            "A = Variable(a, requires_grad=True)",
            "F = Variable(f, requires_grad=True)",
            "-",
            "out = mm()(A, F)",
            "out = out.mean()",
            "out.backward()"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=4, insert_id=1101778)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=1101779)",
            "Insert(target_node=IN(type=call), node=('identifier', 'print'), position=0, insert_id=1101780)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1101781)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1101782)",
            "Insert(target_node=IN(type=argument_list), node=('call', None), position=1, insert_id=1101783)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1101784)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1101785)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1101786)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'a'), position=0, insert_id=1101787)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1101788)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to_dense'), position=2, insert_id=1101789)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1101790)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1101791)"
        ],
        "plus_line": 0,
        "minus_line": 0,
        "AST_diff_line": 14,
        "number": 6834,
        "neg_line": [
            "-"
        ],
        "pos_line": [
            "+print(a.to_dense())"
        ],
        "core_change": "+print(a.to_dense()) -",
        "core_API": "LongTensor"
    },
    {
        "commit_hash": "95ab1563a510771eeb7abff2645a826e9981d85f",
        "index": "4f438c51..f71c84d9 100644",
        "commit_message": "support prefix for FeedfreeInferenceRunner\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Callback(object):",
            "def trigger_epoch(self):",
            "\"\"\"",
            "Triggered after every epoch.",
            "-",
            "-        In this function, ``self.epoch_num`` would be the number of epoch finished.",
            "\"\"\"",
            "self.epoch_num += 1",
            "self._trigger_epoch()"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"\"\"\nTriggered after every epoch.\n\n        In this function, ``self.epoch_num`` would be the number of epoch finished.\n\"\"\"), value='\"\"\"\\nTriggered after every epoch.\\n\"\"\"')"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 6835,
        "neg_line": [
            "-",
            "-In this function, ``self.epoch_num`` would be the number of epoch finished."
        ],
        "pos_line": [],
        "core_change": "- -In this function, ``self.epoch_num`` would be the number of epoch finished.",
        "core_API": "_trigger_epoch"
    },
    {
        "commit_hash": "c61e22a8e0e393b7d701611437f595656cf16003",
        "index": "09d77235..12e095a4 100644",
        "commit_message": "Fix unit test failures with torch==1.1.0 (#1840)\n\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def setup(app):",
            "",
            "# @jpchen's hack to get rtd builder to install latest pytorch",
            "if 'READTHEDOCS' in os.environ:",
            "-    os.system('pip install https://download.pytorch.org/whl/cpu/torch-1.0.0-cp27-cp27mu-linux_x86_64.whl')",
            "+    os.system('pip install https://download.pytorch.org/whl/cpu/torch-1.1.0-cp27-cp27mu-linux_x86_64.whl')"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text='pip install https://download.pytorch.org/whl/cpu/torch-1.0.0-cp27-cp27mu-linux_x86_64.whl'), value=\"'pip install https://download.pytorch.org/whl/cpu/torch-1.1.0-cp27-cp27mu-linux_x86_64.whl'\")"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 6838,
        "neg_line": [
            "-os.system('pip install https://download.pytorch.org/whl/cpu/torch-1.0.0-cp27-cp27mu-linux_x86_64.whl')"
        ],
        "pos_line": [
            "+os.system('pip install https://download.pytorch.org/whl/cpu/torch-1.1.0-cp27-cp27mu-linux_x86_64.whl')"
        ],
        "core_change": "-os.system('pip install https://download.pytorch.org/whl/cpu/torch-1.0.0-cp27-cp27mu-linux_x86_64.whl') +os.system('pip install https://download.pytorch.org/whl/cpu/torch-1.1.0-cp27-cp27mu-linux_x86_64.whl')",
        "core_API": "system"
    },
    {
        "commit_hash": "5fdae21c08345b635eeec4a47542c2f957f7cdc1",
        "index": "1c4fea3b35..3a19ae8556 100644",
        "commit_message": "Fixed failing gradient test for elementwise trunc (#6255)\n\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def trunc(",
            "elif not (\"int\" in str(x.dtype)):",
            "if not ret.get_shape().ndims == 0:",
            "ret = tf.tensor_scatter_nd_update(",
            "-                x, tf.where(tf.greater(x, 0)), tf.math.floor(x[x > 0])",
            "+                x, tf.where(tf.greater_equal(x, 0)), tf.math.floor(x[x >= 0])",
            ")",
            "ret = tf.tensor_scatter_nd_update(",
            "ret, tf.where(tf.less(x, 0)), tf.math.ceil(x[x < 0])",
            ")",
            "else:",
            "-            ret = (tf.math.floor if ret > 0 else tf.math.ceil)(ret)",
            "+            ret = (tf.math.floor if ret >= 0 else tf.math.ceil)(ret)",
            "return ret"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1982549)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1982550)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1982551)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'floor'), position=2, insert_id=1982552)",
            "Move(target_node=ASTNode(type=conditional_expression), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=1982553)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1982554)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'math'), position=2, insert_id=1982555)",
            "Insert(target_node=ASTNode(type=comparison_operator), node=('>=', '>='), position=1, insert_id=1982556)",
            "Update(target_node=ASTNode(type=identifier, text=greater), value='greater_equal')",
            "Insert(target_node=ASTNode(type=comparison_operator), node=('>=', '>='), position=1, insert_id=1982557)",
            "Delete(target_node=ASTNode(type=>, text=>))",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=math))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=floor))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=>, text=>))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 20,
        "number": 6839,
        "neg_line": [
            "-x, tf.where(tf.greater(x, 0)), tf.math.floor(x[x > 0])",
            "-ret = (tf.math.floor if ret > 0 else tf.math.ceil)(ret)"
        ],
        "pos_line": [
            "+x, tf.where(tf.greater_equal(x, 0)), tf.math.floor(x[x >= 0])",
            "+ret = (tf.math.floor if ret >= 0 else tf.math.ceil)(ret)"
        ],
        "core_change": "-x, tf.where(tf.greater(x, 0)), tf.math.floor(x[x > 0]) +x, tf.where(tf.greater_equal(x, 0)), tf.math.floor(x[x >= 0]) -ret = (tf.math.floor if ret > 0 else tf.math.ceil)(ret) +ret = (tf.math.floor if ret >= 0 else tf.math.ceil)(ret)",
        "core_API": "get_shape"
    },
    {
        "commit_hash": "3f82bcece96b7fdabdd25460cfac48273c677686",
        "index": "1c2e6129..b25bdae3 100644",
        "commit_message": "TF: Fix #2247, explicitly squeeze image to support matplotlib<3.3.0 (#2248)\n\n\n",
        "file": "d2l-en.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def predict_seq2seq(net, src_sentence, src_vocab, tgt_vocab, num_steps,",
            "if pred == tgt_vocab['<eos>']:",
            "break",
            "output_seq.append(pred.numpy())",
            "-    return ' '.join(tgt_vocab.to_tokens(tf.reshape(output_seq, shape = -1).numpy().tolist())), attention_weight_seq# Alias defined in config.ini",
            "+    return ' '.join(tgt_vocab.to_tokens(tf.reshape(output_seq, shape = -1).numpy().tolist())), attention_weight_seq",
            "+",
            "+",
            "+# Alias defined in config.ini",
            "size = lambda a: tf.size(a).numpy()",
            "",
            "reshape = tf.reshape"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 6841,
        "neg_line": [
            "-return ' '.join(tgt_vocab.to_tokens(tf.reshape(output_seq, shape = -1).numpy().tolist())), attention_weight_seq# Alias defined in config.ini"
        ],
        "pos_line": [
            "+return ' '.join(tgt_vocab.to_tokens(tf.reshape(output_seq, shape = -1).numpy().tolist())), attention_weight_seq",
            "+",
            "+",
            "+# Alias defined in config.ini"
        ],
        "core_change": "-return ' '.join(tgt_vocab.to_tokens(tf.reshape(output_seq, shape = -1).numpy().tolist())), attention_weight_seq# Alias defined in config.ini +return ' '.join(tgt_vocab.to_tokens(tf.reshape(output_seq, shape = -1).numpy().tolist())), attention_weight_seq + + +# Alias defined in config.ini",
        "core_API": "append"
    },
    {
        "commit_hash": "1cf6c9f2a258f3abf558ec74dabfa7c7295a98dd",
        "index": "b21fd18d..bf55fe7c 100644",
        "commit_message": "fix -inf issue in focal loss\n\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class FocalLoss(nn.Module):",
            "device=input.device, dtype=input.dtype)",
            "",
            "# compute the actual focal loss",
            "-        prob = input_soft * target_one_hot",
            "+        prob = input_soft * target_one_hot + self.eps",
            "focal = -torch.log(prob) * self.alpha * (1. - prob) ** self.gamma",
            "-        loss_tmp = 1. - torch.sum(focal, dim=1)",
            "+        loss_tmp = torch.sum(focal, dim=1)",
            "",
            "loss = -1",
            "if self.reduction == 'none':"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=call), position=2)",
            "Insert(target_node=ASTNode(type=assignment), node=('binary_operator', None), position=2, insert_id=474923)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=binary_operator), position=0)",
            "Insert(target_node=IN(type=binary_operator), node=('+', '+'), position=1, insert_id=474924)",
            "Insert(target_node=IN(type=binary_operator), node=('attribute', None), position=2, insert_id=474925)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=474926)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=474927)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'eps'), position=2, insert_id=474928)",
            "Delete(target_node=ASTNode(type=float, text=1.))",
            "Delete(target_node=ASTNode(type=-, text=-))",
            "Delete(target_node=ASTNode(type=binary_operator))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 11,
        "number": 6843,
        "neg_line": [
            "-prob = input_soft * target_one_hot",
            "-loss_tmp = 1. - torch.sum(focal, dim=1)"
        ],
        "pos_line": [
            "+prob = input_soft * target_one_hot + self.eps",
            "+loss_tmp = torch.sum(focal, dim=1)"
        ],
        "core_change": "-prob = input_soft * target_one_hot +prob = input_soft * target_one_hot + self.eps -loss_tmp = 1. - torch.sum(focal, dim=1) +loss_tmp = torch.sum(focal, dim=1)",
        "core_API": "log"
    },
    {
        "commit_hash": "ac59f458247451ec8e2400f81c37f93ab7f75026",
        "index": "649371ac..817e85e4 100644",
        "commit_message": "Test fixes for compatibility with PyTorch master (#1416)\n\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_gaussian_mixture_model():",
            "cluster_assignments = dist.Categorical(true_mix_proportions).sample(torch.Size((N,)))",
            "data = dist.Normal(true_cluster_means[cluster_assignments], 1.0).sample()",
            "nuts_kernel = NUTS(gmm, adapt_step_size=True, max_iarange_nesting=1)",
            "-    mcmc_run = MCMC(nuts_kernel, num_samples=500, warmup_steps=200).run(data)",
            "+    mcmc_run = MCMC(nuts_kernel, num_samples=300, warmup_steps=100).run(data)",
            "posterior = EmpiricalMarginal(mcmc_run, sites=[\"phi\", \"cluster_means\"]).mean.sort()[0]",
            "assert_equal(posterior[0], true_mix_proportions, prec=0.05)",
            "assert_equal(posterior[1], true_cluster_means, prec=0.2)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=integer, text=500), value='300')",
            "Update(target_node=ASTNode(type=integer, text=200), value='100')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 6848,
        "neg_line": [
            "-mcmc_run = MCMC(nuts_kernel, num_samples=500, warmup_steps=200).run(data)"
        ],
        "pos_line": [
            "+mcmc_run = MCMC(nuts_kernel, num_samples=300, warmup_steps=100).run(data)"
        ],
        "core_change": "-mcmc_run = MCMC(nuts_kernel, num_samples=500, warmup_steps=200).run(data) +mcmc_run = MCMC(nuts_kernel, num_samples=300, warmup_steps=100).run(data)",
        "core_API": "Categorical"
    },
    {
        "commit_hash": "3bfac353ca8b48a4a7b38846f9da6ea77d1781da",
        "index": "5c6546d85..fe2e9d0d5 100644",
        "commit_message": "Fix tensors to literal\n\n",
        "file": "PySyft.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def tensors_to_literals(tensor_list):",
            "",
            "for tensor in tensor_list:",
            "",
            "-            if type(tensor) == torch.tensor:",
            "+            if type(tensor) == torch.Tensor:",
            "",
            "literal_list.append(tensor.item())"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tensor), value='Tensor')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 6850,
        "neg_line": [
            "-if type(tensor) == torch.tensor:"
        ],
        "pos_line": [
            "+if type(tensor) == torch.Tensor:"
        ],
        "core_change": "-if type(tensor) == torch.tensor: +if type(tensor) == torch.Tensor:",
        "core_API": "append"
    },
    {
        "commit_hash": "ffc35f41b41d6be6e067934b680f7abda2e91d49",
        "index": "86845cdc8..66f7a78fc 100644",
        "commit_message": "Update link in wiki_bio dataset (#3651)\n\n* update link in wiki_bio dataset\n\n* run linter and update dummy data\n\n* fix markdown so that test passes (even though I didnt break it)\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class WikiBio(datasets.GeneratorBasedBuilder):",
            "",
            "def _split_generators(self, dl_manager):",
            "\"\"\"Returns SplitGenerators.\"\"\"",
            "-        my_urls = _URL",
            "-        data_dir = dl_manager.download_and_extract(my_urls)",
            "+        data_dir = dl_manager.download_and_extract(_URL)",
            "data_path = os.path.join(data_dir, \"wikipedia-biography-dataset\")",
            "return [",
            "datasets.SplitGenerator("
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=my_urls), value='_URL')",
            "Delete(target_node=ASTNode(type=identifier, text=my_urls))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=_URL))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 6,
        "number": 6852,
        "neg_line": [
            "-my_urls = _URL",
            "-data_dir = dl_manager.download_and_extract(my_urls)"
        ],
        "pos_line": [
            "+data_dir = dl_manager.download_and_extract(_URL)"
        ],
        "core_change": "-my_urls = _URL -data_dir = dl_manager.download_and_extract(my_urls) +data_dir = dl_manager.download_and_extract(_URL)",
        "core_API": "download_and_extract"
    },
    {
        "commit_hash": "d9ebd7162cb7da82d4ca834224b609e159a72c9e",
        "index": "b3c694cfd..158038a31 100644",
        "commit_message": "Fix complex support unit tests; Fix deprecation warnings in enh related tests\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TemporalConvNet(nn.Module):",
            "elif self.mask_nonlinear == \"relu\":",
            "est_mask = F.relu(score)",
            "elif self.mask_nonlinear == \"sigmoid\":",
            "-            est_mask = F.sigmoid(score)",
            "+            est_mask = torch.sigmoid(score)",
            "elif self.mask_nonlinear == \"tanh\":",
            "-            est_mask = F.tanh(score)",
            "+            est_mask = torch.tanh(score)",
            "else:",
            "raise ValueError(\"Unsupported mask non-linear function\")",
            "return est_mask"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=136355)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=argument_list), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=F), value='torch')",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=136356)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'score'), position=1, insert_id=136357)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=136358)",
            "Update(target_node=ASTNode(type=identifier, text=F), value='torch')",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=score))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 11,
        "number": 6857,
        "neg_line": [
            "-est_mask = F.sigmoid(score)",
            "-est_mask = F.tanh(score)"
        ],
        "pos_line": [
            "+est_mask = torch.sigmoid(score)",
            "+est_mask = torch.tanh(score)"
        ],
        "core_change": "-est_mask = F.sigmoid(score) +est_mask = torch.sigmoid(score) -est_mask = F.tanh(score) +est_mask = torch.tanh(score)",
        "core_API": "relu"
    },
    {
        "commit_hash": "43043ee4d5c672f7fbc22ac9559e8164731fd053",
        "index": "9fe5f4309f..c0eeb11daf 100644",
        "commit_message": "[RLlib] Tf2x preparation; part 2 (upgrading `try_import_tf()`). (#9136)\n\n* WIP.\n\n* Fixes.\n\n* LINT.\n\n* WIP.\n\n* WIP.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* WIP.\n\n* Fixes.\n\n* Test\n\n* Fix.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* LINT.\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class _QueueRunner(threading.Thread):",
            "",
            "def __init__(self, input_reader, queue, keys, dtypes):",
            "threading.Thread.__init__(self)",
            "-        self.sess = tf.get_default_session()",
            "+        self.sess = tf1.get_default_session()",
            "self.daemon = True",
            "self.input_reader = input_reader",
            "self.keys = keys",
            "self.queue = queue",
            "-        self.placeholders = [tf.placeholder(dtype) for dtype in dtypes]",
            "+        self.placeholders = [tf1.placeholder(dtype) for dtype in dtypes]",
            "self.enqueue_op = queue.enqueue(dict(zip(keys, self.placeholders)))",
            "",
            "def enqueue(self, batch):"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tf), value='tf1')",
            "Update(target_node=ASTNode(type=identifier, text=tf), value='tf1')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 6863,
        "neg_line": [
            "-self.sess = tf.get_default_session()",
            "-self.placeholders = [tf.placeholder(dtype) for dtype in dtypes]"
        ],
        "pos_line": [
            "+self.sess = tf1.get_default_session()",
            "+self.placeholders = [tf1.placeholder(dtype) for dtype in dtypes]"
        ],
        "core_change": "-self.sess = tf.get_default_session() +self.sess = tf1.get_default_session() -self.placeholders = [tf.placeholder(dtype) for dtype in dtypes] +self.placeholders = [tf1.placeholder(dtype) for dtype in dtypes]",
        "core_API": "__init__"
    },
    {
        "commit_hash": "4bc900eb67d99027bff4a41c1499aaeb0eb220a2",
        "index": "1b5346a..a565164 100644",
        "commit_message": "Fix typo\n\n",
        "file": "gpt-neo.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def gpt2_model_mesh(features, labels, mode, params):",
            "",
            "output = gpt2.model(X=features, params=params, mesh=mesh,",
            "past=None, reuse=tf.AUTO_REUSE,",
            "-                                    train=mode==tf.estimator.ModeKeys.TRAIN, mesh=mesh)",
            "+                                    train=mode==tf.estimator.ModeKeys.TRAIN)",
            "",
            "",
            "# logits :: [batch, seq, vocab]"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=mesh))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=mesh))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 6865,
        "neg_line": [
            "-train=mode==tf.estimator.ModeKeys.TRAIN, mesh=mesh)"
        ],
        "pos_line": [
            "+train=mode==tf.estimator.ModeKeys.TRAIN)"
        ],
        "core_change": "-train=mode==tf.estimator.ModeKeys.TRAIN, mesh=mesh) +train=mode==tf.estimator.ModeKeys.TRAIN)",
        "core_API": "model"
    },
    {
        "commit_hash": "01b8c33cf36bd94b0e6e5c8d38f5f2c17497825c",
        "index": "feaf383bd..1b6ec2c69 100644",
        "commit_message": "[Train] Fix `prepare_data_loader` with `enable_reproducibility` (#30266)\n\nCalling train.torch.enable_reproducibility before train.torch.prepare_data_loader causes an exception to be raised of the num_workers in DataLoader is bigger than 0 and the worker_init_fn in DataLoader is not set. The exception is caused by the worker_init_fn, which has a value of None, being used as a callable in seeded_worker_init_fn. This was untested.\n\nThis PR fixes this oversight and ensures that this is tested in CI (and also removes a duplicate test in the process).\n\nSigned-off-by: Antoni Baum <antoni.baum@protonmail.com>\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_enable_reproducibility(ray_start_4_cpus_2_gpus, use_gpu):",
            "torch.randn(dataset_length, 3, 32, 32),",
            "torch.randint(low=0, high=1000, size=(dataset_length,)),",
            ")",
            "-        dataloader = torch.utils.data.DataLoader(dataset, batch_size=64)",
            "+",
            "+        # num_workers > 0 tests for https://github.com/ray-project/ray/issues/30247",
            "+        dataloader = torch.utils.data.DataLoader(",
            "+            dataset, batch_size=64, num_workers=data_loader_num_workers",
            "+        )",
            "dataloader = train.torch.prepare_data_loader(dataloader)",
            "",
            "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=1106533)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=1106534)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'num_workers'), position=0, insert_id=1106535)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1106536)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'data_loader_num_workers'), position=2, insert_id=1106537)"
        ],
        "plus_line": 4,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 6868,
        "neg_line": [
            "-dataloader = torch.utils.data.DataLoader(dataset, batch_size=64)"
        ],
        "pos_line": [
            "+",
            "+# num_workers > 0 tests for https://github.com/ray-project/ray/issues/30247",
            "+dataloader = torch.utils.data.DataLoader(",
            "+dataset, batch_size=64, num_workers=data_loader_num_workers",
            "+)"
        ],
        "core_change": "-dataloader = torch.utils.data.DataLoader(dataset, batch_size=64) + +# num_workers > 0 tests for https://github.com/ray-project/ray/issues/30247 +dataloader = torch.utils.data.DataLoader( +dataset, batch_size=64, num_workers=data_loader_num_workers +)",
        "core_API": "randn"
    },
    {
        "commit_hash": "7b75aa9fa55bee577e2c7403301ed31103125a35",
        "index": "fd568af10..9bfe6aa28 100644",
        "commit_message": "[TPU] Doc, fix xla_spawn.py, only preprocess dataset once (#4223)\n\n* [TPU] Doc, fix xla_spawn.py, only preprocess dataset once\n\n* Update examples/README.md\n\n* [xla_spawn] Add `_mp_fn` to other Trainer scripts\n\n* [TPU] Fix: eval dataloader was None\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "def main():",
            "eval_datasets = [eval_dataset]",
            "if data_args.task_name == \"mnli\":",
            "mnli_mm_data_args = dataclasses.replace(data_args, task_name=\"mnli-mm\")",
            "-            eval_datasets.append(",
            "-                GlueDataset(mnli_mm_data_args, tokenizer=tokenizer, local_rank=training_args.local_rank, evaluate=True)",
            "-            )",
            "+            eval_datasets.append(GlueDataset(mnli_mm_data_args, tokenizer=tokenizer, evaluate=True))",
            "",
            "for eval_dataset in eval_datasets:",
            "result = trainer.evaluate(eval_dataset=eval_dataset)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=local_rank))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=training_args))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=local_rank))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 8,
        "number": 6872,
        "neg_line": [
            "-eval_datasets.append(",
            "-GlueDataset(mnli_mm_data_args, tokenizer=tokenizer, local_rank=training_args.local_rank, evaluate=True)",
            "-)"
        ],
        "pos_line": [
            "+eval_datasets.append(GlueDataset(mnli_mm_data_args, tokenizer=tokenizer, evaluate=True))"
        ],
        "core_change": "-eval_datasets.append( -GlueDataset(mnli_mm_data_args, tokenizer=tokenizer, local_rank=training_args.local_rank, evaluate=True) -) +eval_datasets.append(GlueDataset(mnli_mm_data_args, tokenizer=tokenizer, evaluate=True))",
        "core_API": "replace"
    },
    {
        "commit_hash": "9876a8fd3d774a80279a9068ac5db189abd020ce",
        "index": "f800e76..06bb557 100644",
        "commit_message": "Fix RoI pooling for resnet101.\n\n",
        "file": "tf-faster-rcnn.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Resnet101(Network):",
            "self._layers['conv5_3'] = net",
            "with tf.variable_scope('resnet_v1_101', 'resnet_v1_101',",
            "regularizer=tf.contrib.layers.l2_regularizer(cfg.TRAIN.WEIGHT_DECAY)):",
            "-   # build the anchors for the image",
            "+      # build the anchors for the image",
            "self._anchor_component()",
            "",
            "# rpn"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 6876,
        "neg_line": [
            "-# build the anchors for the image"
        ],
        "pos_line": [
            "+# build the anchors for the image"
        ],
        "core_change": "-# build the anchors for the image +# build the anchors for the image",
        "core_API": "variable_scope"
    },
    {
        "commit_hash": "db934c67508ef8aed715544526fdf78a06dde2f4",
        "index": "0f63d86d..ff37e8ab 100755",
        "commit_message": "fix more tests\n\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class UnetModelTests(ModelTesterMixin, unittest.TestCase):",
            "# fmt: off",
            "expected_output_slice = torch.tensor([0.2891, -0.1899, 0.2595, -0.6214, 0.0968, -0.2622, 0.4688, 0.1311, 0.0053])",
            "# fmt: on",
            "-        self.assertTrue(torch.allclose(output_slice, expected_output_slice, rtol=1e-3))",
            "+        self.assertTrue(torch.allclose(output_slice, expected_output_slice, rtol=1e-2))",
            "",
            "",
            "class GlideSuperResUNetTests(ModelTesterMixin, unittest.TestCase):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=float, text=1e-3), value='1e-2')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 6877,
        "neg_line": [
            "-self.assertTrue(torch.allclose(output_slice, expected_output_slice, rtol=1e-3))"
        ],
        "pos_line": [
            "+self.assertTrue(torch.allclose(output_slice, expected_output_slice, rtol=1e-2))"
        ],
        "core_change": "-self.assertTrue(torch.allclose(output_slice, expected_output_slice, rtol=1e-3)) +self.assertTrue(torch.allclose(output_slice, expected_output_slice, rtol=1e-2))",
        "core_API": "tensor"
    },
    {
        "commit_hash": "edb78974b84b324df08ec198e6ff7f267ee23d33",
        "index": "0cd2eed3..d41fc9d5 100644",
        "commit_message": "upgrade detection / some v2 compat fix\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class SingleCostTrainer(TowerTrainer):",
            "",
            "grads_no_vars = xla.compile(xla_func)",
            "if ctx.has_own_variables:",
            "-                    varlist = ctx.get_collection_in_tower(tf.GraphKeys.TRAINABLE_VARIABLES)",
            "+                    varlist = ctx.get_collection_in_tower(tfv1.GraphKeys.TRAINABLE_VARIABLES)",
            "else:",
            "-                    varlist = tf.trainable_variables()",
            "+                    varlist = tfv1.trainable_variables()",
            "return list(zip(grads_no_vars, varlist))",
            "",
            "return get_grad_fn"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tf), value='tfv1')",
            "Update(target_node=ASTNode(type=identifier, text=tf), value='tfv1')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 6888,
        "neg_line": [
            "-varlist = ctx.get_collection_in_tower(tf.GraphKeys.TRAINABLE_VARIABLES)",
            "-varlist = tf.trainable_variables()"
        ],
        "pos_line": [
            "+varlist = ctx.get_collection_in_tower(tfv1.GraphKeys.TRAINABLE_VARIABLES)",
            "+varlist = tfv1.trainable_variables()"
        ],
        "core_change": "-varlist = ctx.get_collection_in_tower(tf.GraphKeys.TRAINABLE_VARIABLES) +varlist = ctx.get_collection_in_tower(tfv1.GraphKeys.TRAINABLE_VARIABLES) -varlist = tf.trainable_variables() +varlist = tfv1.trainable_variables()",
        "core_API": "compile"
    },
    {
        "commit_hash": "d11f4a9934fdc7eef4edbe378cf913df27e39399",
        "index": "2831e08e..4e81de6e 100644",
        "commit_message": "bugfix\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class InMemoryDataset(Dataset):",
            "",
            "for item, key in product(data_list, keys):",
            "data[key].append(item[key])",
            "-            s = slices[key][-1] + item[key].size(item.cat_dim(key))",
            "+            s = slices[key][-1] + item[key].size(item.cat_dim(key, item))",
            "slices[key].append(s)",
            "",
            "for key in keys:",
            "-            data[key] = torch.cat(data[key], dim=data_list[0].cat_dim(key))",
            "+            data[key] = torch.cat(",
            "+                data[key], dim=data_list[0].cat_dim(key, item))",
            "slices[key] = torch.LongTensor(slices[key])",
            "",
            "return data, slices"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('subscript', None), position=0, insert_id=1067598)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'data'), position=0, insert_id=1067599)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=1067600)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'key'), position=2, insert_id=1067601)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=1067602)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=subscript), position=1)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1067603)",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'item'), position=3, insert_id=1067604)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1067605)",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'item'), position=3, insert_id=1067606)",
            "Delete(target_node=ASTNode(type=identifier, text=data))",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=identifier, text=key))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))"
        ],
        "plus_line": 3,
        "minus_line": 2,
        "AST_diff_line": 15,
        "number": 6895,
        "neg_line": [
            "-s = slices[key][-1] + item[key].size(item.cat_dim(key))",
            "-data[key] = torch.cat(data[key], dim=data_list[0].cat_dim(key))"
        ],
        "pos_line": [
            "+s = slices[key][-1] + item[key].size(item.cat_dim(key, item))",
            "+data[key] = torch.cat(",
            "+data[key], dim=data_list[0].cat_dim(key, item))"
        ],
        "core_change": "-s = slices[key][-1] + item[key].size(item.cat_dim(key)) +s = slices[key][-1] + item[key].size(item.cat_dim(key, item)) -data[key] = torch.cat(data[key], dim=data_list[0].cat_dim(key)) +data[key] = torch.cat( +data[key], dim=data_list[0].cat_dim(key, item))",
        "core_API": "cat_dim"
    },
    {
        "commit_hash": "c164c651dc382635f1135cf843e8fbd523d5e293",
        "index": "70b5f31f3..3fcfb884e 100755",
        "commit_message": "[CLIP] fix logit_scale init (#13436)\n\n* fix logit_scale init\n\n* add logit_scale_init_value as config param\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class CLIPModel(CLIPPreTrainedModel):",
            "",
            "self.visual_projection = nn.Linear(self.vision_embed_dim, self.projection_dim, bias=False)",
            "self.text_projection = nn.Linear(self.text_embed_dim, self.projection_dim, bias=False)",
            "-        self.logit_scale = nn.Parameter(torch.ones([]))",
            "+        self.logit_scale = nn.Parameter(torch.ones([]) * self.config.logit_scale_init_value)",
            "",
            "self.init_weights()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('binary_operator', None), position=1, insert_id=1212327)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=binary_operator), node=('*', '*'), position=1, insert_id=1212328)",
            "Insert(target_node=IN(type=binary_operator), node=('attribute', None), position=2, insert_id=1212329)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1212330)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1212331)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'logit_scale_init_value'), position=2, insert_id=1212332)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=1212333)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1212334)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'config'), position=2, insert_id=1212335)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 6896,
        "neg_line": [
            "-self.logit_scale = nn.Parameter(torch.ones([]))"
        ],
        "pos_line": [
            "+self.logit_scale = nn.Parameter(torch.ones([]) * self.config.logit_scale_init_value)"
        ],
        "core_change": "-self.logit_scale = nn.Parameter(torch.ones([])) +self.logit_scale = nn.Parameter(torch.ones([]) * self.config.logit_scale_init_value)",
        "core_API": "Linear"
    },
    {
        "commit_hash": "2b967af9bf9d0d839085717faeb02f8344188848",
        "index": "8ab878a0..2b367593 100644",
        "commit_message": "fixed uint8 warning (#2241)\n\n\n",
        "file": "mmdetection.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class BBoxHead(nn.Module):",
            "keep_inds = pos_is_gts_.new_ones(num_rois)",
            "keep_inds[:len(pos_is_gts_)] = pos_keep",
            "",
            "-            bboxes_list.append(bboxes[keep_inds])",
            "+            bboxes_list.append(bboxes[keep_inds.type(torch.bool)])",
            "",
            "return bboxes_list"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=subscript), node=('call', None), position=2, insert_id=640577)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=640578)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=640579)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=keep_inds), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=640580)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'type'), position=2, insert_id=640581)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=640582)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=640583)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=640584)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=640585)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=640586)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'bool'), position=2, insert_id=640587)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 12,
        "number": 6898,
        "neg_line": [
            "-bboxes_list.append(bboxes[keep_inds])"
        ],
        "pos_line": [
            "+bboxes_list.append(bboxes[keep_inds.type(torch.bool)])"
        ],
        "core_change": "-bboxes_list.append(bboxes[keep_inds]) +bboxes_list.append(bboxes[keep_inds.type(torch.bool)])",
        "core_API": "new_ones"
    },
    {
        "commit_hash": "2b4d35b505bf7605f4d21690f0a485a43a5909eb",
        "index": "d3532012..969cfb3f 100644",
        "commit_message": "fix the supernode context\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class GatSuperNode(MessagePassing):",
            "",
            "# this is not correct it should be more hs and not x_i there based on the paper supplementary table 3!",
            "# in the paper it's h_s_ex in the pytorch it's x !",
            "-        cs_i = scatter_add( torch.mul(asv, self.mol_attend(self.dropout(h_s_ex))).transpose(0,1), \\",
            "+        cs_i = scatter_add( torch.mul(asv, self.mol_attend(self.dropout(x))).transpose(0,1), \\",
            "batch, dim_size=superatom_num).transpose(0,1)",
            "",
            "cs_i = F.elu(cs_i)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=h_s_ex), value='x')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 6899,
        "neg_line": [
            "-cs_i = scatter_add( torch.mul(asv, self.mol_attend(self.dropout(h_s_ex))).transpose(0,1), \\"
        ],
        "pos_line": [
            "+cs_i = scatter_add( torch.mul(asv, self.mol_attend(self.dropout(x))).transpose(0,1), \\"
        ],
        "core_change": "-cs_i = scatter_add( torch.mul(asv, self.mol_attend(self.dropout(h_s_ex))).transpose(0,1), \\ +cs_i = scatter_add( torch.mul(asv, self.mol_attend(self.dropout(x))).transpose(0,1), \\",
        "core_API": "mul"
    },
    {
        "commit_hash": "3e116ed33145e80110d0c9b13c9de5207ca19d30",
        "index": "84994f9b4..9797a8fa6 100644",
        "commit_message": "Making TF TransfoXL model compliant with AMP (#10264)\n\n* Fix AMP\n\n* Apply style\n\n* Remove unused import\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFAdaptiveSoftmaxMask(tf.keras.layers.Layer):",
            "cur_logprob = self._gather_logprob(cur_tail_logprob, cur_target)",
            "cur_logprob += cur_head_logprob[:, self.cutoff_ends[1] + i - 1]",
            "if target is not None:",
            "-                    loss += tf.scatter_nd(mask_idx, -cur_logprob, tf.cast(shape_list(loss), dtype=tf.int64))",
            "+                    loss += tf.scatter_nd(mask_idx, -cur_logprob, shape_list(loss))",
            "out = tf.concat(out, axis=-1)",
            "",
            "if target is not None:"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=cast))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=dtype))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=int64))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 15,
        "number": 6908,
        "neg_line": [
            "-loss += tf.scatter_nd(mask_idx, -cur_logprob, tf.cast(shape_list(loss), dtype=tf.int64))"
        ],
        "pos_line": [
            "+loss += tf.scatter_nd(mask_idx, -cur_logprob, shape_list(loss))"
        ],
        "core_change": "-loss += tf.scatter_nd(mask_idx, -cur_logprob, tf.cast(shape_list(loss), dtype=tf.int64)) +loss += tf.scatter_nd(mask_idx, -cur_logprob, shape_list(loss))",
        "core_API": "_gather_logprob"
    },
    {
        "commit_hash": "e172f0087a176b36efe2de8e91efc3e621f36a64",
        "index": "89e46c2..6c4dc45 100644",
        "commit_message": "BatchNorm2D -> BatchNorm2d (#558)\n\n* BatchNorm2D -> BatchNorm2d\n\n* Fix typo\n",
        "file": "tinygrad.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class SpeedyResNet:",
            "# TODO: add whitening",
            "self.net = [",
            "nn.Conv2d(3, 64, kernel_size=1),",
            "-      nn.BatchNorm2D(64, track_running_stats=False, eps=1e-12, momentum=0.8),",
            "+      nn.BatchNorm2d(64, track_running_stats=False, eps=1e-12, momentum=0.8),",
            "lambda x: x.relu(),",
            "ConvGroup(64, 128, short=False),",
            "ConvGroup(128, 256, short=True),"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=BatchNorm2D), value='BatchNorm2d')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 6912,
        "neg_line": [
            "-nn.BatchNorm2D(64, track_running_stats=False, eps=1e-12, momentum=0.8),"
        ],
        "pos_line": [
            "+nn.BatchNorm2d(64, track_running_stats=False, eps=1e-12, momentum=0.8),"
        ],
        "core_change": "-nn.BatchNorm2D(64, track_running_stats=False, eps=1e-12, momentum=0.8), +nn.BatchNorm2d(64, track_running_stats=False, eps=1e-12, momentum=0.8),",
        "core_API": "Conv2d"
    },
    {
        "commit_hash": "bb514fa0fdce8b19b2ac933ad250f1307f9c3af8",
        "index": "14cf519f..d1134576 100644",
        "commit_message": "Clean background_labels in the dense heads (#3221)\n\n* Clean background_labels in the dense heads\n\n* clean format\n\n* Reformat\n\n* Fix CI bug\n\n* fix fsaf and the comments\n\n* fix newly added heads\n\n* fix yolact\n\n* complete comments\n",
        "file": "mmdetection.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def binary_cross_entropy(pred,",
            "torch.Tensor: The calculated loss",
            "\"\"\"",
            "if pred.dim() != label.dim():",
            "-        label, weight = _expand_binary_labels(label, weight, pred.size(-1))",
            "+        label, weight = _expand_onehot_labels(label, weight, pred.size(-1))",
            "",
            "# weighted element-wise losses",
            "if weight is not None:"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=_expand_binary_labels), value='_expand_onehot_labels')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 6913,
        "neg_line": [
            "-label, weight = _expand_binary_labels(label, weight, pred.size(-1))"
        ],
        "pos_line": [
            "+label, weight = _expand_onehot_labels(label, weight, pred.size(-1))"
        ],
        "core_change": "-label, weight = _expand_binary_labels(label, weight, pred.size(-1)) +label, weight = _expand_onehot_labels(label, weight, pred.size(-1))",
        "core_API": "dim"
    },
    {
        "commit_hash": "b41cffaa93e8205bd8bd309f82c33c07c420eefd",
        "index": "1663102..7740f38 100644",
        "commit_message": "Fix a few issues loading pretrained vit/bit npz weights w/ num_classes=0 __init__ arg. Missed a few other small classifier handling detail on Mlp, GhostNet, Levit. Should fix #713\n\n",
        "file": "pytorch-image-models.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class Visformer(nn.Module):",
            "self.num_features = embed_dim if self.vit_stem else embed_dim * 2",
            "self.norm = norm_layer(self.num_features)",
            "self.global_pool, self.head = create_classifier(self.num_features, self.num_classes, pool_type=global_pool)",
            "-        self.head = nn.Linear(self.num_features, num_classes)",
            "",
            "# weights init",
            "if self.pos_embed:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=head))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=nn))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=Linear))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=num_features))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=num_classes))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 21,
        "number": 6916,
        "neg_line": [
            "-self.head = nn.Linear(self.num_features, num_classes)"
        ],
        "pos_line": [],
        "core_change": "-self.head = nn.Linear(self.num_features, num_classes)",
        "core_API": "Linear"
    },
    {
        "commit_hash": "66c827656f1a43d1427961eedf4786c4440c89ee",
        "index": "7c85c7290..bd5fe6f32 100644",
        "commit_message": "fix typo in test gpt2\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class GPT2ModelLanguageGenerationTest(unittest.TestCase):",
            "@slow",
            "def test_lm_generate_gpt2(self):",
            "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")",
            "-        input_ids = torch.tensor([[463, 3290]], dtype=torch.long, device=torch_device)  # The dog",
            "+        input_ids = torch.tensor([[464, 3290]], dtype=torch.long, device=torch_device)  # The dog",
            "expected_output_ids = [",
            "464,",
            "3290,"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=integer, text=463), value='464')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 6921,
        "neg_line": [
            "-input_ids = torch.tensor([[463, 3290]], dtype=torch.long, device=torch_device)  # The dog"
        ],
        "pos_line": [
            "+input_ids = torch.tensor([[464, 3290]], dtype=torch.long, device=torch_device)  # The dog"
        ],
        "core_change": "-input_ids = torch.tensor([[463, 3290]], dtype=torch.long, device=torch_device)  # The dog +input_ids = torch.tensor([[464, 3290]], dtype=torch.long, device=torch_device)  # The dog",
        "core_API": "from_pretrained"
    },
    {
        "commit_hash": "66641e8e683757c05148373cd43a00f034df302e",
        "index": "72e7579b..fe96f873 100644",
        "commit_message": "Clarify and fix for ResNeXt\n\n",
        "file": "d2l-en.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "keras",
        "change": [
            "class ResNeXtBlock(tf.keras.Model):",
            "bot_channels = int(round(num_channels * bot_mul))",
            "self.conv1 = tf.keras.layers.Conv2D(bot_channels, 1, strides=1)",
            "self.conv2 = tf.keras.layers.Conv2D(bot_channels, 3, strides=strides,",
            "-                                            padding=\"same\",  groups=bot_channels//groups)",
            "+                                            padding=\"same\",",
            "+                                            groups=bot_channels//groups)",
            "self.conv3 = tf.keras.layers.Conv2D(num_channels, 1, strides=1)",
            "self.bn1 = tf.keras.layers.BatchNormalization()",
            "self.bn2 = tf.keras.layers.BatchNormalization()",
            "self.bn3 = tf.keras.layers.BatchNormalization()",
            "if use_1x1conv:",
            "-            self.conv4 = tf.keras.layers.Conv2D(num_channels, 1, strides=strides)",
            "+            self.conv4 = tf.keras.layers.Conv2D(num_channels, 1,",
            "+                                                strides=strides)",
            "self.bn4 = tf.keras.layers.BatchNormalization()",
            "else:",
            "self.conv4 = None"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 4,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 6925,
        "neg_line": [
            "-padding=\"same\",  groups=bot_channels//groups)",
            "-self.conv4 = tf.keras.layers.Conv2D(num_channels, 1, strides=strides)"
        ],
        "pos_line": [
            "+padding=\"same\",",
            "+groups=bot_channels//groups)",
            "+self.conv4 = tf.keras.layers.Conv2D(num_channels, 1,",
            "+strides=strides)"
        ],
        "core_change": "-padding=\"same\",  groups=bot_channels//groups) +padding=\"same\", +groups=bot_channels//groups) -self.conv4 = tf.keras.layers.Conv2D(num_channels, 1, strides=strides) +self.conv4 = tf.keras.layers.Conv2D(num_channels, 1, +strides=strides)",
        "core_API": "Conv2D"
    },
    {
        "commit_hash": "e3cc4487fe66e03ec85970ea2db8e5fb34c455f4",
        "index": "6002cc7be..4c66d47b8 100644",
        "commit_message": "Fix CIs for PyTorch 1.13 (#20686)\n\n* fix 1\n\n* fix 2\n\n* fix 3\n\n* fix 4\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class BloomForSequenceClassification(BloomPreTrainedModel):",
            "sequence_lengths = -1",
            "else:",
            "if input_ids is not None:",
            "-                sequence_lengths = torch.ne(input_ids, self.config.pad_token_id).sum(dim=-1) - 1",
            "+                sequence_lengths = (torch.ne(input_ids, self.config.pad_token_id).sum(-1) - 1).to(logits.device)",
            "else:",
            "sequence_lengths = -1",
            "logger.warning("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('call', None), position=5, insert_id=1181301)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1181302)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1181303)",
            "Insert(target_node=IN(type=attribute), node=('parenthesized_expression', None), position=0, insert_id=1181304)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1181305)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to'), position=2, insert_id=1181306)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1181307)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=1181308)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1181309)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('(', '('), position=0, insert_id=1181310)",
            "Move(target_node=IN(type=parenthesized_expression), node=ASTNode(type=binary_operator), position=1)",
            "Insert(target_node=IN(type=parenthesized_expression), node=(')', ')'), position=2, insert_id=1181311)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'logits'), position=0, insert_id=1181312)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1181313)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=1181314)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=unary_operator, text=-1), position=1)",
            "Delete(target_node=ASTNode(type=identifier, text=dim))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 19,
        "number": 6930,
        "neg_line": [
            "-sequence_lengths = torch.ne(input_ids, self.config.pad_token_id).sum(dim=-1) - 1"
        ],
        "pos_line": [
            "+sequence_lengths = (torch.ne(input_ids, self.config.pad_token_id).sum(-1) - 1).to(logits.device)"
        ],
        "core_change": "-sequence_lengths = torch.ne(input_ids, self.config.pad_token_id).sum(dim=-1) - 1 +sequence_lengths = (torch.ne(input_ids, self.config.pad_token_id).sum(-1) - 1).to(logits.device)",
        "core_API": "ne"
    },
    {
        "commit_hash": "5854c7de8c2c9192446be6cceeec51ed6439c23c",
        "index": "ff9489ec..60c35869 100644",
        "commit_message": "misc small fixes for old TF version (#810)\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class StagingInput(FeedfreeInput):",
            "def _setup_graph(self):",
            "self.stage_op = self._input._get_stage_op()",
            "unstage_ops = self._input._get_unstage_ops()",
            "-            unstage_op = tf.group(unstage_ops, name='unstage_all')",
            "+            unstage_op = tf.group(*unstage_ops, name='unstage_all')",
            "self._check_dependency_op = unstage_ops[0]",
            "self.fetches = tf.train.SessionRunArgs(",
            "fetches=[self.stage_op, unstage_op])"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('list_splat', None), position=1, insert_id=2282655)",
            "Insert(target_node=IN(type=list_splat), node=('*', '*'), position=0, insert_id=2282656)",
            "Move(target_node=IN(type=list_splat), node=ASTNode(type=identifier, text=unstage_ops), position=1)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 3,
        "number": 6931,
        "neg_line": [
            "-unstage_op = tf.group(unstage_ops, name='unstage_all')"
        ],
        "pos_line": [
            "+unstage_op = tf.group(*unstage_ops, name='unstage_all')"
        ],
        "core_change": "-unstage_op = tf.group(unstage_ops, name='unstage_all') +unstage_op = tf.group(*unstage_ops, name='unstage_all')",
        "core_API": "_get_stage_op"
    },
    {
        "commit_hash": "29f83beb20f577448f69af4b7d7d4b28abc34304",
        "index": "8b1f040..26317a7 100644",
        "commit_message": "fix a bug cause by torch.tensor()\n\n",
        "file": "deep-learning-for-image-processing.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Loss(nn.Module):",
            "# positive mask will never selected",
            "# 获取负样本",
            "con_neg = con.clone()",
            "-        con_neg[mask] = torch.tensor(0.0)",
            "+        con_neg[mask] = 0.0",
            "# 按照confidence_loss降序排列 con_idx(Tensor: [N, 8732])",
            "_, con_idx = con_neg.sort(dim=1, descending=True)",
            "_, con_rank = con_idx.sort(dim=1)  # 这个步骤比较巧妙"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=float, text=0.0), position=2)",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=tensor))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 6933,
        "neg_line": [
            "-con_neg[mask] = torch.tensor(0.0)"
        ],
        "pos_line": [
            "+con_neg[mask] = 0.0"
        ],
        "core_change": "-con_neg[mask] = torch.tensor(0.0) +con_neg[mask] = 0.0",
        "core_API": "clone"
    },
    {
        "commit_hash": "c02f4922b513ca4f2821b5c08f29fde893febea3",
        "index": "8c00a3f6..fd868e9e 100644",
        "commit_message": "Fix model type in lightning (#4451)\n\n\n",
        "file": "nni.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class LightningModule(pl.LightningModule):",
            "Lightning modules used in NNI should inherit this class.",
            "\"\"\"",
            "",
            "-    def set_model(self, model: Union[Type[nn.Module], nn.Module]) -> NoReturn:",
            "-        if isinstance(model, type):",
            "-            self.model = model()",
            "-        else:",
            "+    def set_model(self, model: Union[Type[nn.Module], nn.Module]) -> None:",
            "+        if isinstance(model, nn.Module):",
            "self.model = model",
            "+        else:",
            "+            self.model = model()",
            "",
            "",
            "Trainer = nni.trace(pl.Trainer)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=3, insert_id=1432439)",
            "Move(target_node=ASTNode(type=module), node=ASTNode(type=expression_statement), position=7)",
            "Move(target_node=IN(type=if_statement), node=ASTNode(type=if, text=if), position=0)",
            "Move(target_node=IN(type=if_statement), node=ASTNode(type=call), position=1)",
            "Move(target_node=IN(type=if_statement), node=ASTNode(type=:, text=:), position=2)",
            "Insert(target_node=IN(type=if_statement), node=('block', ''), position=3, insert_id=1432440)",
            "Insert(target_node=ASTNode(type=comparison_operator), node=('none', 'None'), position=7, insert_id=1432441)",
            "Insert(target_node=ASTNode(type=assignment), node=('identifier', 'else'), position=0, insert_id=1432442)",
            "Insert(target_node=ASTNode(type=assignment), node=(':', ':'), position=1, insert_id=1432443)",
            "Insert(target_node=ASTNode(type=assignment), node=('type', None), position=2, insert_id=1432444)",
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=3, insert_id=1432445)",
            "Move(target_node=IN(type=type), node=ASTNode(type=attribute), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=type), value='nn')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=type), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1432446)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Module'), position=2, insert_id=1432447)",
            "Delete(target_node=ASTNode(type=identifier, text=NoReturn))",
            "Delete(target_node=ASTNode(type=block))",
            "Delete(target_node=ASTNode(type=else, text=else))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=block, text=))",
            "Delete(target_node=ASTNode(type=else_clause))",
            "Delete(target_node=ASTNode(type=if_statement))"
        ],
        "plus_line": 4,
        "minus_line": 4,
        "AST_diff_line": 23,
        "number": 6940,
        "neg_line": [
            "-def set_model(self, model: Union[Type[nn.Module], nn.Module]) -> NoReturn:",
            "-if isinstance(model, type):",
            "-self.model = model()",
            "-else:"
        ],
        "pos_line": [
            "+def set_model(self, model: Union[Type[nn.Module], nn.Module]) -> None:",
            "+if isinstance(model, nn.Module):",
            "+else:",
            "+self.model = model()"
        ],
        "core_change": "-def set_model(self, model: Union[Type[nn.Module], nn.Module]) -> NoReturn: -if isinstance(model, type): -self.model = model() -else: +def set_model(self, model: Union[Type[nn.Module], nn.Module]) -> None: +if isinstance(model, nn.Module): +else: +self.model = model()",
        "core_API": "trace"
    },
    {
        "commit_hash": "cc7705d3b68a1f59ada2425498635be06c611c08",
        "index": "22db4a33..8d940d4e 100644",
        "commit_message": "Fix generation bug with large beam sizes (>50)\n\n",
        "file": "fairseq.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SequenceGenerator(object):",
            "# and values < cand_size indicate candidate active hypos.",
            "# After, the min values per row are the top candidate active hypos",
            "active_mask = buffer('active_mask')",
            "-            torch.add((eos_mask*cand_size).type_as(cand_offsets), cand_offsets[:eos_mask.size(1)],",
            "+            torch.add(eos_mask.type_as(cand_offsets)*cand_size, cand_offsets[:eos_mask.size(1)],",
            "out=active_mask)",
            "",
            "# get the top beam_size active hypotheses, which are just the hypos"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('binary_operator', None), position=1, insert_id=225935)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=binary_operator), node=('*', '*'), position=1, insert_id=225936)",
            "Insert(target_node=IN(type=binary_operator), node=('identifier', 'cand_size'), position=2, insert_id=225937)",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=eos_mask), position=0)",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=*, text=*))",
            "Delete(target_node=ASTNode(type=identifier, text=cand_size))",
            "Delete(target_node=ASTNode(type=binary_operator))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=parenthesized_expression))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 6951,
        "neg_line": [
            "-torch.add((eos_mask*cand_size).type_as(cand_offsets), cand_offsets[:eos_mask.size(1)],"
        ],
        "pos_line": [
            "+torch.add(eos_mask.type_as(cand_offsets)*cand_size, cand_offsets[:eos_mask.size(1)],"
        ],
        "core_change": "-torch.add((eos_mask*cand_size).type_as(cand_offsets), cand_offsets[:eos_mask.size(1)], +torch.add(eos_mask.type_as(cand_offsets)*cand_size, cand_offsets[:eos_mask.size(1)],",
        "core_API": "add"
    },
    {
        "commit_hash": "3866a13466762a349ad0f7ccdf3478d6e9fb13dd",
        "index": "e7d206c3ff..ca1b3eae0b 100644",
        "commit_message": "fix test_sum for torch. out arg not provided by torch. handled by decorator\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def sum(",
            "dtype=dtype,",
            "out=out,",
            ")",
            "-    return torch.sum(input=x, dim=axis, dtype=dtype, keepdim=keepdims, out=out)",
            "+    return torch.sum(input=x, dim=axis, dtype=dtype, keepdim=keepdims)",
            "",
            "",
            "def var("
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=out))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=out))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 6954,
        "neg_line": [
            "-return torch.sum(input=x, dim=axis, dtype=dtype, keepdim=keepdims, out=out)"
        ],
        "pos_line": [
            "+return torch.sum(input=x, dim=axis, dtype=dtype, keepdim=keepdims)"
        ],
        "core_change": "-return torch.sum(input=x, dim=axis, dtype=dtype, keepdim=keepdims, out=out) +return torch.sum(input=x, dim=axis, dtype=dtype, keepdim=keepdims)",
        "core_API": "sum"
    },
    {
        "commit_hash": "8777de3c4b8a84bfce94036bd95ebc2afbfce3e8",
        "index": "6b915c3e..aa1f056a 100755",
        "commit_message": "Fix import order in model, reformatting.\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Synchronization(Optimizer):",
            "self.update_weight = update_weight",
            "",
            "def tf_step(self, time, variables, source_variables, **kwargs):",
            "-        last_update = tf.get_variable(name='last-update', dtype=tf.int32, initializer=(-self.update_frequency), trainable=False)",
            "+        last_update = tf.get_variable(",
            "+            name='last-update',",
            "+            dtype=tf.int32,",
            "+            initializer=(-self.update_frequency),",
            "+            trainable=False",
            "+        )",
            "",
            "def true_fn():",
            "diffs = list()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 6,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 6955,
        "neg_line": [
            "-last_update = tf.get_variable(name='last-update', dtype=tf.int32, initializer=(-self.update_frequency), trainable=False)"
        ],
        "pos_line": [
            "+last_update = tf.get_variable(",
            "+name='last-update',",
            "+dtype=tf.int32,",
            "+initializer=(-self.update_frequency),",
            "+trainable=False",
            "+)"
        ],
        "core_change": "-last_update = tf.get_variable(name='last-update', dtype=tf.int32, initializer=(-self.update_frequency), trainable=False) +last_update = tf.get_variable( +name='last-update', +dtype=tf.int32, +initializer=(-self.update_frequency), +trainable=False +)",
        "core_API": "get_variable"
    },
    {
        "commit_hash": "af6ede0afd5e9e4157436c6737ae68e551f193d0",
        "index": "04588577..56c0e3fc 100644",
        "commit_message": "fix some unit tests\n\n",
        "file": "flair.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "def test_train_resume_sequence_tagging_training(results_base_path, tasks_base_pa",
            "corpus_1 = flair.datasets.ColumnCorpus(",
            "data_folder=tasks_base_path / \"fashion\", column_format={0: \"text\", 2: \"ner\"}",
            ")",
            "-    corpus_2 = flair.datasets.GERMEVAL(base_path=tasks_base_path)",
            "+    corpus_2 = flair.datasets.GERMEVAL_14(base_path=tasks_base_path)",
            "",
            "corpus = MultiCorpus([corpus_1, corpus_2])",
            "tag_dictionary = corpus.make_tag_dictionary(\"ner\")"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=GERMEVAL), value='GERMEVAL_14')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 6956,
        "neg_line": [
            "-corpus_2 = flair.datasets.GERMEVAL(base_path=tasks_base_path)"
        ],
        "pos_line": [
            "+corpus_2 = flair.datasets.GERMEVAL_14(base_path=tasks_base_path)"
        ],
        "core_change": "-corpus_2 = flair.datasets.GERMEVAL(base_path=tasks_base_path) +corpus_2 = flair.datasets.GERMEVAL_14(base_path=tasks_base_path)",
        "core_API": "ColumnCorpus"
    },
    {
        "commit_hash": "0178e6cc227e0f6d60cb5f3675edd1cda9a0396e",
        "index": "0dc8711b..360efa0a 100755",
        "commit_message": "Fix unbalanced gradients bug in ZeRO-2 gradient accumulation (#545)\n\n* Use zero-tensors for missing gradients to avoid size mismatch\n\n* Unit test for unbalanced gradients in ZeRO\n\n* Formatting fixes\n",
        "file": "DeepSpeed.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class LinearStack(torch.nn.Module):",
            "self.output_dim = output_dim",
            "self.hidden_dim = hidden_dim",
            "",
            "-        self.input_layer = VerboseLinear(in_features=self.input_dim,",
            "-                                         out_features=self.hidden_dim)",
            "+        self.input_layer = torch.nn.Linear(in_features=self.input_dim,",
            "+                                           out_features=self.hidden_dim)",
            "self.layers = torch.nn.ModuleList([",
            "torch.nn.Linear(in_features=self.hidden_dim,",
            "out_features=self.hidden_dim,"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=84662)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=84663)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=84664)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Linear'), position=2, insert_id=84665)",
            "Update(target_node=ASTNode(type=identifier, text=VerboseLinear), value='torch')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=VerboseLinear), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=84666)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'nn'), position=2, insert_id=84667)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 8,
        "number": 6962,
        "neg_line": [
            "-self.input_layer = VerboseLinear(in_features=self.input_dim,",
            "-out_features=self.hidden_dim)"
        ],
        "pos_line": [
            "+self.input_layer = torch.nn.Linear(in_features=self.input_dim,",
            "+out_features=self.hidden_dim)"
        ],
        "core_change": "-self.input_layer = VerboseLinear(in_features=self.input_dim, -out_features=self.hidden_dim) +self.input_layer = torch.nn.Linear(in_features=self.input_dim, +out_features=self.hidden_dim)",
        "core_API": "Linear"
    },
    {
        "commit_hash": "b09b73e89eecc95952d2a9479fd015f9b78a241a",
        "index": "= (- cum_num_nodes[batch]) + (batch * max_num_nodes)",
        "commit_message": "Fix out of index issue in topk.\n\nIn topk, it makes dense and fixed node tensor according to x.shape and initialize all values to -2.\nSo, a graph which is smaller than biggest graph in batch has dummy nodes.\nAnd then, x is copied to dense tensor and ordered by score.\nIf some scores in a graph which is smaller than biggest graph are smaller than -2,\nthey are moved to out of node size of original graph and dummy nodes are filled to those position.\nSo, returned perm has dummy node that have out of index.\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def topk(x, ratio, batch, min_score=None, tol=1e-7):",
            "index = torch.arange(batch.size(0), dtype=torch.long, device=x.device)",
            "index = (index - cum_num_nodes[batch]) + (batch * max_num_nodes)",
            "",
            "-        dense_x = x.new_full((batch_size * max_num_nodes, ), -2)",
            "+        dense_x = x.new_full((batch_size * max_num_nodes, ),",
            "+                             torch.finfo(x.dtype).min)",
            "dense_x[index] = x",
            "dense_x = dense_x.view(batch_size, max_num_nodes)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=3, insert_id=1035458)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=4, insert_id=1035459)",
            "Insert(target_node=IN(type=attribute), node=('call', None), position=0, insert_id=1035460)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1035461)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'min'), position=2, insert_id=1035462)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1035463)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1035464)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1035465)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1035466)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'finfo'), position=2, insert_id=1035467)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1035468)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=1035469)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=), text=)), position=2)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'x'), position=0, insert_id=1035470)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1035471)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dtype'), position=2, insert_id=1035472)",
            "Delete(target_node=ASTNode(type=unary_operator, text=-2))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 6963,
        "neg_line": [
            "-dense_x = x.new_full((batch_size * max_num_nodes, ), -2)"
        ],
        "pos_line": [
            "+dense_x = x.new_full((batch_size * max_num_nodes, ),",
            "+torch.finfo(x.dtype).min)"
        ],
        "core_change": "-dense_x = x.new_full((batch_size * max_num_nodes, ), -2) +dense_x = x.new_full((batch_size * max_num_nodes, ), +torch.finfo(x.dtype).min)",
        "core_API": "arange"
    },
    {
        "commit_hash": "04e997fe0da22b8190da38e0d3224d9ccdf57218",
        "index": "b5f69c9f9d..2691a11bae 100644",
        "commit_message": "Fix TF2 / rllib test (#5846)\n\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "keras",
        "change": [
            "class RecurrentTFModelV2(TFModelV2):",
            "shape=(None, obs_space.shape[0]))",
            "state_in_h = tf.keras.layers.Input(shape=(256, ))",
            "state_in_c = tf.keras.layers.Input(shape=(256, ))",
            "-                seq_in = tf.keras.layers.Input(shape=())",
            "+                seq_in = tf.keras.layers.Input(shape=(), dtype=tf.int32)",
            "",
            "# Send to LSTM cell",
            "lstm_out, state_h, state_c = tf.keras.layers.LSTM("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=2547488)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=2547489)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=2547490)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2547491)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=2547492)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2547493)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2547494)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'int32'), position=2, insert_id=2547495)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 6965,
        "neg_line": [
            "-seq_in = tf.keras.layers.Input(shape=())"
        ],
        "pos_line": [
            "+seq_in = tf.keras.layers.Input(shape=(), dtype=tf.int32)"
        ],
        "core_change": "-seq_in = tf.keras.layers.Input(shape=()) +seq_in = tf.keras.layers.Input(shape=(), dtype=tf.int32)",
        "core_API": "Input"
    },
    {
        "commit_hash": "e5dd34bfe81e018fbad30fdd1d841f03ee6673f1",
        "index": "bba0be3f..c402aa64 100644",
        "commit_message": "Rename variable to fix mypy issue\n\n",
        "file": "flair.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ScalarMix(torch.nn.Module):",
            "normed_weights = torch.nn.functional.softmax(",
            "torch.cat([parameter for parameter in self.scalar_parameters]), dim=0",
            ")",
            "-        normed_weights = torch.split(normed_weights, split_size_or_sections=1)",
            "+        normed_weights_split = torch.split(normed_weights, split_size_or_sections=1)",
            "",
            "pieces = []",
            "-        for weight, tensor in zip(normed_weights, tensors):",
            "+        for weight, tensor in zip(normed_weights_split, tensors):",
            "pieces.append(weight * tensor)",
            "return self.gamma * sum(pieces)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=normed_weights), value='normed_weights_split')",
            "Update(target_node=ASTNode(type=identifier, text=normed_weights), value='normed_weights_split')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 6968,
        "neg_line": [
            "-normed_weights = torch.split(normed_weights, split_size_or_sections=1)",
            "-for weight, tensor in zip(normed_weights, tensors):"
        ],
        "pos_line": [
            "+normed_weights_split = torch.split(normed_weights, split_size_or_sections=1)",
            "+for weight, tensor in zip(normed_weights_split, tensors):"
        ],
        "core_change": "-normed_weights = torch.split(normed_weights, split_size_or_sections=1) +normed_weights_split = torch.split(normed_weights, split_size_or_sections=1) -for weight, tensor in zip(normed_weights, tensors): +for weight, tensor in zip(normed_weights_split, tensors):",
        "core_API": "softmax"
    },
    {
        "commit_hash": "ce8653db0667daa30160ffacbd06cee9c9cf48b7",
        "index": "858d2d3cad..8a433a8cfe 100644",
        "commit_message": "fix small formatting error in functional/backends/tensorflow/experimental/activations\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def batch_norm(",
            "variance = tf.math.reduce_variance(x, axis=dims)",
            "x = tf.transpose(x, perm=(0, *range(2, ndims), 1))",
            "ret = tf.nn.batch_normalization(x, mean, variance, offset, scale, eps)",
            "-    return tf.transpose(ret, perm=(0, ndims-1, *range(1, ndims-1)))",
            "+    return tf.transpose(ret, perm=(0, ndims - 1, *range(1, ndims - 1)))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 6970,
        "neg_line": [
            "-return tf.transpose(ret, perm=(0, ndims-1, *range(1, ndims-1)))"
        ],
        "pos_line": [
            "+return tf.transpose(ret, perm=(0, ndims - 1, *range(1, ndims - 1)))"
        ],
        "core_change": "-return tf.transpose(ret, perm=(0, ndims-1, *range(1, ndims-1))) +return tf.transpose(ret, perm=(0, ndims - 1, *range(1, ndims - 1)))",
        "core_API": "reduce_variance"
    },
    {
        "commit_hash": "320b7a7e011cf1804d20c0e9a10d6035feb66a92",
        "index": "47fdb6e8b..11c4ba631 100644",
        "commit_message": "fix #1416\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "logger = logging.getLogger(__name__)  # pylint: disable=invalid-name",
            "",
            "try:",
            "import tensorflow as tf",
            "-    assert int(tf.__version__[0]) >= 2",
            "+    assert hasattr(tf, '__version__') and int(tf.__version__[0]) >= 2",
            "_tf_available = True  # pylint: disable=invalid-name",
            "logger.info(\"TensorFlow version {} available.\".format(tf.__version__))",
            "except (ImportError, AssertionError):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assert_statement), node=('boolean_operator', None), position=1, insert_id=2385940)",
            "Insert(target_node=IN(type=boolean_operator), node=('call', None), position=0, insert_id=2385941)",
            "Insert(target_node=IN(type=boolean_operator), node=('and', 'and'), position=1, insert_id=2385942)",
            "Move(target_node=IN(type=boolean_operator), node=ASTNode(type=comparison_operator), position=2)",
            "Insert(target_node=IN(type=call), node=('identifier', 'hasattr'), position=0, insert_id=2385943)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2385944)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2385945)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'tf'), position=1, insert_id=2385946)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=2385947)",
            "Insert(target_node=IN(type=argument_list), node=('string', \"'__version__'\"), position=3, insert_id=2385948)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=2385949)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 6975,
        "neg_line": [
            "-assert int(tf.__version__[0]) >= 2"
        ],
        "pos_line": [
            "+assert hasattr(tf, '__version__') and int(tf.__version__[0]) >= 2"
        ],
        "core_change": "-assert int(tf.__version__[0]) >= 2 +assert hasattr(tf, '__version__') and int(tf.__version__[0]) >= 2",
        "core_API": "getLogger"
    },
    {
        "commit_hash": "98cc35b6a8e53e15829ce64fd8da835db0c61da9",
        "index": "62b8e1da..fd99068d 100644",
        "commit_message": "Abstract accelerator (step 3) (#2677)\n\n* Integrate accelerator abstraction interface into deepspeed/\n\n* Fix error message in fp16/fused_optimizer\n\n* fix error message in fp16/unfused_optimizer.py\n\n* assign get_accelerator().pin_memory() result to input Tensor name\n\n* no need to check cuda and whether nvtx supported\n\n* move try-except into inner most block\n\n* call Event() and Stream() in get_accelerator() for data type\n\n* Make Stream and Event as properties of abstract interface so they can be used as data type in deepspeed\n\n* Apply op_builder backend api change from #2705 from @jeffra\n\n* fix tests where Builder NAME is used\n\n* keep original ...Builder.NAME interface instead of ...Builder().NAME interface\n\n* fix builder closure for installation\n\n* fix randomltd builder\n\n* add comments to clarify create_op_builder and get_op_builder\n\n* fix compatibility with pip install -e\n\nCo-authored-by: Cheng Li <pistasable@gmail.com>\nCo-authored-by: Olatunji Ruwase <olruwase@microsoft.com>\n",
        "file": "DeepSpeed.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class NcclBackend(object):",
            "recvbuf_scale = [",
            "torch.zeros(1,",
            "dtype=worker_scale.dtype,",
            "-                        device=torch.device(local_rank)) for i in range(self.size)",
            "+                        device=torch.device(get_accelerator().device_name(local_rank)))",
            "+            for i in range(self.size)",
            "]",
            "",
            "# communication phase 1"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('(', '('), position=0, insert_id=74911)",
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=1, insert_id=74912)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=2, insert_id=74913)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=74914)",
            "Move(target_node=IN(type=call), node=ASTNode(type=argument_list), position=1)",
            "Insert(target_node=IN(type=attribute), node=('call', None), position=0, insert_id=74915)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=74916)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device_name'), position=2, insert_id=74917)",
            "Insert(target_node=IN(type=call), node=('identifier', 'get_accelerator'), position=0, insert_id=74918)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=74919)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=74920)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=74921)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 12,
        "number": 6977,
        "neg_line": [
            "-device=torch.device(local_rank)) for i in range(self.size)"
        ],
        "pos_line": [
            "+device=torch.device(get_accelerator().device_name(local_rank)))",
            "+for i in range(self.size)"
        ],
        "core_change": "-device=torch.device(local_rank)) for i in range(self.size) +device=torch.device(get_accelerator().device_name(local_rank))) +for i in range(self.size)",
        "core_API": "zeros"
    },
    {
        "commit_hash": "07b1b56d5c59ecc62e12fafd17f947934a9edb63",
        "index": "ec5714776..6b70f6af1 100644",
        "commit_message": "Fix setting device when creating \"inf\" monitor value in `ModelCheckpoint` (#10118)\n\nCo-authored-by: thomas chaton <thomas@grid.ai>\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ModelCheckpoint(Callback):",
            "",
            "# do not save nan, replace with +/- inf",
            "if isinstance(current, torch.Tensor) and torch.isnan(current):",
            "-            current = torch.tensor(float(\"inf\" if self.mode == \"min\" else \"-inf\"))",
            "+            current = torch.tensor(float(\"inf\" if self.mode == \"min\" else \"-inf\"), device=current.device)",
            "",
            "filepath = self._get_metric_interpolated_filepath_name(monitor_candidates, trainer, del_filepath)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=521902)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=521903)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=521904)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=521905)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=521906)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'current'), position=0, insert_id=521907)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=521908)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=521909)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 6979,
        "neg_line": [
            "-current = torch.tensor(float(\"inf\" if self.mode == \"min\" else \"-inf\"))"
        ],
        "pos_line": [
            "+current = torch.tensor(float(\"inf\" if self.mode == \"min\" else \"-inf\"), device=current.device)"
        ],
        "core_change": "-current = torch.tensor(float(\"inf\" if self.mode == \"min\" else \"-inf\")) +current = torch.tensor(float(\"inf\" if self.mode == \"min\" else \"-inf\"), device=current.device)",
        "core_API": "isnan"
    },
    {
        "commit_hash": "618ac4edb1bf01124413f5b9d2fe0ef157f5edd8",
        "index": "d27dcbac..fcbe8fb2 100644",
        "commit_message": "lots of fixes, tests working\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class QNAFModel(QModel):",
            "l_matrix = flat_stddev",
            "l_matrix = tf.exp(l_matrix)",
            "else:",
            "-            l_matrix = tf.map_fn(fn=tf.diag, elems=flat_stddev)",
            "+            l_matrix = tf.linalg.diag(diagonal=flat_stddev)",
            "",
            "l_entries = self.l_entries[name].apply(x=embedding)",
            "l_entries = tf.exp(l_entries)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2233046)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=diag), position=2)",
            "Update(target_node=ASTNode(type=identifier, text=map_fn), value='linalg')",
            "Update(target_node=ASTNode(type=identifier, text=elems), value='diagonal')",
            "Delete(target_node=ASTNode(type=identifier, text=fn))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 12,
        "number": 6981,
        "neg_line": [
            "-l_matrix = tf.map_fn(fn=tf.diag, elems=flat_stddev)"
        ],
        "pos_line": [
            "+l_matrix = tf.linalg.diag(diagonal=flat_stddev)"
        ],
        "core_change": "-l_matrix = tf.map_fn(fn=tf.diag, elems=flat_stddev) +l_matrix = tf.linalg.diag(diagonal=flat_stddev)",
        "core_API": "exp"
    },
    {
        "commit_hash": "b5e2b183af5e40e33a4dc7659e697d137259d56e",
        "index": "837ab17a1..a9969b715 100755",
        "commit_message": "Doc styler examples (#14953)\n\n* Fix bad examples\n\n* Add black formatting to style_doc\n\n* Use first nonempty line\n\n* Put it at the right place\n\n* Don't add spaces to empty lines\n\n* Better templates\n\n* Deal with triple quotes in docstrings\n\n* Result of style_doc\n\n* Enable mdx treatment and fix code examples in MDXs\n\n* Result of doc styler on doc source files\n\n* Last fixes\n\n* Break copy from\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class UniSpeechSatForPreTraining(UniSpeechSatPreTrainedModel):",
            "...     outputs = model(input_values, mask_time_indices=mask_time_indices)",
            "",
            ">>> # compute cosine similarity between predicted (=projected_states) and target (=projected_quantized_states)",
            "-        >>> cosine_sim = torch.cosine_similarity(",
            "-        ...     outputs.projected_states, outputs.projected_quantized_states, dim=-1",
            "-        ... )",
            "+        >>> cosine_sim = torch.cosine_similarity(outputs.projected_states, outputs.projected_quantized_states, dim=-1)",
            "",
            ">>> # show that cosine similarity is much higher than random",
            ">>> assert cosine_sim[mask_time_indices].mean() > 0.5"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=ellipsis, text=...))",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=ERROR))"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 6,
        "number": 6982,
        "neg_line": [
            "->>> cosine_sim = torch.cosine_similarity(",
            "-...     outputs.projected_states, outputs.projected_quantized_states, dim=-1",
            "-... )"
        ],
        "pos_line": [
            "+>>> cosine_sim = torch.cosine_similarity(outputs.projected_states, outputs.projected_quantized_states, dim=-1)"
        ],
        "core_change": "->>> cosine_sim = torch.cosine_similarity( -...     outputs.projected_states, outputs.projected_quantized_states, dim=-1 -... ) +>>> cosine_sim = torch.cosine_similarity(outputs.projected_states, outputs.projected_quantized_states, dim=-1)",
        "core_API": "cosine_similarity"
    },
    {
        "commit_hash": "7482178162b779506a54538f2cf2565c8b88c597",
        "index": "81deba67..c5b2572f 100644",
        "commit_message": "default fast model loading 🔥 (#1115)\n\n* make accelerate hard dep\n\n* default fast init\n\n* move params to cpu when device map is None\n\n* handle device_map=None\n\n* handle torch < 1.9\n\n* remove device_map=\"auto\"\n\n* style\n\n* add accelerate in torch extra\n\n* remove accelerate from extras[\"test\"]\n\n* raise an error if torch is available but not accelerate\n\n* update installation docs\n\n* Apply suggestions from code review\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* improve defautl loading speed even further, allow disabling fats loading\n\n* address review comments\n\n* adapt the tests\n\n* fix test_stable_diffusion_fast_load\n\n* fix test_read_init\n\n* temp fix for dummy checks\n\n* Trigger Build\n\n* Apply suggestions from code review\n\nCo-authored-by: Anton Lozhkov <anton@huggingface.co>\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\nCo-authored-by: Anton Lozhkov <anton@huggingface.co>\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class StableDiffusionInpaintLegacyPipelineIntegrationTests(unittest.TestCase):",
            ")",
            "",
            "pipe = StableDiffusionInpaintPipeline.from_pretrained(",
            "-            \"CompVis/stable-diffusion-v1-4\", revision=\"fp16\", torch_dtype=torch.float16, device_map=\"auto\"",
            "+            \"CompVis/stable-diffusion-v1-4\", revision=\"fp16\", torch_dtype=torch.float16",
            ")",
            "pipe.to(torch_device)",
            "pipe.set_progress_bar_config(disable=None)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=device_map))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=string, text=\"auto\"))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 6984,
        "neg_line": [
            "-\"CompVis/stable-diffusion-v1-4\", revision=\"fp16\", torch_dtype=torch.float16, device_map=\"auto\""
        ],
        "pos_line": [
            "+\"CompVis/stable-diffusion-v1-4\", revision=\"fp16\", torch_dtype=torch.float16"
        ],
        "core_change": "-\"CompVis/stable-diffusion-v1-4\", revision=\"fp16\", torch_dtype=torch.float16, device_map=\"auto\" +\"CompVis/stable-diffusion-v1-4\", revision=\"fp16\", torch_dtype=torch.float16",
        "core_API": "from_pretrained"
    },
    {
        "commit_hash": "95b6d985ffb231d9a2047c039c95660913063b9d",
        "index": "7fa9c7a4..96cdf558 100644",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SelfAttentiveSpanExtractor(SpanExtractor):",
            "# Above we were masking the widths of spans with respect to the max",
            "# span width in the batch. Here we are masking the spans which were",
            "# originally passed in as padding.",
            "-            return attended_text_embeddings * span_indices_mask.unsqueeze(-1).float()",
            "+            return attended_text_embeddings * span_indices_mask.unsqueeze(-1)",
            "",
            "return attended_text_embeddings"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=IN(type=root), node=ASTNode(type=call), position=0)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=float))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 6995,
        "neg_line": [
            "-return attended_text_embeddings * span_indices_mask.unsqueeze(-1).float()"
        ],
        "pos_line": [
            "+return attended_text_embeddings * span_indices_mask.unsqueeze(-1)"
        ],
        "core_change": "-return attended_text_embeddings * span_indices_mask.unsqueeze(-1).float() +return attended_text_embeddings * span_indices_mask.unsqueeze(-1)",
        "core_API": "unsqueeze"
    },
    {
        "commit_hash": "809aa8986a29e434c1fb631395ea406576357461",
        "index": "fddddbd16e..adf1f51339 100644",
        "commit_message": "fixing outer() tensorflow backend to pass array-api test\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def svd(",
            "def outer(",
            "x1: Union[tf.Tensor, tf.Variable],",
            "x2: Union[tf.Tensor, tf.Variable],",
            "-    out: Optional[tf.Tensor] = None",
            "+    out: Optional[tf.Tensor] = None,",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "-    ret = tf.experimental.numpy.outer(x1, x2, out=out)",
            "+    ret = tf.experimental.numpy.outer(x1, x2)",
            "if ivy.exists(out):",
            "return ivy.inplace_update(out, ret)",
            "return ret"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=7, insert_id=2006954)",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=out))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=out))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 6,
        "number": 6997,
        "neg_line": [
            "-out: Optional[tf.Tensor] = None",
            "-ret = tf.experimental.numpy.outer(x1, x2, out=out)"
        ],
        "pos_line": [
            "+out: Optional[tf.Tensor] = None,",
            "+ret = tf.experimental.numpy.outer(x1, x2)"
        ],
        "core_change": "-out: Optional[tf.Tensor] = None +out: Optional[tf.Tensor] = None, -ret = tf.experimental.numpy.outer(x1, x2, out=out) +ret = tf.experimental.numpy.outer(x1, x2)",
        "core_API": "outer"
    },
    {
        "commit_hash": "34da9a1fd21d5a50e8d4c49fee0ace78a09705f7",
        "index": "ef61737e..41926f48 100755",
        "commit_message": "bug fix in im2im\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Model(ModelDesc):",
            "fake_output = tf.image.grayscale_to_rgb(fake_output)",
            "viz = (tf.concat(2, [input, output, fake_output]) + 1.0) * 128.0",
            "viz = tf.cast(tf.clip_by_value(viz, 0, 255), tf.uint8, name='viz')",
            "-        tf.image_summary('gen', viz, max_outputs=max(30, BATCH))",
            "+        tf.summary.image('gen', viz, max_outputs=max(30, BATCH))",
            "",
            "all_vars = tf.trainable_variables()",
            "self.g_vars = [v for v in all_vars if v.name.startswith('gen/')]"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=2308558)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=2308559)",
            "Insert(target_node=ASTNode(type=attribute), node=('identifier', 'image'), position=2, insert_id=2308560)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=tf), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=image_summary), value='summary')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=image_summary), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 6999,
        "neg_line": [
            "-tf.image_summary('gen', viz, max_outputs=max(30, BATCH))"
        ],
        "pos_line": [
            "+tf.summary.image('gen', viz, max_outputs=max(30, BATCH))"
        ],
        "core_change": "-tf.image_summary('gen', viz, max_outputs=max(30, BATCH)) +tf.summary.image('gen', viz, max_outputs=max(30, BATCH))",
        "core_API": "grayscale_to_rgb"
    },
    {
        "commit_hash": "8eee4fa9e133fe873a7993ba746d32ca2b687551",
        "index": "4a503f469..ac14d99b8 100644",
        "commit_message": "Add metrics usage examples and tests (#1820)\n\n* add metrics usage examples and tests\n\n* update template\n\n* remove instruction sentence\n\n* add metrics dependencies to the tests requirements\n\n* try fix pip install timeout\n\n* try again\n\n* try again\n\n* try again\n\n* try again by moving unbabel-comet\n\n* try again by ignoring fixed deps of comet\n\n* fix some comet deps\n\n* move deps\n\n* try again\n\n* download wordnet for meteor\n\n* style\n\n* don't test comet on windows\n\n* style\n\n* remove comet comment\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class BERTScore(datasets.Metric):",
            "batch_size=batch_size,",
            ")",
            "output_dict = {",
            "-            \"precision\": P,",
            "-            \"recall\": R,",
            "-            \"f1\": F,",
            "+            \"precision\": P.tolist(),",
            "+            \"recall\": R.tolist(),",
            "+            \"f1\": F.tolist(),",
            "\"hashcode\": hashcode,",
            "}",
            "return output_dict"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=pair), node=('call', None), position=2, insert_id=1787273)",
            "Insert(target_node=ASTNode(type=pair), node=('call', None), position=2, insert_id=1787274)",
            "Insert(target_node=ASTNode(type=pair), node=('call', None), position=2, insert_id=1787275)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1787276)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1787277)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1787278)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1787279)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1787280)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1787281)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=P), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1787282)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tolist'), position=2, insert_id=1787283)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1787284)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1787285)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=R), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1787286)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tolist'), position=2, insert_id=1787287)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1787288)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1787289)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=F), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1787290)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tolist'), position=2, insert_id=1787291)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1787292)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1787293)"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 24,
        "number": 7010,
        "neg_line": [
            "-\"precision\": P,",
            "-\"recall\": R,",
            "-\"f1\": F,"
        ],
        "pos_line": [
            "+\"precision\": P.tolist(),",
            "+\"recall\": R.tolist(),",
            "+\"f1\": F.tolist(),"
        ],
        "core_change": "-\"precision\": P, -\"recall\": R, -\"f1\": F, +\"precision\": P.tolist(), +\"recall\": R.tolist(), +\"f1\": F.tolist(),",
        "core_API": "tolist"
    },
    {
        "commit_hash": "5dda1735fda047f4242d28f91e6e457b9760d52d",
        "index": "c574a009..5e626937 100644",
        "commit_message": "Inference support for `mps` device (#355)\n\n* Initial support for mps in Stable Diffusion pipeline.\n\n* Initial \"warmup\" implementation when using mps.\n\n* Make some deterministic tests pass with mps.\n\n* Disable training tests when using mps.\n\n* SD: generate latents in CPU then move to device.\n\nThis is especially important when using the mps device, because\ngenerators are not supported there. See for example\nhttps://github.com/pytorch/pytorch/issues/84288.\n\nIn addition, the other pipelines seem to use the same approach: generate\nthe random samples then move to the appropriate device.\n\nAfter this change, generating an image in MPS produces the same result\nas when using the CPU, if the same seed is used.\n\n* Remove prints.\n\n* Pass AutoencoderKL test_output_pretrained with mps.\n\nSampling from `posterior` must be done in CPU.\n\n* Style\n\n* Do not use torch.long for log op in mps device.\n\n* Perform incompatible padding ops in CPU.\n\nUNet tests now pass.\nSee https://github.com/pytorch/pytorch/issues/84535\n\n* Style: fix import order.\n\n* Remove unused symbols.\n\n* Remove MPSWarmupMixin, do not apply automatically.\n\nWe do apply warmup in the tests, but not during normal use.\nThis adopts some PR suggestions by @patrickvonplaten.\n\n* Add comment for mps fallback to CPU step.\n\n* Add README_mps.md for mps installation and use.\n\n* Apply `black` to modified files.\n\n* Restrict README_mps to SD, show measures in table.\n\n* Make PNDM indexing compatible with mps.\n\nAddresses #239.\n\n* Do not use float64 when using LDMScheduler.\n\nFixes #358.\n\n* Fix typo identified by @patil-suraj\n\nCo-authored-by: Suraj Patil <surajp815@gmail.com>\n\n* Adapt example to new output style.\n\n* Restore 1:1 results reproducibility with CompVis.\n\nHowever, mps latents need to be generated in CPU because generators\ndon't work in the mps device.\n\n* Move PyTorch nightly to requirements.\n\n* Adapt `test_scheduler_outputs_equivalence` ton MPS.\n\n* mps: skip training tests instead of ignoring silently.\n\n* Make VQModel tests pass on mps.\n\n* mps ddim tests: warmup, increase tolerance.\n\n* ScoreSdeVeScheduler indexing made mps compatible.\n\n* Make ldm pipeline tests pass using warmup.\n\n* Style\n\n* Simplify casting as suggested in PR.\n\n* Add Known Issues to readme.\n\n* `isort` import order.\n\n* Remove _mps_warmup helpers from ModelMixin.\n\nAnd just make changes to the tests.\n\n* Skip tests using unittest decorator for consistency.\n\n* Remove temporary var.\n\n* Remove spurious blank space.\n\n* Remove unused symbol.\n\n* Remove README_mps.\n\nCo-authored-by: Suraj Patil <surajp815@gmail.com>\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com> \n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class NCSNppModelTests(ModelTesterMixin, unittest.TestCase):",
            "num_channels = 3",
            "",
            "noise = floats_tensor((batch_size, num_channels) + sizes).to(torch_device)",
            "-        time_step = torch.tensor(batch_size * [10]).to(torch_device)",
            "+        time_step = torch.tensor(batch_size * [10]).to(dtype=torch.int32, device=torch_device)",
            "",
            "return {\"sample\": noise, \"timestep\": time_step}"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=1, insert_id=105171)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=105172)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=105173)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=105174)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=105175)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=105176)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=105177)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=105178)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=identifier, text=torch_device), position=2)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=105179)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=105180)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'int32'), position=2, insert_id=105181)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 12,
        "number": 7013,
        "neg_line": [
            "-time_step = torch.tensor(batch_size * [10]).to(torch_device)"
        ],
        "pos_line": [
            "+time_step = torch.tensor(batch_size * [10]).to(dtype=torch.int32, device=torch_device)"
        ],
        "core_change": "-time_step = torch.tensor(batch_size * [10]).to(torch_device) +time_step = torch.tensor(batch_size * [10]).to(dtype=torch.int32, device=torch_device)",
        "core_API": "tensor"
    },
    {
        "commit_hash": "95b6d985ffb231d9a2047c039c95660913063b9d",
        "index": "3f3e8241..1b221ef7 100644",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class CnnEncoder(Seq2VecEncoder):",
            "def get_output_dim(self) -> int:",
            "return self._output_dim",
            "",
            "-    def forward(self, tokens: torch.Tensor, mask: torch.Tensor):",
            "+    def forward(self, tokens: torch.Tensor, mask: torch.BoolTensor):",
            "if mask is not None:",
            "-            tokens = tokens * mask.unsqueeze(-1).float()",
            "+            tokens = tokens * mask.unsqueeze(-1)",
            "",
            "# Our input is expected to have shape `(batch_size, num_tokens, embedding_dim)`.  The",
            "# convolution layers expect input of shape `(batch_size, in_channels, sequence_length)`,"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=Tensor), value='BoolTensor')",
            "Move(target_node=ASTNode(type=binary_operator), node=ASTNode(type=call), position=2)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=float))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 9,
        "number": 7018,
        "neg_line": [
            "-def forward(self, tokens: torch.Tensor, mask: torch.Tensor):",
            "-tokens = tokens * mask.unsqueeze(-1).float()"
        ],
        "pos_line": [
            "+def forward(self, tokens: torch.Tensor, mask: torch.BoolTensor):",
            "+tokens = tokens * mask.unsqueeze(-1)"
        ],
        "core_change": "-def forward(self, tokens: torch.Tensor, mask: torch.Tensor): +def forward(self, tokens: torch.Tensor, mask: torch.BoolTensor): -tokens = tokens * mask.unsqueeze(-1).float() +tokens = tokens * mask.unsqueeze(-1)",
        "core_API": "unsqueeze"
    },
    {
        "commit_hash": "069f1632e6b10fe0effe7a4f6cf972f43ea8bc9a",
        "index": "08b22b3..b3d021b 100644",
        "commit_message": "add options for anchors; fix alignment issue with resnet maxpool; more documentation.\n\n",
        "file": "tf-faster-rcnn.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Network(object):",
            "y1 = tf.slice(rois, [0, 2], [-1, 1], name=\"y1\") / height",
            "x2 = tf.slice(rois, [0, 3], [-1, 1], name=\"x2\") / width",
            "y2 = tf.slice(rois, [0, 4], [-1, 1], name=\"y2\") / height",
            "-      bboxes = tf.concat([y1, x1, y2, x2], axis=1)",
            "+      # Won't be backpropagated to rois anyway, but to save time",
            "+      bboxes = tf.stop_gradient(tf.concat([y1, x1, y2, x2], axis=1))",
            "pre_pool_size = cfg.POOLING_SIZE * 2",
            "crops = tf.image.crop_and_resize(bottom, bboxes, tf.to_int32(batch_ids), [pre_pool_size, pre_pool_size], name=\"crops\")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2334107)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=2334108)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2334109)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2334110)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'stop_gradient'), position=2, insert_id=2334111)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2334112)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=call), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2334113)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 7024,
        "neg_line": [
            "-bboxes = tf.concat([y1, x1, y2, x2], axis=1)"
        ],
        "pos_line": [
            "+# Won't be backpropagated to rois anyway, but to save time",
            "+bboxes = tf.stop_gradient(tf.concat([y1, x1, y2, x2], axis=1))"
        ],
        "core_change": "-bboxes = tf.concat([y1, x1, y2, x2], axis=1) +# Won't be backpropagated to rois anyway, but to save time +bboxes = tf.stop_gradient(tf.concat([y1, x1, y2, x2], axis=1))",
        "core_API": "slice"
    },
    {
        "commit_hash": "2b2a2f8df27d7ee211f52ea1b482a8ef0baf8ba4",
        "index": "23c513393..ab4444019 100644",
        "commit_message": "[Bart] Fix: put dummy_inputs on correct device (#3398)\n\n* Dummy inputs to model.device\n\n* Move self.device to ModuleUtilsMixin\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class PretrainedBartModel(PreTrainedModel):",
            "@property",
            "def dummy_inputs(self):",
            "pad_token = self.config.pad_token_id",
            "-        input_ids = torch.tensor([[0, 6, 10, 4, 2], [0, 8, 12, 2, pad_token]])",
            "-        decoder_input_ids, decoder_attn_mask = _prepare_bart_decoder_inputs(self.config, input_ids,)",
            "+        input_ids = torch.tensor([[0, 6, 10, 4, 2], [0, 8, 12, 2, pad_token]], device=self.device)",
            "+        decoder_input_ids, decoder_attn_mask = _prepare_bart_decoder_inputs(self.config, input_ids)",
            "dummy_inputs = {",
            "\"decoder_input_ids\": decoder_input_ids,",
            "\"attention_mask\": input_ids.ne(pad_token),"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1240531)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1240532)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=1240533)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1240534)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=1240535)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=1240536)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1240537)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=1240538)",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 9,
        "number": 7025,
        "neg_line": [
            "-input_ids = torch.tensor([[0, 6, 10, 4, 2], [0, 8, 12, 2, pad_token]])",
            "-decoder_input_ids, decoder_attn_mask = _prepare_bart_decoder_inputs(self.config, input_ids,)"
        ],
        "pos_line": [
            "+input_ids = torch.tensor([[0, 6, 10, 4, 2], [0, 8, 12, 2, pad_token]], device=self.device)",
            "+decoder_input_ids, decoder_attn_mask = _prepare_bart_decoder_inputs(self.config, input_ids)"
        ],
        "core_change": "-input_ids = torch.tensor([[0, 6, 10, 4, 2], [0, 8, 12, 2, pad_token]]) -decoder_input_ids, decoder_attn_mask = _prepare_bart_decoder_inputs(self.config, input_ids,) +input_ids = torch.tensor([[0, 6, 10, 4, 2], [0, 8, 12, 2, pad_token]], device=self.device) +decoder_input_ids, decoder_attn_mask = _prepare_bart_decoder_inputs(self.config, input_ids)",
        "core_API": "tensor"
    },
    {
        "commit_hash": "9e0fd78051da2d86f7c5be2cfa03648ec0554141",
        "index": "9340eb04f..dbab7d5d1 100755",
        "commit_message": "Fix reference to tpu short seq length (#13686)\n\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class FNetBasicFourierTransform(nn.Module):",
            "\"dft_mat_hidden\", torch.tensor(linalg.dft(config.hidden_size), dtype=torch.complex64)",
            ")",
            "self.register_buffer(",
            "-                    \"dft_mat_seq\", torch.tensor(linalg.dft(config.tpu_short_sequence_length), dtype=torch.complex64)",
            "+                    \"dft_mat_seq\", torch.tensor(linalg.dft(config.tpu_short_seq_length), dtype=torch.complex64)",
            ")",
            "self.fourier_transform = partial(",
            "two_dim_matmul, matrix_dim_one=self.dft_mat_seq, matrix_dim_two=self.dft_mat_hidden"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tpu_short_sequence_length), value='tpu_short_seq_length')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 7027,
        "neg_line": [
            "-\"dft_mat_seq\", torch.tensor(linalg.dft(config.tpu_short_sequence_length), dtype=torch.complex64)"
        ],
        "pos_line": [
            "+\"dft_mat_seq\", torch.tensor(linalg.dft(config.tpu_short_seq_length), dtype=torch.complex64)"
        ],
        "core_change": "-\"dft_mat_seq\", torch.tensor(linalg.dft(config.tpu_short_sequence_length), dtype=torch.complex64) +\"dft_mat_seq\", torch.tensor(linalg.dft(config.tpu_short_seq_length), dtype=torch.complex64)",
        "core_API": "tensor"
    },
    {
        "commit_hash": "a5b98f025bab56e28310108e355f226b514edad0",
        "index": "efaf1df3a..0c8af56f2 100644",
        "commit_message": "Fix docstring issues (#2072)\n\n* Fix docstring module name of filesystems\n\n* Fix docstring missing blank after comma\n\n* Fix docstring missing trailing dot\n\n* Fix docstring cross-referencing in Returns\n\n* Fix docstring cross-referencing in description\n\n* Fix docstring non-rendered args descriptions\n\n* Fix docstring cross-reference content prefix\n\n* Fix docstring line length\n\n* Fix docstring of SplitGenerator\n\n* Fix docstring of Split\n\n* Fix docstring document cross-reference in Split\n\n* Fix docstring of DatasetDict.shuffle\n\n* Fix docstring of DatasetBuilder\n\n* Add docstring attributes of DatasetInfo\n\n* Change docstring default rendering of Attributes in conf.py\n\n* Fix docstring of list_datasets\n\n* Fix docstring of load_dataset\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "def list_datasets(with_community_datasets=True, with_details=False):",
            "\"\"\"List all the datasets scripts available on HuggingFace AWS bucket.",
            "",
            "Args:",
            "-        with_community_datasets (Optional ``bool``): Include the community provided datasets (default: ``True``)",
            "-        with_details (Optional ``bool``): Return the full details on the datasets instead of only the short name (default: ``False``)",
            "+        with_community_datasets (``bool``, optional, default ``True``): Include the community provided datasets.",
            "+        with_details (``bool``, optional, default ``False``): Return the full details on the datasets instead of only the short name.",
            "\"\"\"",
            "api = HfApi()",
            "return api.dataset_list(with_community_datasets=with_community_datasets, id_only=bool(not with_details))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"\"\"List all the datasets scripts available on HuggingFace AWS bucket.\n\nArgs:\n        with_community_datasets (Optional ``bool``): Include the community provided datasets (default: ``True``)\n        with_details (Optional ``bool``): Return the full details on the datasets instead of only the short name (default: ``False``)\n\"\"\"), value='\"\"\"List all the datasets scripts available on HuggingFace AWS bucket.\\n\\nArgs:\\n        with_community_datasets (``bool``, optional, default ``True``): Include the community provided datasets.\\n        with_details (``bool``, optional, default ``False``): Return the full details on the datasets instead of only the short name.\\n\"\"\"')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 1,
        "number": 7028,
        "neg_line": [
            "-with_community_datasets (Optional ``bool``): Include the community provided datasets (default: ``True``)",
            "-with_details (Optional ``bool``): Return the full details on the datasets instead of only the short name (default: ``False``)"
        ],
        "pos_line": [
            "+with_community_datasets (``bool``, optional, default ``True``): Include the community provided datasets.",
            "+with_details (``bool``, optional, default ``False``): Return the full details on the datasets instead of only the short name."
        ],
        "core_change": "-with_community_datasets (Optional ``bool``): Include the community provided datasets (default: ``True``) -with_details (Optional ``bool``): Return the full details on the datasets instead of only the short name (default: ``False``) +with_community_datasets (``bool``, optional, default ``True``): Include the community provided datasets. +with_details (``bool``, optional, default ``False``): Return the full details on the datasets instead of only the short name.",
        "core_API": "dataset_list"
    },
    {
        "commit_hash": "6434c36c890c01427b4e2538acb9e0aafb1699ee",
        "index": "6cd98714..55f61b3a 100644",
        "commit_message": "[Bug] Fix/flipping returns wrong transformation matrices (#648)\n\n* Proper transformation matrices returned test fixed\n\n* Coordinate tests added to hflip and vflip\n\n* Rounding added before converting to int\n\n* Out of bound assertions fixed\n\nCo-authored-by: Feher Gergo (CC-AD/EPE-Bp) <bosch.feher@extaccount.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestVerticalFlipFn:",
            "input.to(device)",
            "",
            "expected_transform = torch.tensor([[[1., 0., 0.],",
            "-                                            [0., -1., 3.],",
            "+                                            [0., -1., 2.],",
            "[0., 0., 1.]]])  # 1 x 3 x 3",
            "",
            "identity = torch.tensor([[[1., 0., 0.],"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Update(target_node=ASTNode(type=float, text=3.), value='2.')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 7029,
        "neg_line": [
            "-[0., -1., 3.],"
        ],
        "pos_line": [
            "+[0., -1., 2.],"
        ],
        "core_change": "-[0., -1., 3.], +[0., -1., 2.],",
        "core_API": "to"
    },
    {
        "commit_hash": "d215aa258e77f43b6c36950bc9a484d5d849eb4f",
        "index": "7da2d8ec..b2de2634 100644",
        "commit_message": "fix torch.nonzero usage in pytorch 1.5 (#2602)\n\n\n",
        "file": "mmdetection.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class FSAFHead(RetinaHead):",
            "loc_weight = torch.ones_like(reg_loss)",
            "cls_weight = torch.ones_like(cls_loss)",
            "pos_flags = assigned_gt_inds >= 0  # positive pixel flag",
            "-        pos_indices = torch.nonzero(pos_flags).flatten()",
            "+        pos_indices = torch.nonzero(pos_flags, as_tuple=False).flatten()",
            "",
            "if pos_flags.any():  # pos pixels exist",
            "pos_assigned_gt_inds = assigned_gt_inds[pos_flags]"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=638776)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=638777)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'as_tuple'), position=0, insert_id=638778)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=638779)",
            "Insert(target_node=IN(type=keyword_argument), node=('false', 'False'), position=2, insert_id=638780)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 7030,
        "neg_line": [
            "-pos_indices = torch.nonzero(pos_flags).flatten()"
        ],
        "pos_line": [
            "+pos_indices = torch.nonzero(pos_flags, as_tuple=False).flatten()"
        ],
        "core_change": "-pos_indices = torch.nonzero(pos_flags).flatten() +pos_indices = torch.nonzero(pos_flags, as_tuple=False).flatten()",
        "core_API": "ones_like"
    },
    {
        "commit_hash": "81c260fb483e92c39195a1ca36e65bed33868157",
        "index": "467cdf26..ba08b0b7 100644",
        "commit_message": "Fixed audio/bag/binary feature tests to work on GPU (#1600)\n\n* Fixed audio/bag/binary feature tests to work on GPU\n\n* Added timeseries feature test (#1601)\n\n* Removed device transfer calls from encoders to features\n",
        "file": "ludwig.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def bag_config():",
            "@pytest.mark.parametrize(\"encoder\", [\"embed\"])",
            "def test_bag_input_feature(bag_config: Dict, encoder: str) -> None:",
            "bag_config.update({\"encoder\": encoder})",
            "-    bag_input_feature = BagInputFeature(bag_config)",
            "-    bag_tensor = torch.randn([BATCH_SIZE, SEQ_SIZE, BAG_W_SIZE], dtype=torch.float32)",
            "+    bag_input_feature = BagInputFeature(bag_config).to(DEVICE)",
            "+    bag_tensor = torch.randn([BATCH_SIZE, SEQ_SIZE, BAG_W_SIZE], dtype=torch.float32).to(DEVICE)",
            "encoder_output = bag_input_feature(bag_tensor)",
            "-    assert encoder_output[\"encoder_output\"].shape[1:][1:] == bag_input_feature.encoder_obj.output_shape",
            "+    assert encoder_output[\"encoder_output\"].shape[1:][1:] == bag_input_feature.output_shape"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('call', None), position=2, insert_id=608138)",
            "Insert(target_node=ASTNode(type=assignment), node=('call', None), position=2, insert_id=608139)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=608140)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=608141)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=608142)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=608143)",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=bag_input_feature), position=0)",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=608144)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to'), position=2, insert_id=608145)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=608146)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'DEVICE'), position=1, insert_id=608147)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=608148)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=608149)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to'), position=2, insert_id=608150)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=608151)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'DEVICE'), position=1, insert_id=608152)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=608153)",
            "Delete(target_node=ASTNode(type=identifier, text=encoder_obj))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 23,
        "number": 7035,
        "neg_line": [
            "-bag_input_feature = BagInputFeature(bag_config)",
            "-bag_tensor = torch.randn([BATCH_SIZE, SEQ_SIZE, BAG_W_SIZE], dtype=torch.float32)",
            "-assert encoder_output[\"encoder_output\"].shape[1:][1:] == bag_input_feature.encoder_obj.output_shape"
        ],
        "pos_line": [
            "+bag_input_feature = BagInputFeature(bag_config).to(DEVICE)",
            "+bag_tensor = torch.randn([BATCH_SIZE, SEQ_SIZE, BAG_W_SIZE], dtype=torch.float32).to(DEVICE)",
            "+assert encoder_output[\"encoder_output\"].shape[1:][1:] == bag_input_feature.output_shape"
        ],
        "core_change": "-bag_input_feature = BagInputFeature(bag_config) -bag_tensor = torch.randn([BATCH_SIZE, SEQ_SIZE, BAG_W_SIZE], dtype=torch.float32) +bag_input_feature = BagInputFeature(bag_config).to(DEVICE) +bag_tensor = torch.randn([BATCH_SIZE, SEQ_SIZE, BAG_W_SIZE], dtype=torch.float32).to(DEVICE) -assert encoder_output[\"encoder_output\"].shape[1:][1:] == bag_input_feature.encoder_obj.output_shape +assert encoder_output[\"encoder_output\"].shape[1:][1:] == bag_input_feature.output_shape",
        "core_API": "parametrize"
    },
    {
        "commit_hash": "741f404d5f212aebc1c9a013f9563454fed640ea",
        "index": "57dd8eff..3c0332ba 100644",
        "commit_message": "fix dump callback\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Callback(object):",
            "def before_train(self, trainer):",
            "self.trainer = trainer",
            "self.graph = tf.get_default_graph()",
            "-        self.sess = tf.get_default_session()",
            "self.epoch_num = 0",
            "self._before_train()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=sess))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=get_default_session))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 15,
        "number": 7036,
        "neg_line": [
            "-self.sess = tf.get_default_session()"
        ],
        "pos_line": [],
        "core_change": "-self.sess = tf.get_default_session()",
        "core_API": "get_default_graph"
    },
    {
        "commit_hash": "0c1f7cba1acf3ecdecea47b914072dfef1878beb",
        "index": "d5b796eb94..57616c2dba 100644",
        "commit_message": "lintfixbot: Auto-commit fixed lint errors in codebase\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tensorflow",
        "change": [
            "def test_tensorflow_shape(",
            "",
            "",
            "@handle_frontend_test(",
            "-fn_tree=\"tensorflow.shape_n\",",
            "-dtype_and_x=helpers.dtype_and_values(",
            "-    available_dtypes=helpers.get_dtypes(\"valid\"),",
            "-    max_num_dims=5),",
            "-output_dtype=st.sampled_from([\"int32\", \"int64\"]),",
            "+    fn_tree=\"tensorflow.shape_n\",",
            "+    dtype_and_x=helpers.dtype_and_values(",
            "+        available_dtypes=helpers.get_dtypes(\"valid\"), max_num_dims=5",
            "+    ),",
            "+    output_dtype=st.sampled_from([\"int32\", \"int64\"]),",
            ")",
            "def test_tensorflow_shape_n(",
            "*,"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 5,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 7037,
        "neg_line": [
            "-fn_tree=\"tensorflow.shape_n\",",
            "-dtype_and_x=helpers.dtype_and_values(",
            "-available_dtypes=helpers.get_dtypes(\"valid\"),",
            "-max_num_dims=5),",
            "-output_dtype=st.sampled_from([\"int32\", \"int64\"]),"
        ],
        "pos_line": [
            "+fn_tree=\"tensorflow.shape_n\",",
            "+dtype_and_x=helpers.dtype_and_values(",
            "+available_dtypes=helpers.get_dtypes(\"valid\"), max_num_dims=5",
            "+),",
            "+output_dtype=st.sampled_from([\"int32\", \"int64\"]),"
        ],
        "core_change": "-fn_tree=\"tensorflow.shape_n\", -dtype_and_x=helpers.dtype_and_values( -available_dtypes=helpers.get_dtypes(\"valid\"), -max_num_dims=5), -output_dtype=st.sampled_from([\"int32\", \"int64\"]), +fn_tree=\"tensorflow.shape_n\", +dtype_and_x=helpers.dtype_and_values( +available_dtypes=helpers.get_dtypes(\"valid\"), max_num_dims=5 +), +output_dtype=st.sampled_from([\"int32\", \"int64\"]),",
        "core_API": "dtype_and_values"
    },
    {
        "commit_hash": "cf5f31721ef489bb82bf4a7bb2d493713d48447d",
        "index": "9ffaa123..4988b6cc 100644",
        "commit_message": "Sync for 4.0b4 release (#950)\n\n* sync for 4.0b4 release\n\n* fix extra space character in build.sh and add a simple prediction test for smoke testing on older macs\n",
        "file": "coremltools.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "models",
        "change": [
            "def convert(",
            "if convert_to == 'mil':",
            "return proto_spec # Returns the MIL program",
            "",
            "-    model = coremltools.models.MLModel(proto_spec, useCPUOnly=True)",
            "+    useCPUOnly = kwargs.get(\"useCPUOnly\", True)",
            "+    model = coremltools.models.MLModel(proto_spec, useCPUOnly=useCPUOnly)",
            "",
            "if minimum_deployment_target is not None:",
            "check_deployment_compatibility("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=2, insert_id=2652305)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=2652306)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'useCPUOnly'), position=0, insert_id=2652307)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=2652308)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=2652309)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2652310)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2652311)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'kwargs'), position=0, insert_id=2652312)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2652313)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'get'), position=2, insert_id=2652314)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2652315)",
            "Insert(target_node=IN(type=argument_list), node=('string', '\"useCPUOnly\"'), position=1, insert_id=2652316)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=2652317)",
            "Insert(target_node=IN(type=argument_list), node=('true', 'True'), position=3, insert_id=2652318)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=2652319)",
            "Insert(target_node=ASTNode(type=keyword_argument), node=('identifier', 'useCPUOnly'), position=2, insert_id=2652320)",
            "Delete(target_node=ASTNode(type=true, text=True))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 7038,
        "neg_line": [
            "-model = coremltools.models.MLModel(proto_spec, useCPUOnly=True)"
        ],
        "pos_line": [
            "+useCPUOnly = kwargs.get(\"useCPUOnly\", True)",
            "+model = coremltools.models.MLModel(proto_spec, useCPUOnly=useCPUOnly)"
        ],
        "core_change": "-model = coremltools.models.MLModel(proto_spec, useCPUOnly=True) +useCPUOnly = kwargs.get(\"useCPUOnly\", True) +model = coremltools.models.MLModel(proto_spec, useCPUOnly=useCPUOnly)",
        "core_API": "MLModel"
    },
    {
        "commit_hash": "e245065fbcc7701da528fbe2568242d50586a0a3",
        "index": "50591acf3..95b7fd206 100755",
        "commit_message": "limit auto scaling batch size to the size of the training dataset (#3271)\n\n* fix\n\n* fix and test\n\n* fix merge error\n\n* test for max dataset size\n\n* changelog\n\n* update docs\n\n* fix merge\n\n* unused imports\n\n* imports\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "dataloader",
        "change": [
            "def test_auto_scale_batch_size_set_model_attribute(tmpdir, use_hparams):",
            "",
            "trainer = Trainer(default_root_dir=tmpdir, max_epochs=1, auto_scale_batch_size=True)",
            "trainer.tune(model, datamodule_fit)",
            "-    assert trainer.datamodule == datamodule_fit",
            "after_batch_size = model.hparams.batch_size if use_hparams else model.batch_size",
            "+    assert trainer.datamodule == datamodule_fit",
            "assert before_batch_size != after_batch_size",
            "+    assert after_batch_size <= len(trainer.train_dataloader.dataset)",
            "assert datamodule_fit.batch_size == after_batch_size",
            "# should be left unchanged, since it was not passed to .tune()",
            "assert datamodule_model.batch_size == 111"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=expression_statement), node=ASTNode(type=module), position=3)",
            "Insert(target_node=ASTNode(type=module), node=('assert_statement', None), position=7, insert_id=1770335)",
            "Insert(target_node=IN(type=assert_statement), node=('assert', 'assert'), position=0, insert_id=1770336)",
            "Insert(target_node=IN(type=assert_statement), node=('comparison_operator', None), position=1, insert_id=1770337)",
            "Insert(target_node=IN(type=comparison_operator), node=('identifier', 'after_batch_size'), position=0, insert_id=1770338)",
            "Insert(target_node=IN(type=comparison_operator), node=('<=', '<='), position=1, insert_id=1770339)",
            "Insert(target_node=IN(type=comparison_operator), node=('call', None), position=2, insert_id=1770340)",
            "Insert(target_node=IN(type=call), node=('identifier', 'len'), position=0, insert_id=1770341)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1770342)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1770343)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=1770344)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1770345)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1770346)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1770347)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dataset'), position=2, insert_id=1770348)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'trainer'), position=0, insert_id=1770349)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1770350)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'train_dataloader'), position=2, insert_id=1770351)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 18,
        "number": 7045,
        "neg_line": [
            "-assert trainer.datamodule == datamodule_fit"
        ],
        "pos_line": [
            "+assert trainer.datamodule == datamodule_fit",
            "+assert after_batch_size <= len(trainer.train_dataloader.dataset)"
        ],
        "core_change": "-assert trainer.datamodule == datamodule_fit +assert trainer.datamodule == datamodule_fit +assert after_batch_size <= len(trainer.train_dataloader.dataset)",
        "core_API": "tune"
    },
    {
        "commit_hash": "35fe3213ef5b1c566c4d73a9d90b0c2aace04f88",
        "index": "69ac74fc..cd86c2ac 100644",
        "commit_message": "RANSAC improvements (#1435)\n\n- Added LU solver for RANSAC-homography, it speeds-up the minimal-solver part x10\n- Now find_fundamental weights is optional, as it was for homography\n- Fix test precision and made one xFail test to be compulsory\n- Fixed tests for fundamental matrix, as one have to have >= 8 points (also added to docstring)\n- updates deprecated torch.svd to torch.linalg.svd\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestSolveWithMask:",
            ")",
            "def test_all_bad(self, device, dtype):",
            "A = torch.ones(10, 3, 3, device=device, dtype=dtype)",
            "-        B = torch.ones(3, 10, device=device, dtype=dtype)",
            "+        B = torch.ones(10, 3, device=device, dtype=dtype)",
            "",
            "X, _, mask = safe_solve_with_mask(B, A)",
            "assert torch.equal(mask, torch.zeros_like(mask))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=integer, text=3), value='10')",
            "Update(target_node=ASTNode(type=integer, text=10), value='3')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 7048,
        "neg_line": [
            "-B = torch.ones(3, 10, device=device, dtype=dtype)"
        ],
        "pos_line": [
            "+B = torch.ones(10, 3, device=device, dtype=dtype)"
        ],
        "core_change": "-B = torch.ones(3, 10, device=device, dtype=dtype) +B = torch.ones(10, 3, device=device, dtype=dtype)",
        "core_API": "ones"
    },
    {
        "commit_hash": "e0c7bbf341e8ab6d8e7103f48cf92a743eab9376",
        "index": "a02a67c81..0a5fc5cce 100644",
        "commit_message": "Clean up host model API (#298)\n\n* Add new columns to TorchModel\n\n* Add allow_model_copy and allow_run_inference args\n\n* Snapshot shouldn't include model objects\n\n* Fix tests\n\n* Create get_serialized_model_with_id\n\n* Create get_model route\n\n* Create get_model_copy\n\n* Create global variables for storing error messages\n\n* Rename allow_model_copy -> allow_get_model_copy\n\n* Fix typo\n\n* Implement serve_encrypted_model\n\n* Improve comments\n\n* Add tests for new arguments\n\n* Fix test\n\n* Standardize results\n\n* Update tests\n\n* Fix small bug\n\n* Improve delete error messages\n\n* Fix typo\n\n* Fix test\n\n* Extend syft with a friendly api\n\n* Better argument names\n\n* Try to fix tests\n\n* Try to fix tests\n\n* Fix tests\n\n* Added TODO\n\n* Update notebooks\n\n* download -> download_model\n\n",
        "file": "PySyft.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def snapshot(worker):",
            "objects.append(WorkerObject(worker_id=worker.id, object=obj, id=key))",
            "",
            "db.session.add_all(objects)",
            "-    db.session.add_all(models)",
            "db.session.commit()",
            "last_snapshot_keys = current_keys"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=db))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=session))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=add_all))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=models))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 13,
        "number": 7053,
        "neg_line": [
            "-db.session.add_all(models)"
        ],
        "pos_line": [],
        "core_change": "-db.session.add_all(models)",
        "core_API": "append"
    },
    {
        "commit_hash": "5904c459791243914eaa79ae7aa8a6e4019526f3",
        "index": "f9fe187..d1ebd49 100644",
        "commit_message": "mlp layernorm fix (#676)\n\nSummary:\nPull Request resolved: https://github.com/facebookresearch/pytext/pull/676\n\nfix oss test\n\nReviewed By: hikushalhere, seayoung1112\n\nDifferential Revision: D15725808\n\nfbshipit-source-id: e3916e59df55e4f54123b451521c439bf3d07aa5\n\n",
        "file": "pytext.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class MLPDecoder(DecoderBase):",
            "layers.append(nn.Linear(in_dim, dim))",
            "layers.append(nn.ReLU())",
            "if config.layer_norm:",
            "-                layers.append(LayerNorm(dim))",
            "+                layers.append(nn.LayerNorm(dim))",
            "in_dim = dim",
            "if config.out_dim:",
            "out_dim = config.out_dim"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1472681)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'nn'), position=0, insert_id=1472682)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1472683)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=LayerNorm), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 7059,
        "neg_line": [
            "-layers.append(LayerNorm(dim))"
        ],
        "pos_line": [
            "+layers.append(nn.LayerNorm(dim))"
        ],
        "core_change": "-layers.append(LayerNorm(dim)) +layers.append(nn.LayerNorm(dim))",
        "core_API": "append"
    },
    {
        "commit_hash": "58cf39bd1ac5b1f03fd8d49c20d33fff53e10b9e",
        "index": "d87e6a10..84aab7f5 100644",
        "commit_message": "merge code for kaggle house (#1186)\n\n* merge code for kaggle house\n\n* fix\n",
        "file": "d2l-en.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "arange = tf.range",
            "astype = tf.cast",
            "int32 = tf.int32",
            "float32 = tf.float32",
            "+concat = tf.concat",
            "numpy = lambda x, *args, **kwargs: x.numpy(*args, **kwargs)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=4, insert_id=1917611)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1917612)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'concat'), position=0, insert_id=1917613)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1917614)",
            "Insert(target_node=IN(type=assignment), node=('attribute', None), position=2, insert_id=1917615)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=1917616)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1917617)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'concat'), position=2, insert_id=1917618)"
        ],
        "plus_line": 0,
        "minus_line": 0,
        "AST_diff_line": 8,
        "number": 7060,
        "neg_line": [],
        "pos_line": [
            "+concat = tf.concat"
        ],
        "core_change": "+concat = tf.concat",
        "core_API": "numpy"
    },
    {
        "commit_hash": "c34f93f7e43155ffedcdbd1ee7144eae1a415ec6",
        "index": "68a2e93d8..3807dc0ea 100644",
        "commit_message": "fix\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class FrameScoreFeats(AbsFeatsExtract):",
            "pad = self.win_length // 2",
            "input_lengths = input_lengths + 2 * pad",
            "",
            "-            olens = torch.div((input_lengths - self.win_length), self.hop_length) + 1",
            "+            olens = (",
            "+                torch.div(",
            "+                    (input_lengths - self.win_length),",
            "+                    self.hop_length,",
            "+                    rounding_mode=\"floor\",",
            "+                )",
            "+                + 1",
            "+            )",
            "output.masked_fill_(make_pad_mask(olens, output, 1), 0.0)",
            "else:",
            "olens = None"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('parenthesized_expression', None), position=2, insert_id=119843)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('(', '('), position=0, insert_id=119844)",
            "Move(target_node=IN(type=parenthesized_expression), node=ASTNode(type=binary_operator), position=1)",
            "Insert(target_node=IN(type=parenthesized_expression), node=(')', ')'), position=2, insert_id=119845)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=119846)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=119847)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=6, insert_id=119848)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'rounding_mode'), position=0, insert_id=119849)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=119850)",
            "Insert(target_node=IN(type=keyword_argument), node=('string', '\"floor\"'), position=2, insert_id=119851)"
        ],
        "plus_line": 8,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 7061,
        "neg_line": [
            "-olens = torch.div((input_lengths - self.win_length), self.hop_length) + 1"
        ],
        "pos_line": [
            "+olens = (",
            "+torch.div(",
            "+(input_lengths - self.win_length),",
            "+self.hop_length,",
            "+rounding_mode=\"floor\",",
            "+)",
            "++ 1",
            "+)"
        ],
        "core_change": "-olens = torch.div((input_lengths - self.win_length), self.hop_length) + 1 +olens = ( +torch.div( +(input_lengths - self.win_length), +self.hop_length, +rounding_mode=\"floor\", +) ++ 1 +)",
        "core_API": "div"
    },
    {
        "commit_hash": "ffa0e3ba8f497da569a46769c5b775423a99ffbc",
        "index": "1e605cc..f1a98bf 100644",
        "commit_message": "Fix torchscript fp16 on int inputs\n\nuse .half() only when input is float\n\nSigned-off-by: Jonathan Chang <31893406+cccntu@users.noreply.github.com>\n\n",
        "file": "nebullvm.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class PytorchBackendCompiler(Compiler):",
            "input_sample = input_data.get_list(1)[0]",
            "if self.device is Device.GPU:",
            "if quantization_type is QuantizationType.HALF:",
            "-                input_sample = [t.cuda().half() for t in input_sample]",
            "+                input_sample = [",
            "+                    t.cuda().half() if torch.is_floating_point(t) else t.cuda()",
            "+                    for t in input_sample",
            "+                ]",
            "else:",
            "input_sample = [t.cuda() for t in input_sample]"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=list_comprehension), node=('conditional_expression', None), position=1, insert_id=651147)",
            "Move(target_node=IN(type=conditional_expression), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=conditional_expression), node=('if', 'if'), position=1, insert_id=651148)",
            "Insert(target_node=IN(type=conditional_expression), node=('call', None), position=2, insert_id=651149)",
            "Insert(target_node=IN(type=conditional_expression), node=('else', 'else'), position=3, insert_id=651150)",
            "Insert(target_node=IN(type=conditional_expression), node=('call', None), position=4, insert_id=651151)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=651152)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=651153)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=651154)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=651155)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=651156)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=651157)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'is_floating_point'), position=2, insert_id=651158)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=651159)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 't'), position=1, insert_id=651160)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=651161)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 't'), position=0, insert_id=651162)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=651163)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cuda'), position=2, insert_id=651164)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=651165)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=651166)"
        ],
        "plus_line": 4,
        "minus_line": 1,
        "AST_diff_line": 21,
        "number": 7063,
        "neg_line": [
            "-input_sample = [t.cuda().half() for t in input_sample]"
        ],
        "pos_line": [
            "+input_sample = [",
            "+t.cuda().half() if torch.is_floating_point(t) else t.cuda()",
            "+for t in input_sample",
            "+]"
        ],
        "core_change": "-input_sample = [t.cuda().half() for t in input_sample] +input_sample = [ +t.cuda().half() if torch.is_floating_point(t) else t.cuda() +for t in input_sample +]",
        "core_API": "get_list"
    },
    {
        "commit_hash": "3335de5f441ee1b3824e16dcd98db620e40beaba",
        "index": "09fefd85..6b2c9819 100644",
        "commit_message": "add vq-wav2vec (#1029)\n\nSummary:\nsanitized vq-wav2vec implementation. i will also add docs to this. i have a fixed-up checkpoint that this code can load and verified that it produces same results as what we used in paper\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1029\n\nDifferential Revision: D20129246\n\nPulled By: alexeib\n\nfbshipit-source-id: f72f455e0c309168e644ab86ec18c768c308da98\n\n",
        "file": "fairseq.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "the corresponding GitHub repo: https://github.com/hendrycks/GELUs",
            "import math",
            "",
            "import torch",
            "+import torch.nn as nn",
            "",
            "",
            "def gelu_accurate(x):",
            "if not hasattr(gelu_accurate, \"_a\"):",
            "gelu_accurate._a = math.sqrt(2 / math.pi)",
            "-    return 0.5 * x * (1 + torch.tanh(gelu_accurate._a * (x + 0.044715 * torch.pow(x, 3))))",
            "+    return (",
            "+        0.5 * x * (1 + torch.tanh(gelu_accurate._a * (x + 0.044715 * torch.pow(x, 3))))",
            "+    )",
            "",
            "",
            "def gelu(x: torch.Tensor) -> torch.Tensor:",
            "-    if hasattr(torch.nn.functional, 'gelu'):",
            "+    if hasattr(torch.nn.functional, \"gelu\"):",
            "return torch.nn.functional.gelu(x.float()).type_as(x)",
            "else:",
            "return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('import_statement', None), position=4, insert_id=215565)",
            "Insert(target_node=IN(type=import_statement), node=('import', 'import'), position=0, insert_id=215566)",
            "Insert(target_node=IN(type=import_statement), node=('aliased_import', None), position=1, insert_id=215567)",
            "Insert(target_node=ASTNode(type=return_statement), node=('parenthesized_expression', None), position=1, insert_id=215568)",
            "Insert(target_node=IN(type=aliased_import), node=('dotted_name', None), position=0, insert_id=215569)",
            "Insert(target_node=IN(type=aliased_import), node=('as', 'as'), position=1, insert_id=215570)",
            "Insert(target_node=IN(type=aliased_import), node=('identifier', 'nn'), position=2, insert_id=215571)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('(', '('), position=0, insert_id=215572)",
            "Move(target_node=IN(type=parenthesized_expression), node=ASTNode(type=binary_operator), position=1)",
            "Insert(target_node=IN(type=parenthesized_expression), node=(')', ')'), position=2, insert_id=215573)",
            "Insert(target_node=IN(type=dotted_name), node=('identifier', 'torch'), position=0, insert_id=215574)",
            "Insert(target_node=IN(type=dotted_name), node=('.', '.'), position=1, insert_id=215575)",
            "Insert(target_node=IN(type=dotted_name), node=('identifier', 'nn'), position=2, insert_id=215576)",
            "Update(target_node=ASTNode(type=string, text='gelu'), value='\"gelu\"')"
        ],
        "plus_line": 4,
        "minus_line": 2,
        "AST_diff_line": 14,
        "number": 7067,
        "neg_line": [
            "-return 0.5 * x * (1 + torch.tanh(gelu_accurate._a * (x + 0.044715 * torch.pow(x, 3))))",
            "-if hasattr(torch.nn.functional, 'gelu'):"
        ],
        "pos_line": [
            "+import torch.nn as nn",
            "+return (",
            "+0.5 * x * (1 + torch.tanh(gelu_accurate._a * (x + 0.044715 * torch.pow(x, 3))))",
            "+)",
            "+if hasattr(torch.nn.functional, \"gelu\"):"
        ],
        "core_change": "+import torch.nn as nn -return 0.5 * x * (1 + torch.tanh(gelu_accurate._a * (x + 0.044715 * torch.pow(x, 3)))) +return ( +0.5 * x * (1 + torch.tanh(gelu_accurate._a * (x + 0.044715 * torch.pow(x, 3)))) +) -if hasattr(torch.nn.functional, 'gelu'): +if hasattr(torch.nn.functional, \"gelu\"):",
        "core_API": "sqrt"
    },
    {
        "commit_hash": "a3e55be685fb52ac8c7caafd6835f521bcbcff9d",
        "index": "3e01492c..3977eda1 100755",
        "commit_message": "fixed and improved shape handling for exploration\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class GaussianNoise(Exploration):",
            "",
            "super(GaussianNoise, self).__init__(scope=scope, summary_labels=summary_labels)",
            "",
            "-    def tf_explore(self, episode, timestep, action_spec):",
            "-        return tf.random_normal(shape=action_spec['shape'], mean=self.mu, stddev=self.sigma)",
            "+    def tf_explore(self, episode, timestep, shape):",
            "+        return tf.random_normal(shape=shape, mean=self.mu, stddev=self.sigma)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=action_spec), value='shape')",
            "Update(target_node=ASTNode(type=identifier, text=action_spec), value='shape')",
            "Move(target_node=ASTNode(type=keyword_argument), node=ASTNode(type=identifier, text=action_spec), position=2)",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=string, text='shape'))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 7,
        "number": 7068,
        "neg_line": [
            "-def tf_explore(self, episode, timestep, action_spec):",
            "-return tf.random_normal(shape=action_spec['shape'], mean=self.mu, stddev=self.sigma)"
        ],
        "pos_line": [
            "+def tf_explore(self, episode, timestep, shape):",
            "+return tf.random_normal(shape=shape, mean=self.mu, stddev=self.sigma)"
        ],
        "core_change": "-def tf_explore(self, episode, timestep, action_spec): -return tf.random_normal(shape=action_spec['shape'], mean=self.mu, stddev=self.sigma) +def tf_explore(self, episode, timestep, shape): +return tf.random_normal(shape=shape, mean=self.mu, stddev=self.sigma)",
        "core_API": "random_normal"
    },
    {
        "commit_hash": "9b6385488617ee99188b60c0525cbe4ec7e8f2a0",
        "index": "75b81431..f79e7926 100644",
        "commit_message": "Improve reproduceability 2/3 (#1906)\n\n* [Repro] Correct reproducability\n\n* up\n\n* up\n\n* uP\n\n* up\n\n* need better image\n\n* allow conversion from no state dict checkpoints\n\n* up\n\n* up\n\n* up\n\n* up\n\n* check tensors\n\n* check tensors\n\n* check tensors\n\n* check tensors\n\n* next try\n\n* up\n\n* up\n\n* better name\n\n* up\n\n* up\n\n* Apply suggestions from code review\n\n* correct more\n\n* up\n\n* replace all torch randn\n\n* fix\n\n* correct\n\n* correct\n\n* finish\n\n* fix more\n\n* up\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class PaintByExampleImageEncoder(CLIPPreTrainedModel):",
            "self.proj_out = nn.Linear(config.hidden_size, self.proj_size)",
            "",
            "# uncondition for scaling",
            "-        self.uncond_vector = nn.Parameter(torch.rand((1, 1, self.proj_size)))",
            "+        self.uncond_vector = nn.Parameter(torch.randn((1, 1, self.proj_size)))",
            "",
            "def forward(self, pixel_values):",
            "clip_output = self.model(pixel_values=pixel_values)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=rand), value='randn')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 7070,
        "neg_line": [
            "-self.uncond_vector = nn.Parameter(torch.rand((1, 1, self.proj_size)))"
        ],
        "pos_line": [
            "+self.uncond_vector = nn.Parameter(torch.randn((1, 1, self.proj_size)))"
        ],
        "core_change": "-self.uncond_vector = nn.Parameter(torch.rand((1, 1, self.proj_size))) +self.uncond_vector = nn.Parameter(torch.randn((1, 1, self.proj_size)))",
        "core_API": "Linear"
    },
    {
        "commit_hash": "d26b37e744ea980977e266adf48736451b73c583",
        "index": "3a2d56d87..b1a2b8075 100644",
        "commit_message": "Speech2TextTransformer (#10175)\n\n* s2t\n\n* fix config\n\n* conversion script\n\n* fix import\n\n* add tokenizer\n\n* fix tok init\n\n* fix tokenizer\n\n* first version working\n\n* fix embeds\n\n* fix lm head\n\n* remove extra heads\n\n* fix convert script\n\n* handle encoder attn mask\n\n* style\n\n* better enc attn mask\n\n* override _prepare_attention_mask_for_generation\n\n* handle attn_maks in encoder and decoder\n\n* input_ids => input_features\n\n* enable use_cache\n\n* remove old code\n\n* expand embeddings if needed\n\n* remove logits bias\n\n* masked_lm_loss => loss\n\n* hack tokenizer to support feature processing\n\n* fix model_input_names\n\n* style\n\n* fix error message\n\n* doc\n\n* remove inputs_embeds\n\n* remove input_embeds\n\n* remove unnecessary docstring\n\n* quality\n\n* SpeechToText => Speech2Text\n\n* style\n\n* remove shared_embeds\n\n* subsample => conv\n\n* remove Speech2TextTransformerDecoderWrapper\n\n* update output_lengths formula\n\n* fix table\n\n* remove max_position_embeddings\n\n* update conversion scripts\n\n* add possibility to do upper case for now\n\n* add FeatureExtractor and Processor\n\n* add tests for extractor\n\n* require_torch_audio => require_torchaudio\n\n* add processor test\n\n* update import\n\n* remove classification head\n\n* attention mask is now 1D\n\n* update docstrings\n\n* attention mask should be of type long\n\n* handle attention mask from generate\n\n* alwyas return attention_mask\n\n* fix test\n\n* style\n\n* doc\n\n* Speech2TextTransformer => Speech2Text\n\n* Speech2TextTransformerConfig => Speech2TextConfig\n\n* remove dummy_inputs\n\n* nit\n\n* style\n\n* multilinguial tok\n\n* fix tokenizer\n\n* add tgt_lang setter\n\n* save lang_codes\n\n* fix tokenizer\n\n* add forced_bos_token_id to tokenizer\n\n* apply review suggestions\n\n* add torchaudio to extra deps\n\n* add speech deps to CI\n\n* fix dep\n\n* add libsndfile to ci\n\n* libsndfile1\n\n* add speech to extras all\n\n* libsndfile1 -> libsndfile1\n\n* libsndfile\n\n* libsndfile1-dev\n\n* apt update\n\n* add sudo to install\n\n* update deps table\n\n* install libsndfile1-dev on CI\n\n* tuple to list\n\n* init conv layer\n\n* add model tests\n\n* quality\n\n* add integration tests\n\n* skip_special_tokens\n\n* add speech_to_text_transformer in toctree\n\n* fix tokenizer\n\n* fix fp16 tests\n\n* add tokenizer tests\n\n* fix copyright\n\n* input_values => input_features\n\n* doc\n\n* add model in readme\n\n* doc\n\n* change checkpoint names\n\n* fix copyright\n\n* fix code example\n\n* add max_model_input_sizes in tokenizer\n\n* fix integration tests\n\n* add do_lower_case to tokenizer\n\n* remove clamp trick\n\n* fix \"Add modeling imports here\"\n\n* fix copyrights\n\n* fix tests\n\n* SpeechToTextTransformer => SpeechToText\n\n* fix naming\n\n* fix table formatting\n\n* fix typo\n\n* style\n\n* fix typos\n\n* remove speech dep from extras[testing]\n\n* fix copies\n\n* rename doc file,\n\n* put imports under is_torch_available\n\n* run feat extract tests when torch is available\n\n* dummy objects for processor and extractor\n\n* fix imports in tests\n\n* fix import in modeling test\n\n* fxi imports\n\n* fix torch import\n\n* fix imports again\n\n* fix positional embeddings\n\n* fix typo in import\n\n* adapt new extractor refactor\n\n* style\n\n* fix torchscript test\n\n* doc\n\n* doc\n\n* Apply suggestions from code review\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* fix docs, copied from, style\n\n* fix docstring\n\n* handle imports\n\n* remove speech from all extra deps\n\n* remove s2t from seq2seq lm mapping\n\n* better names\n\n* skip training tests\n\n* add install instructions\n\n* List => Tuple\n\n* doc\n\n* fix conversion script\n\n* fix urls\n\n* add instruction for libsndfile\n\n* fix fp16 test\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class GenerationMixin:",
            ") -> torch.LongTensor:",
            "decoder_start_token_id = self._get_decoder_start_token_id(decoder_start_token_id, bos_token_id)",
            "decoder_input_ids = (",
            "-            torch.ones((input_ids.shape[0], 1), dtype=input_ids.dtype, device=input_ids.device)",
            "-            * decoder_start_token_id",
            "+            torch.ones((input_ids.shape[0], 1), dtype=torch.long, device=input_ids.device) * decoder_start_token_id",
            ")",
            "return decoder_input_ids"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=input_ids), value='torch')",
            "Update(target_node=ASTNode(type=identifier, text=dtype), value='long')"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 7071,
        "neg_line": [
            "-torch.ones((input_ids.shape[0], 1), dtype=input_ids.dtype, device=input_ids.device)",
            "-* decoder_start_token_id"
        ],
        "pos_line": [
            "+torch.ones((input_ids.shape[0], 1), dtype=torch.long, device=input_ids.device) * decoder_start_token_id"
        ],
        "core_change": "-torch.ones((input_ids.shape[0], 1), dtype=input_ids.dtype, device=input_ids.device) -* decoder_start_token_id +torch.ones((input_ids.shape[0], 1), dtype=torch.long, device=input_ids.device) * decoder_start_token_id",
        "core_API": "_get_decoder_start_token_id"
    },
    {
        "commit_hash": "98291f0d33b704d1e4d14197278f2996ac8f4b4a",
        "index": "4d5a472d..9502bd4a 100644",
        "commit_message": "[WIP] - Documentation Cleaning and Coding Style (#516)\n\n* Activation Cleaning Docstring Test\n\n* Requirements Pinned with range to insure tested versions are used. Range are used to prevent updating requirements all the time.\n\n* setup.cfg file added with PEP8 configuration\n\n* activation.py refactored\n\n* docstring fixed - ready for documentation unittest\n\n* Yapf correction for max_line_length: 120\n\n* test yapf refactored\n\n* Requirements conflict solved\n\n* Yapf Style modified and merged in file \"setup.cfg\"\n\n* Yapf Confiuguration Updated\n\n* Code Refactored with new YAPF formating style\n\n* Code Refactored with new YAPF formating style\n\n* Code Refactored with new YAPF formating style\n\n* tl.layers.pooling YAPF reformat\n\n* yapf updated\n\n* gitignore updated\n\n* YAPF Style Fixing Attempt\n\n* Space Error Fix\n\n* Style Correction\n\n* Assertion Codacy Errors Corrected\n\n* Error Fix\n\n* Assertion Refactored\n\n* YAPF Style Applied to Master\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class PReluLayer(Layer):",
            "",
            "# with tf.name_scope(name) as scope:",
            "with tf.variable_scope(name):",
            "-            alphas = tf.get_variable(name='alphas', shape=w_shape, initializer=a_init, dtype=LayersConfig.tf_dtype, **a_init_args)",
            "+            alphas = tf.get_variable(",
            "+                name='alphas', shape=w_shape, initializer=a_init, dtype=LayersConfig.tf_dtype, **a_init_args",
            "+            )",
            "try:  # TF 1.0",
            "self.outputs = tf.nn.relu(self.inputs) + tf.multiply(alphas, (self.inputs - tf.abs(self.inputs))) * 0.5",
            "except Exception:  # TF 0.12"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 7077,
        "neg_line": [
            "-alphas = tf.get_variable(name='alphas', shape=w_shape, initializer=a_init, dtype=LayersConfig.tf_dtype, **a_init_args)"
        ],
        "pos_line": [
            "+alphas = tf.get_variable(",
            "+name='alphas', shape=w_shape, initializer=a_init, dtype=LayersConfig.tf_dtype, **a_init_args",
            "+)"
        ],
        "core_change": "-alphas = tf.get_variable(name='alphas', shape=w_shape, initializer=a_init, dtype=LayersConfig.tf_dtype, **a_init_args) +alphas = tf.get_variable( +name='alphas', shape=w_shape, initializer=a_init, dtype=LayersConfig.tf_dtype, **a_init_args +)",
        "core_API": "name_scope"
    },
    {
        "commit_hash": "1daab8714463e63c8f4b4ba4caed3cc1ef97a195",
        "index": "4519061b..da9d60c4 100644",
        "commit_message": "[torch] Enable GPU training (#1508)\n\n* Added more descriptive description_summary, fixed bug in TabNet combiner\n\n* Updated modules to do proper GPU conversion\n\n* Functional GPU training\n\n* Tabnet working, removed RayRemoteTrainer\n\n* Temporary changes to unblock GPU training\n\n* Functional test_seq_encoder\n\n* Cleaned up PR\n\n* Fixed failing CPU tests\n\n* Responded to comments\n\n* Fix failing GPU tests in test_api.py\n\n* [torch] Fix GPU tests for test_visualizations (#1539)\n\nUpdated torch.aranges to registered buffers in embedding_modules.py\n\n* [torch] Fix test_encoder.py for GPU (#1544)\n\n* Accept and move model to GPU as part of Predictor init (#1559)\n\n* [torch] Fixed tests in test_experiment.py (#1562)\n\n* Fixed tests in tests/integration_tests/test_experiment.py\n\n* Moved set embed and inputs to correct device in tests/ludwig/encoders/test_set_encoders.py\n\n* Moved set date and inputs to correct device in tests/ludwig/encoders/test_date_encoders.py\n\n* Moved bag embed and inputs to correct device in tests/ludwig/encoders/test_bag_encoders.py\n\n* Moved sequence embed and inputs to correct device in tests/ludwig/features/test_sequence_features.py\n\n* [torch] Fixed GPU issues in combiners.py (#1568)\n\n* Fix GPU tests in test_collect.py (#1569)\n\n* Added model to Predictor initialization\n\n* Fixed embedding modules tests (#1554)\n\n* Fixed embedding modules tests\n* Moved embedding modules to device in tests, removed duplicate argument\n\nCo-authored-by: Shreya Rajpal <shreya.rajpal@gmail.com>\n\n* Fixed encoders tests except test_h3_rnn_embed (#1556)\n\n* Fixed encoders tests except test_h3_rnn_embed\n\n* Addressed Shreya's comments\n\n* Moved embedding modules to device, used registered buffers\n\nCo-authored-by: Shreya Rajpal <shreya.rajpal@gmail.com>\n\n* Pass ECD object to Predictor\n\n* Removed model arg from predict function\n\nCo-authored-by: Jeffrey Tang <jeff@predibase.com>\nCo-authored-by: Jeffrey Tang <810895+jeffreyftang@users.noreply.github.com>\nCo-authored-by: Animesh Kumar <anmshkmr@users.noreply.github.com>\n",
        "file": "ludwig.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_api_training_determinism(csv_filename):",
            "",
            "divergence = False",
            "for weight_1, weight_2 in zip(model_weights_1, model_weights_2):",
            "-            if not np.allclose(weight_1, weight_2):",
            "+            if not torch.allclose(weight_1, weight_2):",
            "divergence = True",
            "break",
            "assert divergence, 'model_1 and model_2 have identical weights with different seeds!'",
            "",
            "for weight_1, weight_3 in zip(model_weights_1, model_weights_3):",
            "-            assert np.allclose(weight_1, weight_3)",
            "+            assert torch.allclose(weight_1, weight_3)",
            "",
            "",
            "def run_api_commands("
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=np), value='torch')",
            "Update(target_node=ASTNode(type=identifier, text=np), value='torch')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 7079,
        "neg_line": [
            "-if not np.allclose(weight_1, weight_2):",
            "-assert np.allclose(weight_1, weight_3)"
        ],
        "pos_line": [
            "+if not torch.allclose(weight_1, weight_2):",
            "+assert torch.allclose(weight_1, weight_3)"
        ],
        "core_change": "-if not np.allclose(weight_1, weight_2): +if not torch.allclose(weight_1, weight_2): -assert np.allclose(weight_1, weight_3) +assert torch.allclose(weight_1, weight_3)",
        "core_API": "allclose"
    },
    {
        "commit_hash": "c864e447aff682c9dacdf39b7371e39af6b5148e",
        "index": "3d370c99..99d28355 100644",
        "commit_message": "refactor(framework): pytorch splitting refactor (#2317)\n\n* refactor(framework): split pytorch impl to pytorch and torchscript\n\n* style: reformat\n\n* style: minor fix\n\n* doc: fix pytorch/torchscript changes\n\n* chore: add torchscript github workflow\n\n* style: minor fix\n\n* Update bentoml/_internal/frameworks/common/pytorch.py\n\nCo-authored-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* Update bentoml/_internal/frameworks/common/pytorch.py\n\nCo-authored-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* Update bentoml/_internal/frameworks/common/pytorch.py\n\nCo-authored-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* Update bentoml/_internal/frameworks/common/pytorch.py\n\nCo-authored-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* Update bentoml/_internal/frameworks/pytorch.py\n\nCo-authored-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* Update bentoml/_internal/frameworks/common/pytorch.py\n\nCo-authored-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* Update bentoml/_internal/frameworks/common/pytorch.py\n\nCo-authored-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* Update bentoml/_internal/frameworks/common/pytorch.py\n\nCo-authored-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* style: minor fix\n\n* test: pytorch frameworks test save format\n\n* chore: update codecov.yml for torchscript\n\n* chore: github workflow add torchscript\n\n* test(framework): disable pytorch container batch_axis=1 test\n\n* chore: remove double import\n\n* fix: pytorch tensor to device\n\nCo-authored-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n",
        "file": "BentoML.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "models",
        "change": [
            "def predict_df(model: pl.LightningModule, df: pd.DataFrame):",
            "def test_pl_save_load():",
            "model: \"pl.LightningModule\" = AdditionModel()",
            "tag = bentoml.pytorch_lightning.save(\"pytorch_lightning_test\", model)",
            "-    info = bentoml.models.get(tag)",
            "-    assert_have_file_extension(info.path, \".pt\")",
            "+    bentoml_model = bentoml.models.get(tag)",
            "+    assert_have_file_extension(bentoml_model.path, \".pt\")",
            "+    assert bentoml_model.info.context.get(\"model_format\") == \"pytorch_lightning:v1\"",
            "",
            "pl_loaded: \"pl.LightningModule\" = bentoml.pytorch_lightning.load(tag)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('assert_statement', None), position=6, insert_id=2650512)",
            "Insert(target_node=IN(type=assert_statement), node=('assert', 'assert'), position=0, insert_id=2650513)",
            "Insert(target_node=IN(type=assert_statement), node=('comparison_operator', None), position=1, insert_id=2650514)",
            "Update(target_node=ASTNode(type=identifier, text=info), value='bentoml_model')",
            "Insert(target_node=IN(type=comparison_operator), node=('call', None), position=0, insert_id=2650515)",
            "Insert(target_node=IN(type=comparison_operator), node=('==', '=='), position=1, insert_id=2650516)",
            "Insert(target_node=IN(type=comparison_operator), node=('string', '\"pytorch_lightning:v1\"'), position=2, insert_id=2650517)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2650518)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2650519)",
            "Update(target_node=ASTNode(type=identifier, text=info), value='bentoml_model')",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2650520)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2650521)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'get'), position=2, insert_id=2650522)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2650523)",
            "Insert(target_node=IN(type=argument_list), node=('string', '\"model_format\"'), position=1, insert_id=2650524)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2650525)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2650526)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2650527)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'context'), position=2, insert_id=2650528)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'bentoml_model'), position=0, insert_id=2650529)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2650530)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'info'), position=2, insert_id=2650531)"
        ],
        "plus_line": 3,
        "minus_line": 2,
        "AST_diff_line": 22,
        "number": 7080,
        "neg_line": [
            "-info = bentoml.models.get(tag)",
            "-assert_have_file_extension(info.path, \".pt\")"
        ],
        "pos_line": [
            "+bentoml_model = bentoml.models.get(tag)",
            "+assert_have_file_extension(bentoml_model.path, \".pt\")",
            "+assert bentoml_model.info.context.get(\"model_format\") == \"pytorch_lightning:v1\""
        ],
        "core_change": "-info = bentoml.models.get(tag) -assert_have_file_extension(info.path, \".pt\") +bentoml_model = bentoml.models.get(tag) +assert_have_file_extension(bentoml_model.path, \".pt\") +assert bentoml_model.info.context.get(\"model_format\") == \"pytorch_lightning:v1\"",
        "core_API": "save"
    },
    {
        "commit_hash": "2a24412846b45cda96f1da91ec30e41e296911a5",
        "index": "efd7603a..0b59a6e7 100644",
        "commit_message": "[Enhance]: Avoid crash in empty gt training of GFL (#4631)\n\n* avoid crash in empty gt training of GFL\n\n* real fix of the bug\n",
        "file": "mmdetection.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class GFLHead(AnchorHead):",
            "else:",
            "loss_bbox = bbox_pred.sum() * 0",
            "loss_dfl = bbox_pred.sum() * 0",
            "-            weight_targets = torch.tensor(0).cuda()",
            "+            weight_targets = bbox_pred.new_tensor(0)",
            "",
            "# cls (qfl) loss",
            "loss_cls = self.loss_cls("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=argument_list), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=torch), value='bbox_pred')",
            "Update(target_node=ASTNode(type=identifier, text=tensor), value='new_tensor')",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=cuda))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 7082,
        "neg_line": [
            "-weight_targets = torch.tensor(0).cuda()"
        ],
        "pos_line": [
            "+weight_targets = bbox_pred.new_tensor(0)"
        ],
        "core_change": "-weight_targets = torch.tensor(0).cuda() +weight_targets = bbox_pred.new_tensor(0)",
        "core_API": "sum"
    },
    {
        "commit_hash": "62e3d1dbdc88af9ed8afb37f7e8ecc011f5486a3",
        "index": "392bf477..39c2482b 100644",
        "commit_message": "Scope TokenIndexer output by indexer name (#3597)\n\n* Indexer tests are now passing, at least\n\n* Fixed some masking issues, and padding keys for one config file\n\n* TextFieldEmbedder tests pass\n\n* Fixing fixtures, some more tests passing\n\n* Fixed weird ordering bug\n\n* All TokenEmbedder tests passing?\n\n* Update fixtures\n\n* Fix more hard-coded references\n\n* fix field tests\n\n* fix dataset reader tests\n\n* Fix iterator tests\n\n* More tests passing\n\n* fix hotflip and some other tests\n\n* more tests\n\n* more test fixes\n\n* more tests\n\n* most tests passing; I think the remaining ones are spacy model changes\n\n* hard-code POS tag test\n\n* last test, I think\n\n* black\n\n* updated black\n\n* flake8\n\n* mypy\n\n* black again\n\n* fix training configs\n\n* remove reference to embedder_to_indexer_map\n\n* Other fixes from PR comments\n\n* fix breakage from incorrect merge during rebase\n\n* flake, some docstring formatting\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class BiaffineDependencyParser(Model):",
            "\"\"\"",
            "# Parameters",
            "",
            "-        words : Dict[str, torch.LongTensor], required",
            "+        words : TextFieldTensors, required",
            "The output of ``TextField.as_array()``, which should typically be passed directly to a",
            "``TextFieldEmbedder``. This output is a dictionary mapping keys to ``TokenIndexer``",
            "tensors.  At its most basic, using a ``SingleIdTokenIndexer`` this is : ``{\"tokens\":"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 7088,
        "neg_line": [
            "-words : Dict[str, torch.LongTensor], required"
        ],
        "pos_line": [
            "+words : TextFieldTensors, required"
        ],
        "core_change": "-words : Dict[str, torch.LongTensor], required +words : TextFieldTensors, required",
        "core_API": "as_array"
    },
    {
        "commit_hash": "1fdabd5370e67e1fc7f1c39cf3990503d89f9d23",
        "index": "5cc9347b..ca63f9f0 100644",
        "commit_message": "Add yesqa in the pre-commit (#1246)\n\n* Add yesqa in the pre-commit\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Apply suggestions from code review\n\nCo-authored-by: Christian Clauss <cclauss@me.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Christian Clauss <cclauss@me.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SIFTDescriptor(nn.Module):",
            "",
            "ang_bins = []",
            "for i in range(0, self.num_ang_bins):",
            "-            out = self.pk((bo0_big == i).to(input.dtype) * wo0_big + (bo1_big == i).to(input.dtype) * wo1_big)  # noqa",
            "+            out = self.pk((bo0_big == i).to(input.dtype) * wo0_big + (bo1_big == i).to(input.dtype) * wo1_big)",
            "ang_bins.append(out)",
            "ang_bins = torch.cat(ang_bins, dim=1)",
            "ang_bins = ang_bins.view(B, -1)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 7090,
        "neg_line": [
            "-out = self.pk((bo0_big == i).to(input.dtype) * wo0_big + (bo1_big == i).to(input.dtype) * wo1_big)  # noqa"
        ],
        "pos_line": [
            "+out = self.pk((bo0_big == i).to(input.dtype) * wo0_big + (bo1_big == i).to(input.dtype) * wo1_big)"
        ],
        "core_change": "-out = self.pk((bo0_big == i).to(input.dtype) * wo0_big + (bo1_big == i).to(input.dtype) * wo1_big)  # noqa +out = self.pk((bo0_big == i).to(input.dtype) * wo0_big + (bo1_big == i).to(input.dtype) * wo1_big)",
        "core_API": "pk"
    },
    {
        "commit_hash": "90e5b05a2d2ff3e1f59328bc284aeff5d4abe951",
        "index": "bddf248..4aa3638 100644",
        "commit_message": "Fix end-of-epoch with record_stream\n\n",
        "file": "apex.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class data_prefetcher():",
            "torch.cuda.current_stream().wait_stream(self.stream)",
            "input = self.next_input",
            "target = self.next_target",
            "-        input.record_stream(torch.cuda.current_stream())",
            "-        target.record_stream(torch.cuda.current_stream())",
            "+        if input is not None:",
            "+            input.record_stream(torch.cuda.current_stream())",
            "+        if target is not None:",
            "+            target.record_stream(torch.cuda.current_stream())",
            "self.preload()",
            "return input, target"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=4, insert_id=53135)",
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=5, insert_id=53136)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=53137)",
            "Insert(target_node=IN(type=if_statement), node=('comparison_operator', None), position=1, insert_id=53138)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=53139)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=53140)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=53141)",
            "Insert(target_node=IN(type=if_statement), node=('comparison_operator', None), position=1, insert_id=53142)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=53143)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=53144)",
            "Insert(target_node=IN(type=comparison_operator), node=('identifier', 'input'), position=0, insert_id=53145)",
            "Insert(target_node=IN(type=comparison_operator), node=('is not', 'is'), position=1, insert_id=53146)",
            "Insert(target_node=IN(type=comparison_operator), node=('is not', 'not'), position=2, insert_id=53147)",
            "Insert(target_node=IN(type=comparison_operator), node=('none', 'None'), position=3, insert_id=53148)",
            "Move(target_node=IN(type=block), node=ASTNode(type=expression_statement), position=0)",
            "Insert(target_node=IN(type=comparison_operator), node=('identifier', 'target'), position=0, insert_id=53149)",
            "Insert(target_node=IN(type=comparison_operator), node=('is not', 'is'), position=1, insert_id=53150)",
            "Insert(target_node=IN(type=comparison_operator), node=('is not', 'not'), position=2, insert_id=53151)",
            "Insert(target_node=IN(type=comparison_operator), node=('none', 'None'), position=3, insert_id=53152)",
            "Move(target_node=IN(type=block), node=ASTNode(type=expression_statement), position=0)"
        ],
        "plus_line": 4,
        "minus_line": 2,
        "AST_diff_line": 20,
        "number": 7092,
        "neg_line": [
            "-input.record_stream(torch.cuda.current_stream())",
            "-target.record_stream(torch.cuda.current_stream())"
        ],
        "pos_line": [
            "+if input is not None:",
            "+input.record_stream(torch.cuda.current_stream())",
            "+if target is not None:",
            "+target.record_stream(torch.cuda.current_stream())"
        ],
        "core_change": "-input.record_stream(torch.cuda.current_stream()) -target.record_stream(torch.cuda.current_stream()) +if input is not None: +input.record_stream(torch.cuda.current_stream()) +if target is not None: +target.record_stream(torch.cuda.current_stream())",
        "core_API": "current_stream"
    },
    {
        "commit_hash": "7650dcd7ae6f289dd1d27db18c600470432826a6",
        "index": "7dc7ef5b..90d4e3af 100644",
        "commit_message": "fix lint tests for test files\n\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Tester(unittest.TestCase):",
            "self.assertAlmostEqual(error.item(), 0.0, places=4)",
            "",
            "# functional",
            "-        self.assertTrue(torch.allclose(points_dst,",
            "-            tgm.TransformPoints()(dst_homo_src, points_src)))",
            "+        self.assertTrue(",
            "+            torch.allclose(",
            "+                points_dst,",
            "+                tgm.TransformPoints()(",
            "+                    dst_homo_src,",
            "+                    points_src)))",
            "",
            "def test_transform_points_gradcheck(self):",
            "# generate input data"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 6,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 7093,
        "neg_line": [
            "-self.assertTrue(torch.allclose(points_dst,",
            "-tgm.TransformPoints()(dst_homo_src, points_src)))"
        ],
        "pos_line": [
            "+self.assertTrue(",
            "+torch.allclose(",
            "+points_dst,",
            "+tgm.TransformPoints()(",
            "+dst_homo_src,",
            "+points_src)))"
        ],
        "core_change": "-self.assertTrue(torch.allclose(points_dst, -tgm.TransformPoints()(dst_homo_src, points_src))) +self.assertTrue( +torch.allclose( +points_dst, +tgm.TransformPoints()( +dst_homo_src, +points_src)))",
        "core_API": "assertAlmostEqual"
    },
    {
        "commit_hash": "a5658a5d588d261fc286780aaba9ca493c86cc09",
        "index": "090dcee0..9e30a175 100644",
        "commit_message": "bugfixes\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class Net(nn.Module):",
            "x = F.elu(self.conv3(adjs[1], x))",
            "x = F.elu(self.conv4(adjs[1], x))",
            "x = self.pool(x)",
            "-        x = x.contiguous().view(-1, num_first_fc * 64)",
            "+        x = x.contiguous().view(-1, num_first_fc * 16)",
            "x = F.elu(self.fc1(x))",
            "x = F.elu(self.fc2(x))",
            "x = F.dropout(x, training=self.training)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=integer, text=64), value='16')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 7094,
        "neg_line": [
            "-x = x.contiguous().view(-1, num_first_fc * 64)"
        ],
        "pos_line": [
            "+x = x.contiguous().view(-1, num_first_fc * 16)"
        ],
        "core_change": "-x = x.contiguous().view(-1, num_first_fc * 64) +x = x.contiguous().view(-1, num_first_fc * 16)",
        "core_API": "elu"
    },
    {
        "commit_hash": "dc82068a6dc5e934f2c14741564869dfc5d003e3",
        "index": "b7ffe0671..222ae0c3d 100644",
        "commit_message": "FIX E523,E541,E741\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "cuda",
        "change": [
            "class WarpCTC(chainer.Chain):",
            "# get ctc loss",
            "from chainer_ctc.warpctc import ctc as warp_ctc",
            "",
            "-        self.loss = warp_ctc(y_hat, ilens, [cuda.to_cpu(l.data) for l in ys])[0]",
            "+        self.loss = warp_ctc(y_hat, ilens, [cuda.to_cpu(y.data) for y in ys])[0]",
            "logging.info(\"ctc loss:\" + str(self.loss.data))",
            "",
            "return self.loss"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=l), value='y')",
            "Update(target_node=ASTNode(type=identifier, text=l), value='y')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 7096,
        "neg_line": [
            "-self.loss = warp_ctc(y_hat, ilens, [cuda.to_cpu(l.data) for l in ys])[0]"
        ],
        "pos_line": [
            "+self.loss = warp_ctc(y_hat, ilens, [cuda.to_cpu(y.data) for y in ys])[0]"
        ],
        "core_change": "-self.loss = warp_ctc(y_hat, ilens, [cuda.to_cpu(l.data) for l in ys])[0] +self.loss = warp_ctc(y_hat, ilens, [cuda.to_cpu(y.data) for y in ys])[0]",
        "core_API": "to_cpu"
    },
    {
        "commit_hash": "fdc907675050cb9e3aa887bf89d6c92519178349",
        "index": "83bc8737..48add8a9 100755",
        "commit_message": "fix use of deprecated TF functions.\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Model(ModelDesc):",
            "",
            "log_pi_a_given_s = tf.reduce_sum(",
            "log_probs * tf.one_hot(action, NUM_ACTIONS), 1)",
            "-        advantage = tf.sub(tf.stop_gradient(self.value), futurereward, name='advantage')",
            "+        advantage = tf.subtract(tf.stop_gradient(self.value), futurereward, name='advantage')",
            "policy_loss = tf.reduce_sum(log_pi_a_given_s * advantage, name='policy_loss')",
            "xentropy_loss = tf.reduce_sum(",
            "self.logits * log_probs, name='xentropy_loss')"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=sub), value='subtract')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 7102,
        "neg_line": [
            "-advantage = tf.sub(tf.stop_gradient(self.value), futurereward, name='advantage')"
        ],
        "pos_line": [
            "+advantage = tf.subtract(tf.stop_gradient(self.value), futurereward, name='advantage')"
        ],
        "core_change": "-advantage = tf.sub(tf.stop_gradient(self.value), futurereward, name='advantage') +advantage = tf.subtract(tf.stop_gradient(self.value), futurereward, name='advantage')",
        "core_API": "reduce_sum"
    },
    {
        "commit_hash": "f11a8a62d27c2740af5df940973d231fd5fcb038",
        "index": "1d0da2a..ed21c06 100644",
        "commit_message": "generator seed fix for DDP mAP drop (#9545)\n\n* Try to fix DDP mAP drop by setting generator's seed to RANK\n\n* Fix default activation bug\n\n* Update dataloaders.py\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Update dataloaders.py\n\nCo-authored-by: Glenn Jocher <glenn.jocher@ultralytics.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "yolov5.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "def parse_model(d, ch):  # model_dict, input_channels(3)",
            "LOGGER.info(f\"\\n{'':>3}{'from':>18}{'n':>3}{'params':>10}  {'module':<40}{'arguments':<30}\")",
            "anchors, nc, gd, gw, act = d['anchors'], d['nc'], d['depth_multiple'], d['width_multiple'], d.get('activation')",
            "if act:",
            "-        Conv.act = eval(act)  # redefine default activation, i.e. Conv.act = nn.SiLU()",
            "+        Conv.default_act = eval(act)  # redefine default activation, i.e. Conv.default_act = nn.SiLU()",
            "LOGGER.info(f\"{colorstr('activation:')} {act}\")  # print",
            "na = (len(anchors[0]) // 2) if isinstance(anchors, list) else anchors  # number of anchors",
            "no = na * (nc + 5)  # number of outputs = anchors * (classes + 5)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=act), value='default_act')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 7106,
        "neg_line": [
            "-Conv.act = eval(act)  # redefine default activation, i.e. Conv.act = nn.SiLU()"
        ],
        "pos_line": [
            "+Conv.default_act = eval(act)  # redefine default activation, i.e. Conv.default_act = nn.SiLU()"
        ],
        "core_change": "-Conv.act = eval(act)  # redefine default activation, i.e. Conv.act = nn.SiLU() +Conv.default_act = eval(act)  # redefine default activation, i.e. Conv.default_act = nn.SiLU()",
        "core_API": "info"
    },
    {
        "commit_hash": "aa62fe36dfc25bada8bc937c950b0bd1936ccbf6",
        "index": "95092c5cb..caafa9a3c 100644",
        "commit_message": "add testing PT 1.12 (#13386)\n\n* add testing PT 1.12\n* Fix quantization tests\n* Fix another set of tests\n* Fix check since https://github.com/pytorch/pytorch/pull/80139 is only going to be available for 1.13\n* Skip this test for now for 1.12\n\nCo-authored-by: SeanNaren <sean@grid.ai>\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_unresolvable_import_paths():",
            "with mock.patch(\"sys.argv\", [\"any.py\", \"--print_config\"]), redirect_stdout(out), pytest.raises(SystemExit):",
            "LightningCLI(TestModel, run=False)",
            "",
            "-    assert \"a_func: torch.softmax\" in out.getvalue()",
            "+    assert \"a_func: torch.nn.Softmax\" in out.getvalue()",
            "",
            "",
            "def test_pytorch_profiler_init_args():"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"a_func: torch.softmax\"), value='\"a_func: torch.nn.Softmax\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 7107,
        "neg_line": [
            "-assert \"a_func: torch.softmax\" in out.getvalue()"
        ],
        "pos_line": [
            "+assert \"a_func: torch.nn.Softmax\" in out.getvalue()"
        ],
        "core_change": "-assert \"a_func: torch.softmax\" in out.getvalue() +assert \"a_func: torch.nn.Softmax\" in out.getvalue()",
        "core_API": "patch"
    },
    {
        "commit_hash": "d1cf5e599fb2f9b7fec1bf6aaa4d7a0ded0f2671",
        "index": "abb3047c..2dae8315 100644",
        "commit_message": "bug fix for retinanet with 2 classes (fg/bg)\n\n",
        "file": "mmdetection.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def anchor_target_single(flat_anchors,",
            "",
            "",
            "def expand_binary_labels(labels, label_weights, label_channels):",
            "-    bin_labels = labels.new_full(",
            "-        (labels.size(0), label_channels), 0, dtype=torch.float32)",
            "+    bin_labels = labels.new_full((labels.size(0), label_channels), 0)",
            "inds = torch.nonzero(labels >= 1).squeeze()",
            "if inds.numel() > 0:",
            "bin_labels[inds, labels[inds] - 1] = 1"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=dtype))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=float32))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 8,
        "number": 7108,
        "neg_line": [
            "-bin_labels = labels.new_full(",
            "-(labels.size(0), label_channels), 0, dtype=torch.float32)"
        ],
        "pos_line": [
            "+bin_labels = labels.new_full((labels.size(0), label_channels), 0)"
        ],
        "core_change": "-bin_labels = labels.new_full( -(labels.size(0), label_channels), 0, dtype=torch.float32) +bin_labels = labels.new_full((labels.size(0), label_channels), 0)",
        "core_API": "new_full"
    },
    {
        "commit_hash": "0655e4a2bd89ccc20f4f1157f65b3e5a61f140e1",
        "index": "f76b1a9a..95dce24f 100755",
        "commit_message": "fixed lstm, added gru, other fixes\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFOptimizer(Optimizer):",
            "arguments: Dict of arguments for passing to fn_loss as **kwargs.",
            "fn_loss: A callable taking arguments as kwargs and returning the loss op.",
            "\"\"\"",
            "-        loss = fn_loss(**arguments)",
            "+        # Trivial operation to enforce control dependency",
            "+        previous_variables = [util.identity_operation(x=variable) for variable in variables]",
            "",
            "# Force loss value to be calculated.",
            "-        with tf.control_dependencies(control_inputs=(loss,)):",
            "-            # Trivial operation to enforce control dependency",
            "-            previous_variables = [util.identity_operation(x=variable) for variable in variables]",
            "+        with tf.control_dependencies(control_inputs=previous_variables):",
            "+            loss = fn_loss(**arguments)",
            "",
            "# The actual tensorflow minimize op.",
            "-        with tf.control_dependencies(control_inputs=previous_variables):",
            "+        with tf.control_dependencies(control_inputs=(loss,)):",
            "# colocate_gradients_with_ops=True",
            "applied = self.optimizer.minimize(loss=loss, var_list=variables)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=with_statement), node=ASTNode(type=module), position=2)",
            "Move(target_node=ASTNode(type=with_statement), node=ASTNode(type=block), position=3)",
            "Move(target_node=ASTNode(type=with_statement), node=ASTNode(type=block, text=), position=3)",
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=list_comprehension), position=5)",
            "Update(target_node=ASTNode(type=identifier, text=previous_variables), value='loss')",
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=call), position=2)",
            "Update(target_node=ASTNode(type=identifier, text=loss), value='previous_variables')"
        ],
        "plus_line": 5,
        "minus_line": 5,
        "AST_diff_line": 7,
        "number": 7109,
        "neg_line": [
            "-loss = fn_loss(**arguments)",
            "-with tf.control_dependencies(control_inputs=(loss,)):",
            "-# Trivial operation to enforce control dependency",
            "-previous_variables = [util.identity_operation(x=variable) for variable in variables]",
            "-with tf.control_dependencies(control_inputs=previous_variables):"
        ],
        "pos_line": [
            "+# Trivial operation to enforce control dependency",
            "+previous_variables = [util.identity_operation(x=variable) for variable in variables]",
            "+with tf.control_dependencies(control_inputs=previous_variables):",
            "+loss = fn_loss(**arguments)",
            "+with tf.control_dependencies(control_inputs=(loss,)):"
        ],
        "core_change": "-loss = fn_loss(**arguments) +# Trivial operation to enforce control dependency +previous_variables = [util.identity_operation(x=variable) for variable in variables] -with tf.control_dependencies(control_inputs=(loss,)): -# Trivial operation to enforce control dependency -previous_variables = [util.identity_operation(x=variable) for variable in variables] +with tf.control_dependencies(control_inputs=previous_variables): +loss = fn_loss(**arguments) -with tf.control_dependencies(control_inputs=previous_variables): +with tf.control_dependencies(control_inputs=(loss,)):",
        "core_API": "identity_operation"
    },
    {
        "commit_hash": "d9680f61c81d70de0687e38dd52a0338c2a75931",
        "index": "51185884..109e49c2 100644",
        "commit_message": "fix empty tensor and create test\n\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Cropping1D(Layer):",
            "return tf.TensorShape([input_shape[0], length, input_shape[2]])",
            "",
            "def call(self, inputs):",
            "-    if sum(self.cropping) >= inputs.shape[1]:",
            "+    if tf.not_equal(tf.size(inputs), 0) and sum(self.cropping) >= inputs.shape[1]:",
            "raise ValueError(",
            "'cropping parameter of Cropping layer is too high,' +",
            "'the result of crop' + str(inputs.shape) + ' with cropping ' +"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=if_statement), node=('boolean_operator', None), position=1, insert_id=2082579)",
            "Insert(target_node=IN(type=boolean_operator), node=('call', None), position=0, insert_id=2082580)",
            "Insert(target_node=IN(type=boolean_operator), node=('and', 'and'), position=1, insert_id=2082581)",
            "Move(target_node=IN(type=boolean_operator), node=ASTNode(type=comparison_operator), position=2)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2082582)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2082583)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2082584)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2082585)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'not_equal'), position=2, insert_id=2082586)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2082587)",
            "Insert(target_node=IN(type=argument_list), node=('call', None), position=1, insert_id=2082588)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=2082589)",
            "Insert(target_node=IN(type=argument_list), node=('integer', '0'), position=3, insert_id=2082590)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=2082591)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2082592)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2082593)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2082594)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2082595)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'size'), position=2, insert_id=2082596)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2082597)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'inputs'), position=1, insert_id=2082598)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2082599)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 22,
        "number": 7120,
        "neg_line": [
            "-if sum(self.cropping) >= inputs.shape[1]:"
        ],
        "pos_line": [
            "+if tf.not_equal(tf.size(inputs), 0) and sum(self.cropping) >= inputs.shape[1]:"
        ],
        "core_change": "-if sum(self.cropping) >= inputs.shape[1]: +if tf.not_equal(tf.size(inputs), 0) and sum(self.cropping) >= inputs.shape[1]:",
        "core_API": "TensorShape"
    },
    {
        "commit_hash": "37e98945728f9961fd33d598a954358d79988c28",
        "index": "6fa8ec68..51b5f381 100644",
        "commit_message": "fix flake8 style in tensorpack/\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class SessionUpdate(object):",
            "self.sess = sess",
            "self.assign_ops = defaultdict(list)",
            "for v in vars_to_update:",
            "-            #p = tf.placeholder(v.dtype, shape=v.get_shape())",
            "+            # p = tf.placeholder(v.dtype, shape=v.get_shape())",
            "with tf.device('/cpu:0'):",
            "p = tf.placeholder(v.dtype)",
            "savename = get_savename_from_varname(v.name)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 7121,
        "neg_line": [
            "-#p = tf.placeholder(v.dtype, shape=v.get_shape())"
        ],
        "pos_line": [
            "+# p = tf.placeholder(v.dtype, shape=v.get_shape())"
        ],
        "core_change": "-#p = tf.placeholder(v.dtype, shape=v.get_shape()) +# p = tf.placeholder(v.dtype, shape=v.get_shape())",
        "core_API": "placeholder"
    },
    {
        "commit_hash": "093e901c979c0f8b0af92f79010b53db1ddcc972",
        "index": "6eb847d..be7f9de 100644",
        "commit_message": "fix deprecation warnings; remove pydoop dependency; update README\n\n",
        "file": "TensorFlowOnSpark.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def main_fun(argv, ctx):",
            "# Print the summaries to screen.",
            "for name, value in names_to_values.iteritems():",
            "summary_name = 'eval/%s' % name",
            "-      op = tf.scalar_summary(summary_name, value, collections=[])",
            "+      op = tf.summary.scalar(summary_name, value, collections=[])",
            "op = tf.Print(op, [value], summary_name)",
            "tf.add_to_collection(tf.GraphKeys.SUMMARIES, op)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=2213709)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=2213710)",
            "Insert(target_node=ASTNode(type=attribute), node=('identifier', 'scalar'), position=2, insert_id=2213711)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=tf), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=scalar_summary), value='summary')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=scalar_summary), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 7128,
        "neg_line": [
            "-op = tf.scalar_summary(summary_name, value, collections=[])"
        ],
        "pos_line": [
            "+op = tf.summary.scalar(summary_name, value, collections=[])"
        ],
        "core_change": "-op = tf.scalar_summary(summary_name, value, collections=[]) +op = tf.summary.scalar(summary_name, value, collections=[])",
        "core_API": "iteritems"
    },
    {
        "commit_hash": "b7766fc10e3b47db21860bed554613968f9434d4",
        "index": "345328f2..632e2128 100644",
        "commit_message": "fix swig name. add some df\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def get_predict_func(config):",
            "",
            "# check output_var_names against output_vars",
            "if output_var_names is not None:",
            "-        output_vars = [tf.get_default_graph().get_tensor_by_name(n) for n in output_var_names]",
            "+        output_vars = [tf.get_default_graph().get_tensor_by_name(get_op_var_name(n)[1])",
            "+                       for n in output_var_names]",
            "else:",
            "output_vars = []"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('(', '('), position=0, insert_id=2317224)",
            "Insert(target_node=ASTNode(type=argument_list), node=('subscript', None), position=1, insert_id=2317225)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=2, insert_id=2317226)",
            "Insert(target_node=IN(type=subscript), node=('call', None), position=0, insert_id=2317227)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=2317228)",
            "Insert(target_node=IN(type=subscript), node=('integer', '1'), position=2, insert_id=2317229)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=2317230)",
            "Insert(target_node=IN(type=call), node=('identifier', 'get_op_var_name'), position=0, insert_id=2317231)",
            "Move(target_node=IN(type=call), node=ASTNode(type=argument_list), position=1)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 7129,
        "neg_line": [
            "-output_vars = [tf.get_default_graph().get_tensor_by_name(n) for n in output_var_names]"
        ],
        "pos_line": [
            "+output_vars = [tf.get_default_graph().get_tensor_by_name(get_op_var_name(n)[1])",
            "+for n in output_var_names]"
        ],
        "core_change": "-output_vars = [tf.get_default_graph().get_tensor_by_name(n) for n in output_var_names] +output_vars = [tf.get_default_graph().get_tensor_by_name(get_op_var_name(n)[1]) +for n in output_var_names]",
        "core_API": "get_default_graph"
    },
    {
        "commit_hash": "8e76796fd0535d8fbc9934e7f6604a1abcfb119e",
        "index": "edd3470687..3558d73b39 100644",
        "commit_message": "ci: Redo `format.sh --all` script & backfill lint fixes (#9956)\n\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class EpsilonGreedy(Exploration):",
            "tf.random.categorical(random_valid_action_logits, 1), axis=1)",
            "",
            "chose_random = tf.random.uniform(",
            "-            tf.stack([batch_size]),",
            "-            minval=0, maxval=1, dtype=tf.float32) < epsilon",
            "+            tf.stack([batch_size]), minval=0, maxval=1,",
            "+            dtype=tf.float32) < epsilon",
            "",
            "action = tf.cond(",
            "pred=tf.constant(explore, dtype=tf.bool)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 7130,
        "neg_line": [
            "-tf.stack([batch_size]),",
            "-minval=0, maxval=1, dtype=tf.float32) < epsilon"
        ],
        "pos_line": [
            "+tf.stack([batch_size]), minval=0, maxval=1,",
            "+dtype=tf.float32) < epsilon"
        ],
        "core_change": "-tf.stack([batch_size]), -minval=0, maxval=1, dtype=tf.float32) < epsilon +tf.stack([batch_size]), minval=0, maxval=1, +dtype=tf.float32) < epsilon",
        "core_API": "categorical"
    },
    {
        "commit_hash": "a5c2d9eb89014712aab7dc89f2f020af1be9f89c",
        "index": "488944e2..3a084f5a 100644",
        "commit_message": "Catch nan loss in training loop (#2029)\n\n* catch nan loss\n\n* small fix\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Trainer(Registrable):",
            "self.optimizer.zero_grad()",
            "",
            "loss = self.batch_loss(batch, for_training=True)",
            "+            if torch.isnan(loss):",
            "+                raise ValueError(\"nan loss encountered\")",
            "+",
            "loss.backward()",
            "",
            "train_loss += loss.item()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=3, insert_id=32397)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=32398)",
            "Insert(target_node=IN(type=if_statement), node=('call', None), position=1, insert_id=32399)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=32400)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=32401)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=32402)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=32403)",
            "Insert(target_node=IN(type=block), node=('raise_statement', None), position=0, insert_id=32404)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=32405)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=32406)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'isnan'), position=2, insert_id=32407)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=32408)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'loss'), position=1, insert_id=32409)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=32410)",
            "Insert(target_node=IN(type=raise_statement), node=('raise', 'raise'), position=0, insert_id=32411)",
            "Insert(target_node=IN(type=raise_statement), node=('call', None), position=1, insert_id=32412)",
            "Insert(target_node=IN(type=call), node=('identifier', 'ValueError'), position=0, insert_id=32413)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=32414)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=32415)",
            "Insert(target_node=IN(type=argument_list), node=('string', '\"nan loss encountered\"'), position=1, insert_id=32416)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=32417)"
        ],
        "plus_line": 2,
        "minus_line": 0,
        "AST_diff_line": 21,
        "number": 7132,
        "neg_line": [],
        "pos_line": [
            "+if torch.isnan(loss):",
            "+raise ValueError(\"nan loss encountered\")",
            "+"
        ],
        "core_change": "+if torch.isnan(loss): +raise ValueError(\"nan loss encountered\") +",
        "core_API": "zero_grad"
    },
    {
        "commit_hash": "1ed2ebf60d87ef12bd063c7c58e484e19189c754",
        "index": "dad2d1cec..120a48c09 100755",
        "commit_message": "[style] consistent nn. and nn.functional (#12124)\n\n* consistent nn. and nn.functional\n\n* fix glitch\n\n* fix glitch #2\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class BertGenerationEmbeddings(nn.Module):",
            "self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)",
            "# self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load",
            "# any TensorFlow checkpoint file",
            "-        self.LayerNorm = torch.nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)",
            "+        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)",
            "self.dropout = nn.Dropout(config.hidden_dropout_prob)",
            "",
            "# position_ids (1, len position emb) is contiguous in memory and exported when serialized"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 7137,
        "neg_line": [
            "-self.LayerNorm = torch.nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)"
        ],
        "pos_line": [
            "+self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)"
        ],
        "core_change": "-self.LayerNorm = torch.nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps) +self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)",
        "core_API": "Embedding"
    },
    {
        "commit_hash": "43043ee4d5c672f7fbc22ac9559e8164731fd053",
        "index": "ed79b6444e..46f6a86ff7 100644",
        "commit_message": "[RLlib] Tf2x preparation; part 2 (upgrading `try_import_tf()`). (#9136)\n\n* WIP.\n\n* Fixes.\n\n* LINT.\n\n* WIP.\n\n* WIP.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* WIP.\n\n* Fixes.\n\n* Test\n\n* Fix.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* LINT.\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class WorkerSet:",
            "def session_creator():",
            "logger.debug(\"Creating TF session {}\".format(",
            "config[\"tf_session_args\"]))",
            "-            return tf.Session(",
            "-                config=tf.ConfigProto(**config[\"tf_session_args\"]))",
            "+            return tf1.Session(",
            "+                config=tf1.ConfigProto(**config[\"tf_session_args\"]))",
            "",
            "if isinstance(config[\"input\"], FunctionType):",
            "input_creator = config[\"input\"]"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tf), value='tf1')",
            "Update(target_node=ASTNode(type=identifier, text=tf), value='tf1')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 7142,
        "neg_line": [
            "-return tf.Session(",
            "-config=tf.ConfigProto(**config[\"tf_session_args\"]))"
        ],
        "pos_line": [
            "+return tf1.Session(",
            "+config=tf1.ConfigProto(**config[\"tf_session_args\"]))"
        ],
        "core_change": "-return tf.Session( -config=tf.ConfigProto(**config[\"tf_session_args\"])) +return tf1.Session( +config=tf1.ConfigProto(**config[\"tf_session_args\"]))",
        "core_API": "debug"
    },
    {
        "commit_hash": "a7e5a5f7b4e70c1df705c0ece4f046e44fc22525",
        "index": "44e72413..ffd5ed2c 100644",
        "commit_message": "Fix and enable some tfcoreml converter convent tests\n\n",
        "file": "coremltools.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def constant_propagation(nnssa):",
            "if len(constant_nodes) > 0:",
            "with tf.Graph().as_default() as graph:",
            "tf.import_graph_def(new_graph, name=\"\")",
            "-                with tf.Session(graph=graph) as sess:",
            "+                with tf.compat.v1.Session(graph=graph) as sess:",
            "query_list = []",
            "for c in constant_nodes:",
            "for j in range(constant_node_num_outputs[c]):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=1916078)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=1916079)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1916080)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1916081)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'v1'), position=2, insert_id=1916082)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=tf), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'compat'), position=2, insert_id=1916083)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 7143,
        "neg_line": [
            "-with tf.Session(graph=graph) as sess:"
        ],
        "pos_line": [
            "+with tf.compat.v1.Session(graph=graph) as sess:"
        ],
        "core_change": "-with tf.Session(graph=graph) as sess: +with tf.compat.v1.Session(graph=graph) as sess:",
        "core_API": "Graph"
    },
    {
        "commit_hash": "03f60ac33fff015ce061ad927a228ab7a8c08f36",
        "index": "c814fceac..10f38a90e 100644",
        "commit_message": "Fix broken keras image logging with tf1 (#513)\n\n\n",
        "file": "wandb.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "keras",
        "change": [
            "def patch_tf_keras():",
            "from tensorflow.python.keras.engine import training_arrays",
            "from tensorflow.python.keras.engine import training_generator",
            "",
            "-    training_v2 = wandb.util.import_module('tensorflow.python.keras.engine.training_v2')",
            "+    training_v2 = wandb.util.get_module('tensorflow.python.keras.engine.training_v2')",
            "old_arrays = training_arrays.fit_loop",
            "old_generator = training_generator.fit_generator",
            "if training_v2:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=import_module), value='get_module')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 7145,
        "neg_line": [
            "-training_v2 = wandb.util.import_module('tensorflow.python.keras.engine.training_v2')"
        ],
        "pos_line": [
            "+training_v2 = wandb.util.get_module('tensorflow.python.keras.engine.training_v2')"
        ],
        "core_change": "-training_v2 = wandb.util.import_module('tensorflow.python.keras.engine.training_v2') +training_v2 = wandb.util.get_module('tensorflow.python.keras.engine.training_v2')",
        "core_API": "import_module"
    },
    {
        "commit_hash": "8d2fc705320b02a63ae1bc83a4aa7ae1363e3cc8",
        "index": "04116375..a88cace7 100644",
        "commit_message": "fixed f1 score\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SignedGCN(torch.nn.Module):",
            "with torch.no_grad():",
            "pos_p = self.discriminate(z, pos_edge_index)[:, :2].max(dim=1)[1]",
            "neg_p = self.discriminate(z, neg_edge_index)[:, :2].max(dim=1)[1]",
            "-        pred = torch.cat([pos_p, neg_p]).cpu()",
            "+        pred = 1 - torch.cat([pos_p, neg_p]).cpu()",
            "y = torch.cat(",
            "-            [pred.new_zeros((pos_p.size(0))),",
            "-             pred.new_ones(neg_p.size(0))])",
            "+            [pred.new_ones((pos_p.size(0))),",
            "+             pred.new_zeros(neg_p.size(0))])",
            "pred, y = pred.numpy(), y.numpy()",
            "",
            "auc = roc_auc_score(y, pred)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('binary_operator', None), position=2, insert_id=1057982)",
            "Insert(target_node=IN(type=binary_operator), node=('integer', '1'), position=0, insert_id=1057983)",
            "Insert(target_node=IN(type=binary_operator), node=('-', '-'), position=1, insert_id=1057984)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=call), position=2)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 6,
        "number": 7148,
        "neg_line": [
            "-pred = torch.cat([pos_p, neg_p]).cpu()",
            "-[pred.new_zeros((pos_p.size(0))),",
            "-pred.new_ones(neg_p.size(0))])"
        ],
        "pos_line": [
            "+pred = 1 - torch.cat([pos_p, neg_p]).cpu()",
            "+[pred.new_ones((pos_p.size(0))),",
            "+pred.new_zeros(neg_p.size(0))])"
        ],
        "core_change": "-pred = torch.cat([pos_p, neg_p]).cpu() +pred = 1 - torch.cat([pos_p, neg_p]).cpu() -[pred.new_zeros((pos_p.size(0))), -pred.new_ones(neg_p.size(0))]) +[pred.new_ones((pos_p.size(0))), +pred.new_zeros(neg_p.size(0))])",
        "core_API": "no_grad"
    },
    {
        "commit_hash": "02741de596405df0876bfff281bb7ec8ff04be37",
        "index": "43dd729d..7e4112b6 100644",
        "commit_message": "Fix CI build by reverting changes in corr2d() (#1042)\n\nSigned-off-by: terrytangyuan <terrytangyuan@gmail.com>\n",
        "file": "d2l-en.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def corr2d(X, K):  #@save",
            "Y = tf.Variable(tf.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1)))",
            "for i in range(Y.shape[0]):",
            "for j in range(Y.shape[1]):",
            "-            Y[i, j].assign(tf.reduce_sum(X[i: i + h, j: j + w] * K))",
            "+            Y[i, j].assign(tf.cast(tf.reduce_sum(",
            "+                X[i: i + h, j: j + w] * K), dtype=tf.float32))",
            "return Y"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1918190)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=1918191)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=1918192)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1918193)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cast'), position=2, insert_id=1918194)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1918195)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=call), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=1918196)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1918197)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=1918198)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=1918199)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1918200)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=1918201)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=1918202)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1918203)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'float32'), position=2, insert_id=1918204)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 16,
        "number": 7154,
        "neg_line": [
            "-Y[i, j].assign(tf.reduce_sum(X[i: i + h, j: j + w] * K))"
        ],
        "pos_line": [
            "+Y[i, j].assign(tf.cast(tf.reduce_sum(",
            "+X[i: i + h, j: j + w] * K), dtype=tf.float32))"
        ],
        "core_change": "-Y[i, j].assign(tf.reduce_sum(X[i: i + h, j: j + w] * K)) +Y[i, j].assign(tf.cast(tf.reduce_sum( +X[i: i + h, j: j + w] * K), dtype=tf.float32))",
        "core_API": "Variable"
    },
    {
        "commit_hash": "96e51546bff004c240c959593f6a8f0f29c0f131",
        "index": "67ff430..aacce68 100644",
        "commit_message": "Fix get layer variables issue (#245)\n\n\n",
        "file": "tflearn.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def get_layer_variables_by_name(name):",
            "A list of Variables.",
            "",
            "\"\"\"",
            "-    return tf.get_collection(tf.GraphKeys.LAYER_VARIABLES + '/' + name + '/')",
            "+    return tf.get_collection(tf.GraphKeys.LAYER_VARIABLES + '/' + name)",
            "",
            "",
            "def get_value(var, session=None):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=+, text=+))",
            "Delete(target_node=ASTNode(type=string, text='/'))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 7155,
        "neg_line": [
            "-return tf.get_collection(tf.GraphKeys.LAYER_VARIABLES + '/' + name + '/')"
        ],
        "pos_line": [
            "+return tf.get_collection(tf.GraphKeys.LAYER_VARIABLES + '/' + name)"
        ],
        "core_change": "-return tf.get_collection(tf.GraphKeys.LAYER_VARIABLES + '/' + name + '/') +return tf.get_collection(tf.GraphKeys.LAYER_VARIABLES + '/' + name)",
        "core_API": "get_collection"
    },
    {
        "commit_hash": "e408e146b50d7350b6714086ead69de03199cfb3",
        "index": "385a1902..07dea7ec 100644",
        "commit_message": "Search space zoo example fix (#2801)\n\n\n",
        "file": "nni.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "layers",
        "change": [
            "if __name__ == \"__main__\":",
            "parser = ArgumentParser(\"enas\")",
            "parser.add_argument(\"--batch-size\", default=128, type=int)",
            "parser.add_argument(\"--log-frequency\", default=10, type=int)",
            "-    # parser.add_argument(\"--search-for\", choices=[\"macro\", \"micro\"], default=\"macro\")",
            "parser.add_argument(\"--epochs\", default=None, type=int, help=\"Number of epochs (default: macro 310, micro 150)\")",
            "parser.add_argument(\"--visualization\", default=False, action=\"store_true\")",
            "args = parser.parse_args()"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 7157,
        "neg_line": [
            "-# parser.add_argument(\"--search-for\", choices=[\"macro\", \"micro\"], default=\"macro\")"
        ],
        "pos_line": [],
        "core_change": "-# parser.add_argument(\"--search-for\", choices=[\"macro\", \"micro\"], default=\"macro\")",
        "core_API": "add_argument"
    },
    {
        "commit_hash": "d4c0895cb98f5aa6f112a6f28533a49ad9d6312a",
        "index": "a1060a51..29af27be 100644",
        "commit_message": "Bump torch, torchvision, python versions (#2663)\n\n* Bump torch, torchvision, python versions\n\n* Fix .nonzero() usage\n\n* Add missing licenses\n\n* Fix more .nonzero() errors\n\n* Fix errors\n\n* Fix pyplot.hist invocation\n\n* Work around torch 1.6 incompatibility with pyplot.hist\n\n* Fix more pyplot.hist() errors\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def count_accuracy(X, true_counts, air, batch_size):",
            "inferred_counts_m = count_vec_to_mat(inferred_counts, 3)",
            "counts += torch.mm(true_counts_m.t(), inferred_counts_m)",
            "error_ind = 1 - (true_counts_batch == inferred_counts)",
            "-        error_ix = error_ind.nonzero().squeeze()",
            "+        error_ix = error_ind.nonzero(as_tuple=False).squeeze()",
            "error_latents.append(latents_to_tensor((z_where, z_pres)).index_select(0, error_ix))",
            "error_indicators.append(error_ind)",
            "",
            "acc = counts.diag().sum().float() / X.size(0)",
            "-    error_indices = torch.cat(error_indicators).nonzero().squeeze()",
            "+    error_indices = torch.cat(error_indicators).nonzero(as_tuple=False).squeeze()",
            "if X.is_cuda:",
            "error_indices = error_indices.cuda()",
            "return acc, counts, torch.cat(error_latents), error_indices"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=690881)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=argument_list), position=1)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=690882)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=690883)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=1, insert_id=690884)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=1, insert_id=690885)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'as_tuple'), position=0, insert_id=690886)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=690887)",
            "Insert(target_node=IN(type=keyword_argument), node=('false', 'False'), position=2, insert_id=690888)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'as_tuple'), position=0, insert_id=690889)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=690890)",
            "Insert(target_node=IN(type=keyword_argument), node=('false', 'False'), position=2, insert_id=690891)",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 15,
        "number": 7159,
        "neg_line": [
            "-error_ix = error_ind.nonzero().squeeze()",
            "-error_indices = torch.cat(error_indicators).nonzero().squeeze()"
        ],
        "pos_line": [
            "+error_ix = error_ind.nonzero(as_tuple=False).squeeze()",
            "+error_indices = torch.cat(error_indicators).nonzero(as_tuple=False).squeeze()"
        ],
        "core_change": "-error_ix = error_ind.nonzero().squeeze() +error_ix = error_ind.nonzero(as_tuple=False).squeeze() -error_indices = torch.cat(error_indicators).nonzero().squeeze() +error_indices = torch.cat(error_indicators).nonzero(as_tuple=False).squeeze()",
        "core_API": "mm"
    },
    {
        "commit_hash": "34263ab196cbfab65f2afa1150e6cf6db5ffd8b6",
        "index": "90c78c295..a9de92165 100755",
        "commit_message": "rnn->encoders, removes _th suffix, same filenames for chainer and pytorch, modify tests to not rely on _th\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def main():",
            "# recog",
            "logging.info('backend = ' + args.backend)",
            "if args.backend == \"chainer\":",
            "-        from espnet.asr.chain.asr_chainer import recog",
            "+        from espnet.asr.chain.asr import recog",
            "recog(args)",
            "elif args.backend == \"pytorch\":",
            "-        from espnet.asr.pytorch.asr_pytorch import recog",
            "+        from espnet.asr.pytorch.asr import recog",
            "recog(args)",
            "else:",
            "raise ValueError(\"Only chainer and pytorch are supported.\")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=import_from_statement), node=ASTNode(type=dotted_name), position=3)",
            "Update(target_node=ASTNode(type=identifier, text=asr_pytorch), value='asr')",
            "Insert(target_node=ASTNode(type=import_from_statement), node=('dotted_name', None), position=3, insert_id=178747)",
            "Update(target_node=ASTNode(type=identifier, text=asr_chainer), value='asr')",
            "Insert(target_node=IN(type=dotted_name), node=('identifier', 'recog'), position=0, insert_id=178748)",
            "Delete(target_node=ASTNode(type=identifier, text=recog))",
            "Delete(target_node=ASTNode(type=dotted_name))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 7,
        "number": 7160,
        "neg_line": [
            "-from espnet.asr.chain.asr_chainer import recog",
            "-from espnet.asr.pytorch.asr_pytorch import recog"
        ],
        "pos_line": [
            "+from espnet.asr.chain.asr import recog",
            "+from espnet.asr.pytorch.asr import recog"
        ],
        "core_change": "-from espnet.asr.chain.asr_chainer import recog +from espnet.asr.chain.asr import recog -from espnet.asr.pytorch.asr_pytorch import recog +from espnet.asr.pytorch.asr import recog",
        "core_API": "info"
    },
    {
        "commit_hash": "c61e22a8e0e393b7d701611437f595656cf16003",
        "index": "b2dacbcf..95ab280d 100644",
        "commit_message": "Fix unit test failures with torch==1.1.0 (#1840)\n\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class EKFState(object):",
            "S = H.mm(P).mm(H.transpose(-1, -2)) + R  # innovation cov",
            "",
            "K_prefix = self._cov.mm(H.transpose(-1, -2))",
            "-        dx = K_prefix.mm(torch.gesv(dz.unsqueeze(1), S)[0]).squeeze(1)  # K*dz",
            "+        dx = K_prefix.mm(torch.solve(dz.unsqueeze(1), S)[0]).squeeze(1)  # K*dz",
            "x = self._dynamic_model.geodesic_difference(x, -dx)",
            "",
            "I = eye_like(x, self._dynamic_model.dimension)  # noqa: E741",
            "-        ImKH = I - K_prefix.mm(torch.gesv(H, S)[0])",
            "+        ImKH = I - K_prefix.mm(torch.solve(H, S)[0])",
            "# *Joseph form* of covariance update for numerical stability.",
            "P = ImKH.mm(self.cov).mm(ImKH.transpose(-1, -2)) \\",
            "-            + K_prefix.mm(torch.gesv((K_prefix.mm(torch.gesv(R, S)[0])).transpose(-1, -2),",
            "+            + K_prefix.mm(torch.solve((K_prefix.mm(torch.solve(R, S)[0])).transpose(-1, -2),",
            "S)[0])",
            "",
            "pred_mean = x"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=720346)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'K_prefix'), position=0, insert_id=720347)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=720348)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'mm'), position=2, insert_id=720349)",
            "Update(target_node=ASTNode(type=identifier, text=gesv), value='solve')",
            "Update(target_node=ASTNode(type=identifier, text=gesv), value='solve')",
            "Update(target_node=ASTNode(type=identifier, text=gesv), value='solve')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=gesv), value='solve')",
            "Delete(target_node=ASTNode(type=identifier, text=K_prefix))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=mm))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 13,
        "number": 7167,
        "neg_line": [
            "-dx = K_prefix.mm(torch.gesv(dz.unsqueeze(1), S)[0]).squeeze(1)  # K*dz",
            "-ImKH = I - K_prefix.mm(torch.gesv(H, S)[0])",
            "-+ K_prefix.mm(torch.gesv((K_prefix.mm(torch.gesv(R, S)[0])).transpose(-1, -2),"
        ],
        "pos_line": [
            "+dx = K_prefix.mm(torch.solve(dz.unsqueeze(1), S)[0]).squeeze(1)  # K*dz",
            "+ImKH = I - K_prefix.mm(torch.solve(H, S)[0])",
            "++ K_prefix.mm(torch.solve((K_prefix.mm(torch.solve(R, S)[0])).transpose(-1, -2),"
        ],
        "core_change": "-dx = K_prefix.mm(torch.gesv(dz.unsqueeze(1), S)[0]).squeeze(1)  # K*dz +dx = K_prefix.mm(torch.solve(dz.unsqueeze(1), S)[0]).squeeze(1)  # K*dz -ImKH = I - K_prefix.mm(torch.gesv(H, S)[0]) +ImKH = I - K_prefix.mm(torch.solve(H, S)[0]) -+ K_prefix.mm(torch.gesv((K_prefix.mm(torch.gesv(R, S)[0])).transpose(-1, -2), ++ K_prefix.mm(torch.solve((K_prefix.mm(torch.solve(R, S)[0])).transpose(-1, -2),",
        "core_API": "mm"
    },
    {
        "commit_hash": "72d7f8c7610fa69cf086cf5e45e2de73db757353",
        "index": "c05ba3b1..65414518 100644",
        "commit_message": "fix typo\n",
        "file": "stable-diffusion-webui.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def split_cross_attention_forward(self, x, context=None, mask=None):",
            "q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h=h), (q_in, k_in, v_in))",
            "del q_in, k_in, v_in",
            "",
            "-    r1 = torch.zeros(q.shape[0], q.shape[1], v.shape[2], device=q.device. dtype=q.dtype)",
            "+    r1 = torch.zeros(q.shape[0], q.shape[1], v.shape[2], device=q.device, dtype=q.dtype)",
            "",
            "stats = torch.cuda.memory_stats(q.device)",
            "mem_active = stats['active_bytes.all.current']"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=7, insert_id=1142454)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=8, insert_id=1142455)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=9, insert_id=1142456)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=identifier, text=device), position=0)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type==, text==), position=1)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=attribute), position=2)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=identifier, text=dtype), position=0)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type==, text==), position=1)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=attribute), position=2)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 13,
        "number": 7168,
        "neg_line": [
            "-r1 = torch.zeros(q.shape[0], q.shape[1], v.shape[2], device=q.device. dtype=q.dtype)"
        ],
        "pos_line": [
            "+r1 = torch.zeros(q.shape[0], q.shape[1], v.shape[2], device=q.device, dtype=q.dtype)"
        ],
        "core_change": "-r1 = torch.zeros(q.shape[0], q.shape[1], v.shape[2], device=q.device. dtype=q.dtype) +r1 = torch.zeros(q.shape[0], q.shape[1], v.shape[2], device=q.device, dtype=q.dtype)",
        "core_API": "zeros"
    },
    {
        "commit_hash": "37e98945728f9961fd33d598a954358d79988c28",
        "index": "2f8eece4..627feaa5 100644",
        "commit_message": "fix flake8 style in tensorpack/\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class FeedfreeInferenceRunner(Callback):",
            "for inf in self.infs:",
            "inf.before_inference()",
            "",
            "-        sess = tf.get_default_session()",
            "sz = self._input_data.size()",
            "with get_tqdm(total=sz) as pbar:",
            "for _ in range(sz):"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=sess))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=get_default_session))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 12,
        "number": 7169,
        "neg_line": [
            "-sess = tf.get_default_session()"
        ],
        "pos_line": [],
        "core_change": "-sess = tf.get_default_session()",
        "core_API": "before_inference"
    },
    {
        "commit_hash": "455c6390938a5c737fa63e78396cedae41e4e87e",
        "index": "4e3791e48..4974f36fa 100644",
        "commit_message": "CDN urls (#4030)\n\n* [file_utils] use_cdn + documentation\n\n* Move to cdn. urls for weights\n\n* [urls] Hotfix for bert-base-japanese\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "models",
        "change": [
            "logger = logging.getLogger(__name__)",
            "# for the pretrained weights provided with the models",
            "####################################################",
            "TF_XXX_PRETRAINED_MODEL_ARCHIVE_MAP = {",
            "-    \"xxx-base-uncased\": \"https://s3.amazonaws.com/models.huggingface.co/bert/xxx-base-uncased-tf_model.h5\",",
            "-    \"xxx-large-uncased\": \"https://s3.amazonaws.com/models.huggingface.co/bert/xxx-large-uncased-tf_model.h5\",",
            "+    \"xxx-base-uncased\": \"https://cdn.huggingface.co/xxx-base-uncased-tf_model.h5\",",
            "+    \"xxx-large-uncased\": \"https://cdn.huggingface.co/xxx-large-uncased-tf_model.h5\",",
            "}"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=pair), node=ASTNode(type=dictionary), position=2)",
            "Insert(target_node=ASTNode(type=dictionary), node=(',', ','), position=2, insert_id=2689813)",
            "Insert(target_node=ASTNode(type=pair), node=(':', ':'), position=1, insert_id=2689814)",
            "Update(target_node=ASTNode(type=string, text=\"https://s3.amazonaws.com/models.huggingface.co/bert/xxx-base-uncased-tf_model.h5\"), value='\"https://cdn.huggingface.co/xxx-base-uncased-tf_model.h5\"')",
            "Move(target_node=ASTNode(type=pair), node=ASTNode(type=:, text=:), position=1)",
            "Update(target_node=ASTNode(type=string, text=\"https://s3.amazonaws.com/models.huggingface.co/bert/xxx-large-uncased-tf_model.h5\"), value='\"https://cdn.huggingface.co/xxx-large-uncased-tf_model.h5\"')",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 8,
        "number": 7171,
        "neg_line": [
            "-\"xxx-base-uncased\": \"https://s3.amazonaws.com/models.huggingface.co/bert/xxx-base-uncased-tf_model.h5\",",
            "-\"xxx-large-uncased\": \"https://s3.amazonaws.com/models.huggingface.co/bert/xxx-large-uncased-tf_model.h5\","
        ],
        "pos_line": [
            "+\"xxx-base-uncased\": \"https://cdn.huggingface.co/xxx-base-uncased-tf_model.h5\",",
            "+\"xxx-large-uncased\": \"https://cdn.huggingface.co/xxx-large-uncased-tf_model.h5\","
        ],
        "core_change": "-\"xxx-base-uncased\": \"https://s3.amazonaws.com/models.huggingface.co/bert/xxx-base-uncased-tf_model.h5\", -\"xxx-large-uncased\": \"https://s3.amazonaws.com/models.huggingface.co/bert/xxx-large-uncased-tf_model.h5\", +\"xxx-base-uncased\": \"https://cdn.huggingface.co/xxx-base-uncased-tf_model.h5\", +\"xxx-large-uncased\": \"https://cdn.huggingface.co/xxx-large-uncased-tf_model.h5\",",
        "core_API": "getLogger"
    },
    {
        "commit_hash": "f11a8a62d27c2740af5df940973d231fd5fcb038",
        "index": "273e73d..2fe99be 100644",
        "commit_message": "generator seed fix for DDP mAP drop (#9545)\n\n* Try to fix DDP mAP drop by setting generator's seed to RANK\n\n* Fix default activation bug\n\n* Update dataloaders.py\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Update dataloaders.py\n\nCo-authored-by: Glenn Jocher <glenn.jocher@ultralytics.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "yolov5.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "def autopad(k, p=None, d=1):  # kernel, padding, dilation",
            "",
            "class Conv(nn.Module):",
            "# Standard convolution with args(ch_in, ch_out, kernel, stride, padding, groups, dilation, activation)",
            "-    act = nn.SiLU()  # default activation",
            "+    default_act = nn.SiLU()  # default activation",
            "",
            "def __init__(self, c1, c2, k=1, s=1, p=None, g=1, d=1, act=True):",
            "super().__init__()",
            "self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p, d), groups=g, dilation=d, bias=False)",
            "self.bn = nn.BatchNorm2d(c2)",
            "-        self.act = self.act if act is True else act if isinstance(act, nn.Module) else nn.Identity()",
            "+        self.act = self.default_act if act is True else act if isinstance(act, nn.Module) else nn.Identity()",
            "",
            "def forward(self, x):",
            "return self.act(self.bn(self.conv(x)))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=act), value='default_act')",
            "Update(target_node=ASTNode(type=identifier, text=act), value='default_act')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 7172,
        "neg_line": [
            "-act = nn.SiLU()  # default activation",
            "-self.act = self.act if act is True else act if isinstance(act, nn.Module) else nn.Identity()"
        ],
        "pos_line": [
            "+default_act = nn.SiLU()  # default activation",
            "+self.act = self.default_act if act is True else act if isinstance(act, nn.Module) else nn.Identity()"
        ],
        "core_change": "-act = nn.SiLU()  # default activation +default_act = nn.SiLU()  # default activation -self.act = self.act if act is True else act if isinstance(act, nn.Module) else nn.Identity() +self.act = self.default_act if act is True else act if isinstance(act, nn.Module) else nn.Identity()",
        "core_API": "SiLU"
    },
    {
        "commit_hash": "93bbf1b87ae9f0c9275b2148ac6e6fdd84bb53e4",
        "index": "a1604b4..122d107 100755",
        "commit_message": "best possible recall fix\n\n",
        "file": "yolov5.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def check_img_size(img_size, s=32):",
            "",
            "def check_best_possible_recall(dataset, anchors, thr):",
            "# Check best possible recall of dataset with current anchors",
            "-    wh = torch.tensor(np.concatenate([l[:, 3:5] * s for s, l in zip(dataset.shapes, dataset.labels)]))  # width-height",
            "-    ratio = wh[:, None] / anchors.view(-1, 2)[None]  # ratio",
            "+    wh = torch.tensor(np.concatenate([l[:, 3:5] * s for s, l in zip(dataset.shapes, dataset.labels)])).float()  # wh",
            "+    ratio = wh[:, None] / anchors.view(-1, 2).cpu()[None]  # ratio",
            "m = torch.max(ratio, 1. / ratio).max(2)[0]  # max ratio",
            "bpr = (m.min(1)[0] < thr).float().mean()  # best possible recall",
            "mr = (m < thr).float().mean()  # match ratio"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('call', None), position=2, insert_id=1304528)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1304529)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1304530)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1304531)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'float'), position=2, insert_id=1304532)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1304533)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1304534)",
            "Insert(target_node=ASTNode(type=subscript), node=('call', None), position=0, insert_id=1304535)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1304536)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1304537)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1304538)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cpu'), position=2, insert_id=1304539)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1304540)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1304541)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 16,
        "number": 7176,
        "neg_line": [
            "-wh = torch.tensor(np.concatenate([l[:, 3:5] * s for s, l in zip(dataset.shapes, dataset.labels)]))  # width-height",
            "-ratio = wh[:, None] / anchors.view(-1, 2)[None]  # ratio"
        ],
        "pos_line": [
            "+wh = torch.tensor(np.concatenate([l[:, 3:5] * s for s, l in zip(dataset.shapes, dataset.labels)])).float()  # wh",
            "+ratio = wh[:, None] / anchors.view(-1, 2).cpu()[None]  # ratio"
        ],
        "core_change": "-wh = torch.tensor(np.concatenate([l[:, 3:5] * s for s, l in zip(dataset.shapes, dataset.labels)]))  # width-height -ratio = wh[:, None] / anchors.view(-1, 2)[None]  # ratio +wh = torch.tensor(np.concatenate([l[:, 3:5] * s for s, l in zip(dataset.shapes, dataset.labels)])).float()  # wh +ratio = wh[:, None] / anchors.view(-1, 2).cpu()[None]  # ratio",
        "core_API": "tensor"
    },
    {
        "commit_hash": "e6a107c14eec6dde40bc3c73c4e2b54dae3996df",
        "index": "ac9b27e8c..550642a54 100644",
        "commit_message": "Generalize TorchHook to FrameworkHook (#2561)\n\n* create FrameworkHook and move generic hooking functionality over\n\n* PEP-ify import statements all around\n\n* PR comment - fix docstring\n\n",
        "file": "PySyft.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_spinup_time(hook):",
            "spun up inside web frameworks are created quickly enough to not cause timeout errors\"\"\"",
            "data = []",
            "for i in range(10000):",
            "-        data.append(th.Tensor(5, 5).random_(100))",
            "+        data.append(torch.Tensor(5, 5).random_(100))",
            "start_time = time()",
            "dummy = sy.VirtualWorker(hook, id=\"dummy\", data=data)",
            "end_time = time()"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 7178,
        "neg_line": [
            "-data.append(th.Tensor(5, 5).random_(100))"
        ],
        "pos_line": [
            "+data.append(torch.Tensor(5, 5).random_(100))"
        ],
        "core_change": "-data.append(th.Tensor(5, 5).random_(100)) +data.append(torch.Tensor(5, 5).random_(100))",
        "core_API": "append"
    },
    {
        "commit_hash": "17dceae7a1de5577cd0c07a97dcd5821a08af07c",
        "index": "faa0725b5..18064cc5d 100644",
        "commit_message": "Fix circle ci flaky fail of wmt example  (#3485)\n\n* force bleu\n\n* fix wrong file name\n\n* rename file\n\n* different filenames for each example test\n\n* test files should clean up after themselves\n\n* test files should clean up after themselves\n\n* do not force bleu\n\n* correct typo\n\n* fix isort\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class TestBartExamples(unittest.TestCase):",
            "def test_bart_cnn_cli(self):",
            "stream_handler = logging.StreamHandler(sys.stdout)",
            "logger.addHandler(stream_handler)",
            "-        tmp = Path(tempfile.gettempdir()) / \"utest_generations.hypo\"",
            "+        tmp = Path(tempfile.gettempdir()) / \"utest_generations_bart_sum.hypo\"",
            "with tmp.open(\"w\") as f:",
            "f.write(\"\\n\".join(articles))",
            "-        testargs = [\"evaluate_cnn.py\", str(tmp), \"output.txt\"]",
            "+        testargs = [\"evaluate_cnn.py\", str(tmp), output_file_name]",
            "with patch.object(sys, \"argv\", testargs):",
            "_run_generate()",
            "-            self.assertTrue(Path(\"output.txt\").exists())",
            "+            self.assertTrue(Path(output_file_name).exists())",
            "+            os.remove(Path(output_file_name))"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=11, insert_id=1545746)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=1545747)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1545748)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1545749)",
            "Update(target_node=ASTNode(type=string, text=\"utest_generations.hypo\"), value='\"utest_generations_bart_sum.hypo\"')",
            "Insert(target_node=ASTNode(type=list), node=('identifier', 'output_file_name'), position=5, insert_id=1545750)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'os'), position=0, insert_id=1545751)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1545752)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'remove'), position=2, insert_id=1545753)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1545754)",
            "Insert(target_node=IN(type=argument_list), node=('call', None), position=1, insert_id=1545755)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1545756)",
            "Insert(target_node=IN(type=call), node=('identifier', 'Path'), position=0, insert_id=1545757)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1545758)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1545759)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'output_file_name'), position=1, insert_id=1545760)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1545761)",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'output_file_name'), position=1, insert_id=1545762)",
            "Delete(target_node=ASTNode(type=string, text=\"output.txt\"))",
            "Delete(target_node=ASTNode(type=string, text=\"output.txt\"))"
        ],
        "plus_line": 4,
        "minus_line": 3,
        "AST_diff_line": 20,
        "number": 7179,
        "neg_line": [
            "-tmp = Path(tempfile.gettempdir()) / \"utest_generations.hypo\"",
            "-testargs = [\"evaluate_cnn.py\", str(tmp), \"output.txt\"]",
            "-self.assertTrue(Path(\"output.txt\").exists())"
        ],
        "pos_line": [
            "+tmp = Path(tempfile.gettempdir()) / \"utest_generations_bart_sum.hypo\"",
            "+testargs = [\"evaluate_cnn.py\", str(tmp), output_file_name]",
            "+self.assertTrue(Path(output_file_name).exists())",
            "+os.remove(Path(output_file_name))"
        ],
        "core_change": "-tmp = Path(tempfile.gettempdir()) / \"utest_generations.hypo\" +tmp = Path(tempfile.gettempdir()) / \"utest_generations_bart_sum.hypo\" -testargs = [\"evaluate_cnn.py\", str(tmp), \"output.txt\"] +testargs = [\"evaluate_cnn.py\", str(tmp), output_file_name] -self.assertTrue(Path(\"output.txt\").exists()) +self.assertTrue(Path(output_file_name).exists()) +os.remove(Path(output_file_name))",
        "core_API": "StreamHandler"
    },
    {
        "commit_hash": "d08bc09495a289519a774e881f399cf864178f9b",
        "index": "4eae988e..6acb6c44 100644",
        "commit_message": "Fix unused variable found (#1250)\n\n* Fix Unused variable found\n\n* more fixes\n\n* Update kornia/enhance/histogram.py\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def find_homography_dlt_iterated(",
            "'''Function, which finds homography via iteratively-reweighted",
            "least squares ToDo: add citation'''",
            "H: torch.Tensor = find_homography_dlt(points1, points2, weights)",
            "-    for i in range(n_iter - 1):",
            "+    for _ in range(n_iter - 1):",
            "errors: torch.Tensor = symmetric_transfer_error(points1, points2, H, False)",
            "weights_new: torch.Tensor = torch.exp(-errors / (2.0 * (soft_inl_th ** 2)))",
            "H = find_homography_dlt(points1, points2, weights_new)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=i), value='_')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 7183,
        "neg_line": [
            "-for i in range(n_iter - 1):"
        ],
        "pos_line": [
            "+for _ in range(n_iter - 1):"
        ],
        "core_change": "-for i in range(n_iter - 1): +for _ in range(n_iter - 1):",
        "core_API": "exp"
    },
    {
        "commit_hash": "ed63b7ee9e68ba120893aaaaca0d4525a56ed484",
        "index": "97f57b07..140247a9 100644",
        "commit_message": "upgrade to pytorch 0.4.0 (#1126)\n\n* bump pytorch to 0.4 + fix sanitize\n\n* remove check for Variable in block_orthogonal\n\n* rename parameter split_size=\n\n* remove checks for Variable\n\n* fix more tests\n\n* fixes\n\n* get tests to pass\n\n* fix warnings\n\n* get rid of some of the Variables\n\n* more tests passing\n\n* more elimination of variables\n\n* finish removing all Variables\n\n* pylint and such\n\n* a few fixes\n\n* move torch.no_grad into model.forward_on_instances\n\n* more pytorch 0.4 changes\n\n* detach() -> data\n\n* pylint\n\n* fix bad tensor creation\n\n* fix types\n\n* remove print statement\n\n* more 0.4 goodness\n\n* factor out is_tensor\n\n* add no_grad to elmo command\n\n* cleanup\n\n* remove TODO\n\n* address PR feedback\n\n* replace all() with item()\n\n* more cleanup\n\n* further cleanup\n\n* remove Variable\n\n* really fix merge conflict\n\n* fix pylint\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestBilinearSimilarityFunction(AllenNlpTestCase):",
            "bilinear._bias = Parameter(torch.from_numpy(numpy.asarray([0])).float())",
            "a_vectors = numpy.random.rand(5, 4, 3, 6, 4)",
            "b_vectors = numpy.random.rand(5, 4, 3, 6, 7)",
            "-        a_variables = Variable(torch.from_numpy(a_vectors).float())",
            "-        b_variables = Variable(torch.from_numpy(b_vectors).float())",
            "+        a_variables = torch.from_numpy(a_vectors).float()",
            "+        b_variables = torch.from_numpy(b_vectors).float()",
            "result = bilinear(a_variables, b_variables).data.numpy()",
            "assert result.shape == (5, 4, 3, 6)",
            "expected_result = numpy.dot(numpy.dot(numpy.transpose(a_vectors[3, 2, 1, 3]), weights),"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=call), position=2)",
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=call), position=2)",
            "Delete(target_node=ASTNode(type=identifier, text=Variable))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=identifier, text=Variable))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 12,
        "number": 7184,
        "neg_line": [
            "-a_variables = Variable(torch.from_numpy(a_vectors).float())",
            "-b_variables = Variable(torch.from_numpy(b_vectors).float())"
        ],
        "pos_line": [
            "+a_variables = torch.from_numpy(a_vectors).float()",
            "+b_variables = torch.from_numpy(b_vectors).float()"
        ],
        "core_change": "-a_variables = Variable(torch.from_numpy(a_vectors).float()) -b_variables = Variable(torch.from_numpy(b_vectors).float()) +a_variables = torch.from_numpy(a_vectors).float() +b_variables = torch.from_numpy(b_vectors).float()",
        "core_API": "from_numpy"
    },
    {
        "commit_hash": "fbb7e18cccffa7abd4f0c541581f565e516efb0d",
        "index": "1d4b392380..0537b95a90 100644",
        "commit_message": "small fix for `get_item`.\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def get_item(",
            "x: torch.Tensor,",
            "query: torch.Tensor,",
            ") -> torch.Tensor:",
            "-    if ivy.dtype(query, as_native=True) is torch.bool:",
            "-        return x.__getitem__(query)",
            "-    return x.__getitem__(query.to(torch.int64))",
            "+    if ivy.is_array(query) and ivy.dtype(query, as_native=True) is not torch.bool:",
            "+        return x.__getitem__(query.to(torch.int64))",
            "+    return x.__getitem__(query)",
            "",
            "",
            "def to_numpy(x: torch.Tensor, /, *, copy: bool = True) -> np.ndarray:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=block), node=ASTNode(type=return_statement), position=1)",
            "Insert(target_node=ASTNode(type=if_statement), node=('boolean_operator', None), position=1, insert_id=326125)",
            "Insert(target_node=IN(type=boolean_operator), node=('call', None), position=0, insert_id=326126)",
            "Insert(target_node=IN(type=boolean_operator), node=('and', 'and'), position=1, insert_id=326127)",
            "Move(target_node=IN(type=boolean_operator), node=ASTNode(type=comparison_operator), position=2)",
            "Move(target_node=ASTNode(type=block), node=ASTNode(type=return_statement), position=0)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=326128)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=326129)",
            "Insert(target_node=ASTNode(type=comparison_operator), node=('is not', 'is'), position=1, insert_id=326130)",
            "Insert(target_node=ASTNode(type=comparison_operator), node=('is not', 'not'), position=2, insert_id=326131)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'ivy'), position=0, insert_id=326132)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=326133)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'is_array'), position=2, insert_id=326134)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=326135)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'query'), position=1, insert_id=326136)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=326137)",
            "Delete(target_node=ASTNode(type=is, text=is))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 17,
        "number": 7185,
        "neg_line": [
            "-if ivy.dtype(query, as_native=True) is torch.bool:",
            "-return x.__getitem__(query)",
            "-return x.__getitem__(query.to(torch.int64))"
        ],
        "pos_line": [
            "+if ivy.is_array(query) and ivy.dtype(query, as_native=True) is not torch.bool:",
            "+return x.__getitem__(query.to(torch.int64))",
            "+return x.__getitem__(query)"
        ],
        "core_change": "-if ivy.dtype(query, as_native=True) is torch.bool: -return x.__getitem__(query) -return x.__getitem__(query.to(torch.int64)) +if ivy.is_array(query) and ivy.dtype(query, as_native=True) is not torch.bool: +return x.__getitem__(query.to(torch.int64)) +return x.__getitem__(query)",
        "core_API": "dtype"
    },
    {
        "commit_hash": "19e0b4a385c8690fe314788c8491ccf2ca4c4c35",
        "index": "e815e7f..f8117e0 100644",
        "commit_message": "Fix Example in BertBlock Comment\n",
        "file": "autokeras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "keras",
        "change": [
            "class BertBlock(block_module.Block):",
            "```python",
            "# Using the Transformer Block with AutoModel.",
            "import autokeras as ak",
            "-        from autokeras import BERTBlock",
            "+        from autokeras.blocks import BertBlock",
            "from tensorflow.keras import losses",
            "",
            "input_node = ak.TextInput()",
            "-        output_node = BERTBlock(max_seq_len=128)(input_node)",
            "+        output_node = BertBlock(max_sequence_length=128)(input_node)",
            "output_node = ak.ClassificationHead()(output_node)",
            "clf = ak.AutoModel(inputs=input_node, outputs=output_node, max_trials=10)",
            "```"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=module), position=9)",
            "Insert(target_node=ASTNode(type=module), node=('ERROR', None), position=1, insert_id=2466500)",
            "Insert(target_node=IN(type=ERROR), node=('string', '``'), position=0, insert_id=2466501)",
            "Insert(target_node=IN(type=ERROR), node=('\"', '`'), position=1, insert_id=2466502)",
            "Insert(target_node=ASTNode(type=dotted_name), node=('.', '.'), position=1, insert_id=2466503)",
            "Insert(target_node=ASTNode(type=dotted_name), node=('identifier', 'blocks'), position=2, insert_id=2466504)",
            "Update(target_node=ASTNode(type=identifier, text=BERTBlock), value='BertBlock')",
            "Update(target_node=ASTNode(type=identifier, text=BERTBlock), value='BertBlock')",
            "Update(target_node=ASTNode(type=identifier, text=max_seq_len), value='max_sequence_length')",
            "Delete(target_node=ASTNode(type=string, text=``))",
            "Delete(target_node=ASTNode(type=\", text=`))",
            "Delete(target_node=ASTNode(type=ERROR))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 12,
        "number": 7187,
        "neg_line": [
            "-from autokeras import BERTBlock",
            "-output_node = BERTBlock(max_seq_len=128)(input_node)"
        ],
        "pos_line": [
            "+from autokeras.blocks import BertBlock",
            "+output_node = BertBlock(max_sequence_length=128)(input_node)"
        ],
        "core_change": "-from autokeras import BERTBlock +from autokeras.blocks import BertBlock -output_node = BERTBlock(max_seq_len=128)(input_node) +output_node = BertBlock(max_sequence_length=128)(input_node)",
        "core_API": "TextInput"
    },
    {
        "commit_hash": "caa3371376abe7149c1ab1b8eb5bcfe02c91423c",
        "index": "06d27c7..56ea340 100644",
        "commit_message": "Fix Pulsar backend batched radius handling.\n\nSummary: This fixes a corner case for multi-radius handling for the pulsar backend. The additional dimensionality check ensures that the batched parsing for radiuses is only performed when appropriate.\n\nReviewed By: bottler\n\nDifferential Revision: D25387708\n\nfbshipit-source-id: c486dcf327f812265b7ca8ca5ef5c6a31e6d4549\n\n",
        "file": "pytorch3d.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class PulsarPointsRenderer(nn.Module):",
            "raster_rad = self.rasterizer.raster_settings.radius",
            "if kwargs.get(\"radius_world\", False):",
            "return raster_rad",
            "-        if isinstance(raster_rad, torch.Tensor) and raster_rad.numel() > 1:",
            "+        if (",
            "+            isinstance(raster_rad, torch.Tensor)",
            "+            and raster_rad.numel() > 1",
            "+            and raster_rad.ndim > 1",
            "+        ):",
            "# In this case it must be a batched torch tensor.",
            "raster_rad = raster_rad[cloud_idx]",
            "if orthogonal_projection:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=if_statement), node=('parenthesized_expression', None), position=1, insert_id=921469)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('(', '('), position=0, insert_id=921470)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('boolean_operator', None), position=1, insert_id=921471)",
            "Insert(target_node=IN(type=parenthesized_expression), node=(')', ')'), position=2, insert_id=921472)",
            "Move(target_node=IN(type=boolean_operator), node=ASTNode(type=boolean_operator), position=0)",
            "Insert(target_node=IN(type=boolean_operator), node=('and', 'and'), position=1, insert_id=921473)",
            "Insert(target_node=IN(type=boolean_operator), node=('comparison_operator', None), position=2, insert_id=921474)",
            "Insert(target_node=IN(type=comparison_operator), node=('attribute', None), position=0, insert_id=921475)",
            "Insert(target_node=IN(type=comparison_operator), node=('>', '>'), position=1, insert_id=921476)",
            "Insert(target_node=IN(type=comparison_operator), node=('integer', '1'), position=2, insert_id=921477)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'raster_rad'), position=0, insert_id=921478)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=921479)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'ndim'), position=2, insert_id=921480)"
        ],
        "plus_line": 5,
        "minus_line": 1,
        "AST_diff_line": 13,
        "number": 7191,
        "neg_line": [
            "-if isinstance(raster_rad, torch.Tensor) and raster_rad.numel() > 1:"
        ],
        "pos_line": [
            "+if (",
            "+isinstance(raster_rad, torch.Tensor)",
            "+and raster_rad.numel() > 1",
            "+and raster_rad.ndim > 1",
            "+):"
        ],
        "core_change": "-if isinstance(raster_rad, torch.Tensor) and raster_rad.numel() > 1: +if ( +isinstance(raster_rad, torch.Tensor) +and raster_rad.numel() > 1 +and raster_rad.ndim > 1 +):",
        "core_API": "get"
    },
    {
        "commit_hash": "bc9170ffe1e0e26bd89117404fe7ee2e32cdc1cf",
        "index": "6b0e181b..17964008 100644",
        "commit_message": "fix: added eps in geometry/rotmat_to_quaternion (#1665)\n\nFor numerical stability and to avoid nan gradients added an eps in case the trace is -1 so that the sqrt would be negative.\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def rotation_matrix_to_quaternion(",
            "trace: torch.Tensor = m00 + m11 + m22",
            "",
            "def trace_positive_cond():",
            "-        sq = torch.sqrt(trace + 1.0) * 2.0  # sq = 4 * qw.",
            "+        sq = torch.sqrt(trace + 1.0 + eps) * 2.0  # sq = 4 * qw.",
            "qw = 0.25 * sq",
            "qx = safe_zero_division(m21 - m12, sq)",
            "qy = safe_zero_division(m02 - m20, sq)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=binary_operator), node=ASTNode(type=binary_operator), position=0)",
            "Insert(target_node=ASTNode(type=binary_operator), node=('+', '+'), position=1, insert_id=403984)",
            "Insert(target_node=ASTNode(type=binary_operator), node=('identifier', 'eps'), position=2, insert_id=403985)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 3,
        "number": 7193,
        "neg_line": [
            "-sq = torch.sqrt(trace + 1.0) * 2.0  # sq = 4 * qw."
        ],
        "pos_line": [
            "+sq = torch.sqrt(trace + 1.0 + eps) * 2.0  # sq = 4 * qw."
        ],
        "core_change": "-sq = torch.sqrt(trace + 1.0) * 2.0  # sq = 4 * qw. +sq = torch.sqrt(trace + 1.0 + eps) * 2.0  # sq = 4 * qw.",
        "core_API": "sqrt"
    },
    {
        "commit_hash": "65bb86eb432e1437658cca2d920f8514bf867de5",
        "index": "f25c34bc..0184141c 100644",
        "commit_message": "Fix batch log pdf masked view (#322)\n\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Bernoulli(Distribution):",
            "# when the data is a ragged tensor. also useful for KL annealing. this entire logic",
            "# will likely be done in a better/cleaner way in the future",
            "if log_pdf_mask is not None:",
            "-            # TODO fix this to broadcasting as below, e.g. by instead:",
            "-            # logsum *= log_pdf_mask  # Then continue with broadcasting logic below.",
            "-            return torch.sum(log_pdf_mask * logsum, -1)",
            "+            logsum = logsum * log_pdf_mask",
            "batch_log_pdf_shape = self.batch_shape(ps) + (1,)",
            "return torch.sum(logsum, -1).contiguous().view(batch_log_pdf_shape)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=block), node=('expression_statement', None), position=0, insert_id=762626)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=762627)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=identifier, text=logsum), position=0)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=762628)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=binary_operator), position=2)",
            "Move(target_node=ASTNode(type=identifier, text=log_pdf_mask), node=ASTNode(type=binary_operator), position=1)",
            "Insert(target_node=ASTNode(type=binary_operator), node=('identifier', 'logsum'), position=0, insert_id=762629)",
            "Delete(target_node=ASTNode(type=return, text=return))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=sum))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=unary_operator, text=-1))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=return_statement))"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 19,
        "number": 7194,
        "neg_line": [
            "-# TODO fix this to broadcasting as below, e.g. by instead:",
            "-# logsum *= log_pdf_mask  # Then continue with broadcasting logic below.",
            "-return torch.sum(log_pdf_mask * logsum, -1)"
        ],
        "pos_line": [
            "+logsum = logsum * log_pdf_mask"
        ],
        "core_change": "-# TODO fix this to broadcasting as below, e.g. by instead: -# logsum *= log_pdf_mask  # Then continue with broadcasting logic below. -return torch.sum(log_pdf_mask * logsum, -1) +logsum = logsum * log_pdf_mask",
        "core_API": "sum"
    },
    {
        "commit_hash": "f242d3cff5197fd5510cb043bd166baa92413054",
        "index": "341eaf1b3..fa9f000ed 100644",
        "commit_message": "update on when learn is defiend with test query, fix on documentation pip3, and use ensamble net by default\n\n",
        "file": "mindsdb.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ModelTrainer(BaseModule):",
            "model_name = self.transaction.persistent_model_metadata.model_name",
            "",
            "ml_models = [",
            "-            ('pytorch.models.fully_connected_net', {})",
            "+            #('pytorch.models.fully_connected_net', {})",
            "#, ('pytorch.models.ensemble_conv_net', {})",
            "-            #, ('pytorch.models.ensemble_fully_connected_net', {})",
            "+            ('pytorch.models.ensemble_fully_connected_net', {})",
            "]",
            "",
            "self.train_start_time = time.time()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text='pytorch.models.fully_connected_net'), value=\"'pytorch.models.ensemble_fully_connected_net'\")"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 1,
        "number": 7195,
        "neg_line": [
            "-('pytorch.models.fully_connected_net', {})",
            "-#, ('pytorch.models.ensemble_fully_connected_net', {})"
        ],
        "pos_line": [
            "+#('pytorch.models.fully_connected_net', {})",
            "+('pytorch.models.ensemble_fully_connected_net', {})"
        ],
        "core_change": "-('pytorch.models.fully_connected_net', {}) +#('pytorch.models.fully_connected_net', {}) -#, ('pytorch.models.ensemble_fully_connected_net', {}) +('pytorch.models.ensemble_fully_connected_net', {})",
        "core_API": "time"
    },
    {
        "commit_hash": "20c20a2066e0bb9a1062a8e474f17812d4238748",
        "index": "788fcf0016..a32eddaede 100644",
        "commit_message": "lintfixbot: Auto-commit fixed lint errors in codebase\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def gelu(",
            "",
            "",
            "@with_unsupported_dtypes({\"1.11.0 and below\": (\"float16\",)}, backend_version)",
            "-def sigmoid(",
            "-    x: torch.Tensor,",
            "-    /,",
            "-    *,",
            "-    out: Optional[torch.Tensor] = None",
            "-) -> torch.Tensor:",
            "+def sigmoid(x: torch.Tensor, /, *, out: Optional[torch.Tensor] = None) -> torch.Tensor:",
            "if not ivy.is_array(x):",
            "x = torch.tensor(x)",
            "return torch.sigmoid(x, out=out)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 0,
        "minus_line": 4,
        "AST_diff_line": 17,
        "number": 7201,
        "neg_line": [
            "-def sigmoid(",
            "-x: torch.Tensor,",
            "-/,",
            "-*,",
            "-out: Optional[torch.Tensor] = None",
            "-) -> torch.Tensor:"
        ],
        "pos_line": [
            "+def sigmoid(x: torch.Tensor, /, *, out: Optional[torch.Tensor] = None) -> torch.Tensor:"
        ],
        "core_change": "-def sigmoid( -x: torch.Tensor, -/, -*, -out: Optional[torch.Tensor] = None -) -> torch.Tensor: +def sigmoid(x: torch.Tensor, /, *, out: Optional[torch.Tensor] = None) -> torch.Tensor:",
        "core_API": "is_array"
    },
    {
        "commit_hash": "fec3112cbacf19cd211109b1bc0b33a502595aca",
        "index": "32026d77..2753e64e 100644",
        "commit_message": "Update to PyTorch 1.9.0 (#2887)\n\n* Update to PyTorch 1.9.0\n\n* Update torch.cholesky, torch.symeig\n\n* Fix torch.tensordot, torch.qr, funsor dependency\n\n* Fix torch import, AutoGuideList usage\n\n* Ignore bug in PyTorch jit\n\n* Ignore tracer warnings\n\n* Replace torch.solve with torch.linalg.solve\n\n* Xfail vectorized markov funsor tests\n\n* Update funsor version\n\n* Switch MNIST mirrors\n\n* Pin pillow version\n\n* Bump torchvision version\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def vsgp_multiclass(num_steps, whiten):",
            "pyro.set_rng_seed(0)",
            "X = torch.rand(100, 1)",
            "K = (-0.5 * (X - X.t()).pow(2) / 0.01).exp() + torch.eye(100) * 1e-6",
            "-    f = K.cholesky().matmul(torch.randn(100, 3))",
            "+    f = torch.linalg.cholesky(K).matmul(torch.randn(100, 3))",
            "y = f.argmax(dim=-1)",
            "",
            "kernel = gp.kernels.Sum(gp.kernels.Matern32(1),"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=677215)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=677216)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=K), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=677217)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'linalg'), position=2, insert_id=677218)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 7202,
        "neg_line": [
            "-f = K.cholesky().matmul(torch.randn(100, 3))"
        ],
        "pos_line": [
            "+f = torch.linalg.cholesky(K).matmul(torch.randn(100, 3))"
        ],
        "core_change": "-f = K.cholesky().matmul(torch.randn(100, 3)) +f = torch.linalg.cholesky(K).matmul(torch.randn(100, 3))",
        "core_API": "set_rng_seed"
    },
    {
        "commit_hash": "ebda0121172d812761550182e60b6d81ab2a5ed1",
        "index": "7bc3efac..8dc275ee 100644",
        "commit_message": "Rnn scratch fix (#1450)\n\n* Update rnn-scratch.md\n\n* Update rnn-scratch.md\n\n* Revert \"Update rnn-scratch.md\"\n\nThis reverts commit 16c00a668da740d0eded48b6e83805e067a763ea.\n\n* Revert \"Update rnn-scratch.md\"\n\nThis reverts commit 7cd51e90e4d03c3af568db64a2cad7c985a34855.\n\n* Update rnn-scratch.md\n\n* Update tensorflow.py\n",
        "file": "d2l-en.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def train_ch8(model, train_iter, vocab, num_hiddens, lr, num_epochs,",
            "use_random_iter=False):",
            "\"\"\"Train a model (defined in Chapter 8).\"\"\"",
            "params = get_params(len(vocab), num_hiddens)",
            "-    loss = tf.keras.losses.SparseCategoricalCrossentropy()",
            "+    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)",
            "animator = d2l.Animator(xlabel='epoch', ylabel='perplexity',",
            "legend=['train'], xlim=[1, num_epochs])",
            "updater = tf.keras.optimizers.SGD(lr)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=1, insert_id=1917555)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'from_logits'), position=0, insert_id=1917556)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1917557)",
            "Insert(target_node=IN(type=keyword_argument), node=('true', 'True'), position=2, insert_id=1917558)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 7204,
        "neg_line": [
            "-loss = tf.keras.losses.SparseCategoricalCrossentropy()"
        ],
        "pos_line": [
            "+loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
        ],
        "core_change": "-loss = tf.keras.losses.SparseCategoricalCrossentropy() +loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)",
        "core_API": "SparseCategoricalCrossentropy"
    },
    {
        "commit_hash": "205bc4152c6a90bad41922f60e79fea10218c29a",
        "index": "ad23f12bd..a4f7913ba 100755",
        "commit_message": "Fix GPT-NeoX-20B past handling, attention computation (#17811)\n\n* Fix GPT-NeoX-20B past handling, swap attention computation to hopefully avoid NaN, update docs\n\n* 20B tests\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class GPTNeoXForCausalLM(GPTNeoXPreTrainedModel):",
            "attention_mask = input_ids.new_ones(input_shape)",
            "",
            "# cut decoder_input_ids if past is used",
            "-        if past is not None:",
            "+        if past and past[0] is not None:",
            "input_ids = input_ids[:, -1:]",
            "",
            "return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"past_key_values\": past}"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=if_statement), node=('boolean_operator', None), position=1, insert_id=1194834)",
            "Move(target_node=IN(type=boolean_operator), node=ASTNode(type=identifier, text=past), position=0)",
            "Insert(target_node=IN(type=boolean_operator), node=('and', 'and'), position=1, insert_id=1194835)",
            "Move(target_node=IN(type=boolean_operator), node=ASTNode(type=comparison_operator), position=2)",
            "Insert(target_node=ASTNode(type=comparison_operator), node=('subscript', None), position=0, insert_id=1194836)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'past'), position=0, insert_id=1194837)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=1194838)",
            "Insert(target_node=IN(type=subscript), node=('integer', '0'), position=2, insert_id=1194839)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=1194840)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 7215,
        "neg_line": [
            "-if past is not None:"
        ],
        "pos_line": [
            "+if past and past[0] is not None:"
        ],
        "core_change": "-if past is not None: +if past and past[0] is not None:",
        "core_API": "new_ones"
    },
    {
        "commit_hash": "54c174e111aa4a6fe0f7044d9409edd9a125ad53",
        "index": "31063f7..2282144 100644",
        "commit_message": "Fix keep_dims/keepdims compatibility issues(#64)\n\n",
        "file": "CapsNet-Tensorflow.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def routing(input, b_IJ):",
            "input = tf.tile(input, [1, 1, 160, 1, 1])",
            "assert input.get_shape() == [cfg.batch_size, 1152, 160, 8, 1]",
            "",
            "-    u_hat = tf.reduce_sum(W * input, axis=3, keepdims=True)",
            "+    u_hat = reduce_sum(W * input, axis=3, keepdims=True)",
            "u_hat = tf.reshape(u_hat, shape=[-1, 1152, 10, 16, 1])",
            "assert u_hat.get_shape() == [cfg.batch_size, 1152, 10, 16, 1]"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=reduce_sum), position=0)",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 7216,
        "neg_line": [
            "-u_hat = tf.reduce_sum(W * input, axis=3, keepdims=True)"
        ],
        "pos_line": [
            "+u_hat = reduce_sum(W * input, axis=3, keepdims=True)"
        ],
        "core_change": "-u_hat = tf.reduce_sum(W * input, axis=3, keepdims=True) +u_hat = reduce_sum(W * input, axis=3, keepdims=True)",
        "core_API": "tile"
    },
    {
        "commit_hash": "fec3112cbacf19cd211109b1bc0b33a502595aca",
        "index": "a3b4fb40..87a5e940 100644",
        "commit_message": "Update to PyTorch 1.9.0 (#2887)\n\n* Update to PyTorch 1.9.0\n\n* Update torch.cholesky, torch.symeig\n\n* Fix torch.tensordot, torch.qr, funsor dependency\n\n* Fix torch import, AutoGuideList usage\n\n* Ignore bug in PyTorch jit\n\n* Ignore tracer warnings\n\n* Replace torch.solve with torch.linalg.solve\n\n* Xfail vectorized markov funsor tests\n\n* Update funsor version\n\n* Switch MNIST mirrors\n\n* Pin pillow version\n\n* Bump torchvision version\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SpanningTree(TorchDistribution):",
            "import gpytorch",
            "log_det = gpytorch.lazy.NonLazyTensor(truncated).logdet()",
            "except ImportError:",
            "-            log_det = torch.cholesky(truncated).diag().log().sum() * 2",
            "+            log_det = torch.linalg.cholesky(truncated).diag().log().sum() * 2",
            "return log_det + log_diag[:-1].sum()",
            "",
            "def log_prob(self, edges):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=676917)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=676918)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=torch), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'linalg'), position=2, insert_id=676919)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 7222,
        "neg_line": [
            "-log_det = torch.cholesky(truncated).diag().log().sum() * 2"
        ],
        "pos_line": [
            "+log_det = torch.linalg.cholesky(truncated).diag().log().sum() * 2"
        ],
        "core_change": "-log_det = torch.cholesky(truncated).diag().log().sum() * 2 +log_det = torch.linalg.cholesky(truncated).diag().log().sum() * 2",
        "core_API": "NonLazyTensor"
    },
    {
        "commit_hash": "cc6804d175ffd69e6ff6c9089eaf54b997209443",
        "index": "f2cb8651a2..34a27139bd 100644",
        "commit_message": "tf solve fix (#6522)\n\n* solve fix-- square checking before det computation\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def solve(",
            "else:",
            "x1 = tf.broadcast_to(x1, output_shape + x1.shape[-2:])",
            "x2 = tf.broadcast_to(x2, output_shape + x2.shape[-2:])",
            "-        if tf.math.reduce_any(tf.linalg.det(x1) == 0) or tf.math.reduce_any(",
            "-            tf.linalg.det(x2) == 0",
            "-        ):",
            "+        if tf.math.reduce_any(tf.linalg.det(x1) == 0) or (",
            "+            x2.shape[-1] == x2.shape[-2] and tf.math.reduce_any(",
            "+                tf.linalg.det(x2) == 0)):",
            "return x1",
            "ret = tf.linalg.solve(x1, x2)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=boolean_operator), node=('parenthesized_expression', None), position=2, insert_id=1980201)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('(', '('), position=0, insert_id=1980202)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('boolean_operator', None), position=1, insert_id=1980203)",
            "Insert(target_node=IN(type=parenthesized_expression), node=(')', ')'), position=2, insert_id=1980204)",
            "Insert(target_node=IN(type=boolean_operator), node=('comparison_operator', None), position=0, insert_id=1980205)",
            "Insert(target_node=IN(type=boolean_operator), node=('and', 'and'), position=1, insert_id=1980206)",
            "Move(target_node=IN(type=boolean_operator), node=ASTNode(type=call), position=2)",
            "Insert(target_node=IN(type=comparison_operator), node=('subscript', None), position=0, insert_id=1980207)",
            "Insert(target_node=IN(type=comparison_operator), node=('==', '=='), position=1, insert_id=1980208)",
            "Insert(target_node=IN(type=comparison_operator), node=('subscript', None), position=2, insert_id=1980209)",
            "Insert(target_node=IN(type=subscript), node=('attribute', None), position=0, insert_id=1980210)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=1980211)",
            "Insert(target_node=IN(type=subscript), node=('unary_operator', '-1'), position=2, insert_id=1980212)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=1980213)",
            "Insert(target_node=IN(type=subscript), node=('attribute', None), position=0, insert_id=1980214)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=1980215)",
            "Insert(target_node=IN(type=subscript), node=('unary_operator', '-2'), position=2, insert_id=1980216)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=1980217)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'x2'), position=0, insert_id=1980218)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1980219)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'shape'), position=2, insert_id=1980220)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'x2'), position=0, insert_id=1980221)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1980222)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'shape'), position=2, insert_id=1980223)"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 24,
        "number": 7223,
        "neg_line": [
            "-if tf.math.reduce_any(tf.linalg.det(x1) == 0) or tf.math.reduce_any(",
            "-tf.linalg.det(x2) == 0",
            "-):"
        ],
        "pos_line": [
            "+if tf.math.reduce_any(tf.linalg.det(x1) == 0) or (",
            "+x2.shape[-1] == x2.shape[-2] and tf.math.reduce_any(",
            "+tf.linalg.det(x2) == 0)):"
        ],
        "core_change": "-if tf.math.reduce_any(tf.linalg.det(x1) == 0) or tf.math.reduce_any( -tf.linalg.det(x2) == 0 -): +if tf.math.reduce_any(tf.linalg.det(x1) == 0) or ( +x2.shape[-1] == x2.shape[-2] and tf.math.reduce_any( +tf.linalg.det(x2) == 0)):",
        "core_API": "broadcast_to"
    },
    {
        "commit_hash": "3dda5ac9fcf390d8b83eb855249360711707c11c",
        "index": "98d713f4..f772c8dd 100644",
        "commit_message": "for discussion: incorporate black code formatter (#3308)\n\n* setup files\n\n* run black\n\n* undo\n\n* update CONTRIBUTING.md\n\n* fix quotes in test_other_modules\n\n* make flake8 happy\n\n* set black to 100 characters per line\n\n* move type: ignore to where mypy wants them\n\n* more flake8\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Entropy(Metric):",
            "",
            "log_probs = torch.nn.functional.log_softmax(logits, dim=-1)",
            "probabilities = torch.exp(log_probs) * mask.unsqueeze(-1)",
            "-        weighted_negative_likelihood = - log_probs * probabilities",
            "+        weighted_negative_likelihood = -log_probs * probabilities",
            "entropy = weighted_negative_likelihood.sum(-1)",
            "",
            "self._entropy += entropy.sum() / mask.sum()"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 7228,
        "neg_line": [
            "-weighted_negative_likelihood = - log_probs * probabilities"
        ],
        "pos_line": [
            "+weighted_negative_likelihood = -log_probs * probabilities"
        ],
        "core_change": "-weighted_negative_likelihood = - log_probs * probabilities +weighted_negative_likelihood = -log_probs * probabilities",
        "core_API": "log_softmax"
    },
    {
        "commit_hash": "5ce464f1b16783a5d99ca824a81fab34281afc45",
        "index": "32205993..c993cf6b 100644",
        "commit_message": "Fix interpolation comments.\n\nPiperOrigin-RevId: 329882604\n\n",
        "file": "tf-quant-finance.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def interpolate(x,",
            "# won't go out of bounds.",
            "lower_encoding = tf.math.maximum(upper_indices - 1, 0)",
            "upper_encoding = tf.math.minimum(upper_indices, x_data_size - 1)",
            "-      # Prepare indices for `tf.gather_nd` or `tf.one_hot`",
            "+      # Prepare indices for `tf.gather` or `tf.one_hot`",
            "# TODO(b/156720909): Extract get_slice logic into a common utilities",
            "# module for cubic and linear interpolation",
            "if optimize_for_tpu:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 7231,
        "neg_line": [
            "-# Prepare indices for `tf.gather_nd` or `tf.one_hot`"
        ],
        "pos_line": [
            "+# Prepare indices for `tf.gather` or `tf.one_hot`"
        ],
        "core_change": "-# Prepare indices for `tf.gather_nd` or `tf.one_hot` +# Prepare indices for `tf.gather` or `tf.one_hot`",
        "core_API": "maximum"
    },
    {
        "commit_hash": "31563e056da7a8813071022d465384748d395d30",
        "index": "ba068869e..bce2fc531 100644",
        "commit_message": "Restore TF embeddings and attention layers to their previous version (#9890)\n\n* Refacto BERT\n\n* Restore all the concerned models\n\n* Remove print\n\n* Update template\n\n* Apply Sylvain's and Morgan's comments\n\n* Fix cast\n\n* Put the cast inside call\n\n* Remove cond in ebds\n\n* Fix funnel\n\n* Restore previous dot product (attention_scores) computation\n\n* Add ConvBERT and BART\n\n* Make all the S2S models ONNX compliant\n\n* Fix test\n\n* Fix check copies\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def _make_causal_mask(input_ids_shape: tf.TensorShape, past_key_values_length: i",
            "",
            "if past_key_values_length > 0:",
            "mask = tf.concat([tf.zeros((tgt_len, past_key_values_length), dtype=tf.float32), mask], axis=-1)",
            "-    return tf.broadcast_to(mask[None, None, :, :], (bsz, 1, tgt_len, tgt_len + past_key_values_length))",
            "+",
            "+    return tf.tile(mask[None, None, :, :], (bsz, 1, 1, 1))",
            "",
            "",
            "def _expand_mask(mask: tf.Tensor, tgt_len: Optional[int] = None, past_key_values_length: int = 0):",
            "\"\"\"",
            "Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.",
            "\"\"\"",
            "-    bsz, src_len = shape_list(mask)",
            "+    src_len = shape_list(mask)[1]",
            "tgt_len = tgt_len if tgt_len is not None else src_len",
            "-",
            "-    expanded_mask = tf.cast(tf.broadcast_to(mask[:, None, None, :], (bsz, 1, tgt_len, src_len)), tf.float32)",
            "+    expanded_mask = tf.cast(tf.tile(mask[:, None, None, :], (1, 1, tgt_len, 1)), tf.float32)",
            "",
            "return (1.0 - expanded_mask) * LARGE_NEGATIVE"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=identifier, text=src_len), position=0)",
            "Insert(target_node=ASTNode(type=assignment), node=('subscript', None), position=3, insert_id=2372425)",
            "Move(target_node=IN(type=subscript), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=2372426)",
            "Insert(target_node=IN(type=subscript), node=('integer', '1'), position=2, insert_id=2372427)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=2372428)",
            "Update(target_node=ASTNode(type=identifier, text=broadcast_to), value='tile')",
            "Update(target_node=ASTNode(type=identifier, text=broadcast_to), value='tile')",
            "Insert(target_node=ASTNode(type=tuple), node=('integer', '1'), position=5, insert_id=2372429)",
            "Insert(target_node=ASTNode(type=tuple), node=('integer', '1'), position=8, insert_id=2372430)",
            "Insert(target_node=ASTNode(type=tuple), node=('integer', '1'), position=1, insert_id=2372431)",
            "Insert(target_node=ASTNode(type=tuple), node=('integer', '1'), position=8, insert_id=2372432)",
            "Delete(target_node=ASTNode(type=identifier, text=tgt_len))",
            "Delete(target_node=ASTNode(type=identifier, text=tgt_len))",
            "Delete(target_node=ASTNode(type=+, text=+))",
            "Delete(target_node=ASTNode(type=identifier, text=past_key_values_length))",
            "Delete(target_node=ASTNode(type=binary_operator))",
            "Delete(target_node=ASTNode(type=identifier, text=bsz))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=pattern_list))",
            "Delete(target_node=ASTNode(type=identifier, text=bsz))",
            "Delete(target_node=ASTNode(type=identifier, text=src_len))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 22,
        "number": 7233,
        "neg_line": [
            "-return tf.broadcast_to(mask[None, None, :, :], (bsz, 1, tgt_len, tgt_len + past_key_values_length))",
            "-bsz, src_len = shape_list(mask)",
            "-",
            "-expanded_mask = tf.cast(tf.broadcast_to(mask[:, None, None, :], (bsz, 1, tgt_len, src_len)), tf.float32)"
        ],
        "pos_line": [
            "+",
            "+return tf.tile(mask[None, None, :, :], (bsz, 1, 1, 1))",
            "+src_len = shape_list(mask)[1]",
            "+expanded_mask = tf.cast(tf.tile(mask[:, None, None, :], (1, 1, tgt_len, 1)), tf.float32)"
        ],
        "core_change": "-return tf.broadcast_to(mask[None, None, :, :], (bsz, 1, tgt_len, tgt_len + past_key_values_length)) + +return tf.tile(mask[None, None, :, :], (bsz, 1, 1, 1)) -bsz, src_len = shape_list(mask) +src_len = shape_list(mask)[1] - -expanded_mask = tf.cast(tf.broadcast_to(mask[:, None, None, :], (bsz, 1, tgt_len, src_len)), tf.float32) +expanded_mask = tf.cast(tf.tile(mask[:, None, None, :], (1, 1, tgt_len, 1)), tf.float32)",
        "core_API": "concat"
    },
    {
        "commit_hash": "e84af8ce18fb12ff76f0e99d27d080b37a4363d9",
        "index": "96247b04..41086cf4 100644",
        "commit_message": "minor fix\n\n",
        "file": "mmdetection.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def anchor_target_single(flat_anchors,",
            "num_valid_anchors = anchors.shape[0]",
            "bbox_targets = torch.zeros_like(anchors)",
            "bbox_weights = torch.zeros_like(anchors)",
            "-    labels = gt_labels.new_zeros(num_valid_anchors)",
            "-    label_weights = gt_labels.new_zeros(num_valid_anchors, dtype=torch.float)",
            "+    labels = anchors.new_zeros(num_valid_anchors, dtype=torch.long)",
            "+    label_weights = anchors.new_zeros(num_valid_anchors, dtype=torch.float)",
            "",
            "pos_inds = sampling_result.pos_inds",
            "neg_inds = sampling_result.neg_inds"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=gt_labels), value='anchors')",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=647080)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=647081)",
            "Update(target_node=ASTNode(type=identifier, text=gt_labels), value='anchors')",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=647082)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=647083)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=647084)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=647085)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=647086)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'long'), position=2, insert_id=647087)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 10,
        "number": 7238,
        "neg_line": [
            "-labels = gt_labels.new_zeros(num_valid_anchors)",
            "-label_weights = gt_labels.new_zeros(num_valid_anchors, dtype=torch.float)"
        ],
        "pos_line": [
            "+labels = anchors.new_zeros(num_valid_anchors, dtype=torch.long)",
            "+label_weights = anchors.new_zeros(num_valid_anchors, dtype=torch.float)"
        ],
        "core_change": "-labels = gt_labels.new_zeros(num_valid_anchors) -label_weights = gt_labels.new_zeros(num_valid_anchors, dtype=torch.float) +labels = anchors.new_zeros(num_valid_anchors, dtype=torch.long) +label_weights = anchors.new_zeros(num_valid_anchors, dtype=torch.float)",
        "core_API": "zeros_like"
    },
    {
        "commit_hash": "6ed6ed29b118c7e57d696e3b82bc1e2a0c49a8bd",
        "index": "cbd26ff0e..4b6dfa08d 100644",
        "commit_message": "fix device issue (#20227)\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def _segment_reduce(values, index, segment_reduce_fn, name):",
            "dim=0,",
            ")",
            "",
            "-    output_values = segment_means.clone().view(new_shape.tolist())",
            "+    output_values = segment_means.clone().view(new_shape.tolist()).to(values.dtype)",
            "output_index = range_index_map(index.batch_shape(), index.num_segments)",
            "return output_values, output_index"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1185520)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=1185521)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1185522)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to'), position=2, insert_id=1185523)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1185524)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=1185525)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1185526)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'values'), position=0, insert_id=1185527)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1185528)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dtype'), position=2, insert_id=1185529)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 7246,
        "neg_line": [
            "-output_values = segment_means.clone().view(new_shape.tolist())"
        ],
        "pos_line": [
            "+output_values = segment_means.clone().view(new_shape.tolist()).to(values.dtype)"
        ],
        "core_change": "-output_values = segment_means.clone().view(new_shape.tolist()) +output_values = segment_means.clone().view(new_shape.tolist()).to(values.dtype)",
        "core_API": "clone"
    },
    {
        "commit_hash": "5b69726503584815f2493708cb55e0bed0102383",
        "index": "12977abb..b08b851a 100755",
        "commit_message": "various scope and test fixes\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class PolicyGradientModel(Model):",
            "",
            "with tf.variable_scope('distribution'):",
            "for action, distribution in self.distribution.items():",
            "-                distribution.create_tf_operations(x=self.network.output, deterministic=self.deterministic)",
            "+                with tf.variable_scope(action):",
            "+                    distribution.create_tf_operations(x=self.network.output, deterministic=self.deterministic)",
            "self.action_taken[action] = distribution.sample()",
            "",
            "if self.baseline:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=block), node=('with_statement', None), position=0, insert_id=2242943)",
            "Insert(target_node=IN(type=with_statement), node=('with', 'with'), position=0, insert_id=2242944)",
            "Insert(target_node=IN(type=with_statement), node=('with_clause', None), position=1, insert_id=2242945)",
            "Insert(target_node=IN(type=with_statement), node=(':', ':'), position=2, insert_id=2242946)",
            "Move(target_node=IN(type=with_statement), node=ASTNode(type=block), position=3)",
            "Insert(target_node=IN(type=with_clause), node=('with_item', None), position=0, insert_id=2242947)",
            "Insert(target_node=IN(type=with_item), node=('call', None), position=0, insert_id=2242948)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2242949)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2242950)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2242951)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2242952)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'variable_scope'), position=2, insert_id=2242953)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2242954)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'action'), position=1, insert_id=2242955)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2242956)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 15,
        "number": 7261,
        "neg_line": [
            "-distribution.create_tf_operations(x=self.network.output, deterministic=self.deterministic)"
        ],
        "pos_line": [
            "+with tf.variable_scope(action):",
            "+distribution.create_tf_operations(x=self.network.output, deterministic=self.deterministic)"
        ],
        "core_change": "-distribution.create_tf_operations(x=self.network.output, deterministic=self.deterministic) +with tf.variable_scope(action): +distribution.create_tf_operations(x=self.network.output, deterministic=self.deterministic)",
        "core_API": "variable_scope"
    },
    {
        "commit_hash": "f4b024edf32e7dda4f2538a9343b72b59e324c5b",
        "index": "79e428dab..a93f07c88 100644",
        "commit_message": "Fix summary iterator import for TF 2 compatibility (#732)\n\n\n",
        "file": "wandb.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def stream_tfevents(path, file_api, run, step=0, namespace=\"\"):",
            "last_row = {}",
            "global_step_key = namespaced_tag(\"global_step\", namespace)",
            "try:",
            "-        for summary in tf.train.summary_iterator(path):",
            "+        for summary in summary_iterator(path):",
            "parsed = tf_summary_to_dict(summary, namespace=namespace)",
            "if last_step != parsed[global_step_key]:",
            "last_step = parsed[global_step_key]"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=summary_iterator), position=0)",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=train))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 7263,
        "neg_line": [
            "-for summary in tf.train.summary_iterator(path):"
        ],
        "pos_line": [
            "+for summary in summary_iterator(path):"
        ],
        "core_change": "-for summary in tf.train.summary_iterator(path): +for summary in summary_iterator(path):",
        "core_API": "summary_iterator"
    },
    {
        "commit_hash": "5d51637bb05d0f728ddf44605627f90ebe3de19b",
        "index": "916ade97..7afc68dd 100644",
        "commit_message": "add doc and fix bug in experiment resume/view (#3524)\n\n\n",
        "file": "nni.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class Experiments:",
            "if os.path.exists(self.experiment_file):",
            "try:",
            "with open(self.experiment_file, 'r') as file:",
            "-                    return json.load(file)",
            "+                    return json_tricks.load(file)",
            "except ValueError:",
            "return {}",
            "return {}"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=json), value='json_tricks')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 7267,
        "neg_line": [
            "-return json.load(file)"
        ],
        "pos_line": [
            "+return json_tricks.load(file)"
        ],
        "core_change": "-return json.load(file) +return json_tricks.load(file)",
        "core_API": "exists"
    },
    {
        "commit_hash": "54e55b52d4886d4c63e592310b4253e01c606285",
        "index": "4ea4d808a..4860fce72 100644",
        "commit_message": "Fixing conversation test for torch 1.8 (#10545)\n\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SimpleConversationPipelineTests(unittest.TestCase):",
            "model = GPT2LMHeadModel(config)",
            "# Force model output to be L",
            "V, D = model.lm_head.weight.shape",
            "-        bias = torch.zeros(V, requires_grad=True)",
            "-        weight = torch.zeros((V, D), requires_grad=True)",
            "+        bias = torch.zeros(V)",
            "bias[76] = 1",
            "+        weight = torch.zeros((V, D), requires_grad=True)",
            "",
            "model.lm_head.bias = torch.nn.Parameter(bias)",
            "model.lm_head.weight = torch.nn.Parameter(weight)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=expression_statement), node=ASTNode(type=module), position=4)",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=requires_grad))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=true, text=True))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 6,
        "number": 7268,
        "neg_line": [
            "-bias = torch.zeros(V, requires_grad=True)",
            "-weight = torch.zeros((V, D), requires_grad=True)"
        ],
        "pos_line": [
            "+bias = torch.zeros(V)",
            "+weight = torch.zeros((V, D), requires_grad=True)"
        ],
        "core_change": "-bias = torch.zeros(V, requires_grad=True) -weight = torch.zeros((V, D), requires_grad=True) +bias = torch.zeros(V) +weight = torch.zeros((V, D), requires_grad=True)",
        "core_API": "zeros"
    },
    {
        "commit_hash": "4ee0bea795470699da5dbf1fd75f3e741baee877",
        "index": "77918b48f..dfae40d31 100644",
        "commit_message": "bugfix\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TransLoss(torch.nn.Module):",
            "loss (torch.Tensor): transducer loss",
            "",
            "\"\"\"",
            "-        if trans_type == \"warp-transducer\" and pred_pad.dtype != torch.float32:",
            "+        if self.trans_type == \"warp-transducer\" and pred_pad.dtype != torch.float32:",
            "# warprnnt_pytorch only supports float32",
            "pred_pad = pred_pad.to(dtype=torch.float32)",
            "if self.trans_type == \"warp-rnnt\":"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 7270,
        "neg_line": [
            "-if trans_type == \"warp-transducer\" and pred_pad.dtype != torch.float32:"
        ],
        "pos_line": [
            "+if self.trans_type == \"warp-transducer\" and pred_pad.dtype != torch.float32:"
        ],
        "core_change": "-if trans_type == \"warp-transducer\" and pred_pad.dtype != torch.float32: +if self.trans_type == \"warp-transducer\" and pred_pad.dtype != torch.float32:",
        "core_API": "to"
    },
    {
        "commit_hash": "13a5b7db992ad5cf77895e4c0f76595314390418",
        "index": "296493bf1..36735619d 100644",
        "commit_message": "Fix ArrowWriter closes stream at exit (#1971)\n\n* Test ArrowWriter closes stream at exit\n\n* Implement ArrowWriter as context manager and close stream at exit\n\n* Specify exception class\n\n* Use ArrowWriter as context manager\n\n* Fix ArrowWriter.close\n\n* Fix ArrowWriter.close\n\n* Test stream already closed at ArrowWriter context exit\n\n* Revert \"Use ArrowWriter as context manager\"\n\nThis reverts commit b28967fe0bd0bed1cf460eba7f789f1ff8f8370a.\n\n* Use ArrowWriter as context manager in tests\n\n* Generalize Exception type in ArrowWriter.close\n\n* Use ArrowWriter as context manager in benchmarks\n\n* Use ArrowWriter as context manager in arrow_writer and builder\n\n* Use ArrowWriter as context manager in arrow_dataset\n\n* Use ArrowWriter as context manager in metric\n\n* Revert \"Use ArrowWriter as context manager in metric\"\n\nThis reverts commit 7f287fb380479ee829dda2296188080774b72621.\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "def generate_examples(features: dict, num_examples=100, seq_shapes=None):",
            "def generate_example_dataset(dataset_path, features, num_examples=100, seq_shapes=None):",
            "dummy_data = generate_examples(features, num_examples=num_examples, seq_shapes=seq_shapes)",
            "",
            "-    writer = datasets.ArrowWriter(features=features, path=dataset_path)",
            "-    for key, record in dummy_data:",
            "-        example = features.encode_example(record)",
            "-        writer.write(example)",
            "+    with datasets.ArrowWriter(features=features, path=dataset_path) as writer:",
            "+        for key, record in dummy_data:",
            "+            example = features.encode_example(record)",
            "+            writer.write(example)",
            "",
            "-    num_final_examples, num_bytes = writer.finalize()",
            "+        num_final_examples, num_bytes = writer.finalize()",
            "",
            "assert (",
            "num_final_examples == num_examples"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('with_statement', None), position=3, insert_id=1786655)",
            "Insert(target_node=IN(type=with_statement), node=('with', 'with'), position=0, insert_id=1786656)",
            "Insert(target_node=IN(type=with_statement), node=('with_clause', None), position=1, insert_id=1786657)",
            "Insert(target_node=IN(type=with_statement), node=(':', ':'), position=2, insert_id=1786658)",
            "Insert(target_node=IN(type=with_statement), node=('block', None), position=3, insert_id=1786659)",
            "Insert(target_node=IN(type=with_clause), node=('with_item', None), position=0, insert_id=1786660)",
            "Move(target_node=IN(type=block), node=ASTNode(type=for_statement), position=0)",
            "Move(target_node=IN(type=block), node=ASTNode(type=expression_statement), position=1)",
            "Insert(target_node=IN(type=with_item), node=('as_pattern', None), position=0, insert_id=1786661)",
            "Move(target_node=IN(type=as_pattern), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=as_pattern), node=('as', 'as'), position=1, insert_id=1786662)",
            "Insert(target_node=IN(type=as_pattern), node=('as_pattern_target', None), position=2, insert_id=1786663)",
            "Insert(target_node=IN(type=as_pattern_target), node=('identifier', 'writer'), position=0, insert_id=1786664)",
            "Delete(target_node=ASTNode(type=identifier, text=writer))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 5,
        "minus_line": 5,
        "AST_diff_line": 17,
        "number": 7271,
        "neg_line": [
            "-writer = datasets.ArrowWriter(features=features, path=dataset_path)",
            "-for key, record in dummy_data:",
            "-example = features.encode_example(record)",
            "-writer.write(example)",
            "-num_final_examples, num_bytes = writer.finalize()"
        ],
        "pos_line": [
            "+with datasets.ArrowWriter(features=features, path=dataset_path) as writer:",
            "+for key, record in dummy_data:",
            "+example = features.encode_example(record)",
            "+writer.write(example)",
            "+num_final_examples, num_bytes = writer.finalize()"
        ],
        "core_change": "-writer = datasets.ArrowWriter(features=features, path=dataset_path) -for key, record in dummy_data: -example = features.encode_example(record) -writer.write(example) +with datasets.ArrowWriter(features=features, path=dataset_path) as writer: +for key, record in dummy_data: +example = features.encode_example(record) +writer.write(example) -num_final_examples, num_bytes = writer.finalize() +num_final_examples, num_bytes = writer.finalize()",
        "core_API": "ArrowWriter"
    },
    {
        "commit_hash": "2e6db6fba3cf1fb13aa31b1f7a77f9ccac9c4e74",
        "index": "72276707d..a304bf683 100644",
        "commit_message": "style: code style fixes (#1046)\n\n* fix: automatic code style correction\n\n* fix: squad_iterator\n\n* fix: delete document_bert_ner_iterator\n\n* fix: revert json files to dev version\n\n* fix: removed from registry\n\n* refactor: fix merge mistakes\n\n",
        "file": "DeepPavlov.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def make_module_spec(options, weight_file):",
            "",
            "# Input placeholders to the biLM.",
            "tokens = tf.placeholder(shape=(None, None), dtype=tf.string, name='ph2tokens')",
            "-        sequence_len = tf.placeholder(shape=(None, ), dtype=tf.int32, name='ph2sequence_len')",
            "+        sequence_len = tf.placeholder(shape=(None,), dtype=tf.int32, name='ph2sequence_len')",
            "",
            "tok_shape = tf.shape(tokens)",
            "line_tokens = tf.reshape(tokens, shape=[-1], name='reshape2line_tokens')"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 7274,
        "neg_line": [
            "-sequence_len = tf.placeholder(shape=(None, ), dtype=tf.int32, name='ph2sequence_len')"
        ],
        "pos_line": [
            "+sequence_len = tf.placeholder(shape=(None,), dtype=tf.int32, name='ph2sequence_len')"
        ],
        "core_change": "-sequence_len = tf.placeholder(shape=(None, ), dtype=tf.int32, name='ph2sequence_len') +sequence_len = tf.placeholder(shape=(None,), dtype=tf.int32, name='ph2sequence_len')",
        "core_API": "placeholder"
    },
    {
        "commit_hash": "4e7a854dd07db4453539f956f998a3fda306eb50",
        "index": "1deff27..a37f506 100644",
        "commit_message": "Update helpers.py\n\nFixing out of memory error by loading the checkpoint onto the CPU.\n",
        "file": "pytorch-image-models.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def resume_checkpoint(model, checkpoint_path):",
            "optimizer_state = None",
            "resume_epoch = None",
            "if os.path.isfile(checkpoint_path):",
            "-        checkpoint = torch.load(checkpoint_path)",
            "+        checkpoint = torch.load(checkpoint_path, map_location='cpu')",
            "if isinstance(checkpoint, dict) and 'state_dict' in checkpoint:",
            "new_state_dict = OrderedDict()",
            "for k, v in checkpoint['state_dict'].items():"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=900062)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=900063)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'map_location'), position=0, insert_id=900064)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=900065)",
            "Insert(target_node=IN(type=keyword_argument), node=('string', \"'cpu'\"), position=2, insert_id=900066)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 7276,
        "neg_line": [
            "-checkpoint = torch.load(checkpoint_path)"
        ],
        "pos_line": [
            "+checkpoint = torch.load(checkpoint_path, map_location='cpu')"
        ],
        "core_change": "-checkpoint = torch.load(checkpoint_path) +checkpoint = torch.load(checkpoint_path, map_location='cpu')",
        "core_API": "isfile"
    },
    {
        "commit_hash": "89879e2c76e86c685e44c47a6cdb82f7e645c142",
        "index": "cabfa7b6..27719b92 100644",
        "commit_message": "Fix keras docstrings\n\nPiperOrigin-RevId: 424275818\n\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TruncatedNormal(tf.compat.v1.truncated_normal_initializer):",
            ">>> a = initializer(shape=(2, 2))",
            ">>> b = initializer(shape=(2, 2))",
            ">>> tf.reduce_sum(a - b) == 0",
            "-  <tf.Tensor: shape=(), dtype=bool, numpy=True>",
            "+  <tf.Tensor: shape=(), dtype=bool, numpy=False>",
            "",
            "@end_compatibility",
            "\"\"\""
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=comparison_operator), node=('false', 'False'), position=0, insert_id=2069279)",
            "Delete(target_node=ASTNode(type=true, text=True))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 7279,
        "neg_line": [
            "-<tf.Tensor: shape=(), dtype=bool, numpy=True>"
        ],
        "pos_line": [
            "+<tf.Tensor: shape=(), dtype=bool, numpy=False>"
        ],
        "core_change": "-<tf.Tensor: shape=(), dtype=bool, numpy=True> +<tf.Tensor: shape=(), dtype=bool, numpy=False>",
        "core_API": "reduce_sum"
    },
    {
        "commit_hash": "3b3024da70a7ada6599390c5b3e1a721c9a4aa4c",
        "index": "8eb7ac779..455fdc34f 100644",
        "commit_message": "TF port of ESM (#19587)\n\n* Partial TF port for ESM model\n\n* Add ESM-TF tests\n\n* Add the various imports for TF-ESM\n\n* TF weight conversion almost ready\n\n* Stop ignoring the decoder weights in PT\n\n* Add tests and lots of fixes\n\n* fix-copies\n\n* Fix imports, add model docs\n\n* Add get_vocab() to tokenizer\n\n* Fix vocab links for pretrained files\n\n* Allow multiple inputs with a sep\n\n* Use EOS as SEP token because ESM vocab lacks SEP\n\n* Correctly return special tokens mask from ESM tokenizer\n\n* make fixup\n\n* Stop testing unsupported embedding resizing\n\n* Handle TF bias correctly\n\n* Skip all models with slow tokenizers in the token classification test\n\n* Fixing the batch/unbatcher of pipelines to accomodate the `None` being\n\npassed around.\n\n* Fixing pipeline bug caused by slow tokenizer  being different.\n\n* Update src/transformers/models/esm/modeling_tf_esm.py\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>\n\n* Update src/transformers/models/esm/modeling_tf_esm.py\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>\n\n* Update src/transformers/models/esm/modeling_tf_esm.py\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>\n\n* Update set_input_embeddings and the copyright notices\n\nCo-authored-by: Your Name <you@example.com>\nCo-authored-by: Nicolas Patry <patry.nicolas@protonmail.com>\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class PipelineIterator(IterableDataset):",
            "elif isinstance(element[0], np.ndarray):",
            "loader_batched[k] = tuple(np.expand_dims(el[self._loader_batch_index], 0) for el in element)",
            "continue",
            "-                if isinstance(element[self._loader_batch_index], torch.Tensor):",
            "+                if element is None:",
            "+                    # This can happen for optional data that get passed around",
            "+                    loader_batched[k] = None",
            "+                elif isinstance(element[self._loader_batch_index], torch.Tensor):",
            "# Take correct batch data, but make it looked like batch_size=1",
            "# For compatibility with other methods within transformers"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=if_statement), node=('comparison_operator', None), position=1, insert_id=1188697)",
            "Insert(target_node=ASTNode(type=if_statement), node=(':', ':'), position=2, insert_id=1188698)",
            "Insert(target_node=ASTNode(type=if_statement), node=('block', None), position=3, insert_id=1188699)",
            "Insert(target_node=ASTNode(type=if_statement), node=('elif_clause', None), position=4, insert_id=1188700)",
            "Insert(target_node=IN(type=comparison_operator), node=('identifier', 'element'), position=0, insert_id=1188701)",
            "Insert(target_node=IN(type=comparison_operator), node=('is', 'is'), position=1, insert_id=1188702)",
            "Insert(target_node=IN(type=comparison_operator), node=('none', 'None'), position=2, insert_id=1188703)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=1188704)",
            "Insert(target_node=IN(type=elif_clause), node=('elif', 'elif'), position=0, insert_id=1188705)",
            "Move(target_node=IN(type=elif_clause), node=ASTNode(type=call), position=1)",
            "Move(target_node=IN(type=elif_clause), node=ASTNode(type=:, text=:), position=2)",
            "Move(target_node=IN(type=elif_clause), node=ASTNode(type=block, text=), position=3)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1188706)",
            "Insert(target_node=IN(type=assignment), node=('subscript', None), position=0, insert_id=1188707)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1188708)",
            "Insert(target_node=IN(type=assignment), node=('none', 'None'), position=2, insert_id=1188709)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'loader_batched'), position=0, insert_id=1188710)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=1188711)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'k'), position=2, insert_id=1188712)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=1188713)"
        ],
        "plus_line": 4,
        "minus_line": 1,
        "AST_diff_line": 20,
        "number": 7282,
        "neg_line": [
            "-if isinstance(element[self._loader_batch_index], torch.Tensor):"
        ],
        "pos_line": [
            "+if element is None:",
            "+# This can happen for optional data that get passed around",
            "+loader_batched[k] = None",
            "+elif isinstance(element[self._loader_batch_index], torch.Tensor):"
        ],
        "core_change": "-if isinstance(element[self._loader_batch_index], torch.Tensor): +if element is None: +# This can happen for optional data that get passed around +loader_batched[k] = None +elif isinstance(element[self._loader_batch_index], torch.Tensor):",
        "core_API": "expand_dims"
    },
    {
        "commit_hash": "90e98e56c46bc466d4ad7712bab93566afe5d1d0",
        "index": "651bd5d3..e69c1e9e 100644",
        "commit_message": "Fix doc formatting and improve doc styling (#4072)\n\n* switch to pydoc-markdown with custom processor\n\n* add some extra css\n\n* fixes\n\n* fixes\n\n* minor tweaks\n\n* fixes\n\n* add breadcrumbs\n\n* fixes\n\n* fix arg formatting\n\n* fix\n\n* fixes\n\n* more fixes\n\n* fix\n\n* fix cross-refs within module\n\n* fix dev requirements\n\n* pin pydoc-markdown to latest commit\n\n* small refactor and docstring fixes\n\n* more small fixes\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ElmoTokenEmbedder(TokenEmbedder):",
            "",
            "# Returns",
            "",
            "-        The ELMo representations for the input sequence, shape",
            "-        `(batch_size, timesteps, embedding_dim)`",
            "+        `torch.Tensor`",
            "+            The ELMo representations for the input sequence, shape",
            "+            `(batch_size, timesteps, embedding_dim)`",
            "\"\"\"",
            "elmo_output = self._elmo(tokens, word_inputs)",
            "elmo_representations = elmo_output[\"elmo_representations\"][0]"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('ERROR', None), position=0, insert_id=18559)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=class_definition), position=0)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=\", text=\"\"\"), position=1)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=], text=]), position=2)",
            "Insert(target_node=ASTNode(type=class_definition), node=('block', None), position=5, insert_id=18560)",
            "Insert(target_node=ASTNode(type=ERROR), node=('expression_statement', None), position=0, insert_id=18561)",
            "Insert(target_node=ASTNode(type=ERROR), node=('ERROR', None), position=1, insert_id=18562)",
            "Insert(target_node=ASTNode(type=ERROR), node=('pattern_list', None), position=5, insert_id=18563)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=18564)",
            "Insert(target_node=IN(type=expression_statement), node=('string', '`torch.Tensor`'), position=0, insert_id=18565)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=identifier, text=The), position=0)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=identifier, text=ELMo), position=1)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=identifier, text=representations), position=2)",
            "Move(target_node=IN(type=pattern_list), node=ASTNode(type=identifier, text=sequence), position=0)",
            "Move(target_node=IN(type=pattern_list), node=ASTNode(type=,, text=,), position=1)",
            "Move(target_node=IN(type=pattern_list), node=ASTNode(type=identifier, text=shape), position=2)",
            "Insert(target_node=IN(type=expression_statement), node=('string', '`(batch_size, timesteps, embedding_dim)`'), position=0, insert_id=18566)",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=string, text=`(batch_size, timesteps, embedding_dim)`))",
            "Delete(target_node=ASTNode(type=block, text=))",
            "Delete(target_node=ASTNode(type=ERROR))"
        ],
        "plus_line": 3,
        "minus_line": 2,
        "AST_diff_line": 21,
        "number": 7291,
        "neg_line": [
            "-The ELMo representations for the input sequence, shape",
            "-`(batch_size, timesteps, embedding_dim)`"
        ],
        "pos_line": [
            "+`torch.Tensor`",
            "+The ELMo representations for the input sequence, shape",
            "+`(batch_size, timesteps, embedding_dim)`"
        ],
        "core_change": "-The ELMo representations for the input sequence, shape -`(batch_size, timesteps, embedding_dim)` +`torch.Tensor` +The ELMo representations for the input sequence, shape +`(batch_size, timesteps, embedding_dim)`",
        "core_API": "_elmo"
    },
    {
        "commit_hash": "5f420b381fdd50a6a0a4b7d3e493fd69c7ec6d0f",
        "index": "9ab23203..53446c68 100644",
        "commit_message": "fix step increment\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class DistributedModel(object):",
            "",
            "grad_var_list = list(zip(self.gradients, self.global_network.get_variables()))",
            "",
            "-            global_step_inc = self.global_step.assign_add(self.batch_size)",
            "+            global_step_inc = self.global_step.assign_add(tf.shape(self.state)[0])",
            "",
            "self.assign_global_to_local = tf.group(*[v1.assign(v2) for v1, v2 in",
            "zip(self.local_network.get_variables(),"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('subscript', None), position=1, insert_id=2246298)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=2, insert_id=2246299)",
            "Insert(target_node=IN(type=subscript), node=('call', None), position=0, insert_id=2246300)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=2246301)",
            "Insert(target_node=IN(type=subscript), node=('integer', '0'), position=2, insert_id=2246302)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=2246303)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2246304)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2246305)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2246306)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'shape'), position=2, insert_id=2246307)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2246308)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=attribute), position=1)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=), text=)), position=2)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=2246309)",
            "Update(target_node=ASTNode(type=identifier, text=batch_size), value='state')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 16,
        "number": 7293,
        "neg_line": [
            "-global_step_inc = self.global_step.assign_add(self.batch_size)"
        ],
        "pos_line": [
            "+global_step_inc = self.global_step.assign_add(tf.shape(self.state)[0])"
        ],
        "core_change": "-global_step_inc = self.global_step.assign_add(self.batch_size) +global_step_inc = self.global_step.assign_add(tf.shape(self.state)[0])",
        "core_API": "get_variables"
    },
    {
        "commit_hash": "e557e627fb1581a46018f7e9be6672880d71f9a0",
        "index": "11ebc4d2..a5511ed8 100644",
        "commit_message": "fix example docs style (#517)\n\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "with tf.Session() as sess:",
            "",
            "## Note that, the rewards here with random action",
            "running_reward = rAll if running_reward is None else running_reward * 0.99 + rAll * 0.01",
            "-        print(",
            "-            \"Episode [%d/%d] sum reward:%f running reward:%f took:%.5fs %s\" %",
            "-            (i, num_episodes, rAll, running_reward, time.time() - episode_time, '' if rAll == 0 else ' !!!!!!!!')",
            "-        )",
            "+        print(\"Episode [%d/%d] sum reward:%f running reward:%f took:%.5fs %s\" % \\",
            "+            (i, num_episodes, rAll, running_reward, time.time() - episode_time, '' if rAll == 0 else ' !!!!!!!!'))"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 4,
        "AST_diff_line": 17,
        "number": 7296,
        "neg_line": [
            "-print(",
            "-\"Episode [%d/%d] sum reward:%f running reward:%f took:%.5fs %s\" %",
            "-(i, num_episodes, rAll, running_reward, time.time() - episode_time, '' if rAll == 0 else ' !!!!!!!!')",
            "-)"
        ],
        "pos_line": [
            "+print(\"Episode [%d/%d] sum reward:%f running reward:%f took:%.5fs %s\" % \\",
            "+(i, num_episodes, rAll, running_reward, time.time() - episode_time, '' if rAll == 0 else ' !!!!!!!!'))"
        ],
        "core_change": "-print( -\"Episode [%d/%d] sum reward:%f running reward:%f took:%.5fs %s\" % -(i, num_episodes, rAll, running_reward, time.time() - episode_time, '' if rAll == 0 else ' !!!!!!!!') -) +print(\"Episode [%d/%d] sum reward:%f running reward:%f took:%.5fs %s\" % \\ +(i, num_episodes, rAll, running_reward, time.time() - episode_time, '' if rAll == 0 else ' !!!!!!!!'))",
        "core_API": "Session"
    },
    {
        "commit_hash": "68cb6994ebe8af72e7fe7ffb826e070cb8a2cec6",
        "index": "711c2455..c67cc49e 100755",
        "commit_message": "bug fix in HED example\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Model(ModelDesc):",
            "for idx, b in enumerate([b1, b2, b3, b4, b5, final_map]):",
            "output = tf.nn.sigmoid(b, name='output{}'.format(idx+1))",
            "xentropy = class_balanced_sigmoid_cross_entropy(",
            "-                b, edgemap,",
            "+                tf.squeeze(b, [3]), edgemap,",
            "name='xentropy{}'.format(idx+1))",
            "costs.append(xentropy)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=1, insert_id=2309689)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=2309690)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2309691)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2309692)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2309693)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2309694)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'squeeze'), position=2, insert_id=2309695)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2309696)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=b), position=1)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=,, text=,), position=2)",
            "Insert(target_node=IN(type=argument_list), node=('list', None), position=3, insert_id=2309697)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=2309698)",
            "Insert(target_node=IN(type=list), node=('[', '['), position=0, insert_id=2309699)",
            "Insert(target_node=IN(type=list), node=('integer', '3'), position=1, insert_id=2309700)",
            "Insert(target_node=IN(type=list), node=(']', ']'), position=2, insert_id=2309701)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 15,
        "number": 7298,
        "neg_line": [
            "-b, edgemap,"
        ],
        "pos_line": [
            "+tf.squeeze(b, [3]), edgemap,"
        ],
        "core_change": "-b, edgemap, +tf.squeeze(b, [3]), edgemap,",
        "core_API": "sigmoid"
    },
    {
        "commit_hash": "acbe0b4e5f6cff6ce8fe8496d2c225b808dba9b1",
        "index": "3032bbb3f8..028a2e266d 100644",
        "commit_message": "Fix twin q bug (#4108)\n\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class ActorCriticLoss(object):",
            "",
            "q_t_selected = tf.squeeze(q_t, axis=len(q_t.shape) - 1)",
            "if twin_q:",
            "-            twin_q_t_selected = tf.squeeze(q_t, axis=len(q_t.shape) - 1)",
            "+            twin_q_t_selected = tf.squeeze(twin_q_t, axis=len(q_t.shape) - 1)",
            "q_tp1 = tf.minimum(q_tp1, twin_q_tp1)",
            "",
            "q_tp1_best = tf.squeeze(input=q_tp1, axis=len(q_tp1.shape) - 1)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=q_t), value='twin_q_t')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 7300,
        "neg_line": [
            "-twin_q_t_selected = tf.squeeze(q_t, axis=len(q_t.shape) - 1)"
        ],
        "pos_line": [
            "+twin_q_t_selected = tf.squeeze(twin_q_t, axis=len(q_t.shape) - 1)"
        ],
        "core_change": "-twin_q_t_selected = tf.squeeze(q_t, axis=len(q_t.shape) - 1) +twin_q_t_selected = tf.squeeze(twin_q_t, axis=len(q_t.shape) - 1)",
        "core_API": "squeeze"
    },
    {
        "commit_hash": "66d4b129903058365f6cf1d4fb4ac73814ca3bfc",
        "index": "a0204166..9a161e1d 100644",
        "commit_message": "Fix examples and tutorials to use the updated tensor API (#886)\n\n* Fix examples and tutorials to use the updated tensor API\n\n* small fixes\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "\"source\": [",
            "\"def model(xs, ys=None):\\n\",",
            "\"    # sample z from the prior \\n\",",
            "-    \"    prior_mu = Variable(torch.zeros([batch_size, z_dim]))\\n\",",
            "-    \"    prior_sigma = Variable(torch.ones([batch_size, z_dim]))\\n\",",
            "+    \"    prior_mu = torch.zeros([batch_size, z_dim])\\n\",",
            "+    \"    prior_sigma = torch.ones([batch_size, z_dim])\\n\",",
            "\"    zs = pyro.sample(\\\"z\\\", dist.Normal(prior_mu, prior_sigma))\\n\",",
            "\"\\n\",",
            "\"    # if the label y is observed, sample from the prior.\\n\",",
            "\"    # otherwise, observe the value \\n\",",
            "-    \"    alpha_prior = Variable(torch.ones([batch_size, 10]) / (10.))\\n\",",
            "+    \"    alpha_prior = torch.ones([batch_size, 10]) / (10.)\\n\",",
            "\"    if ys is None:\\n\",",
            "\"        ys = pyro.sample(\\\"y\\\", dist.OneHotCategorical(alpha_prior))\\n\",",
            "\"    else:\\n\","
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"    prior_mu = Variable(torch.zeros([batch_size, z_dim]))\\n\"), value='\"    prior_mu = torch.zeros([batch_size, z_dim])\\\\n\"')",
            "Update(target_node=ASTNode(type=string, text=\"    prior_sigma = Variable(torch.ones([batch_size, z_dim]))\\n\"), value='\"    prior_sigma = torch.ones([batch_size, z_dim])\\\\n\"')",
            "Update(target_node=ASTNode(type=string, text=\"    alpha_prior = Variable(torch.ones([batch_size, 10]) / (10.))\\n\"), value='\"    alpha_prior = torch.ones([batch_size, 10]) / (10.)\\\\n\"')"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 3,
        "number": 7305,
        "neg_line": [
            "-\"    prior_mu = Variable(torch.zeros([batch_size, z_dim]))\\n\",",
            "-\"    prior_sigma = Variable(torch.ones([batch_size, z_dim]))\\n\",",
            "-\"    alpha_prior = Variable(torch.ones([batch_size, 10]) / (10.))\\n\","
        ],
        "pos_line": [
            "+\"    prior_mu = torch.zeros([batch_size, z_dim])\\n\",",
            "+\"    prior_sigma = torch.ones([batch_size, z_dim])\\n\",",
            "+\"    alpha_prior = torch.ones([batch_size, 10]) / (10.)\\n\","
        ],
        "core_change": "-\"    prior_mu = Variable(torch.zeros([batch_size, z_dim]))\\n\", -\"    prior_sigma = Variable(torch.ones([batch_size, z_dim]))\\n\", +\"    prior_mu = torch.zeros([batch_size, z_dim])\\n\", +\"    prior_sigma = torch.ones([batch_size, z_dim])\\n\", -\"    alpha_prior = Variable(torch.ones([batch_size, 10]) / (10.))\\n\", +\"    alpha_prior = torch.ones([batch_size, 10]) / (10.)\\n\",",
        "core_API": "zeros"
    },
    {
        "commit_hash": "ac9185d41b8e0a2ec6aa2642de055cd3bd6aa425",
        "index": "5e5e4ba180..edcfa09067 100644",
        "commit_message": "lintfixbot: Auto-commit fixed lint errors in codebase\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "from .. import backend_version",
            "",
            "",
            "@with_unsupported_dtypes({\"1.11.0 and below\": (\"float16\",)}, backend_version)",
            "-def l2_normalize(x: torch.Tensor,",
            "-                 axis: int = None,",
            "-                 out: torch.Tensor = None",
            "-                 ) -> torch.Tensor:",
            "+def l2_normalize(",
            "+    x: torch.Tensor, axis: int = None, out: torch.Tensor = None",
            "+) -> torch.Tensor:",
            "",
            "return torch.nn.functional.normalize(x, p=2, dim=axis, out=out)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 17,
        "number": 7307,
        "neg_line": [
            "-def l2_normalize(x: torch.Tensor,",
            "-axis: int = None,",
            "-out: torch.Tensor = None",
            "-) -> torch.Tensor:"
        ],
        "pos_line": [
            "+def l2_normalize(",
            "+x: torch.Tensor, axis: int = None, out: torch.Tensor = None",
            "+) -> torch.Tensor:"
        ],
        "core_change": "-def l2_normalize(x: torch.Tensor, -axis: int = None, -out: torch.Tensor = None -) -> torch.Tensor: +def l2_normalize( +x: torch.Tensor, axis: int = None, out: torch.Tensor = None +) -> torch.Tensor:",
        "core_API": "normalize"
    },
    {
        "commit_hash": "ce3093a3b219e1a7e3fe352e78484383d77f3d3d",
        "index": "1b832c8f..92486f93 100644",
        "commit_message": "PEP8 fixes.\n\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "layers",
        "change": [
            "y = img_input",
            "for i in range(nlayers):",
            "y_prepool = convresblock(y, nfeats=nfeats_all[i + 1], ksize=ksize)",
            "y = MaxPooling2D(pool_size=(pool_sizes[i], pool_sizes[i]))(y_prepool)",
            "-    wheres[i] = layers.Lambda(getwhere, output_shape=lambda x: x[0])([y_prepool, y])",
            "+    wheres[i] = layers.Lambda(",
            "+        getwhere, output_shape=lambda x: x[0])([y_prepool, y])",
            "",
            "# Now build the decoder, and use the stored 'where' masks to place the features",
            "for i in range(nlayers):"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 7315,
        "neg_line": [
            "-wheres[i] = layers.Lambda(getwhere, output_shape=lambda x: x[0])([y_prepool, y])"
        ],
        "pos_line": [
            "+wheres[i] = layers.Lambda(",
            "+getwhere, output_shape=lambda x: x[0])([y_prepool, y])"
        ],
        "core_change": "-wheres[i] = layers.Lambda(getwhere, output_shape=lambda x: x[0])([y_prepool, y]) +wheres[i] = layers.Lambda( +getwhere, output_shape=lambda x: x[0])([y_prepool, y])",
        "core_API": "Lambda"
    },
    {
        "commit_hash": "ccfcb3bbf4bf949e1cf8d2e18f615da77ac78f23",
        "index": "66a228fae..cd5c7d68d 100644",
        "commit_message": "fix error calculator for report false\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "cuda",
        "change": [
            "class E2E(ASRInterface, chainer.Chain):",
            "xs = xs.reshape(batch, -1, self.dims)",
            "xs = [xs[i, :ilens[i], :] for i in range(len(ilens))]",
            "loss_ctc = self.ctc(xs, ys_pad_cpu)",
            "-            with chainer.no_backprop_mode():",
            "-                ys_hat = chainer.backends.cuda.to_cpu(self.ctc.argmax(xs).data)",
            "-            cer_ctc = self.error_calculator(ys_hat, ys_pad_cpu, is_ctc=True)",
            "+            if self.error_calculator is not None:",
            "+                with chainer.no_backprop_mode():",
            "+                    ys_hat = chainer.backends.cuda.to_cpu(self.ctc.argmax(xs).data)",
            "+                cer_ctc = self.error_calculator(ys_hat, ys_pad_cpu, is_ctc=True)",
            "",
            "# Compute cer/wer",
            "with chainer.no_backprop_mode():"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=4, insert_id=1826174)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=1826175)",
            "Insert(target_node=IN(type=if_statement), node=('comparison_operator', None), position=1, insert_id=1826176)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=1826177)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=1826178)",
            "Insert(target_node=IN(type=comparison_operator), node=('attribute', None), position=0, insert_id=1826179)",
            "Insert(target_node=IN(type=comparison_operator), node=('is not', 'is'), position=1, insert_id=1826180)",
            "Insert(target_node=IN(type=comparison_operator), node=('is not', 'not'), position=2, insert_id=1826181)",
            "Insert(target_node=IN(type=comparison_operator), node=('none', 'None'), position=3, insert_id=1826182)",
            "Move(target_node=IN(type=block), node=ASTNode(type=with_statement), position=0)",
            "Move(target_node=IN(type=block), node=ASTNode(type=expression_statement), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=1826183)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1826184)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'error_calculator'), position=2, insert_id=1826185)"
        ],
        "plus_line": 4,
        "minus_line": 3,
        "AST_diff_line": 14,
        "number": 7317,
        "neg_line": [
            "-with chainer.no_backprop_mode():",
            "-ys_hat = chainer.backends.cuda.to_cpu(self.ctc.argmax(xs).data)",
            "-cer_ctc = self.error_calculator(ys_hat, ys_pad_cpu, is_ctc=True)"
        ],
        "pos_line": [
            "+if self.error_calculator is not None:",
            "+with chainer.no_backprop_mode():",
            "+ys_hat = chainer.backends.cuda.to_cpu(self.ctc.argmax(xs).data)",
            "+cer_ctc = self.error_calculator(ys_hat, ys_pad_cpu, is_ctc=True)"
        ],
        "core_change": "-with chainer.no_backprop_mode(): -ys_hat = chainer.backends.cuda.to_cpu(self.ctc.argmax(xs).data) -cer_ctc = self.error_calculator(ys_hat, ys_pad_cpu, is_ctc=True) +if self.error_calculator is not None: +with chainer.no_backprop_mode(): +ys_hat = chainer.backends.cuda.to_cpu(self.ctc.argmax(xs).data) +cer_ctc = self.error_calculator(ys_hat, ys_pad_cpu, is_ctc=True)",
        "core_API": "reshape"
    },
    {
        "commit_hash": "193237d906e794554ee13c042f5ce6ea2199bcfa",
        "index": "6fa359e4..41f2f908 100644",
        "commit_message": "py 2.7 fix\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def read_ply(path):",
            "face = None",
            "if 'face' in data:",
            "faces = data['face']['vertex_indices']",
            "-        faces = [torch.tensor(f, dtype=torch.long) for f in faces]",
            "+        faces = [torch.tensor(fa, dtype=torch.long) for fa in faces]",
            "face = torch.stack(faces, dim=-1)",
            "",
            "data = Data(pos=pos)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=f), value='fa')",
            "Update(target_node=ASTNode(type=identifier, text=f), value='fa')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 7323,
        "neg_line": [
            "-faces = [torch.tensor(f, dtype=torch.long) for f in faces]"
        ],
        "pos_line": [
            "+faces = [torch.tensor(fa, dtype=torch.long) for fa in faces]"
        ],
        "core_change": "-faces = [torch.tensor(f, dtype=torch.long) for f in faces] +faces = [torch.tensor(fa, dtype=torch.long) for fa in faces]",
        "core_API": "tensor"
    },
    {
        "commit_hash": "5866646cc8347e45c17ca1204f3e770712365f99",
        "index": "6b3c661..af0aa7d 100644",
        "commit_message": "Fix float zeros format (#5491)\n\n* Fix float zeros format\n\n* 255 to integer\n",
        "file": "yolov5.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def kmean_anchors(dataset='./data/coco128.yaml', n=9, img_size=640, thr=4.0, gen",
            "\"\"\"",
            "from scipy.cluster.vq import kmeans",
            "",
            "-    thr = 1. / thr",
            "+    thr = 1 / thr",
            "prefix = colorstr('autoanchor: ')",
            "",
            "def metric(k, wh):  # compute metrics",
            "r = wh[:, None] / k[None]",
            "-        x = torch.min(r, 1. / r).min(2)[0]  # ratio metric",
            "+        x = torch.min(r, 1 / r).min(2)[0]  # ratio metric",
            "# x = wh_iou(wh, torch.tensor(k))  # iou metric",
            "return x, x.max(1)[0]  # x, best_x"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=binary_operator), node=('integer', '1'), position=0, insert_id=1296318)",
            "Insert(target_node=ASTNode(type=binary_operator), node=('integer', '1'), position=0, insert_id=1296319)",
            "Delete(target_node=ASTNode(type=float, text=1.))",
            "Delete(target_node=ASTNode(type=float, text=1.))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 4,
        "number": 7328,
        "neg_line": [
            "-thr = 1. / thr",
            "-x = torch.min(r, 1. / r).min(2)[0]  # ratio metric"
        ],
        "pos_line": [
            "+thr = 1 / thr",
            "+x = torch.min(r, 1 / r).min(2)[0]  # ratio metric"
        ],
        "core_change": "-thr = 1. / thr +thr = 1 / thr -x = torch.min(r, 1. / r).min(2)[0]  # ratio metric +x = torch.min(r, 1 / r).min(2)[0]  # ratio metric",
        "core_API": "min"
    },
    {
        "commit_hash": "0ca37c9879ae36774ca508c7093c3840c37eb910",
        "index": "b767e674..5bf22571 100644",
        "commit_message": "Quick fixes\n\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TensorBoard(Callback):",
            "embedding_input = tf.reshape(embedding_input,",
            "(step, int(embedding_size)))",
            "shape = (self.embeddings_data[0].shape[0], int(embedding_size))",
            "-                    embedding = tf.Variable(tf.zeros(shape),",
            "-                                            name=layer.name + '_embedding')",
            "+                    embedding = K.variable(K.zeros(shape),",
            "+                                           name=layer.name + '_embedding')",
            "embeddings_vars[layer.name] = embedding",
            "batch = tf.assign(embedding[batch_id:batch_id + step],",
            "embedding_input)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tf), value='K')",
            "Update(target_node=ASTNode(type=identifier, text=Variable), value='variable')",
            "Update(target_node=ASTNode(type=identifier, text=tf), value='K')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 3,
        "number": 7333,
        "neg_line": [
            "-embedding = tf.Variable(tf.zeros(shape),",
            "-name=layer.name + '_embedding')"
        ],
        "pos_line": [
            "+embedding = K.variable(K.zeros(shape),",
            "+name=layer.name + '_embedding')"
        ],
        "core_change": "-embedding = tf.Variable(tf.zeros(shape), -name=layer.name + '_embedding') +embedding = K.variable(K.zeros(shape), +name=layer.name + '_embedding')",
        "core_API": "reshape"
    },
    {
        "commit_hash": "42c8edca54eb4f4999b7f95260991e8dec176bb0",
        "index": "9a00d6de..cd105f3e 100644",
        "commit_message": "Simplify logs management (#1696)\n\n* Move each haystack module's logger configuration into the respective file and configure the handlers properly\n\n* Implement most changes from #1714\n\n* Remove accidentally committed git merge tags ':D\n\n* Remove the debug logs capture feature\n\n* Remove more references to debug_logs\n\n* Fix issue with FARMReader that somehow made it to master\n\n* Add devices parameter to Inferencer\n\n* Change log of APEX message to DEBUG and lower the 'Starting <docstore>...' messages to DEBUG as well\n\n* Change log level of a few logs from modeling\n\n* Silence the transformers warning\n\n* Remove empty line below the workers :)\n\n* Fix two more levels in the tutorials logs\n\nCo-authored-by: github-actions[bot] <41898282+github-actions[bot]@users.noreply.github.com>\nCo-authored-by: bogdankostic <bogdankostic@web.de>\n",
        "file": "haystack.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Inferencer:",
            "",
            "model = AdaptiveModel.convert_from_transformers(model_name_or_path,",
            "revision=revision,",
            "-                                                            device=devices[0],",
            "+                                                            device=devices[0],  # type: ignore",
            "task_type=task_type,",
            "**kwargs)",
            "processor = Processor.convert_from_transformers(model_name_or_path,"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 7334,
        "neg_line": [
            "-device=devices[0],"
        ],
        "pos_line": [
            "+device=devices[0],  # type: ignore"
        ],
        "core_change": "-device=devices[0], +device=devices[0],  # type: ignore",
        "core_API": "convert_from_transformers"
    },
    {
        "commit_hash": "25725c85692893366d5ce9caaa6aa8abadd5f6c6",
        "index": "8a14f6d..4e4440d 100755",
        "commit_message": "bug fix\n\n",
        "file": "yolov3.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def build_targets(p, targets, model):",
            "if use_all_anchors:",
            "na = anchor_vec.shape[0]  # number of anchors",
            "a = torch.arange(na).view(-1, 1).repeat(1, nt).view(-1)",
            "-                t = targets.repeat(na, 1)",
            "+                t = t.repeat(na, 1)",
            "else:  # use best anchor only",
            "iou, a = iou.max(0)  # best iou and anchor"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=targets), value='t')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 7336,
        "neg_line": [
            "-t = targets.repeat(na, 1)"
        ],
        "pos_line": [
            "+t = t.repeat(na, 1)"
        ],
        "core_change": "-t = targets.repeat(na, 1) +t = t.repeat(na, 1)",
        "core_API": "arange"
    },
    {
        "commit_hash": "43043ee4d5c672f7fbc22ac9559e8164731fd053",
        "index": "c1889340fd..e7e4f4a208 100644",
        "commit_message": "[RLlib] Tf2x preparation; part 2 (upgrading `try_import_tf()`). (#9136)\n\n* WIP.\n\n* Fixes.\n\n* LINT.\n\n* WIP.\n\n* WIP.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* WIP.\n\n* Fixes.\n\n* Test\n\n* Fix.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* LINT.\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class LSTM(Model):",
            "if self.state_in:",
            "c_in, h_in = self.state_in",
            "else:",
            "-            c_in = tf.placeholder(",
            "+            c_in = tf1.placeholder(",
            "tf.float32, [None, lstm.state_size.c], name=\"c\")",
            "-            h_in = tf.placeholder(",
            "+            h_in = tf1.placeholder(",
            "tf.float32, [None, lstm.state_size.h], name=\"h\")",
            "self.state_in = [c_in, h_in]",
            "",
            "# Setup LSTM outputs",
            "-        state_in = tf.nn.rnn_cell.LSTMStateTuple(c_in, h_in)",
            "-        lstm_out, lstm_state = tf.nn.dynamic_rnn(",
            "+        state_in = tf1.nn.rnn_cell.LSTMStateTuple(c_in, h_in)",
            "+        lstm_out, lstm_state = tf1.nn.dynamic_rnn(",
            "lstm,",
            "last_layer,",
            "initial_state=state_in,"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tf), value='tf1')",
            "Update(target_node=ASTNode(type=identifier, text=tf), value='tf1')",
            "Update(target_node=ASTNode(type=identifier, text=tf), value='tf1')",
            "Update(target_node=ASTNode(type=identifier, text=tf), value='tf1')"
        ],
        "plus_line": 4,
        "minus_line": 4,
        "AST_diff_line": 4,
        "number": 7337,
        "neg_line": [
            "-c_in = tf.placeholder(",
            "-h_in = tf.placeholder(",
            "-state_in = tf.nn.rnn_cell.LSTMStateTuple(c_in, h_in)",
            "-lstm_out, lstm_state = tf.nn.dynamic_rnn("
        ],
        "pos_line": [
            "+c_in = tf1.placeholder(",
            "+h_in = tf1.placeholder(",
            "+state_in = tf1.nn.rnn_cell.LSTMStateTuple(c_in, h_in)",
            "+lstm_out, lstm_state = tf1.nn.dynamic_rnn("
        ],
        "core_change": "-c_in = tf.placeholder( +c_in = tf1.placeholder( -h_in = tf.placeholder( +h_in = tf1.placeholder( -state_in = tf.nn.rnn_cell.LSTMStateTuple(c_in, h_in) -lstm_out, lstm_state = tf.nn.dynamic_rnn( +state_in = tf1.nn.rnn_cell.LSTMStateTuple(c_in, h_in) +lstm_out, lstm_state = tf1.nn.dynamic_rnn(",
        "core_API": "placeholder"
    },
    {
        "commit_hash": "0eca74fa5f7bf82f3b93e3e38dd1d84cfedc5630",
        "index": "6e5aa62..1e216c4 100644",
        "commit_message": "lint fixes\n\nSummary:\nRan the linter.\nTODO: need to update the linter as per D21353065.\n\nReviewed By: bottler\n\nDifferential Revision: D21362270\n\nfbshipit-source-id: ad0e781de0a29f565ad25c43bc94a19b1828c020\n\n",
        "file": "pytorch3d.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestBlending(unittest.TestCase):",
            "# of the image with surrounding padded values.",
            "N, S, K = 1, 8, 2",
            "device = torch.device(\"cuda\")",
            "-        pix_to_face = -torch.ones((N, S, S, K), dtype=torch.int64, device=device)",
            "+        pix_to_face = torch.full(",
            "+            (N, S, S, K), fill_value=-1, dtype=torch.int64, device=device",
            "+        )",
            "h = int(S / 2)",
            "pix_to_face_full = torch.randint(",
            "size=(N, h, h, K), low=0, high=100, device=device"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=call), position=2)",
            "Update(target_node=ASTNode(type=identifier, text=ones), value='full')",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=925710)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=925711)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'fill_value'), position=0, insert_id=925712)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=925713)",
            "Insert(target_node=IN(type=keyword_argument), node=('unary_operator', '-1'), position=2, insert_id=925714)",
            "Delete(target_node=ASTNode(type=-, text=-))",
            "Delete(target_node=ASTNode(type=unary_operator))"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 7343,
        "neg_line": [
            "-pix_to_face = -torch.ones((N, S, S, K), dtype=torch.int64, device=device)"
        ],
        "pos_line": [
            "+pix_to_face = torch.full(",
            "+(N, S, S, K), fill_value=-1, dtype=torch.int64, device=device",
            "+)"
        ],
        "core_change": "-pix_to_face = -torch.ones((N, S, S, K), dtype=torch.int64, device=device) +pix_to_face = torch.full( +(N, S, S, K), fill_value=-1, dtype=torch.int64, device=device +)",
        "core_API": "device"
    },
    {
        "commit_hash": "181433458114523e83e079f6367c43e8f569ee5b",
        "index": "cfb6c67dc..bd5a45089 100644",
        "commit_message": "[bugfix] Always return batch indices to prevent duplicated logic for the users (#9432)\n\nCo-authored-by: Adrian Wälchli <aedu.waelchli@gmail.com>\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "dataloader",
        "change": [
            "class PredictionEpochLoop(Loop):",
            "self.current_batch_indices = batch_sampler.batch_indices",
            "if self.should_store_predictions:",
            "self._all_batch_indices.append(batch_sampler.batch_indices)",
            "+        else:",
            "+            warning_cache.warn(\"Lightning couldn't infer the indices fetched for your dataloader.\")"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=4, insert_id=1769345)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1769346)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'else'), position=0, insert_id=1769347)",
            "Insert(target_node=IN(type=assignment), node=(':', ':'), position=1, insert_id=1769348)",
            "Insert(target_node=IN(type=assignment), node=('type', None), position=2, insert_id=1769349)",
            "Insert(target_node=IN(type=type), node=('call', None), position=0, insert_id=1769350)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1769351)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1769352)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'warning_cache'), position=0, insert_id=1769353)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1769354)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'warn'), position=2, insert_id=1769355)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1769356)",
            "Insert(target_node=IN(type=argument_list), node=('string', '\"Lightning couldn\\'t infer the indices fetched for your dataloader.\"'), position=1, insert_id=1769357)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1769358)"
        ],
        "plus_line": 2,
        "minus_line": 0,
        "AST_diff_line": 14,
        "number": 7346,
        "neg_line": [],
        "pos_line": [
            "+else:",
            "+warning_cache.warn(\"Lightning couldn't infer the indices fetched for your dataloader.\")"
        ],
        "core_change": "+else: +warning_cache.warn(\"Lightning couldn't infer the indices fetched for your dataloader.\")",
        "core_API": "append"
    },
    {
        "commit_hash": "01b293b84fd2b1eeb80a01afe44d341d9c9c6486",
        "index": "8ebf8a9b..02b113dd 100644",
        "commit_message": "0.4.0 fixes\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class AttSplineConv(Module):",
            "is_open_spline = torch.ByteTensor(repeat_to(is_open_spline, dim))",
            "self.register_buffer('is_open_spline', is_open_spline)",
            "",
            "-        weight = torch.Tensor(kernel_size.prod(), in_channels, out_channels)",
            "-        self.weight = Parameter(weight)",
            "+        K = kernel_size.prod().item()",
            "+        self.weight = Parameter(torch.Tensor(K, in_channels, out_channels))",
            "",
            "self.root_weight = Parameter(torch.Tensor(in_channels, out_channels))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=weight), value='K')",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1074943)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=1074944)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=1074945)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1074946)",
            "Update(target_node=ASTNode(type=identifier, text=in_channels), value='item')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=in_channels), position=2)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1074947)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=), text=)), position=1)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1074948)",
            "Insert(target_node=IN(type=argument_list), node=('call', None), position=1, insert_id=1074949)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1074950)",
            "Move(target_node=IN(type=call), node=ASTNode(type=attribute), position=0)",
            "Move(target_node=IN(type=call), node=ASTNode(type=argument_list), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=weight), value='K')",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1074951)",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'in_channels'), position=3, insert_id=1074952)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=1074953)",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'out_channels'), position=5, insert_id=1074954)",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=out_channels))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 25,
        "number": 7348,
        "neg_line": [
            "-weight = torch.Tensor(kernel_size.prod(), in_channels, out_channels)",
            "-self.weight = Parameter(weight)"
        ],
        "pos_line": [
            "+K = kernel_size.prod().item()",
            "+self.weight = Parameter(torch.Tensor(K, in_channels, out_channels))"
        ],
        "core_change": "-weight = torch.Tensor(kernel_size.prod(), in_channels, out_channels) -self.weight = Parameter(weight) +K = kernel_size.prod().item() +self.weight = Parameter(torch.Tensor(K, in_channels, out_channels))",
        "core_API": "ByteTensor"
    },
    {
        "commit_hash": "5740c6120b3ebd249f95f78366ac02d6107b9d30",
        "index": "dc6bedfe..182d1e28 100644",
        "commit_message": "fix\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def tree_decomposition(mol):",
            "rows = [[i] * len(atom2clique[i]) for i in range(mol.GetNumAtoms())]",
            "row = torch.tensor(list(chain.from_iterable(rows)))",
            "col = torch.tensor(list(chain.from_iterable(atom2clique)))",
            "-    atom2clique = torch.stack([row, col], dim=0)",
            "+    atom2clique = torch.stack([row, col], dim=0).to(torch.long)",
            "",
            "return edge_index, atom2clique, len(cliques)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1033196)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=1033197)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1033198)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to'), position=2, insert_id=1033199)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1033200)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=1033201)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1033202)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1033203)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1033204)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'long'), position=2, insert_id=1033205)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 7356,
        "neg_line": [
            "-atom2clique = torch.stack([row, col], dim=0)"
        ],
        "pos_line": [
            "+atom2clique = torch.stack([row, col], dim=0).to(torch.long)"
        ],
        "core_change": "-atom2clique = torch.stack([row, col], dim=0) +atom2clique = torch.stack([row, col], dim=0).to(torch.long)",
        "core_API": "GetNumAtoms"
    },
    {
        "commit_hash": "ae32dee3f153887533b55158bdd2f3e1bf5460ac",
        "index": "7f9bf47e..ac976d72 100644",
        "commit_message": "lots of bug fixes\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "import scipy.spatial",
            "",
            "",
            "def nn_graph(pos, k=6):",
            "-    row = torch.arange(0, pos.size(0)).view(-1, 1).repeat(1, 6).view(-1).long()",
            "+    row = torch.arange(0, pos.size(0)).view(-1, 1).repeat(1, k).view(-1).long()",
            "tree = scipy.spatial.cKDTree(pos.numpy())",
            "_, col = tree.query(pos, k + 1)",
            "col = torch.LongTensor(col[:len(col)])[:, 1:].contiguous().view(-1)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'k'), position=3, insert_id=1081096)",
            "Delete(target_node=ASTNode(type=integer, text=6))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 7362,
        "neg_line": [
            "-row = torch.arange(0, pos.size(0)).view(-1, 1).repeat(1, 6).view(-1).long()"
        ],
        "pos_line": [
            "+row = torch.arange(0, pos.size(0)).view(-1, 1).repeat(1, k).view(-1).long()"
        ],
        "core_change": "-row = torch.arange(0, pos.size(0)).view(-1, 1).repeat(1, 6).view(-1).long() +row = torch.arange(0, pos.size(0)).view(-1, 1).repeat(1, k).view(-1).long()",
        "core_API": "arange"
    },
    {
        "commit_hash": "c37cb15c8b35f5844a493bd8a78241089c07406f",
        "index": "a8887c08..a7ace6d0 100644",
        "commit_message": "fix has_isolated_nodes for homogeneous graphs (#4138)\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def contains_isolated_nodes(edge_index, num_nodes=None):",
            "\"\"\"",
            "num_nodes = maybe_num_nodes(edge_index, num_nodes)",
            "edge_index, _ = remove_self_loops(edge_index)",
            "-    return torch.unique(edge_index.view(-1)).size(0) < num_nodes",
            "+    return torch.unique(edge_index.view(-1)).numel() < num_nodes",
            "",
            "",
            "def remove_isolated_nodes(edge_index, edge_attr=None, num_nodes=None):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=size), value='numel')",
            "Delete(target_node=ASTNode(type=integer, text=0))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 7363,
        "neg_line": [
            "-return torch.unique(edge_index.view(-1)).size(0) < num_nodes"
        ],
        "pos_line": [
            "+return torch.unique(edge_index.view(-1)).numel() < num_nodes"
        ],
        "core_change": "-return torch.unique(edge_index.view(-1)).size(0) < num_nodes +return torch.unique(edge_index.view(-1)).numel() < num_nodes",
        "core_API": "unique"
    },
    {
        "commit_hash": "da3aceb876e0f92b65405a20198836c08ee80d10",
        "index": "f5a53d47..c2cb1e96 100644",
        "commit_message": "Add fp16 support (#963)\n\n* Update imgwarp.py\n\n* update fix\n\n* move _torch_inverse_cast under utils\n\n* fix some tests in pyramid\n\n* fix issues\n\n* implement other autocasting functions\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def zca_mean(inp: torch.Tensor, dim: int = 0,",
            "else:",
            "cov = cov / float(N)",
            "",
            "-    U, S, _ = torch.svd(cov)",
            "+    U, S, _ = _torch_svd_cast(cov)",
            "",
            "S = S.reshape(-1, 1)",
            "S_inv_root: torch.Tensor = torch.rsqrt(S + eps)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=torch), value='_torch_svd_cast')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=torch), position=0)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=svd))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 7364,
        "neg_line": [
            "-U, S, _ = torch.svd(cov)"
        ],
        "pos_line": [
            "+U, S, _ = _torch_svd_cast(cov)"
        ],
        "core_change": "-U, S, _ = torch.svd(cov) +U, S, _ = _torch_svd_cast(cov)",
        "core_API": "svd"
    },
    {
        "commit_hash": "c2c9dddf6d99bfa14b8ffb65b507a6be50b0ad6e",
        "index": "62172752..0c29dd3f 100755",
        "commit_message": "Fixed general problems with variable handling and various related changes, removed DDQN\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Queue(Memory):",
            "updates=internal",
            "))",
            "for name, action in actions.items():",
            "-                assignments.append(tf.scatter_update(ref=self.actions_memory[name], indices=indices, updates=action))",
            "+                assignments.append(tf.scatter_update(",
            "+                    ref=self.actions_memory[name],",
            "+                    indices=indices,",
            "+                    updates=action",
            "+                ))",
            "assignments.append(tf.scatter_update(ref=self.terminal_memory, indices=indices, updates=terminal))",
            "assignments.append(tf.scatter_update(ref=self.reward_memory, indices=indices, updates=reward))"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 5,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 7367,
        "neg_line": [
            "-assignments.append(tf.scatter_update(ref=self.actions_memory[name], indices=indices, updates=action))"
        ],
        "pos_line": [
            "+assignments.append(tf.scatter_update(",
            "+ref=self.actions_memory[name],",
            "+indices=indices,",
            "+updates=action",
            "+))"
        ],
        "core_change": "-assignments.append(tf.scatter_update(ref=self.actions_memory[name], indices=indices, updates=action)) +assignments.append(tf.scatter_update( +ref=self.actions_memory[name], +indices=indices, +updates=action +))",
        "core_API": "items"
    },
    {
        "commit_hash": "4bce53aece5765f68c357ad2a5c9950041467c47",
        "index": "74ebe959..8f92b89b 100644",
        "commit_message": "Fix device for TraceTailAdaptive_ELBO (#1860)\n\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TraceTailAdaptive_ELBO(Trace_ELBO):",
            "# rank the particles according to p/q",
            "log_pq = log_p - log_q",
            "rank = torch.argsort(log_pq, descending=False)",
            "-        rank = torch.index_select(torch.arange(self.num_particles) + 1, -1, rank).type_as(log_pq)",
            "+        rank = torch.index_select(torch.arange(self.num_particles, device=log_pq.device) + 1, -1, rank).type_as(log_pq)",
            "",
            "# compute the particle-specific weights used to construct the surrogate loss",
            "gamma = torch.pow(rank, self.tail_adaptive_beta).detach()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=720329)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=720330)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=720331)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=720332)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=720333)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'log_pq'), position=0, insert_id=720334)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=720335)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=720336)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 7368,
        "neg_line": [
            "-rank = torch.index_select(torch.arange(self.num_particles) + 1, -1, rank).type_as(log_pq)"
        ],
        "pos_line": [
            "+rank = torch.index_select(torch.arange(self.num_particles, device=log_pq.device) + 1, -1, rank).type_as(log_pq)"
        ],
        "core_change": "-rank = torch.index_select(torch.arange(self.num_particles) + 1, -1, rank).type_as(log_pq) +rank = torch.index_select(torch.arange(self.num_particles, device=log_pq.device) + 1, -1, rank).type_as(log_pq)",
        "core_API": "argsort"
    },
    {
        "commit_hash": "ce149f64517883caf8abce7718c7bca5769f43ce",
        "index": "23ada7295..bb4c1d017 100644",
        "commit_message": "Fix support for dataclasses with ClassVar/InitVar in `apply_to_collection` (#9702)\n\nCo-authored-by: Adrian Wälchli <aedu.waelchli@gmail.com>\nCo-authored-by: Carlos Mocholi <carlossmocholi@gmail.com>\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_tpu_sync_dist():",
            "\"\"\"Test tpu spawn sync dist operation.\"\"\"",
            "",
            "def test_sync_dist(_):",
            "-        sync = _Sync(TPUSpawnPlugin().reduce, should=True, op=torch.distributed.ReduceOp.SUM)",
            "+        sync = _Sync(TPUSpawnPlugin().reduce, should=True, _op=torch.distributed.ReduceOp.SUM)",
            "value = torch.tensor([1.0])",
            "value = (sync(value),)",
            "assert value.item() == 8"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=op), value='_op')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 7371,
        "neg_line": [
            "-sync = _Sync(TPUSpawnPlugin().reduce, should=True, op=torch.distributed.ReduceOp.SUM)"
        ],
        "pos_line": [
            "+sync = _Sync(TPUSpawnPlugin().reduce, should=True, _op=torch.distributed.ReduceOp.SUM)"
        ],
        "core_change": "-sync = _Sync(TPUSpawnPlugin().reduce, should=True, op=torch.distributed.ReduceOp.SUM) +sync = _Sync(TPUSpawnPlugin().reduce, should=True, _op=torch.distributed.ReduceOp.SUM)",
        "core_API": "tensor"
    },
    {
        "commit_hash": "cee211ad16eb0a0147d195bcfac740118806216c",
        "index": "70a6ea06..8da99ea5 100644",
        "commit_message": "fixed graph conv on gpu\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def mm_diagonal(a, b, transpose=False):",
            "",
            "value = a[target] * value",
            "",
            "-    return torch.sparse.FloatTensor(index, value, b.size())",
            "+    return SparseTensor(index, value, b.size())"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=torch), value='SparseTensor')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=torch), position=0)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=sparse))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=FloatTensor))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 7374,
        "neg_line": [
            "-return torch.sparse.FloatTensor(index, value, b.size())"
        ],
        "pos_line": [
            "+return SparseTensor(index, value, b.size())"
        ],
        "core_change": "-return torch.sparse.FloatTensor(index, value, b.size()) +return SparseTensor(index, value, b.size())",
        "core_API": "FloatTensor"
    },
    {
        "commit_hash": "711afaa7720ebaba4a3739c1753c02568039993d",
        "index": "3d8a6570..d8ad5d69 100644",
        "commit_message": "Fix division by zero when there are zero-length spans in MismatchedEmbedder. (#4615)\n\n* Implment MattG's fix for NaN gradients in MismatchedEmbedder.\n\nFix `clamp_min` on embeddings.\n\nImplment MattG's fix for NaN gradients in MismatchedEmbedder.\n\n* Fix NaN gradients caused by weird tokens in MismatchedEmbedder.\n\nFixed division by zero error when there are zero-length spans in the input to a\nmismatched embedder.\n\n* Add changelog message.\n\n* Re-run `black` to get code formatting right.\n\n* combine fixed sections after merging with master\n\nCo-authored-by: Matt Gardner <mattg@allenai.org>\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class PretrainedTransformerMismatchedEmbedder(TokenEmbedder):",
            "span_embeddings_sum = span_embeddings.sum(2)",
            "span_embeddings_len = span_mask.sum(2)",
            "# Shape: (batch_size, num_orig_tokens, embedding_size)",
            "-        orig_embeddings = span_embeddings_sum / span_embeddings_len",
            "+        orig_embeddings = span_embeddings_sum / torch.clamp_min(span_embeddings_len, 1)",
            "",
            "# All the places where the span length is zero, write in zeros.",
            "orig_embeddings[(span_embeddings_len == 0).expand(orig_embeddings.shape)] = 0"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=binary_operator), node=('call', None), position=2, insert_id=7231)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=7232)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=7233)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=7234)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=7235)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'clamp_min'), position=2, insert_id=7236)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=7237)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=span_embeddings_len), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=7238)",
            "Insert(target_node=IN(type=argument_list), node=('integer', '1'), position=3, insert_id=7239)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=7240)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 7375,
        "neg_line": [
            "-orig_embeddings = span_embeddings_sum / span_embeddings_len"
        ],
        "pos_line": [
            "+orig_embeddings = span_embeddings_sum / torch.clamp_min(span_embeddings_len, 1)"
        ],
        "core_change": "-orig_embeddings = span_embeddings_sum / span_embeddings_len +orig_embeddings = span_embeddings_sum / torch.clamp_min(span_embeddings_len, 1)",
        "core_API": "sum"
    },
    {
        "commit_hash": "2016ad76724934151dffbd33e53ee7eb3bd16d95",
        "index": "9d1d48f..1258caf 100644",
        "commit_message": "Bug fix: make the CNN LM compatible with GPU\n\nSummary:\nSo we could run offline eval in GPU.\n**It does NOT affect CPU and NNPI inference**\n1. Let padding to use same device and dtype as input tensor in case states is already in GPU.\n2. register offsets as buffer so it could be moved to GPU\nElse it reports error when running in GPU: P340717906\nFound it's an optimization performed by jit.trace, which treat the rep.offset as const:\n```\n_12 = torch.view(CONSTANTS.c1, [1, 1, 1, -1])\n....\nstate_ind_offset = torch.expand(_12, [_14, _16, int(_17), -1], implicit=False)\n```\nSee full traced code before(P340717906) vs after (P340940281)\n\nSolution: https://discuss.pytorch.org/t/right-way-to-place-tensors-on-devices/78012\n\nDifferential Revision: D27318640\n\nfbshipit-source-id: c8767c2986af10e618314a0e3c3df1a002cdd452\n\n",
        "file": "pytext.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def pad_float_tensor(",
            "bs = shape[0]",
            "max_batch_len = pad_length(bs, batch_padding_control, -1)",
            "shape[0] = max_batch_len - bs",
            "-    pad_tensor = torch.zeros(shape, dtype=torch.float)",
            "+    pad_tensor = input_tensor.new_zeros(shape)",
            "new_tensor = torch.cat([input_tensor, pad_tensor], 0)",
            "return new_tensor"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=zeros), value='new_zeros')",
            "Update(target_node=ASTNode(type=identifier, text=torch), value='input_tensor')",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=dtype))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=float))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 7378,
        "neg_line": [
            "-pad_tensor = torch.zeros(shape, dtype=torch.float)"
        ],
        "pos_line": [
            "+pad_tensor = input_tensor.new_zeros(shape)"
        ],
        "core_change": "-pad_tensor = torch.zeros(shape, dtype=torch.float) +pad_tensor = input_tensor.new_zeros(shape)",
        "core_API": "zeros"
    },
    {
        "commit_hash": "d9f1874e3489edc915228dcfdae64d5294034464",
        "index": "78b0cba31..dbaa9a938 100644",
        "commit_message": "[RLlib] Minor fixes (torch GPU bugs + some cleanup). (#11609)\n\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def after_init(policy: Policy, obs_space: gym.spaces.Space,",
            "config: TrainerConfigDict) -> None:",
            "ComputeTDErrorMixin.__init__(policy)",
            "TargetNetworkMixin.__init__(policy, obs_space, action_space, config)",
            "-    # Move target net to device (this is done autoatically for the",
            "+    # Move target net to device (this is done automatically for the",
            "# policy.model, but not for any other models the policy has).",
            "policy.target_q_model = policy.target_q_model.to(policy.device)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 7379,
        "neg_line": [
            "-# Move target net to device (this is done autoatically for the"
        ],
        "pos_line": [
            "+# Move target net to device (this is done automatically for the"
        ],
        "core_change": "-# Move target net to device (this is done autoatically for the +# Move target net to device (this is done automatically for the",
        "core_API": "__init__"
    },
    {
        "commit_hash": "d48602ad657f2217fb0e70c5a1f6b4a50614f04f",
        "index": "f0eb34034e..484777a150 100644",
        "commit_message": "fix errors flagged by lint\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def as_ivy_dtype(dtype_in: Union[torch.dtype, str, bool, int, float], /) -> ivy.",
            "",
            "",
            "@with_unsupported_dtypes({\"1.11.0 and below\": (\"uint16\",)}, backend_version)",
            "-def as_native_dtype(",
            "-    dtype_in: Union[torch.dtype, str, bool, int, float], /",
            "-) -> torch.dtype:",
            "+def as_native_dtype(dtype_in: Union[torch.dtype, str, bool, int, float]) -> torch.dtype:",
            "if dtype_in is int:",
            "return ivy.default_int_dtype(as_native=True)",
            "if dtype_in is float:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=function_definition), node=('ERROR', None), position=6, insert_id=269172)",
            "Move(target_node=ASTNode(type=function_definition), node=ASTNode(type=block, text=), position=7)",
            "Insert(target_node=IN(type=ERROR), node=('assignment', None), position=0, insert_id=269173)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=:, text=:), position=1)",
            "Insert(target_node=IN(type=assignment), node=('attribute', None), position=0, insert_id=269174)",
            "Insert(target_node=IN(type=assignment), node=(':', ':'), position=1, insert_id=269175)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=ERROR), position=2)",
            "Insert(target_node=IN(type=assignment), node=('type', None), position=3, insert_id=269176)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=subscript), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=ERROR), position=1)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=2)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=dtype), position=3)",
            "Insert(target_node=ASTNode(type=ERROR), node=('identifier', 'if'), position=0, insert_id=269177)",
            "Move(target_node=IN(type=type), node=ASTNode(type=comparison_operator), position=0)",
            "Insert(target_node=ASTNode(type=ERROR), node=('identifier', 'torch'), position=3, insert_id=269178)",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=/, text=/))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=block))",
            "Delete(target_node=ASTNode(type=if, text=if))",
            "Delete(target_node=ASTNode(type=if_statement))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 24,
        "number": 7384,
        "neg_line": [
            "-def as_native_dtype(",
            "-dtype_in: Union[torch.dtype, str, bool, int, float], /",
            "-) -> torch.dtype:"
        ],
        "pos_line": [
            "+def as_native_dtype(dtype_in: Union[torch.dtype, str, bool, int, float]) -> torch.dtype:"
        ],
        "core_change": "-def as_native_dtype( -dtype_in: Union[torch.dtype, str, bool, int, float], / -) -> torch.dtype: +def as_native_dtype(dtype_in: Union[torch.dtype, str, bool, int, float]) -> torch.dtype:",
        "core_API": "default_int_dtype"
    },
    {
        "commit_hash": "dc87f526d413ff1763c5a4a6021fa93baa117694",
        "index": "6565a52c..af5dd6a4 100644",
        "commit_message": "Fix common tests for FP16 (#1588)\n\n* Fix common tests for FP16\n\n* revert\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class PipelineTesterMixin:",
            "",
            "with tempfile.TemporaryDirectory() as tmpdir:",
            "pipe.save_pretrained(tmpdir)",
            "-            pipe_loaded = self.pipeline_class.from_pretrained(tmpdir)",
            "+            pipe_loaded = self.pipeline_class.from_pretrained(tmpdir, torch_dtype=torch.float16)",
            "pipe_loaded.to(torch_device)",
            "pipe_loaded.set_progress_bar_config(disable=None)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=94726)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=94727)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'torch_dtype'), position=0, insert_id=94728)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=94729)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=94730)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=94731)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=94732)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'float16'), position=2, insert_id=94733)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 7386,
        "neg_line": [
            "-pipe_loaded = self.pipeline_class.from_pretrained(tmpdir)"
        ],
        "pos_line": [
            "+pipe_loaded = self.pipeline_class.from_pretrained(tmpdir, torch_dtype=torch.float16)"
        ],
        "core_change": "-pipe_loaded = self.pipeline_class.from_pretrained(tmpdir) +pipe_loaded = self.pipeline_class.from_pretrained(tmpdir, torch_dtype=torch.float16)",
        "core_API": "TemporaryDirectory"
    },
    {
        "commit_hash": "093e901c979c0f8b0af92f79010b53db1ddcc972",
        "index": "86e9cba..7d495fc 100644",
        "commit_message": "fix deprecation warnings; remove pydoop dependency; update README\n\n",
        "file": "TensorFlowOnSpark.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def repeat_op(repetitions, inputs, op, *args, **kwargs):",
            "ValueError: if the op is unknown or wrong.",
            "\"\"\"",
            "scope = kwargs.pop('scope', None)",
            "-  with tf.variable_op_scope([inputs], scope, 'RepeatOp'):",
            "+  with tf.variable_scope(scope, 'RepeatOp', [inputs]):",
            "tower = inputs",
            "for _ in range(repetitions):",
            "tower = op(tower, *args, **kwargs)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=variable_op_scope), value='variable_scope')",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'scope'), position=1, insert_id=2213649)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=2213650)",
            "Insert(target_node=ASTNode(type=argument_list), node=('string', \"'RepeatOp'\"), position=3, insert_id=2213651)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=2213652)",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=scope))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=string, text='RepeatOp'))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 7391,
        "neg_line": [
            "-with tf.variable_op_scope([inputs], scope, 'RepeatOp'):"
        ],
        "pos_line": [
            "+with tf.variable_scope(scope, 'RepeatOp', [inputs]):"
        ],
        "core_change": "-with tf.variable_op_scope([inputs], scope, 'RepeatOp'): +with tf.variable_scope(scope, 'RepeatOp', [inputs]):",
        "core_API": "pop"
    },
    {
        "commit_hash": "34263ab196cbfab65f2afa1150e6cf6db5ffd8b6",
        "index": "a9522cf0d..820b0be1a 100755",
        "commit_message": "rnn->encoders, removes _th suffix, same filenames for chainer and pytorch, modify tests to not rely on _th\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def main():",
            "# train",
            "logging.info('backend = ' + args.backend)",
            "if args.backend == \"chainer\":",
            "-        from espnet.lm.chain.lm_chainer import train",
            "+        from espnet.lm.chain.lm import train",
            "train(args)",
            "elif args.backend == \"pytorch\":",
            "-        from espnet.lm.pytorch.lm_pytorch import train",
            "+        from espnet.lm.pytorch.lm import train",
            "train(args)",
            "else:",
            "raise ValueError(\"Only chainer and pytorch are supported.\")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=import_from_statement), node=ASTNode(type=dotted_name), position=3)",
            "Update(target_node=ASTNode(type=identifier, text=lm_pytorch), value='lm')",
            "Insert(target_node=ASTNode(type=import_from_statement), node=('dotted_name', None), position=3, insert_id=178755)",
            "Update(target_node=ASTNode(type=identifier, text=lm_chainer), value='lm')",
            "Insert(target_node=IN(type=dotted_name), node=('identifier', 'train'), position=0, insert_id=178756)",
            "Delete(target_node=ASTNode(type=identifier, text=train))",
            "Delete(target_node=ASTNode(type=dotted_name))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 7,
        "number": 7392,
        "neg_line": [
            "-from espnet.lm.chain.lm_chainer import train",
            "-from espnet.lm.pytorch.lm_pytorch import train"
        ],
        "pos_line": [
            "+from espnet.lm.chain.lm import train",
            "+from espnet.lm.pytorch.lm import train"
        ],
        "core_change": "-from espnet.lm.chain.lm_chainer import train +from espnet.lm.chain.lm import train -from espnet.lm.pytorch.lm_pytorch import train +from espnet.lm.pytorch.lm import train",
        "core_API": "info"
    },
    {
        "commit_hash": "d87c1ba4805d56134484eee45ad52ff16d7eaec7",
        "index": "f1df3028..fb5ba5f9 100644",
        "commit_message": "Minor fix for `mask_feature` (#6043)\n\nA careless error - it should be `rand_like` instead of `randn_like`.\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def mask_feature(x: Tensor, p: float = 0.5, mode: str = 'col',",
            "mask = torch.rand(x.size(1), device=x.device) >= p",
            "mask = mask.view(1, -1)",
            "else:",
            "-        mask = torch.randn_like(x) >= p",
            "+        mask = torch.rand_like(x) >= p",
            "",
            "x = x.masked_fill(~mask, fill_value)",
            "return x, mask"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=randn_like), value='rand_like')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 7394,
        "neg_line": [
            "-mask = torch.randn_like(x) >= p"
        ],
        "pos_line": [
            "+mask = torch.rand_like(x) >= p"
        ],
        "core_change": "-mask = torch.randn_like(x) >= p +mask = torch.rand_like(x) >= p",
        "core_API": "rand"
    },
    {
        "commit_hash": "459488bba2ff838f2b0f1392561a30ee0b611ea6",
        "index": "7623b0d..c6e0a20 100644",
        "commit_message": "fix linter (#630)\n\n* fix linter\n\n* no imports okay\n\n* explicit bases\n\n* disable in pylintrc\n",
        "file": "tinygrad.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TorchBuffer(InterpretedBuffer):",
            "fxn_for_op : ClassVar = torch_fxn_for_op",
            "",
            "@staticmethod",
            "-  def fromCPU(data): return TorchBuffer(torch.from_numpy(data).requires_grad_(False).to(device))",
            "-  def toCPU(x): return x._buf.cpu().numpy()",
            "+  def fromCPU(x): return TorchBuffer(torch.from_numpy(x).requires_grad_(False).to(device))",
            "+  def toCPU(self): return self._buf.cpu().numpy()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=function_definition), node=('parameters', None), position=2, insert_id=1159248)",
            "Move(target_node=ASTNode(type=function_definition), node=ASTNode(type=parameters), position=2)",
            "Insert(target_node=IN(type=parameters), node=('(', '('), position=0, insert_id=1159249)",
            "Insert(target_node=IN(type=parameters), node=('identifier', 'self'), position=1, insert_id=1159250)",
            "Insert(target_node=IN(type=parameters), node=(')', ')'), position=2, insert_id=1159251)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=1159252)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1159253)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1159254)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=argument_list), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=x), value='self')",
            "Update(target_node=ASTNode(type=identifier, text=data), value='x')",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=data))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=parameters))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 18,
        "number": 7400,
        "neg_line": [
            "-def fromCPU(data): return TorchBuffer(torch.from_numpy(data).requires_grad_(False).to(device))",
            "-def toCPU(x): return x._buf.cpu().numpy()"
        ],
        "pos_line": [
            "+def fromCPU(x): return TorchBuffer(torch.from_numpy(x).requires_grad_(False).to(device))",
            "+def toCPU(self): return self._buf.cpu().numpy()"
        ],
        "core_change": "-def fromCPU(data): return TorchBuffer(torch.from_numpy(data).requires_grad_(False).to(device)) -def toCPU(x): return x._buf.cpu().numpy() +def fromCPU(x): return TorchBuffer(torch.from_numpy(x).requires_grad_(False).to(device)) +def toCPU(self): return self._buf.cpu().numpy()",
        "core_API": "from_numpy"
    },
    {
        "commit_hash": "04a6ad1981abd959f12585f42212e947cfa96318",
        "index": "2869e2ac..3445eca9 100644",
        "commit_message": "adopt torch.testing.assert_close (#1031)\n\n* adopt torch.testing.assert_close\n\n* use torch.testing.assert_close\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add TODO\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestMedianBlur:",
            "op_module = kornia.filters.MedianBlur((3, 5))",
            "actual = op_module(img)",
            "expected = op(img, kernel_size)",
            "-        assert_allclose(actual, expected)",
            "+        assert_close(actual, expected)"
        ],
        "hunk_index": 4,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=assert_allclose), value='assert_close')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 7401,
        "neg_line": [
            "-assert_allclose(actual, expected)"
        ],
        "pos_line": [
            "+assert_close(actual, expected)"
        ],
        "core_change": "-assert_allclose(actual, expected) +assert_close(actual, expected)",
        "core_API": "MedianBlur"
    },
    {
        "commit_hash": "bbe31c6735998b61e97ac7deb80d84484f488bb2",
        "index": "cfbac445..25fc006d 100644",
        "commit_message": "Fixed the issue of NaN gradients by adding epsilon in focal loss (#924)\n\n\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def binary_focal_loss_with_logits(",
            "",
            "probs = torch.sigmoid(input)",
            "target = target.unsqueeze(dim=1)",
            "-    loss_tmp = -alpha * torch.pow((1. - probs), gamma) * target * torch.log(probs + eps) \\",
            "-               - (1 - alpha) * torch.pow(probs, gamma) * (1. - target) * torch.log(1. - probs + eps)",
            "+    loss_tmp = - alpha * torch.pow((1. - probs + eps), gamma) * target * torch.log(probs + eps) \\",
            "+               - (1 - alpha) * torch.pow(probs + eps, gamma) * (1. - target) * torch.log(1. - probs + eps)",
            "+",
            "loss_tmp = loss_tmp.squeeze(dim=1)",
            "",
            "if reduction == 'none':"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=429020)",
            "Insert(target_node=ASTNode(type=argument_list), node=('binary_operator', None), position=1, insert_id=429021)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=429022)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=429023)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'pow'), position=2, insert_id=429024)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=identifier, text=probs), position=0)",
            "Insert(target_node=IN(type=binary_operator), node=('+', '+'), position=1, insert_id=429025)",
            "Insert(target_node=IN(type=binary_operator), node=('identifier', 'eps'), position=2, insert_id=429026)",
            "Insert(target_node=ASTNode(type=parenthesized_expression), node=('binary_operator', None), position=1, insert_id=429027)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=binary_operator), position=0)",
            "Insert(target_node=IN(type=binary_operator), node=('+', '+'), position=1, insert_id=429028)",
            "Insert(target_node=IN(type=binary_operator), node=('identifier', 'eps'), position=2, insert_id=429029)",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=pow))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 7402,
        "neg_line": [
            "-loss_tmp = -alpha * torch.pow((1. - probs), gamma) * target * torch.log(probs + eps) \\",
            "-- (1 - alpha) * torch.pow(probs, gamma) * (1. - target) * torch.log(1. - probs + eps)"
        ],
        "pos_line": [
            "+loss_tmp = - alpha * torch.pow((1. - probs + eps), gamma) * target * torch.log(probs + eps) \\",
            "+- (1 - alpha) * torch.pow(probs + eps, gamma) * (1. - target) * torch.log(1. - probs + eps)",
            "+"
        ],
        "core_change": "-loss_tmp = -alpha * torch.pow((1. - probs), gamma) * target * torch.log(probs + eps) \\ -- (1 - alpha) * torch.pow(probs, gamma) * (1. - target) * torch.log(1. - probs + eps) +loss_tmp = - alpha * torch.pow((1. - probs + eps), gamma) * target * torch.log(probs + eps) \\ +- (1 - alpha) * torch.pow(probs + eps, gamma) * (1. - target) * torch.log(1. - probs + eps) +",
        "core_API": "sigmoid"
    },
    {
        "commit_hash": "3f82bcece96b7fdabdd25460cfac48273c677686",
        "index": "f059fce9..dc77d597 100644",
        "commit_message": "TF: Fix #2247, explicitly squeeze image to support matplotlib<3.3.0 (#2248)\n\n\n",
        "file": "d2l-en.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def predict_seq2seq(net, src_sentence, src_vocab, tgt_vocab, num_steps,",
            "if pred == tgt_vocab['<eos>']:",
            "break",
            "output_seq.append(pred)",
            "-    return ' '.join(tgt_vocab.to_tokens(output_seq)), attention_weight_seq# Alias defined in config.ini",
            "+    return ' '.join(tgt_vocab.to_tokens(output_seq)), attention_weight_seq",
            "+",
            "+",
            "+# Alias defined in config.ini",
            "nn_Module = nn.Module",
            "",
            "ones_like = torch.ones_like"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 7403,
        "neg_line": [
            "-return ' '.join(tgt_vocab.to_tokens(output_seq)), attention_weight_seq# Alias defined in config.ini"
        ],
        "pos_line": [
            "+return ' '.join(tgt_vocab.to_tokens(output_seq)), attention_weight_seq",
            "+",
            "+",
            "+# Alias defined in config.ini"
        ],
        "core_change": "-return ' '.join(tgt_vocab.to_tokens(output_seq)), attention_weight_seq# Alias defined in config.ini +return ' '.join(tgt_vocab.to_tokens(output_seq)), attention_weight_seq + + +# Alias defined in config.ini",
        "core_API": "append"
    },
    {
        "commit_hash": "ad7115add4e391007853fa7f465448af8e5ec306",
        "index": "b2228ae6..0b64265d 100644",
        "commit_message": "Scale Factor Fix (#2039)\n\n* fix (scale_factor): use new_tensor(scale_factor) in case it is numpy array\n\n* reformat (models): reformat with flake8, yapf, and isort to pass CI\n\n* fix (bbox_mapping): fix scale_factor bug in bbox_mapping\n\n* fix img_meta bug\n\nCo-authored-by: beansi <zhangwenwei@sensetime.com>\n",
        "file": "mmdetection.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class CascadeRCNN(BaseDetector, RPNTestMixin):",
            "scale_factor if rescale else det_bboxes)",
            "else:",
            "_bboxes = (",
            "-                        det_bboxes[:, :4] *",
            "-                        torch.from_numpy(scale_factor).to(det_bboxes.device)",
            "+                        det_bboxes[:, :4] * det_bboxes.new_tensor(scale_factor)",
            "if rescale else det_bboxes)",
            "",
            "mask_rois = bbox2roi([_bboxes])"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=argument_list), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=torch), value='det_bboxes')",
            "Update(target_node=ASTNode(type=identifier, text=from_numpy), value='new_tensor')",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=to))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=det_bboxes))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=device))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 15,
        "number": 7405,
        "neg_line": [
            "-det_bboxes[:, :4] *",
            "-torch.from_numpy(scale_factor).to(det_bboxes.device)"
        ],
        "pos_line": [
            "+det_bboxes[:, :4] * det_bboxes.new_tensor(scale_factor)"
        ],
        "core_change": "-det_bboxes[:, :4] * -torch.from_numpy(scale_factor).to(det_bboxes.device) +det_bboxes[:, :4] * det_bboxes.new_tensor(scale_factor)",
        "core_API": "from_numpy"
    },
    {
        "commit_hash": "8c30b6e69097dbbda1dcbf47e9f1fdb1aeec736c",
        "index": "41612b1..6e16d33 100644",
        "commit_message": "fix the optimizer iteration increment logic when gradient accumulation is enabled (#3631)\n\nSigned-off-by: Vignesh Kothapalli <k.vignesh1420@gmail.com>\n",
        "file": "horovod.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class LocalGradientAggregationHelper:",
            "# If optimizer tracks iterations, we increment it on steps where we",
            "# are not going to call `apply_gradients()`.",
            "def increment_optimizer_iteration():",
            "-            if hasattr(optimizer, \"_iterations\") and optimizer._iterations is not None:",
            "-                return optimizer._iterations.assign_add(1).op",
            "+            # (kvignesh1420): Since all `tf.OptimizerV2` instances have the `iterations`",
            "+            # property for modifying the underlying `optimizer._iterations`, it is safe to use",
            "+            # the property instead of the private variable. For instance, the keras",
            "+            # `LossScaleOptimizer` inherits `tf.Optimizer` and exposes the cleaner `iterations`",
            "+            # property instead of the unsafe `_iterations`.",
            "+",
            "+            if hasattr(optimizer, \"iterations\") and optimizer.iterations is not None:",
            "+                return optimizer.iterations.assign_add(1).op",
            "return tf.no_op()",
            "",
            "with tf.control_dependencies([tf.group(*get_not_none_from_list(flattended_args0))]):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"_iterations\"), value='\"iterations\"')",
            "Update(target_node=ASTNode(type=identifier, text=_iterations), value='iterations')",
            "Update(target_node=ASTNode(type=identifier, text=_iterations), value='iterations')"
        ],
        "plus_line": 7,
        "minus_line": 2,
        "AST_diff_line": 3,
        "number": 7406,
        "neg_line": [
            "-if hasattr(optimizer, \"_iterations\") and optimizer._iterations is not None:",
            "-return optimizer._iterations.assign_add(1).op"
        ],
        "pos_line": [
            "+# (kvignesh1420): Since all `tf.OptimizerV2` instances have the `iterations`",
            "+# property for modifying the underlying `optimizer._iterations`, it is safe to use",
            "+# the property instead of the private variable. For instance, the keras",
            "+# `LossScaleOptimizer` inherits `tf.Optimizer` and exposes the cleaner `iterations`",
            "+# property instead of the unsafe `_iterations`.",
            "+",
            "+if hasattr(optimizer, \"iterations\") and optimizer.iterations is not None:",
            "+return optimizer.iterations.assign_add(1).op"
        ],
        "core_change": "-if hasattr(optimizer, \"_iterations\") and optimizer._iterations is not None: -return optimizer._iterations.assign_add(1).op +# (kvignesh1420): Since all `tf.OptimizerV2` instances have the `iterations` +# property for modifying the underlying `optimizer._iterations`, it is safe to use +# the property instead of the private variable. For instance, the keras +# `LossScaleOptimizer` inherits `tf.Optimizer` and exposes the cleaner `iterations` +# property instead of the unsafe `_iterations`. + +if hasattr(optimizer, \"iterations\") and optimizer.iterations is not None: +return optimizer.iterations.assign_add(1).op",
        "core_API": "assign_add"
    },
    {
        "commit_hash": "57c82c6900fd3025ddfea82f59b8a36b2392191b",
        "index": "5db177c..4eba479 100644",
        "commit_message": "fix variable_scope for older TF\n\n",
        "file": "tflearn.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def bidirectional_rnn(incoming, rnncell_fw, rnncell_bw, return_seq=False,",
            "",
            "input_shape = utils.get_incoming_shape(incoming)",
            "",
            "-    with tf.variable_scope(scope, name, [incoming]) as scope:",
            "+    with tf.variable_scope(scope, name, values=[incoming]) as scope:",
            "name = scope.name",
            "",
            "# TODO: DropoutWrapper"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=2352279)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'values'), position=0, insert_id=2352280)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2352281)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=list), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 7407,
        "neg_line": [
            "-with tf.variable_scope(scope, name, [incoming]) as scope:"
        ],
        "pos_line": [
            "+with tf.variable_scope(scope, name, values=[incoming]) as scope:"
        ],
        "core_change": "-with tf.variable_scope(scope, name, [incoming]) as scope: +with tf.variable_scope(scope, name, values=[incoming]) as scope:",
        "core_API": "get_incoming_shape"
    },
    {
        "commit_hash": "2e6db6fba3cf1fb13aa31b1f7a77f9ccac9c4e74",
        "index": "d5c7b616e..a22307046 100644",
        "commit_message": "style: code style fixes (#1046)\n\n* fix: automatic code style correction\n\n* fix: squad_iterator\n\n* fix: delete document_bert_ner_iterator\n\n* fix: revert json files to dev version\n\n* fix: removed from registry\n\n* refactor: fix merge mistakes\n\n",
        "file": "DeepPavlov.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class BertClassifierModel(LRScheduledTFModel):",
            "self.token_types_ph = tf.placeholder(shape=(None, None), dtype=tf.int32, name='token_types_ph')",
            "",
            "if not self.one_hot_labels:",
            "-            self.y_ph = tf.placeholder(shape=(None, ), dtype=tf.int32, name='y_ph')",
            "+            self.y_ph = tf.placeholder(shape=(None,), dtype=tf.int32, name='y_ph')",
            "else:",
            "self.y_ph = tf.placeholder(shape=(None, self.n_classes), dtype=tf.float32, name='y_ph')"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 7411,
        "neg_line": [
            "-self.y_ph = tf.placeholder(shape=(None, ), dtype=tf.int32, name='y_ph')"
        ],
        "pos_line": [
            "+self.y_ph = tf.placeholder(shape=(None,), dtype=tf.int32, name='y_ph')"
        ],
        "core_change": "-self.y_ph = tf.placeholder(shape=(None, ), dtype=tf.int32, name='y_ph') +self.y_ph = tf.placeholder(shape=(None,), dtype=tf.int32, name='y_ph')",
        "core_API": "placeholder"
    },
    {
        "commit_hash": "c95292dc187e2d7699fe20e0db04192b304321d1",
        "index": "7ac84c12..7c11d7a7 100644",
        "commit_message": "several bug fix and improvement\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def FixedUnPooling(x, shape, unpool_mat=None):",
            "shape = shape2d(shape)",
            "",
            "# a faster implementation for this special case",
            "-    if shape[0] == 2 and  shape[1] == 2 and unpool_mat is None:",
            "+    if shape[0] == 2 and shape[1] == 2 and unpool_mat is None:",
            "return UnPooling2x2ZeroFilled(x)",
            "",
            "input_shape = tf.shape(x)",
            "if unpool_mat is None:",
            "mat = np.zeros(shape, dtype='float32')",
            "mat[0][0] = 1",
            "-        unpool_mat = tf.Variable(mat, trainable=False, name='unpool_mat')",
            "+        unpool_mat = tf.constant(mat, name='unpool_mat')",
            "elif isinstance(unpool_mat, np.ndarray):",
            "-        unpool_mat = tf.Variable(unpool_mat, trainable=False, name='unpool_mat')",
            "+        unpool_mat = tf.constant(unpool_mat, name='unpool_mat')",
            "assert unpool_mat.get_shape().as_list() == list(shape)",
            "",
            "# perform a tensor-matrix kronecker product"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=Variable), value='constant')",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=2312902)",
            "Update(target_node=ASTNode(type=identifier, text=Variable), value='constant')",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=keyword_argument), position=3)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'name'), position=0, insert_id=2312903)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2312904)",
            "Insert(target_node=IN(type=keyword_argument), node=('string', \"'unpool_mat'\"), position=2, insert_id=2312905)",
            "Delete(target_node=ASTNode(type=identifier, text=trainable))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=false, text=False))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=trainable))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=false, text=False))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=name))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=string, text='unpool_mat'))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 21,
        "number": 7415,
        "neg_line": [
            "-if shape[0] == 2 and  shape[1] == 2 and unpool_mat is None:",
            "-unpool_mat = tf.Variable(mat, trainable=False, name='unpool_mat')",
            "-unpool_mat = tf.Variable(unpool_mat, trainable=False, name='unpool_mat')"
        ],
        "pos_line": [
            "+if shape[0] == 2 and shape[1] == 2 and unpool_mat is None:",
            "+unpool_mat = tf.constant(mat, name='unpool_mat')",
            "+unpool_mat = tf.constant(unpool_mat, name='unpool_mat')"
        ],
        "core_change": "-if shape[0] == 2 and  shape[1] == 2 and unpool_mat is None: +if shape[0] == 2 and shape[1] == 2 and unpool_mat is None: -unpool_mat = tf.Variable(mat, trainable=False, name='unpool_mat') +unpool_mat = tf.constant(mat, name='unpool_mat') -unpool_mat = tf.Variable(unpool_mat, trainable=False, name='unpool_mat') +unpool_mat = tf.constant(unpool_mat, name='unpool_mat')",
        "core_API": "shape"
    },
    {
        "commit_hash": "f5bd38388cb478f0f74b8a9059550b246320f038",
        "index": "563f03ca..8ec645fc 100644",
        "commit_message": "Fixed typo. Fixed action_shape for DQN\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class DeepQNetwork(ValueFunction):",
            "",
            "with tf.name_scope(\"update\"):",
            "self.q_targets = tf.placeholder(tf.float32, [None], name='q_targets')",
            "-            self.actions = tf.placeholder(tf.int32, [None, self.action_count], name='actions')",
            "+            self.actions = tf.placeholder(tf.int32, [None], name='actions')",
            "",
            "# Q values for actions taken in batch",
            "print(self.actions)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=action_count))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 7416,
        "neg_line": [
            "-self.actions = tf.placeholder(tf.int32, [None, self.action_count], name='actions')"
        ],
        "pos_line": [
            "+self.actions = tf.placeholder(tf.int32, [None], name='actions')"
        ],
        "core_change": "-self.actions = tf.placeholder(tf.int32, [None, self.action_count], name='actions') +self.actions = tf.placeholder(tf.int32, [None], name='actions')",
        "core_API": "name_scope"
    },
    {
        "commit_hash": "02b176c4ce14340d26d42825523f406959c6c202",
        "index": "c264f89d0..e1ad471b0 100644",
        "commit_message": "Fix torch version comparisons (#18460)\n\nComparisons like\nversion.parse(torch.__version__) > version.parse(\"1.6\")\nare True for torch==1.6.0+cu101 or torch==1.6.0+cpu\n\nversion.parse(version.parse(torch.__version__).base_version) are preferred (and available in pytorch_utils.py\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def _get_learning_rate(self):",
            "last_lr = (",
            "# backward compatibility for pytorch schedulers",
            "self.lr_scheduler.get_last_lr()[0]",
            "-            if version.parse(torch.__version__) >= version.parse(\"1.4\")",
            "+            if version.parse(version.parse(torch.__version__).base_version) >= version.parse(\"1.4\")",
            "else self.lr_scheduler.get_lr()[0]",
            ")",
            "return last_lr"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('call', None), position=0, insert_id=1193606)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=1193607)",
            "Insert(target_node=ASTNode(type=attribute), node=('identifier', 'base_version'), position=2, insert_id=1193608)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1193609)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1193610)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'version'), position=0, insert_id=1193611)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1193612)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'parse'), position=2, insert_id=1193613)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1193614)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=attribute), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1193615)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 7421,
        "neg_line": [
            "-if version.parse(torch.__version__) >= version.parse(\"1.4\")"
        ],
        "pos_line": [
            "+if version.parse(version.parse(torch.__version__).base_version) >= version.parse(\"1.4\")"
        ],
        "core_change": "-if version.parse(torch.__version__) >= version.parse(\"1.4\") +if version.parse(version.parse(torch.__version__).base_version) >= version.parse(\"1.4\")",
        "core_API": "get_last_lr"
    },
    {
        "commit_hash": "58b16d86772894220fd4d23c0ef92a7148b6b32e",
        "index": "128bd36..ed3b974 100644",
        "commit_message": "fix bug in tf-onnx conversion\n\n",
        "file": "nebullvm.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def get_outputs_sizes_tf(",
            ") -> List[Tuple[int, ...]]:",
            "outputs = tf_model(*input_tensors)",
            "if isinstance(outputs, tf.Tensor):",
            "-        return [tuple(tf.shape(outputs))]",
            "-    return [tuple(x.size()) for x in outputs]",
            "+        return [tuple(outputs.shape)]",
            "+    return [tuple(x.shape) for x in outputs]",
            "",
            "",
            "def create_model_inputs_tf("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=attribute), position=1)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=), text=)), position=2)",
            "Update(target_node=ASTNode(type=identifier, text=size), value='shape')",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=attribute), position=1)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=), text=)), position=2)",
            "Update(target_node=ASTNode(type=identifier, text=tf), value='outputs')",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=outputs))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=), text=)))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 15,
        "number": 7422,
        "neg_line": [
            "-return [tuple(tf.shape(outputs))]",
            "-return [tuple(x.size()) for x in outputs]"
        ],
        "pos_line": [
            "+return [tuple(outputs.shape)]",
            "+return [tuple(x.shape) for x in outputs]"
        ],
        "core_change": "-return [tuple(tf.shape(outputs))] -return [tuple(x.size()) for x in outputs] +return [tuple(outputs.shape)] +return [tuple(x.shape) for x in outputs]",
        "core_API": "shape"
    },
    {
        "commit_hash": "1deaf051331959e8c919c73e858c19551d1ff77b",
        "index": "a26d8c0..eee8e4c 100644",
        "commit_message": "Replaced our queue with tensorflow queue and corrected quantization_channels key error.\n\nFix accidental restore comment.\n\nRemove unnecessary print.\n\nAdd newline to wavenet_ops.py.\n\n",
        "file": "tensorflow-wavenet.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def main():",
            "next_sample,",
            "feed_dict={samples: window})",
            "",
            "-        sample = np.random.choice(np.arange(quantization_steps), p=prediction)",
            "+        sample = np.random.choice(np.arange(quantization_channels), p=prediction)",
            "waveform.append(sample)",
            "print('Sample {:3<d}/{:3<d}: {}'",
            ".format(step + 1, args.samples, sample))"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=quantization_steps), value='quantization_channels')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 7425,
        "neg_line": [
            "-sample = np.random.choice(np.arange(quantization_steps), p=prediction)"
        ],
        "pos_line": [
            "+sample = np.random.choice(np.arange(quantization_channels), p=prediction)"
        ],
        "core_change": "-sample = np.random.choice(np.arange(quantization_steps), p=prediction) +sample = np.random.choice(np.arange(quantization_channels), p=prediction)",
        "core_API": "choice"
    },
    {
        "commit_hash": "1e859f75070ea2c145850bddc61a0de82f69db61",
        "index": "114d109d..c84ed954 100644",
        "commit_message": "Fix half of the tests that are failing in v1.\n\nMost of them are failing since the actual code are expected to run only in v2 (eg need eager/resource variable, or certain fix we added is only applied to the v2 code path).\n\nPiperOrigin-RevId: 394765626\n\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def make_preprocessing_model(file_dir):",
            "",
            "def make_training_model():",
            "\"\"\"Make a trainable model for the preprocessed inputs.\"\"\"",
            "-  float_in = tf.keras.Input(shape=(1,), dtype=\"float64\", name=\"float_col\")",
            "+  float_in = tf.keras.Input(shape=(1,), dtype=\"float32\", name=\"float_col\")",
            "# After preprocessing, both the string and int column are integer ready for",
            "# embedding.",
            "int_in = tf.keras.Input(shape=(1,), dtype=\"int64\", name=\"int_col\")"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"float64\"), value='\"float32\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 7428,
        "neg_line": [
            "-float_in = tf.keras.Input(shape=(1,), dtype=\"float64\", name=\"float_col\")"
        ],
        "pos_line": [
            "+float_in = tf.keras.Input(shape=(1,), dtype=\"float32\", name=\"float_col\")"
        ],
        "core_change": "-float_in = tf.keras.Input(shape=(1,), dtype=\"float64\", name=\"float_col\") +float_in = tf.keras.Input(shape=(1,), dtype=\"float32\", name=\"float_col\")",
        "core_API": "Input"
    },
    {
        "commit_hash": "230765d64550c83638b35bb1933b88227eabfb91",
        "index": "e6953b6c..00000000",
        "commit_message": "fix: enable binary dense encoder (#2957)\n\n\n",
        "file": "ludwig.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "-import torch",
            "-",
            "-from ludwig.encoders.binary_encoders import BinaryPassthroughEncoder",
            "-",
            "-",
            "-def test_binary_passthrough_encoder():",
            "-    binary_encoder = BinaryPassthroughEncoder()",
            "-    inputs = torch.rand(2, 1)",
            "-    outputs = binary_encoder(inputs)",
            "-    assert outputs.shape[1:] == binary_encoder.output_shape"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 0,
        "minus_line": 4,
        "AST_diff_line": 17,
        "number": 7429,
        "neg_line": [
            "-import torch",
            "-",
            "-from ludwig.encoders.binary_encoders import BinaryPassthroughEncoder",
            "-",
            "-",
            "-def test_binary_passthrough_encoder():",
            "-binary_encoder = BinaryPassthroughEncoder()",
            "-inputs = torch.rand(2, 1)",
            "-outputs = binary_encoder(inputs)",
            "-assert outputs.shape[1:] == binary_encoder.output_shape"
        ],
        "pos_line": [],
        "core_change": "-import torch - -from ludwig.encoders.binary_encoders import BinaryPassthroughEncoder - - -def test_binary_passthrough_encoder(): -binary_encoder = BinaryPassthroughEncoder() -inputs = torch.rand(2, 1) -outputs = binary_encoder(inputs) -assert outputs.shape[1:] == binary_encoder.output_shape",
        "core_API": "rand"
    },
    {
        "commit_hash": "35fe3213ef5b1c566c4d73a9d90b0c2aace04f88",
        "index": "2cc834a6..956a11f7 100644",
        "commit_message": "RANSAC improvements (#1435)\n\n- Added LU solver for RANSAC-homography, it speeds-up the minimal-solver part x10\n- Now find_fundamental weights is optional, as it was for homography\n- Fix test precision and made one xFail test to be compulsory\n- Fixed tests for fundamental matrix, as one have to have >= 8 points (also added to docstring)\n- updates deprecated torch.svd to torch.linalg.svd\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def triangulate_points(",
            "# 1. Solve the system Ax=0 with smallest eigenvalue",
            "# 2. Return homogeneous coordinates",
            "",
            "-    _, _, V = torch.svd(X)",
            "+    _, _, V = _torch_svd_cast(X)",
            "",
            "points3d_h = V[..., -1]",
            "points3d: torch.Tensor = convert_points_from_homogeneous(points3d_h)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=torch), value='_torch_svd_cast')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=torch), position=0)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=svd))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 7430,
        "neg_line": [
            "-_, _, V = torch.svd(X)"
        ],
        "pos_line": [
            "+_, _, V = _torch_svd_cast(X)"
        ],
        "core_change": "-_, _, V = torch.svd(X) +_, _, V = _torch_svd_cast(X)",
        "core_API": "svd"
    },
    {
        "commit_hash": "c86747510ee737bbc20e9c0f1622696c04bf6fbc",
        "index": "00ae1e4..b34cd5f 100644",
        "commit_message": "Update Makefile with target field\n\nSummary:\n- update makefile with target for export to aggregate all export configs into one file\n- fix the bug that keeps flags set at fp16 precision and cuda enable after gpu-fp16 export\n\nReviewed By: mikekgfb\n\nDifferential Revision: D27278841\n\nfbshipit-source-id: d27794cb73f5ea3fbb9bfbedfdb671e0bb80770b\n\n",
        "file": "pytext.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "cuda",
        "change": [
            "def export_saved_model_to_caffe2(",
            "def export_saved_model_to_torchscript(",
            "saved_model_path: str, path: str, export_config: ExportConfig",
            ") -> None:",
            "+    cuda.CUDA_ENABLED = False",
            "+    precision.FP16_ENABLED = False",
            "task, train_config, _training_state = load(saved_model_path)",
            "task.torchscript_export(task.model, path, False, 1, export_config=export_config)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('block', None), position=0, insert_id=1861562)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=1861563)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=1, insert_id=1861564)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1861565)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1861566)",
            "Insert(target_node=IN(type=assignment), node=('attribute', None), position=0, insert_id=1861567)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1861568)",
            "Insert(target_node=IN(type=assignment), node=('false', 'False'), position=2, insert_id=1861569)",
            "Insert(target_node=IN(type=assignment), node=('attribute', None), position=0, insert_id=1861570)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1861571)",
            "Insert(target_node=IN(type=assignment), node=('false', 'False'), position=2, insert_id=1861572)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cuda'), position=0, insert_id=1861573)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1861574)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'CUDA_ENABLED'), position=2, insert_id=1861575)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'precision'), position=0, insert_id=1861576)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1861577)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'FP16_ENABLED'), position=2, insert_id=1861578)",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 2,
        "minus_line": 0,
        "AST_diff_line": 18,
        "number": 7431,
        "neg_line": [],
        "pos_line": [
            "+cuda.CUDA_ENABLED = False",
            "+precision.FP16_ENABLED = False"
        ],
        "core_change": "+cuda.CUDA_ENABLED = False +precision.FP16_ENABLED = False",
        "core_API": "torchscript_export"
    },
    {
        "commit_hash": "65cedfc2b7ee5075170b98f13cde45f05538fa4f",
        "index": "657c97a5..ecfb25fe 100644",
        "commit_message": "l2 reg fix\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "layers",
        "change": [
            "class TestDQFDAgent(unittest.TestCase):",
            "tf.reset_default_graph()",
            "",
            "# DQFD uses l2-reg",
            "-        network_builder = layered_network_builder(layers_config=[{'type': 'dense', 'size': 32,",
            "-                                                                  'weights_regularizer': 'tensorflow.contrib.layers.python.layers.regularizers.l2_regularizer',",
            "-                                                                  'weights_regularizer_kwargs': {",
            "-                                                                      'scale': 0.001",
            "-                                                                  }",
            "+        network_builder = layered_network_builder(layers_config=[{'type': 'dense',",
            "+                                                                  'size': 32,",
            "+                                                                  'l2_regularization': 0.001",
            "}])",
            "",
            "agent = DQFDAgent(config=config, network_builder=network_builder)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=dictionary), node=ASTNode(type=pair), position=5)",
            "Move(target_node=ASTNode(type=dictionary), node=ASTNode(type=}, text=}), position=6)",
            "Update(target_node=ASTNode(type=string, text='scale'), value=\"'l2_regularization'\")",
            "Delete(target_node=ASTNode(type=string, text='weights_regularizer'))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=string, text='tensorflow.contrib.layers.python.layers.regularizers.l2_regularizer'))",
            "Delete(target_node=ASTNode(type=pair))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=string, text='weights_regularizer_kwargs'))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type={, text={))",
            "Delete(target_node=ASTNode(type=dictionary))",
            "Delete(target_node=ASTNode(type=pair))",
            "Delete(target_node=ASTNode(type=}, text=}))"
        ],
        "plus_line": 3,
        "minus_line": 5,
        "AST_diff_line": 14,
        "number": 7432,
        "neg_line": [
            "-network_builder = layered_network_builder(layers_config=[{'type': 'dense', 'size': 32,",
            "-'weights_regularizer': 'tensorflow.contrib.layers.python.layers.regularizers.l2_regularizer',",
            "-'weights_regularizer_kwargs': {",
            "-'scale': 0.001",
            "-}"
        ],
        "pos_line": [
            "+network_builder = layered_network_builder(layers_config=[{'type': 'dense',",
            "+'size': 32,",
            "+'l2_regularization': 0.001"
        ],
        "core_change": "-network_builder = layered_network_builder(layers_config=[{'type': 'dense', 'size': 32, -'weights_regularizer': 'tensorflow.contrib.layers.python.layers.regularizers.l2_regularizer', -'weights_regularizer_kwargs': { -'scale': 0.001 -} +network_builder = layered_network_builder(layers_config=[{'type': 'dense', +'size': 32, +'l2_regularization': 0.001",
        "core_API": "reset_default_graph"
    },
    {
        "commit_hash": "94ba8ef8a62ed1d0e241368af6570a2b69af4608",
        "index": "880710a1..6bfa8016 100644",
        "commit_message": "Fix two warnings related to deprecated function names.\n\nPiperOrigin-RevId: 284760914\n\n",
        "file": "tf-quant-finance.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def interpolate(x,",
            "args[3], args[4], validate_args)",
            "",
            "with tf.control_dependencies(",
            "-        [tf.assert_equal(tf.shape(x_data), tf.shape(y_data))]):",
            "+        [tf.compat.v1.assert_equal(tf.shape(x_data), tf.shape(y_data))]):",
            "# Call 1D linear interpolation function for each batch separately.",
            "return tf.reshape(",
            "tf.map_fn("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=2347371)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=2347372)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2347373)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2347374)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'v1'), position=2, insert_id=2347375)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=tf), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'compat'), position=2, insert_id=2347376)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 7433,
        "neg_line": [
            "-[tf.assert_equal(tf.shape(x_data), tf.shape(y_data))]):"
        ],
        "pos_line": [
            "+[tf.compat.v1.assert_equal(tf.shape(x_data), tf.shape(y_data))]):"
        ],
        "core_change": "-[tf.assert_equal(tf.shape(x_data), tf.shape(y_data))]): +[tf.compat.v1.assert_equal(tf.shape(x_data), tf.shape(y_data))]):",
        "core_API": "control_dependencies"
    },
    {
        "commit_hash": "f670d9f1f299a604884c0b1ce58f36661ae1167e",
        "index": "6e9a878a..ce16bf47 100644",
        "commit_message": "Fixes fp16 inference for speech recognition example (#2438)\n\nSummary:\nCurrently the speech recognition fails for fp16 option as the input is\nnot converted to half. This fixes this issue.\n\n# Before submitting\n\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\n- [x] Did you make sure to update the docs?\n- [ ] Did you write any new necessary tests?\n\n## What does this PR do?\nFixes fp16 inference for speech recognition example.\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding �\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2438\n\nReviewed By: apoorv2904, xuqiantong\n\nDifferential Revision: D31594529\n\nPulled By: alexeib\n\nfbshipit-source-id: 75d1fe3f00b2e86381f28f79daea2cd516233e22\n\n",
        "file": "fairseq.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def main(args, task=None, model_state=None):",
            "wps_meter = TimeMeter()",
            "for sample in t:",
            "sample = utils.move_to_cuda(sample) if use_cuda else sample",
            "+            if use_fp16:",
            "+                sample = utils.apply_to_sample(apply_half, sample)",
            "if \"net_input\" not in sample:",
            "continue"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=4, insert_id=205374)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=205375)",
            "Insert(target_node=IN(type=if_statement), node=('identifier', 'use_fp16'), position=1, insert_id=205376)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=205377)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=205378)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=205379)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=205380)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'sample'), position=0, insert_id=205381)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=205382)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=205383)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=205384)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=205385)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'utils'), position=0, insert_id=205386)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=205387)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'apply_to_sample'), position=2, insert_id=205388)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=205389)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'apply_half'), position=1, insert_id=205390)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=205391)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'sample'), position=3, insert_id=205392)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=205393)"
        ],
        "plus_line": 2,
        "minus_line": 0,
        "AST_diff_line": 20,
        "number": 7437,
        "neg_line": [],
        "pos_line": [
            "+if use_fp16:",
            "+sample = utils.apply_to_sample(apply_half, sample)"
        ],
        "core_change": "+if use_fp16: +sample = utils.apply_to_sample(apply_half, sample)",
        "core_API": "move_to_cuda"
    },
    {
        "commit_hash": "06cc307b15fafdab194bc394d23fa113200c7f97",
        "index": "5228e1b..148527c 100644",
        "commit_message": "fix bugs in WGAN, WGAN-GP, DRAGAN\n\noutput of discriminator is changed into linear output (logit value) from\nsigmoid output\n\n",
        "file": "tensorflow-generative-model-collections.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class WGAN(object):",
            "D_fake, D_fake_logits, _ = self.discriminator(G, is_training=True, reuse=True)",
            "",
            "# get loss for discriminator",
            "-        d_loss_real = - tf.reduce_mean(D_real)",
            "-        d_loss_fake = tf.reduce_mean(D_fake)",
            "+        d_loss_real = - tf.reduce_mean(D_real_logits)",
            "+        d_loss_fake = tf.reduce_mean(D_fake_logits)",
            "",
            "self.d_loss = d_loss_real + d_loss_fake"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2206369)",
            "Update(target_node=ASTNode(type=identifier, text=D_fake), value='D_fake_logits')",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2206370)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2206371)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'reduce_mean'), position=2, insert_id=2206372)",
            "Update(target_node=ASTNode(type=identifier, text=D_real), value='D_real_logits')",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=reduce_mean))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 11,
        "number": 7439,
        "neg_line": [
            "-d_loss_real = - tf.reduce_mean(D_real)",
            "-d_loss_fake = tf.reduce_mean(D_fake)"
        ],
        "pos_line": [
            "+d_loss_real = - tf.reduce_mean(D_real_logits)",
            "+d_loss_fake = tf.reduce_mean(D_fake_logits)"
        ],
        "core_change": "-d_loss_real = - tf.reduce_mean(D_real) -d_loss_fake = tf.reduce_mean(D_fake) +d_loss_real = - tf.reduce_mean(D_real_logits) +d_loss_fake = tf.reduce_mean(D_fake_logits)",
        "core_API": "discriminator"
    },
    {
        "commit_hash": "6eb51450fa2a440a45e02b29f01e4f2aa4f70a4d",
        "index": "060b78e92..fdae51f72 100644",
        "commit_message": "TF Examples Rewrite (#18451)\n\n* Finished QA example\n\n* Dodge a merge conflict\n\n* Update text classification and LM examples\n\n* Update NER example\n\n* New Keras metrics WIP, fix NER example\n\n* Update NER example\n\n* Update MC, summarization and translation examples\n\n* Add XLA warnings when shapes are variable\n\n* Make sure batch_size is consistently scaled by num_replicas\n\n* Add PushToHubCallback to all models\n\n* Add docs links for KerasMetricCallback\n\n* Add docs links for prepare_tf_dataset and jit_compile\n\n* Correct inferred model names\n\n* Don't assume the dataset has 'lang'\n\n* Don't assume the dataset has 'lang'\n\n* Write metrics in text classification\n\n* Add 'framework' to TrainingArguments and TFTrainingArguments\n\n* Export metrics in all examples and add tests\n\n* Fix training args for Flax\n\n* Update command line args for translation test\n\n* make fixup\n\n* Fix accidentally running other tests in fp16\n\n* Remove do_train/do_eval from run_clm.py\n\n* Remove do_train/do_eval from run_mlm.py\n\n* Add tensorflow tests to circleci\n\n* Fix circleci\n\n* Update examples/tensorflow/language-modeling/run_mlm.py\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>\n\n* Update examples/tensorflow/test_tensorflow_examples.py\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>\n\n* Update examples/tensorflow/translation/run_translation.py\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>\n\n* Update examples/tensorflow/token-classification/run_ner.py\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>\n\n* Fix save path for tests\n\n* Fix some model card kwargs\n\n* Explain the magical -1000\n\n* Actually enable tests this time\n\n* Skip text classification PR until we fix shape inference\n\n* make fixup\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFTrainingArguments(TrainingArguments):",
            "def _setup_strategy(self) -> Tuple[\"tf.distribute.Strategy\", int]:",
            "logger.info(\"Tensorflow: setting up strategy\")",
            "",
            "-        if self.xla:",
            "-            tf.config.optimizer.set_jit(True)",
            "-",
            "gpus = tf.config.list_physical_devices(\"GPU\")",
            "",
            "# Set to float16 at first"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=if, text=if))",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=xla))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=config))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=optimizer))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=set_jit))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=true, text=True))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=block))",
            "Delete(target_node=ASTNode(type=if_statement))"
        ],
        "plus_line": 0,
        "minus_line": 2,
        "AST_diff_line": 24,
        "number": 7440,
        "neg_line": [
            "-if self.xla:",
            "-tf.config.optimizer.set_jit(True)",
            "-"
        ],
        "pos_line": [],
        "core_change": "-if self.xla: -tf.config.optimizer.set_jit(True) -",
        "core_API": "info"
    },
    {
        "commit_hash": "6b58e1550744589d1c6944afcc9738b19d22b4bc",
        "index": "17f952a2c..dc2c08521 100644",
        "commit_message": "Fix torch 1.8.0 segmentation fault (#10546)\n\n* Only run one test\n\n* Patch segfault\n\n* Fix summarization pipeline\n\n* Ready for merge\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SimpleSummarizationPipelineTests(unittest.TestCase):",
            "# Bias output towards L",
            "V, C = model.lm_head.weight.shape",
            "",
            "-        bias = torch.zeros(V, requires_grad=True)",
            "+        bias = torch.zeros(V)",
            "bias[76] = 10",
            "",
            "model.lm_head.bias = torch.nn.Parameter(bias)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=requires_grad))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=true, text=True))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 7443,
        "neg_line": [
            "-bias = torch.zeros(V, requires_grad=True)"
        ],
        "pos_line": [
            "+bias = torch.zeros(V)"
        ],
        "core_change": "-bias = torch.zeros(V, requires_grad=True) +bias = torch.zeros(V)",
        "core_API": "zeros"
    },
    {
        "commit_hash": "d08bc09495a289519a774e881f399cf864178f9b",
        "index": "933db4fc..64751683 100644",
        "commit_message": "Fix unused variable found (#1250)\n\n* Fix Unused variable found\n\n* more fixes\n\n* Update kornia/enhance/histogram.py\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestRemap:",
            "grid = kornia.utils.create_meshgrid(height, width, normalized_coordinates=False, device=device).to(dtype)",
            "grid += 1.0  # apply some shift",
            "input_tuple = (img, grid[..., 0], grid[..., 1])",
            "-        op_traced = torch.jit.trace(op_script, input_tuple)",
            "+        _ = torch.jit.trace(op_script, input_tuple)",
            "",
            "# 2. Generate different input",
            "batch_size, channels, height, width = 2, 2, 2, 5"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=op_traced), value='_')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 7446,
        "neg_line": [
            "-op_traced = torch.jit.trace(op_script, input_tuple)"
        ],
        "pos_line": [
            "+_ = torch.jit.trace(op_script, input_tuple)"
        ],
        "core_change": "-op_traced = torch.jit.trace(op_script, input_tuple) +_ = torch.jit.trace(op_script, input_tuple)",
        "core_API": "create_meshgrid"
    },
    {
        "commit_hash": "3c084177c661ec10c5442b4305930bf3057fe97b",
        "index": "1f83c169..1626e949 100644",
        "commit_message": "Data loader bug fix and Attention bug fix\n\n",
        "file": "TTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class AttentionRNN(nn.Module):",
            "def __init__(self, out_dim, annot_dim, memory_dim,",
            "score_mask_value=-float(\"inf\")):",
            "super(AttentionRNN, self).__init__()",
            "-        self.rnn_cell = nn.GRUCell(annot_dim + memory_dim, out_dim)",
            "+        self.rnn_cell = nn.GRUCell(out_dim + memory_dim, out_dim)",
            "self.alignment_model = BahdanauAttention(annot_dim, out_dim, out_dim)",
            "self.score_mask_value = score_mask_value"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=annot_dim), value='out_dim')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 7447,
        "neg_line": [
            "-self.rnn_cell = nn.GRUCell(annot_dim + memory_dim, out_dim)"
        ],
        "pos_line": [
            "+self.rnn_cell = nn.GRUCell(out_dim + memory_dim, out_dim)"
        ],
        "core_change": "-self.rnn_cell = nn.GRUCell(annot_dim + memory_dim, out_dim) +self.rnn_cell = nn.GRUCell(out_dim + memory_dim, out_dim)",
        "core_API": "GRUCell"
    },
    {
        "commit_hash": "ed63b7ee9e68ba120893aaaaca0d4525a56ed484",
        "index": "70903822..28828615 100644",
        "commit_message": "upgrade to pytorch 0.4.0 (#1126)\n\n* bump pytorch to 0.4 + fix sanitize\n\n* remove check for Variable in block_orthogonal\n\n* rename parameter split_size=\n\n* remove checks for Variable\n\n* fix more tests\n\n* fixes\n\n* get tests to pass\n\n* fix warnings\n\n* get rid of some of the Variables\n\n* more tests passing\n\n* more elimination of variables\n\n* finish removing all Variables\n\n* pylint and such\n\n* a few fixes\n\n* move torch.no_grad into model.forward_on_instances\n\n* more pytorch 0.4 changes\n\n* detach() -> data\n\n* pylint\n\n* fix bad tensor creation\n\n* fix types\n\n* remove print statement\n\n* more 0.4 goodness\n\n* factor out is_tensor\n\n* add no_grad to elmo command\n\n* cleanup\n\n* remove TODO\n\n* address PR feedback\n\n* replace all() with item()\n\n* more cleanup\n\n* further cleanup\n\n* remove Variable\n\n* really fix merge conflict\n\n* fix pylint\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class PytorchSeq2VecWrapper(Seq2VecEncoder):",
            "# batch size is the second dimension here, because pytorch",
            "# returns RNN state as a tensor of shape (num_layers * num_directions,",
            "# batch_size, hidden_size)",
            "-            zeros = state.data.new(num_layers_times_directions,",
            "-                                   batch_size - num_valid,",
            "-                                   encoding_dim).fill_(0)",
            "-            zeros = Variable(zeros)",
            "+            zeros = state.new_zeros(num_layers_times_directions,",
            "+                                    batch_size - num_valid,",
            "+                                    encoding_dim)",
            "state = torch.cat([state, zeros], 1)",
            "",
            "# Restore the original indices and return the final state of the"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=call), position=2)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=data), value='new_zeros')",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=new))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=fill_))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=integer, text=0))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=identifier, text=zeros))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=Variable))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=zeros))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 3,
        "minus_line": 4,
        "AST_diff_line": 24,
        "number": 7448,
        "neg_line": [
            "-zeros = state.data.new(num_layers_times_directions,",
            "-batch_size - num_valid,",
            "-encoding_dim).fill_(0)",
            "-zeros = Variable(zeros)"
        ],
        "pos_line": [
            "+zeros = state.new_zeros(num_layers_times_directions,",
            "+batch_size - num_valid,",
            "+encoding_dim)"
        ],
        "core_change": "-zeros = state.data.new(num_layers_times_directions, -batch_size - num_valid, -encoding_dim).fill_(0) -zeros = Variable(zeros) +zeros = state.new_zeros(num_layers_times_directions, +batch_size - num_valid, +encoding_dim)",
        "core_API": "new"
    },
    {
        "commit_hash": "a7df7cab6ad27bef44a0284d98562ec7de315619",
        "index": "fd6c2a9aef..d43055fa32 100644",
        "commit_message": "docstring fixes and some backend fixes\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def cross(",
            "promote_type = torch.promote_types(x1.dtype, x2.dtype)",
            "x1 = x1.type(promote_type)",
            "x2 = x2.type(promote_type)",
            "+    print(torch.__version__)",
            "return torch.cross(input=x1, other=x2, dim=axis, out=out)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=call), position=0)",
            "Insert(target_node=ASTNode(type=call), node=('ERROR', None), position=1, insert_id=351212)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=2, insert_id=351213)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'print'), position=0, insert_id=351214)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=351215)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=351216)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=351217)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=351218)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=351219)",
            "Insert(target_node=IN(type=attribute), node=('identifier', '__version__'), position=2, insert_id=351220)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 10,
        "number": 7450,
        "neg_line": [],
        "pos_line": [
            "+print(torch.__version__)"
        ],
        "core_change": "+print(torch.__version__)",
        "core_API": "promote_types"
    },
    {
        "commit_hash": "3e22cdabc140f6fa5f443f71c2069d3472e97652",
        "index": "458ccc72..9eeb4837 100644",
        "commit_message": "fix doc\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def to_hetero(module: Module, metadata: Metadata, aggr: str = \"sum\",",
            "import torch",
            "from torch_geometric.nn import SAGEConv, to_hetero",
            "",
            "-        Net(torch.nn.Module):",
            "+        class GNN(torch.nn.Module):",
            "def __init__(self):",
            "-                self.conv1 = SAGEConv(-1, 16)",
            "-                self.conv2 = SAGEConv(16, 16)",
            "+                self.conv1 = SAGEConv((-1, -1), 32)",
            "+                self.conv2 = SAGEConv((32, 32), 32)",
            "",
            "def forward(self, x, edge_index):",
            "x = self.conv1(x, edge_index).relu()",
            "x = self.conv2(x, edge_index).relu()",
            "return x",
            "",
            "-        model = Net()",
            "+        model = GNN()",
            "",
            "node_types = ['paper', 'author']",
            "edge_types = ["
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=Net), value='class')",
            "Insert(target_node=ASTNode(type=ERROR), node=('identifier', 'GNN'), position=2, insert_id=999979)",
            "Update(target_node=ASTNode(type=identifier, text=Net), value='GNN')",
            "Insert(target_node=ASTNode(type=argument_list), node=('tuple', None), position=1, insert_id=999980)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=999981)",
            "Insert(target_node=ASTNode(type=argument_list), node=('integer', '32'), position=3, insert_id=999982)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=4, insert_id=999983)",
            "Insert(target_node=ASTNode(type=argument_list), node=('tuple', None), position=1, insert_id=999984)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=999985)",
            "Insert(target_node=ASTNode(type=argument_list), node=('integer', '32'), position=3, insert_id=999986)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=4, insert_id=999987)",
            "Insert(target_node=IN(type=tuple), node=('(', '('), position=0, insert_id=999988)",
            "Move(target_node=IN(type=tuple), node=ASTNode(type=unary_operator, text=-1), position=1)",
            "Move(target_node=IN(type=tuple), node=ASTNode(type=,, text=,), position=2)",
            "Insert(target_node=IN(type=tuple), node=('unary_operator', '-1'), position=3, insert_id=999989)",
            "Move(target_node=IN(type=tuple), node=ASTNode(type=), text=)), position=4)",
            "Insert(target_node=IN(type=tuple), node=('(', '('), position=0, insert_id=999990)",
            "Insert(target_node=IN(type=tuple), node=('integer', '32'), position=1, insert_id=999991)",
            "Move(target_node=IN(type=tuple), node=ASTNode(type=,, text=,), position=2)",
            "Update(target_node=ASTNode(type=integer, text=16), value='32')",
            "Move(target_node=IN(type=tuple), node=ASTNode(type=integer, text=16), position=3)",
            "Move(target_node=IN(type=tuple), node=ASTNode(type=), text=)), position=4)",
            "Delete(target_node=ASTNode(type=integer, text=16))",
            "Delete(target_node=ASTNode(type=integer, text=16))"
        ],
        "plus_line": 4,
        "minus_line": 4,
        "AST_diff_line": 24,
        "number": 7451,
        "neg_line": [
            "-Net(torch.nn.Module):",
            "-self.conv1 = SAGEConv(-1, 16)",
            "-self.conv2 = SAGEConv(16, 16)",
            "-model = Net()"
        ],
        "pos_line": [
            "+class GNN(torch.nn.Module):",
            "+self.conv1 = SAGEConv((-1, -1), 32)",
            "+self.conv2 = SAGEConv((32, 32), 32)",
            "+model = GNN()"
        ],
        "core_change": "-Net(torch.nn.Module): +class GNN(torch.nn.Module): -self.conv1 = SAGEConv(-1, 16) -self.conv2 = SAGEConv(16, 16) +self.conv1 = SAGEConv((-1, -1), 32) +self.conv2 = SAGEConv((32, 32), 32) -model = Net() +model = GNN()",
        "core_API": "conv1"
    },
    {
        "commit_hash": "e628e7f68a73946a8ee63d543d1873a5a46c8453",
        "index": "e85363b..35ef74d 100644",
        "commit_message": "Fix indexing\n\n",
        "file": "PyTorch-YOLOv3.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def get_batch_statistics(outputs, targets, iou_threshold):",
            "continue",
            "",
            "# Filter target_boxes by pred_label so that we only match against boxes of our own label",
            "-                filtered_target_position, filtered_targets = zip(*filter(lambda x: target_labels[x] == pred_label, enumerate(target_boxes)))",
            "-",
            "+                filtered_target_position, filtered_targets = zip(*filter(lambda x: target_labels[x[0]] == pred_label, enumerate(target_boxes)))",
            "+",
            "# Find the best matching target for our predicted box",
            "-                iou, box_filtered_index = bbox_iou(pred_box.unsqueeze(0), filtered_targets).max(0)",
            "-",
            "+                iou, box_filtered_index = bbox_iou(pred_box.unsqueeze(0), torch.stack(filtered_targets)).max(0)",
            "+",
            "# Remap the index in the list of filtered targets for that label to the index in the list with all targets.",
            "box_index = filtered_target_position[box_filtered_index]"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=3, insert_id=904984)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=4, insert_id=904985)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=904986)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=904987)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=904988)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=904989)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'stack'), position=2, insert_id=904990)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=904991)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=filtered_targets), position=1)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=), text=)), position=2)",
            "Insert(target_node=ASTNode(type=subscript), node=('subscript', None), position=2, insert_id=904992)",
            "Insert(target_node=ASTNode(type=subscript), node=(']', ']'), position=3, insert_id=904993)",
            "Move(target_node=IN(type=subscript), node=ASTNode(type=identifier, text=x), position=0)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=904994)",
            "Insert(target_node=IN(type=subscript), node=('integer', '0'), position=2, insert_id=904995)",
            "Move(target_node=IN(type=subscript), node=ASTNode(type=], text=]), position=3)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 16,
        "number": 7456,
        "neg_line": [
            "-filtered_target_position, filtered_targets = zip(*filter(lambda x: target_labels[x] == pred_label, enumerate(target_boxes)))",
            "-",
            "-iou, box_filtered_index = bbox_iou(pred_box.unsqueeze(0), filtered_targets).max(0)",
            "-"
        ],
        "pos_line": [
            "+filtered_target_position, filtered_targets = zip(*filter(lambda x: target_labels[x[0]] == pred_label, enumerate(target_boxes)))",
            "+",
            "+iou, box_filtered_index = bbox_iou(pred_box.unsqueeze(0), torch.stack(filtered_targets)).max(0)",
            "+"
        ],
        "core_change": "-filtered_target_position, filtered_targets = zip(*filter(lambda x: target_labels[x] == pred_label, enumerate(target_boxes))) - +filtered_target_position, filtered_targets = zip(*filter(lambda x: target_labels[x[0]] == pred_label, enumerate(target_boxes))) + -iou, box_filtered_index = bbox_iou(pred_box.unsqueeze(0), filtered_targets).max(0) - +iou, box_filtered_index = bbox_iou(pred_box.unsqueeze(0), torch.stack(filtered_targets)).max(0) +",
        "core_API": "unsqueeze"
    },
    {
        "commit_hash": "753d1e6fcde5e6466d915470c68d2158d8d62439",
        "index": "9173de1a..10063786 100644",
        "commit_message": "Fix `pyg-lib` `hetero_neighbor_sampler` usage (#5697)\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class NeighborSampler(BaseSampler):",
            "# TODO (matthias) Add `disjoint` option to `NeighborSampler`",
            "# TODO (matthias) `return_edge_id` if edge features present",
            "disjoint = self.node_time_dict is not None",
            "-                out = torch.ops.pyg.hetero_neighbor_sample_cpu(",
            "+                out = torch.ops.pyg.hetero_neighbor_sample(",
            "self.node_types,",
            "self.edge_types,",
            "self.colptr_dict,"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=hetero_neighbor_sample_cpu), value='hetero_neighbor_sample')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 7457,
        "neg_line": [
            "-out = torch.ops.pyg.hetero_neighbor_sample_cpu("
        ],
        "pos_line": [
            "+out = torch.ops.pyg.hetero_neighbor_sample("
        ],
        "core_change": "-out = torch.ops.pyg.hetero_neighbor_sample_cpu( +out = torch.ops.pyg.hetero_neighbor_sample(",
        "core_API": "hetero_neighbor_sample_cpu"
    },
    {
        "commit_hash": "a08fc92533723c39e08ba5a38bd7f1a16585c174",
        "index": "3b7dd4930a..c24a9ae461 100644",
        "commit_message": "small fixes for various gradient tests.\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "functional",
        "change": [
            "def test_stop_gradient(x_raw, dtype, tensor_fn, device, call):",
            "# Tf graph mode cannot create variables as part of the computation graph",
            "assert np.array_equal(",
            "call(ivy.stop_gradient, x),",
            "-            ivy.functional.backends.numpy.array(x_raw, dtype),",
            "+            np.array(x_raw, dtype=dtype),",
            ")",
            "# compilation test",
            "if call in [helpers.torch_call]:"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=ivy), value='np')",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=ivy), position=0)",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1731902)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=1731903)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1731904)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=identifier, text=dtype), position=2)",
            "Delete(target_node=ASTNode(type=identifier, text=functional))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=backends))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=numpy))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 16,
        "number": 7463,
        "neg_line": [
            "-ivy.functional.backends.numpy.array(x_raw, dtype),"
        ],
        "pos_line": [
            "+np.array(x_raw, dtype=dtype),"
        ],
        "core_change": "-ivy.functional.backends.numpy.array(x_raw, dtype), +np.array(x_raw, dtype=dtype),",
        "core_API": "array_equal"
    },
    {
        "commit_hash": "10258724d135656da526d5f6b5aaaeee787e4f04",
        "index": "48083a2a..f810e213 100644",
        "commit_message": "linter fixes\n\n",
        "file": "TTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def run_model_torch(model, inputs, CONFIG, truncated, speaker_id=None, style_mel",
            "decoder_output, postnet_output, alignments, stop_tokens = model.inference(",
            "inputs, speaker_ids=speaker_id, speaker_embeddings=speaker_embeddings)",
            "elif 'glow' in CONFIG.model.lower():",
            "-        inputs_lengths = torch.tensor(inputs.shape[1:2]).to(inputs.device)",
            "+        inputs_lengths = torch.tensor(inputs.shape[1:2]).to(inputs.device)  # pylint: disable=not-callable",
            "postnet_output, _, _, _, alignments, _, _ = model.inference(inputs, inputs_lengths)",
            "postnet_output = postnet_output.permute(0, 2, 1)",
            "# these only belong to tacotron models."
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 7465,
        "neg_line": [
            "-inputs_lengths = torch.tensor(inputs.shape[1:2]).to(inputs.device)"
        ],
        "pos_line": [
            "+inputs_lengths = torch.tensor(inputs.shape[1:2]).to(inputs.device)  # pylint: disable=not-callable"
        ],
        "core_change": "-inputs_lengths = torch.tensor(inputs.shape[1:2]).to(inputs.device) +inputs_lengths = torch.tensor(inputs.shape[1:2]).to(inputs.device)  # pylint: disable=not-callable",
        "core_API": "inference"
    },
    {
        "commit_hash": "7977bc56439dcea2ee0c4f27b7ae7919ab620151",
        "index": "3807dc0ea..68a2e93d8 100644",
        "commit_message": "fix\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class FrameScoreFeats(AbsFeatsExtract):",
            "pad = self.win_length // 2",
            "input_lengths = input_lengths + 2 * pad",
            "",
            "-            olens = (",
            "-                torch.div(",
            "-                    (input_lengths - self.win_length),",
            "-                    self.hop_length,",
            "-                    rounding_mode=\"floor\",",
            "-                )",
            "-                + 1",
            "-            )",
            "+            olens = torch.div((input_lengths - self.win_length), self.hop_length) + 1",
            "output.masked_fill_(make_pad_mask(olens, output, 1), 0.0)",
            "else:",
            "olens = None"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=binary_operator), position=2)",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=rounding_mode))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=string, text=\"floor\"))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=parenthesized_expression))"
        ],
        "plus_line": 1,
        "minus_line": 8,
        "AST_diff_line": 10,
        "number": 7466,
        "neg_line": [
            "-olens = (",
            "-torch.div(",
            "-(input_lengths - self.win_length),",
            "-self.hop_length,",
            "-rounding_mode=\"floor\",",
            "-)",
            "-+ 1",
            "-)"
        ],
        "pos_line": [
            "+olens = torch.div((input_lengths - self.win_length), self.hop_length) + 1"
        ],
        "core_change": "-olens = ( -torch.div( -(input_lengths - self.win_length), -self.hop_length, -rounding_mode=\"floor\", -) -+ 1 -) +olens = torch.div((input_lengths - self.win_length), self.hop_length) + 1",
        "core_API": "div"
    },
    {
        "commit_hash": "850970e081687df6427898948a27df37ab4de5d3",
        "index": "1ab3bb6..23a4862 100644",
        "commit_message": "`feature_visualization()` CUDA fix (#3925)\n\n\n",
        "file": "yolov5.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def feature_visualization(x, module_type, stage, n=64, save_dir=Path('runs/detec",
            "f = f\"stage{stage}_{module_type.split('.')[-1]}_features.png\"  # filename",
            "",
            "plt.figure(tight_layout=True)",
            "-            blocks = torch.chunk(x[0], channels, dim=0)  # select batch index 0, block by channels",
            "+            blocks = torch.chunk(x[0].cpu(), channels, dim=0)  # select batch index 0, block by channels",
            "n = min(n, channels)  # number of plots",
            "ax = plt.subplots(math.ceil(n / 8), 8, tight_layout=True)[1].ravel()  # 8 rows x n/8 cols",
            "for i in range(n):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=1, insert_id=1297775)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1297776)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1297777)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=subscript), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1297778)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cpu'), position=2, insert_id=1297779)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1297780)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1297781)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 7471,
        "neg_line": [
            "-blocks = torch.chunk(x[0], channels, dim=0)  # select batch index 0, block by channels"
        ],
        "pos_line": [
            "+blocks = torch.chunk(x[0].cpu(), channels, dim=0)  # select batch index 0, block by channels"
        ],
        "core_change": "-blocks = torch.chunk(x[0], channels, dim=0)  # select batch index 0, block by channels +blocks = torch.chunk(x[0].cpu(), channels, dim=0)  # select batch index 0, block by channels",
        "core_API": "split"
    },
    {
        "commit_hash": "32295b15a131c07884b409e485c3b823bcb5a3dc",
        "index": "998534709..07421bfa5 100644",
        "commit_message": "Gelu10 (#15676)\n\n* Add GeLU10 (clipped version of GeLU) to transformers to improve quantization performances.\n\n* Add unittests.\n\n* Import tensorflow after `is_tf_available` check.\n\n* Fix tensorflow wrong function `tf.tensor` to `tf.constant`\n\n* style.\n\n* use `tf.math.max`\n\n* Fix tf tests.\n\n* style.\n\n* style style style style style style\n\n* style style style style style style\n\n* Address @sgugger comments.\n\n* Fix wrong operator for raising ValueError for ClippedGELUActivation.\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "ACT2FN = {",
            "\"gelu_new\": NewGELUActivation(),",
            "\"gelu_fast\": FastGELUActivation(),",
            "\"quick_gelu\": QuickGELUActivation(),",
            "+    \"gelu_10\": ClippedGELUActivation(-10, 10),",
            "\"mish\": MishActivation(),",
            "\"linear\": LinearActivation(),",
            "\"sigmoid\": nn.Sigmoid(),"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=ERROR), node=('pair', None), position=9, insert_id=1206626)",
            "Insert(target_node=ASTNode(type=ERROR), node=(',', ','), position=10, insert_id=1206627)",
            "Insert(target_node=IN(type=pair), node=('string', '\"gelu_10\"'), position=0, insert_id=1206628)",
            "Insert(target_node=IN(type=pair), node=(':', ':'), position=1, insert_id=1206629)",
            "Insert(target_node=IN(type=pair), node=('call', None), position=2, insert_id=1206630)",
            "Insert(target_node=IN(type=call), node=('identifier', 'ClippedGELUActivation'), position=0, insert_id=1206631)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1206632)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1206633)",
            "Insert(target_node=IN(type=argument_list), node=('unary_operator', '-10'), position=1, insert_id=1206634)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=1206635)",
            "Insert(target_node=IN(type=argument_list), node=('integer', '10'), position=3, insert_id=1206636)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=1206637)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 12,
        "number": 7472,
        "neg_line": [],
        "pos_line": [
            "+\"gelu_10\": ClippedGELUActivation(-10, 10),"
        ],
        "core_change": "+\"gelu_10\": ClippedGELUActivation(-10, 10),",
        "core_API": "Sigmoid"
    },
    {
        "commit_hash": "f46226466550f0c2933df79bf7dd8ef3ef6b9990",
        "index": "0f47b35..83719e8 100644",
        "commit_message": "Fix tests for bleu score\n\n",
        "file": "seq2seq.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def accumulate_strings(values, name=\"strings\"):",
            "initial_value=[],",
            "dtype=tf.string,",
            "trainable=False,",
            "-      collections=[tf.GraphKeys.LOCAL_VARIABLES],",
            "+      collections=[],",
            "validate_shape=True)",
            "value_tensor = tf.identity(strings)",
            "update_op = tf.assign("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=GraphKeys))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=LOCAL_VARIABLES))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 7475,
        "neg_line": [
            "-collections=[tf.GraphKeys.LOCAL_VARIABLES],"
        ],
        "pos_line": [
            "+collections=[],"
        ],
        "core_change": "-collections=[tf.GraphKeys.LOCAL_VARIABLES], +collections=[],",
        "core_API": "identity"
    },
    {
        "commit_hash": "9260b685d714a704b2dc8c64e3e5041b4d3a64c3",
        "index": "4365fcf0..7303f532 100644",
        "commit_message": "fix lint errors\n\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def transform_points(dst_homo_src, points_src):",
            "# to homogeneous",
            "points_src_h = convert_points_to_homogeneous(points_src)  # BxNx3",
            "# transform coordinates",
            "-    points_dst_h = torch.matmul(dst_homo_src, points_src_h.transpose(1, 2))  # Bx3xN",
            "+    points_dst_h = torch.matmul(dst_homo_src, points_src_h.transpose(1, 2))",
            "points_dst_h = points_dst_h.permute(0, 2, 1)  # BxNx3",
            "# to euclidean",
            "points_dst = convert_points_from_homogeneous(points_dst_h)  # BxNx2"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 7480,
        "neg_line": [
            "-points_dst_h = torch.matmul(dst_homo_src, points_src_h.transpose(1, 2))  # Bx3xN"
        ],
        "pos_line": [
            "+points_dst_h = torch.matmul(dst_homo_src, points_src_h.transpose(1, 2))"
        ],
        "core_change": "-points_dst_h = torch.matmul(dst_homo_src, points_src_h.transpose(1, 2))  # Bx3xN +points_dst_h = torch.matmul(dst_homo_src, points_src_h.transpose(1, 2))",
        "core_API": "matmul"
    },
    {
        "commit_hash": "211055216792cbb52ab6d355f698c194f9c55efb",
        "index": "c892316a..62b8e1da 100644",
        "commit_message": "Fixes for torch 1.14 due to new torch.numel return type (#2522)\n\n* fixes for new torch.numel return type\n\n* address comment\n",
        "file": "DeepSpeed.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class NcclBackend(object):",
            "buffer_m = torch.cat([buffer_m, empty_tensor])",
            "",
            "buffer_m.add_(worker_error)",
            "-        worker_scale = torch.norm(buffer_m) / np.sqrt(torch.numel(buffer_m))",
            "+        worker_scale = torch.norm(buffer_m) / np.sqrt(buffer_m.numel())",
            "worker_error.set_(buffer_m - worker_scale *",
            "buffer_m.sign().add_(1).bool().float().add_(-0.5).mul_(2.0))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=buffer_m), position=0)",
            "Delete(target_node=ASTNode(type=identifier, text=torch))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 7483,
        "neg_line": [
            "-worker_scale = torch.norm(buffer_m) / np.sqrt(torch.numel(buffer_m))"
        ],
        "pos_line": [
            "+worker_scale = torch.norm(buffer_m) / np.sqrt(buffer_m.numel())"
        ],
        "core_change": "-worker_scale = torch.norm(buffer_m) / np.sqrt(torch.numel(buffer_m)) +worker_scale = torch.norm(buffer_m) / np.sqrt(buffer_m.numel())",
        "core_API": "cat"
    },
    {
        "commit_hash": "899883644fcccc3482370994740d5882f15d3609",
        "index": "e72f316b3..41caa19cb 100644",
        "commit_message": "Fix test fails and warnings\n\nAttention output was in bnij ordering instead of ijbn which everything\nelse will expect. This was an oversight on my part, and keeps the\nattention inputs/outputs identical to the original code.\n\nAlso moved back from tensor slicing to index_select in rel_shift_bnij to\nmake the tracer happy.\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class XLNetRelativeAttention(nn.Module):",
            "attn_vec = torch.einsum('bnij,jbnd->ibnd', attn_prob, v_head_h)",
            "",
            "if self.output_attentions:",
            "-            return attn_vec, attn_prob",
            "+            return attn_vec, torch.einsum('bnij->ijbn', attn_prob)",
            "",
            "return attn_vec"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=expression_list), node=('call', None), position=2, insert_id=1246286)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1246287)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1246288)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1246289)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1246290)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'einsum'), position=2, insert_id=1246291)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1246292)",
            "Insert(target_node=IN(type=argument_list), node=('string', \"'bnij->ijbn'\"), position=1, insert_id=1246293)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=1246294)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=attn_prob), position=3)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=1246295)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 7493,
        "neg_line": [
            "-return attn_vec, attn_prob"
        ],
        "pos_line": [
            "+return attn_vec, torch.einsum('bnij->ijbn', attn_prob)"
        ],
        "core_change": "-return attn_vec, attn_prob +return attn_vec, torch.einsum('bnij->ijbn', attn_prob)",
        "core_API": "einsum"
    },
    {
        "commit_hash": "85f894e03d63a7085f6144057e305a4a8e926d04",
        "index": "8110fcc..b96d774 100644",
        "commit_message": "Fix ViT in21k representation (pre_logits) layer handling across old and new npz checkpoints\n\n",
        "file": "pytorch-image-models.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "def _load_weights(model: nn.Module, checkpoint_path: str, prefix: str = 'resnet/",
            "model.stem.conv.weight.copy_(stem_conv_w)",
            "model.norm.weight.copy_(t2p(weights[f'{prefix}group_norm/gamma']))",
            "model.norm.bias.copy_(t2p(weights[f'{prefix}group_norm/beta']))",
            "-    if isinstance(model.head.fc, nn.Conv2d) and \\",
            "+    if isinstance(getattr(model.head, 'fc', None), nn.Conv2d) and \\",
            "model.head.fc.weight.shape[0] == weights[f'{prefix}head/conv2d/kernel'].shape[-1]:",
            "model.head.fc.weight.copy_(t2p(weights[f'{prefix}head/conv2d/kernel']))",
            "model.head.fc.bias.copy_(t2p(weights[f'{prefix}head/conv2d/bias']))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=1, insert_id=1477772)",
            "Insert(target_node=IN(type=call), node=('identifier', 'getattr'), position=0, insert_id=1477773)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1477774)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1477775)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=attribute), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=1477776)",
            "Insert(target_node=IN(type=argument_list), node=('string', \"'fc'\"), position=3, insert_id=1477777)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=4, insert_id=1477778)",
            "Insert(target_node=IN(type=argument_list), node=('none', 'None'), position=5, insert_id=1477779)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=6, insert_id=1477780)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=fc))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 13,
        "number": 7495,
        "neg_line": [
            "-if isinstance(model.head.fc, nn.Conv2d) and \\"
        ],
        "pos_line": [
            "+if isinstance(getattr(model.head, 'fc', None), nn.Conv2d) and \\"
        ],
        "core_change": "-if isinstance(model.head.fc, nn.Conv2d) and \\ +if isinstance(getattr(model.head, 'fc', None), nn.Conv2d) and \\",
        "core_API": "copy_"
    },
    {
        "commit_hash": "6bda07558398bcbe8611c17995ed244796341cb0",
        "index": "d8e222ad..1f8e5206 100644",
        "commit_message": "Minor fix for `pyg-lib` usage in `HeteroLinear` and `RGCNConv` (#5510)\n\npyg_lib pathway is chosen if cuda is available and pyg_lib is available.\nbut if the input is not cuda we should not be using the pyg_lib pathway.\nsimple fix\n\nwithout this a ton of CI fails for testing rgcnconv and heterolinear w/\ncpu inputs\n\n\n[errors.txt](https://github.com/pyg-team/pytorch_geometric/files/9629902/errors.txt)\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "cuda",
        "change": [
            "class HeteroLinear(torch.nn.Module):",
            "self.is_sorted = is_sorted",
            "self.kwargs = kwargs",
            "",
            "-        self._WITH_PYG_LIB = torch.cuda.is_available() and _WITH_PYG_LIB",
            "+        self._WITH_PYG_LIB = _WITH_PYG_LIB",
            "",
            "if self._WITH_PYG_LIB:",
            "self.weight = torch.nn.Parameter("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=identifier, text=_WITH_PYG_LIB), position=2)",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=cuda))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=is_available))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=and, text=and))",
            "Delete(target_node=ASTNode(type=boolean_operator))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 14,
        "number": 7497,
        "neg_line": [
            "-self._WITH_PYG_LIB = torch.cuda.is_available() and _WITH_PYG_LIB"
        ],
        "pos_line": [
            "+self._WITH_PYG_LIB = _WITH_PYG_LIB"
        ],
        "core_change": "-self._WITH_PYG_LIB = torch.cuda.is_available() and _WITH_PYG_LIB +self._WITH_PYG_LIB = _WITH_PYG_LIB",
        "core_API": "is_available"
    },
    {
        "commit_hash": "ad8b2b9d08eec5271581dabdf75aec3a83190f0b",
        "index": "11cab675..c57572e2 100644",
        "commit_message": "hyperpose compatible:\n(1)maxpool and batchnorm dataformat debuged,support \"channels_first\"\n(2)vgg forward fixed\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "layers",
        "change": [
            "class VGG(Model):",
            "",
            "inputs = inputs * 255 - np.array([123.68, 116.779, 103.939], dtype=np.float32).reshape([1, 1, 1, 3])",
            "",
            "-        out = self.layers(inputs)",
            "+        out = self.layers.forward(inputs)",
            "return out"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=2625102)",
            "Insert(target_node=ASTNode(type=attribute), node=('identifier', 'forward'), position=2, insert_id=2625103)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 3,
        "number": 7499,
        "neg_line": [
            "-out = self.layers(inputs)"
        ],
        "pos_line": [
            "+out = self.layers.forward(inputs)"
        ],
        "core_change": "-out = self.layers(inputs) +out = self.layers.forward(inputs)",
        "core_API": "array"
    },
    {
        "commit_hash": "c05ace221339102abb1c8b0933b9cbfd51ddd4a3",
        "index": "1b4d381a9..f49db1f93 100644",
        "commit_message": "Use f-strings in the dataset scripts (#3291)\n\n* Finishes #3257\n\nUsed f-strings to format the .py files in the dataset folder\n\n* Fix style\n\n* Fix hkcancor dataset\n\nCo-authored-by: Mario Šaško <mario@huggingface.co>\nCo-authored-by: mariosasko <mariosasko777@gmail.com>\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class TeluguBooks(datasets.GeneratorBasedBuilder):",
            "path_to_manual_file = os.path.abspath(os.path.expanduser(dl_manager.manual_dir))",
            "if not os.path.exists(path_to_manual_file):",
            "raise FileNotFoundError(",
            "-                \"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('telugu_books', data_dir=...)` that includes file name {}. Manual download instructions: {}\".format(",
            "-                    path_to_manual_file,",
            "-                    _FILENAME,",
            "-                    self.manual_download_instructions,",
            "-                )",
            "+                f\"{path_to_manual_file} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('telugu_books', data_dir=...)` that includes file name {_FILENAME}. Manual download instructions: {self.manual_download_instructions}\"",
            ")",
            "return [",
            "datasets.SplitGenerator("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('string', 'f\"{path_to_manual_file} does not exist. Make sure you insert a manual dir via `datasets.load_dataset(\\'telugu_books\\', data_dir=...)` that includes file name {_FILENAME}. Manual download instructions: {self.manual_download_instructions}\"'), position=1, insert_id=1781652)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=), text=)), position=2)",
            "Delete(target_node=ASTNode(type=string, text=\"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('telugu_books', data_dir=...)` that includes file name {}. Manual download instructions: {}\"))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=format))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=path_to_manual_file))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=_FILENAME))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=manual_download_instructions))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=), text=)))"
        ],
        "plus_line": 1,
        "minus_line": 5,
        "AST_diff_line": 19,
        "number": 7504,
        "neg_line": [
            "-\"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('telugu_books', data_dir=...)` that includes file name {}. Manual download instructions: {}\".format(",
            "-path_to_manual_file,",
            "-_FILENAME,",
            "-self.manual_download_instructions,",
            "-)"
        ],
        "pos_line": [
            "+f\"{path_to_manual_file} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('telugu_books', data_dir=...)` that includes file name {_FILENAME}. Manual download instructions: {self.manual_download_instructions}\""
        ],
        "core_change": "-\"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('telugu_books', data_dir=...)` that includes file name {}. Manual download instructions: {}\".format( -path_to_manual_file, -_FILENAME, -self.manual_download_instructions, -) +f\"{path_to_manual_file} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('telugu_books', data_dir=...)` that includes file name {_FILENAME}. Manual download instructions: {self.manual_download_instructions}\"",
        "core_API": "abspath"
    },
    {
        "commit_hash": "fec3112cbacf19cd211109b1bc0b33a502595aca",
        "index": "bebbe719..c4d7c6a6 100644",
        "commit_message": "Update to PyTorch 1.9.0 (#2887)\n\n* Update to PyTorch 1.9.0\n\n* Update torch.cholesky, torch.symeig\n\n* Fix torch.tensordot, torch.qr, funsor dependency\n\n* Fix torch import, AutoGuideList usage\n\n* Ignore bug in PyTorch jit\n\n* Ignore tracer warnings\n\n* Replace torch.solve with torch.linalg.solve\n\n* Xfail vectorized markov funsor tests\n\n* Update funsor version\n\n* Switch MNIST mirrors\n\n* Pin pillow version\n\n* Bump torchvision version\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class GammaGaussian:",
            "Integrates out all latent state (i.e. operating on event dimensions) of Gaussian component.",
            "\"\"\"",
            "n = self.dim()",
            "-        chol_P = self.precision.cholesky()",
            "+        chol_P = torch.linalg.cholesky(self.precision)",
            "chol_P_u = self.info_vec.unsqueeze(-1).triangular_solve(chol_P, upper=False).solution.squeeze(-1)",
            "u_P_u = chol_P_u.pow(2).sum(-1)",
            "# considering GammaGaussian as a Gaussian with precision = s * precision, info_vec = s * info_vec,"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=677034)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=attribute), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=677035)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=677036)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'linalg'), position=2, insert_id=677037)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 7511,
        "neg_line": [
            "-chol_P = self.precision.cholesky()"
        ],
        "pos_line": [
            "+chol_P = torch.linalg.cholesky(self.precision)"
        ],
        "core_change": "-chol_P = self.precision.cholesky() +chol_P = torch.linalg.cholesky(self.precision)",
        "core_API": "dim"
    },
    {
        "commit_hash": "e13aae416a5983bcfece80cf6acc7361e1269552",
        "index": "f3df14d0..edba197e 100644",
        "commit_message": "fix #2285 (#2286)\n\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class MultiLabelField(Field[torch.Tensor]):",
            "def as_tensor(self, padding_lengths: Dict[str, int]) -> torch.Tensor:",
            "# pylint: disable=unused-argument",
            "",
            "-        tensor = torch.zeros(self._num_labels)  # vector of zeros",
            "+        tensor = torch.zeros(self._num_labels, dtype=torch.long)  # vector of zeros",
            "if self._label_ids:",
            "tensor.scatter_(0, torch.LongTensor(self._label_ids), 1)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=31603)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=31604)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=31605)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=31606)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=31607)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=31608)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=31609)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'long'), position=2, insert_id=31610)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 7513,
        "neg_line": [
            "-tensor = torch.zeros(self._num_labels)  # vector of zeros"
        ],
        "pos_line": [
            "+tensor = torch.zeros(self._num_labels, dtype=torch.long)  # vector of zeros"
        ],
        "core_change": "-tensor = torch.zeros(self._num_labels)  # vector of zeros +tensor = torch.zeros(self._num_labels, dtype=torch.long)  # vector of zeros",
        "core_API": "zeros"
    },
    {
        "commit_hash": "595bf86e69ad7452498f32166ece985d9cc012be",
        "index": "279ce530..35496b3c 100644",
        "commit_message": "Fix RepPoints TTA (#3435)\n\n\n",
        "file": "mmdetection.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class RepPointsDetector(SingleStageDetector):",
            "for x, img_meta in zip(feats, img_metas):",
            "# only one image in the batch",
            "outs = self.bbox_head(x)",
            "-            bbox_inputs = outs + (img_metas, self.test_cfg, False, False)",
            "+            bbox_inputs = outs + (img_meta, self.test_cfg, False, False)",
            "det_bboxes, det_scores = self.bbox_head.get_bboxes(*bbox_inputs)[0]",
            "aug_bboxes.append(det_bboxes)",
            "aug_scores.append(det_scores)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=img_metas), value='img_meta')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 7514,
        "neg_line": [
            "-bbox_inputs = outs + (img_metas, self.test_cfg, False, False)"
        ],
        "pos_line": [
            "+bbox_inputs = outs + (img_meta, self.test_cfg, False, False)"
        ],
        "core_change": "-bbox_inputs = outs + (img_metas, self.test_cfg, False, False) +bbox_inputs = outs + (img_meta, self.test_cfg, False, False)",
        "core_API": "bbox_head"
    },
    {
        "commit_hash": "23efadd9a394d9d69e10e8539a7332665b118df0",
        "index": "c59f860c..632dc5ae 100644",
        "commit_message": "upgrade to pytorch 1.2 (#3182)\n\n* first attempt at pytorch 1.2\n\n* explicit is better than implicit\n\n* more explicit\n\n* attempt to fix flaky tests\n\n* pylint\n\n* no disable dropout\n\n* disable dropout by default\n\n* restore dropout, don't deepcopy\n\n* change batch size for biaffine_dependency_parser_multilang_test, maybe that will make it pass? :(\n\n* try batch size 10\n\n* ignore bad gradient parameter\n\n* cleanup\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class FBetaMeasure(Metric):",
            "",
            "if mask is None:",
            "mask = torch.ones_like(gold_labels)",
            "-        mask = mask.to(torch.uint8)",
            "+        mask = mask.to(dtype=torch.bool)",
            "gold_labels = gold_labels.float()",
            "",
            "argmax_predictions = predictions.max(dim=-1)[1].float()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=1, insert_id=24370)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=24371)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=24372)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=attribute), position=2)",
            "Update(target_node=ASTNode(type=identifier, text=uint8), value='bool')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 7517,
        "neg_line": [
            "-mask = mask.to(torch.uint8)"
        ],
        "pos_line": [
            "+mask = mask.to(dtype=torch.bool)"
        ],
        "core_change": "-mask = mask.to(torch.uint8) +mask = mask.to(dtype=torch.bool)",
        "core_API": "ones_like"
    },
    {
        "commit_hash": "013e48495c4bf9bf5dbb0c58fc356b94a6c1b824",
        "index": "2d8eaa41..07133841 100644",
        "commit_message": "fix init method in order to load models with previous SequenceTagger\n\n",
        "file": "flair.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ViterbiLoss(torch.nn.Module):",
            "targets = torch.tensor(formatted_targets, dtype=torch.long).unsqueeze(2).to(flair.device)",
            "",
            "# Squeeze crf scores matrices in 1-dim shape and gather scores at targets by matrix indices",
            "-        scores_at_targets = torch.gather(features.view(batch_size, seq_len, -1), 2, targets).squeeze(0).squeeze(0)",
            "+        scores_at_targets = torch.gather(features.view(batch_size, seq_len, -1), 2, targets)",
            "scores_at_targets = pack_padded_sequence(scores_at_targets, lengths.values, batch_first=True)[0]",
            "gold_score = scores_at_targets.sum()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=IN(type=root), node=ASTNode(type=call), position=0)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=squeeze))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=integer, text=0))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=squeeze))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=integer, text=0))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 16,
        "number": 7518,
        "neg_line": [
            "-scores_at_targets = torch.gather(features.view(batch_size, seq_len, -1), 2, targets).squeeze(0).squeeze(0)"
        ],
        "pos_line": [
            "+scores_at_targets = torch.gather(features.view(batch_size, seq_len, -1), 2, targets)"
        ],
        "core_change": "-scores_at_targets = torch.gather(features.view(batch_size, seq_len, -1), 2, targets).squeeze(0).squeeze(0) +scores_at_targets = torch.gather(features.view(batch_size, seq_len, -1), 2, targets)",
        "core_API": "tensor"
    },
    {
        "commit_hash": "4be551a1069e62ba8a5e7c12537c6672e3f11c6f",
        "index": "0992a28..be91b6b 100644",
        "commit_message": "Fix keep_dims/dims parameter deprecation of new vension of TF\n\n",
        "file": "CapsNet-Tensorflow.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class CapsNet(object):",
            "else:",
            "# self.masked_v = tf.matmul(tf.squeeze(self.caps2), tf.reshape(self.Y, (-1, 10, 1)), transpose_a=True)",
            "self.masked_v = tf.multiply(tf.squeeze(self.caps2), tf.reshape(self.Y, (-1, 10, 1)))",
            "-                self.v_length = tf.sqrt(tf.reduce_sum(tf.square(self.caps2), axis=2, keep_dims=True) + epsilon)",
            "+                self.v_length = tf.sqrt(reduce_sum(tf.square(self.caps2), axis=2, keepdims=True) + epsilon)",
            "",
            "# 2. Reconstructe the MNIST images with 3 FC layers",
            "# [batch_size, 1, 16, 1] => [batch_size, 16] => [batch_size, 512]"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=reduce_sum), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=keep_dims), value='keepdims')",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 7519,
        "neg_line": [
            "-self.v_length = tf.sqrt(tf.reduce_sum(tf.square(self.caps2), axis=2, keep_dims=True) + epsilon)"
        ],
        "pos_line": [
            "+self.v_length = tf.sqrt(reduce_sum(tf.square(self.caps2), axis=2, keepdims=True) + epsilon)"
        ],
        "core_change": "-self.v_length = tf.sqrt(tf.reduce_sum(tf.square(self.caps2), axis=2, keep_dims=True) + epsilon) +self.v_length = tf.sqrt(reduce_sum(tf.square(self.caps2), axis=2, keepdims=True) + epsilon)",
        "core_API": "matmul"
    },
    {
        "commit_hash": "c117200d11d0b25c4453ab90661de7492c522f2f",
        "index": "9aa96717..0c1ef6a7 100644",
        "commit_message": "Fix docs requirements and cevae link (#2225)\n\n* Fix docs requirements and cevae link\n\n* Install only cpu version of torchvision on rtd\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def setup(app):",
            "",
            "",
            "# @jpchen's hack to get rtd builder to install latest pytorch",
            "+# See similar line in the install section of .travis.yml",
            "if 'READTHEDOCS' in os.environ:",
            "-    os.system('pip install torch==1.3.0+cpu -f https://download.pytorch.org/whl/torch_stable.html')",
            "+    os.system('pip install torch==1.3.0+cpu torchvision==0.4.0+cpu '",
            "+              '-f https://download.pytorch.org/whl/torch_stable.html')"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('concatenated_string', None), position=1, insert_id=705964)",
            "Update(target_node=ASTNode(type=string, text='pip install torch==1.3.0+cpu -f https://download.pytorch.org/whl/torch_stable.html'), value=\"'pip install torch==1.3.0+cpu torchvision==0.4.0+cpu '\")",
            "Move(target_node=IN(type=concatenated_string), node=ASTNode(type=string, text='pip install torch==1.3.0+cpu -f https://download.pytorch.org/whl/torch_stable.html'), position=0)",
            "Insert(target_node=IN(type=concatenated_string), node=('string', \"'-f https://download.pytorch.org/whl/torch_stable.html'\"), position=1, insert_id=705965)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 7523,
        "neg_line": [
            "-os.system('pip install torch==1.3.0+cpu -f https://download.pytorch.org/whl/torch_stable.html')"
        ],
        "pos_line": [
            "+# See similar line in the install section of .travis.yml",
            "+os.system('pip install torch==1.3.0+cpu torchvision==0.4.0+cpu '",
            "+'-f https://download.pytorch.org/whl/torch_stable.html')"
        ],
        "core_change": "+# See similar line in the install section of .travis.yml -os.system('pip install torch==1.3.0+cpu -f https://download.pytorch.org/whl/torch_stable.html') +os.system('pip install torch==1.3.0+cpu torchvision==0.4.0+cpu ' +'-f https://download.pytorch.org/whl/torch_stable.html')",
        "core_API": "system"
    },
    {
        "commit_hash": "6bff9c75dcfe936dad58c2ce80591864e873161e",
        "index": "6ed47b8..e251d0c 100644",
        "commit_message": "Cleanup model_factory imports, consistent __all__ for models, fixed inception_v4 weight url\n\n",
        "file": "pytorch-image-models.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class InceptionV4(nn.Module):",
            "x = self.forward_features(x)",
            "if self.drop_rate > 0:",
            "x = F.dropout(x, p=self.drop_rate, training=self.training)",
            "-        x = self.classif(x)",
            "+        x = self.last_linear(x)",
            "return x"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=classif), value='last_linear')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 7527,
        "neg_line": [
            "-x = self.classif(x)"
        ],
        "pos_line": [
            "+x = self.last_linear(x)"
        ],
        "core_change": "-x = self.classif(x) +x = self.last_linear(x)",
        "core_API": "forward_features"
    },
    {
        "commit_hash": "02fe540f22fa83d6b7e1009037bc6402103f5637",
        "index": "01b688df..91f35419 100644",
        "commit_message": "Fix lint errors due to flake8 upgrade\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Uniform(Distribution):",
            "if x.size != a.size():",
            "a = a.expand_as(x)",
            "b = b.expand_as(x)",
            "-        l = x.ge(a).type_as(a)",
            "-        u = x.le(b).type_as(b)",
            "+        lb = x.ge(a).type_as(a)",
            "+        ub = x.le(b).type_as(b)",
            "batch_log_pdf_shape = self.batch_shape(a, b) + (1,)",
            "-        return torch.sum(torch.log(l.mul(u)) - torch.log(b - a), -1).contiguous().view(batch_log_pdf_shape)",
            "+        return torch.sum(torch.log(lb.mul(ub)) - torch.log(b - a), -1).contiguous().view(batch_log_pdf_shape)",
            "",
            "def analytic_mean(self, a=None, b=None):",
            "a, b = self._sanitize_input(a, b)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=l), value='lb')",
            "Update(target_node=ASTNode(type=identifier, text=u), value='ub')",
            "Update(target_node=ASTNode(type=identifier, text=l), value='lb')",
            "Update(target_node=ASTNode(type=identifier, text=u), value='ub')"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 4,
        "number": 7528,
        "neg_line": [
            "-l = x.ge(a).type_as(a)",
            "-u = x.le(b).type_as(b)",
            "-return torch.sum(torch.log(l.mul(u)) - torch.log(b - a), -1).contiguous().view(batch_log_pdf_shape)"
        ],
        "pos_line": [
            "+lb = x.ge(a).type_as(a)",
            "+ub = x.le(b).type_as(b)",
            "+return torch.sum(torch.log(lb.mul(ub)) - torch.log(b - a), -1).contiguous().view(batch_log_pdf_shape)"
        ],
        "core_change": "-l = x.ge(a).type_as(a) -u = x.le(b).type_as(b) +lb = x.ge(a).type_as(a) +ub = x.le(b).type_as(b) -return torch.sum(torch.log(l.mul(u)) - torch.log(b - a), -1).contiguous().view(batch_log_pdf_shape) +return torch.sum(torch.log(lb.mul(ub)) - torch.log(b - a), -1).contiguous().view(batch_log_pdf_shape)",
        "core_API": "size"
    },
    {
        "commit_hash": "6aaeb70670d4540900e55f4ac532546656447bd2",
        "index": "93e81173..f17761e7 100644",
        "commit_message": "Add bilinear attention (#1349)\n\n* Add bilinear attention, some code cleanup, make semantic parsers use new attention\n\n* Fix pylint and docs\n\n* Increase beam size for wikitables ERM parser, so test doesn't fail\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class CosineAttention(Attention):",
            "\"\"\"",
            "Computes attention between a vector and a matrix using cosine similarity.",
            "\"\"\"",
            "-",
            "@overrides",
            "-    def _forward_internal(self,",
            "-                          vector: torch.Tensor,",
            "-                          matrix: torch.Tensor,",
            "-                          matrix_mask: torch.Tensor = None) -> torch.Tensor:",
            "+    def _forward_internal(self, vector: torch.Tensor, matrix: torch.Tensor) -> torch.Tensor:",
            "a_norm = vector / (vector.norm(p=2, dim=-1, keepdim=True) + 1e-13)",
            "b_norm = matrix / (matrix.norm(p=2, dim=-1, keepdim=True) + 1e-13)",
            "return torch.bmm(a_norm.unsqueeze(dim=1), b_norm.transpose(-1, -2)).squeeze(1)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=matrix_mask))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=Tensor))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=type))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=none, text=None))",
            "Delete(target_node=ASTNode(type=typed_default_parameter))"
        ],
        "plus_line": 1,
        "minus_line": 4,
        "AST_diff_line": 11,
        "number": 7539,
        "neg_line": [
            "-",
            "-def _forward_internal(self,",
            "-vector: torch.Tensor,",
            "-matrix: torch.Tensor,",
            "-matrix_mask: torch.Tensor = None) -> torch.Tensor:"
        ],
        "pos_line": [
            "+def _forward_internal(self, vector: torch.Tensor, matrix: torch.Tensor) -> torch.Tensor:"
        ],
        "core_change": "- -def _forward_internal(self, -vector: torch.Tensor, -matrix: torch.Tensor, -matrix_mask: torch.Tensor = None) -> torch.Tensor: +def _forward_internal(self, vector: torch.Tensor, matrix: torch.Tensor) -> torch.Tensor:",
        "core_API": "norm"
    },
    {
        "commit_hash": "b6a671703b7e9e20b6a6992d0765c29e12b714f0",
        "index": "c0f89550..f5ab2991 100644",
        "commit_message": "fix reshape for depth2normals\n\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SpatialGradient(nn.Module):",
            "self.kernel.size(2) // 2,",
            "self.kernel.size(2) // 2]",
            "out_channels: int = 3 if self.order == 2 else 2",
            "-        padded_inp: torch.Tensor = F.pad(input.view(b * c, 1, h, w), spatial_pad, 'replicate')[:, :, None]",
            "+        padded_inp: torch.Tensor = F.pad(input.reshape(b * c, 1, h, w), spatial_pad, 'replicate')[:, :, None]",
            "return F.conv3d(padded_inp, kernel_flip, padding=0).view(b, c, out_channels, h, w)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=view), value='reshape')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 7549,
        "neg_line": [
            "-padded_inp: torch.Tensor = F.pad(input.view(b * c, 1, h, w), spatial_pad, 'replicate')[:, :, None]"
        ],
        "pos_line": [
            "+padded_inp: torch.Tensor = F.pad(input.reshape(b * c, 1, h, w), spatial_pad, 'replicate')[:, :, None]"
        ],
        "core_change": "-padded_inp: torch.Tensor = F.pad(input.view(b * c, 1, h, w), spatial_pad, 'replicate')[:, :, None] +padded_inp: torch.Tensor = F.pad(input.reshape(b * c, 1, h, w), spatial_pad, 'replicate')[:, :, None]",
        "core_API": "size"
    },
    {
        "commit_hash": "bc1f93c29936d50f91ade1fcf346f3db17ad6ab4",
        "index": "816813c8..5d501036 100644",
        "commit_message": "Fix device allocation\n",
        "file": "TTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SSIMLoss(torch.nn.Module):",
            "",
            "if ssim_loss.item() > 1.0:",
            "print(f\" > SSIM loss is out-of-range {ssim_loss.item()}, setting it 1.0\")",
            "-            ssim_loss = torch.tensor([1.0])",
            "+            ssim_loss = torch.tensor([1.0], device=ssim_loss.device)",
            "",
            "if ssim_loss.item() < 0.0:",
            "print(f\" > SSIM loss is out-of-range {ssim_loss.item()}, setting it 0.0\")",
            "-            ssim_loss = torch.tensor([0.0])",
            "+            ssim_loss = torch.tensor([0.0], device=ssim_loss.device)",
            "",
            "return ssim_loss"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1252673)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1252674)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1252675)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tensor'), position=2, insert_id=1252676)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1252677)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1252678)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1252679)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1252680)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=1252681)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1252682)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=1252683)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=1252684)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1252685)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=1252686)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'ssim_loss'), position=0, insert_id=1252687)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1252688)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=1252689)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'ssim_loss'), position=0, insert_id=1252690)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1252691)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=1252692)",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=tensor))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 25,
        "number": 7559,
        "neg_line": [
            "-ssim_loss = torch.tensor([1.0])",
            "-ssim_loss = torch.tensor([0.0])"
        ],
        "pos_line": [
            "+ssim_loss = torch.tensor([1.0], device=ssim_loss.device)",
            "+ssim_loss = torch.tensor([0.0], device=ssim_loss.device)"
        ],
        "core_change": "-ssim_loss = torch.tensor([1.0]) +ssim_loss = torch.tensor([1.0], device=ssim_loss.device) -ssim_loss = torch.tensor([0.0]) +ssim_loss = torch.tensor([0.0], device=ssim_loss.device)",
        "core_API": "item"
    },
    {
        "commit_hash": "7951741769c872368a490b301a73844ec42bb462",
        "index": "58c839edf..1e0181e17 100644",
        "commit_message": "Fix Spanish data file URL in wiki_lingua dataset (#3806)\n\n* Fix Spanish data file URL in wiki_lingua dataset\n\n* Update metadata JSON\n\n* Fix URL also in create_dummy.py script\n\n* Change dataset version\n\n* Update metadata JSON\n\n* Update dummy data paths\n\n* Fix Spanish URL in path to dummy data\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "_URLs = {",
            "class WikiLingua(datasets.GeneratorBasedBuilder):",
            "\"\"\"TODO: Short description of my dataset.\"\"\"",
            "",
            "-    VERSION = datasets.Version(\"1.1.0\")",
            "+    VERSION = datasets.Version(\"1.1.1\")",
            "",
            "# This is an example of a dataset with multiple configurations.",
            "# If you don't want/need to define several sub-sets in your dataset,"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"1.1.0\"), value='\"1.1.1\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 7560,
        "neg_line": [
            "-VERSION = datasets.Version(\"1.1.0\")"
        ],
        "pos_line": [
            "+VERSION = datasets.Version(\"1.1.1\")"
        ],
        "core_change": "-VERSION = datasets.Version(\"1.1.0\") +VERSION = datasets.Version(\"1.1.1\")",
        "core_API": "Version"
    },
    {
        "commit_hash": "1c86dba3ec3658ab32d1a05efcd063ea8cad73d5",
        "index": "32fb6df5..fce46e52 100644",
        "commit_message": "Fix the flakiness of oed example (#1645)\n\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class GPBayesOptimizer(pyro.optim.multi.MultiOptimizer):",
            "x_init = self.gpmodel.X.new_empty(1).uniform_(",
            "self.constraints.lower_bound, self.constraints.upper_bound)",
            "x, y = self.find_a_candidate(differentiable, x_init)",
            "+            if torch.isnan(y):",
            "+                continue",
            "candidates.append(x)",
            "values.append(y)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=3, insert_id=726366)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=726367)",
            "Insert(target_node=IN(type=if_statement), node=('call', None), position=1, insert_id=726368)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=726369)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=726370)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=726371)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=726372)",
            "Insert(target_node=IN(type=block), node=('continue_statement', None), position=0, insert_id=726373)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=726374)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=726375)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'isnan'), position=2, insert_id=726376)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=726377)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'y'), position=1, insert_id=726378)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=726379)",
            "Insert(target_node=IN(type=continue_statement), node=('continue', 'continue'), position=0, insert_id=726380)"
        ],
        "plus_line": 2,
        "minus_line": 0,
        "AST_diff_line": 15,
        "number": 7564,
        "neg_line": [],
        "pos_line": [
            "+if torch.isnan(y):",
            "+continue"
        ],
        "core_change": "+if torch.isnan(y): +continue",
        "core_API": "new_empty"
    },
    {
        "commit_hash": "845e5d84923a887b795045ef2b78fe0ac85aa8b2",
        "index": "f8ba2fed..908cd549 100644",
        "commit_message": "Fix `GraphStore` with empty edge indices (#4968)\n\n* fix graphstore\n\n* changelog:\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def edge_tensor_type_to_adj_type(",
            "size=(src.size()[0] + dst.size()[0], ))",
            "return out.view(2, -1)",
            "",
            "-        return torch.stack(tensor_tuple, dim=0)",
            "+        return torch.stack([src, dst], dim=0)",
            "",
            "elif attr.layout == EdgeLayout.CSR:  # CSR: (rowptr, col)",
            "return SparseTensor(rowptr=src, col=dst, is_sorted=True,"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('list', None), position=1, insert_id=974312)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=974313)",
            "Insert(target_node=IN(type=list), node=('[', '['), position=0, insert_id=974314)",
            "Insert(target_node=IN(type=list), node=('identifier', 'src'), position=1, insert_id=974315)",
            "Move(target_node=IN(type=list), node=ASTNode(type=,, text=,), position=2)",
            "Insert(target_node=IN(type=list), node=('identifier', 'dst'), position=3, insert_id=974316)",
            "Insert(target_node=IN(type=list), node=(']', ']'), position=4, insert_id=974317)",
            "Delete(target_node=ASTNode(type=identifier, text=tensor_tuple))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 7567,
        "neg_line": [
            "-return torch.stack(tensor_tuple, dim=0)"
        ],
        "pos_line": [
            "+return torch.stack([src, dst], dim=0)"
        ],
        "core_change": "-return torch.stack(tensor_tuple, dim=0) +return torch.stack([src, dst], dim=0)",
        "core_API": "size"
    },
    {
        "commit_hash": "8f8fe80d6c3852ac6c9e789aa6bde2db88ce50c9",
        "index": "09a9b5c9..3e3477c0 100644",
        "commit_message": "FeedfreePredictor and example on ImageNet eval (fix #772)\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def dependency_of_fetches(fetches, op):",
            "\"\"\"",
            "try:",
            "from tensorflow.python.client.session import _FetchHandler as FetchHandler",
            "-        handler = FetchHandler(tf.get_default_graph(), fetches, {})",
            "+        # use the graph of the op, so that this function can be called without being under a default graph",
            "+        handler = FetchHandler(op.graph, fetches, {})",
            "targets = tuple(handler.fetches() + handler.targets())",
            "except ImportError:",
            "if isinstance(fetches, list):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=attribute), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=tf), value='op')",
            "Update(target_node=ASTNode(type=identifier, text=get_default_graph), value='graph')",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 7569,
        "neg_line": [
            "-handler = FetchHandler(tf.get_default_graph(), fetches, {})"
        ],
        "pos_line": [
            "+# use the graph of the op, so that this function can be called without being under a default graph",
            "+handler = FetchHandler(op.graph, fetches, {})"
        ],
        "core_change": "-handler = FetchHandler(tf.get_default_graph(), fetches, {}) +# use the graph of the op, so that this function can be called without being under a default graph +handler = FetchHandler(op.graph, fetches, {})",
        "core_API": "get_default_graph"
    },
    {
        "commit_hash": "3a6ebd0ddc07e9ce4cd87c896a9321edd9ea67f6",
        "index": "5c807517..528cf4fe 100644",
        "commit_message": "Temporary patch to fix CI build\n\nPiperOrigin-RevId: 439587488\n\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "models",
        "change": [
            "class ShardedVariableTest(tf.test.TestCase):",
            "self.assertAllClose(got, expect)",
            "self.assertGreater(len(model.variables), len(loaded_model.variables))",
            "",
            "-    with self.assertRaises(ValueError):",
            "-      with self.strategy.scope():",
            "-        keras.models.load_model(saved_dir)",
            "+    with self.strategy.scope():",
            "+      keras.models.load_model(saved_dir)",
            "",
            "def test_slot_variable_checkpointing(self):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=IN(type=root), node=ASTNode(type=with_statement), position=0)",
            "Delete(target_node=ASTNode(type=with, text=with))",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=assertRaises))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=ValueError))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=with_item))",
            "Delete(target_node=ASTNode(type=with_clause))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=block))"
        ],
        "plus_line": 2,
        "minus_line": 3,
        "AST_diff_line": 15,
        "number": 7572,
        "neg_line": [
            "-with self.assertRaises(ValueError):",
            "-with self.strategy.scope():",
            "-keras.models.load_model(saved_dir)"
        ],
        "pos_line": [
            "+with self.strategy.scope():",
            "+keras.models.load_model(saved_dir)"
        ],
        "core_change": "-with self.assertRaises(ValueError): -with self.strategy.scope(): -keras.models.load_model(saved_dir) +with self.strategy.scope(): +keras.models.load_model(saved_dir)",
        "core_API": "assertAllClose"
    },
    {
        "commit_hash": "c54327ec91c71ee74f5034a696c4786cc18bbb59",
        "index": "2deac3c6..9f6e625f 100644",
        "commit_message": "Unify Runner classes via BaseRunner parent class (#328)\n\n* - Finished one TODO: n-step cumulated discounted reward (added horizon parameter with default=0 (no horizon) to the method).\n- Completed commenting on model.py and runner.py class(es).\n- Defined all Model variables in __init__ (mostly None) and commented their purpose and functionality.\n- Fixed pep8 compliance in the Runner classes.\n- ThreadedRunner: Renamed `max_timesteps` into `max_episode_timesteps` to match Runner's `run` implementation (all backwards compatible).\n\n* - corrected some comments in model.py regarding parallel RL (threaded and replica)\nand nature of distributed_spec.\n\n* - added returned game_name to `connect` in unreal_engine.py\n\n* Added registry to Environment class (containing only MinimalTest).\nAdded from_spec method to Environment class so that Envs can now be created through this factory.\nChanged a comment in UE4 script.\n\n* - added returned game_name to `connect` in unreal_engine.py\n\n* Added registry to Environment class (containing only MinimalTest).\nAdded from_spec method to Environment class so that Envs can now be created through this factory.\nChanged a comment in UE4 script.\n\n* Added registry to Environment class (containing only MinimalTest).\nAdded from_spec method to Environment class so that Envs can now be created through this factory.\nChanged a comment in UE4 script.\n\n* - added returned game_name to `connect` in unreal_engine.py\n\n* - solved conflicts with TF master branch\n- fixed grayscale image capture for examples/unreal_engine.py\n\n* Fixed bug for distributed_spec != None.\ntf.train.Saver complained about duplicate variables.\n\n* Fixed warnings due to deprecated function parameters.\n\n* Avoid race-condition where order in episode_rewards won't match order in episode_lengths.\n\n* Add possiblity to pass in model saver-frequency by a) episode count, b) seconds, c) timesteps.\n\n* Introduced BaseRunner class.\nRestructured existing Runner classes (Runner and ThreadedRunner) to both inherit from BaseRunner.\nUnified the interfaces used to control these classes and to handle per-episode and per-n-episode reporting.\nAdded multi-threaded testing to random and constant agent tests (configurable for all agents via run_type flag).\n\n* - added returned game_name to `connect` in unreal_engine.py\n\n* Added registry to Environment class (containing only MinimalTest).\nAdded from_spec method to Environment class so that Envs can now be created through this factory.\nChanged a comment in UE4 script.\n\n* - solved conflicts with TF master branch\n- fixed grayscale image capture for examples/unreal_engine.py\n\n* Fixed warnings due to deprecated function parameters.\n\n* Avoid race-condition where order in episode_rewards won't match order in episode_lengths.\n\n* Add possiblity to pass in model saver-frequency by a) episode count, b) seconds, c) timesteps.\n\n* Introduced BaseRunner class.\nRestructured existing Runner classes (Runner and ThreadedRunner) to both inherit from BaseRunner.\nUnified the interfaces used to control these classes and to handle per-episode and per-n-episode reporting.\nAdded multi-threaded testing to random and constant agent tests (configurable for all agents via run_type flag).\n\n* - added returned game_name to `connect` in unreal_engine.py\n\n* Added registry to Environment class (containing only MinimalTest).\nAdded from_spec method to Environment class so that Envs can now be created through this factory.\nChanged a comment in UE4 script.\n\n* - added returned game_name to `connect` in unreal_engine.py\n\n* - solved conflicts with TF master branch\n- fixed grayscale image capture for examples/unreal_engine.py\n\n* Fixed warnings due to deprecated function parameters.\n\n* Avoid race-condition where order in episode_rewards won't match order in episode_lengths.\n\n* Add possiblity to pass in model saver-frequency by a) episode count, b) seconds, c) timesteps.\n\n* Introduced BaseRunner class.\nRestructured existing Runner classes (Runner and ThreadedRunner) to both inherit from BaseRunner.\nUnified the interfaces used to control these classes and to handle per-episode and per-n-episode reporting.\nAdded multi-threaded testing to random and constant agent tests (configurable for all agents via run_type flag).\n\n* Added registry to Environment class (containing only MinimalTest).\nAdded from_spec method to Environment class so that Envs can now be created through this factory.\nChanged a comment in UE4 script.\n\n* - solved conflicts with TF master branch\n- fixed grayscale image capture for examples/unreal_engine.py\n\n* Fix python 2.x incompatibility (inspect.signature not supported by 2.x)\n\n* Next attempt: Fix python 2.x incompatibility (inspect.signature not supported by 2.x)\n\n* Added run_mode bit-field to test cases. Different modes for any test-agent may be set:\nsingle, multi-threaded, and distributed-tf. The default is `single`.\nAlso added num_parallel_worker setting to all test-agents. This setting determines how many parallel agents\nare used in non-single run_mode tests.\n\n* Bug fix: \"No network-spec\"-error in multi-threaded runs using the WorkerAgent factory.\n\n* Bug fix: Add episode run time to runner stats.\nSend network_spec to WorkerAgent-factory regardless (robust against non-learning agents that don't have a network)\n\n* Bug fix: Multithreaded runner would save many times per one save-interval.\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Grayscale(Preprocessor):",
            "",
            "def tf_process(self, tensor):",
            "weights = tf.reshape(tensor=self.weights, shape=(tuple(1 for _ in range(util.rank(tensor) - 1)) + (3,)))",
            "-        return tf.reduce_sum(input_tensor=(weights * tensor), axis=-1, keep_dims=True)",
            "+        return tf.reduce_sum(input_tensor=(weights * tensor), axis=-1, keepdims=True)",
            "",
            "def processed_shape(self, shape):",
            "return tuple(shape[:-1]) + (1,)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=keep_dims), value='keepdims')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 7579,
        "neg_line": [
            "-return tf.reduce_sum(input_tensor=(weights * tensor), axis=-1, keep_dims=True)"
        ],
        "pos_line": [
            "+return tf.reduce_sum(input_tensor=(weights * tensor), axis=-1, keepdims=True)"
        ],
        "core_change": "-return tf.reduce_sum(input_tensor=(weights * tensor), axis=-1, keep_dims=True) +return tf.reduce_sum(input_tensor=(weights * tensor), axis=-1, keepdims=True)",
        "core_API": "reshape"
    },
    {
        "commit_hash": "6643ade2f7ebd3c91ac0228d69867a48d4a83e40",
        "index": "06960a85..90286454 100644",
        "commit_message": "More cuda fixes (#379)\n\nDefault use_cuda=torch.Tensor.is_cuda in irange and friends\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class RaoBlackwellizationTests(TestCase):",
            "mean_1_error = torch.sum(torch.pow(pyro.param(\"mean_1_%d\" % k), 2.0))",
            "mean_2_error = torch.sum(torch.pow(pyro.param(\"mean_2_%d\" % k), 2.0))",
            "superfluous_error = torch.max(torch.max(mean_0_error, mean_1_error), mean_2_error)",
            "-                    superfluous_errors.append(superfluous_error.data.numpy()[0])",
            "+                    superfluous_errors.append(superfluous_error.data.cpu().numpy()[0])",
            "",
            "if step % 500 == 0 and self.verbose:",
            "print(\"mu error, log(sigma) error:  %.4f, %.4f\" % (mu_error, log_sig_error))"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('call', None), position=0, insert_id=760607)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=760608)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=760609)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=760610)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cpu'), position=2, insert_id=760611)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=760612)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=760613)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 7581,
        "neg_line": [
            "-superfluous_errors.append(superfluous_error.data.numpy()[0])"
        ],
        "pos_line": [
            "+superfluous_errors.append(superfluous_error.data.cpu().numpy()[0])"
        ],
        "core_change": "-superfluous_errors.append(superfluous_error.data.numpy()[0]) +superfluous_errors.append(superfluous_error.data.cpu().numpy()[0])",
        "core_API": "sum"
    },
    {
        "commit_hash": "784072ff925650becb1876e3e153c0782b20dc6a",
        "index": "9bfc61a1..ea62ba69 100644",
        "commit_message": "Fix MNIST url redirection (#2775)\n\n* Fix MNIST url redirection\n\n* Bump pytorch and torchvision versions to test release\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "if 'READTHEDOCS' in os.environ:",
            "# TODO replace with torch_stable before release",
            "# os.system('pip install torch==1.8.0+cpu torchvision==0.9.0+cpu '",
            "#           '-f https://download.pytorch.org/whl/torch_stable.html')",
            "-    # TODO replace with torch_test once torchvision binaries are released",
            "-    # os.system('pip install torch torchvision '",
            "-    #           '-f https://download.pytorch.org/whl/test/cpu/torch_test.html')",
            "-    # This is the last nightly release of 1.8.0 before splitting to 1.9.0.",
            "-    os.system('pip install --pre '",
            "-              'torch==1.8.0.dev20210210+cpu '",
            "-              'torchvision==0.9.0.dev20210210+cpu '",
            "-              '-f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html')",
            "+    os.system('pip install torch==1.8.0+cpu torchvision==0.9.0 '",
            "+              '-f https://download.pytorch.org/whl/test/cpu/torch_test.html')"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('concatenated_string', None), position=0, insert_id=684385)",
            "Insert(target_node=IN(type=concatenated_string), node=('string', \"'pip install torch==1.8.0+cpu torchvision==0.9.0 '\"), position=0, insert_id=684386)",
            "Insert(target_node=IN(type=concatenated_string), node=('string', \"'-f https://download.pytorch.org/whl/test/cpu/torch_test.html'\"), position=1, insert_id=684387)",
            "Delete(target_node=ASTNode(type=string, text='pip install --pre '))",
            "Delete(target_node=ASTNode(type=string, text='torch==1.8.0.dev20210210+cpu '))",
            "Delete(target_node=ASTNode(type=string, text='torchvision==0.9.0.dev20210210+cpu '))",
            "Delete(target_node=ASTNode(type=string, text='-f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html'))",
            "Delete(target_node=ASTNode(type=concatenated_string))"
        ],
        "plus_line": 2,
        "minus_line": 8,
        "AST_diff_line": 8,
        "number": 7586,
        "neg_line": [
            "-# TODO replace with torch_test once torchvision binaries are released",
            "-# os.system('pip install torch torchvision '",
            "-#           '-f https://download.pytorch.org/whl/test/cpu/torch_test.html')",
            "-# This is the last nightly release of 1.8.0 before splitting to 1.9.0.",
            "-os.system('pip install --pre '",
            "-'torch==1.8.0.dev20210210+cpu '",
            "-'torchvision==0.9.0.dev20210210+cpu '",
            "-'-f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html')"
        ],
        "pos_line": [
            "+os.system('pip install torch==1.8.0+cpu torchvision==0.9.0 '",
            "+'-f https://download.pytorch.org/whl/test/cpu/torch_test.html')"
        ],
        "core_change": "-# TODO replace with torch_test once torchvision binaries are released -# os.system('pip install torch torchvision ' -#           '-f https://download.pytorch.org/whl/test/cpu/torch_test.html') -# This is the last nightly release of 1.8.0 before splitting to 1.9.0. -os.system('pip install --pre ' -'torch==1.8.0.dev20210210+cpu ' -'torchvision==0.9.0.dev20210210+cpu ' -'-f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html') +os.system('pip install torch==1.8.0+cpu torchvision==0.9.0 ' +'-f https://download.pytorch.org/whl/test/cpu/torch_test.html')",
        "core_API": "system"
    },
    {
        "commit_hash": "e143e8bb2083992529e43a15af3aeffcde20304e",
        "index": "02077085..72da8103 100644",
        "commit_message": "layer tests and model tests bug fixed\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "models",
        "change": [
            "class LayerNode_Test(CustomTestCase):",
            ")",
            "emb, nce = emb_net([inputs, labels])",
            "",
            "-            model = tl.models.Model(inputs=[inputs, labels], outputs=[emb, nce], name=\"word2vec_model\")",
            "+            model = tl.models.Model(inputs=[inputs, labels], outputs=[emb, nce])",
            "return model",
            "",
            "net = get_word2vec()"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=name))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=string, text=\"word2vec_model\"))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 7587,
        "neg_line": [
            "-model = tl.models.Model(inputs=[inputs, labels], outputs=[emb, nce], name=\"word2vec_model\")"
        ],
        "pos_line": [
            "+model = tl.models.Model(inputs=[inputs, labels], outputs=[emb, nce])"
        ],
        "core_change": "-model = tl.models.Model(inputs=[inputs, labels], outputs=[emb, nce], name=\"word2vec_model\") +model = tl.models.Model(inputs=[inputs, labels], outputs=[emb, nce])",
        "core_API": "Model"
    },
    {
        "commit_hash": "da2f1325014a158be36d62bdd20293ef97e574f4",
        "index": "80b4cebf..b0f0ed11 100644",
        "commit_message": "enable black in the precommit (#1777)\n\n* enable black in the precommit\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add some fixes\n\n* update libface detection url\n\n* added url from kornia checkpoint\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_get_hanning_kernel1d_5(device, dtype):",
            "",
            "def test_get_hanning_kernel2d_3x4(device, dtype):",
            "kernel = kornia.filters.get_hanning_kernel2d((3, 4), dtype=dtype, device=device)",
            "-    expected = torch.tensor([[0., 0.00, 0.00, 0.],",
            "-                             [0., 0.75, 0.75, 0.],",
            "-                             [0., 0.00, 0.00, 0.]], dtype=dtype, device=device)",
            "+    expected = torch.tensor(",
            "+        [[0.0, 0.00, 0.00, 0.0], [0.0, 0.75, 0.75, 0.0], [0.0, 0.00, 0.00, 0.0]], dtype=dtype, device=device",
            "+    )",
            "assert kernel.shape == (3, 4)",
            "assert_close(kernel, expected)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=list), node=('[', '['), position=0, insert_id=402594)",
            "Move(target_node=ASTNode(type=list), node=ASTNode(type=,, text=,), position=3)",
            "Insert(target_node=ASTNode(type=list), node=(',', ','), position=5, insert_id=402595)",
            "Move(target_node=ASTNode(type=list), node=ASTNode(type=], text=]), position=7)",
            "Move(target_node=ASTNode(type=list), node=ASTNode(type=[, text=[), position=0)",
            "Update(target_node=ASTNode(type=float, text=0.), value='0.0')",
            "Insert(target_node=ASTNode(type=list), node=('float', '0.00'), position=4, insert_id=402596)",
            "Update(target_node=ASTNode(type=float, text=0.), value='0.0')",
            "Insert(target_node=ASTNode(type=list), node=(']', ']'), position=9, insert_id=402597)",
            "Update(target_node=ASTNode(type=float, text=0.), value='0.0')",
            "Insert(target_node=ASTNode(type=list), node=('float', '0.75'), position=6, insert_id=402598)",
            "Move(target_node=ASTNode(type=list), node=ASTNode(type=,, text=,), position=7)",
            "Update(target_node=ASTNode(type=float, text=0.), value='0.0')",
            "Update(target_node=ASTNode(type=float, text=0.), value='0.0')",
            "Move(target_node=ASTNode(type=list), node=ASTNode(type=,, text=,), position=2)",
            "Move(target_node=ASTNode(type=list), node=ASTNode(type=float, text=0.00), position=6)",
            "Update(target_node=ASTNode(type=float, text=0.), value='0.0')",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=float, text=0.75))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=float, text=0.00))",
            "Delete(target_node=ASTNode(type=], text=]))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 22,
        "number": 7588,
        "neg_line": [
            "-expected = torch.tensor([[0., 0.00, 0.00, 0.],",
            "-[0., 0.75, 0.75, 0.],",
            "-[0., 0.00, 0.00, 0.]], dtype=dtype, device=device)"
        ],
        "pos_line": [
            "+expected = torch.tensor(",
            "+[[0.0, 0.00, 0.00, 0.0], [0.0, 0.75, 0.75, 0.0], [0.0, 0.00, 0.00, 0.0]], dtype=dtype, device=device",
            "+)"
        ],
        "core_change": "-expected = torch.tensor([[0., 0.00, 0.00, 0.], -[0., 0.75, 0.75, 0.], -[0., 0.00, 0.00, 0.]], dtype=dtype, device=device) +expected = torch.tensor( +[[0.0, 0.00, 0.00, 0.0], [0.0, 0.75, 0.75, 0.0], [0.0, 0.00, 0.00, 0.0]], dtype=dtype, device=device +)",
        "core_API": "get_hanning_kernel2d"
    },
    {
        "commit_hash": "e8b41cfeec0b70b692e1a042e6f1e542f1fe446d",
        "index": "12e185c2..335c9c04 100644",
        "commit_message": "Fix `DBLP` download, move `get_edge_index` to `testing` package (#6901)\n\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def hetero_data():",
            "data['paper'].x = torch.randn(8, 16)",
            "data['author'].x = torch.randn(10, 8)",
            "",
            "-    data['paper', 'paper'].edge_index = get_edge_index(8, 8, num_edges=10)",
            "+    data['paper', 'paper'].edge_index = get_random_edge_index(8, 8, 10)",
            "data['paper', 'paper'].edge_attr = torch.randn(10, 16)",
            "-    data['paper', 'author'].edge_index = get_edge_index(8, 10, num_edges=10)",
            "+    data['paper', 'author'].edge_index = get_random_edge_index(8, 10, 10)",
            "data['paper', 'author'].edge_attr = torch.randn(10, 8)",
            "-    data['author', 'paper'].edge_index = get_edge_index(10, 8, num_edges=10)",
            "+    data['author', 'paper'].edge_index = get_random_edge_index(10, 8, 10)",
            "data['author', 'paper'].edge_attr = torch.randn(10, 8)",
            "",
            "return data"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=get_edge_index), value='get_random_edge_index')",
            "Update(target_node=ASTNode(type=identifier, text=get_edge_index), value='get_random_edge_index')",
            "Update(target_node=ASTNode(type=identifier, text=get_edge_index), value='get_random_edge_index')",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=integer, text=10), position=5)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=integer, text=10), position=5)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=integer, text=10), position=5)",
            "Delete(target_node=ASTNode(type=identifier, text=num_edges))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=identifier, text=num_edges))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=identifier, text=num_edges))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 15,
        "number": 7590,
        "neg_line": [
            "-data['paper', 'paper'].edge_index = get_edge_index(8, 8, num_edges=10)",
            "-data['paper', 'author'].edge_index = get_edge_index(8, 10, num_edges=10)",
            "-data['author', 'paper'].edge_index = get_edge_index(10, 8, num_edges=10)"
        ],
        "pos_line": [
            "+data['paper', 'paper'].edge_index = get_random_edge_index(8, 8, 10)",
            "+data['paper', 'author'].edge_index = get_random_edge_index(8, 10, 10)",
            "+data['author', 'paper'].edge_index = get_random_edge_index(10, 8, 10)"
        ],
        "core_change": "-data['paper', 'paper'].edge_index = get_edge_index(8, 8, num_edges=10) +data['paper', 'paper'].edge_index = get_random_edge_index(8, 8, 10) -data['paper', 'author'].edge_index = get_edge_index(8, 10, num_edges=10) +data['paper', 'author'].edge_index = get_random_edge_index(8, 10, 10) -data['author', 'paper'].edge_index = get_edge_index(10, 8, num_edges=10) +data['author', 'paper'].edge_index = get_random_edge_index(10, 8, 10)",
        "core_API": "randn"
    },
    {
        "commit_hash": "5abd2d51396134b1d3e5831adb8d25572f39c003",
        "index": "c1db528..bb85a63 100644",
        "commit_message": "fix nightly test (#1873)\n\n* fix nightly test\n\n* test coverage\n",
        "file": "autokeras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def deserialize_keras_object(",
            ")",
            ")",
            "else:",
            "-        return tf.keras.utils.deserialize_keras_object(",
            "+        return tf.keras.utils.deserialize_keras_object(  # pragma: no cover",
            "config, custom_objects, module_objects, printable_module_name",
            ")"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 7591,
        "neg_line": [
            "-return tf.keras.utils.deserialize_keras_object("
        ],
        "pos_line": [
            "+return tf.keras.utils.deserialize_keras_object(  # pragma: no cover"
        ],
        "core_change": "-return tf.keras.utils.deserialize_keras_object( +return tf.keras.utils.deserialize_keras_object(  # pragma: no cover",
        "core_API": "deserialize_keras_object"
    },
    {
        "commit_hash": "5d392216277a27319ad86f22996e793d43fc5e3c",
        "index": "eb5fda8a3..92300eae1 100644",
        "commit_message": "serde, torch: fix typo in numpy tensor serializer. (#3644)\n\n\n",
        "file": "PySyft.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def numpy_tensor_serializer(worker: AbstractWorker, tensor: torch.Tensor) -> bin",
            "\"Torch to Numpy serializer can only be used with tensors that do not require grad. \"",
            "\"Detaching tensor to continue\"",
            ")",
            "-        tensor = torch.detach()",
            "+        tensor = tensor.detach()",
            "",
            "np_tensor = tensor.numpy()",
            "outfile = io.BytesIO()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=torch), value='tensor')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 7593,
        "neg_line": [
            "-tensor = torch.detach()"
        ],
        "pos_line": [
            "+tensor = tensor.detach()"
        ],
        "core_change": "-tensor = torch.detach() +tensor = tensor.detach()",
        "core_API": "detach"
    },
    {
        "commit_hash": "43043ee4d5c672f7fbc22ac9559e8164731fd053",
        "index": "652c1e65ed..8992f8065b 100644",
        "commit_message": "[RLlib] Tf2x preparation; part 2 (upgrading `try_import_tf()`). (#9136)\n\n* WIP.\n\n* Fixes.\n\n* LINT.\n\n* WIP.\n\n* WIP.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* WIP.\n\n* Fixes.\n\n* Test\n\n* Fix.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* LINT.\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class ModelCatalog:",
            "",
            "dtype, shape = ModelCatalog.get_action_shape(action_space)",
            "",
            "-        return tf.placeholder(dtype, shape=shape, name=name)",
            "+        return tf1.placeholder(dtype, shape=shape, name=name)",
            "",
            "@staticmethod",
            "@DeveloperAPI"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tf), value='tf1')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 7594,
        "neg_line": [
            "-return tf.placeholder(dtype, shape=shape, name=name)"
        ],
        "pos_line": [
            "+return tf1.placeholder(dtype, shape=shape, name=name)"
        ],
        "core_change": "-return tf.placeholder(dtype, shape=shape, name=name) +return tf1.placeholder(dtype, shape=shape, name=name)",
        "core_API": "get_action_shape"
    },
    {
        "commit_hash": "7f5045ec3a2eff82a37d04bcafdfb4abf86a1f67",
        "index": "c107a6da..c665a447 100644",
        "commit_message": "Update distirbuted training APIs. (#847)\n\n* update distirbuted training APIs.\n\n* update prefetch\n\n* fix\n\n* format\n\n* format\n\n* fix format\n\n* format\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "if __name__ == '__main__':",
            "# validation_dataset = training_dataset.map(data_aug_valid, num_parallel_calls=multiprocessing.cpu_count())",
            "trainer = tl.distributed.Trainer(",
            "build_training_func=build_train, training_dataset=training_dataset, optimizer=tf.train.AdamOptimizer,",
            "-        optimizer_args={'learning_rate': 0.0001}, batch_size=128, num_epochs=50000, prefetch_buffer_size=4096",
            "+        optimizer_args={'learning_rate': 0.0001}, batch_size=128, prefetch_size=128",
            "# validation_dataset=validation_dataset, build_validation_func=build_validation",
            ")"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=num_epochs), value='prefetch_size')",
            "Update(target_node=ASTNode(type=integer, text=50000), value='128')",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=prefetch_buffer_size))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=integer, text=4096))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 7595,
        "neg_line": [
            "-optimizer_args={'learning_rate': 0.0001}, batch_size=128, num_epochs=50000, prefetch_buffer_size=4096"
        ],
        "pos_line": [
            "+optimizer_args={'learning_rate': 0.0001}, batch_size=128, prefetch_size=128"
        ],
        "core_change": "-optimizer_args={'learning_rate': 0.0001}, batch_size=128, num_epochs=50000, prefetch_buffer_size=4096 +optimizer_args={'learning_rate': 0.0001}, batch_size=128, prefetch_size=128",
        "core_API": "map"
    },
    {
        "commit_hash": "57cdffb45205f1eeeff0737e9f3bb8f57103a29f",
        "index": "3800f4be..cc49187a 100644",
        "commit_message": "Fix the annotation for a method's return type (#3376)\n\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class LstmTagger(Model):",
            "",
            "def forward(self,",
            "sentence: Dict[str, torch.Tensor],",
            "-                labels: torch.Tensor = None) -> torch.Tensor:",
            "+                labels: torch.Tensor = None) -> Dict[str, torch.Tensor]:",
            "mask = get_text_field_mask(sentence)",
            "embeddings = self.word_embeddings(sentence)",
            "encoder_out = self.encoder(embeddings, mask)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=type), node=('subscript', None), position=0, insert_id=23284)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'Dict'), position=0, insert_id=23285)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=23286)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'str'), position=2, insert_id=23287)",
            "Insert(target_node=IN(type=subscript), node=(',', ','), position=3, insert_id=23288)",
            "Move(target_node=IN(type=subscript), node=ASTNode(type=attribute), position=4)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=5, insert_id=23289)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 7597,
        "neg_line": [
            "-labels: torch.Tensor = None) -> torch.Tensor:"
        ],
        "pos_line": [
            "+labels: torch.Tensor = None) -> Dict[str, torch.Tensor]:"
        ],
        "core_change": "-labels: torch.Tensor = None) -> torch.Tensor: +labels: torch.Tensor = None) -> Dict[str, torch.Tensor]:",
        "core_API": "word_embeddings"
    },
    {
        "commit_hash": "d04d51d02f8cfa9044f403e1cf1ca43cbc63c284",
        "index": "d954a51..01c4160 100644",
        "commit_message": "Fix broken test due to summary writer\n",
        "file": "skflow.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TensorFlowEstimator(BaseEstimator):",
            "# Set up a single operator to merge all the summaries",
            "summary_op = tf.merge_all_summaries()",
            "# Set up summary writer to a tmp directory",
            "-        self._summary_writer = tf.train.SummaryWriter('/tmp/tensorflow_logs', graph_def=sess.graph_def)",
            "+        self._summary_writer = tf.train.SummaryWriter('/tmp/tensorflow_logs', graph_def=self._session.graph_def)",
            "",
            "def fit(self, X, y):",
            "\"\"\"Builds a neural network model given provided `model_fn` and training"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=2173810)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=2173811)",
            "Update(target_node=ASTNode(type=identifier, text=sess), value='self')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=sess), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', '_session'), position=2, insert_id=2173812)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 7601,
        "neg_line": [
            "-self._summary_writer = tf.train.SummaryWriter('/tmp/tensorflow_logs', graph_def=sess.graph_def)"
        ],
        "pos_line": [
            "+self._summary_writer = tf.train.SummaryWriter('/tmp/tensorflow_logs', graph_def=self._session.graph_def)"
        ],
        "core_change": "-self._summary_writer = tf.train.SummaryWriter('/tmp/tensorflow_logs', graph_def=sess.graph_def) +self._summary_writer = tf.train.SummaryWriter('/tmp/tensorflow_logs', graph_def=self._session.graph_def)",
        "core_API": "merge_all_summaries"
    },
    {
        "commit_hash": "1537ba3ad477e1be8889a880d0522766a3644c0a",
        "index": "f95389fa..7480a9da 100644",
        "commit_message": "tests: rename tests and fix mlflow integration (#3108)\n\n\n",
        "file": "BentoML.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "models",
        "change": [
            "def test_inputs(framework: str | None) -> list[tuple[ModuleType, FrameworkTestMo",
            ")",
            "except ModuleNotFoundError as e:",
            "logger.warning(",
            "-                f\"Failed to find test module for framework {framework_name} (tests.integration.frameworks.models.{framework_name})\"",
            "+                f\"Failed to find test module for framework {framework_name} (tests.integration.frameworks.models.{framework_name}): {e}\"",
            ")",
            "",
            "return ["
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=f\"Failed to find test module for framework {framework_name} (tests.integration.frameworks.models.{framework_name})\"), value='f\"Failed to find test module for framework {framework_name} (tests.integration.frameworks.models.{framework_name}): {e}\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 7602,
        "neg_line": [
            "-f\"Failed to find test module for framework {framework_name} (tests.integration.frameworks.models.{framework_name})\""
        ],
        "pos_line": [
            "+f\"Failed to find test module for framework {framework_name} (tests.integration.frameworks.models.{framework_name}): {e}\""
        ],
        "core_change": "-f\"Failed to find test module for framework {framework_name} (tests.integration.frameworks.models.{framework_name})\" +f\"Failed to find test module for framework {framework_name} (tests.integration.frameworks.models.{framework_name}): {e}\"",
        "core_API": "warning"
    },
    {
        "commit_hash": "d215aa258e77f43b6c36950bc9a484d5d849eb4f",
        "index": "93d98213..ed736c77 100644",
        "commit_message": "fix torch.nonzero usage in pytorch 1.5 (#2602)\n\n\n",
        "file": "mmdetection.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "from ..builder import LOSSES",
            "",
            "def _expand_onehot_labels(labels, label_weights, label_channels):",
            "bin_labels = labels.new_full((labels.size(0), label_channels), 0)",
            "-    inds = torch.nonzero((labels >= 0) & (labels < label_channels)).squeeze()",
            "+    inds = torch.nonzero(",
            "+        (labels >= 0) & (labels < label_channels), as_tuple=False).squeeze()",
            "if inds.numel() > 0:",
            "bin_labels[inds, labels[inds]] = 1",
            "bin_label_weights = label_weights.view(-1, 1).expand("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=638818)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=638819)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'as_tuple'), position=0, insert_id=638820)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=638821)",
            "Insert(target_node=IN(type=keyword_argument), node=('false', 'False'), position=2, insert_id=638822)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 7603,
        "neg_line": [
            "-inds = torch.nonzero((labels >= 0) & (labels < label_channels)).squeeze()"
        ],
        "pos_line": [
            "+inds = torch.nonzero(",
            "+(labels >= 0) & (labels < label_channels), as_tuple=False).squeeze()"
        ],
        "core_change": "-inds = torch.nonzero((labels >= 0) & (labels < label_channels)).squeeze() +inds = torch.nonzero( +(labels >= 0) & (labels < label_channels), as_tuple=False).squeeze()",
        "core_API": "new_full"
    },
    {
        "commit_hash": "6573a49ac057a5fde73a604637bd519d3f842205",
        "index": "2f81e104b..6f43f4eac 100644",
        "commit_message": "fix: tf_model save\n\n",
        "file": "DeepPavlov.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFModel(Trainable, Inferable, metaclass=TfModelMeta):",
            "print('\\n:: Model saved to {} \\n'.format(self.model_path_.as_posix()))",
            "",
            "def get_checkpoint_state(self):",
            "-        if self.model_path_.is_dir():",
            "-            return tf.train.get_checkpoint_state(self.model_path_)",
            "-        return tf.train.get_checkpoint_state(self.model_path_.parent)",
            "+        if self._model_file:",
            "+            return tf.train.get_checkpoint_state(self.model_path_.parent)",
            "+        return tf.train.get_checkpoint_state(self.model_path_)",
            "",
            "@check_path_exists('dir')",
            "@overrides"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=IN(type=root), node=ASTNode(type=block), position=0)",
            "Move(target_node=ASTNode(type=if_statement), node=ASTNode(type=attribute), position=1)",
            "Move(target_node=ASTNode(type=if_statement), node=ASTNode(type=block), position=4)",
            "Update(target_node=ASTNode(type=identifier, text=model_path_), value='_model_file')",
            "Move(target_node=ASTNode(type=block), node=ASTNode(type=return_statement), position=0)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=is_dir))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 12,
        "number": 7606,
        "neg_line": [
            "-if self.model_path_.is_dir():",
            "-return tf.train.get_checkpoint_state(self.model_path_)",
            "-return tf.train.get_checkpoint_state(self.model_path_.parent)"
        ],
        "pos_line": [
            "+if self._model_file:",
            "+return tf.train.get_checkpoint_state(self.model_path_.parent)",
            "+return tf.train.get_checkpoint_state(self.model_path_)"
        ],
        "core_change": "-if self.model_path_.is_dir(): -return tf.train.get_checkpoint_state(self.model_path_) -return tf.train.get_checkpoint_state(self.model_path_.parent) +if self._model_file: +return tf.train.get_checkpoint_state(self.model_path_.parent) +return tf.train.get_checkpoint_state(self.model_path_)",
        "core_API": "as_posix"
    },
    {
        "commit_hash": "1fc868d28a5cc404efbcbf7d85aee9eb120e3060",
        "index": "24985a7632..df830f5c17 100644",
        "commit_message": "small format fixes\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def random_normal(",
            "true_shape: List[int] = shape",
            "mean = mean.item() if isinstance(mean, torch.Tensor) else mean",
            "std = std.item() if isinstance(std, torch.Tensor) else std",
            "-    return torch.normal(",
            "-        mean, std, true_shape, device=default_device(device)",
            "-    )",
            "+    return torch.normal(mean, std, true_shape, device=default_device(device))",
            "",
            "",
            "def multinomial("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 17,
        "number": 7607,
        "neg_line": [
            "-return torch.normal(",
            "-mean, std, true_shape, device=default_device(device)",
            "-)"
        ],
        "pos_line": [
            "+return torch.normal(mean, std, true_shape, device=default_device(device))"
        ],
        "core_change": "-return torch.normal( -mean, std, true_shape, device=default_device(device) -) +return torch.normal(mean, std, true_shape, device=default_device(device))",
        "core_API": "item"
    },
    {
        "commit_hash": "7e81a79c62112d3040d649fc3608ebdfc99ad027",
        "index": "0e814caf43..bec37aaea4 100644",
        "commit_message": "Fixed lint failure\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "gelu.unsupported_dtypes = (\"float16\",)",
            "",
            "def sigmoid(x: torch.Tensor, /, *, out: Optional[torch.Tensor] = None) -> torch.Tensor:",
            "if not ivy.is_array(x):",
            "-        x=torch.tensor(x)",
            "+        x = torch.tensor(x)",
            "return torch.sigmoid(x, out=out)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 7613,
        "neg_line": [
            "-x=torch.tensor(x)"
        ],
        "pos_line": [
            "+x = torch.tensor(x)"
        ],
        "core_change": "-x=torch.tensor(x) +x = torch.tensor(x)",
        "core_API": "is_array"
    },
    {
        "commit_hash": "aabdc95342c2e389990ece9e4ff5963757338383",
        "index": "acf587a236..6be39c7833 100644",
        "commit_message": "fix format failures\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def one_hot(indices, depth: int, device: Optional[str] = None):",
            ")",
            "",
            "",
            "-def shape(",
            "-    x: torch.Tensor, as_tensor: bool = False",
            "-) -> Union[torch.Tensor, List[int]]:",
            "+def shape(x: torch.Tensor, as_tensor: bool = False) -> Union[torch.Tensor, List[int]]:",
            "if as_tensor:",
            "return torch.tensor(x.shape)",
            "else:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 7614,
        "neg_line": [
            "-def shape(",
            "-x: torch.Tensor, as_tensor: bool = False",
            "-) -> Union[torch.Tensor, List[int]]:"
        ],
        "pos_line": [
            "+def shape(x: torch.Tensor, as_tensor: bool = False) -> Union[torch.Tensor, List[int]]:"
        ],
        "core_change": "-def shape( -x: torch.Tensor, as_tensor: bool = False -) -> Union[torch.Tensor, List[int]]: +def shape(x: torch.Tensor, as_tensor: bool = False) -> Union[torch.Tensor, List[int]]:",
        "core_API": "tensor"
    },
    {
        "commit_hash": "b82fe7d258a621b953dcb3b78bd03da4fef70a44",
        "index": "beb18cba0..e0574efbd 100644",
        "commit_message": "Replace strided slice with tf.expand_dims (#10078)\n\n* Replace tf.newaxis -> tf.expand_dims\n\n* Fix tests\n\n* Fix tests\n\n* Use reshape when a tensors needs a double expand\n\n* Fix GPT2\n\n* Fix GPT2\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFSequenceSummary(tf.keras.layers.Layer):",
            ")  # A tensor full of shape [batch] or [batch, num choices] full of sequence length",
            "cls_shape = shape_list(cls_index)",
            "if len(cls_shape) <= len(hidden_shape) - 2:",
            "-                cls_index = cls_index[..., tf.newaxis]",
            "+                cls_index = tf.expand_dims(cls_index, axis=-1)",
            "# else:",
            "# cls_index = cls_index[..., tf.newaxis]",
            "# cls_index = cls_index.expand((-1,) * (cls_index.dim()-1) + (hidden_states.size(-1),))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('call', None), position=2, insert_id=2371638)",
            "Move(target_node=IN(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2371639)",
            "Update(target_node=ASTNode(type=identifier, text=newaxis), value='expand_dims')",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2371640)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'cls_index'), position=1, insert_id=2371641)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=,, text=,), position=2)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=3, insert_id=2371642)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=2371643)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'axis'), position=0, insert_id=2371644)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2371645)",
            "Insert(target_node=IN(type=keyword_argument), node=('unary_operator', '-1'), position=2, insert_id=2371646)",
            "Delete(target_node=ASTNode(type=identifier, text=cls_index))",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=ellipsis, text=...))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 7616,
        "neg_line": [
            "-cls_index = cls_index[..., tf.newaxis]"
        ],
        "pos_line": [
            "+cls_index = tf.expand_dims(cls_index, axis=-1)"
        ],
        "core_change": "-cls_index = cls_index[..., tf.newaxis] +cls_index = tf.expand_dims(cls_index, axis=-1)",
        "core_API": "expand_dims"
    },
    {
        "commit_hash": "2766dd1d6e84ec1fa45f5ca799e3b47676bd34b6",
        "index": "50bcc451..36ed668b 100644",
        "commit_message": "Fix #813 - GlowTTS training  (#814)\n\n* Fix #813\n\n* Update glow_tts recipe\n\n* Fix glow-tts test\n\n* Linter fix\n\n* Run data dep init only in training\n",
        "file": "TTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Encoder(nn.Module):",
            "# set duration predictor input",
            "if g is not None:",
            "g_exp = g.expand(-1, -1, x.size(-1))",
            "-            x_dp = torch.cat([torch.detach(x), g_exp], 1)",
            "+            x_dp = torch.cat([x.detach(), g_exp], 1)",
            "else:",
            "-            x_dp = torch.detach(x)",
            "+            x_dp = x.detach()",
            "# final projection layer",
            "x_m = self.proj_m(x) * x_mask",
            "if not self.mean_only:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=torch), value='x')",
            "Update(target_node=ASTNode(type=identifier, text=torch), value='x')",
            "Delete(target_node=ASTNode(type=identifier, text=x))",
            "Delete(target_node=ASTNode(type=identifier, text=x))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 4,
        "number": 7617,
        "neg_line": [
            "-x_dp = torch.cat([torch.detach(x), g_exp], 1)",
            "-x_dp = torch.detach(x)"
        ],
        "pos_line": [
            "+x_dp = torch.cat([x.detach(), g_exp], 1)",
            "+x_dp = x.detach()"
        ],
        "core_change": "-x_dp = torch.cat([torch.detach(x), g_exp], 1) +x_dp = torch.cat([x.detach(), g_exp], 1) -x_dp = torch.detach(x) +x_dp = x.detach()",
        "core_API": "expand"
    },
    {
        "commit_hash": "4251a86130e46e9d8a5e52ffc1c7f86bbd7f3f8a",
        "index": "86cc467b..20e28e11 100755",
        "commit_message": "Fixed problem in running standardize, added tf/fn_preprocess to model\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class NaturalGradient(Optimizer):",
            "deltas = self.solver.solve(fn_x=fisher_matrix_product, x_init=None, b=[-grad for grad in loss_gradients], f_args=(kldiv_gradients,))",
            "",
            "# delta' * F",
            "-        delta_fisher_matrix_product = fisher_matrix_product(x=deltas)",
            "+        delta_fisher_matrix_product = fisher_matrix_product(deltas=deltas, kldiv_grads=kldiv_gradients)",
            "",
            "# c' = 0.5 * delta' * F * delta'  (= lambda * c)",
            "# TODO: Why constant and hence KL-divergence sometimes negative?"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type==, text==), node=ASTNode(type=keyword_argument), position=2)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=3, insert_id=2236049)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=2236050)",
            "Insert(target_node=ASTNode(type=keyword_argument), node=('identifier', 'deltas'), position=2, insert_id=2236051)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'kldiv_grads'), position=0, insert_id=2236052)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2236053)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'kldiv_gradients'), position=2, insert_id=2236054)",
            "Delete(target_node=ASTNode(type=identifier, text=x))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 7618,
        "neg_line": [
            "-delta_fisher_matrix_product = fisher_matrix_product(x=deltas)"
        ],
        "pos_line": [
            "+delta_fisher_matrix_product = fisher_matrix_product(deltas=deltas, kldiv_grads=kldiv_gradients)"
        ],
        "core_change": "-delta_fisher_matrix_product = fisher_matrix_product(x=deltas) +delta_fisher_matrix_product = fisher_matrix_product(deltas=deltas, kldiv_grads=kldiv_gradients)",
        "core_API": "solve"
    },
    {
        "commit_hash": "859ec92da51ff56b49e3e1f1beefa75767edc175",
        "index": "b7d76f566..fa96550e7 100644",
        "commit_message": "Make Trainer.__test_using_best_weights use cloud_io's load to support more storage backends (#3694)\n\n* Split out changes from #3563 to make that PR easier to review. This formats the file according to the Black formatter\n\n* Store a reference to the trainer on the datamodule\n\nFixes #3682\n\n* Update data_connector.py\n\n* Update data_connector.py\n\n* Update test_datamodules.py\n\n* Support more storage backends in trainer.test using best weights\n\nSimilar to #3692\n\n* Update trainer.py\n\n* Update trainer.py\n\nuse cloud_io load directly\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Trainer(",
            ")",
            "return {}",
            "",
            "-            ckpt = torch.load(ckpt_path, map_location=lambda storage, loc: storage)",
            "+            ckpt = pl_load(ckpt_path, map_location=lambda storage, loc: storage)",
            "model.load_state_dict(ckpt['state_dict'])",
            "",
            "# attach dataloaders"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=torch), value='pl_load')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=torch), position=0)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=load))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 7619,
        "neg_line": [
            "-ckpt = torch.load(ckpt_path, map_location=lambda storage, loc: storage)"
        ],
        "pos_line": [
            "+ckpt = pl_load(ckpt_path, map_location=lambda storage, loc: storage)"
        ],
        "core_change": "-ckpt = torch.load(ckpt_path, map_location=lambda storage, loc: storage) +ckpt = pl_load(ckpt_path, map_location=lambda storage, loc: storage)",
        "core_API": "load"
    },
    {
        "commit_hash": "dd26aa143adcc8c68bc044d96860b4739ebd1f11",
        "index": "4ab50bca..1ece13cf 100644",
        "commit_message": "Fixed from_networkx to support empty edge lists\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def from_networkx(G):",
            "",
            "G = nx.convert_node_labels_to_integers(G)",
            "G = G.to_directed() if not nx.is_directed(G) else G",
            "-    edge_index = torch.tensor(list(G.edges)).t().contiguous()",
            "+    edge_index = torch.LongTensor(list(G.edges)).t().contiguous()",
            "",
            "data = {}"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tensor), value='LongTensor')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 7621,
        "neg_line": [
            "-edge_index = torch.tensor(list(G.edges)).t().contiguous()"
        ],
        "pos_line": [
            "+edge_index = torch.LongTensor(list(G.edges)).t().contiguous()"
        ],
        "core_change": "-edge_index = torch.tensor(list(G.edges)).t().contiguous() +edge_index = torch.LongTensor(list(G.edges)).t().contiguous()",
        "core_API": "convert_node_labels_to_integers"
    },
    {
        "commit_hash": "c74f40cbe86047da9723cf4d68f6baddec4a3f6f",
        "index": "57c22bda..53720511 100644",
        "commit_message": "Fix build test. (#1057)\n\n* 2.2.1 release\n\n* Fix build test.\n\n* apply yapf\n\n* ping yapf to 0.28.0\n\n* fix build\n\n* use yapf 0.29\n\n* fix yapf\n\n* include tests in make format.\n\n* ping autoflake and isort version.\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def _map_fn_train(img, target):",
            "",
            "def _map_fn_test(img, target):",
            "# 1. Crop the central [height, width] of the image.",
            "-    img = tf.image.resize_with_pad(img, 24, 24)",
            "+    img = tf.image.resize_with_pad(img, 24, 24)",
            "# 2. Subtract off the mean and divide by the variance of the pixels.",
            "img = tf.image.per_image_standardization(img)",
            "img = tf.reshape(img, (24, 24, 3))"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 7625,
        "neg_line": [
            "-img = tf.image.resize_with_pad(img, 24, 24)"
        ],
        "pos_line": [
            "+img = tf.image.resize_with_pad(img, 24, 24)"
        ],
        "core_change": "-img = tf.image.resize_with_pad(img, 24, 24) +img = tf.image.resize_with_pad(img, 24, 24)",
        "core_API": "resize_with_pad"
    },
    {
        "commit_hash": "4fbcf8ea496bece21b9442d71280257b9953152a",
        "index": "694984ac0..d013bc910 100644",
        "commit_message": "Fix TF input for np.ndarray (#9294)\n\n* Fix input for np.ndarray\"\n\n* add a test\n\n* add a test\n\n* Add a test\n\n* Apply style\n\n* Fix test\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def input_processing(func, config, input_ids, **kwargs):",
            "signature.pop(\"kwargs\", None)",
            "parameter_names = list(signature.keys())",
            "output = {}",
            "-    allowed_types = (tf.Tensor, bool, int, ModelOutput, tuple, list, dict)",
            "+    allowed_types = (tf.Tensor, bool, int, ModelOutput, tuple, list, dict, np.ndarray)",
            "",
            "if \"inputs\" in kwargs[\"kwargs_call\"]:",
            "warnings.warn("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=tuple), node=(',', ','), position=14, insert_id=2375083)",
            "Insert(target_node=ASTNode(type=tuple), node=('attribute', None), position=15, insert_id=2375084)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'np'), position=0, insert_id=2375085)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2375086)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'ndarray'), position=2, insert_id=2375087)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 7627,
        "neg_line": [
            "-allowed_types = (tf.Tensor, bool, int, ModelOutput, tuple, list, dict)"
        ],
        "pos_line": [
            "+allowed_types = (tf.Tensor, bool, int, ModelOutput, tuple, list, dict, np.ndarray)"
        ],
        "core_change": "-allowed_types = (tf.Tensor, bool, int, ModelOutput, tuple, list, dict) +allowed_types = (tf.Tensor, bool, int, ModelOutput, tuple, list, dict, np.ndarray)",
        "core_API": "pop"
    },
    {
        "commit_hash": "5d23f2416b03e8ad3deaa3b3ee9705177e26dc08",
        "index": "d4f5f81f..cebc8974 100755",
        "commit_message": "fix linting.\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Model(GANModelDesc):",
            "GaussianDistribution(\"uni_a\", 1),",
            "GaussianDistribution(\"uni_b\", 1)])",
            "# prior: the assumption how the factors are presented in the dataset",
            "-        prior = tf.constant([0.1] * 10 + [0,0], tf.float32, [12], name='prior')",
            "+        prior = tf.constant([0.1] * 10 + [0, 0], tf.float32, [12], name='prior')",
            "batch_prior = tf.tile(tf.expand_dims(prior, 0), [BATCH, 1], name='batch_prior')",
            "",
            "# sample the latent code zc:",
            "sample = self.factors.dists[0].sample(",
            "-            BATCH, tf.constant([0.1]*10, tf.float32, shape=[10]))",
            "+            BATCH, tf.constant([0.1] * 10, tf.float32, shape=[10]))",
            "z_cat = symbf.remove_shape(sample, 0, name='z_cat')",
            "# still sample the latent code from a uniform distribution.",
            "z_uni_a = symbf.remove_shape("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 7630,
        "neg_line": [
            "-prior = tf.constant([0.1] * 10 + [0,0], tf.float32, [12], name='prior')",
            "-BATCH, tf.constant([0.1]*10, tf.float32, shape=[10]))"
        ],
        "pos_line": [
            "+prior = tf.constant([0.1] * 10 + [0, 0], tf.float32, [12], name='prior')",
            "+BATCH, tf.constant([0.1] * 10, tf.float32, shape=[10]))"
        ],
        "core_change": "-prior = tf.constant([0.1] * 10 + [0,0], tf.float32, [12], name='prior') +prior = tf.constant([0.1] * 10 + [0, 0], tf.float32, [12], name='prior') -BATCH, tf.constant([0.1]*10, tf.float32, shape=[10])) +BATCH, tf.constant([0.1] * 10, tf.float32, shape=[10]))",
        "core_API": "constant"
    },
    {
        "commit_hash": "21f73d22a5ebfb3e4a1034c76051673c3914df8f",
        "index": "f256fbe06a..ca3af62dff 100644",
        "commit_message": "small fix for tensorflow iinfo.\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def closest_valid_dtype(type):",
            "",
            "",
            "def iinfo(type):",
            "-    return tf.experimental.numpy.iinfo(dtype_from_str(type))",
            "+    return tf.experimental.numpy.iinfo(dtype_to_str(type))",
            "",
            "",
            "class Finfo:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=dtype_from_str), value='dtype_to_str')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 7631,
        "neg_line": [
            "-return tf.experimental.numpy.iinfo(dtype_from_str(type))"
        ],
        "pos_line": [
            "+return tf.experimental.numpy.iinfo(dtype_to_str(type))"
        ],
        "core_change": "-return tf.experimental.numpy.iinfo(dtype_from_str(type)) +return tf.experimental.numpy.iinfo(dtype_to_str(type))",
        "core_API": "iinfo"
    },
    {
        "commit_hash": "4fd8977eafd658056c92ffa6294027e77cdc46c0",
        "index": "b415b8689a..abdef2fc8c 100644",
        "commit_message": "[RLlib] Minor cleanup in preparation to tf2.x support. (#9130)\n\n* WIP.\n\n* Fixes.\n\n* LINT.\n\n* Fixes.\n\n* Fixes and LINT.\n\n* WIP.\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def huber_loss(x, delta=1.0):",
            "\"\"\"Reference: https://en.wikipedia.org/wiki/Huber_loss\"\"\"",
            "return tf.where(",
            "tf.abs(x) < delta,",
            "-        tf.square(x) * 0.5, delta * (tf.abs(x) - 0.5 * delta))",
            "+        tf.math.square(x) * 0.5, delta * (tf.abs(x) - 0.5 * delta))",
            "",
            "",
            "def reduce_mean_ignore_inf(x, axis):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=2145942)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=2145943)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=tf), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'math'), position=2, insert_id=2145944)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 7632,
        "neg_line": [
            "-tf.square(x) * 0.5, delta * (tf.abs(x) - 0.5 * delta))"
        ],
        "pos_line": [
            "+tf.math.square(x) * 0.5, delta * (tf.abs(x) - 0.5 * delta))"
        ],
        "core_change": "-tf.square(x) * 0.5, delta * (tf.abs(x) - 0.5 * delta)) +tf.math.square(x) * 0.5, delta * (tf.abs(x) - 0.5 * delta))",
        "core_API": "where"
    },
    {
        "commit_hash": "c89bdfbe720bc8f41c7dc6db5473a2cb0955f224",
        "index": "843d59b98..c706a7986 100644",
        "commit_message": "Reorganize repo (#8580)\n\n* Put models in subfolders\n\n* Styling\n\n* Fix imports in tests\n\n* More fixes in test imports\n\n* Sneaky hidden imports\n\n* Fix imports in doc files\n\n* More sneaky imports\n\n* Finish fixing tests\n\n* Fix examples\n\n* Fix path for copies\n\n* More fixes for examples\n\n* Fix dummy files\n\n* More fixes for example\n\n* More model import fixes\n\n* Is this why you're unhappy GitHub?\n\n* Fix imports in conver command\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "models",
        "change": [
            "class HfArgumentParserTest(unittest.TestCase):",
            "logger = logging.logging.getLogger()",
            "with CaptureLogger(logger) as cl:",
            "# this action activates the env var",
            "-            logging.get_logger(\"transformers.tokenization_bart\")",
            "+            logging.get_logger(\"transformers.models.bart.tokenization_bart\")",
            "self.assertIn(\"Unknown option TRANSFORMERS_VERBOSITY=super-error\", cl.out)",
            "",
            "# no need to restore as nothing was changed"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"transformers.tokenization_bart\"), value='\"transformers.models.bart.tokenization_bart\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 7635,
        "neg_line": [
            "-logging.get_logger(\"transformers.tokenization_bart\")"
        ],
        "pos_line": [
            "+logging.get_logger(\"transformers.models.bart.tokenization_bart\")"
        ],
        "core_change": "-logging.get_logger(\"transformers.tokenization_bart\") +logging.get_logger(\"transformers.models.bart.tokenization_bart\")",
        "core_API": "getLogger"
    },
    {
        "commit_hash": "ba9b6419049e09fc5c7298c75d6264364575dcf6",
        "index": "ee137b7e2..33df97fbc 100644",
        "commit_message": "Sinc Convs - fixed scale tests and pytorch compability\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class BarkScale:",
            "def convert(f):",
            "\"\"\"Convert Hz to Bark.\"\"\"",
            "b = torch.div(f, 1000.0)",
            "-        b = torch.square(b) * 1.4",
            "+        b = torch.pow(b, 2.0) * 1.4",
            "b = torch.pow(b + 1.0, 0.69)",
            "return b * 75.0 + 25.0"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=square), value='pow')",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=143441)",
            "Insert(target_node=ASTNode(type=argument_list), node=('float', '2.0'), position=3, insert_id=143442)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 3,
        "number": 7639,
        "neg_line": [
            "-b = torch.square(b) * 1.4"
        ],
        "pos_line": [
            "+b = torch.pow(b, 2.0) * 1.4"
        ],
        "core_change": "-b = torch.square(b) * 1.4 +b = torch.pow(b, 2.0) * 1.4",
        "core_API": "div"
    },
    {
        "commit_hash": "645c7c386e62d2fb1d50f4621c1a52645a13869f",
        "index": "553aab6..7982545 100644",
        "commit_message": "codemod for 0.4 (#331) (#336)\n\n* codemod for 0.4\n\n* better IN key removal\n\n* fix ac\n\n* update to use some device\n\n* rnn flatten_param\n",
        "file": "examples.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def finish_episode():",
            "for r in policy.rewards[::-1]:",
            "R = r + args.gamma * R",
            "rewards.insert(0, R)",
            "-    rewards = torch.Tensor(rewards)",
            "-    rewards = (rewards - rewards.mean()) / (rewards.std() + np.finfo(np.float32).eps)",
            "+    rewards = torch.tensor(rewards)",
            "+    rewards = (rewards - rewards.mean()) / (rewards.std() + eps)",
            "for log_prob, reward in zip(policy.saved_log_probs, rewards):",
            "policy_loss.append(-log_prob * reward)",
            "optimizer.zero_grad()"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=Tensor), value='tensor')",
            "Move(target_node=ASTNode(type=binary_operator), node=ASTNode(type=identifier, text=eps), position=2)",
            "Delete(target_node=ASTNode(type=identifier, text=np))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=finfo))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=np))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=float32))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 16,
        "number": 7640,
        "neg_line": [
            "-rewards = torch.Tensor(rewards)",
            "-rewards = (rewards - rewards.mean()) / (rewards.std() + np.finfo(np.float32).eps)"
        ],
        "pos_line": [
            "+rewards = torch.tensor(rewards)",
            "+rewards = (rewards - rewards.mean()) / (rewards.std() + eps)"
        ],
        "core_change": "-rewards = torch.Tensor(rewards) -rewards = (rewards - rewards.mean()) / (rewards.std() + np.finfo(np.float32).eps) +rewards = torch.tensor(rewards) +rewards = (rewards - rewards.mean()) / (rewards.std() + eps)",
        "core_API": "insert"
    },
    {
        "commit_hash": "5a1338603a15848302fb1958ef3c9ef426935654",
        "index": "cab2eb7..38922ed 100644",
        "commit_message": "Fix attention scores shape definition\n\n",
        "file": "seq2seq.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class AttentionDecoder(RNNDecoder):",
            "logits=self.vocab_size,",
            "predicted_ids=tf.TensorShape([]),",
            "cell_output=self.cell.output_size,",
            "-        attention_scores=tf.concat([0, self.attention_values[1:-1]], 0),",
            "+        attention_scores=tf.concat(",
            "+            [[0], tf.shape(self.attention_values)[1:-1]], 0),",
            "attention_context=self.attention_values.get_shape()[-1])",
            "",
            "@property"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=list), node=('list', None), position=1, insert_id=2158896)",
            "Insert(target_node=IN(type=list), node=('[', '['), position=0, insert_id=2158897)",
            "Move(target_node=IN(type=list), node=ASTNode(type=integer, text=0), position=1)",
            "Insert(target_node=IN(type=list), node=(']', ']'), position=2, insert_id=2158898)",
            "Insert(target_node=ASTNode(type=subscript), node=('call', None), position=0, insert_id=2158899)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2158900)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2158901)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2158902)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2158903)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'shape'), position=2, insert_id=2158904)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2158905)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=attribute), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2158906)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 13,
        "number": 7643,
        "neg_line": [
            "-attention_scores=tf.concat([0, self.attention_values[1:-1]], 0),"
        ],
        "pos_line": [
            "+attention_scores=tf.concat(",
            "+[[0], tf.shape(self.attention_values)[1:-1]], 0),"
        ],
        "core_change": "-attention_scores=tf.concat([0, self.attention_values[1:-1]], 0), +attention_scores=tf.concat( +[[0], tf.shape(self.attention_values)[1:-1]], 0),",
        "core_API": "TensorShape"
    },
    {
        "commit_hash": "c7fd1d9fb18862318b133c9474d37c5085850070",
        "index": "2103314d..f6446948 100755",
        "commit_message": "fix deprecations about casting & initializers in tf1.13\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Model(ModelDesc):",
            "cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)",
            "cost = tf.reduce_mean(cost, name='cross_entropy_loss')",
            "",
            "-        correct = tf.to_float(tf.nn.in_top_k(logits, label, 1), name='correct')",
            "+        correct = tf.cast(tf.nn.in_top_k(logits, label, 1), tf.float32, name='correct')",
            "# monitor training error",
            "add_moving_summary(tf.reduce_mean(correct, name='accuracy'))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=to_float), value='cast')",
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=3, insert_id=2278959)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=2278960)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2278961)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2278962)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'float32'), position=2, insert_id=2278963)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 7647,
        "neg_line": [
            "-correct = tf.to_float(tf.nn.in_top_k(logits, label, 1), name='correct')"
        ],
        "pos_line": [
            "+correct = tf.cast(tf.nn.in_top_k(logits, label, 1), tf.float32, name='correct')"
        ],
        "core_change": "-correct = tf.to_float(tf.nn.in_top_k(logits, label, 1), name='correct') +correct = tf.cast(tf.nn.in_top_k(logits, label, 1), tf.float32, name='correct')",
        "core_API": "sparse_softmax_cross_entropy_with_logits"
    },
    {
        "commit_hash": "49102dabe6a080d80dd2487de25247d574897b4f",
        "index": "72388e4b6..3083ea120 100644",
        "commit_message": "set up precommit and fixed flake8 compliance issues\n\n",
        "file": "PySyft.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestTorchHook(TestCase):",
            "'torch.CharTensor': torch.CharTensor,",
            "'torch.ShortTensor': torch.ShortTensor,",
            "'torch.IntTensor': torch.IntTensor,",
            "-            'torch.LongTensor': torch.LongTensor",
            "+            'torch.LongTensor': torch.LongTensor,",
            "}",
            "",
            "for k, v in tensor_types.items():"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=expression_statement), node=(',', ','), position=1, insert_id=855650)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 7648,
        "neg_line": [
            "-'torch.LongTensor': torch.LongTensor"
        ],
        "pos_line": [
            "+'torch.LongTensor': torch.LongTensor,"
        ],
        "core_change": "-'torch.LongTensor': torch.LongTensor +'torch.LongTensor': torch.LongTensor,",
        "core_API": "items"
    },
    {
        "commit_hash": "6fc4378cc65c3896e802f037bd8d289412d6b2c1",
        "index": "efda3db6..1ac04194 100644",
        "commit_message": "[FasterRCNN] fix bug in gradient propagation\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def sample_fast_rcnn_targets(boxes, gt_boxes, gt_labels):",
            "ret_labels = tf.concat(",
            "[tf.gather(gt_labels, fg_inds_wrt_gt),",
            "tf.zeros_like(bg_inds, dtype=tf.int64)], axis=0, name='sampled_labels')",
            "-    return ret_boxes, tf.stop_gradient(ret_labels), fg_inds_wrt_gt",
            "+    return tf.stop_gradient(ret_boxes), tf.stop_gradient(ret_labels), fg_inds_wrt_gt",
            "",
            "",
            "@under_name_scope()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=expression_list), node=('call', None), position=0, insert_id=2292650)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2292651)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2292652)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2292653)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2292654)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'stop_gradient'), position=2, insert_id=2292655)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2292656)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=ret_boxes), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2292657)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 7650,
        "neg_line": [
            "-return ret_boxes, tf.stop_gradient(ret_labels), fg_inds_wrt_gt"
        ],
        "pos_line": [
            "+return tf.stop_gradient(ret_boxes), tf.stop_gradient(ret_labels), fg_inds_wrt_gt"
        ],
        "core_change": "-return ret_boxes, tf.stop_gradient(ret_labels), fg_inds_wrt_gt +return tf.stop_gradient(ret_boxes), tf.stop_gradient(ret_labels), fg_inds_wrt_gt",
        "core_API": "concat"
    },
    {
        "commit_hash": "c7fd1d9fb18862318b133c9474d37c5085850070",
        "index": "29627259..73867f55 100755",
        "commit_message": "fix deprecations about casting & initializers in tf1.13\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Model(ModelDesc):",
            "cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)",
            "cost = tf.reduce_mean(cost, name='cross_entropy_loss')",
            "",
            "-        wrong = tf.to_float(tf.logical_not(tf.nn.in_top_k(logits, label, 1)), name='wrong_vector')",
            "+        wrong = tf.cast(tf.logical_not(tf.nn.in_top_k(logits, label, 1)), tf.float32, name='wrong_vector')",
            "# monitor training error",
            "add_moving_summary(tf.reduce_mean(wrong, name='train_error'))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=to_float), value='cast')",
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=3, insert_id=2278938)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=2278939)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2278940)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2278941)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'float32'), position=2, insert_id=2278942)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 7651,
        "neg_line": [
            "-wrong = tf.to_float(tf.logical_not(tf.nn.in_top_k(logits, label, 1)), name='wrong_vector')"
        ],
        "pos_line": [
            "+wrong = tf.cast(tf.logical_not(tf.nn.in_top_k(logits, label, 1)), tf.float32, name='wrong_vector')"
        ],
        "core_change": "-wrong = tf.to_float(tf.logical_not(tf.nn.in_top_k(logits, label, 1)), name='wrong_vector') +wrong = tf.cast(tf.logical_not(tf.nn.in_top_k(logits, label, 1)), tf.float32, name='wrong_vector')",
        "core_API": "sparse_softmax_cross_entropy_with_logits"
    },
    {
        "commit_hash": "2bd4c84340a3c35b011026657a9ed30e380d0d75",
        "index": "5c060cd9..d2eef23b 100644",
        "commit_message": "Rewrite multigpu testing with mmddp (#622)\n\n* rewrite multigpu testing with mmddp\n\n* fix an indent\n\n* update slurm testing script\n\n* update readme and scripts\n\n",
        "file": "mmdetection.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DistributedGroupSampler(Sampler):",
            "",
            "indices = [",
            "indices[j] for i in list(",
            "-                torch.randperm(",
            "-                    len(indices) // self.samples_per_gpu, generator=g))",
            "+                torch.randperm(len(indices) // self.samples_per_gpu,",
            "+                               generator=g))",
            "for j in range(i * self.samples_per_gpu, (i + 1) *",
            "self.samples_per_gpu)",
            "]"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 7653,
        "neg_line": [
            "-torch.randperm(",
            "-len(indices) // self.samples_per_gpu, generator=g))"
        ],
        "pos_line": [
            "+torch.randperm(len(indices) // self.samples_per_gpu,",
            "+generator=g))"
        ],
        "core_change": "-torch.randperm( -len(indices) // self.samples_per_gpu, generator=g)) +torch.randperm(len(indices) // self.samples_per_gpu, +generator=g))",
        "core_API": "randperm"
    },
    {
        "commit_hash": "95b6d985ffb231d9a2047c039c95660913063b9d",
        "index": "a5c40bd6..e69b18cb 100644",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class BiMpmMatching(nn.Module, FromParams):",
            "len_1 = get_lengths_from_binary_sequence_mask(mask_1)",
            "len_2 = get_lengths_from_binary_sequence_mask(mask_2)",
            "",
            "-        # (batch, seq_len*)",
            "-        mask_1, mask_2 = mask_1.float(), mask_2.float()",
            "-",
            "# explicitly set masked weights to zero",
            "# (batch_size, seq_len*, hidden_dim)",
            "context_1 = context_1 * mask_1.unsqueeze(-1)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=mask_1))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=mask_2))",
            "Delete(target_node=ASTNode(type=pattern_list))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=mask_1))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=float))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=mask_2))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=float))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=expression_list))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 0,
        "minus_line": 2,
        "AST_diff_line": 25,
        "number": 7654,
        "neg_line": [
            "-# (batch, seq_len*)",
            "-mask_1, mask_2 = mask_1.float(), mask_2.float()",
            "-"
        ],
        "pos_line": [],
        "core_change": "-# (batch, seq_len*) -mask_1, mask_2 = mask_1.float(), mask_2.float() -",
        "core_API": "float"
    },
    {
        "commit_hash": "7db18ce0c28c0b7c93e9a61af55433f2d88d4213",
        "index": "19205474..ad60b298 100644",
        "commit_message": "fix mypy errors\n\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def convert_points_from_homogeneous(points: torch.Tensor,",
            "",
            "# we check for points at infinity",
            "z_vec: torch.Tensor = points[..., -1:]",
            "-    scale: torch.Tensor = 1. / torch.clamp(z_vec, eps)",
            "+    scale: torch.Tensor = torch.tensor(1.) / torch.clamp(z_vec, eps)",
            "",
            "return scale * points[..., :-1]"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=binary_operator), node=('call', None), position=0, insert_id=470389)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=470390)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=470391)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=470392)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=470393)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tensor'), position=2, insert_id=470394)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=470395)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=float, text=1.), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=470396)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 7658,
        "neg_line": [
            "-scale: torch.Tensor = 1. / torch.clamp(z_vec, eps)"
        ],
        "pos_line": [
            "+scale: torch.Tensor = torch.tensor(1.) / torch.clamp(z_vec, eps)"
        ],
        "core_change": "-scale: torch.Tensor = 1. / torch.clamp(z_vec, eps) +scale: torch.Tensor = torch.tensor(1.) / torch.clamp(z_vec, eps)",
        "core_API": "clamp"
    },
    {
        "commit_hash": "df9de56a9bc376452c7df0f28d837ec65964289f",
        "index": "6b79c5f72..60cb68967 100644",
        "commit_message": "Fix stray objects from FSS `Worker/Plan` initialization (#3525)\n\n* Fix stray objects from FSS `Worker/Plan` initialization\n\n* Update test/torch/tensors/test_additive_shared.py\n\nCo-authored-by: Théo Ryffel <theo.leffyr@gmail.com>\n\n* Update test/torch/tensors/test_additive_shared.py\n\nCo-authored-by: Théo Ryffel <theo.leffyr@gmail.com>\n\n* Update test/torch/tensors/test_additive_shared.py\n\nCo-authored-by: Théo Ryffel <theo.leffyr@gmail.com>\n\n* Update test/torch/tensors/test_additive_shared.py\n\nCo-authored-by: Théo Ryffel <theo.leffyr@gmail.com>\n\nCo-authored-by: Théo Ryffel <theo.leffyr@gmail.com>\n",
        "file": "PySyft.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class BaseWorker(AbstractWorker):",
            "",
            "# storage object for crypto primitives",
            "self.crypto_store = PrimitiveStorage(owner=self)",
            "-        # declare the plans used for crypto computations",
            "-        sy.frameworks.torch.mpc.fss.initialize_crypto_plans(self)",
            "",
            "def register_obj(self, obj):",
            "self.object_store.register_obj(self, obj)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=sy))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=frameworks))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=mpc))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=fss))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=initialize_crypto_plans))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 0,
        "minus_line": 2,
        "AST_diff_line": 22,
        "number": 7665,
        "neg_line": [
            "-# declare the plans used for crypto computations",
            "-sy.frameworks.torch.mpc.fss.initialize_crypto_plans(self)"
        ],
        "pos_line": [],
        "core_change": "-# declare the plans used for crypto computations -sy.frameworks.torch.mpc.fss.initialize_crypto_plans(self)",
        "core_API": "initialize_crypto_plans"
    },
    {
        "commit_hash": "86ba09af5eea6fb605c39da57297b6d5d3b2c77d",
        "index": "ef6c9e9..fbd7a79 100644",
        "commit_message": "Disable tests in TF1 using tf2.enabled().\n\nAlso fix image conversion in mnist_export_v2 to prevent float overflow in dbg mode.\n\nPiperOrigin-RevId: 307367586\n\n",
        "file": "hub.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class ExportTest(tf.test.TestCase):",
            "",
            "if __name__ == \"__main__\":",
            "# This test is only supported in TF 2.0.",
            "-  if LooseVersion(tf.__version__) >= LooseVersion(\"2.0.0-beta0\"):",
            "+  if tf.executing_eagerly():",
            "logging.info(\"Using TF version: %s\", tf.__version__)",
            "tf.test.main()",
            "else:",
            "logging.warning(\"Skipping running tests for TF Version: %s\", tf.__version__)",
            "-"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=ASTNode(type=if_statement), node=ASTNode(type=call), position=1)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=argument_list), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=__version__), value='executing_eagerly')",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=(, text=(), position=0)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=), text=)), position=1)",
            "Delete(target_node=ASTNode(type=identifier, text=LooseVersion))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=>=, text=>=))",
            "Delete(target_node=ASTNode(type=identifier, text=LooseVersion))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=string, text=\"2.0.0-beta0\"))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=comparison_operator))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 15,
        "number": 7666,
        "neg_line": [
            "-if LooseVersion(tf.__version__) >= LooseVersion(\"2.0.0-beta0\"):",
            "-"
        ],
        "pos_line": [
            "+if tf.executing_eagerly():"
        ],
        "core_change": "-if LooseVersion(tf.__version__) >= LooseVersion(\"2.0.0-beta0\"): +if tf.executing_eagerly(): -",
        "core_API": "executing_eagerly"
    },
    {
        "commit_hash": "5f5fa7d8d0c3e74bf9c5460eb6ca3754dec708eb",
        "index": "3d66f1adb..b48c6cd3a 100644",
        "commit_message": "Fix Conv1D block for FastSpeech\n\nFrom the paper:\n\n> Different from the 2-layer dense network in Transformer, we use a\n> 2-layer 1D convolutional network with ReLU activation\n\n> dropout are added after the self-attention network and 1D\nconvolutional network respectively.\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class MultiLayeredConv1d(torch.nn.Module):",
            "",
            "\"\"\"",
            "x = torch.relu(self.w_1(x.transpose(-1, 1))).transpose(-1, 1)",
            "-        return self.w_2(self.dropout(x).transpose(-1, 1)).transpose(-1, 1)",
            "+        x = torch.relu(self.w_2(x.transpose(-1, 1))).transpose(-1, 1)",
            "+        return self.dropout(x)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=166476)",
            "Insert(target_node=ASTNode(type=module), node=('return_statement', None), position=4, insert_id=166477)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=166478)",
            "Insert(target_node=IN(type=return_statement), node=('return', 'return'), position=0, insert_id=166479)",
            "Move(target_node=IN(type=return_statement), node=ASTNode(type=call), position=1)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'x'), position=0, insert_id=166480)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=166481)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=call), position=2)",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=166482)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=166483)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=166484)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'relu'), position=2, insert_id=166485)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=166486)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=166487)",
            "Insert(target_node=IN(type=argument_list), node=('call', None), position=1, insert_id=166488)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=166489)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=166490)",
            "Move(target_node=IN(type=call), node=ASTNode(type=argument_list), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'x'), position=0, insert_id=166491)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=transpose), position=2)",
            "Delete(target_node=ASTNode(type=return, text=return))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=return_statement))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 25,
        "number": 7671,
        "neg_line": [
            "-return self.w_2(self.dropout(x).transpose(-1, 1)).transpose(-1, 1)"
        ],
        "pos_line": [
            "+x = torch.relu(self.w_2(x.transpose(-1, 1))).transpose(-1, 1)",
            "+return self.dropout(x)"
        ],
        "core_change": "-return self.w_2(self.dropout(x).transpose(-1, 1)).transpose(-1, 1) +x = torch.relu(self.w_2(x.transpose(-1, 1))).transpose(-1, 1) +return self.dropout(x)",
        "core_API": "relu"
    },
    {
        "commit_hash": "76e97a8ae6131760b30afd138490141a01d05ac8",
        "index": "1f12364..5800cd4 100755",
        "commit_message": "🔧 Fix #131 and Fix #132\n\n",
        "file": "TensorFlowTTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFTacotron2(tf.keras.Model):",
            "training=True,",
            ")",
            "",
            "-    @tf.function(experimental_relax_shapes=True)",
            "def call(",
            "self,",
            "input_ids,"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=,, text=,), node=ASTNode(type=ERROR), position=6)",
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=(, text=(), position=4)",
            "Insert(target_node=ASTNode(type=ERROR), node=(',', ','), position=6, insert_id=2215405)",
            "Delete(target_node=ASTNode(type=@, text=@))",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=function))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=identifier, text=experimental_relax_shapes))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=true, text=True))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=decorator))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 18,
        "number": 7672,
        "neg_line": [
            "-@tf.function(experimental_relax_shapes=True)"
        ],
        "pos_line": [],
        "core_change": "-@tf.function(experimental_relax_shapes=True)",
        "core_API": "function"
    },
    {
        "commit_hash": "cbe6abfcdc06645d01006907eda2660687d268a1",
        "index": "89aca80a6d..4617f9b60b 100644",
        "commit_message": "Import fixes, lint fixes and numpy style docstring fix.\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def diff(",
            "x = x if type(x) == torch.Tensor else torch.Tensor(x)",
            "prepend = (",
            "prepend",
            "-        if type(prepend) == torch.Tensor or prepend == None",
            "+        if type(prepend) == torch.Tensor or prepend is None",
            "else torch.Tensor(prepend)",
            ")",
            "append = (",
            "append",
            "-        if type(append) == torch.Tensor or append == None",
            "+        if type(append) == torch.Tensor or append is None",
            "else torch.Tensor(append)",
            ")",
            "return torch.diff(x, n=n, dim=axis, prepend=prepend, append=append)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=comparison_operator), node=('is', 'is'), position=1, insert_id=279144)",
            "Insert(target_node=ASTNode(type=comparison_operator), node=('is', 'is'), position=1, insert_id=279145)",
            "Delete(target_node=ASTNode(type===, text===))",
            "Delete(target_node=ASTNode(type===, text===))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 4,
        "number": 7673,
        "neg_line": [
            "-if type(prepend) == torch.Tensor or prepend == None",
            "-if type(append) == torch.Tensor or append == None"
        ],
        "pos_line": [
            "+if type(prepend) == torch.Tensor or prepend is None",
            "+if type(append) == torch.Tensor or append is None"
        ],
        "core_change": "-if type(prepend) == torch.Tensor or prepend == None +if type(prepend) == torch.Tensor or prepend is None -if type(append) == torch.Tensor or append == None +if type(append) == torch.Tensor or append is None",
        "core_API": "Tensor"
    },
    {
        "commit_hash": "4c5f830728c6449f3ddff2b18647e345b504ea5e",
        "index": "34370acc13..51c3903ea2 100644",
        "commit_message": "more numpy tests fixes\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def argmax(",
            "keepdims: bool = False,",
            "out: Optional[Union[tf.Tensor, tf.Variable]] = None,",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "-    ret = tf.constant(x).numpy().argmax(axis=axis, keepdims=keepdims)",
            "-    ret = tf.convert_to_tensor(ret, dtype=ret.dtype)",
            "-",
            "-    return ret",
            "+    ret = x.numpy().argmax(axis=axis, keepdims=keepdims)",
            "+    return tf.convert_to_tensor(ret, dtype=ret.dtype)",
            "",
            "",
            "def argmin("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=block), node=('return_statement', None), position=1, insert_id=1992467)",
            "Insert(target_node=IN(type=return_statement), node=('return', 'return'), position=0, insert_id=1992468)",
            "Move(target_node=IN(type=return_statement), node=ASTNode(type=call), position=1)",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=x), position=0)",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=constant))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=identifier, text=ret))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=return, text=return))",
            "Delete(target_node=ASTNode(type=identifier, text=ret))",
            "Delete(target_node=ASTNode(type=return_statement))"
        ],
        "plus_line": 2,
        "minus_line": 3,
        "AST_diff_line": 19,
        "number": 7676,
        "neg_line": [
            "-ret = tf.constant(x).numpy().argmax(axis=axis, keepdims=keepdims)",
            "-ret = tf.convert_to_tensor(ret, dtype=ret.dtype)",
            "-",
            "-return ret"
        ],
        "pos_line": [
            "+ret = x.numpy().argmax(axis=axis, keepdims=keepdims)",
            "+return tf.convert_to_tensor(ret, dtype=ret.dtype)"
        ],
        "core_change": "-ret = tf.constant(x).numpy().argmax(axis=axis, keepdims=keepdims) -ret = tf.convert_to_tensor(ret, dtype=ret.dtype) - -return ret +ret = x.numpy().argmax(axis=axis, keepdims=keepdims) +return tf.convert_to_tensor(ret, dtype=ret.dtype)",
        "core_API": "constant"
    },
    {
        "commit_hash": "e47f59865fe6deaeb08a8580dd581cf070ba6e74",
        "index": "7089f361..049f08fd 100644",
        "commit_message": "[Fix] Fixes jit tests in feature for pytorch 1.6 (#858)\n\n* fix test jit issues in feature module\n\n* add github actions job for fast test pytorch versions\n\n* fixed orientation module mypy issues\n\n* add torch.jit.annotation for mypy issues\n\n* remove import cast\n\n* preset self.angle_detector as nn.Module\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SOSNet(nn.Module):",
            "",
            "return",
            "",
            "-    def forward(self, input: torch.Tensor, eps: float = 1e-10) -> torch.Tensor:   # type: ignore",
            "+    def forward(self, input: torch.Tensor, eps: float = 1e-10) -> torch.Tensor:",
            "descr = self.desc_norm(self.layers(input) + eps)",
            "descr = descr.view(descr.size(0), -1)",
            "return descr"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 7684,
        "neg_line": [
            "-def forward(self, input: torch.Tensor, eps: float = 1e-10) -> torch.Tensor:   # type: ignore"
        ],
        "pos_line": [
            "+def forward(self, input: torch.Tensor, eps: float = 1e-10) -> torch.Tensor:"
        ],
        "core_change": "-def forward(self, input: torch.Tensor, eps: float = 1e-10) -> torch.Tensor:   # type: ignore +def forward(self, input: torch.Tensor, eps: float = 1e-10) -> torch.Tensor:",
        "core_API": "desc_norm"
    },
    {
        "commit_hash": "48de336898a04b864f7fb4a4b5969d880b375fe1",
        "index": "507354b0..4396b933 100644",
        "commit_message": "Token characters encoder (#40)\n\n* Added TokenCharactersEncoder (still needs tests)\n\n* Added tests, but it's not working with the LSTM\n\n* Got tests to pass with CNN\n\n* Better name\n\n* Fix lint errors\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestCnnEncoder(AllenNlpTestCase):",
            "",
            "def test_forward_does_correct_computation(self):",
            "encoder = CnnEncoder(embedding_dim=2, num_filters=1, ngram_filter_sizes=(1, 2))",
            "-        const_init = lambda tensor: torch.nn.init.constant(tensor, 1.)",
            "-        initializer = InitializerApplicator(default_initializer=const_init)",
            "+        constant_init = lambda tensor: torch.nn.init.constant(tensor, 1.)",
            "+        initializer = InitializerApplicator(default_initializer=constant_init)",
            "initializer(encoder)",
            "input_tensor = Variable(torch.FloatTensor([[[.7, .8], [.1, 1.5]]]))",
            "encoder_output = encoder(input_tensor)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=const_init), value='constant_init')",
            "Update(target_node=ASTNode(type=identifier, text=const_init), value='constant_init')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 7691,
        "neg_line": [
            "-const_init = lambda tensor: torch.nn.init.constant(tensor, 1.)",
            "-initializer = InitializerApplicator(default_initializer=const_init)"
        ],
        "pos_line": [
            "+constant_init = lambda tensor: torch.nn.init.constant(tensor, 1.)",
            "+initializer = InitializerApplicator(default_initializer=constant_init)"
        ],
        "core_change": "-const_init = lambda tensor: torch.nn.init.constant(tensor, 1.) -initializer = InitializerApplicator(default_initializer=const_init) +constant_init = lambda tensor: torch.nn.init.constant(tensor, 1.) +initializer = InitializerApplicator(default_initializer=constant_init)",
        "core_API": "constant"
    },
    {
        "commit_hash": "aff7fd17afa55f2862e51bf066300e1ff3ba0456",
        "index": "481d1e28..45df30d4 100644",
        "commit_message": "Update kernels.py (#1940)\n\n* Update kernels.py\n\nChanged x.pow(2.0) to x**2.0 to make it stylistically consistent with sigma**2\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def gaussian(window_size: int, sigma: float) -> torch.Tensor:",
            "x = torch.arange(window_size, device=device, dtype=dtype) - window_size // 2",
            "if window_size % 2 == 0:",
            "x = x + 0.5",
            "-    gauss = torch.exp(-x.pow(2.0) / (2 * sigma**2))",
            "+    gauss = torch.exp(-(x**2.0) / (2 * sigma**2))",
            "return gauss / gauss.sum()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=unary_operator), node=('parenthesized_expression', None), position=1, insert_id=398528)",
            "Move(target_node=IN(type=parenthesized_expression), node=ASTNode(type=(, text=(), position=0)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('binary_operator', None), position=1, insert_id=398529)",
            "Move(target_node=IN(type=parenthesized_expression), node=ASTNode(type=), text=)), position=2)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=identifier, text=x), position=0)",
            "Insert(target_node=IN(type=binary_operator), node=('**', '**'), position=1, insert_id=398530)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=float, text=2.0), position=2)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=pow))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 12,
        "number": 7695,
        "neg_line": [
            "-gauss = torch.exp(-x.pow(2.0) / (2 * sigma**2))"
        ],
        "pos_line": [
            "+gauss = torch.exp(-(x**2.0) / (2 * sigma**2))"
        ],
        "core_change": "-gauss = torch.exp(-x.pow(2.0) / (2 * sigma**2)) +gauss = torch.exp(-(x**2.0) / (2 * sigma**2))",
        "core_API": "arange"
    },
    {
        "commit_hash": "455c6390938a5c737fa63e78396cedae41e4e87e",
        "index": "abe6e844e..0bd89ae74 100644",
        "commit_message": "CDN urls (#4030)\n\n* [file_utils] use_cdn + documentation\n\n* Move to cdn. urls for weights\n\n* [urls] Hotfix for bert-base-japanese\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "models",
        "change": [
            "from .modeling_tf_utils import TFPreTrainedModel, get_initializer, shape_list",
            "logger = logging.getLogger(__name__)",
            "",
            "TF_ROBERTA_PRETRAINED_MODEL_ARCHIVE_MAP = {",
            "-    \"roberta-base\": \"https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-tf_model.h5\",",
            "-    \"roberta-large\": \"https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-tf_model.h5\",",
            "-    \"roberta-large-mnli\": \"https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-mnli-tf_model.h5\",",
            "-    \"distilroberta-base\": \"https://s3.amazonaws.com/models.huggingface.co/bert/distilroberta-base-tf_model.h5\",",
            "+    \"roberta-base\": \"https://cdn.huggingface.co/roberta-base-tf_model.h5\",",
            "+    \"roberta-large\": \"https://cdn.huggingface.co/roberta-large-tf_model.h5\",",
            "+    \"roberta-large-mnli\": \"https://cdn.huggingface.co/roberta-large-mnli-tf_model.h5\",",
            "+    \"distilroberta-base\": \"https://cdn.huggingface.co/distilroberta-base-tf_model.h5\",",
            "}"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=,, text=,), node=ASTNode(type=dictionary), position=7)",
            "Insert(target_node=ASTNode(type=dictionary), node=(',', ','), position=2, insert_id=2689793)",
            "Insert(target_node=ASTNode(type=pair), node=(':', ':'), position=1, insert_id=2689794)",
            "Update(target_node=ASTNode(type=string, text=\"https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-tf_model.h5\"), value='\"https://cdn.huggingface.co/roberta-base-tf_model.h5\"')",
            "Update(target_node=ASTNode(type=string, text=\"https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-tf_model.h5\"), value='\"https://cdn.huggingface.co/roberta-large-tf_model.h5\"')",
            "Update(target_node=ASTNode(type=string, text=\"https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-mnli-tf_model.h5\"), value='\"https://cdn.huggingface.co/roberta-large-mnli-tf_model.h5\"')",
            "Move(target_node=ASTNode(type=pair), node=ASTNode(type=:, text=:), position=1)",
            "Update(target_node=ASTNode(type=string, text=\"https://s3.amazonaws.com/models.huggingface.co/bert/distilroberta-base-tf_model.h5\"), value='\"https://cdn.huggingface.co/distilroberta-base-tf_model.h5\"')",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 4,
        "minus_line": 4,
        "AST_diff_line": 10,
        "number": 7697,
        "neg_line": [
            "-\"roberta-base\": \"https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-tf_model.h5\",",
            "-\"roberta-large\": \"https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-tf_model.h5\",",
            "-\"roberta-large-mnli\": \"https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-mnli-tf_model.h5\",",
            "-\"distilroberta-base\": \"https://s3.amazonaws.com/models.huggingface.co/bert/distilroberta-base-tf_model.h5\","
        ],
        "pos_line": [
            "+\"roberta-base\": \"https://cdn.huggingface.co/roberta-base-tf_model.h5\",",
            "+\"roberta-large\": \"https://cdn.huggingface.co/roberta-large-tf_model.h5\",",
            "+\"roberta-large-mnli\": \"https://cdn.huggingface.co/roberta-large-mnli-tf_model.h5\",",
            "+\"distilroberta-base\": \"https://cdn.huggingface.co/distilroberta-base-tf_model.h5\","
        ],
        "core_change": "-\"roberta-base\": \"https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-tf_model.h5\", -\"roberta-large\": \"https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-tf_model.h5\", -\"roberta-large-mnli\": \"https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-mnli-tf_model.h5\", -\"distilroberta-base\": \"https://s3.amazonaws.com/models.huggingface.co/bert/distilroberta-base-tf_model.h5\", +\"roberta-base\": \"https://cdn.huggingface.co/roberta-base-tf_model.h5\", +\"roberta-large\": \"https://cdn.huggingface.co/roberta-large-tf_model.h5\", +\"roberta-large-mnli\": \"https://cdn.huggingface.co/roberta-large-mnli-tf_model.h5\", +\"distilroberta-base\": \"https://cdn.huggingface.co/distilroberta-base-tf_model.h5\",",
        "core_API": "getLogger"
    },
    {
        "commit_hash": "5ea3d76dfb54585dc0ecee7ec1ad567c165c922c",
        "index": "2eef2df..5c0bd6b 100644",
        "commit_message": "Topological sort, zero_grads (#119)\n\n* Topological sort, zero_grads\n\n* Bug fix, add test\n\n* Add zero_grads\n\n* Put deepwalk function in backward\n\n* Move zero_grad to optim\n\n* Fix gradcheck hack\n\nCo-authored-by: holonomicjl <58403584+holonomicjl@users.noreply.github.com>\n",
        "file": "tinygrad.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "optim",
        "change": [
            "class TinyConvNet:",
            "def train(model, optim, steps, BS=128, gpu=False):",
            "losses, accuracies = [], []",
            "for i in (t := trange(steps, disable=os.getenv('CI') is not None)):",
            "+    optim.zero_grad()",
            "samp = np.random.randint(0, X_train.shape[0], size=(BS))",
            "",
            "x = Tensor(X_train[samp].reshape((-1, 28*28)).astype(np.float32), gpu=gpu)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('block', None), position=0, insert_id=1619823)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=1619824)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=1619825)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1619826)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1619827)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'optim'), position=0, insert_id=1619828)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1619829)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'zero_grad'), position=2, insert_id=1619830)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1619831)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1619832)",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 11,
        "number": 7698,
        "neg_line": [],
        "pos_line": [
            "+optim.zero_grad()"
        ],
        "core_change": "+optim.zero_grad()",
        "core_API": "getenv"
    },
    {
        "commit_hash": "448df3cde688af8d2f8fdcaf5ca5d0b036df5362",
        "index": "91270555..61b0839e 100644",
        "commit_message": "Remove redundant padding_value from LSTMEncoder (fixes #1807) (#1816)\n\nSummary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/1816\n\nDifferential Revision: D20365924\n\nPulled By: myleott\n\nfbshipit-source-id: e0349004d53f2d5e42eb7f3bf98db093f8c27fbf\n\n",
        "file": "fairseq.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class LSTMEncoder(FairseqEncoder):",
            "packed_outs, (final_hiddens, final_cells) = self.lstm(packed_x, (h0, c0))",
            "",
            "# unpack outputs and apply dropout",
            "-        x, _ = nn.utils.rnn.pad_packed_sequence(packed_outs, padding_value=self.padding_value)",
            "+        x, _ = nn.utils.rnn.pad_packed_sequence(packed_outs, padding_value=self.padding_idx)",
            "x = F.dropout(x, p=self.dropout_out, training=self.training)",
            "assert list(x.size()) == [seqlen, bsz, self.output_units]"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=padding_value), value='padding_idx')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 7700,
        "neg_line": [
            "-x, _ = nn.utils.rnn.pad_packed_sequence(packed_outs, padding_value=self.padding_value)"
        ],
        "pos_line": [
            "+x, _ = nn.utils.rnn.pad_packed_sequence(packed_outs, padding_value=self.padding_idx)"
        ],
        "core_change": "-x, _ = nn.utils.rnn.pad_packed_sequence(packed_outs, padding_value=self.padding_value) +x, _ = nn.utils.rnn.pad_packed_sequence(packed_outs, padding_value=self.padding_idx)",
        "core_API": "lstm"
    },
    {
        "commit_hash": "2b967af9bf9d0d839085717faeb02f8344188848",
        "index": "4ab1e578..4ee7c0a3 100644",
        "commit_message": "fixed uint8 warning (#2241)\n\n\n",
        "file": "mmdetection.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class CascadeRCNN(BaseDetector, RPNTestMixin):",
            "device=device,",
            "dtype=torch.uint8))",
            "pos_inds = torch.cat(pos_inds)",
            "-                    mask_feats = bbox_feats[pos_inds]",
            "+                    mask_feats = bbox_feats[pos_inds.type(torch.bool)]",
            "mask_head = self.mask_head[i]",
            "mask_pred = mask_head(mask_feats)",
            "mask_targets = mask_head.get_target(sampling_results, gt_masks,"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=subscript), node=('call', None), position=2, insert_id=640590)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=640591)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=640592)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=pos_inds), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=640593)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'type'), position=2, insert_id=640594)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=640595)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=640596)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=640597)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=640598)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=640599)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'bool'), position=2, insert_id=640600)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 12,
        "number": 7702,
        "neg_line": [
            "-mask_feats = bbox_feats[pos_inds]"
        ],
        "pos_line": [
            "+mask_feats = bbox_feats[pos_inds.type(torch.bool)]"
        ],
        "core_change": "-mask_feats = bbox_feats[pos_inds] +mask_feats = bbox_feats[pos_inds.type(torch.bool)]",
        "core_API": "cat"
    },
    {
        "commit_hash": "d28ca29e52f7635f24470d696c4b61ec26661fa0",
        "index": "0756075..df02588 100644",
        "commit_message": "Ref #118: Additional fixes to neural_translation_word example.\n\n",
        "file": "skflow.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "if os.path.exists(os.path.join(PATH, 'graph.pbtxt')):",
            "translator = skflow.TensorFlowEstimator.restore(PATH)",
            "else:",
            "translator = skflow.TensorFlowEstimator(model_fn=translate_model,",
            "-        n_classes=n_words,",
            "+        n_classes=n_fr_words,",
            "optimizer='Adam', learning_rate=0.01, batch_size=128,",
            "continue_training=True, steps=100)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=n_words), value='n_fr_words')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 7704,
        "neg_line": [
            "-n_classes=n_words,"
        ],
        "pos_line": [
            "+n_classes=n_fr_words,"
        ],
        "core_change": "-n_classes=n_words, +n_classes=n_fr_words,",
        "core_API": "exists"
    },
    {
        "commit_hash": "4fd8977eafd658056c92ffa6294027e77cdc46c0",
        "index": "c6249ff7d1..6654bd8293 100644",
        "commit_message": "[RLlib] Minor cleanup in preparation to tf2.x support. (#9130)\n\n* WIP.\n\n* Fixes.\n\n* LINT.\n\n* Fixes.\n\n* Fixes and LINT.\n\n* WIP.\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class ParameterNoise(Exploration):",
            "added_noises.append(",
            "tf.assign(",
            "noise,",
            "-                    tf.random_normal(",
            "+                    tf.random.normal(",
            "shape=noise.shape,",
            "stddev=self.stddev,",
            "dtype=tf.float32)))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=2145937)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=2145938)",
            "Insert(target_node=ASTNode(type=attribute), node=('identifier', 'normal'), position=2, insert_id=2145939)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=tf), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=random_normal), value='random')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=random_normal), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 7705,
        "neg_line": [
            "-tf.random_normal("
        ],
        "pos_line": [
            "+tf.random.normal("
        ],
        "core_change": "-tf.random_normal( +tf.random.normal(",
        "core_API": "append"
    },
    {
        "commit_hash": "2d784c140ea73c8c1b7f4b262616328d14afb134",
        "index": "c9f54c0513..8ef9b31735 100644",
        "commit_message": "remove excess examples & fix docstrings\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def vander(",
            "increasing: Optional[bool] = False,",
            "out: Optional[Union[tf.Tensor, tf.Variable]] = None,",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "-    return tf.experimental.numpy.vander(",
            "-        x, N=N, increasing=increasing",
            "-    )",
            "+    return tf.experimental.numpy.vander(x, N=N, increasing=increasing)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 17,
        "number": 7712,
        "neg_line": [
            "-return tf.experimental.numpy.vander(",
            "-x, N=N, increasing=increasing",
            "-)"
        ],
        "pos_line": [
            "+return tf.experimental.numpy.vander(x, N=N, increasing=increasing)"
        ],
        "core_change": "-return tf.experimental.numpy.vander( -x, N=N, increasing=increasing -) +return tf.experimental.numpy.vander(x, N=N, increasing=increasing)",
        "core_API": "vander"
    },
    {
        "commit_hash": "c402309470740e17973eeee22a7b90123f85b310",
        "index": "5caeadbe3..0deb9411d 100644",
        "commit_message": "fix CI error\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class LogMel(torch.nn.Module):",
            "return \", \".join(f\"{k}={v}\" for k, v in self.mel_options.items())",
            "",
            "def forward(",
            "-        self,",
            "-        feat: torch.Tensor,",
            "-        ilens: torch.Tensor = None,",
            "+        self, feat: torch.Tensor, ilens: torch.Tensor = None",
            ") -> Tuple[torch.Tensor, torch.Tensor]:",
            "# feat: (B, T, D1) x melmat: (D1, D2) -> mel_feat: (B, T, D2)",
            "mel_feat = torch.matmul(feat, self.melmat)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 1,
        "number": 7714,
        "neg_line": [
            "-self,",
            "-feat: torch.Tensor,",
            "-ilens: torch.Tensor = None,"
        ],
        "pos_line": [
            "+self, feat: torch.Tensor, ilens: torch.Tensor = None"
        ],
        "core_change": "-self, -feat: torch.Tensor, -ilens: torch.Tensor = None, +self, feat: torch.Tensor, ilens: torch.Tensor = None",
        "core_API": "items"
    },
    {
        "commit_hash": "7697c025ecdc4ecc6bbfa2194e53b4e7f96b7697",
        "index": "ecad2436..7ef82af9 100755",
        "commit_message": "progress on distributed mode, optimizer None allowed, various fixes\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Categorical(Distribution):",
            "# \"Normalized\" logits",
            "logits = tf.log(x=probabilities)",
            "",
            "-        logits = tf.Print(logits, (logits, probabilities, state_value))",
            "-",
            "return logits, probabilities, state_value",
            "",
            "def state_value(self, distr_params):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=logits))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=Print))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=logits))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=logits))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=probabilities))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=state_value))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=tuple))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 22,
        "number": 7717,
        "neg_line": [
            "-logits = tf.Print(logits, (logits, probabilities, state_value))",
            "-"
        ],
        "pos_line": [],
        "core_change": "-logits = tf.Print(logits, (logits, probabilities, state_value)) -",
        "core_API": "log"
    },
    {
        "commit_hash": "a93739431299871f29d8014a343be9ad10162377",
        "index": "0b2065ca0..89cb56d43 100644",
        "commit_message": "[Metrics] Unification of regression (#4166)\n\n* moved to utility\n\n* add files\n\n* unify\n\n* add desc\n\n* update\n\n* end of line\n\n* Apply suggestions from code review\n\nCo-authored-by: Justus Schock <12886177+justusschock@users.noreply.github.com>\n\n* Apply suggestions from code review\n\nCo-authored-by: Justus Schock <12886177+justusschock@users.noreply.github.com>\n\n* add back functional test in new interface\n\n* pep8\n\n* doctest fix\n\n* test name fix\n\n* unify psnr + add class psnr, TODO: psnr test refactor ala mean squared error\n\n* unify psnr\n\n* rm unused code\n\n* pep8\n\n* docs\n\n* unify ssim\n\n* lower tolerance for ssim\n\n* fix import\n\n* pep8\n\n* docs\n\n* flake8\n\n* test smaller images\n\n* trying to fix test\n\n* no ddp test for ssim\n\n* pep8\n\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\nCo-authored-by: Justus Schock <12886177+justusschock@users.noreply.github.com>\nCo-authored-by: Teddy Koker <teddy.koker@gmail.com>\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class MeanAbsoluteError(Metric):",
            "preds: Predictions from model",
            "target: Ground truth values",
            "\"\"\"",
            "-        self._check_same_shape(preds, target)",
            "-        abs_error = torch.abs(preds - target)",
            "+        sum_abs_error, n_obs = _mean_absolute_error_update(preds, target)",
            "",
            "-        self.sum_abs_error += torch.sum(abs_error)",
            "-        self.total += target.numel()",
            "+        self.sum_abs_error += sum_abs_error",
            "+        self.total += n_obs",
            "",
            "def compute(self):",
            "\"\"\"",
            "Computes mean absolute error over state.",
            "\"\"\"",
            "-        return self.sum_abs_error / self.total",
            "+        return _mean_absolute_error_compute(self.sum_abs_error, self.total)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"\"\"\n        self._check_same_shape(preds, target)\n        abs_error = torch.abs(preds - target)\n\n        self.sum_abs_error += torch.sum(abs_error)\n        self.total += target.numel()\n\ndef compute(self):\n\"\"\"), value='\"\"\"\\n        sum_abs_error, n_obs = _mean_absolute_error_update(preds, target)\\n\\n        self.sum_abs_error += sum_abs_error\\n        self.total += n_obs\\n\\ndef compute(self):\\n\"\"\"')",
            "Insert(target_node=ASTNode(type=return_statement), node=('call', None), position=1, insert_id=559413)",
            "Insert(target_node=IN(type=call), node=('identifier', '_mean_absolute_error_compute'), position=0, insert_id=559414)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=559415)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=559416)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=attribute), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=559417)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=attribute), position=3)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=559418)",
            "Delete(target_node=ASTNode(type=/, text=/))",
            "Delete(target_node=ASTNode(type=binary_operator))"
        ],
        "plus_line": 4,
        "minus_line": 5,
        "AST_diff_line": 11,
        "number": 7719,
        "neg_line": [
            "-self._check_same_shape(preds, target)",
            "-abs_error = torch.abs(preds - target)",
            "-self.sum_abs_error += torch.sum(abs_error)",
            "-self.total += target.numel()",
            "-return self.sum_abs_error / self.total"
        ],
        "pos_line": [
            "+sum_abs_error, n_obs = _mean_absolute_error_update(preds, target)",
            "+self.sum_abs_error += sum_abs_error",
            "+self.total += n_obs",
            "+return _mean_absolute_error_compute(self.sum_abs_error, self.total)"
        ],
        "core_change": "-self._check_same_shape(preds, target) -abs_error = torch.abs(preds - target) +sum_abs_error, n_obs = _mean_absolute_error_update(preds, target) -self.sum_abs_error += torch.sum(abs_error) -self.total += target.numel() +self.sum_abs_error += sum_abs_error +self.total += n_obs -return self.sum_abs_error / self.total +return _mean_absolute_error_compute(self.sum_abs_error, self.total)",
        "core_API": "_check_same_shape"
    },
    {
        "commit_hash": "334889b15ee80051b9fb97bf312fc52184c85f97",
        "index": "ea28175..4b96426 100644",
        "commit_message": "fix batchnorm for TF 0.12\n\n",
        "file": "tflearn.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def init_training_mode():",
            "# 'is_training' collection stores the training mode variable",
            "coll = tf.get_collection('is_training')",
            "if len(coll) == 0:",
            "-        tr_var = tf.get_variable(",
            "+        tr_var = variable(",
            "\"is_training\", dtype=tf.bool, shape=[],",
            "initializer=tf.constant_initializer(False),",
            "trainable=False)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tf), value='variable')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=tf), position=0)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=get_variable))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 7720,
        "neg_line": [
            "-tr_var = tf.get_variable("
        ],
        "pos_line": [
            "+tr_var = variable("
        ],
        "core_change": "-tr_var = tf.get_variable( +tr_var = variable(",
        "core_API": "get_collection"
    },
    {
        "commit_hash": "50a8cee5e571d62840e31aad183a2634d54945d8",
        "index": "f34b006e..c23a7a1f 100644",
        "commit_message": "[Fix]: Fix torch.randperm (#5014)\n\n* Update random_sampler.py\n\n* Update random_sampler.py\n",
        "file": "mmdetection.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class RandomSampler(BaseSampler):",
            "else:",
            "device = 'cpu'",
            "gallery = torch.tensor(gallery, dtype=torch.long, device=device)",
            "-        perm = torch.randperm(gallery.numel(), device=gallery.device)[:num]",
            "+        # This is a temporary fix. We can revert the following code",
            "+        # when PyTorch fixes the abnormal return of torch.randperm.",
            "+        # See: https://github.com/open-mmlab/mmdetection/pull/5014",
            "+        perm = torch.randperm(gallery.numel())[:num].to(device=gallery.device)",
            "rand_inds = gallery[perm]",
            "if not is_tensor:",
            "rand_inds = rand_inds.cpu().numpy()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('call', None), position=2, insert_id=629043)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=629044)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=629045)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=subscript), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=629046)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to'), position=2, insert_id=629047)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=629048)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=keyword_argument), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=629049)",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 4,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 7723,
        "neg_line": [
            "-perm = torch.randperm(gallery.numel(), device=gallery.device)[:num]"
        ],
        "pos_line": [
            "+# This is a temporary fix. We can revert the following code",
            "+# when PyTorch fixes the abnormal return of torch.randperm.",
            "+# See: https://github.com/open-mmlab/mmdetection/pull/5014",
            "+perm = torch.randperm(gallery.numel())[:num].to(device=gallery.device)"
        ],
        "core_change": "-perm = torch.randperm(gallery.numel(), device=gallery.device)[:num] +# This is a temporary fix. We can revert the following code +# when PyTorch fixes the abnormal return of torch.randperm. +# See: https://github.com/open-mmlab/mmdetection/pull/5014 +perm = torch.randperm(gallery.numel())[:num].to(device=gallery.device)",
        "core_API": "tensor"
    },
    {
        "commit_hash": "e430d940d2e8d6f870485fd09eccaa4fe9781675",
        "index": "a7423f07..7a19bdb4 100644",
        "commit_message": "Fixing `to_heterogeneous` to work with GPU (#4910)\n\n* fixing to_heterogeneous to work with gpu data\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\nCo-authored-by: Your Name <devnull@nvidia.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Data(BaseData, FeatureStore, GraphStore):",
            "node_ids, index_map = {}, torch.empty_like(node_type)",
            "for i, key in enumerate(node_type_names):",
            "node_ids[i] = (node_type == i).nonzero(as_tuple=False).view(-1)",
            "-            index_map[node_ids[i]] = torch.arange(len(node_ids[i]))",
            "+            index_map[node_ids[i]] = torch.arange(len(node_ids[i]),",
            "+                                                  device=index_map.device)",
            "",
            "# We iterate over edge types to find the local edge indices:",
            "edge_ids = {}"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=974983)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=974984)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=974985)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=974986)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=974987)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'index_map'), position=0, insert_id=974988)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=974989)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=974990)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 7724,
        "neg_line": [
            "-index_map[node_ids[i]] = torch.arange(len(node_ids[i]))"
        ],
        "pos_line": [
            "+index_map[node_ids[i]] = torch.arange(len(node_ids[i]),",
            "+device=index_map.device)"
        ],
        "core_change": "-index_map[node_ids[i]] = torch.arange(len(node_ids[i])) +index_map[node_ids[i]] = torch.arange(len(node_ids[i]), +device=index_map.device)",
        "core_API": "empty_like"
    },
    {
        "commit_hash": "7f5045ec3a2eff82a37d04bcafdfb4abf86a1f67",
        "index": "cd23795e..c3c5ac39 100755",
        "commit_message": "Update distirbuted training APIs. (#847)\n\n* update distirbuted training APIs.\n\n* update prefetch\n\n* fix\n\n* format\n\n* format\n\n* fix format\n\n* format\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "if __name__ == '__main__':",
            "# validation_dataset = make_dataset(X_val, y_val)",
            "trainer = tl.distributed.Trainer(",
            "build_training_func=build_train, training_dataset=training_dataset, optimizer=tf.train.AdamOptimizer,",
            "-        optimizer_args={'learning_rate': 0.001}, batch_size=500, num_epochs=500, prefetch_buffer_size=4096",
            "+        optimizer_args={'learning_rate': 0.001}, batch_size=500, prefetch_size=500",
            "# validation_dataset=validation_dataset, build_validation_func=build_validation",
            ")"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=num_epochs), value='prefetch_size')",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=prefetch_buffer_size))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=integer, text=4096))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 7738,
        "neg_line": [
            "-optimizer_args={'learning_rate': 0.001}, batch_size=500, num_epochs=500, prefetch_buffer_size=4096"
        ],
        "pos_line": [
            "+optimizer_args={'learning_rate': 0.001}, batch_size=500, prefetch_size=500"
        ],
        "core_change": "-optimizer_args={'learning_rate': 0.001}, batch_size=500, num_epochs=500, prefetch_buffer_size=4096 +optimizer_args={'learning_rate': 0.001}, batch_size=500, prefetch_size=500",
        "core_API": "Trainer"
    },
    {
        "commit_hash": "c34f93f7e43155ffedcdbd1ee7144eae1a415ec6",
        "index": "80c0a1085..65439e4cc 100644",
        "commit_message": "fix\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Stft(torch.nn.Module, InversibleInterface):",
            "pad = self.n_fft // 2",
            "ilens = ilens + 2 * pad",
            "",
            "-            olens = torch.div((ilens - self.n_fft), self.hop_length) + 1",
            "+            olens = (",
            "+                torch.div((ilens - self.n_fft), self.hop_length, rounding_mode=\"floor\")",
            "+                + 1",
            "+            )",
            "output.masked_fill_(make_pad_mask(olens, output, 1), 0.0)",
            "else:",
            "olens = None"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('parenthesized_expression', None), position=2, insert_id=119833)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('(', '('), position=0, insert_id=119834)",
            "Move(target_node=IN(type=parenthesized_expression), node=ASTNode(type=binary_operator), position=1)",
            "Insert(target_node=IN(type=parenthesized_expression), node=(')', ')'), position=2, insert_id=119835)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=119836)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=119837)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'rounding_mode'), position=0, insert_id=119838)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=119839)",
            "Insert(target_node=IN(type=keyword_argument), node=('string', '\"floor\"'), position=2, insert_id=119840)"
        ],
        "plus_line": 4,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 7739,
        "neg_line": [
            "-olens = torch.div((ilens - self.n_fft), self.hop_length) + 1"
        ],
        "pos_line": [
            "+olens = (",
            "+torch.div((ilens - self.n_fft), self.hop_length, rounding_mode=\"floor\")",
            "++ 1",
            "+)"
        ],
        "core_change": "-olens = torch.div((ilens - self.n_fft), self.hop_length) + 1 +olens = ( +torch.div((ilens - self.n_fft), self.hop_length, rounding_mode=\"floor\") ++ 1 +)",
        "core_API": "div"
    },
    {
        "commit_hash": "bde5b36b5e8c53f1ca0ec3cdab74837a46ba3abb",
        "index": "c177c902..ecad2436 100755",
        "commit_message": "fix terminal reward order in runner\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Categorical(Distribution):",
            "# \"Normalized\" logits",
            "logits = tf.log(x=probabilities)",
            "",
            "-        # logits = tf.Print(logits, (logits, probabilities, state_value))",
            "+        logits = tf.Print(logits, (logits, probabilities, state_value))",
            "",
            "return logits, probabilities, state_value"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=2, insert_id=2242541)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=2242542)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'logits'), position=0, insert_id=2242543)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=2242544)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=2242545)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2242546)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2242547)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2242548)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2242549)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Print'), position=2, insert_id=2242550)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2242551)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'logits'), position=1, insert_id=2242552)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=2242553)",
            "Insert(target_node=IN(type=argument_list), node=('tuple', None), position=3, insert_id=2242554)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=2242555)",
            "Insert(target_node=IN(type=tuple), node=('(', '('), position=0, insert_id=2242556)",
            "Insert(target_node=IN(type=tuple), node=('identifier', 'logits'), position=1, insert_id=2242557)",
            "Insert(target_node=IN(type=tuple), node=(',', ','), position=2, insert_id=2242558)",
            "Insert(target_node=IN(type=tuple), node=('identifier', 'probabilities'), position=3, insert_id=2242559)",
            "Insert(target_node=IN(type=tuple), node=(',', ','), position=4, insert_id=2242560)",
            "Insert(target_node=IN(type=tuple), node=('identifier', 'state_value'), position=5, insert_id=2242561)",
            "Insert(target_node=IN(type=tuple), node=(')', ')'), position=6, insert_id=2242562)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 22,
        "number": 7743,
        "neg_line": [
            "-# logits = tf.Print(logits, (logits, probabilities, state_value))"
        ],
        "pos_line": [
            "+logits = tf.Print(logits, (logits, probabilities, state_value))"
        ],
        "core_change": "-# logits = tf.Print(logits, (logits, probabilities, state_value)) +logits = tf.Print(logits, (logits, probabilities, state_value))",
        "core_API": "log"
    },
    {
        "commit_hash": "02eeea355ef78db694d729194777744a00490340",
        "index": "ac58d593..b0f901d3 100644",
        "commit_message": "Add `combine` support to `MultiAggregation` (#5000)\n\n* Add combine supoort to MultiAggregation\n\n* changelog\n\n* fix\n\n* Add multi_aggr_kwargs in MessagePassing\n\n* Fix jit\n\n* update\n\n* update\n\n* fix\n\n* update\n\nCo-authored-by: Padarn Wilson <padarn.wilson@grabtaxi.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* update\n\n* fix\n\n* update\n\n* docs\n\n* Add attn combine and use dense reduce\n\n* fix\n\n* Fix docs and many typos\n\n* fix docs\n\n* Change kwwargs for MultiAggregation\n\n* update test\n\n* update\n\n* update\n\n* update\n\nCo-authored-by: Guohao Li <lighaime@gmail.com>\nCo-authored-by: Padarn Wilson <padarn.wilson@grabtaxi.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class MessagePassing(torch.nn.Module):",
            "edge_mask = self._edge_mask",
            "",
            "if edge_mask is None:",
            "-            raise ValueError(f\"Could not found a pre-defined 'edge_mask' as \"",
            "-                             f\"part of {self.__class__.__name__}\")",
            "+            raise ValueError(f\"Could not find a pre-defined 'edge_mask' as \"",
            "+                             f\"part of {self.__class__.__name__}.\")",
            "",
            "if self._apply_sigmoid:",
            "edge_mask = edge_mask.sigmoid()"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('concatenated_string', None), position=0, insert_id=1488039)",
            "Insert(target_node=IN(type=concatenated_string), node=('string', 'f\"Could not find a pre-defined \\'edge_mask\\' as \"'), position=0, insert_id=1488040)",
            "Insert(target_node=IN(type=concatenated_string), node=('string', 'f\"part of {self.__class__.__name__}.\"'), position=1, insert_id=1488041)",
            "Delete(target_node=ASTNode(type=string, text=f\"Could not found a pre-defined 'edge_mask' as \"))",
            "Delete(target_node=ASTNode(type=string, text=f\"part of {self.__class__.__name__}\"))",
            "Delete(target_node=ASTNode(type=concatenated_string))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 6,
        "number": 7744,
        "neg_line": [
            "-raise ValueError(f\"Could not found a pre-defined 'edge_mask' as \"",
            "-f\"part of {self.__class__.__name__}\")"
        ],
        "pos_line": [
            "+raise ValueError(f\"Could not find a pre-defined 'edge_mask' as \"",
            "+f\"part of {self.__class__.__name__}.\")"
        ],
        "core_change": "-raise ValueError(f\"Could not found a pre-defined 'edge_mask' as \" -f\"part of {self.__class__.__name__}\") +raise ValueError(f\"Could not find a pre-defined 'edge_mask' as \" +f\"part of {self.__class__.__name__}.\")",
        "core_API": "sigmoid"
    },
    {
        "commit_hash": "15e0b56e393bf04f245cd15efc3650560a02f591",
        "index": "858c416..713e55e 100644",
        "commit_message": "compile works (#688)\n\n* compile works\n\n* runtimes\n\n* line count\n\n* fix custom, to tg dtype\n\n* meh, that's fine with lazy import\n",
        "file": "tinygrad.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "torch_fxn_for_op: Dict[Op, Callable] = {**base_fxn_for_op, **{",
            "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else (\"mps\" if getenv(\"MPS\", 0) else \"cpu\"))",
            "class TorchBuffer(InterpretedBuffer):",
            "fxn_for_op: ClassVar = torch_fxn_for_op",
            "-  to_tinygrad_dtype = staticmethod(lambda lbuf: {torch.float16: dtypes.float16, torch.float32: dtypes.float32}[lbuf.dtype])",
            "+  def to_tinygrad_dtype(self): return {torch.float16: dtypes.float16, torch.float32: dtypes.float32}[self._buf.dtype]",
            "",
            "@staticmethod",
            "def fromCPU(x): return TorchBuffer(torch.from_numpy(x).requires_grad_(False).to(device))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=ERROR), node=('identifier', 'def'), position=5, insert_id=1158762)",
            "Insert(target_node=ASTNode(type=ERROR), node=('call', None), position=6, insert_id=1158763)",
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=:, text=:), position=7)",
            "Insert(target_node=ASTNode(type=ERROR), node=('return', 'return'), position=8, insert_id=1158764)",
            "Move(target_node=IN(type=call), node=ASTNode(type=identifier, text=to_tinygrad_dtype), position=0)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1158765)",
            "Move(target_node=ASTNode(type=binary_operator), node=ASTNode(type=subscript), position=0)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=(, text=(), position=0)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'self'), position=1, insert_id=1158766)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1158767)",
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=1158768)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=1158769)",
            "Update(target_node=ASTNode(type=identifier, text=lbuf), value='self')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=lbuf), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', '_buf'), position=2, insert_id=1158770)",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=staticmethod))",
            "Delete(target_node=ASTNode(type=lambda, text=lambda))",
            "Delete(target_node=ASTNode(type=identifier, text=lbuf))",
            "Delete(target_node=ASTNode(type=lambda_parameters))",
            "Delete(target_node=ASTNode(type=lambda))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 25,
        "number": 7751,
        "neg_line": [
            "-to_tinygrad_dtype = staticmethod(lambda lbuf: {torch.float16: dtypes.float16, torch.float32: dtypes.float32}[lbuf.dtype])"
        ],
        "pos_line": [
            "+def to_tinygrad_dtype(self): return {torch.float16: dtypes.float16, torch.float32: dtypes.float32}[self._buf.dtype]"
        ],
        "core_change": "-to_tinygrad_dtype = staticmethod(lambda lbuf: {torch.float16: dtypes.float16, torch.float32: dtypes.float32}[lbuf.dtype]) +def to_tinygrad_dtype(self): return {torch.float16: dtypes.float16, torch.float32: dtypes.float32}[self._buf.dtype]",
        "core_API": "device"
    },
    {
        "commit_hash": "d0fbe2cb63933a634ae508fa19bba701caabcd76",
        "index": "c7699cc..f0f6b4c 100644",
        "commit_message": "fix typo pucuda.gl --> pycuda.gl (#1379)\n\nSummary:\nEvery time I try to run code, I get this warning:\n\n```\n  warnings.warn(\"Can't import pucuda.gl, not importing MeshRasterizerOpenGL.\")\n```\n\nOf course, `pucuda` is a typo of `pycuda`.\n\nThis PR fixes the typo\n\nPull Request resolved: https://github.com/facebookresearch/pytorch3d/pull/1379\n\nReviewed By: kjchalup\n\nDifferential Revision: D41295562\n\nPulled By: bottler\n\nfbshipit-source-id: 2bfa2a2dbe20a5347861d36fbff5094994c1253d\n\n",
        "file": "pytorch3d.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "cuda",
        "change": [
            "def _can_import_egl_and_pycuda():",
            "try:",
            "import pycuda.gl",
            "except (ImportError, ImportError, ModuleNotFoundError):",
            "-        warnings.warn(\"Can't import pucuda.gl, not importing MeshRasterizerOpenGL.\")",
            "+        warnings.warn(\"Can't import pycuda.gl, not importing MeshRasterizerOpenGL.\")",
            "return False",
            "",
            "return True"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"Can't import pucuda.gl, not importing MeshRasterizerOpenGL.\"), value='\"Can\\'t import pycuda.gl, not importing MeshRasterizerOpenGL.\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 7754,
        "neg_line": [
            "-warnings.warn(\"Can't import pucuda.gl, not importing MeshRasterizerOpenGL.\")"
        ],
        "pos_line": [
            "+warnings.warn(\"Can't import pycuda.gl, not importing MeshRasterizerOpenGL.\")"
        ],
        "core_change": "-warnings.warn(\"Can't import pucuda.gl, not importing MeshRasterizerOpenGL.\") +warnings.warn(\"Can't import pycuda.gl, not importing MeshRasterizerOpenGL.\")",
        "core_API": "warn"
    },
    {
        "commit_hash": "c328b155e93dc637efd484cc43f4c5f3353a167e",
        "index": "8125331..668a383 100644",
        "commit_message": "Random erasing crash fix and args pass through\n\n",
        "file": "pytorch-image-models.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class RandomErasingTorch:",
            "left = random.randint(0, img_w - w)",
            "if self.per_pixel:",
            "img[:, top:top + h, left:left + w] = torch.empty(",
            "-                            (chan, h, w), dtype=batch.dtype).cuda().normal_()",
            "+                            (chan, h, w), dtype=batch.dtype).normal_().cuda()",
            "else:",
            "img[:, top:top + h, left:left + w] = c",
            "break"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=normal_), value='cuda')",
            "Update(target_node=ASTNode(type=identifier, text=cuda), value='normal_')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 7757,
        "neg_line": [
            "-(chan, h, w), dtype=batch.dtype).cuda().normal_()"
        ],
        "pos_line": [
            "+(chan, h, w), dtype=batch.dtype).normal_().cuda()"
        ],
        "core_change": "-(chan, h, w), dtype=batch.dtype).cuda().normal_() +(chan, h, w), dtype=batch.dtype).normal_().cuda()",
        "core_API": "randint"
    },
    {
        "commit_hash": "4d998154a31f186fbf154a39372315b225ccb826",
        "index": "bfa19cf..23272cf 100644",
        "commit_message": "* lint fixes\n\n",
        "file": "skflow.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TensorFlowEstimator(BaseEstimator):",
            "self._summaries = self._graph.get_operation_by_name('MergeSummary/MergeSummary')",
            "",
            "# Restore session.",
            "-            self._session = tf.Session(self.tf_master, config=ConfigAddon(verbose=self.verbose).config)",
            "+            self._session = tf.Session(",
            "+                self.tf_master,",
            "+                config=ConfigAddon(verbose=self.verbose).config)",
            "checkpoint_path = tf.train.latest_checkpoint(path)",
            "if checkpoint_path is None:",
            "raise ValueError(\"Missing checkpoint files in the %s. Please \""
        ],
        "hunk_index": 2,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 7767,
        "neg_line": [
            "-self._session = tf.Session(self.tf_master, config=ConfigAddon(verbose=self.verbose).config)"
        ],
        "pos_line": [
            "+self._session = tf.Session(",
            "+self.tf_master,",
            "+config=ConfigAddon(verbose=self.verbose).config)"
        ],
        "core_change": "-self._session = tf.Session(self.tf_master, config=ConfigAddon(verbose=self.verbose).config) +self._session = tf.Session( +self.tf_master, +config=ConfigAddon(verbose=self.verbose).config)",
        "core_API": "get_operation_by_name"
    },
    {
        "commit_hash": "68ce4c7f85b5ab26c8f2f7b7faaba6b30636d7ce",
        "index": "b7736116..e6459447 100644",
        "commit_message": "fix crash when num procs > 1 (#2623)\n\nSummary:\nthis fixes a crash when using num_workers > 0 and num processes > 1 (e.g. by launching on slurm with tasks-per-node > 1 or using torch.distributed/torchrun with num gpus > 1). the root cause is some kind of bug in torch.split() that holds on to memory. when data loaders recycle, the program crashes when trying to release already released memory\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2623\n\nReviewed By: arbabu123\n\nDifferential Revision: D32342431\n\nPulled By: alexeib\n\nfbshipit-source-id: 278c7e0b09cfd770f375865d472bc52fb5835bd2\n\n",
        "file": "fairseq.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def all_reduce_dict(data: Mapping[str, Any], device, group) -> Dict[str, Any]:",
            "return data",
            "buf = torch.cat([t.view(-1) for t in data.values()]).to(device=device)",
            "all_reduce(buf, group=group)",
            "-        split_buf = torch.split(buf, [t.numel() for t in data.values()])",
            "+        split_buf = torch.split(buf.clone(), [t.numel() for t in data.values()])",
            "reduced_data = [t.view_as(orig) for t, orig in zip(split_buf, data.values())]",
            "return OrderedDict(zip(data.keys(), reduced_data))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=1, insert_id=205321)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=205322)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=205323)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=buf), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=205324)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'clone'), position=2, insert_id=205325)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=205326)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=205327)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 7774,
        "neg_line": [
            "-split_buf = torch.split(buf, [t.numel() for t in data.values()])"
        ],
        "pos_line": [
            "+split_buf = torch.split(buf.clone(), [t.numel() for t in data.values()])"
        ],
        "core_change": "-split_buf = torch.split(buf, [t.numel() for t in data.values()]) +split_buf = torch.split(buf.clone(), [t.numel() for t in data.values()])",
        "core_API": "cat"
    },
    {
        "commit_hash": "0b0942777f17b11cf1a78b57f0f26d2d1cfab40f",
        "index": "e9ed2aa3..f4c556a3 100644",
        "commit_message": "Torch audio refactor (#1976)\n\n* Switched soundfile read for torch audio\n\n* Removed unused import\n\n* Working on test_experiment_audio_inputs\n\n* Fixed shape issue by indexing into tensor before preprocessing to padded matrix\n\n* Switched type setting to torch.float32\n\n* Removed soundfile dependency, added html read flow\n\n* Working on test_experiment_audio_inputs\n\n* Fixed shape issue by indexing into tensor before preprocessing to padded matrix\n\n* Added retry for flakey loads, catch for failed reads, and default audio logic for missing audio data\n\n* Fixed read audio logic\n\n* Fixed read audio logic again\n\n* Shortened error messages because they were redundant\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "ludwig.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def generate_audio(feature):",
            "os.makedirs(destination_folder)",
            "",
            "audio_dest_path = os.path.join(destination_folder, audio_filename)",
            "-        soundfile.write(audio_dest_path, audio, sampling_rate)",
            "+        torchaudio.save(audio_dest_path, audio_tensor, sampling_rate)",
            "",
            "except OSError as e:",
            "raise OSError(\"Unable to create a folder for audio or save audio to disk.\" \"{}\".format(e))"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=soundfile), value='torchaudio')",
            "Update(target_node=ASTNode(type=identifier, text=write), value='save')",
            "Update(target_node=ASTNode(type=identifier, text=audio), value='audio_tensor')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 3,
        "number": 7776,
        "neg_line": [
            "-soundfile.write(audio_dest_path, audio, sampling_rate)"
        ],
        "pos_line": [
            "+torchaudio.save(audio_dest_path, audio_tensor, sampling_rate)"
        ],
        "core_change": "-soundfile.write(audio_dest_path, audio, sampling_rate) +torchaudio.save(audio_dest_path, audio_tensor, sampling_rate)",
        "core_API": "makedirs"
    },
    {
        "commit_hash": "e5a33d9ef0c72a8e6d81c4de06b51ab430008ed3",
        "index": "ca966c26..398f2bd1 100644",
        "commit_message": "fix metrics to ensure cpu vs gpu (#178)\n\n* fix metrics to ensure cpu vs gpu\n\n* factor out function\n\n* this time it should really be correct\n\n* switch copy_ to fill_\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class F1Measure(Metric):",
            "raise ConfigurationError(\"A gold label passed to F1Measure contains an id >= {}, \"",
            "\"the number of classes.\".format(num_classes))",
            "if mask is None:",
            "-            mask = torch.ones(gold_labels.size())",
            "+            mask = ones_like(gold_labels)",
            "mask = mask.float()",
            "gold_labels = gold_labels.float()",
            "positive_label_mask = gold_labels.eq(self._positive_label).float()"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=torch), value='ones_like')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=torch), position=0)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=gold_labels), position=1)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=), text=)), position=2)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=ones))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=size))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=), text=)))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 14,
        "number": 7777,
        "neg_line": [
            "-mask = torch.ones(gold_labels.size())"
        ],
        "pos_line": [
            "+mask = ones_like(gold_labels)"
        ],
        "core_change": "-mask = torch.ones(gold_labels.size()) +mask = ones_like(gold_labels)",
        "core_API": "ones"
    },
    {
        "commit_hash": "c4c532d25e012dbe6ab1ac14bca75e53e0acc621",
        "index": "85cf1d03..4b56b3ac 100644",
        "commit_message": "pylint -> flake8 (#3288)\n\n* pylint\n\n* update pylint\n\n* undo a lot of the raise / else\n\n* add bound on typed-ast\n\n* first mypy fixes\n\n* new flag\n\n* fix mypy errors\n\n* requirements.txt\n\n* pylint -> flake8\n\n* mypy 0.720 -> mypy 0.730\n\n* add back erroneously removed initial newline\n\n* remove .pylintrc\n\n* remove pylintrc from Dockerfile\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ScalarMix(torch.nn.Module):",
            "in range(mixture_size)])",
            "self.gamma = Parameter(torch.FloatTensor([1.0]), requires_grad=trainable)",
            "",
            "-    def forward(self, tensors: List[torch.Tensor],  # pylint: disable=arguments-differ",
            "+    def forward(self, tensors: List[torch.Tensor],",
            "mask: torch.Tensor = None) -> torch.Tensor:",
            "\"\"\"",
            "Compute a weighted average of the ``tensors``.  The input tensors an be any shape"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 7778,
        "neg_line": [
            "-def forward(self, tensors: List[torch.Tensor],  # pylint: disable=arguments-differ"
        ],
        "pos_line": [
            "+def forward(self, tensors: List[torch.Tensor],"
        ],
        "core_change": "-def forward(self, tensors: List[torch.Tensor],  # pylint: disable=arguments-differ +def forward(self, tensors: List[torch.Tensor],",
        "core_API": "FloatTensor"
    },
    {
        "commit_hash": "8d7eaeeab3a3d03bdb18b71296058129d0cba969",
        "index": "5384756f..daf8b64f 100644",
        "commit_message": "Fix default image on image read failure (#3073)\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "ludwig.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ImageInputFeature(ImageFeatureMixin, InputFeature):",
            ")",
            "",
            "def forward(self, inputs: torch.Tensor) -> torch.Tensor:",
            "-        assert isinstance(inputs, torch.Tensor)",
            "-        assert inputs.dtype in [torch.float32]",
            "+        assert isinstance(inputs, torch.Tensor), f\"inputs to image feature must be a torch tensor, got {type(inputs)}\"",
            "+        assert inputs.dtype in [torch.float32], f\"inputs to image feature must be a float32 tensor, got {inputs.dtype}\"",
            "",
            "inputs_encoded = self.encoder_obj(inputs)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assert_statement), node=(',', ','), position=2, insert_id=595904)",
            "Insert(target_node=ASTNode(type=assert_statement), node=('string', 'f\"inputs to image feature must be a torch tensor, got {type(inputs)}\"'), position=3, insert_id=595905)",
            "Insert(target_node=ASTNode(type=assert_statement), node=(',', ','), position=2, insert_id=595906)",
            "Insert(target_node=ASTNode(type=assert_statement), node=('string', 'f\"inputs to image feature must be a float32 tensor, got {inputs.dtype}\"'), position=3, insert_id=595907)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 4,
        "number": 7781,
        "neg_line": [
            "-assert isinstance(inputs, torch.Tensor)",
            "-assert inputs.dtype in [torch.float32]"
        ],
        "pos_line": [
            "+assert isinstance(inputs, torch.Tensor), f\"inputs to image feature must be a torch tensor, got {type(inputs)}\"",
            "+assert inputs.dtype in [torch.float32], f\"inputs to image feature must be a float32 tensor, got {inputs.dtype}\""
        ],
        "core_change": "-assert isinstance(inputs, torch.Tensor) -assert inputs.dtype in [torch.float32] +assert isinstance(inputs, torch.Tensor), f\"inputs to image feature must be a torch tensor, got {type(inputs)}\" +assert inputs.dtype in [torch.float32], f\"inputs to image feature must be a float32 tensor, got {inputs.dtype}\"",
        "core_API": "encoder_obj"
    },
    {
        "commit_hash": "5d41f6848129cab1cd52dd368f3575c33d94d531",
        "index": "a3a3999..ba525cb 100644",
        "commit_message": "Fix example USE semantic similarity calculation\n\nPiperOrigin-RevId: 323331362\n\n",
        "file": "hub.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "\"  sts_encode2 = tf.nn.l2_normalize(embed(tf.constant(batch['sent_2'].tolist())), axis=1)\\n\",",
            "\"  cosine_similarities = tf.reduce_sum(tf.multiply(sts_encode1, sts_encode2), axis=1)\\n\",",
            "\"  clip_cosine_similarities = tf.clip_by_value(cosine_similarities, -1.0, 1.0)\\n\",",
            "-        \"  scores = 1.0 - tf.acos(clip_cosine_similarities)\\n\",",
            "+        \"  scores = 1.0 - tf.acos(clip_cosine_similarities) / math.pi\\n\",",
            "\"  \\\"\\\"\\\"Returns the similarity scores\\\"\\\"\\\"\\n\",",
            "\"  return scores\\n\",",
            "\"\\n\","
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"  scores = 1.0 - tf.acos(clip_cosine_similarities)\\n\"), value='\"  scores = 1.0 - tf.acos(clip_cosine_similarities) / math.pi\\\\n\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 7784,
        "neg_line": [
            "-\"  scores = 1.0 - tf.acos(clip_cosine_similarities)\\n\","
        ],
        "pos_line": [
            "+\"  scores = 1.0 - tf.acos(clip_cosine_similarities) / math.pi\\n\","
        ],
        "core_change": "-\"  scores = 1.0 - tf.acos(clip_cosine_similarities)\\n\", +\"  scores = 1.0 - tf.acos(clip_cosine_similarities) / math.pi\\n\",",
        "core_API": "l2_normalize"
    },
    {
        "commit_hash": "3520c3a589cd8ad104c313547949951d8c5d17bd",
        "index": "247a85bc..8a021671 100644",
        "commit_message": "fix: to_undirected when num_nodes == 2 (#3627) (#3628)\n\n* fix: to_undirected when num_nodes == 2\n\n* fix\n\n* linting\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_hetero_to_undirected():",
            "data['v', 'w'].edge_weight = edge_weight",
            "data['v', 'w'].edge_attr = edge_attr",
            "",
            "+    from torch_geometric.transforms import ToUndirected",
            "data = ToUndirected()(data)",
            "-    assert data['v', 'v'].edge_index.tolist() == [[0, 0, 1, 2, 2, 3],",
            "-                                                  [1, 2, 0, 0, 3, 2]]",
            "+    assert data['v', 'v'].edge_index.tolist() == [[0, 1, 2, 3], [1, 0, 3, 2]]",
            "assert data['v', 'v'].edge_weight.tolist() == edge_weight[perm].tolist()",
            "assert data['v', 'v'].edge_attr.tolist() == edge_attr[perm].tolist()",
            "assert data['v', 'w'].edge_index.tolist() == edge_index.tolist()",
            "assert data['v', 'w'].edge_weight.tolist() == edge_weight.tolist()",
            "assert data['v', 'w'].edge_attr.tolist() == edge_attr.tolist()",
            "-    assert data['w', 'v'].edge_index.tolist() == [[3, 1, 0], [2, 0, 2]]",
            "+    assert data['w', 'v'].edge_index.tolist() == [[3, 1], [2, 0]]",
            "assert data['w', 'v'].edge_weight.tolist() == edge_weight.tolist()",
            "assert data['w', 'v'].edge_attr.tolist() == edge_attr.tolist()"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('import_from_statement', None), position=3, insert_id=992148)",
            "Insert(target_node=IN(type=import_from_statement), node=('from', 'from'), position=0, insert_id=992149)",
            "Insert(target_node=IN(type=import_from_statement), node=('dotted_name', None), position=1, insert_id=992150)",
            "Insert(target_node=IN(type=import_from_statement), node=('import', 'import'), position=2, insert_id=992151)",
            "Insert(target_node=IN(type=import_from_statement), node=('dotted_name', None), position=3, insert_id=992152)",
            "Insert(target_node=IN(type=dotted_name), node=('identifier', 'torch_geometric'), position=0, insert_id=992153)",
            "Insert(target_node=IN(type=dotted_name), node=('.', '.'), position=1, insert_id=992154)",
            "Insert(target_node=IN(type=dotted_name), node=('identifier', 'transforms'), position=2, insert_id=992155)",
            "Insert(target_node=IN(type=dotted_name), node=('identifier', 'ToUndirected'), position=0, insert_id=992156)",
            "Delete(target_node=ASTNode(type=integer, text=0))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=integer, text=2))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=integer, text=2))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=integer, text=0))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=integer, text=0))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=integer, text=2))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 21,
        "number": 7789,
        "neg_line": [
            "-assert data['v', 'v'].edge_index.tolist() == [[0, 0, 1, 2, 2, 3],",
            "-[1, 2, 0, 0, 3, 2]]",
            "-assert data['w', 'v'].edge_index.tolist() == [[3, 1, 0], [2, 0, 2]]"
        ],
        "pos_line": [
            "+from torch_geometric.transforms import ToUndirected",
            "+assert data['v', 'v'].edge_index.tolist() == [[0, 1, 2, 3], [1, 0, 3, 2]]",
            "+assert data['w', 'v'].edge_index.tolist() == [[3, 1], [2, 0]]"
        ],
        "core_change": "+from torch_geometric.transforms import ToUndirected -assert data['v', 'v'].edge_index.tolist() == [[0, 0, 1, 2, 2, 3], -[1, 2, 0, 0, 3, 2]] +assert data['v', 'v'].edge_index.tolist() == [[0, 1, 2, 3], [1, 0, 3, 2]] -assert data['w', 'v'].edge_index.tolist() == [[3, 1, 0], [2, 0, 2]] +assert data['w', 'v'].edge_index.tolist() == [[3, 1], [2, 0]]",
        "core_API": "tolist"
    },
    {
        "commit_hash": "76b266bfbf8a4a01dd42708f70df36accc66f97d",
        "index": "b426ae43..1b2a10f6 100644",
        "commit_message": " Upgrade to modern Python syntax (#1213)\n\n* Upgrade to modern Python syntax\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Use f-strings\n\n* Placate DeepSource Python\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def joint_pdf(kernel_values1: torch.Tensor, kernel_values2: torch.Tensor, epsilo",
            "\"\"\"",
            "",
            "if not isinstance(kernel_values1, torch.Tensor):",
            "-        raise TypeError(\"Input kernel_values1 type is not a torch.Tensor. Got {}\".format(type(kernel_values1)))",
            "+        raise TypeError(f\"Input kernel_values1 type is not a torch.Tensor. Got {type(kernel_values1)}\")",
            "",
            "if not isinstance(kernel_values2, torch.Tensor):",
            "-        raise TypeError(\"Input kernel_values2 type is not a torch.Tensor. Got {}\".format(type(kernel_values2)))",
            "+        raise TypeError(f\"Input kernel_values2 type is not a torch.Tensor. Got {type(kernel_values2)}\")",
            "",
            "if not kernel_values1.dim() == 3:",
            "raise ValueError(\"Input kernel_values1 must be a of the shape BxN.\" \" Got {}\".format(kernel_values1.shape))"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"\"\"\n\nif not isinstance(kernel_values1, torch.Tensor):\n        raise TypeError(\"Input kernel_values1 type is not a torch.Tensor. Got {}\".format(type(kernel_values1)))\n\nif not isinstance(kernel_values2, torch.Tensor):\n        raise TypeError(\"Input kernel_values2 type is not a torch.Tensor. Got {}\".format(type(kernel_values2)))\n\nif not kernel_values1.dim() == 3:\nraise ValueError(\"Input kernel_values1 must be a of the shape BxN.\" \" Got {}\"), value='\"\"\"\\n\\nif not isinstance(kernel_values1, torch.Tensor):\\n        raise TypeError(f\"Input kernel_values1 type is not a torch.Tensor. Got {type(kernel_values1)}\")\\n\\nif not isinstance(kernel_values2, torch.Tensor):\\n        raise TypeError(f\"Input kernel_values2 type is not a torch.Tensor. Got {type(kernel_values2)}\")\\n\\nif not kernel_values1.dim() == 3:\\nraise ValueError(\"Input kernel_values1 must be a of the shape BxN.\" \" Got {}\"')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 1,
        "number": 7791,
        "neg_line": [
            "-raise TypeError(\"Input kernel_values1 type is not a torch.Tensor. Got {}\".format(type(kernel_values1)))",
            "-raise TypeError(\"Input kernel_values2 type is not a torch.Tensor. Got {}\".format(type(kernel_values2)))"
        ],
        "pos_line": [
            "+raise TypeError(f\"Input kernel_values1 type is not a torch.Tensor. Got {type(kernel_values1)}\")",
            "+raise TypeError(f\"Input kernel_values2 type is not a torch.Tensor. Got {type(kernel_values2)}\")"
        ],
        "core_change": "-raise TypeError(\"Input kernel_values1 type is not a torch.Tensor. Got {}\".format(type(kernel_values1))) +raise TypeError(f\"Input kernel_values1 type is not a torch.Tensor. Got {type(kernel_values1)}\") -raise TypeError(\"Input kernel_values2 type is not a torch.Tensor. Got {}\".format(type(kernel_values2))) +raise TypeError(f\"Input kernel_values2 type is not a torch.Tensor. Got {type(kernel_values2)}\")",
        "core_API": "dim"
    },
    {
        "commit_hash": "1daab8714463e63c8f4b4ba4caed3cc1ef97a195",
        "index": "884f1c45..926a3297 100644",
        "commit_message": "[torch] Enable GPU training (#1508)\n\n* Added more descriptive description_summary, fixed bug in TabNet combiner\n\n* Updated modules to do proper GPU conversion\n\n* Functional GPU training\n\n* Tabnet working, removed RayRemoteTrainer\n\n* Temporary changes to unblock GPU training\n\n* Functional test_seq_encoder\n\n* Cleaned up PR\n\n* Fixed failing CPU tests\n\n* Responded to comments\n\n* Fix failing GPU tests in test_api.py\n\n* [torch] Fix GPU tests for test_visualizations (#1539)\n\nUpdated torch.aranges to registered buffers in embedding_modules.py\n\n* [torch] Fix test_encoder.py for GPU (#1544)\n\n* Accept and move model to GPU as part of Predictor init (#1559)\n\n* [torch] Fixed tests in test_experiment.py (#1562)\n\n* Fixed tests in tests/integration_tests/test_experiment.py\n\n* Moved set embed and inputs to correct device in tests/ludwig/encoders/test_set_encoders.py\n\n* Moved set date and inputs to correct device in tests/ludwig/encoders/test_date_encoders.py\n\n* Moved bag embed and inputs to correct device in tests/ludwig/encoders/test_bag_encoders.py\n\n* Moved sequence embed and inputs to correct device in tests/ludwig/features/test_sequence_features.py\n\n* [torch] Fixed GPU issues in combiners.py (#1568)\n\n* Fix GPU tests in test_collect.py (#1569)\n\n* Added model to Predictor initialization\n\n* Fixed embedding modules tests (#1554)\n\n* Fixed embedding modules tests\n* Moved embedding modules to device in tests, removed duplicate argument\n\nCo-authored-by: Shreya Rajpal <shreya.rajpal@gmail.com>\n\n* Fixed encoders tests except test_h3_rnn_embed (#1556)\n\n* Fixed encoders tests except test_h3_rnn_embed\n\n* Addressed Shreya's comments\n\n* Moved embedding modules to device, used registered buffers\n\nCo-authored-by: Shreya Rajpal <shreya.rajpal@gmail.com>\n\n* Pass ECD object to Predictor\n\n* Removed model arg from predict function\n\nCo-authored-by: Jeffrey Tang <jeff@predibase.com>\nCo-authored-by: Jeffrey Tang <810895+jeffreyftang@users.noreply.github.com>\nCo-authored-by: Animesh Kumar <anmshkmr@users.noreply.github.com>\n",
        "file": "ludwig.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class CategoryInputFeature(CategoryFeatureMixin, InputFeature):",
            "inputs = inputs.unsqueeze(dim=1)",
            "",
            "if inputs.dtype == torch.int8 or inputs.dtype == torch.int16:",
            "-            inputs = inputs.type(torch.IntTensor)",
            "+            inputs = inputs.type(torch.int)",
            "encoder_output = self.encoder_obj(inputs)",
            "",
            "return {\"encoder_output\": encoder_output}"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=IntTensor), value='int')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 7794,
        "neg_line": [
            "-inputs = inputs.type(torch.IntTensor)"
        ],
        "pos_line": [
            "+inputs = inputs.type(torch.int)"
        ],
        "core_change": "-inputs = inputs.type(torch.IntTensor) +inputs = inputs.type(torch.int)",
        "core_API": "unsqueeze"
    },
    {
        "commit_hash": "6d5ba6a970710eaaa14b89d24aace179eb8ee1af",
        "index": "0cf9c25e..3c299fb9 100644",
        "commit_message": "fix a missing tensor for visualization (fix #530)\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def generate_rpn_proposals(boxes, scores, img_shape):",
            "topk_valid_boxes,",
            "nms_indices, name='boxes')",
            "final_scores = tf.gather(topk_valid_scores, nms_indices, name='scores')",
            "+    tf.sigmoid(final_scores, name='probs')  # for visualization",
            "return final_boxes, final_scores"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=2, insert_id=2292445)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=2292446)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2292447)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2292448)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2292449)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2292450)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'sigmoid'), position=2, insert_id=2292451)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2292452)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'final_scores'), position=1, insert_id=2292453)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=2292454)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=3, insert_id=2292455)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=2292456)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'name'), position=0, insert_id=2292457)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2292458)",
            "Insert(target_node=IN(type=keyword_argument), node=('string', \"'probs'\"), position=2, insert_id=2292459)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 15,
        "number": 7797,
        "neg_line": [],
        "pos_line": [
            "+tf.sigmoid(final_scores, name='probs')  # for visualization"
        ],
        "core_change": "+tf.sigmoid(final_scores, name='probs')  # for visualization",
        "core_API": "gather"
    },
    {
        "commit_hash": "5eb95c2d8a5af8a16fba64b58684ed3e3a7d1ec8",
        "index": "f57798ec..7cf2eded 100644",
        "commit_message": "Fix pruners (#2153)\n\n\n",
        "file": "nni.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class L2FilterPruner(WeightRankFilterPruner):",
            "w_l2_norm = torch.sqrt((w ** 2).sum(dim=1))",
            "threshold = torch.topk(w_l2_norm.view(-1), num_prune, largest=False)[0].max()",
            "mask_weight = torch.gt(w_l2_norm, threshold)[:, None, None, None].expand_as(weight).type_as(weight)",
            "-        mask_bias = torch.gt(w_l2_norm, threshold).type_as(weight)",
            "+        mask_bias = torch.gt(w_l2_norm, threshold).type_as(weight).detach() if base_mask['bias_mask'] is not None else None",
            "",
            "-        return {'weight_mask': mask_weight.detach(), 'bias_mask': mask_bias.detach()}",
            "+        return {'weight_mask': mask_weight.detach(), 'bias_mask': mask_bias}",
            "",
            "",
            "class FPGMPruner(WeightRankFilterPruner):"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('conditional_expression', None), position=2, insert_id=667145)",
            "Insert(target_node=IN(type=conditional_expression), node=('call', None), position=0, insert_id=667146)",
            "Insert(target_node=IN(type=conditional_expression), node=('if', 'if'), position=1, insert_id=667147)",
            "Insert(target_node=IN(type=conditional_expression), node=('comparison_operator', None), position=2, insert_id=667148)",
            "Insert(target_node=IN(type=conditional_expression), node=('else', 'else'), position=3, insert_id=667149)",
            "Insert(target_node=IN(type=conditional_expression), node=('none', 'None'), position=4, insert_id=667150)",
            "Move(target_node=ASTNode(type=pair), node=ASTNode(type=identifier, text=mask_bias), position=2)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=667151)",
            "Move(target_node=IN(type=call), node=ASTNode(type=argument_list), position=1)",
            "Insert(target_node=IN(type=comparison_operator), node=('subscript', None), position=0, insert_id=667152)",
            "Insert(target_node=IN(type=comparison_operator), node=('is not', 'is'), position=1, insert_id=667153)",
            "Insert(target_node=IN(type=comparison_operator), node=('is not', 'not'), position=2, insert_id=667154)",
            "Insert(target_node=IN(type=comparison_operator), node=('none', 'None'), position=3, insert_id=667155)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=667156)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'detach'), position=2, insert_id=667157)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'base_mask'), position=0, insert_id=667158)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=667159)",
            "Insert(target_node=IN(type=subscript), node=('string', \"'bias_mask'\"), position=2, insert_id=667160)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=667161)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=detach))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 24,
        "number": 7799,
        "neg_line": [
            "-mask_bias = torch.gt(w_l2_norm, threshold).type_as(weight)",
            "-return {'weight_mask': mask_weight.detach(), 'bias_mask': mask_bias.detach()}"
        ],
        "pos_line": [
            "+mask_bias = torch.gt(w_l2_norm, threshold).type_as(weight).detach() if base_mask['bias_mask'] is not None else None",
            "+return {'weight_mask': mask_weight.detach(), 'bias_mask': mask_bias}"
        ],
        "core_change": "-mask_bias = torch.gt(w_l2_norm, threshold).type_as(weight) +mask_bias = torch.gt(w_l2_norm, threshold).type_as(weight).detach() if base_mask['bias_mask'] is not None else None -return {'weight_mask': mask_weight.detach(), 'bias_mask': mask_bias.detach()} +return {'weight_mask': mask_weight.detach(), 'bias_mask': mask_bias}",
        "core_API": "sqrt"
    },
    {
        "commit_hash": "939d4abdeefc07e63d8bd42e7223365a4bc67942",
        "index": "8d9ea70..2de9541 100644",
        "commit_message": "Fixed CUDA/CPU drop_connect bug\n",
        "file": "EfficientNet-PyTorch.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def drop_connect(inputs, p, training):",
            "batch_size = inputs.shape[0]",
            "keep_prob = 1 - p",
            "random_tensor = keep_prob",
            "-    random_tensor += torch.rand([batch_size, 1, 1, 1], dtype=inputs.dtype)  # uniform [0,1)",
            "+    random_tensor += torch.rand([batch_size, 1, 1, 1], dtype=inputs.dtype, device=inputs.device)",
            "binary_tensor = torch.floor(random_tensor)",
            "output = inputs / keep_prob * binary_tensor",
            "return output"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=116820)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=116821)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=116822)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=116823)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=116824)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'inputs'), position=0, insert_id=116825)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=116826)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=116827)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 7800,
        "neg_line": [
            "-random_tensor += torch.rand([batch_size, 1, 1, 1], dtype=inputs.dtype)  # uniform [0,1)"
        ],
        "pos_line": [
            "+random_tensor += torch.rand([batch_size, 1, 1, 1], dtype=inputs.dtype, device=inputs.device)"
        ],
        "core_change": "-random_tensor += torch.rand([batch_size, 1, 1, 1], dtype=inputs.dtype)  # uniform [0,1) +random_tensor += torch.rand([batch_size, 1, 1, 1], dtype=inputs.dtype, device=inputs.device)",
        "core_API": "rand"
    },
    {
        "commit_hash": "c5b6ceea4a0a151ab78b9c065584bc416d1ee275",
        "index": "43b176a3..56bd9882 100644",
        "commit_message": "tl.layers API Refactoring and various modifications (#667)\n\n* Decorators API Refactored\n\n* extra_requires `all`, `all_cpu` and `all_gpu` added\n\n* Error fix\n\n* YAPF Formating Correction\n\n* Test for private method decorator added\n\n* Test Logging Verbosity Fixed to Debug when runned individually\n\n* YAPF corrections applied\n\n* Changelog Added\n\n* Changelog updated\n\n* PR number changed\n\n* First Refactoring Pass done\n\n* cleaning second pass\n\n* Refactoring 3rd pass\n\n* Refactoring 4th Pass\n\n* Code Error fix\n\n* YAPF Formating Fix\n\n* Arguments now using self\n\n* YAPF error correction\n\n* Bug Fix in Decorator\n\n* act name bug fix\n\n* Error Correction\n\n* YAPF formating fix\n\n* Useless tf.identity removed\n\n* Error Fix\n\n* Changelog Updated\n\n* Error fix in tl.activation\n\n* Documentation error fix\n\n* Lazy Import added\n\n* Import Refactoring with LazyImport when necessary\n\n* Changelog Updated\n\n* Gitter Removed\n\n* Fixed proposed by @zsdonghao\n\n* Documentation updated\n\n* Missing requirements added\n\n* Update to TensorLayer 1.8.6rc1\n\n* Requirements error fix\n\n* Docker Files updated\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def slim_block(x):",
            "x = slim.dropout(x, 0.5, is_training=is_training)",
            "x = slim.fully_connected(x, 800, activation_fn=tf.nn.relu)",
            "x = slim.dropout(x, 0.5, is_training=is_training)",
            "-        logits = slim.fully_connected(x, 10, activation_fn=tf.identity)",
            "+        logits = slim.fully_connected(x, 10, activation_fn=None)",
            "return logits, {}"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=keyword_argument), node=('none', 'None'), position=2, insert_id=2262286)",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=identity))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 7808,
        "neg_line": [
            "-logits = slim.fully_connected(x, 10, activation_fn=tf.identity)"
        ],
        "pos_line": [
            "+logits = slim.fully_connected(x, 10, activation_fn=None)"
        ],
        "core_change": "-logits = slim.fully_connected(x, 10, activation_fn=tf.identity) +logits = slim.fully_connected(x, 10, activation_fn=None)",
        "core_API": "dropout"
    },
    {
        "commit_hash": "e1e33992865ddc9eb5a0d7f0ed1ccfb7d7a33e8c",
        "index": "fd8abeb..59d13b1 100644",
        "commit_message": "ONNX export bug fix #93\n\n",
        "file": "yolov5.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "if __name__ == '__main__':",
            "",
            "# Load pytorch model",
            "google_utils.attempt_download(opt.weights)",
            "-    model = torch.load(opt.weights, map_location=torch.device('cpu'))['model']",
            "+    model = torch.load(opt.weights, map_location=torch.device('cpu'))['model'].float()",
            "model.eval()",
            "model.fuse()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('call', None), position=2, insert_id=1304275)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1304276)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1304277)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=subscript), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1304278)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'float'), position=2, insert_id=1304279)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1304280)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1304281)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 7809,
        "neg_line": [
            "-model = torch.load(opt.weights, map_location=torch.device('cpu'))['model']"
        ],
        "pos_line": [
            "+model = torch.load(opt.weights, map_location=torch.device('cpu'))['model'].float()"
        ],
        "core_change": "-model = torch.load(opt.weights, map_location=torch.device('cpu'))['model'] +model = torch.load(opt.weights, map_location=torch.device('cpu'))['model'].float()",
        "core_API": "attempt_download"
    },
    {
        "commit_hash": "bee6d71646c22eb552ec7d78d439729b38dfa55b",
        "index": "bb3b812f..8ee13203 100644",
        "commit_message": "Fix MHA and LayerNorm on devices other than cuda:0 (fixes #1860) (#1873)\n\nSummary:\nThis also includes a fix required for LayerNorm due to a bug in apex: https://github.com/NVIDIA/apex/issues/770\nPull Request resolved: https://github.com/pytorch/fairseq/pull/1873\n\nReviewed By: ngoyal2707\n\nDifferential Revision: D20564542\n\nPulled By: myleott\n\nfbshipit-source-id: f1603779351671a5a44f606f550281bcdd2aa9b7\n\n",
        "file": "fairseq.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "try:",
            "class FusedLayerNorm(_FusedLayerNorm):",
            "@torch.jit.unused",
            "def forward(self, x):",
            "-            return super().forward(x)",
            "+            with torch.cuda.device(x.device):",
            "+                return super().forward(x)",
            "",
            "except ImportError:",
            "has_fused_layernorm = False"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=block), node=('with_statement', None), position=0, insert_id=214782)",
            "Insert(target_node=IN(type=with_statement), node=('with', 'with'), position=0, insert_id=214783)",
            "Insert(target_node=IN(type=with_statement), node=('with_clause', None), position=1, insert_id=214784)",
            "Insert(target_node=IN(type=with_statement), node=(':', ':'), position=2, insert_id=214785)",
            "Move(target_node=IN(type=with_statement), node=ASTNode(type=block), position=3)",
            "Insert(target_node=IN(type=with_clause), node=('with_item', None), position=0, insert_id=214786)",
            "Insert(target_node=IN(type=with_item), node=('call', None), position=0, insert_id=214787)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=214788)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=214789)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=214790)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=214791)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=214792)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=214793)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=214794)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=214795)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=214796)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=214797)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cuda'), position=2, insert_id=214798)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'x'), position=0, insert_id=214799)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=214800)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=214801)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 21,
        "number": 7814,
        "neg_line": [
            "-return super().forward(x)"
        ],
        "pos_line": [
            "+with torch.cuda.device(x.device):",
            "+return super().forward(x)"
        ],
        "core_change": "-return super().forward(x) +with torch.cuda.device(x.device): +return super().forward(x)",
        "core_API": "device"
    },
    {
        "commit_hash": "2772688247786f92e2f8c5c8ae837246325016f0",
        "index": "060871d14..b961ee356 100644",
        "commit_message": "fixes from mari-linhares feedback\n\n",
        "file": "PySyft.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestSerde(object):",
            "",
            "@pytest.mark.parametrize(\"compress\", [True, False])",
            "def test_torch_Tensor_convenience(self, compress):",
            "+        \"\"\"This test evaluates torch.Tensor.serialize()",
            "+",
            "+        As opposed to using syft.serde.serialize(), torch objects",
            "+        have a convenience function which lets you call .serialize()",
            "+        directly on the tensor itself. This tests to makes sure it",
            "+        works correctly.\"\"\"",
            "+",
            "hook = TorchHook(torch)",
            "",
            "t = Tensor(numpy.random.random((100, 100)))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('block', None), position=0, insert_id=847858)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=847859)",
            "Insert(target_node=IN(type=expression_statement), node=('string', '\"\"\"This test evaluates torch.Tensor.serialize()\\n\\n        As opposed to using syft.serde.serialize(), torch objects\\n        have a convenience function which lets you call .serialize()\\n        directly on the tensor itself. This tests to makes sure it\\n        works correctly.\"\"\"'), position=0, insert_id=847860)",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 5,
        "minus_line": 0,
        "AST_diff_line": 4,
        "number": 7815,
        "neg_line": [],
        "pos_line": [
            "+\"\"\"This test evaluates torch.Tensor.serialize()",
            "+",
            "+As opposed to using syft.serde.serialize(), torch objects",
            "+have a convenience function which lets you call .serialize()",
            "+directly on the tensor itself. This tests to makes sure it",
            "+works correctly.\"\"\"",
            "+"
        ],
        "core_change": "+\"\"\"This test evaluates torch.Tensor.serialize() + +As opposed to using syft.serde.serialize(), torch objects +have a convenience function which lets you call .serialize() +directly on the tensor itself. This tests to makes sure it +works correctly.\"\"\" +",
        "core_API": "parametrize"
    },
    {
        "commit_hash": "db5f46327aaa6207b2beb3a138fb819e67d63d2f",
        "index": "6b91cd056..de6c22c81 100644",
        "commit_message": "fixed wrong sum axis\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_forward(use_token_averaged_energy):",
            ")",
            "xs = torch.randn(2, 256)",
            "if not use_token_averaged_energy:",
            "-        layer(xs, torch.LongTensor([256, 128]))",
            "+        es, elens = layer(xs, torch.LongTensor([256, 128]))",
            "+        assert es.shape[1] == max(elens)",
            "else:",
            "ds = torch.LongTensor([[3, 0, 2], [3, 0, 0]])",
            "dlens = torch.LongTensor([3, 1])"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=block), node=('assert_statement', None), position=1, insert_id=148473)",
            "Insert(target_node=ASTNode(type=expression_statement), node=('assignment', None), position=0, insert_id=148474)",
            "Insert(target_node=IN(type=assert_statement), node=('assert', 'assert'), position=0, insert_id=148475)",
            "Insert(target_node=IN(type=assert_statement), node=('comparison_operator', None), position=1, insert_id=148476)",
            "Insert(target_node=IN(type=assignment), node=('pattern_list', None), position=0, insert_id=148477)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=148478)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=call), position=2)",
            "Insert(target_node=IN(type=comparison_operator), node=('subscript', None), position=0, insert_id=148479)",
            "Insert(target_node=IN(type=comparison_operator), node=('==', '=='), position=1, insert_id=148480)",
            "Insert(target_node=IN(type=comparison_operator), node=('call', None), position=2, insert_id=148481)",
            "Insert(target_node=IN(type=pattern_list), node=('identifier', 'es'), position=0, insert_id=148482)",
            "Insert(target_node=IN(type=pattern_list), node=(',', ','), position=1, insert_id=148483)",
            "Insert(target_node=IN(type=pattern_list), node=('identifier', 'elens'), position=2, insert_id=148484)",
            "Insert(target_node=IN(type=subscript), node=('attribute', None), position=0, insert_id=148485)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=148486)",
            "Insert(target_node=IN(type=subscript), node=('integer', '1'), position=2, insert_id=148487)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=148488)",
            "Insert(target_node=IN(type=call), node=('identifier', 'max'), position=0, insert_id=148489)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=148490)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'es'), position=0, insert_id=148491)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=148492)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'shape'), position=2, insert_id=148493)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=148494)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'elens'), position=1, insert_id=148495)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=148496)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 25,
        "number": 7821,
        "neg_line": [
            "-layer(xs, torch.LongTensor([256, 128]))"
        ],
        "pos_line": [
            "+es, elens = layer(xs, torch.LongTensor([256, 128]))",
            "+assert es.shape[1] == max(elens)"
        ],
        "core_change": "-layer(xs, torch.LongTensor([256, 128])) +es, elens = layer(xs, torch.LongTensor([256, 128])) +assert es.shape[1] == max(elens)",
        "core_API": "randn"
    },
    {
        "commit_hash": "427996d44b663d120eefc598d1c3b9890c31c689",
        "index": "13ce3219..0ea6d1c0 100644",
        "commit_message": "Fix forward in EndpointSpanExtractor (#3042)\n\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class EndpointSpanExtractor(SpanExtractor):",
            "span_widths = span_ends - span_starts",
            "",
            "span_width_embeddings = self._span_width_embedding(span_widths)",
            "-            return torch.cat([combined_tensors, span_width_embeddings], -1)",
            "+            combined_tensors = torch.cat([combined_tensors, span_width_embeddings], -1)",
            "",
            "if span_indices_mask is not None:",
            "return combined_tensors * span_indices_mask.unsqueeze(-1).float()",
            "+",
            "return combined_tensors"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=24959)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=24960)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'combined_tensors'), position=0, insert_id=24961)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=24962)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=call), position=2)",
            "Delete(target_node=ASTNode(type=return, text=return))",
            "Delete(target_node=ASTNode(type=return_statement))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 7823,
        "neg_line": [
            "-return torch.cat([combined_tensors, span_width_embeddings], -1)"
        ],
        "pos_line": [
            "+combined_tensors = torch.cat([combined_tensors, span_width_embeddings], -1)",
            "+"
        ],
        "core_change": "-return torch.cat([combined_tensors, span_width_embeddings], -1) +combined_tensors = torch.cat([combined_tensors, span_width_embeddings], -1) +",
        "core_API": "_span_width_embedding"
    },
    {
        "commit_hash": "70f88eecccb54e344bd8ada1698b4e62ca7d79ff",
        "index": "fb49cb9b2..11d9c07d9 100644",
        "commit_message": "Fix tapas issue (#12063)\n\n* Fix scatter function to be compatible with torch-scatter 2.7.0\n\n* Allow test again\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def _segment_reduce(values, index, segment_reduce_fn, name):",
            "",
            "segment_means = scatter(",
            "src=flat_values,",
            "-        index=flat_index.indices.type(torch.long),",
            "+        index=flat_index.indices.long(),",
            "dim=0,",
            "-        dim_size=flat_index.num_segments,",
            "+        dim_size=int(flat_index.num_segments),",
            "reduce=segment_reduce_fn,",
            ")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=keyword_argument), node=('call', None), position=2, insert_id=1215249)",
            "Insert(target_node=IN(type=call), node=('identifier', 'int'), position=0, insert_id=1215250)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1215251)",
            "Update(target_node=ASTNode(type=identifier, text=type), value='long')",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1215252)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=attribute), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1215253)",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=long))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 11,
        "number": 7824,
        "neg_line": [
            "-index=flat_index.indices.type(torch.long),",
            "-dim_size=flat_index.num_segments,"
        ],
        "pos_line": [
            "+index=flat_index.indices.long(),",
            "+dim_size=int(flat_index.num_segments),"
        ],
        "core_change": "-index=flat_index.indices.type(torch.long), +index=flat_index.indices.long(), -dim_size=flat_index.num_segments, +dim_size=int(flat_index.num_segments),",
        "core_API": "type"
    },
    {
        "commit_hash": "c9cdbb4beda06f82c2c123c6bba6f781f4ab3b5d",
        "index": "d9bfd19..21e7749 100644",
        "commit_message": "Fix sonnet/v2/src/nets/dnc:util_test_gpu test\n\nPiperOrigin-RevId: 362986883\nChange-Id: If791985603e97af2f0bca8c2255a2ba995f27b28\n\n",
        "file": "sonnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class LinearTest(test_utils.TestCase, parameterized.TestCase):",
            "lin_b = linear.Linear(output_size_b, name='lin_b')",
            "input_a = tf.random.uniform([batch_size, input_size])",
            "input_b = tf.random.uniform([batch_size, input_size])",
            "-    with self.assertRaisesRegexp(tf.errors.InvalidArgumentError,",
            "-                                 'Incompatible shapes'):",
            "+    with self.assertRaisesIncompatibleShapesError(",
            "+        tf.errors.InvalidArgumentError):",
            "util.apply_linear((input_a, input_b), (lin_a, lin_b))",
            "",
            "@parameterized.parameters("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=assertRaisesRegexp), value='assertRaisesIncompatibleShapesError')",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=string, text='Incompatible shapes'))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 3,
        "number": 7825,
        "neg_line": [
            "-with self.assertRaisesRegexp(tf.errors.InvalidArgumentError,",
            "-'Incompatible shapes'):"
        ],
        "pos_line": [
            "+with self.assertRaisesIncompatibleShapesError(",
            "+tf.errors.InvalidArgumentError):"
        ],
        "core_change": "-with self.assertRaisesRegexp(tf.errors.InvalidArgumentError, -'Incompatible shapes'): +with self.assertRaisesIncompatibleShapesError( +tf.errors.InvalidArgumentError):",
        "core_API": "Linear"
    },
    {
        "commit_hash": "d0b43268a0b496192331b1e12529d1afb92dd1a7",
        "index": "f7fd881b..ce91d374 100644",
        "commit_message": "[Feat] Enabled doctest for CI (#641)\n\n* Added doctest for CI and validated docs\n\n* Fixed doctests\n\n* Fixed flake8\n\n* test doctest ci\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class PSNRLoss(nn.Module):",
            "- output: :math:`()` a scalar",
            "",
            "Examples:",
            "-        >>> kornia.losses.psnr_loss(torch.ones(1), 1.2*torch.ones(1), 2)",
            "-        tensor(20.0000) # 10 * log(4/((1.2-1)**2)) / log(10)",
            "+        >>> psnr_loss(torch.ones(1), 1.2*torch.ones(1), 2) # 10 * log(4/((1.2-1)**2)) / log(10)",
            "+        tensor(20.0000)",
            "",
            "Reference:",
            "https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio#Definition"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=class_definition), node=('block', ''), position=4, insert_id=433707)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=psnr_loss), position=0)",
            "Delete(target_node=ASTNode(type=identifier, text=output))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=identifier, text=math))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=string, text=`()`))",
            "Delete(target_node=ASTNode(type=identifier, text=a))",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=identifier, text=scalar))",
            "Delete(target_node=ASTNode(type=type))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=block))",
            "Delete(target_node=ASTNode(type=identifier, text=kornia))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=losses))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 3,
        "AST_diff_line": 21,
        "number": 7831,
        "neg_line": [
            "-output: :math:`()` a scalar",
            "->>> kornia.losses.psnr_loss(torch.ones(1), 1.2*torch.ones(1), 2)",
            "-tensor(20.0000) # 10 * log(4/((1.2-1)**2)) / log(10)"
        ],
        "pos_line": [
            "+>>> psnr_loss(torch.ones(1), 1.2*torch.ones(1), 2) # 10 * log(4/((1.2-1)**2)) / log(10)",
            "+tensor(20.0000)"
        ],
        "core_change": "-output: :math:`()` a scalar ->>> kornia.losses.psnr_loss(torch.ones(1), 1.2*torch.ones(1), 2) -tensor(20.0000) # 10 * log(4/((1.2-1)**2)) / log(10) +>>> psnr_loss(torch.ones(1), 1.2*torch.ones(1), 2) # 10 * log(4/((1.2-1)**2)) / log(10) +tensor(20.0000)",
        "core_API": "psnr_loss"
    },
    {
        "commit_hash": "9ff7e555b9398a929c51851256557462576b272c",
        "index": "a7904b612..9fe47a8d1 100644",
        "commit_message": "Update Biosses (#3054)\n\n* add swedish_medical_ner dataset\n\n* update swedish_medical_ner\n\n* fix biosses variable names\n\nCo-authored-by: Quentin Lhoest <42851186+lhoestq@users.noreply.github.com>\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class Biosses(datasets.GeneratorBasedBuilder):",
            "df = pd.read_csv(filepath, sep=\"\\t\", encoding=\"utf-8\")",
            "for idx, row in df.iterrows():",
            "yield idx, {",
            "-                \"sentence 1\": row[\"sentence1\"],",
            "-                \"sentence 2\": row[\"sentence2\"],",
            "+                \"sentence1\": row[\"sentence1\"],",
            "+                \"sentence2\": row[\"sentence2\"],",
            "\"score\": row[\"score\"],",
            "}"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"sentence 1\"), value='\"sentence1\"')",
            "Update(target_node=ASTNode(type=string, text=\"sentence 2\"), value='\"sentence2\"')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 7834,
        "neg_line": [
            "-\"sentence 1\": row[\"sentence1\"],",
            "-\"sentence 2\": row[\"sentence2\"],"
        ],
        "pos_line": [
            "+\"sentence1\": row[\"sentence1\"],",
            "+\"sentence2\": row[\"sentence2\"],"
        ],
        "core_change": "-\"sentence 1\": row[\"sentence1\"], -\"sentence 2\": row[\"sentence2\"], +\"sentence1\": row[\"sentence1\"], +\"sentence2\": row[\"sentence2\"],",
        "core_API": "read_csv"
    },
    {
        "commit_hash": "4bf5042240d33286460b83f3dbf9be77500faab3",
        "index": "aa7c2b23d..a461a6b47 100644",
        "commit_message": "Fix BART tests on GPU (#4298)\n\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class BartForConditionalGeneration(PretrainedBartModel):",
            "if new_num_tokens <= old_num_tokens:",
            "new_bias = self.final_logits_bias[:, :new_num_tokens]",
            "else:",
            "-            extra_bias = torch.zeros((1, new_num_tokens - old_num_tokens))",
            "+            extra_bias = torch.zeros((1, new_num_tokens - old_num_tokens), device=self.final_logits_bias.device)",
            "new_bias = torch.cat([self.final_logits_bias, extra_bias], dim=1)",
            "self.register_buffer(\"final_logits_bias\", new_bias)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1239213)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1239214)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=1239215)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1239216)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=1239217)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1239218)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1239219)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=1239220)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=1239221)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1239222)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'final_logits_bias'), position=2, insert_id=1239223)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 7843,
        "neg_line": [
            "-extra_bias = torch.zeros((1, new_num_tokens - old_num_tokens))"
        ],
        "pos_line": [
            "+extra_bias = torch.zeros((1, new_num_tokens - old_num_tokens), device=self.final_logits_bias.device)"
        ],
        "core_change": "-extra_bias = torch.zeros((1, new_num_tokens - old_num_tokens)) +extra_bias = torch.zeros((1, new_num_tokens - old_num_tokens), device=self.final_logits_bias.device)",
        "core_API": "zeros"
    },
    {
        "commit_hash": "5a81195ea9f4d06b3638b2a76b3e195cd240835d",
        "index": "bc28cdc3d..e8e0cd21d 100644",
        "commit_message": "Fixed label datatype for STS-B (#6492)\n\n* fixed label datatype for sts-b\n\n* naming update\n\n* make style\n\n* make style\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "if is_tf_available():",
            "",
            "return tf.data.Dataset.from_generator(",
            "gen,",
            "-            ({k: tf.int32 for k in input_names}, tf.int64),",
            "+            ({k: tf.int32 for k in input_names}, label_type),",
            "({k: tf.TensorShape([None]) for k in input_names}, tf.TensorShape([])),",
            ")"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tf), value='label_type')",
            "Move(target_node=ASTNode(type=tuple), node=ASTNode(type=identifier, text=tf), position=3)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=int64))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 7846,
        "neg_line": [
            "-({k: tf.int32 for k in input_names}, tf.int64),"
        ],
        "pos_line": [
            "+({k: tf.int32 for k in input_names}, label_type),"
        ],
        "core_change": "-({k: tf.int32 for k in input_names}, tf.int64), +({k: tf.int32 for k in input_names}, label_type),",
        "core_API": "from_generator"
    },
    {
        "commit_hash": "1fb6d47b34ce34aecabd7ed3f849c2bd8ee4782e",
        "index": "aed74146..42200f70 100644",
        "commit_message": "[feat] Added mix augmentations in containers (#1139)\n\n* Enabled mix augmentations\n\n* Added mix augmentations for video data.\n\n* Enabled None label for mix augmentatons\n\n* init patch mix\n\n* Get it run\n\n* Get it worked\n\n* Bug locate\n\n* Samall update\n\n* ready for refactor\n\n* refactor\n\n* Update\n\n* update\n\n* Refactor init\n\n* Fixed everything\n\n* Message refine\n\n* Bug fix\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Code refactoring\n\n* bug fix\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fixed float64 bug, removed GaussianBlur annoying warning, and lint\n\n* Improved pattern\n\n* Fixed cuda\n\n* Added backward compatibility to torch 1.6\n\n* Improved docs\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Refactored code\n\n* Added more tests\n\n* bug fix\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class RandomMixUp(MixAugmentationBase):",
            "return rg.random_mixup_generator(batch_shape[0], self.p, lambda_val, same_on_batch=self.same_on_batch)",
            "",
            "def apply_transform(  # type: ignore",
            "-        self, input: torch.Tensor, label: torch.Tensor, params: Dict[str, torch.Tensor]  # type: ignore",
            "+        self, input: torch.Tensor, label: torch.Tensor, params: Dict[str, torch.Tensor]",
            ") -> Tuple[torch.Tensor, torch.Tensor]:",
            "input_permute = input.index_select(dim=0, index=params['mixup_pairs'].to(input.device))",
            "labels_permute = label.index_select(dim=0, index=params['mixup_pairs'].to(label.device))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 7863,
        "neg_line": [
            "-self, input: torch.Tensor, label: torch.Tensor, params: Dict[str, torch.Tensor]  # type: ignore"
        ],
        "pos_line": [
            "+self, input: torch.Tensor, label: torch.Tensor, params: Dict[str, torch.Tensor]"
        ],
        "core_change": "-self, input: torch.Tensor, label: torch.Tensor, params: Dict[str, torch.Tensor]  # type: ignore +self, input: torch.Tensor, label: torch.Tensor, params: Dict[str, torch.Tensor]",
        "core_API": "random_mixup_generator"
    },
    {
        "commit_hash": "fde0087810c46b28d0e7f3ba92c1f334fd2d0f61",
        "index": "8ab904fa..00cbe119 100644",
        "commit_message": "fixed transform tests\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "from .degree import DegreeAdj",
            "",
            "class DegreeTest(TestCase):",
            "def test_degree_adj(self):",
            "+        index = torch.LongTensor([[0, 0, 1, 2], [1, 2, 0, 1]])",
            "weight = torch.FloatTensor([2, 3, 4, 6])",
            "-        edge = torch.LongTensor([[0, 0, 1, 2], [1, 2, 0, 1]])",
            "-        adj = torch.sparse.FloatTensor(edge, weight, torch.Size([3, 3]))",
            "+        adj = torch.sparse.FloatTensor(index, weight, torch.Size([3, 3]))",
            "",
            "transform = DegreeAdj()",
            "",
            "-        _, adj = transform((None, adj))",
            "+        _, adj, _ = transform((None, adj, None))",
            "adj = adj.to_dense()",
            "",
            "expected_adj_out = ["
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('function_definition', None), position=2, insert_id=1090457)",
            "Move(target_node=IN(type=function_definition), node=ASTNode(type=def, text=def), position=0)",
            "Move(target_node=IN(type=function_definition), node=ASTNode(type=identifier, text=test_degree_adj), position=1)",
            "Move(target_node=IN(type=function_definition), node=ASTNode(type=parameters), position=2)",
            "Move(target_node=IN(type=function_definition), node=ASTNode(type=:, text=:), position=3)",
            "Insert(target_node=IN(type=function_definition), node=('block', None), position=4, insert_id=1090458)",
            "Move(target_node=IN(type=block), node=ASTNode(type=expression_statement), position=0)",
            "Insert(target_node=ASTNode(type=pattern_list), node=(',', ','), position=3, insert_id=1090459)",
            "Insert(target_node=ASTNode(type=pattern_list), node=('identifier', '_'), position=4, insert_id=1090460)",
            "Update(target_node=ASTNode(type=identifier, text=edge), value='index')",
            "Update(target_node=ASTNode(type=identifier, text=edge), value='index')",
            "Insert(target_node=ASTNode(type=tuple), node=(',', ','), position=4, insert_id=1090461)",
            "Insert(target_node=ASTNode(type=tuple), node=('none', 'None'), position=5, insert_id=1090462)",
            "Delete(target_node=ASTNode(type=block, text=))",
            "Delete(target_node=ASTNode(type=function_definition))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 15,
        "number": 7866,
        "neg_line": [
            "-edge = torch.LongTensor([[0, 0, 1, 2], [1, 2, 0, 1]])",
            "-adj = torch.sparse.FloatTensor(edge, weight, torch.Size([3, 3]))",
            "-_, adj = transform((None, adj))"
        ],
        "pos_line": [
            "+index = torch.LongTensor([[0, 0, 1, 2], [1, 2, 0, 1]])",
            "+adj = torch.sparse.FloatTensor(index, weight, torch.Size([3, 3]))",
            "+_, adj, _ = transform((None, adj, None))"
        ],
        "core_change": "+index = torch.LongTensor([[0, 0, 1, 2], [1, 2, 0, 1]]) -edge = torch.LongTensor([[0, 0, 1, 2], [1, 2, 0, 1]]) -adj = torch.sparse.FloatTensor(edge, weight, torch.Size([3, 3])) +adj = torch.sparse.FloatTensor(index, weight, torch.Size([3, 3])) -_, adj = transform((None, adj)) +_, adj, _ = transform((None, adj, None))",
        "core_API": "LongTensor"
    },
    {
        "commit_hash": "6bed86fb78097d610287708485ec4f65c27a4ae9",
        "index": "3edd1ad..b31355f 100644",
        "commit_message": "Fix tests\n\n",
        "file": "seq2seq.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class StackBidirectionalRNNEncoderTest(tf.test.TestCase):",
            "[self.batch_size, cell.output_size])",
            "",
            "def test_encode_with_multi_cell(self):",
            "-    cell = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.LSTMCell(32)] * 4)",
            "+    cell = tf.contrib.rnn.MultiRNNCell(",
            "+        [tf.contrib.rnn.LSTMCell(32) for _ in range(4)])",
            "encoder_output_ = self._test_encode_with_cell(cell)",
            "",
            "for layer_idx in range(4):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('list_comprehension', None), position=1, insert_id=2160284)",
            "Move(target_node=IN(type=list_comprehension), node=ASTNode(type=[, text=[), position=0)",
            "Move(target_node=IN(type=list_comprehension), node=ASTNode(type=call), position=1)",
            "Insert(target_node=IN(type=list_comprehension), node=('for_in_clause', None), position=2, insert_id=2160285)",
            "Insert(target_node=IN(type=list_comprehension), node=(']', ']'), position=3, insert_id=2160286)",
            "Insert(target_node=IN(type=for_in_clause), node=('for', 'for'), position=0, insert_id=2160287)",
            "Insert(target_node=IN(type=for_in_clause), node=('identifier', '_'), position=1, insert_id=2160288)",
            "Insert(target_node=IN(type=for_in_clause), node=('in', 'in'), position=2, insert_id=2160289)",
            "Insert(target_node=IN(type=for_in_clause), node=('call', None), position=3, insert_id=2160290)",
            "Insert(target_node=IN(type=call), node=('identifier', 'range'), position=0, insert_id=2160291)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2160292)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2160293)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=integer, text=4), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2160294)",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=list))",
            "Delete(target_node=ASTNode(type=*, text=*))",
            "Delete(target_node=ASTNode(type=binary_operator))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 18,
        "number": 7870,
        "neg_line": [
            "-cell = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.LSTMCell(32)] * 4)"
        ],
        "pos_line": [
            "+cell = tf.contrib.rnn.MultiRNNCell(",
            "+[tf.contrib.rnn.LSTMCell(32) for _ in range(4)])"
        ],
        "core_change": "-cell = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.LSTMCell(32)] * 4) +cell = tf.contrib.rnn.MultiRNNCell( +[tf.contrib.rnn.LSTMCell(32) for _ in range(4)])",
        "core_API": "MultiRNNCell"
    },
    {
        "commit_hash": "66d4b129903058365f6cf1d4fb4ac73814ca3bfc",
        "index": "264e1686..e63f7d56 100644",
        "commit_message": "Fix examples and tutorials to use the updated tensor API (#886)\n\n* Fix examples and tutorials to use the updated tensor API\n\n* small fixes\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "\"Conceptually `iarange` is the same as `irange` except that it is a vectorized operation (as `torch.arange` is to `range`). As such it potentially enables large speed-ups compared to the explicit `for` loop that appears with `irange`. Let's see how this looks for our running example. First we need `data` to be in the form of a tensor:\\n\",",
            "\"\\n\",",
            "\"```python\\n\",",
            "-    \"data = Variable(torch.zeros(10, 1))\\n\",",
            "+    \"data = torch.zeros(10, 1)\\n\",",
            "\"data[0:6, 0] = torch.ones(6)  # 6 heads and 4 tails\\n\",",
            "\"```\\n\",",
            "\"\\n\","
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"data = Variable(torch.zeros(10, 1))\\n\"), value='\"data = torch.zeros(10, 1)\\\\n\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 7871,
        "neg_line": [
            "-\"data = Variable(torch.zeros(10, 1))\\n\","
        ],
        "pos_line": [
            "+\"data = torch.zeros(10, 1)\\n\","
        ],
        "core_change": "-\"data = Variable(torch.zeros(10, 1))\\n\", +\"data = torch.zeros(10, 1)\\n\",",
        "core_API": "zeros"
    },
    {
        "commit_hash": "3d826058a10ed3a6e4dfb2ed70d44b0f979d673a",
        "index": "ea8ce2374..199027aca 100644",
        "commit_message": "support torch 1.4.0 (OpenMined#4963) (#4982)\n\n* support torch 1.4.0 (OpenMined#4963)\n\n* Fixed namedtuple test for torch==1.4.0\n\n- Updated README.md's with 1.4\n- Linting\n\n* Adding 1.4.0 to CI\n\n* Changed 1.4.0 in CI back to just the target system Ubuntu python 3.7\n\n- We need this for a specific application so this will keep CI more free\n\n* Windows still occasionally causing OOM errors\n\n- Adding a 7th chunk\n\n* Added asyncio to sympc test\n\nCo-authored-by: Madhava Jay <me@madhavajay.com>\n",
        "file": "PySyft.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "from syft.core.common.serde.deserialize import _deserialize",
            "from syft.lib.python import ValuesIndices",
            "",
            "",
            "-def test_torch_valuesindices_serde():",
            "-    x = torch.Tensor([1, 2, 3])",
            "-    y = x.cummax(0)",
            "+def test_torch_valuesindices_serde() -> None:",
            "+    x = torch.Tensor([[1, 2], [1, 2]])",
            "+    y = x.mode()",
            "values = y.values",
            "indices = y.indices"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=function_definition), node=('->', '->'), position=3, insert_id=783430)",
            "Insert(target_node=ASTNode(type=function_definition), node=('type', None), position=4, insert_id=783431)",
            "Insert(target_node=IN(type=type), node=('none', 'None'), position=0, insert_id=783432)",
            "Update(target_node=ASTNode(type=identifier, text=cummax), value='mode')",
            "Insert(target_node=ASTNode(type=list), node=('list', None), position=1, insert_id=783433)",
            "Insert(target_node=ASTNode(type=list), node=('list', None), position=4, insert_id=783434)",
            "Insert(target_node=ASTNode(type=list), node=(']', ']'), position=5, insert_id=783435)",
            "Insert(target_node=IN(type=list), node=('[', '['), position=0, insert_id=783436)",
            "Move(target_node=IN(type=list), node=ASTNode(type=integer, text=1), position=1)",
            "Move(target_node=IN(type=list), node=ASTNode(type=,, text=,), position=2)",
            "Move(target_node=IN(type=list), node=ASTNode(type=integer, text=2), position=3)",
            "Insert(target_node=IN(type=list), node=(']', ']'), position=4, insert_id=783437)",
            "Insert(target_node=IN(type=list), node=('[', '['), position=0, insert_id=783438)",
            "Insert(target_node=IN(type=list), node=('integer', '1'), position=1, insert_id=783439)",
            "Insert(target_node=IN(type=list), node=(',', ','), position=2, insert_id=783440)",
            "Insert(target_node=IN(type=list), node=('integer', '2'), position=3, insert_id=783441)",
            "Move(target_node=IN(type=list), node=ASTNode(type=], text=]), position=4)",
            "Delete(target_node=ASTNode(type=integer, text=3))",
            "Delete(target_node=ASTNode(type=integer, text=0))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 19,
        "number": 7874,
        "neg_line": [
            "-def test_torch_valuesindices_serde():",
            "-x = torch.Tensor([1, 2, 3])",
            "-y = x.cummax(0)"
        ],
        "pos_line": [
            "+def test_torch_valuesindices_serde() -> None:",
            "+x = torch.Tensor([[1, 2], [1, 2]])",
            "+y = x.mode()"
        ],
        "core_change": "-def test_torch_valuesindices_serde(): -x = torch.Tensor([1, 2, 3]) -y = x.cummax(0) +def test_torch_valuesindices_serde() -> None: +x = torch.Tensor([[1, 2], [1, 2]]) +y = x.mode()",
        "core_API": "Tensor"
    },
    {
        "commit_hash": "93a30aae1125fa15234e2dba7f4850701b1e7316",
        "index": "b76848a..5a6d1a5 100644",
        "commit_message": "fix minor bug\n\n",
        "file": "faster-rcnn.pytorch.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class _ProposalTargetLayer(nn.Module):",
            "num_proposal = overlaps.size(1)",
            "num_boxes_per_img = overlaps.size(2)",
            "",
            "-        offset = torch.arange(0, batch_size)*20",
            "+        offset = torch.arange(0, batch_size)*gt_boxes.size(1)",
            "offset = offset.view(-1, 1).type_as(gt_assignment) + gt_assignment",
            "",
            "labels = gt_boxes[:,:,4].contiguous().view(-1).index(offset.view(-1))\\"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=binary_operator), node=('call', None), position=2, insert_id=228213)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=228214)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=228215)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'gt_boxes'), position=0, insert_id=228216)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=228217)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'size'), position=2, insert_id=228218)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=228219)",
            "Insert(target_node=IN(type=argument_list), node=('integer', '1'), position=1, insert_id=228220)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=228221)",
            "Delete(target_node=ASTNode(type=integer, text=20))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 7875,
        "neg_line": [
            "-offset = torch.arange(0, batch_size)*20"
        ],
        "pos_line": [
            "+offset = torch.arange(0, batch_size)*gt_boxes.size(1)"
        ],
        "core_change": "-offset = torch.arange(0, batch_size)*20 +offset = torch.arange(0, batch_size)*gt_boxes.size(1)",
        "core_API": "size"
    },
    {
        "commit_hash": "149a703884ee126d20a9df187abe9176bb4a581d",
        "index": "977b825..85a3f81 100644",
        "commit_message": "add compat.is_gpu_available(); fix test failure\n\n",
        "file": "TensorFlowOnSpark.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def start_cluster_server(ctx, num_gpus=1, rdma=False):",
            "cluster_spec = ctx.cluster_spec",
            "logging.info(\"{0}: Cluster spec: {1}\".format(ctx.worker_num, cluster_spec))",
            "",
            "-  if tf.test.is_built_with_cuda() and num_gpus > 0:",
            "+  if compat.is_gpu_available() and num_gpus > 0:",
            "# compute my index relative to other nodes placed on the same host (for GPU allocation)",
            "my_addr = cluster_spec[ctx.job_name][ctx.task_index]",
            "my_host = my_addr.split(':')[0]"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tf), value='compat')",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=tf), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=test), value='is_gpu_available')",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=test), position=3)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=identifier, text=is_built_with_cuda))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 7878,
        "neg_line": [
            "-if tf.test.is_built_with_cuda() and num_gpus > 0:"
        ],
        "pos_line": [
            "+if compat.is_gpu_available() and num_gpus > 0:"
        ],
        "core_change": "-if tf.test.is_built_with_cuda() and num_gpus > 0: +if compat.is_gpu_available() and num_gpus > 0:",
        "core_API": "info"
    },
    {
        "commit_hash": "f38aaa5be05104ddfc3452249eeb521f139ded99",
        "index": "62371745..97da326c 100755",
        "commit_message": "improved global tensor handling, various other fixes\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Evolutionary(Optimizer):",
            "",
            "return deltas_sum, perturbations",
            "",
            "-            deltas_sum, perturbations = tf.while_loop(",
            "+            deltas_sum, perturbations = self.while_loop(",
            "cond=util.tf_always_true, body=body, loop_vars=(deltas_sum, perturbations),",
            "maximum_iterations=(self.num_samples - 1)",
            ")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tf), value='self')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 7881,
        "neg_line": [
            "-deltas_sum, perturbations = tf.while_loop("
        ],
        "pos_line": [
            "+deltas_sum, perturbations = self.while_loop("
        ],
        "core_change": "-deltas_sum, perturbations = tf.while_loop( +deltas_sum, perturbations = self.while_loop(",
        "core_API": "while_loop"
    },
    {
        "commit_hash": "25844012ee50730a924d5e4a30f6bf2183a4327d",
        "index": "1ead7cb6..3a4d81c2 100644",
        "commit_message": "[Fix] Increase speed of transform_points (#687)\n\n* Faster projection with broadcasting\n\n* transform_points modification\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestRandomCutMix:",
            "",
            "assert_allclose(out_image, expected, rtol=1e-4, atol=1e-4)",
            "assert (out_label[0, :, 0] == label).all()",
            "-        assert (out_label[0, :, 1] == torch.tensor([0, 1])).all()",
            "+        assert (out_label[0, :, 1] == torch.tensor([0, 1], device=device, dtype=dtype)).all()",
            "# cut area = 4 / 12",
            "-        assert_allclose(out_label[0, :, 2], torch.tensor([0.33333, 0.33333], dtype=dtype))",
            "+        assert_allclose(out_label[0, :, 2], torch.tensor([0.33333, 0.33333], device=device, dtype=dtype))",
            "",
            "def test_random_mixup_num2(self, device, dtype):",
            "torch.manual_seed(76)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=436749)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=436750)",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=436751)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=436752)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=436753)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=2, insert_id=436754)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=436755)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=436756)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tensor'), position=2, insert_id=436757)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=436758)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=436759)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=436760)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=436761)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=436762)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=436763)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=2, insert_id=436764)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=436765)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=436766)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=2, insert_id=436767)",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=tensor))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 24,
        "number": 7887,
        "neg_line": [
            "-assert (out_label[0, :, 1] == torch.tensor([0, 1])).all()",
            "-assert_allclose(out_label[0, :, 2], torch.tensor([0.33333, 0.33333], dtype=dtype))"
        ],
        "pos_line": [
            "+assert (out_label[0, :, 1] == torch.tensor([0, 1], device=device, dtype=dtype)).all()",
            "+assert_allclose(out_label[0, :, 2], torch.tensor([0.33333, 0.33333], device=device, dtype=dtype))"
        ],
        "core_change": "-assert (out_label[0, :, 1] == torch.tensor([0, 1])).all() +assert (out_label[0, :, 1] == torch.tensor([0, 1], device=device, dtype=dtype)).all() -assert_allclose(out_label[0, :, 2], torch.tensor([0.33333, 0.33333], dtype=dtype)) +assert_allclose(out_label[0, :, 2], torch.tensor([0.33333, 0.33333], device=device, dtype=dtype))",
        "core_API": "tensor"
    },
    {
        "commit_hash": "2c627cb1ebcc990f92f7c524bd99808f86ca2e50",
        "index": "0836a6a0ed..3afef4854e 100644",
        "commit_message": "fixed failing core.container unit tests.\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "from ivy.core.device import default_device",
            "",
            "",
            "def random_uniform(low=0., high=1., shape=None, dev=None):",
            "-    dev = default_device(dev)",
            "-    with _tf.device('/' + dev.upper()):",
            "+    with _tf.device(default_device(dev)):",
            "return _tf.random.uniform(shape if shape else (), low, high)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=call), position=1)",
            "Delete(target_node=ASTNode(type=identifier, text=dev))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=string, text='/'))",
            "Delete(target_node=ASTNode(type=+, text=+))",
            "Delete(target_node=ASTNode(type=identifier, text=dev))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=upper))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=binary_operator))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 16,
        "number": 7890,
        "neg_line": [
            "-dev = default_device(dev)",
            "-with _tf.device('/' + dev.upper()):"
        ],
        "pos_line": [
            "+with _tf.device(default_device(dev)):"
        ],
        "core_change": "-dev = default_device(dev) -with _tf.device('/' + dev.upper()): +with _tf.device(default_device(dev)):",
        "core_API": "device"
    },
    {
        "commit_hash": "7338454fcf490d42093e88dc78de1b5cf1dbf006",
        "index": "273fea8..e0dd8ee 100644",
        "commit_message": "fix: minor\n\n",
        "file": "deep-voice-conversion.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def train(args, logdir):",
            "# dataflow",
            "df = Net1DataFlow(hp.train1.data_path, hp.train1.batch_size)",
            "",
            "-    ckpt1 = tf.train.latest_checkpoint(logdir1)",
            "-",
            "# set logger for event and model saver",
            "logger.set_logger_dir(logdir)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=ckpt1))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=train))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=latest_checkpoint))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=logdir1))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 16,
        "number": 7898,
        "neg_line": [
            "-ckpt1 = tf.train.latest_checkpoint(logdir1)",
            "-"
        ],
        "pos_line": [],
        "core_change": "-ckpt1 = tf.train.latest_checkpoint(logdir1) -",
        "core_API": "latest_checkpoint"
    },
    {
        "commit_hash": "c5b6ceea4a0a151ab78b9c065584bc416d1ee275",
        "index": "d8dca011..56fc58fb 100755",
        "commit_message": "tl.layers API Refactoring and various modifications (#667)\n\n* Decorators API Refactored\n\n* extra_requires `all`, `all_cpu` and `all_gpu` added\n\n* Error fix\n\n* YAPF Formating Correction\n\n* Test for private method decorator added\n\n* Test Logging Verbosity Fixed to Debug when runned individually\n\n* YAPF corrections applied\n\n* Changelog Added\n\n* Changelog updated\n\n* PR number changed\n\n* First Refactoring Pass done\n\n* cleaning second pass\n\n* Refactoring 3rd pass\n\n* Refactoring 4th Pass\n\n* Code Error fix\n\n* YAPF Formating Fix\n\n* Arguments now using self\n\n* YAPF error correction\n\n* Bug Fix in Decorator\n\n* act name bug fix\n\n* Error Correction\n\n* YAPF formating fix\n\n* Useless tf.identity removed\n\n* Error Fix\n\n* Changelog Updated\n\n* Error fix in tl.activation\n\n* Documentation error fix\n\n* Lazy Import added\n\n* Import Refactoring with LazyImport when necessary\n\n* Changelog Updated\n\n* Gitter Removed\n\n* Fixed proposed by @zsdonghao\n\n* Documentation updated\n\n* Missing requirements added\n\n* Update to TensorLayer 1.8.6rc1\n\n* Requirements error fix\n\n* Docker Files updated\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def Vgg19_simple_api(rgb):",
            "net = FlattenLayer(net, name='flatten')",
            "net = DenseLayer(net, n_units=4096, act=tf.nn.relu, name='fc6')",
            "net = DenseLayer(net, n_units=4096, act=tf.nn.relu, name='fc7')",
            "-    net = DenseLayer(net, n_units=1000, act=tf.identity, name='fc8')",
            "+    net = DenseLayer(net, n_units=1000, act=None, name='fc8')",
            "print(\"build model finished: %fs\" % (time.time() - start_time))",
            "return net"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=keyword_argument), node=('none', 'None'), position=2, insert_id=2262292)",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=identity))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 7904,
        "neg_line": [
            "-net = DenseLayer(net, n_units=1000, act=tf.identity, name='fc8')"
        ],
        "pos_line": [
            "+net = DenseLayer(net, n_units=1000, act=None, name='fc8')"
        ],
        "core_change": "-net = DenseLayer(net, n_units=1000, act=tf.identity, name='fc8') +net = DenseLayer(net, n_units=1000, act=None, name='fc8')",
        "core_API": "time"
    },
    {
        "commit_hash": "4e65b0b72d296981b32e0a21cc5d9d69f58c2720",
        "index": "510e0a219..f59b9a16b 100644",
        "commit_message": "fix style\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class LogMel(torch.nn.Module):",
            "return ', '.join(f'{k}={v}' for k, v in self.mel_options.items())",
            "",
            "def forward(",
            "-        self, feat: torch.Tensor, ilens: torch.Tensor=None,",
            "+        self, feat: torch.Tensor, ilens: torch.Tensor = None,",
            ") -> Tuple[torch.Tensor, torch.Tensor]:",
            "# feat: (B, T, D1) x melmat: (D1, D2) -> mel_feat: (B, T, D2)",
            "mel_feat = torch.matmul(feat, self.melmat)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 7909,
        "neg_line": [
            "-self, feat: torch.Tensor, ilens: torch.Tensor=None,"
        ],
        "pos_line": [
            "+self, feat: torch.Tensor, ilens: torch.Tensor = None,"
        ],
        "core_change": "-self, feat: torch.Tensor, ilens: torch.Tensor=None, +self, feat: torch.Tensor, ilens: torch.Tensor = None,",
        "core_API": "items"
    },
    {
        "commit_hash": "77770ec79883343d32051cfb6a04f64523cd8df1",
        "index": "c44960564..981007f5a 100755",
        "commit_message": "Fix trainer logging_nan_inf_filter in torch_xla mode (#13896)\n\n* Fix logging_nan_inf_filter in torch_xla mode\n\n* Update src/transformers/trainer.py\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n\n* Fix format\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Trainer:",
            "else:",
            "tr_loss_step = self.training_step(model, inputs)",
            "",
            "-                if args.logging_nan_inf_filter and (torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step)):",
            "-                    # if loss is nan or inf simply add the average of previous logged losses",
            "-                    tr_loss += tr_loss / (1 + self.state.global_step - self._globalstep_last_logged)",
            "+                if args.logging_nan_inf_filter and not is_torch_tpu_available():",
            "+                    if torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step):",
            "+                        # if loss is nan or inf simply add the average of previous logged losses",
            "+                        tr_loss += tr_loss / (1 + self.state.global_step - self._globalstep_last_logged)",
            "else:",
            "tr_loss += tr_loss_step"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=if_statement), node=('boolean_operator', None), position=1, insert_id=1210661)",
            "Insert(target_node=ASTNode(type=if_statement), node=(':', ':'), position=2, insert_id=1210662)",
            "Insert(target_node=ASTNode(type=if_statement), node=('block', None), position=3, insert_id=1210663)",
            "Move(target_node=IN(type=boolean_operator), node=ASTNode(type=attribute), position=0)",
            "Move(target_node=IN(type=boolean_operator), node=ASTNode(type=and, text=and), position=1)",
            "Insert(target_node=IN(type=boolean_operator), node=('not_operator', None), position=2, insert_id=1210664)",
            "Insert(target_node=IN(type=block), node=('if_statement', None), position=0, insert_id=1210665)",
            "Insert(target_node=IN(type=not_operator), node=('not', 'not'), position=0, insert_id=1210666)",
            "Insert(target_node=IN(type=not_operator), node=('call', None), position=1, insert_id=1210667)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=1210668)",
            "Move(target_node=IN(type=if_statement), node=ASTNode(type=boolean_operator), position=1)",
            "Move(target_node=IN(type=if_statement), node=ASTNode(type=:, text=:), position=2)",
            "Move(target_node=IN(type=if_statement), node=ASTNode(type=block), position=3)",
            "Insert(target_node=IN(type=call), node=('identifier', 'is_torch_tpu_available'), position=0, insert_id=1210669)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1210670)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=(, text=(), position=0)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1210671)",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=parenthesized_expression))",
            "Delete(target_node=ASTNode(type=boolean_operator))"
        ],
        "plus_line": 4,
        "minus_line": 3,
        "AST_diff_line": 20,
        "number": 7912,
        "neg_line": [
            "-if args.logging_nan_inf_filter and (torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step)):",
            "-# if loss is nan or inf simply add the average of previous logged losses",
            "-tr_loss += tr_loss / (1 + self.state.global_step - self._globalstep_last_logged)"
        ],
        "pos_line": [
            "+if args.logging_nan_inf_filter and not is_torch_tpu_available():",
            "+if torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step):",
            "+# if loss is nan or inf simply add the average of previous logged losses",
            "+tr_loss += tr_loss / (1 + self.state.global_step - self._globalstep_last_logged)"
        ],
        "core_change": "-if args.logging_nan_inf_filter and (torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step)): -# if loss is nan or inf simply add the average of previous logged losses -tr_loss += tr_loss / (1 + self.state.global_step - self._globalstep_last_logged) +if args.logging_nan_inf_filter and not is_torch_tpu_available(): +if torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step): +# if loss is nan or inf simply add the average of previous logged losses +tr_loss += tr_loss / (1 + self.state.global_step - self._globalstep_last_logged)",
        "core_API": "training_step"
    },
    {
        "commit_hash": "360a32811c2c2eee3f182c63f90abc328a809a7e",
        "index": "e85d934..9a6567b 100644",
        "commit_message": "weight_decay fix\n\n",
        "file": "yolov3.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def train():",
            "",
            "# Compute loss",
            "loss, loss_items = compute_loss(pred, targets, model)",
            "-            if torch.isnan(loss):",
            "-                print('WARNING: nan loss detected, skipping batch ', loss_items)",
            "+            if not torch.isfinite(loss):",
            "+                print('WARNING: non-finite loss, skipping batch ', loss_items)",
            "continue",
            "",
            "# Scale loss by nominal batch_size of 64"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=if_statement), node=('not_operator', None), position=1, insert_id=1284220)",
            "Insert(target_node=IN(type=not_operator), node=('not', 'not'), position=0, insert_id=1284221)",
            "Move(target_node=IN(type=not_operator), node=ASTNode(type=call), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=isnan), value='isfinite')",
            "Update(target_node=ASTNode(type=string, text='WARNING: nan loss detected, skipping batch '), value=\"'WARNING: non-finite loss, skipping batch '\")"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 5,
        "number": 7915,
        "neg_line": [
            "-if torch.isnan(loss):",
            "-print('WARNING: nan loss detected, skipping batch ', loss_items)"
        ],
        "pos_line": [
            "+if not torch.isfinite(loss):",
            "+print('WARNING: non-finite loss, skipping batch ', loss_items)"
        ],
        "core_change": "-if torch.isnan(loss): -print('WARNING: nan loss detected, skipping batch ', loss_items) +if not torch.isfinite(loss): +print('WARNING: non-finite loss, skipping batch ', loss_items)",
        "core_API": "isnan"
    },
    {
        "commit_hash": "70e4d0e52445b6e27554777435a84d423bcf3501",
        "index": "40d1ab6f..9fcd77a7 100644",
        "commit_message": "Fix grad_norm handling\n\n",
        "file": "TTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Trainer:",
            "optimizer.step()",
            "",
            "# pytorch skips the step when the norm is 0. So ignore the norm value when it is NaN",
            "-        if isinstance(grad_norm ,torch.Tensor) and (torch.isnan(grad_norm) or torch.isinf(grad_norm)):",
            "+        if isinstance(grad_norm, torch.Tensor) and (torch.isnan(grad_norm) or torch.isinf(grad_norm)):",
            "grad_norm = 0",
            "",
            "step_time = time.time() - step_start_time"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 7917,
        "neg_line": [
            "-if isinstance(grad_norm ,torch.Tensor) and (torch.isnan(grad_norm) or torch.isinf(grad_norm)):"
        ],
        "pos_line": [
            "+if isinstance(grad_norm, torch.Tensor) and (torch.isnan(grad_norm) or torch.isinf(grad_norm)):"
        ],
        "core_change": "-if isinstance(grad_norm ,torch.Tensor) and (torch.isnan(grad_norm) or torch.isinf(grad_norm)): +if isinstance(grad_norm, torch.Tensor) and (torch.isnan(grad_norm) or torch.isinf(grad_norm)):",
        "core_API": "step"
    },
    {
        "commit_hash": "ee950b503eeed5aca3747a4bcf2a40f624b743a0",
        "index": "b2915094..0469c1f4 100755",
        "commit_message": "fixed trpo\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class NaturalGradient(Optimizer):",
            "kldiv_gradients = [",
            "tf.convert_to_tensor(value=grad) for grad in tf.gradients(ys=kldiv, xs=variables)",
            "]",
            "-        # if not all(isinstance(grad, tf.Tensor)):  warning!!!",
            "",
            "# Calculates the product x * F of a given vector x with the fisher matrix F.",
            "# Incorporating the product prevents having to calculate the entire matrix explicitly."
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 7920,
        "neg_line": [
            "-# if not all(isinstance(grad, tf.Tensor)):  warning!!!"
        ],
        "pos_line": [],
        "core_change": "-# if not all(isinstance(grad, tf.Tensor)):  warning!!!",
        "core_API": "convert_to_tensor"
    },
    {
        "commit_hash": "455c6390938a5c737fa63e78396cedae41e4e87e",
        "index": "481fff14e..ddbe5a246 100644",
        "commit_message": "CDN urls (#4030)\n\n* [file_utils] use_cdn + documentation\n\n* Move to cdn. urls for weights\n\n* [urls] Hotfix for bert-base-japanese\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "models",
        "change": [
            "from .modeling_xlm import (",
            "logger = logging.getLogger(__name__)",
            "",
            "FLAUBERT_PRETRAINED_MODEL_ARCHIVE_MAP = {",
            "-    \"flaubert-small-cased\": \"https://s3.amazonaws.com/models.huggingface.co/bert/flaubert/flaubert_small_cased/pytorch_model.bin\",",
            "-    \"flaubert-base-uncased\": \"https://s3.amazonaws.com/models.huggingface.co/bert/flaubert/flaubert_base_uncased/pytorch_model.bin\",",
            "-    \"flaubert-base-cased\": \"https://s3.amazonaws.com/models.huggingface.co/bert/flaubert/flaubert_base_cased/pytorch_model.bin\",",
            "-    \"flaubert-large-cased\": \"https://s3.amazonaws.com/models.huggingface.co/bert/flaubert/flaubert_large_cased/pytorch_model.bin\",",
            "+    \"flaubert-small-cased\": \"https://cdn.huggingface.co/flaubert/flaubert_small_cased/pytorch_model.bin\",",
            "+    \"flaubert-base-uncased\": \"https://cdn.huggingface.co/flaubert/flaubert_base_uncased/pytorch_model.bin\",",
            "+    \"flaubert-base-cased\": \"https://cdn.huggingface.co/flaubert/flaubert_base_cased/pytorch_model.bin\",",
            "+    \"flaubert-large-cased\": \"https://cdn.huggingface.co/flaubert/flaubert_large_cased/pytorch_model.bin\",",
            "}"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=,, text=,), node=ASTNode(type=dictionary), position=7)",
            "Insert(target_node=ASTNode(type=dictionary), node=(',', ','), position=2, insert_id=2689765)",
            "Insert(target_node=ASTNode(type=pair), node=(':', ':'), position=1, insert_id=2689766)",
            "Update(target_node=ASTNode(type=string, text=\"https://s3.amazonaws.com/models.huggingface.co/bert/flaubert/flaubert_small_cased/pytorch_model.bin\"), value='\"https://cdn.huggingface.co/flaubert/flaubert_small_cased/pytorch_model.bin\"')",
            "Update(target_node=ASTNode(type=string, text=\"https://s3.amazonaws.com/models.huggingface.co/bert/flaubert/flaubert_base_uncased/pytorch_model.bin\"), value='\"https://cdn.huggingface.co/flaubert/flaubert_base_uncased/pytorch_model.bin\"')",
            "Update(target_node=ASTNode(type=string, text=\"https://s3.amazonaws.com/models.huggingface.co/bert/flaubert/flaubert_base_cased/pytorch_model.bin\"), value='\"https://cdn.huggingface.co/flaubert/flaubert_base_cased/pytorch_model.bin\"')",
            "Move(target_node=ASTNode(type=pair), node=ASTNode(type=:, text=:), position=1)",
            "Update(target_node=ASTNode(type=string, text=\"https://s3.amazonaws.com/models.huggingface.co/bert/flaubert/flaubert_large_cased/pytorch_model.bin\"), value='\"https://cdn.huggingface.co/flaubert/flaubert_large_cased/pytorch_model.bin\"')",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 4,
        "minus_line": 4,
        "AST_diff_line": 10,
        "number": 7923,
        "neg_line": [
            "-\"flaubert-small-cased\": \"https://s3.amazonaws.com/models.huggingface.co/bert/flaubert/flaubert_small_cased/pytorch_model.bin\",",
            "-\"flaubert-base-uncased\": \"https://s3.amazonaws.com/models.huggingface.co/bert/flaubert/flaubert_base_uncased/pytorch_model.bin\",",
            "-\"flaubert-base-cased\": \"https://s3.amazonaws.com/models.huggingface.co/bert/flaubert/flaubert_base_cased/pytorch_model.bin\",",
            "-\"flaubert-large-cased\": \"https://s3.amazonaws.com/models.huggingface.co/bert/flaubert/flaubert_large_cased/pytorch_model.bin\","
        ],
        "pos_line": [
            "+\"flaubert-small-cased\": \"https://cdn.huggingface.co/flaubert/flaubert_small_cased/pytorch_model.bin\",",
            "+\"flaubert-base-uncased\": \"https://cdn.huggingface.co/flaubert/flaubert_base_uncased/pytorch_model.bin\",",
            "+\"flaubert-base-cased\": \"https://cdn.huggingface.co/flaubert/flaubert_base_cased/pytorch_model.bin\",",
            "+\"flaubert-large-cased\": \"https://cdn.huggingface.co/flaubert/flaubert_large_cased/pytorch_model.bin\","
        ],
        "core_change": "-\"flaubert-small-cased\": \"https://s3.amazonaws.com/models.huggingface.co/bert/flaubert/flaubert_small_cased/pytorch_model.bin\", -\"flaubert-base-uncased\": \"https://s3.amazonaws.com/models.huggingface.co/bert/flaubert/flaubert_base_uncased/pytorch_model.bin\", -\"flaubert-base-cased\": \"https://s3.amazonaws.com/models.huggingface.co/bert/flaubert/flaubert_base_cased/pytorch_model.bin\", -\"flaubert-large-cased\": \"https://s3.amazonaws.com/models.huggingface.co/bert/flaubert/flaubert_large_cased/pytorch_model.bin\", +\"flaubert-small-cased\": \"https://cdn.huggingface.co/flaubert/flaubert_small_cased/pytorch_model.bin\", +\"flaubert-base-uncased\": \"https://cdn.huggingface.co/flaubert/flaubert_base_uncased/pytorch_model.bin\", +\"flaubert-base-cased\": \"https://cdn.huggingface.co/flaubert/flaubert_base_cased/pytorch_model.bin\", +\"flaubert-large-cased\": \"https://cdn.huggingface.co/flaubert/flaubert_large_cased/pytorch_model.bin\",",
        "core_API": "getLogger"
    },
    {
        "commit_hash": "13f86830ba488d5e5731fdfe90ec04640ae82e65",
        "index": "9edf99d8..975306c2 100644",
        "commit_message": "[Fix] Fix solov2 cannot dealing with empty gt image (#9185)\n\n* [Fix] Fix solov2 cannot dealing with empty gt image\n\n* minor fix\n",
        "file": "mmdetection.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SOLOV2Head(SOLOHead):",
            "if num_pos > 0:",
            "loss_mask = torch.cat(loss_mask).sum() / num_pos",
            "else:",
            "-            loss_mask = torch.cat(loss_mask).mean()",
            "+            loss_mask = mask_feats.sum() * 0",
            "",
            "# cate",
            "flatten_labels = ["
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('binary_operator', None), position=4, insert_id=613819)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=binary_operator), node=('*', '*'), position=1, insert_id=613820)",
            "Insert(target_node=IN(type=binary_operator), node=('integer', '0'), position=2, insert_id=613821)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=argument_list), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=torch), value='mask_feats')",
            "Update(target_node=ASTNode(type=identifier, text=cat), value='sum')",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=loss_mask))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=mean))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 15,
        "number": 7924,
        "neg_line": [
            "-loss_mask = torch.cat(loss_mask).mean()"
        ],
        "pos_line": [
            "+loss_mask = mask_feats.sum() * 0"
        ],
        "core_change": "-loss_mask = torch.cat(loss_mask).mean() +loss_mask = mask_feats.sum() * 0",
        "core_API": "cat"
    },
    {
        "commit_hash": "5629292bde3fd5bf7c81d242274bc2c8aa07c6e5",
        "index": "09c7e923..b5b6e132 100644",
        "commit_message": "bug fixes\n\n",
        "file": "TTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Decoder(nn.Module):",
            "else:",
            "stop_token = self.stopnet(stopnet_input)",
            "output = output[:, : self.r * self.memory_dim]",
            "-        return output, stop_token, self.attention_layer.attention_weights",
            "+        return output, stop_token, self.attention.attention_weights",
            "",
            "def _update_memory_input(self, new_memory):",
            "if self.use_memory_queue:"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=attention_layer), value='attention')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 7925,
        "neg_line": [
            "-return output, stop_token, self.attention_layer.attention_weights"
        ],
        "pos_line": [
            "+return output, stop_token, self.attention.attention_weights"
        ],
        "core_change": "-return output, stop_token, self.attention_layer.attention_weights +return output, stop_token, self.attention.attention_weights",
        "core_API": "stopnet"
    },
    {
        "commit_hash": "b40766c3330cf57957952b3a93c9735a8eced727",
        "index": "4b8770dd8..d07b272c1 100644",
        "commit_message": "Fix mypy typing errors in pytorch_lightning/callbacks/model_checkpoint.py (#13617)\n\nCo-authored-by: Carlos Mocholí <carlossmocholi@gmail.com>\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class LightningModule(",
            "return torch.tensor(value, device=self.device)",
            "",
            "@staticmethod",
            "-    def __check_numel_1(value: torch.Tensor, name: str) -> None:",
            "+    def __check_numel_1(value: Tensor, name: str) -> None:",
            "if not torch.numel(value) == 1:",
            "raise ValueError(",
            "f\"`self.log({name}, {value})` was called, but the tensor must have a single element.\""
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=Tensor), position=2)",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 7927,
        "neg_line": [
            "-def __check_numel_1(value: torch.Tensor, name: str) -> None:"
        ],
        "pos_line": [
            "+def __check_numel_1(value: Tensor, name: str) -> None:"
        ],
        "core_change": "-def __check_numel_1(value: torch.Tensor, name: str) -> None: +def __check_numel_1(value: Tensor, name: str) -> None:",
        "core_API": "tensor"
    },
    {
        "commit_hash": "70eedbd35148a4045ffc9caf81b7cf4acf32c48a",
        "index": "e62b9904..c1782589 100644",
        "commit_message": "Refactor category orders in heads (#2374)\n\n* Refactor (all): all category -1 in anchor/bbox head and anchor/bbox target assign\n\n* Fix (datasets): remove label + 1 in datasets\n\n* Fix (bbox_head): fix bug of fc_cls that forget + 1\n\n* Fix (atss_head & free_anchor): fix cat -1 bugs\n\n* Fix (mask_head): remove label + 1 in mask heads\n\n* fix atss\n\n* Fix (rpn): fix cross_entropy_loss bug of RPN\n\n* Fix (anchor_head): fix typo\n\n* Refactor (anchor_head): use background_label rather than num_classes to indicate background class\n\n* Refactor (docstring): add and reformat docstrings\n\n* fix mask iou head\n\n* Fix (mask_head): fix cat -1 bug\n\n* Fix (mask_head): fix bug in mask inference\n\n* Add (tests): add tests for mask rcnn and mask heads\n\n* Refactor (unittest): refactor test_forward\n\n* Refactor (new_empty): use new_full rather than new_empty\n\n* Refactor (background_label): check background_label\n\n* Add TODO\n\n* Refactor (unittest): allow BP in unittest\n\nCo-authored-by: Cao Yuhang <yhcao6@gmail.com>\n",
        "file": "mmdetection.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class GHMC(nn.Module):",
            "\"\"\"",
            "# the target should be binary class label",
            "if pred.dim() != target.dim():",
            "-            target, label_weight = _expand_binary_labels(",
            "+            target, label_weight = _expand_onehot_labels(",
            "target, label_weight, pred.size(-1))",
            "target, label_weight = target.float(), label_weight.float()",
            "edges = self.edges"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=_expand_binary_labels), value='_expand_onehot_labels')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 7930,
        "neg_line": [
            "-target, label_weight = _expand_binary_labels("
        ],
        "pos_line": [
            "+target, label_weight = _expand_onehot_labels("
        ],
        "core_change": "-target, label_weight = _expand_binary_labels( +target, label_weight = _expand_onehot_labels(",
        "core_API": "dim"
    },
    {
        "commit_hash": "ce8f42b12ecd522e3ef7251e0d5f5175075a3fb4",
        "index": "e23f909f..56b97a75 100644",
        "commit_message": "Fixes to enable dev branch to move to pytorch-1.2 (#2001)\n\n* Fixes to enable dev branch to move to pytorch-1.2\n\n* change name for published cpu wheels\n\n* fix failing unit tests\n\n* change docs conf\n\n* change tracking example\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Rejector(TorchDistribution):",
            "x = self.propose(sample_shape) if sample_shape else self.propose()",
            "log_prob_accept = self.log_prob_accept(x)",
            "probs = torch.exp(log_prob_accept).clamp_(0.0, 1.0)",
            "-        done = torch.bernoulli(probs).byte()",
            "+        done = torch.bernoulli(probs).bool()",
            "while not done.all():",
            "proposed_x = self.propose(sample_shape) if sample_shape else self.propose()",
            "log_prob_accept = self.log_prob_accept(proposed_x)",
            "prob_accept = torch.exp(log_prob_accept).clamp_(0.0, 1.0)",
            "-            accept = torch.bernoulli(prob_accept).byte() & ~done",
            "+            accept = torch.bernoulli(prob_accept).bool() & ~done",
            "if accept.any():",
            "x[accept] = proposed_x[accept]",
            "done |= accept"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=717531)",
            "Update(target_node=ASTNode(type=identifier, text=byte), value='bool')",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=717532)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=717533)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=argument_list), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=byte), value='bool')",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 9,
        "number": 7935,
        "neg_line": [
            "-done = torch.bernoulli(probs).byte()",
            "-accept = torch.bernoulli(prob_accept).byte() & ~done"
        ],
        "pos_line": [
            "+done = torch.bernoulli(probs).bool()",
            "+accept = torch.bernoulli(prob_accept).bool() & ~done"
        ],
        "core_change": "-done = torch.bernoulli(probs).byte() +done = torch.bernoulli(probs).bool() -accept = torch.bernoulli(prob_accept).byte() & ~done +accept = torch.bernoulli(prob_accept).bool() & ~done",
        "core_API": "propose"
    },
    {
        "commit_hash": "d3220793528f9fdaaa7440f4694a6a23c2d7c14c",
        "index": "cbeb03476..2435d1ce9 100644",
        "commit_message": "fix bugs around optimizer\n\n",
        "file": "PySyft.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class Plan(Serializable):",
            "# prevent circular dependency",
            "# syft relative",
            "from ...core.node.vm.vm import VirtualMachine  # noqa: F401",
            "-        if self.local_executor is not None:",
            "-            # this is necessary for syfts nn.module, because the plan contains state from the module",
            "-            # in order to use this state, we first need to send the model, and then execute te plan",
            "-            return self.local_executor(**kwargs)",
            "",
            "alice = VirtualMachine(name=\"plan_executor\")",
            "alice_client: client.Client = alice.get_client()"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=if, text=if))",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=local_executor))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=is not, text=is))",
            "Delete(target_node=ASTNode(type=is not, text=not))",
            "Delete(target_node=ASTNode(type=none, text=None))",
            "Delete(target_node=ASTNode(type=comparison_operator))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=return, text=return))",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=local_executor))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=**, text=**))",
            "Delete(target_node=ASTNode(type=identifier, text=kwargs))",
            "Delete(target_node=ASTNode(type=dictionary_splat))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=return_statement))",
            "Delete(target_node=ASTNode(type=block))",
            "Delete(target_node=ASTNode(type=if_statement))"
        ],
        "plus_line": 0,
        "minus_line": 4,
        "AST_diff_line": 25,
        "number": 7936,
        "neg_line": [
            "-if self.local_executor is not None:",
            "-# this is necessary for syfts nn.module, because the plan contains state from the module",
            "-# in order to use this state, we first need to send the model, and then execute te plan",
            "-return self.local_executor(**kwargs)"
        ],
        "pos_line": [],
        "core_change": "-if self.local_executor is not None: -# this is necessary for syfts nn.module, because the plan contains state from the module -# in order to use this state, we first need to send the model, and then execute te plan -return self.local_executor(**kwargs)",
        "core_API": "local_executor"
    },
    {
        "commit_hash": "77795847d501af68a8e738dcffedf0f3c558e635",
        "index": "406a84d..a0de55f 100644",
        "commit_message": "Moved Keras implementation into private module (#650)\n\n* Moved Keras implementation into private module to avoid importing standalone keras when using tf.keras\n\n* Fixed unit tests\n\n",
        "file": "horovod.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "keras",
        "change": [
            "class KerasTests(tf.test.TestCase):",
            "new_opt = new_model.optimizer",
            "os.remove(fname)",
            "",
            "-            self.assertEqual(type(new_opt).__module__, 'horovod.keras.impl')",
            "+            self.assertEqual(type(new_opt).__module__, 'horovod._keras')",
            "self.assertEqual(type(new_opt).__name__, 'TestOptimizer')",
            "self.assertEqual(K.get_value(opt.lr), K.get_value(new_opt.lr))",
            "self.assertEqual(len(opt.get_weights()), len(new_opt.get_weights()))"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text='horovod.keras.impl'), value=\"'horovod._keras'\")"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 7938,
        "neg_line": [
            "-self.assertEqual(type(new_opt).__module__, 'horovod.keras.impl')"
        ],
        "pos_line": [
            "+self.assertEqual(type(new_opt).__module__, 'horovod._keras')"
        ],
        "core_change": "-self.assertEqual(type(new_opt).__module__, 'horovod.keras.impl') +self.assertEqual(type(new_opt).__module__, 'horovod._keras')",
        "core_API": "remove"
    },
    {
        "commit_hash": "ae40f198519f343ffd899c286c1f4e0f15c9e5f5",
        "index": "03685c37d8..fa6f3c7efb 100644",
        "commit_message": "fix the unravel_index to makesure it retures tuple of arrays\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def unravel_index(",
            "/,",
            "*,",
            "out: Optional[torch.Tensor] = None,",
            "-) -> torch.Tensor:",
            "-    temp = indices.detach()",
            "+) -> Tuple:",
            "+    temp = indices.to(torch.int64)",
            "output = []",
            "for dim in reversed(shape):",
            "output.append(temp % dim)",
            "temp = temp // dim",
            "-    return torch.tensor(reversed(output), dtype=torch.int64)",
            "+    return tuple(reversed(output))",
            "",
            "",
            "unravel_index.support_native_out = False"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=torch), value='Tuple')",
            "Move(target_node=ASTNode(type=type), node=ASTNode(type=identifier, text=torch), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=torch), value='tuple')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=torch), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=detach), value='to')",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=attribute), position=1)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=Tensor))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=tensor))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=dtype))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 16,
        "number": 7940,
        "neg_line": [
            "-) -> torch.Tensor:",
            "-temp = indices.detach()",
            "-return torch.tensor(reversed(output), dtype=torch.int64)"
        ],
        "pos_line": [
            "+) -> Tuple:",
            "+temp = indices.to(torch.int64)",
            "+return tuple(reversed(output))"
        ],
        "core_change": "-) -> torch.Tensor: -temp = indices.detach() +) -> Tuple: +temp = indices.to(torch.int64) -return torch.tensor(reversed(output), dtype=torch.int64) +return tuple(reversed(output))",
        "core_API": "detach"
    },
    {
        "commit_hash": "b34be039f9e5e0ca48166f14b849eac089949c68",
        "index": "3ca0fd91..07767189 100644",
        "commit_message": "Karras VE, DDIM and DDPM flax schedulers (#508)\n\n* beta never changes removed from state\n\n* fix typos in docs\n\n* removed unused var\n\n* initial ddim flax scheduler\n\n* import\n\n* added dummy objects\n\n* fix style\n\n* fix typo\n\n* docs\n\n* fix typo in comment\n\n* set return type\n\n* added flax ddom\n\n* fix style\n\n* remake\n\n* pass PRNG key as argument and split before use\n\n* fix doc string\n\n* use config\n\n* added flax Karras VE scheduler\n\n* make style\n\n* fix dummy\n\n* fix ndarray type annotation\n\n* replace returns a new state\n\n* added lms_discrete scheduler\n\n* use self.config\n\n* add_noise needs state\n\n* use config\n\n* use config\n\n* docstring\n\n* added flax score sde ve\n\n* fix imports\n\n* fix typos\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def main():",
            "noise = torch.randn(latents.shape).to(latents.device)",
            "bsz = latents.shape[0]",
            "# Sample a random timestep for each image",
            "-                timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (bsz,), device=latents.device).long()",
            "+                timesteps = torch.randint(",
            "+                    0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device",
            "+                ).long()",
            "",
            "# Add noise to the latents according to the noise magnitude at each timestep",
            "# (this is the forward diffusion process)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=104584)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=noise_scheduler), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=104585)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'config'), position=2, insert_id=104586)"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 7941,
        "neg_line": [
            "-timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (bsz,), device=latents.device).long()"
        ],
        "pos_line": [
            "+timesteps = torch.randint(",
            "+0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device",
            "+).long()"
        ],
        "core_change": "-timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (bsz,), device=latents.device).long() +timesteps = torch.randint( +0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device +).long()",
        "core_API": "randn"
    },
    {
        "commit_hash": "6c4baf6a5551cb6e3d2ca07e70743a758b374117",
        "index": "e847105c..a7c72fba 100644",
        "commit_message": "new unique fixes\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def consecutive_cluster(src, batch=None):",
            "output = arg[src.view(-1)]",
            "output = output.view(size).long()",
            "",
            "-    return (output, None) if batch is None else (output, batch[value])",
            "+    return (output, None) if batch is None else (output, batch[perm])"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=value), value='perm')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 7942,
        "neg_line": [
            "-return (output, None) if batch is None else (output, batch[value])"
        ],
        "pos_line": [
            "+return (output, None) if batch is None else (output, batch[perm])"
        ],
        "core_change": "-return (output, None) if batch is None else (output, batch[value]) +return (output, None) if batch is None else (output, batch[perm])",
        "core_API": "view"
    },
    {
        "commit_hash": "c766743b9436d04c8d5acc38f3915ab90809f6ba",
        "index": "63f2b422..66201365 100644",
        "commit_message": "Minor typo fix\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def cross_entropy_seq(logits, target_seqs, batch_size=None):#, batch_size=1, num",
            ">>> see PTB tutorial for more details",
            ">>> input_data = tf.placeholder(tf.int32, [batch_size, num_steps])",
            ">>> targets = tf.placeholder(tf.int32, [batch_size, num_steps])",
            "-    >>> cost = tf.cost.cross_entropy_seq(network.outputs, targets)",
            "+    >>> cost = tl.cost.cross_entropy_seq(network.outputs, targets)",
            "\"\"\"",
            "try: # TF 1.0",
            "sequence_loss_by_example_fn = tf.contrib.legacy_seq2seq.sequence_loss_by_example"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tf), value='tl')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 7944,
        "neg_line": [
            "->>> cost = tf.cost.cross_entropy_seq(network.outputs, targets)"
        ],
        "pos_line": [
            "+>>> cost = tl.cost.cross_entropy_seq(network.outputs, targets)"
        ],
        "core_change": "->>> cost = tf.cost.cross_entropy_seq(network.outputs, targets) +>>> cost = tl.cost.cross_entropy_seq(network.outputs, targets)",
        "core_API": "placeholder"
    },
    {
        "commit_hash": "34b7db020935a0eb1cc9e984540ab735b88a5f22",
        "index": "90ab4fe4..47244316 100644",
        "commit_message": "chore: enable `singleton-comparison` and cleanup (#3849)\n\n* enable singleton-comparison\n\n* fix triadaptive_model bug\n",
        "file": "haystack.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TriAdaptiveModel(nn.Module):",
            "",
            "# Forward pass for text passages and tables",
            "if \"passage_input_ids\" in kwargs.keys():",
            "-            table_mask = torch.flatten(kwargs[\"is_table\"]) == True",
            "+            table_mask = torch.flatten(kwargs[\"is_table\"]) == 1",
            "",
            "# Current batch consists of only tables",
            "if all(table_mask):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=comparison_operator), node=('integer', '1'), position=2, insert_id=245100)",
            "Delete(target_node=ASTNode(type=true, text=True))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 7946,
        "neg_line": [
            "-table_mask = torch.flatten(kwargs[\"is_table\"]) == True"
        ],
        "pos_line": [
            "+table_mask = torch.flatten(kwargs[\"is_table\"]) == 1"
        ],
        "core_change": "-table_mask = torch.flatten(kwargs[\"is_table\"]) == True +table_mask = torch.flatten(kwargs[\"is_table\"]) == 1",
        "core_API": "keys"
    },
    {
        "commit_hash": "3fd97a0b2f89ffd4ed6ef1af75c399147557987e",
        "index": "26bbff9..860f2ca 100644",
        "commit_message": "Fix SGD when using tf.function.\n\nscatter_nd_sub behaves the same as scatter_sub for IndexedSlices in Eager mode but not in Graph mode. We can just use scatter_sub though.\n\nPiperOrigin-RevId: 256568297\nChange-Id: Ie8267e0f09d26cf92e8256f821423f170e63493a\n\n",
        "file": "sonnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class SGD(base.Module):",
            "optimizer_utils.check_same_dtype(update, parameter)",
            "learning_rate = tf.cast(self.learning_rate, update.dtype.base_dtype)",
            "if isinstance(update, tf.IndexedSlices):",
            "-          parameter.scatter_nd_sub(",
            "-              update.indices, update.values * learning_rate)",
            "+          parameter.scatter_sub(",
            "+              tf.IndexedSlices(update.values * learning_rate, update.indices))",
            "else:",
            "parameter.assign_sub(update * learning_rate)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=2183196)",
            "Update(target_node=ASTNode(type=identifier, text=scatter_nd_sub), value='scatter_sub')",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2183197)",
            "Insert(target_node=IN(type=argument_list), node=('call', None), position=1, insert_id=2183198)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2183199)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2183200)",
            "Move(target_node=IN(type=call), node=ASTNode(type=argument_list), position=1)",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=argument_list), position=2)",
            "Move(target_node=ASTNode(type=binary_operator), node=ASTNode(type=argument_list), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2183201)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2183202)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'IndexedSlices'), position=2, insert_id=2183203)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 12,
        "number": 7948,
        "neg_line": [
            "-parameter.scatter_nd_sub(",
            "-update.indices, update.values * learning_rate)"
        ],
        "pos_line": [
            "+parameter.scatter_sub(",
            "+tf.IndexedSlices(update.values * learning_rate, update.indices))"
        ],
        "core_change": "-parameter.scatter_nd_sub( -update.indices, update.values * learning_rate) +parameter.scatter_sub( +tf.IndexedSlices(update.values * learning_rate, update.indices))",
        "core_API": "check_same_dtype"
    },
    {
        "commit_hash": "300bda9e1a0a37fd9cb9d6d3dede9e5fb22ca153",
        "index": "a0fca99..af2bb8f 100644",
        "commit_message": "Fix ELBO term scaling\n\n",
        "file": "generative-models.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "for it in range(1000000):",
            "T_sample = T(torch.cat([X, z_sample], 1))",
            "",
            "disc = torch.mean(-T_sample)",
            "-    loglike = -nn.binary_cross_entropy(X_sample, X)",
            "+    loglike = -nn.binary_cross_entropy(X_sample, X, size_average=False) / mb_size",
            "",
            "elbo = -(disc + loglike)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('binary_operator', None), position=2, insert_id=1363877)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=unary_operator), position=0)",
            "Insert(target_node=IN(type=binary_operator), node=('/', '/'), position=1, insert_id=1363878)",
            "Insert(target_node=IN(type=binary_operator), node=('identifier', 'mb_size'), position=2, insert_id=1363879)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=1363880)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=1363881)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'size_average'), position=0, insert_id=1363882)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1363883)",
            "Insert(target_node=IN(type=keyword_argument), node=('false', 'False'), position=2, insert_id=1363884)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 7954,
        "neg_line": [
            "-loglike = -nn.binary_cross_entropy(X_sample, X)"
        ],
        "pos_line": [
            "+loglike = -nn.binary_cross_entropy(X_sample, X, size_average=False) / mb_size"
        ],
        "core_change": "-loglike = -nn.binary_cross_entropy(X_sample, X) +loglike = -nn.binary_cross_entropy(X_sample, X, size_average=False) / mb_size",
        "core_API": "cat"
    },
    {
        "commit_hash": "9cc9c26de9b9afbeb91d6aa4cb654ecf8453da93",
        "index": "0c24847481..568e91c567 100644",
        "commit_message": "lintfixbot: Auto-commit fixed lint errors in codebase\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def diff(",
            "x: Union[tf.Tensor, tf.Variable, int, float, list, tuple],",
            "/,",
            "*,",
            "-    out: Optional[Union[tf.Tensor, tf.Variable]] = None",
            "+    out: Optional[Union[tf.Tensor, tf.Variable]] = None,",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "return tf.experimental.numpy.diff(x)",
            "",
            "-",
            "+",
            "@with_unsupported_dtypes({\"2.9.1 and below\": (\"bfloat16, float16,\")}, backend_version)",
            "def zeta(",
            "x: Union[tf.Tensor, tf.Variable],"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=8, insert_id=1973042)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 7958,
        "neg_line": [
            "-out: Optional[Union[tf.Tensor, tf.Variable]] = None",
            "-"
        ],
        "pos_line": [
            "+out: Optional[Union[tf.Tensor, tf.Variable]] = None,",
            "+"
        ],
        "core_change": "-out: Optional[Union[tf.Tensor, tf.Variable]] = None +out: Optional[Union[tf.Tensor, tf.Variable]] = None, - +",
        "core_API": "diff"
    },
    {
        "commit_hash": "6829244c10f519206662bd4f373dd8e064f19a7d",
        "index": "b933f279..bd464a96 100644",
        "commit_message": "modelnet windows fix\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SamplePoints(object):",
            "pos = pos / pos_max",
            "",
            "area = (pos[face[1]] - pos[face[0]]).cross(pos[face[2]] - pos[face[0]])",
            "-        area = torch.sqrt((area**2).sum(dim=-1)) / 2",
            "+        area = area.norm(p=2, dim=1) / 2",
            "",
            "prob = area / area.sum()",
            "sample = torch.multinomial(prob, self.num, replacement=True)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=area), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=sqrt), value='norm')",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=1, insert_id=1055181)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1055182)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=keyword_argument), position=3)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=), text=)), position=4)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'p'), position=0, insert_id=1055183)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1055184)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=integer, text=2), position=2)",
            "Insert(target_node=ASTNode(type=keyword_argument), node=('integer', '1'), position=2, insert_id=1055185)",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=**, text=**))",
            "Delete(target_node=ASTNode(type=binary_operator))",
            "Delete(target_node=ASTNode(type=parenthesized_expression))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=sum))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=unary_operator, text=-1))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=), text=)))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 24,
        "number": 7961,
        "neg_line": [
            "-area = torch.sqrt((area**2).sum(dim=-1)) / 2"
        ],
        "pos_line": [
            "+area = area.norm(p=2, dim=1) / 2"
        ],
        "core_change": "-area = torch.sqrt((area**2).sum(dim=-1)) / 2 +area = area.norm(p=2, dim=1) / 2",
        "core_API": "sqrt"
    },
    {
        "commit_hash": "cbedf214b07009d69626517e9953aa6d8337ab9e",
        "index": "0317671d..74b12e6f 100644",
        "commit_message": "Parameter Metadata Updates (#2736)\n\n* Added split param metadata and changed sample ratio to medium impact\n\n* Schema updates\n\n* Adjusted Expected Impacts\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "ludwig.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class ECDTrainerConfig(BaseTrainerConfig):",
            "",
            "bucketing_field: str = schema_utils.String(",
            "default=None,",
            "-        description=\"When not null, when creating batches, instead of shuffling randomly, the length along the last \"",
            "-        \"dimension of the matrix of the specified input feature is used for bucketing examples and then \"",
            "-        \"randomly shuffled examples from the same bin are sampled. Padding is trimmed to the longest \"",
            "-        \"example in the batch. The specified feature should be either a sequence or text feature and the \"",
            "-        \"encoder encoding it has to be rnn. When used, bucketing improves speed of rnn encoding up to \"",
            "-        \"1.5x, depending on the length distribution of the inputs.\",",
            "+        description=\"Feature to use for bucketing datapoints\",",
            "parameter_metadata=TRAINER_METADATA[\"bucketing_field\"],",
            ")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"When not null, when creating batches, instead of shuffling randomly, the length along the last \"), value='\"Feature to use for bucketing datapoints\"')",
            "Move(target_node=ASTNode(type=keyword_argument), node=ASTNode(type=string, text=\"When not null, when creating batches, instead of shuffling randomly, the length along the last \"), position=2)",
            "Delete(target_node=ASTNode(type=string, text=\"dimension of the matrix of the specified input feature is used for bucketing examples and then \"))",
            "Delete(target_node=ASTNode(type=string, text=\"randomly shuffled examples from the same bin are sampled. Padding is trimmed to the longest \"))",
            "Delete(target_node=ASTNode(type=string, text=\"example in the batch. The specified feature should be either a sequence or text feature and the \"))",
            "Delete(target_node=ASTNode(type=string, text=\"encoder encoding it has to be rnn. When used, bucketing improves speed of rnn encoding up to \"))",
            "Delete(target_node=ASTNode(type=string, text=\"1.5x, depending on the length distribution of the inputs.\"))",
            "Delete(target_node=ASTNode(type=concatenated_string))"
        ],
        "plus_line": 1,
        "minus_line": 6,
        "AST_diff_line": 8,
        "number": 7962,
        "neg_line": [
            "-description=\"When not null, when creating batches, instead of shuffling randomly, the length along the last \"",
            "-\"dimension of the matrix of the specified input feature is used for bucketing examples and then \"",
            "-\"randomly shuffled examples from the same bin are sampled. Padding is trimmed to the longest \"",
            "-\"example in the batch. The specified feature should be either a sequence or text feature and the \"",
            "-\"encoder encoding it has to be rnn. When used, bucketing improves speed of rnn encoding up to \"",
            "-\"1.5x, depending on the length distribution of the inputs.\","
        ],
        "pos_line": [
            "+description=\"Feature to use for bucketing datapoints\","
        ],
        "core_change": "-description=\"When not null, when creating batches, instead of shuffling randomly, the length along the last \" -\"dimension of the matrix of the specified input feature is used for bucketing examples and then \" -\"randomly shuffled examples from the same bin are sampled. Padding is trimmed to the longest \" -\"example in the batch. The specified feature should be either a sequence or text feature and the \" -\"encoder encoding it has to be rnn. When used, bucketing improves speed of rnn encoding up to \" -\"1.5x, depending on the length distribution of the inputs.\", +description=\"Feature to use for bucketing datapoints\",",
        "core_API": "String"
    },
    {
        "commit_hash": "a3e55be685fb52ac8c7caafd6835f521bcbcff9d",
        "index": "6bc787a9..a85f0893 100755",
        "commit_message": "fixed and improved shape handling for exploration\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class OrnsteinUhlenbeckProcess(Exploration):",
            "",
            "super(OrnsteinUhlenbeckProcess, self).__init__(scope=scope, summary_labels=summary_labels)",
            "",
            "-    def tf_explore(self, episode, timestep, action_spec):",
            "-        normal_sample = tf.random_normal(shape=action_spec['shape'], mean=0.0, stddev=1.0)",
            "+    def tf_explore(self, episode, timestep, shape):",
            "+        normal_sample = tf.random_normal(shape=shape, mean=0.0, stddev=1.0)",
            "state = tf.get_variable(",
            "name='ornstein_uhlenbeck',",
            "dtype=util.tf_dtype('float'),",
            "-            shape=action_spec['shape'],",
            "-            initializer=tf.constant_initializer(self.mu)",
            "+            shape=shape,",
            "+            initializer=tf.constant_initializer(self.mu),",
            "+            trainable=False",
            ")",
            "return tf.assign_add(ref=state, value=(self.theta * (self.mu - state) + self.sigma * normal_sample))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=action_spec), value='shape')",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=8, insert_id=2234409)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=9, insert_id=2234410)",
            "Update(target_node=ASTNode(type=identifier, text=action_spec), value='shape')",
            "Move(target_node=ASTNode(type=keyword_argument), node=ASTNode(type=identifier, text=action_spec), position=2)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'trainable'), position=0, insert_id=2234411)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2234412)",
            "Insert(target_node=IN(type=keyword_argument), node=('false', 'False'), position=2, insert_id=2234413)",
            "Update(target_node=ASTNode(type=identifier, text=action_spec), value='shape')",
            "Move(target_node=ASTNode(type=keyword_argument), node=ASTNode(type=identifier, text=action_spec), position=2)",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=string, text='shape'))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=string, text='shape'))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))"
        ],
        "plus_line": 5,
        "minus_line": 4,
        "AST_diff_line": 18,
        "number": 7963,
        "neg_line": [
            "-def tf_explore(self, episode, timestep, action_spec):",
            "-normal_sample = tf.random_normal(shape=action_spec['shape'], mean=0.0, stddev=1.0)",
            "-shape=action_spec['shape'],",
            "-initializer=tf.constant_initializer(self.mu)"
        ],
        "pos_line": [
            "+def tf_explore(self, episode, timestep, shape):",
            "+normal_sample = tf.random_normal(shape=shape, mean=0.0, stddev=1.0)",
            "+shape=shape,",
            "+initializer=tf.constant_initializer(self.mu),",
            "+trainable=False"
        ],
        "core_change": "-def tf_explore(self, episode, timestep, action_spec): -normal_sample = tf.random_normal(shape=action_spec['shape'], mean=0.0, stddev=1.0) +def tf_explore(self, episode, timestep, shape): +normal_sample = tf.random_normal(shape=shape, mean=0.0, stddev=1.0) -shape=action_spec['shape'], -initializer=tf.constant_initializer(self.mu) +shape=shape, +initializer=tf.constant_initializer(self.mu), +trainable=False",
        "core_API": "random_normal"
    },
    {
        "commit_hash": "4dc10b78a551642f86c2bcc3a78ab61e2aa66180",
        "index": "a6b312e31a..f7f6f22a2b 100644",
        "commit_message": "lint fixes\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def linear_resample(x, num_samples: int, axis: int = -1):",
            "else:",
            "x_pre_shape = x_shape[:-1]",
            "x = torch.reshape(x, ([-1, 1] + [num_vals]))",
            "-    if x.dtype not in ['float16','float32','float64']:",
            "-        x=x.type(torch.float32)",
            "+    if x.dtype not in [\"float16\", \"float32\", \"float64\"]:",
            "+        x = x.type(torch.float32)",
            "ret = torch.nn.functional.interpolate(",
            "x, num_samples, mode=\"linear\", align_corners=True",
            ")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text='float16'), value='\"float16\"')",
            "Update(target_node=ASTNode(type=string, text='float32'), value='\"float32\"')",
            "Update(target_node=ASTNode(type=string, text='float64'), value='\"float64\"')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 3,
        "number": 7965,
        "neg_line": [
            "-if x.dtype not in ['float16','float32','float64']:",
            "-x=x.type(torch.float32)"
        ],
        "pos_line": [
            "+if x.dtype not in [\"float16\", \"float32\", \"float64\"]:",
            "+x = x.type(torch.float32)"
        ],
        "core_change": "-if x.dtype not in ['float16','float32','float64']: -x=x.type(torch.float32) +if x.dtype not in [\"float16\", \"float32\", \"float64\"]: +x = x.type(torch.float32)",
        "core_API": "reshape"
    },
    {
        "commit_hash": "0ba7472da92a89af9aca84de5b01a228d18340a2",
        "index": "51eb11fd8..89f40d039 100644",
        "commit_message": "[Testing] Fix LINT/sphinx errors. (#8874)\n\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class NestedSpacesTest(unittest.TestCase):",
            "ModelCatalog.register_custom_model(\"invalid\", InvalidModel)",
            "self.assertRaisesRegexp(",
            "ValueError,",
            "-            \"optimizer got an empty parameter list\",",
            "+            \"Subclasses of TorchModelV2 must also inherit from nn.Module\",",
            "lambda: PGTrainer(",
            "env=\"CartPole-v0\",",
            "config={"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"optimizer got an empty parameter list\"), value='\"Subclasses of TorchModelV2 must also inherit from nn.Module\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 7967,
        "neg_line": [
            "-\"optimizer got an empty parameter list\","
        ],
        "pos_line": [
            "+\"Subclasses of TorchModelV2 must also inherit from nn.Module\","
        ],
        "core_change": "-\"optimizer got an empty parameter list\", +\"Subclasses of TorchModelV2 must also inherit from nn.Module\",",
        "core_API": "register_custom_model"
    },
    {
        "commit_hash": "b562b6611fb53dae9bcffcaaf44d944539ae22de",
        "index": "1fb4521d..f24f3547 100644",
        "commit_message": "Allow directly passing text embeddings to Stable Diffusion Pipeline for prompt weighting (#2071)\n\n* add text embeds to sd\n\n* add text embeds to sd\n\n* finish tests\n\n* finish\n\n* finish\n\n* make style\n\n* fix tests\n\n* make style\n\n* make style\n\n* up\n\n* better docs\n\n* fix\n\n* fix\n\n* new try\n\n* up\n\n* up\n\n* finish\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class PaintByExamplePipeline(DiffusionPipeline):",
            "image_embeddings = image_embeddings.view(bs_embed * num_images_per_prompt, seq_len, -1)",
            "",
            "if do_classifier_free_guidance:",
            "-            uncond_embeddings = uncond_embeddings.repeat(1, image_embeddings.shape[0], 1)",
            "-            uncond_embeddings = uncond_embeddings.view(bs_embed * num_images_per_prompt, 1, -1)",
            "+            negative_prompt_embeds = negative_prompt_embeds.repeat(1, image_embeddings.shape[0], 1)",
            "+            negative_prompt_embeds = negative_prompt_embeds.view(bs_embed * num_images_per_prompt, 1, -1)",
            "",
            "# For classifier free guidance, we need to do two forward passes.",
            "# Here we concatenate the unconditional and text embeddings into a single batch",
            "# to avoid doing two forward passes",
            "-            image_embeddings = torch.cat([uncond_embeddings, image_embeddings])",
            "+            image_embeddings = torch.cat([negative_prompt_embeds, image_embeddings])",
            "",
            "return image_embeddings"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=uncond_embeddings), value='negative_prompt_embeds')",
            "Update(target_node=ASTNode(type=identifier, text=uncond_embeddings), value='negative_prompt_embeds')",
            "Update(target_node=ASTNode(type=identifier, text=uncond_embeddings), value='negative_prompt_embeds')",
            "Update(target_node=ASTNode(type=identifier, text=uncond_embeddings), value='negative_prompt_embeds')",
            "Update(target_node=ASTNode(type=identifier, text=uncond_embeddings), value='negative_prompt_embeds')"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 5,
        "number": 7968,
        "neg_line": [
            "-uncond_embeddings = uncond_embeddings.repeat(1, image_embeddings.shape[0], 1)",
            "-uncond_embeddings = uncond_embeddings.view(bs_embed * num_images_per_prompt, 1, -1)",
            "-image_embeddings = torch.cat([uncond_embeddings, image_embeddings])"
        ],
        "pos_line": [
            "+negative_prompt_embeds = negative_prompt_embeds.repeat(1, image_embeddings.shape[0], 1)",
            "+negative_prompt_embeds = negative_prompt_embeds.view(bs_embed * num_images_per_prompt, 1, -1)",
            "+image_embeddings = torch.cat([negative_prompt_embeds, image_embeddings])"
        ],
        "core_change": "-uncond_embeddings = uncond_embeddings.repeat(1, image_embeddings.shape[0], 1) -uncond_embeddings = uncond_embeddings.view(bs_embed * num_images_per_prompt, 1, -1) +negative_prompt_embeds = negative_prompt_embeds.repeat(1, image_embeddings.shape[0], 1) +negative_prompt_embeds = negative_prompt_embeds.view(bs_embed * num_images_per_prompt, 1, -1) -image_embeddings = torch.cat([uncond_embeddings, image_embeddings]) +image_embeddings = torch.cat([negative_prompt_embeds, image_embeddings])",
        "core_API": "view"
    },
    {
        "commit_hash": "c05ace221339102abb1c8b0933b9cbfd51ddd4a3",
        "index": "0964bb65b..e32574f55 100644",
        "commit_message": "Use f-strings in the dataset scripts (#3291)\n\n* Finishes #3257\n\nUsed f-strings to format the .py files in the dataset folder\n\n* Fix style\n\n* Fix hkcancor dataset\n\nCo-authored-by: Mario Šaško <mario@huggingface.co>\nCo-authored-by: mariosasko <mariosasko777@gmail.com>\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class NarrativeqaManual(datasets.GeneratorBasedBuilder):",
            "",
            "if not os.path.exists(manual_dir):",
            "raise FileNotFoundError(",
            "-                \"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('narrativeqa_manual', data_dir=...)` that includes the stories downloaded from the original repository. Manual download instructions: {}\".format(",
            "-                    manual_dir, self.manual_download_instructions",
            "-                )",
            "+                f\"{manual_dir} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('narrativeqa_manual', data_dir=...)` that includes the stories downloaded from the original repository. Manual download instructions: {self.manual_download_instructions}\"",
            ")",
            "",
            "return ["
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('string', 'f\"{manual_dir} does not exist. Make sure you insert a manual dir via `datasets.load_dataset(\\'narrativeqa_manual\\', data_dir=...)` that includes the stories downloaded from the original repository. Manual download instructions: {self.manual_download_instructions}\"'), position=1, insert_id=1781628)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=), text=)), position=2)",
            "Delete(target_node=ASTNode(type=string, text=\"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('narrativeqa_manual', data_dir=...)` that includes the stories downloaded from the original repository. Manual download instructions: {}\"))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=format))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=manual_dir))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=manual_download_instructions))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=), text=)))"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 16,
        "number": 7975,
        "neg_line": [
            "-\"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('narrativeqa_manual', data_dir=...)` that includes the stories downloaded from the original repository. Manual download instructions: {}\".format(",
            "-manual_dir, self.manual_download_instructions",
            "-)"
        ],
        "pos_line": [
            "+f\"{manual_dir} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('narrativeqa_manual', data_dir=...)` that includes the stories downloaded from the original repository. Manual download instructions: {self.manual_download_instructions}\""
        ],
        "core_change": "-\"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('narrativeqa_manual', data_dir=...)` that includes the stories downloaded from the original repository. Manual download instructions: {}\".format( -manual_dir, self.manual_download_instructions -) +f\"{manual_dir} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('narrativeqa_manual', data_dir=...)` that includes the stories downloaded from the original repository. Manual download instructions: {self.manual_download_instructions}\"",
        "core_API": "exists"
    },
    {
        "commit_hash": "d5218f88275fd57825819c6dab523e30a41b6866",
        "index": "47a20296..afc15000 100644",
        "commit_message": "update sequence generator device change (#2989)\n\nSummary:\n# Before submitting\n\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\n- [ ] Did you make sure to update the docs?\n- [ ] Did you write any new necessary tests?\n\n## What does this PR do?\nFixes Sequence Generator\n\nReproduce the bug:\n1. Background:\n**fairseq.utils.move_to_cuda** function support to change device.\n2. When you change the file **fairseq_cli/generate.py**\nsuppose `gpu_id = 1`\n- line 134 change from\n`model.cuda()`\nto\n`model.cuda(gpu_id)`\n- line 189 change from\n`sample = utils.move_to_cuda(sample) if use_cuda else sample`\nto\n`sample = utils.move_to_cuda(sample, gpu_id) if use_cuda else sample`\n2. you will get an error\n```\nfairseq/sequence_generator.py, line 382, in generate\n    cand_bbsz_idx = cand_beams.add(bbsz_offsets)\nRuntimeError: binary_op(): expected both inputs to be on same device, but input a is on cuda:1 and input b is on cuda:0\n\n```\n\nThe reason of this bug is that `bbsz_offsets` is not assigned to proper cuda device. Therefore, we need to change the line 281 and line 281 of the file **fairseq/sequence_generator.py**\n\nThe test file for this fix is the file **tests/test_sequence_generator.py**\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding �\nvery happy {emoji:1f642}\n\nPull Request resolved: https://github.com/pytorch/fairseq/pull/2989\n\nReviewed By: alexeib\n\nDifferential Revision: D25344265\n\nPulled By: myleott\n\nfbshipit-source-id: 8cb8b389e59a9aa67aec84dbdadcfa2c08c9648f\n\n",
        "file": "fairseq.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SequenceGenerator(nn.Module):",
            "cand_size = 2 * beam_size  # 2 x beam size in case half are EOS",
            "",
            "# offset arrays for converting between different indexing schemes",
            "-        bbsz_offsets = (torch.arange(0, bsz) * beam_size).unsqueeze(1).type_as(tokens)",
            "-        cand_offsets = torch.arange(0, cand_size).type_as(tokens)",
            "+        bbsz_offsets = (torch.arange(0, bsz) * beam_size).unsqueeze(1).type_as(tokens).to(src_tokens.device)",
            "+        cand_offsets = torch.arange(0, cand_size).type_as(tokens).to(src_tokens.device)",
            "",
            "reorder_state: Optional[Tensor] = None",
            "batch_idxs: Optional[Tensor] = None"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('call', None), position=2, insert_id=209647)",
            "Insert(target_node=ASTNode(type=assignment), node=('call', None), position=2, insert_id=209648)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=209649)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=209650)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=209651)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=209652)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=209653)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to'), position=2, insert_id=209654)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=209655)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=209656)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=209657)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=209658)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to'), position=2, insert_id=209659)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=209660)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=209661)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=209662)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'src_tokens'), position=0, insert_id=209663)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=209664)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=209665)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'src_tokens'), position=0, insert_id=209666)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=209667)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=209668)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 24,
        "number": 7977,
        "neg_line": [
            "-bbsz_offsets = (torch.arange(0, bsz) * beam_size).unsqueeze(1).type_as(tokens)",
            "-cand_offsets = torch.arange(0, cand_size).type_as(tokens)"
        ],
        "pos_line": [
            "+bbsz_offsets = (torch.arange(0, bsz) * beam_size).unsqueeze(1).type_as(tokens).to(src_tokens.device)",
            "+cand_offsets = torch.arange(0, cand_size).type_as(tokens).to(src_tokens.device)"
        ],
        "core_change": "-bbsz_offsets = (torch.arange(0, bsz) * beam_size).unsqueeze(1).type_as(tokens) -cand_offsets = torch.arange(0, cand_size).type_as(tokens) +bbsz_offsets = (torch.arange(0, bsz) * beam_size).unsqueeze(1).type_as(tokens).to(src_tokens.device) +cand_offsets = torch.arange(0, cand_size).type_as(tokens).to(src_tokens.device)",
        "core_API": "arange"
    },
    {
        "commit_hash": "4fd8977eafd658056c92ffa6294027e77cdc46c0",
        "index": "a3cf1de8be..c3e936f8e2 100644",
        "commit_message": "[RLlib] Minor cleanup in preparation to tf2.x support. (#9130)\n\n* WIP.\n\n* Fixes.\n\n* LINT.\n\n* Fixes.\n\n* Fixes and LINT.\n\n* WIP.\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class DistributionalQTFModel(TFModelV2):",
            "return tf.nn.relu(action_activation)",
            "",
            "def _f_epsilon(self, x):",
            "-        return tf.sign(x) * tf.sqrt(tf.abs(x))",
            "+        return tf.math.sign(x) * tf.math.sqrt(tf.math.abs(x))"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=2145824)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=2145825)",
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=2145826)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=2145827)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=tf), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'math'), position=2, insert_id=2145828)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=tf), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'math'), position=2, insert_id=2145829)",
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=2145830)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=2145831)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=tf), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'math'), position=2, insert_id=2145832)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 15,
        "number": 7982,
        "neg_line": [
            "-return tf.sign(x) * tf.sqrt(tf.abs(x))"
        ],
        "pos_line": [
            "+return tf.math.sign(x) * tf.math.sqrt(tf.math.abs(x))"
        ],
        "core_change": "-return tf.sign(x) * tf.sqrt(tf.abs(x)) +return tf.math.sign(x) * tf.math.sqrt(tf.math.abs(x))",
        "core_API": "relu"
    },
    {
        "commit_hash": "0070252e186dba391147d72544498eae493dfca1",
        "index": "41cece402..e9d3bae5c 100644",
        "commit_message": "Move complex tensor related functions to espnet2/enh/layers/complex_utils.py; Refactor complex support code; fix unit tests\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class STFTDecoder(AbsDecoder):",
            "ilens (torch.Tensor): input lengths [Batch]",
            "\"\"\"",
            "if not isinstance(input, ComplexTensor) and (",
            "-            is_torch_1_8_plus and not torch.is_complex(input)",
            "+            is_torch_1_9_plus and not torch.is_complex(input)",
            "):",
            "raise TypeError(\"Only support complex tensors for stft decoder\")"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 7985,
        "neg_line": [
            "-is_torch_1_8_plus and not torch.is_complex(input)"
        ],
        "pos_line": [
            "+is_torch_1_9_plus and not torch.is_complex(input)"
        ],
        "core_change": "-is_torch_1_8_plus and not torch.is_complex(input) +is_torch_1_9_plus and not torch.is_complex(input)",
        "core_API": "is_complex"
    },
    {
        "commit_hash": "d95f42711cb0c680cc705555c2cc6f04a88983cd",
        "index": "2c349981..6f035963 100644",
        "commit_message": "refactor(sdk&framework): [Broken] framework runners (#2288)\n\n* refactor(sdk): runner\n\n* refactor(sdk): simpler runner creation\n\n* refactor(frameworks): many fixes\n\n* fix resources_quota\n\n* fi\n\n* fix for <py38\n\n* fix runner\n\n* fix it\n\n* remove model_store in integration tests\n\n* remove runner args\n\n* mod default model_store for integration tests\n\n* try fix keras\n\n* fix picklable_model tests\n\n* fix detectron\n\n* review\n\n* fix picklable_model\n\n* append\n",
        "file": "BentoML.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "models",
        "change": [
            "def test_easyocr_save_load(metadata, image_array, modelstore, save_proc):",
            "_model = save_proc(LANG_LIST, RECOG_NETWORK, DETECT_MODEL, metadata)",
            "assert _model.info.metadata is not None",
            "",
            "-    easyocr_loaded = bentoml.easyocr.load(",
            "-        _model.tag,",
            "-        gpu=False,",
            "-        model_store=modelstore,",
            "-    )",
            "+    easyocr_loaded = bentoml.easyocr.load(_model.tag)",
            "",
            "raw_res = easyocr_loaded.readtext(image_array)"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=gpu))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=false, text=False))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=model_store))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=modelstore))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 1,
        "minus_line": 5,
        "AST_diff_line": 11,
        "number": 7987,
        "neg_line": [
            "-easyocr_loaded = bentoml.easyocr.load(",
            "-_model.tag,",
            "-gpu=False,",
            "-model_store=modelstore,",
            "-)"
        ],
        "pos_line": [
            "+easyocr_loaded = bentoml.easyocr.load(_model.tag)"
        ],
        "core_change": "-easyocr_loaded = bentoml.easyocr.load( -_model.tag, -gpu=False, -model_store=modelstore, -) +easyocr_loaded = bentoml.easyocr.load(_model.tag)",
        "core_API": "load"
    },
    {
        "commit_hash": "c317aaa3de289fc0a69e6b1001ffc586bb4d9c9a",
        "index": "82d6d407..410f7c66 100644",
        "commit_message": "Refactor library namespaces [pre-release][0.6-rc1] (#1412)\n\n* flake fixes\n\n* initial flake8 fixeS\n\n* remove top level from kornia.color\n\n* kornia filters\n\n* kornia losses\n\n* kornia features\n\n* geomtry and all ok\n\n* removed jit module\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* apply formatting and few fixes\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add keep block for isort\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* skip init\n\n* fix the docs\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* remove old code\n\n* simplify ci workflow\n\n* fix circular dependency\n\n* few format fixes\n\n* fix code format test\n\n* remove kornia.jit imports\n\n* final fixes\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* ci fixes\n\n* add versioneer\n\n* fix pnp import\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* exclude version files from pre-commit\n\n* exclude files precommit\n\n* update to pytorch 1.10 and add fixes\n\n* Update tests_cpu.yml\n\n* Update setup_dev_env.sh\n\n* Update tests_cpu.yml\n\n* undo versioneer\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix no_grad\n\n* Apply suggestions from code review\n\n* fix skip windows tests\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Update test_integrated.py\n\n* Apply suggestions from code review\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestErode:",
            "rtol=1e-4,",
            ")",
            "assert_close(",
            "-            erosion(tensor,",
            "-                    torch.ones_like(structural_element),",
            "-                    structuring_element=structural_element,",
            "-                    engine='convolution'),",
            "+            erosion(",
            "+                tensor,",
            "+                torch.ones_like(structural_element),",
            "+                structuring_element=structural_element,",
            "+                engine='convolution',",
            "+            ),",
            "expected,",
            "atol=1e-3,",
            "rtol=1e-3,"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=8, insert_id=413316)"
        ],
        "plus_line": 6,
        "minus_line": 4,
        "AST_diff_line": 1,
        "number": 7992,
        "neg_line": [
            "-erosion(tensor,",
            "-torch.ones_like(structural_element),",
            "-structuring_element=structural_element,",
            "-engine='convolution'),"
        ],
        "pos_line": [
            "+erosion(",
            "+tensor,",
            "+torch.ones_like(structural_element),",
            "+structuring_element=structural_element,",
            "+engine='convolution',",
            "+),"
        ],
        "core_change": "-erosion(tensor, -torch.ones_like(structural_element), -structuring_element=structural_element, -engine='convolution'), +erosion( +tensor, +torch.ones_like(structural_element), +structuring_element=structural_element, +engine='convolution', +),",
        "core_API": "ones_like"
    },
    {
        "commit_hash": "22ccc43670dac93eb7fe81520a84cf3979d05693",
        "index": "fef50e778a..46faa07694 100644",
        "commit_message": "[RLlib] DQN torch version. (#7597)\n\n* Fix.\n\n* Rollback.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* Fix.\n\n* Fix.\n\n* Fix.\n\n* Fix.\n\n* Fix.\n\n* WIP.\n\n* WIP.\n\n* Fix.\n\n* Test case fixes.\n\n* Test case fixes and LINT.\n\n* Test case fixes and LINT.\n\n* Rollback.\n\n* WIP.\n\n* WIP.\n\n* Test case fixes.\n\n* Fix.\n\n* Fix.\n\n* Fix.\n\n* Add regression test for DQN w/ param noise.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* Comment\n\n* Regression test case.\n\n* WIP.\n\n* WIP.\n\n* LINT.\n\n* LINT.\n\n* WIP.\n\n* Fix.\n\n* Fix.\n\n* Fix.\n\n* LINT.\n\n* Fix (SAC does currently not support eager).\n\n* Fix.\n\n* WIP.\n\n* LINT.\n\n* Update rllib/evaluation/sampler.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* Update rllib/evaluation/sampler.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* Update rllib/utils/exploration/exploration.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* Update rllib/utils/exploration/exploration.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* WIP.\n\n* WIP.\n\n* Fix.\n\n* LINT.\n\n* LINT.\n\n* Fix and LINT.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* Fix.\n\n* LINT.\n\n* Fix.\n\n* Fix and LINT.\n\n* Update rllib/utils/exploration/exploration.py\n\n* Update rllib/policy/dynamic_tf_policy.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* Update rllib/policy/dynamic_tf_policy.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* Update rllib/policy/dynamic_tf_policy.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* Fixes.\n\n* WIP.\n\n* LINT.\n\n* Fixes and LINT.\n\n* LINT and fixes.\n\n* LINT.\n\n* Move action_dist back into torch extra_action_out_fn and LINT.\n\n* Working SimpleQ learning cartpole on both torch AND tf.\n\n* Working Rainbow learning cartpole on tf.\n\n* Working Rainbow learning cartpole on tf.\n\n* WIP.\n\n* LINT.\n\n* LINT.\n\n* Update docs and add torch to APEX test.\n\n* LINT.\n\n* Fix.\n\n* LINT.\n\n* Fix.\n\n* Fix.\n\n* Fix and docstrings.\n\n* Fix broken RLlib tests in master.\n\n* Split BAZEL learning tests into cartpole and pendulum (reached the 60min barrier).\n\n* Fix error_outputs option in BAZEL for RLlib regression tests.\n\n* Fix.\n\n* Tune param-noise tests.\n\n* LINT.\n\n* Fix.\n\n* Fix.\n\n* test\n\n* test\n\n* test\n\n* Fix.\n\n* Fix.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* LINT.\n\n* WIP.\n\nCo-authored-by: Eric Liang <ekhliang@gmail.com>\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class SyncReplayOptimizer(PolicyOptimizer):",
            "self.learner_stats[policy_id] = get_learner_stats(info)",
            "replay_buffer = self.replay_buffers[policy_id]",
            "if isinstance(replay_buffer, PrioritizedReplayBuffer):",
            "-                    td_error = info[\"td_error\"]",
            "+                    # TODO(sven): This is currently structured differently for",
            "+                    #  torch/tf. Clean up these results/info dicts across",
            "+                    #  policies (note: fixing this in torch_policy.py will",
            "+                    #  break e.g. DDPPO!).",
            "+                    td_error = info.get(",
            "+                        \"td_error\", info[\"learner_stats\"].get(\"td_error\"))",
            "new_priorities = (",
            "np.abs(td_error) + self.prioritized_replay_eps)",
            "replay_buffer.update_priorities("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('call', None), position=2, insert_id=2146539)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2146540)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2146541)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'info'), position=0, insert_id=2146542)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2146543)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'get'), position=2, insert_id=2146544)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2146545)",
            "Insert(target_node=IN(type=argument_list), node=('string', '\"td_error\"'), position=1, insert_id=2146546)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=2146547)",
            "Insert(target_node=IN(type=argument_list), node=('call', None), position=3, insert_id=2146548)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=2146549)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2146550)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2146551)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=subscript), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2146552)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'get'), position=2, insert_id=2146553)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2146554)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=string, text=\"td_error\"), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2146555)",
            "Insert(target_node=ASTNode(type=subscript), node=('string', '\"learner_stats\"'), position=2, insert_id=2146556)"
        ],
        "plus_line": 6,
        "minus_line": 1,
        "AST_diff_line": 20,
        "number": 7998,
        "neg_line": [
            "-td_error = info[\"td_error\"]"
        ],
        "pos_line": [
            "+# TODO(sven): This is currently structured differently for",
            "+#  torch/tf. Clean up these results/info dicts across",
            "+#  policies (note: fixing this in torch_policy.py will",
            "+#  break e.g. DDPPO!).",
            "+td_error = info.get(",
            "+\"td_error\", info[\"learner_stats\"].get(\"td_error\"))"
        ],
        "core_change": "-td_error = info[\"td_error\"] +# TODO(sven): This is currently structured differently for +#  torch/tf. Clean up these results/info dicts across +#  policies (note: fixing this in torch_policy.py will +#  break e.g. DDPPO!). +td_error = info.get( +\"td_error\", info[\"learner_stats\"].get(\"td_error\"))",
        "core_API": "get"
    },
    {
        "commit_hash": "455c6390938a5c737fa63e78396cedae41e4e87e",
        "index": "3e65495f8..afc28239d 100644",
        "commit_message": "CDN urls (#4030)\n\n* [file_utils] use_cdn + documentation\n\n* Move to cdn. urls for weights\n\n* [urls] Hotfix for bert-base-japanese\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "models",
        "change": [
            "from .modeling_utils import Conv1D, PreTrainedModel, SequenceSummary, prune_conv",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "-OPENAI_GPT_PRETRAINED_MODEL_ARCHIVE_MAP = {",
            "-    \"openai-gpt\": \"https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-pytorch_model.bin\"",
            "-}",
            "+OPENAI_GPT_PRETRAINED_MODEL_ARCHIVE_MAP = {\"openai-gpt\": \"https://cdn.huggingface.co/openai-gpt-pytorch_model.bin\"}",
            "",
            "",
            "def load_tf_weights_in_openai_gpt(model, config, openai_checkpoint_folder_path):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-pytorch_model.bin\"), value='\"https://cdn.huggingface.co/openai-gpt-pytorch_model.bin\"')"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 8002,
        "neg_line": [
            "-OPENAI_GPT_PRETRAINED_MODEL_ARCHIVE_MAP = {",
            "-\"openai-gpt\": \"https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-pytorch_model.bin\"",
            "-}"
        ],
        "pos_line": [
            "+OPENAI_GPT_PRETRAINED_MODEL_ARCHIVE_MAP = {\"openai-gpt\": \"https://cdn.huggingface.co/openai-gpt-pytorch_model.bin\"}"
        ],
        "core_change": "-OPENAI_GPT_PRETRAINED_MODEL_ARCHIVE_MAP = { -\"openai-gpt\": \"https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-pytorch_model.bin\" -} +OPENAI_GPT_PRETRAINED_MODEL_ARCHIVE_MAP = {\"openai-gpt\": \"https://cdn.huggingface.co/openai-gpt-pytorch_model.bin\"}",
        "core_API": "getLogger"
    },
    {
        "commit_hash": "069f1632e6b10fe0effe7a4f6cf972f43ea8bc9a",
        "index": "8b36181..120680e 100644",
        "commit_message": "add options for anchors; fix alignment issue with resnet maxpool; more documentation.\n\n",
        "file": "tf-faster-rcnn.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class vgg16(object):",
            "y1 = tf.slice(rois, [0, 2], [-1, 1], name=\"y1\") / height",
            "x2 = tf.slice(rois, [0, 3], [-1, 1], name=\"x2\") / width",
            "y2 = tf.slice(rois, [0, 4], [-1, 1], name=\"y2\") / height",
            "-      bboxes = tf.concat([y1, x1, y2, x2], 1)",
            "+      bboxes = tf.stop_gradient(tf.concat([y1, x1, y2, x2], 1))",
            "pre_pool_size = cfg.POOLING_SIZE * 2",
            "crops = tf.image.crop_and_resize(bottom, bboxes, tf.to_int32(batch_ids), [pre_pool_size, pre_pool_size], name=\"crops\")"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2334125)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=2334126)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2334127)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2334128)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'stop_gradient'), position=2, insert_id=2334129)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2334130)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=call), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2334131)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 8008,
        "neg_line": [
            "-bboxes = tf.concat([y1, x1, y2, x2], 1)"
        ],
        "pos_line": [
            "+bboxes = tf.stop_gradient(tf.concat([y1, x1, y2, x2], 1))"
        ],
        "core_change": "-bboxes = tf.concat([y1, x1, y2, x2], 1) +bboxes = tf.stop_gradient(tf.concat([y1, x1, y2, x2], 1))",
        "core_API": "slice"
    },
    {
        "commit_hash": "6a3eb110fba645dccfd6473a9d46e8ba5299b3a6",
        "index": "0df79d54..c32fe81d 100644",
        "commit_message": "changed and improved tune.py, fixed policy gradient problem, other internal improvements\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class PolicyGradient(Objective):",
            ")",
            "return tf.math.minimum(x=(scaling * reward), y=(clipped_scaling * reward))",
            "",
            "-        skip_clipping = tf.math.equal(x=clipping_value, y=zero)",
            "+        skip_clipping = tf.math.equal(x=clipping_value, y=one)",
            "scaled = tf.cond(pred=skip_clipping, true_fn=no_clipping, false_fn=apply_clipping)",
            "",
            "loss = -scaled"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=zero), value='one')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 8012,
        "neg_line": [
            "-skip_clipping = tf.math.equal(x=clipping_value, y=zero)"
        ],
        "pos_line": [
            "+skip_clipping = tf.math.equal(x=clipping_value, y=one)"
        ],
        "core_change": "-skip_clipping = tf.math.equal(x=clipping_value, y=zero) +skip_clipping = tf.math.equal(x=clipping_value, y=one)",
        "core_API": "minimum"
    },
    {
        "commit_hash": "a93739431299871f29d8014a343be9ad10162377",
        "index": "56a5407b5..87c1fddf2 100644",
        "commit_message": "[Metrics] Unification of regression (#4166)\n\n* moved to utility\n\n* add files\n\n* unify\n\n* add desc\n\n* update\n\n* end of line\n\n* Apply suggestions from code review\n\nCo-authored-by: Justus Schock <12886177+justusschock@users.noreply.github.com>\n\n* Apply suggestions from code review\n\nCo-authored-by: Justus Schock <12886177+justusschock@users.noreply.github.com>\n\n* add back functional test in new interface\n\n* pep8\n\n* doctest fix\n\n* test name fix\n\n* unify psnr + add class psnr, TODO: psnr test refactor ala mean squared error\n\n* unify psnr\n\n* rm unused code\n\n* pep8\n\n* docs\n\n* unify ssim\n\n* lower tolerance for ssim\n\n* fix import\n\n* pep8\n\n* docs\n\n* flake8\n\n* test smaller images\n\n* trying to fix test\n\n* no ddp test for ssim\n\n* pep8\n\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\nCo-authored-by: Justus Schock <12886177+justusschock@users.noreply.github.com>\nCo-authored-by: Teddy Koker <teddy.koker@gmail.com>\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class MeanSquaredError(Metric):",
            "preds: Predictions from model",
            "target: Ground truth values",
            "\"\"\"",
            "-        self._check_same_shape(preds, target)",
            "-        squared_error = torch.pow(preds - target, 2)",
            "+        sum_squared_error, n_obs = _mean_squared_error_update(preds, target)",
            "",
            "-        self.sum_squared_error += torch.sum(squared_error)",
            "-        self.total += target.numel()",
            "+        self.sum_squared_error += sum_squared_error",
            "+        self.total += n_obs",
            "",
            "def compute(self):",
            "\"\"\"",
            "Computes mean squared error over state.",
            "\"\"\"",
            "-        return self.sum_squared_error / self.total",
            "+        return _mean_squared_error_compute(self.sum_squared_error, self.total)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"\"\"\n        self._check_same_shape(preds, target)\n        squared_error = torch.pow(preds - target, 2)\n\n        self.sum_squared_error += torch.sum(squared_error)\n        self.total += target.numel()\n\ndef compute(self):\n\"\"\"), value='\"\"\"\\n        sum_squared_error, n_obs = _mean_squared_error_update(preds, target)\\n\\n        self.sum_squared_error += sum_squared_error\\n        self.total += n_obs\\n\\ndef compute(self):\\n\"\"\"')",
            "Insert(target_node=ASTNode(type=return_statement), node=('call', None), position=1, insert_id=559429)",
            "Insert(target_node=IN(type=call), node=('identifier', '_mean_squared_error_compute'), position=0, insert_id=559430)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=559431)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=559432)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=attribute), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=559433)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=attribute), position=3)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=559434)",
            "Delete(target_node=ASTNode(type=/, text=/))",
            "Delete(target_node=ASTNode(type=binary_operator))"
        ],
        "plus_line": 4,
        "minus_line": 5,
        "AST_diff_line": 11,
        "number": 8014,
        "neg_line": [
            "-self._check_same_shape(preds, target)",
            "-squared_error = torch.pow(preds - target, 2)",
            "-self.sum_squared_error += torch.sum(squared_error)",
            "-self.total += target.numel()",
            "-return self.sum_squared_error / self.total"
        ],
        "pos_line": [
            "+sum_squared_error, n_obs = _mean_squared_error_update(preds, target)",
            "+self.sum_squared_error += sum_squared_error",
            "+self.total += n_obs",
            "+return _mean_squared_error_compute(self.sum_squared_error, self.total)"
        ],
        "core_change": "-self._check_same_shape(preds, target) -squared_error = torch.pow(preds - target, 2) +sum_squared_error, n_obs = _mean_squared_error_update(preds, target) -self.sum_squared_error += torch.sum(squared_error) -self.total += target.numel() +self.sum_squared_error += sum_squared_error +self.total += n_obs -return self.sum_squared_error / self.total +return _mean_squared_error_compute(self.sum_squared_error, self.total)",
        "core_API": "_check_same_shape"
    },
    {
        "commit_hash": "cb3e5c33f7c2131f723ab79b88f98f8e1df6d927",
        "index": "cbce4e965..03b53c176 100644",
        "commit_message": "Fix a few last paths for the new repo org (#8666)\n\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "models",
        "change": [
            "class CaptureLogger:",
            "",
            ">>> msg = \"Testing 1, 2, 3\"",
            ">>> logging.set_verbosity_info()",
            "-        >>> logger = logging.get_logger(\"transformers.tokenization_bart\")",
            "+        >>> logger = logging.get_logger(\"transformers.models.bart.tokenization_bart\")",
            ">>> with CaptureLogger(logger) as cl:",
            "...     logger.info(msg)",
            ">>> assert cl.out, msg+\"\\n\""
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"transformers.tokenization_bart\"), value='\"transformers.models.bart.tokenization_bart\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 8015,
        "neg_line": [
            "->>> logger = logging.get_logger(\"transformers.tokenization_bart\")"
        ],
        "pos_line": [
            "+>>> logger = logging.get_logger(\"transformers.models.bart.tokenization_bart\")"
        ],
        "core_change": "->>> logger = logging.get_logger(\"transformers.tokenization_bart\") +>>> logger = logging.get_logger(\"transformers.models.bart.tokenization_bart\")",
        "core_API": "set_verbosity_info"
    },
    {
        "commit_hash": "6fc03a2a3fcfef57d6c4a62e5de378efa7f3d336",
        "index": "ca988a7bd..e566f3aea 100644",
        "commit_message": "Fix an issue related to complex_norm argument\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_cat(dim):",
            "assert complex_module.allclose(ret, ret2)",
            "",
            "",
            "-@pytest.mark.parametrize(\"dim\", [0, 1, 2])",
            "+@pytest.mark.parametrize(\"dim\", [None, 0, 1, 2])",
            "@pytest.mark.skipif(not is_torch_1_9_plus, reason=\"Require torch 1.9.0+\")",
            "def test_complex_norm(dim):",
            "mat = ComplexTensor(torch.rand(2, 3, 4), torch.rand(2, 3, 4))",
            "mat_th = torch.complex(mat.real, mat.imag)",
            "norm = complex_norm(mat, dim=dim, keepdim=True)",
            "norm_th = complex_norm(mat_th, dim=dim, keepdim=True)",
            "-    assert (",
            "-        torch.allclose(norm, norm_th)",
            "-        and norm.ndim == mat.ndim",
            "-        and mat.numel() == norm.numel() * mat.size(dim)",
            "-    )",
            "+    assert torch.allclose(norm, norm_th)",
            "+    if dim is not None:",
            "+        assert norm.ndim == mat.ndim and mat.numel() == norm.numel() * mat.size(dim)",
            "",
            "",
            "@pytest.mark.parametrize(\"real_vec\", [True, False])"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('assert_statement', None), position=7, insert_id=127162)",
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=8, insert_id=127163)",
            "Insert(target_node=IN(type=assert_statement), node=('assert', 'assert'), position=0, insert_id=127164)",
            "Move(target_node=IN(type=assert_statement), node=ASTNode(type=call), position=1)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=127165)",
            "Insert(target_node=IN(type=if_statement), node=('comparison_operator', None), position=1, insert_id=127166)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=127167)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=127168)",
            "Insert(target_node=IN(type=comparison_operator), node=('identifier', 'dim'), position=0, insert_id=127169)",
            "Insert(target_node=IN(type=comparison_operator), node=('is not', 'is'), position=1, insert_id=127170)",
            "Insert(target_node=IN(type=comparison_operator), node=('is not', 'not'), position=2, insert_id=127171)",
            "Insert(target_node=IN(type=comparison_operator), node=('none', 'None'), position=3, insert_id=127172)",
            "Move(target_node=IN(type=block), node=ASTNode(type=assert_statement), position=0)",
            "Move(target_node=ASTNode(type=assert_statement), node=ASTNode(type=boolean_operator), position=1)",
            "Move(target_node=ASTNode(type=boolean_operator), node=ASTNode(type=comparison_operator), position=0)",
            "Insert(target_node=ASTNode(type=list), node=('none', 'None'), position=1, insert_id=127173)",
            "Insert(target_node=ASTNode(type=list), node=(',', ','), position=2, insert_id=127174)",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=and, text=and))",
            "Delete(target_node=ASTNode(type=boolean_operator))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=parenthesized_expression))"
        ],
        "plus_line": 3,
        "minus_line": 5,
        "AST_diff_line": 22,
        "number": 8016,
        "neg_line": [
            "-@pytest.mark.parametrize(\"dim\", [0, 1, 2])",
            "-assert (",
            "-torch.allclose(norm, norm_th)",
            "-and norm.ndim == mat.ndim",
            "-and mat.numel() == norm.numel() * mat.size(dim)",
            "-)"
        ],
        "pos_line": [
            "+@pytest.mark.parametrize(\"dim\", [None, 0, 1, 2])",
            "+assert torch.allclose(norm, norm_th)",
            "+if dim is not None:",
            "+assert norm.ndim == mat.ndim and mat.numel() == norm.numel() * mat.size(dim)"
        ],
        "core_change": "-@pytest.mark.parametrize(\"dim\", [0, 1, 2]) +@pytest.mark.parametrize(\"dim\", [None, 0, 1, 2]) -assert ( -torch.allclose(norm, norm_th) -and norm.ndim == mat.ndim -and mat.numel() == norm.numel() * mat.size(dim) -) +assert torch.allclose(norm, norm_th) +if dim is not None: +assert norm.ndim == mat.ndim and mat.numel() == norm.numel() * mat.size(dim)",
        "core_API": "allclose"
    },
    {
        "commit_hash": "5805ba40b9230d046dd0186c124b72fb656192e5",
        "index": "536f4a5f..b4816d63 100644",
        "commit_message": "fixed graph_sage_unsup Attribute error\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "data = dataset[0]",
            "class NeighborSampler(RawNeighborSampler):",
            "def sample(self, batch):",
            "batch = torch.tensor(batch)",
            "-        row, col, _ = self.adj.coo()",
            "+        row, col, _ = self.adj_t.coo()#change adj to adj_t",
            "",
            "# For each node in `batch`, we sample a direct neighbor (as positive",
            "# example) and a random node (as negative example):",
            "pos_batch = random_walk(row, col, batch, walk_length=1,",
            "coalesced=False)[:, 1]",
            "",
            "-        neg_batch = torch.randint(0, self.adj.size(0), (batch.numel(), ),",
            "-                                  dtype=torch.long)",
            "+        neg_batch = torch.randint(0, self.adj_t.size(0), (batch.numel(), ),",
            "+                                  dtype=torch.long)#change adj to adj_t",
            "",
            "batch = torch.cat([batch, pos_batch, neg_batch], dim=0)",
            "return super(NeighborSampler, self).sample(batch)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=adj), value='adj_t')",
            "Update(target_node=ASTNode(type=identifier, text=adj), value='adj_t')"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 2,
        "number": 8017,
        "neg_line": [
            "-row, col, _ = self.adj.coo()",
            "-neg_batch = torch.randint(0, self.adj.size(0), (batch.numel(), ),",
            "-dtype=torch.long)"
        ],
        "pos_line": [
            "+row, col, _ = self.adj_t.coo()#change adj to adj_t",
            "+neg_batch = torch.randint(0, self.adj_t.size(0), (batch.numel(), ),",
            "+dtype=torch.long)#change adj to adj_t"
        ],
        "core_change": "-row, col, _ = self.adj.coo() +row, col, _ = self.adj_t.coo()#change adj to adj_t -neg_batch = torch.randint(0, self.adj.size(0), (batch.numel(), ), -dtype=torch.long) +neg_batch = torch.randint(0, self.adj_t.size(0), (batch.numel(), ), +dtype=torch.long)#change adj to adj_t",
        "core_API": "tensor"
    },
    {
        "commit_hash": "392b308f424d52f6d534791b9be909fcc4d87325",
        "index": "5c0271a838..e3245a7e8a 100644",
        "commit_message": "fixing fn_tree args for atleast_2d and count_nonzero\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "functional",
        "change": [
            "def test_dstack(",
            "",
            "# atleast_2d",
            "@handle_test(",
            "+    fn_tree=\"functional.experimental.atleast_2d\",",
            "dtype_and_x=helpers.dtype_and_values(",
            "available_dtypes=helpers.get_dtypes(\"valid\"),",
            "num_arrays=helpers.ints(min_value=1, max_value=5),"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=ERROR), node=('keyword_argument', None), position=3, insert_id=1642988)",
            "Insert(target_node=ASTNode(type=ERROR), node=(',', ','), position=4, insert_id=1642989)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'fn_tree'), position=0, insert_id=1642990)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1642991)",
            "Insert(target_node=IN(type=keyword_argument), node=('string', '\"functional.experimental.atleast_2d\"'), position=2, insert_id=1642992)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 5,
        "number": 8018,
        "neg_line": [],
        "pos_line": [
            "+fn_tree=\"functional.experimental.atleast_2d\","
        ],
        "core_change": "+fn_tree=\"functional.experimental.atleast_2d\",",
        "core_API": "dtype_and_values"
    },
    {
        "commit_hash": "efc2f94dd1c6806aa249ba418aa118ce6f1165df",
        "index": "17bfdaf..e2ff391 100644",
        "commit_message": "Fixed update of centers for center loss\n",
        "file": "facenet.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def center_loss(features, label, alfa, nrof_classes):",
            "centers_batch = tf.gather(centers, label)",
            "diff = (1 - alfa) * (centers_batch - features)",
            "centers = tf.scatter_sub(centers, label, diff)",
            "-    loss = tf.reduce_mean(tf.square(features - centers_batch))",
            "+    with tf.control_dependencies([centers]):",
            "+        loss = tf.reduce_mean(tf.square(features - centers_batch))",
            "return loss, centers",
            "",
            "def get_image_paths_and_labels(dataset):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('with_statement', None), position=4, insert_id=1926826)",
            "Insert(target_node=IN(type=with_statement), node=('with', 'with'), position=0, insert_id=1926827)",
            "Insert(target_node=IN(type=with_statement), node=('with_clause', None), position=1, insert_id=1926828)",
            "Insert(target_node=IN(type=with_statement), node=(':', ':'), position=2, insert_id=1926829)",
            "Insert(target_node=IN(type=with_statement), node=('block', None), position=3, insert_id=1926830)",
            "Insert(target_node=IN(type=with_clause), node=('with_item', None), position=0, insert_id=1926831)",
            "Move(target_node=IN(type=block), node=ASTNode(type=expression_statement), position=0)",
            "Insert(target_node=IN(type=with_item), node=('call', None), position=0, insert_id=1926832)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1926833)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1926834)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=1926835)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1926836)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'control_dependencies'), position=2, insert_id=1926837)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1926838)",
            "Insert(target_node=IN(type=argument_list), node=('list', None), position=1, insert_id=1926839)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1926840)",
            "Insert(target_node=IN(type=list), node=('[', '['), position=0, insert_id=1926841)",
            "Insert(target_node=IN(type=list), node=('identifier', 'centers'), position=1, insert_id=1926842)",
            "Insert(target_node=IN(type=list), node=(']', ']'), position=2, insert_id=1926843)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 19,
        "number": 8019,
        "neg_line": [
            "-loss = tf.reduce_mean(tf.square(features - centers_batch))"
        ],
        "pos_line": [
            "+with tf.control_dependencies([centers]):",
            "+loss = tf.reduce_mean(tf.square(features - centers_batch))"
        ],
        "core_change": "-loss = tf.reduce_mean(tf.square(features - centers_batch)) +with tf.control_dependencies([centers]): +loss = tf.reduce_mean(tf.square(features - centers_batch))",
        "core_API": "gather"
    },
    {
        "commit_hash": "e10dbde28e3ba5b7bb9b906ec089a7cb35155510",
        "index": "3bfc2c88..6c16d064 100755",
        "commit_message": "Fix q_model clip_gradients -> clip_loss\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class QModel(Model):",
            "",
            "# If loss clipping is used, calculate the huber loss",
            "if config.clip_loss > 0.0:",
            "-                huber_loss = tf.where(condition=(tf.abs(delta) < config.clip_gradients), x=(0.5 * self.loss_per_instance), y=(tf.abs(delta) - 0.5))",
            "+                huber_loss = tf.where(condition=(tf.abs(delta) < config.clip_loss), x=(0.5 * self.loss_per_instance), y=(tf.abs(delta) - 0.5))",
            "self.q_loss = tf.reduce_mean(input_tensor=huber_loss, axis=0)",
            "else:",
            "self.q_loss = tf.reduce_mean(input_tensor=self.loss_per_instance, axis=0)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=clip_gradients), value='clip_loss')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 8020,
        "neg_line": [
            "-huber_loss = tf.where(condition=(tf.abs(delta) < config.clip_gradients), x=(0.5 * self.loss_per_instance), y=(tf.abs(delta) - 0.5))"
        ],
        "pos_line": [
            "+huber_loss = tf.where(condition=(tf.abs(delta) < config.clip_loss), x=(0.5 * self.loss_per_instance), y=(tf.abs(delta) - 0.5))"
        ],
        "core_change": "-huber_loss = tf.where(condition=(tf.abs(delta) < config.clip_gradients), x=(0.5 * self.loss_per_instance), y=(tf.abs(delta) - 0.5)) +huber_loss = tf.where(condition=(tf.abs(delta) < config.clip_loss), x=(0.5 * self.loss_per_instance), y=(tf.abs(delta) - 0.5))",
        "core_API": "where"
    },
    {
        "commit_hash": "1c13d02d700ebcd053a560754f22e4247257a344",
        "index": "8b038d2e..60634ad0 100644",
        "commit_message": "Fix tensor_to_image\n\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def tensor_to_image(tensor):",
            "if not torch.is_tensor(tensor):",
            "raise TypeError(\"Input type is not a torch.Tensor. Got {}\".format(",
            "type(tensor)))",
            "+    tensor = torch.squeeze(tensor)",
            "if len(tensor.shape) > 3 or len(tensor.shape) < 2:",
            "raise ValueError(",
            "\"Input size must be a two or three dimensional tensor\")",
            "-    tensor = torch.squeeze(tensor)",
            "if len(tensor.shape) == 2:",
            "tensor = torch.unsqueeze(tensor, dim=0)",
            "return tensor.permute(1, 2, 0).contiguous().cpu().detach().numpy()"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=expression_statement), node=ASTNode(type=module), position=3)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 8021,
        "neg_line": [
            "-tensor = torch.squeeze(tensor)"
        ],
        "pos_line": [
            "+tensor = torch.squeeze(tensor)"
        ],
        "core_change": "+tensor = torch.squeeze(tensor) -tensor = torch.squeeze(tensor)",
        "core_API": "is_tensor"
    },
    {
        "commit_hash": "733d6bbbf3394839f1755805cae3875317c8d5d1",
        "index": "e1f14cb..eab939d 100644",
        "commit_message": "Fix typing of cifar10_convnet after previous change\n\nPiperOrigin-RevId: 297585986\nChange-Id: Icdcfbdec09feafb1026385bf794d252a626a8219\n\n",
        "file": "sonnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Cifar10ConvNet(base.Module):",
            "",
            "logits = self._logits_module(flat_output)",
            "",
            "-    return {'logits': logits, 'activations': activations}  # pytype: disable=bad-return-type",
            "+    return {'logits': logits, 'activations': activations}"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 8024,
        "neg_line": [
            "-return {'logits': logits, 'activations': activations}  # pytype: disable=bad-return-type"
        ],
        "pos_line": [
            "+return {'logits': logits, 'activations': activations}"
        ],
        "core_change": "-return {'logits': logits, 'activations': activations}  # pytype: disable=bad-return-type +return {'logits': logits, 'activations': activations}",
        "core_API": "_logits_module"
    },
    {
        "commit_hash": "da2f1325014a158be36d62bdd20293ef97e574f4",
        "index": "12f31677..cc32fcfb 100644",
        "commit_message": "enable black in the precommit (#1777)\n\n* enable black in the precommit\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add some fixes\n\n* update libface detection url\n\n* added url from kornia checkpoint\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def get_hanning_kernel1d(kernel_size: int, device=torch.device('cpu'), dtype=tor",
            "return x",
            "",
            "",
            "-def get_hanning_kernel2d(kernel_size: Tuple[int, int],",
            "-                         device=torch.device('cpu'),",
            "-                         dtype=torch.float) -> torch.Tensor:",
            "+def get_hanning_kernel2d(kernel_size: Tuple[int, int], device=torch.device('cpu'), dtype=torch.float) -> torch.Tensor:",
            "r\"\"\"Returns 2d Hanning kernel, used in signal processing and KCF tracker.",
            "",
            "Args:"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 0,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 8025,
        "neg_line": [
            "-def get_hanning_kernel2d(kernel_size: Tuple[int, int],",
            "-device=torch.device('cpu'),",
            "-dtype=torch.float) -> torch.Tensor:"
        ],
        "pos_line": [
            "+def get_hanning_kernel2d(kernel_size: Tuple[int, int], device=torch.device('cpu'), dtype=torch.float) -> torch.Tensor:"
        ],
        "core_change": "-def get_hanning_kernel2d(kernel_size: Tuple[int, int], -device=torch.device('cpu'), -dtype=torch.float) -> torch.Tensor: +def get_hanning_kernel2d(kernel_size: Tuple[int, int], device=torch.device('cpu'), dtype=torch.float) -> torch.Tensor:",
        "core_API": "device"
    },
    {
        "commit_hash": "35f0e10c4f0026fbe663aebe353a2fcb3eb1f93b",
        "index": "e35a9ab0f..dfe3a899e 100644",
        "commit_message": "Fix option names\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def recog(args):",
            "word_dict = rnnlm_args.char_list_dict",
            "char_dict = {x: i for i, x in enumerate(train_args.char_list)}",
            "word_rnnlm = lm_pytorch.ClassifierWithState(lm_pytorch.RNNLM(",
            "-            len(word_dict), rnnlm_args.layers, rnnlm_args.units))",
            "+            len(word_dict), rnnlm_args.layer, rnnlm_args.unit))",
            "torch_load(args.word_rnnlm, word_rnnlm)",
            "word_rnnlm.eval()"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=layers), value='layer')",
            "Update(target_node=ASTNode(type=identifier, text=units), value='unit')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 8027,
        "neg_line": [
            "-len(word_dict), rnnlm_args.layers, rnnlm_args.units))"
        ],
        "pos_line": [
            "+len(word_dict), rnnlm_args.layer, rnnlm_args.unit))"
        ],
        "core_change": "-len(word_dict), rnnlm_args.layers, rnnlm_args.units)) +len(word_dict), rnnlm_args.layer, rnnlm_args.unit))",
        "core_API": "ClassifierWithState"
    },
    {
        "commit_hash": "0178e6cc227e0f6d60cb5f3675edd1cda9a0396e",
        "index": "544458e4..3325d9de 100755",
        "commit_message": "Fix unbalanced gradients bug in ZeRO-2 gradient accumulation (#545)\n\n* Use zero-tensors for missing gradients to avoid size mismatch\n\n* Unit test for unbalanced gradients in ZeRO\n\n* Formatting fixes\n",
        "file": "DeepSpeed.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class FP16_DeepSpeedZeroOptimizer(object):",
            "current_size = 0",
            "for i, tensor in enumerate(tensor_list):",
            "if tensor.grad is None:",
            "-                continue",
            "+                tensor.grad = torch.zeros_like(tensor)",
            "",
            "tensor = tensor.grad",
            "num_elements = tensor.numel()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('block', None), position=0, insert_id=84643)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=84644)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=84645)",
            "Insert(target_node=IN(type=assignment), node=('attribute', None), position=0, insert_id=84646)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=84647)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=84648)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tensor'), position=0, insert_id=84649)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=84650)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'grad'), position=2, insert_id=84651)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=84652)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=84653)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=84654)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=84655)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'zeros_like'), position=2, insert_id=84656)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=84657)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'tensor'), position=1, insert_id=84658)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=84659)",
            "Delete(target_node=ASTNode(type=continue, text=continue))",
            "Delete(target_node=ASTNode(type=continue_statement))",
            "Delete(target_node=ASTNode(type=block))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 20,
        "number": 8028,
        "neg_line": [
            "-continue"
        ],
        "pos_line": [
            "+tensor.grad = torch.zeros_like(tensor)"
        ],
        "core_change": "-continue +tensor.grad = torch.zeros_like(tensor)",
        "core_API": "zeros_like"
    },
    {
        "commit_hash": "6422ee88a9a3bdfd93ec98d2def1e9f2b02eacab",
        "index": "2edb4d84..9e7bd3d1 100644",
        "commit_message": "Replace Distribution+TorchDistribution with a thin Distribution mixin (#769)\n\n* Sketch Distribution class as mixin\n\n* Remove TorchDistribution class\n\n* Simplify TransformedDistribution\n\n* Update torch wrappers for most distributions\n\n* Fix docs\n\n* Use dist.Reshape() to set extra_event_dims\n\n* Fix bugs in Reshape distribution\n\n* Fix rejector tests\n\n* Update _Subsample distribution\n\n* Use .reshape() method for extra_event_dims and sample_shape\n\n* Refactor Distribution -> TorchDistribution class hierarchy\n\n* Update docs\n\n* Fix json error in air.ipynb\n\n* Fix bugs in air.ipynb and abstract_infer.py\n\n* Fix distributions docs\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class RaoBlackwellizationTests(TestCase):",
            "pyro.sample(\"z_%d_%d\" % (i, k),",
            "fakes.NonreparameterizedNormal(ng_zeros(4 - i), ng_ones(4 - i)))",
            "pyro.sample(\"obs_%d\" % i,",
            "-                            dist.Normal(mu_latent, torch.pow(self.lam, -0.5), extra_event_dims=1),",
            "+                            dist.Normal(mu_latent, torch.pow(self.lam, -0.5))",
            "+                                .reshape(extra_event_dims=1),",
            "obs=_x)",
            "for k in range(n_superfluous_top, n_superfluous_top + n_superfluous_bottom):",
            "pyro.sample(\"z_%d_%d\" % (i, k),"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=754725)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=754726)",
            "Insert(target_node=IN(type=attribute), node=('call', None), position=0, insert_id=754727)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=754728)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'reshape'), position=2, insert_id=754729)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=754730)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=keyword_argument), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=754731)",
            "Move(target_node=IN(type=call), node=ASTNode(type=attribute), position=0)",
            "Move(target_node=IN(type=call), node=ASTNode(type=argument_list), position=1)",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 8032,
        "neg_line": [
            "-dist.Normal(mu_latent, torch.pow(self.lam, -0.5), extra_event_dims=1),"
        ],
        "pos_line": [
            "+dist.Normal(mu_latent, torch.pow(self.lam, -0.5))",
            "+.reshape(extra_event_dims=1),"
        ],
        "core_change": "-dist.Normal(mu_latent, torch.pow(self.lam, -0.5), extra_event_dims=1), +dist.Normal(mu_latent, torch.pow(self.lam, -0.5)) +.reshape(extra_event_dims=1),",
        "core_API": "sample"
    },
    {
        "commit_hash": "cd51772bbe4eec0addd4e149b2f47f999020e7a0",
        "index": "0f50fc97..2c03e4df 100644",
        "commit_message": "remove numpy import (#1116)\n\n* remove numpy import\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* undo and add setup develop in gh actions\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* remove create_checkerboard\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestErode:",
            "None, None, :, :",
            "]",
            "assert_allclose(",
            "-            erosion(tensor, torch.ones_like(structural_element), structuring_element=structural_element), expected,",
            "-            atol=1e-4, rtol=1e-4",
            "+            erosion(tensor, torch.ones_like(structural_element), structuring_element=structural_element),",
            "+            expected,",
            "+            atol=1e-4,",
            "+            rtol=1e-4,",
            ")",
            "",
            "def test_exception(self, device, dtype):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=8, insert_id=423222)"
        ],
        "plus_line": 4,
        "minus_line": 2,
        "AST_diff_line": 1,
        "number": 8033,
        "neg_line": [
            "-erosion(tensor, torch.ones_like(structural_element), structuring_element=structural_element), expected,",
            "-atol=1e-4, rtol=1e-4"
        ],
        "pos_line": [
            "+erosion(tensor, torch.ones_like(structural_element), structuring_element=structural_element),",
            "+expected,",
            "+atol=1e-4,",
            "+rtol=1e-4,"
        ],
        "core_change": "-erosion(tensor, torch.ones_like(structural_element), structuring_element=structural_element), expected, -atol=1e-4, rtol=1e-4 +erosion(tensor, torch.ones_like(structural_element), structuring_element=structural_element), +expected, +atol=1e-4, +rtol=1e-4,",
        "core_API": "ones_like"
    },
    {
        "commit_hash": "86e435bbb1e54f169351dbb798141afee7fa1b93",
        "index": "5e212d5f4..bb9e2716d 100644",
        "commit_message": "fixed small typo (#20490)\n\nCo-authored-by: Sandeep Kumar <sandeep.kumar@woven-planet.global>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class VanLayer(nn.Module):",
            "drop_path_rate: float = 0.5,",
            "):",
            "super().__init__()",
            "-        self.drop_path = VanDropPath(drop_path) if drop_path_rate > 0.0 else nn.Identity()",
            "+        self.drop_path = VanDropPath(drop_path_rate) if drop_path_rate > 0.0 else nn.Identity()",
            "self.pre_normomalization = nn.BatchNorm2d(hidden_size)",
            "self.attention = VanSpatialAttentionLayer(hidden_size, config.hidden_act)",
            "self.attention_scaling = VanLayerScaling(hidden_size, config.layer_scale_init_value)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=drop_path), value='drop_path_rate')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 8035,
        "neg_line": [
            "-self.drop_path = VanDropPath(drop_path) if drop_path_rate > 0.0 else nn.Identity()"
        ],
        "pos_line": [
            "+self.drop_path = VanDropPath(drop_path_rate) if drop_path_rate > 0.0 else nn.Identity()"
        ],
        "core_change": "-self.drop_path = VanDropPath(drop_path) if drop_path_rate > 0.0 else nn.Identity() +self.drop_path = VanDropPath(drop_path_rate) if drop_path_rate > 0.0 else nn.Identity()",
        "core_API": "Identity"
    },
    {
        "commit_hash": "aee01133cd4ad4f822994b5c1b8b5f415f27b887",
        "index": "d3a56f92d0..585d8eb3fe 100644",
        "commit_message": "Fix dict/tuple hybrid action space for tensorflow eager execution (#8781)\n\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class DiagGaussian(TFActionDistribution):",
            "@override(ActionDistribution)",
            "def logp(self, x):",
            "return -0.5 * tf.reduce_sum(",
            "-            tf.square((x - self.mean) / self.std), axis=1) - \\",
            "+            tf.square((tf.to_float(x) - self.mean) / self.std), axis=1) - \\",
            "0.5 * np.log(2.0 * np.pi) * tf.to_float(tf.shape(x)[1]) - \\",
            "tf.reduce_sum(self.log_std, axis=1)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=binary_operator), node=('call', None), position=0, insert_id=2146085)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2146086)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2146087)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2146088)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2146089)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to_float'), position=2, insert_id=2146090)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2146091)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=x), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2146092)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 8038,
        "neg_line": [
            "-tf.square((x - self.mean) / self.std), axis=1) - \\"
        ],
        "pos_line": [
            "+tf.square((tf.to_float(x) - self.mean) / self.std), axis=1) - \\"
        ],
        "core_change": "-tf.square((x - self.mean) / self.std), axis=1) - \\ +tf.square((tf.to_float(x) - self.mean) / self.std), axis=1) - \\",
        "core_API": "reduce_sum"
    },
    {
        "commit_hash": "ce8f42b12ecd522e3ef7251e0d5f5175075a3fb4",
        "index": "f140c77f..8fcbbc26 100644",
        "commit_message": "Fixes to enable dev branch to move to pytorch-1.2 (#2001)\n\n* Fixes to enable dev branch to move to pytorch-1.2\n\n* change name for published cpu wheels\n\n* fix failing unit tests\n\n* change docs conf\n\n* change tracking example\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_unnormalized_normal(kernel, jit):",
            "posterior.append(samples)",
            "",
            "posterior = torch.stack([sample[\"z\"] for sample in posterior])",
            "-    assert_equal(torch.mean(posterior), true_mean, prec=0.1)",
            "-    assert_equal(torch.std(posterior), true_std, prec=0.1)",
            "+    assert_close(torch.mean(posterior), true_mean, rtol=0.05)",
            "+    assert_close(torch.std(posterior), true_std, rtol=0.05)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=assert_equal), value='assert_close')",
            "Update(target_node=ASTNode(type=identifier, text=assert_equal), value='assert_close')",
            "Update(target_node=ASTNode(type=identifier, text=prec), value='rtol')",
            "Update(target_node=ASTNode(type=float, text=0.1), value='0.05')",
            "Update(target_node=ASTNode(type=identifier, text=prec), value='rtol')",
            "Update(target_node=ASTNode(type=float, text=0.1), value='0.05')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 6,
        "number": 8045,
        "neg_line": [
            "-assert_equal(torch.mean(posterior), true_mean, prec=0.1)",
            "-assert_equal(torch.std(posterior), true_std, prec=0.1)"
        ],
        "pos_line": [
            "+assert_close(torch.mean(posterior), true_mean, rtol=0.05)",
            "+assert_close(torch.std(posterior), true_std, rtol=0.05)"
        ],
        "core_change": "-assert_equal(torch.mean(posterior), true_mean, prec=0.1) -assert_equal(torch.std(posterior), true_std, prec=0.1) +assert_close(torch.mean(posterior), true_mean, rtol=0.05) +assert_close(torch.std(posterior), true_std, rtol=0.05)",
        "core_API": "append"
    },
    {
        "commit_hash": "660e0b97bd652bd3a0dfd5f847e5cf62502d0469",
        "index": "93d98914f..0e7539546 100644",
        "commit_message": "Fix train_step, test_step and tests for CLIP (#18684)\n\n* Fix train_step and test_step, correctly enable CLIP fit test\n\n* Stop using get_args on older Python versions\n\n* Don't use get_origin either\n\n* UnionType is actually even newer, don't use that either\n\n* Apply the same fix to test_loss_computation\n\n* Just realized I was accidentally skipping a bunch of tests!\n\n* Fix test_loss_computation for models without separable labels\n\n* Fix scalar losses in test_step and train_step\n\n* Stop committing your breakpoints\n\n* Fix Swin loss shape\n\n* Fix Tapas loss shape\n\n* Shape fixes for TAPAS, DeIT, HuBERT and ViTMAE\n\n* Add loss computation to TFMobileBertForPreTraining\n\n* make fixup and move copied from statement\n\n* make fixup and move copied from statement\n\n* Correct copied from\n\n* Add labels and next_sentence_label inputs to TFMobileBERT\n\n* Make sure total_loss is always defined\n\n* Update tests/test_modeling_tf_common.py\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* Fix copied from\n\n* Ensure CTC models get labels in tests\n\n* Ensure CTC models get labels in tests\n\n* Fix tests for vit_mae\n\n* Fix tests for vit_mae\n\n* Fix tests for vit_mae\n\n* Reduce batch size for wav2vec2 testing because it was causing OOM\n\n* Skip some TAPAS tests that are failing\n\n* Skip a failing HuBERT test\n\n* make style\n\n* Fix mobilebertforpretraining test\n\n* Skip Wav2Vec2 tests that use huge amounts of mem\n\n* Skip keras_fit for Wav2Vec2 as well\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFTapasForQuestionAnswering(TFTapasPreTrainedModel):",
            "logits_aggregation = self.aggregation_classifier(pooled_output)",
            "",
            "# Total loss calculation",
            "-        total_loss = 0.0",
            "+        total_loss = tf.zeros(shape=(1,), dtype=tf.float32)",
            "calculate_loss = False",
            "if labels is not None:",
            "calculate_loss = True"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('call', None), position=2, insert_id=2361897)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2361898)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2361899)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2361900)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2361901)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'zeros'), position=2, insert_id=2361902)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2361903)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=1, insert_id=2361904)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=2361905)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=3, insert_id=2361906)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=2361907)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'shape'), position=0, insert_id=2361908)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2361909)",
            "Insert(target_node=IN(type=keyword_argument), node=('tuple', None), position=2, insert_id=2361910)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=2361911)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2361912)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=2361913)",
            "Insert(target_node=IN(type=tuple), node=('(', '('), position=0, insert_id=2361914)",
            "Insert(target_node=IN(type=tuple), node=('integer', '1'), position=1, insert_id=2361915)",
            "Insert(target_node=IN(type=tuple), node=(',', ','), position=2, insert_id=2361916)",
            "Insert(target_node=IN(type=tuple), node=(')', ')'), position=3, insert_id=2361917)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2361918)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2361919)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'float32'), position=2, insert_id=2361920)",
            "Delete(target_node=ASTNode(type=float, text=0.0))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 25,
        "number": 8047,
        "neg_line": [
            "-total_loss = 0.0"
        ],
        "pos_line": [
            "+total_loss = tf.zeros(shape=(1,), dtype=tf.float32)"
        ],
        "core_change": "-total_loss = 0.0 +total_loss = tf.zeros(shape=(1,), dtype=tf.float32)",
        "core_API": "aggregation_classifier"
    },
    {
        "commit_hash": "54497ae347ea1083d754fb65adb287dcb89248d6",
        "index": "e019d965..404dc07d 100644",
        "commit_message": "Fix issues suggested by codacy. (#344)\n\n* remove two dangerous default values\n\n* fix mnist tutorial based on codacy\n\n* address hao's comments.\n\n* remove unused y_op\n\n* hao conv.py\n\n* hao prepro.py\n\n* hao files.py\n\n* remove str statement\n\n* hao example mnist\n\n* yapf\n\n* hao cifar10\n\n* hao inceptionv3\n\n* hao ptb tfrecord image processing\n\n* hao tutorials\n\n* str comment\n\n* str docs\n\n* Update README.md\n\n* remove unused code\n\n* minor fix\n\n* small fix\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def main_lstm_generate_text():",
            "",
            "if __name__ == '__main__':",
            "sess = tf.InteractiveSession()",
            "-    \"\"\"Restore a pretrained embedding matrix.\"\"\"",
            "+    # Restore a pretrained embedding matrix",
            "# main_restore_embedding_layer()",
            "-    \"\"\"How to generate text from a given context.\"\"\"",
            "+",
            "+    # How to generate text from a given context",
            "main_lstm_generate_text()",
            "",
            "#"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=string, text=\"\"\"Restore a pretrained embedding matrix.\"\"\"))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=string, text=\"\"\"How to generate text from a given context.\"\"\"))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 4,
        "number": 8052,
        "neg_line": [
            "-\"\"\"Restore a pretrained embedding matrix.\"\"\"",
            "-\"\"\"How to generate text from a given context.\"\"\""
        ],
        "pos_line": [
            "+# Restore a pretrained embedding matrix",
            "+",
            "+# How to generate text from a given context"
        ],
        "core_change": "-\"\"\"Restore a pretrained embedding matrix.\"\"\" +# Restore a pretrained embedding matrix -\"\"\"How to generate text from a given context.\"\"\" + +# How to generate text from a given context",
        "core_API": "InteractiveSession"
    },
    {
        "commit_hash": "2916e9cf0bb02beb080c07bf910b3baacc5b58ef",
        "index": "2d3c626..90135ad 100644",
        "commit_message": "Fixed problem with triplet demultiplexing. Added a test case for triplet\nloss.\n",
        "file": "facenet.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def triplet_loss(anchor, positive, negative, alpha):",
            "the triplet loss according to the FaceNet paper as a float tensor.",
            "\"\"\"",
            "with tf.variable_scope('triplet_loss'):",
            "-        pos_dist = tf.reduce_sum(tf.square(tf.sub(anchor, positive)), 1)  # Summing over distances in each batch",
            "+        pos_dist = tf.reduce_sum(tf.square(tf.sub(anchor, positive)), 1)",
            "neg_dist = tf.reduce_sum(tf.square(tf.sub(anchor, negative)), 1)",
            "",
            "basic_loss = tf.add(tf.sub(pos_dist,neg_dist), alpha)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 8063,
        "neg_line": [
            "-pos_dist = tf.reduce_sum(tf.square(tf.sub(anchor, positive)), 1)  # Summing over distances in each batch"
        ],
        "pos_line": [
            "+pos_dist = tf.reduce_sum(tf.square(tf.sub(anchor, positive)), 1)"
        ],
        "core_change": "-pos_dist = tf.reduce_sum(tf.square(tf.sub(anchor, positive)), 1)  # Summing over distances in each batch +pos_dist = tf.reduce_sum(tf.square(tf.sub(anchor, positive)), 1)",
        "core_API": "variable_scope"
    },
    {
        "commit_hash": "d257c75c848ccab4d9195300a61195cf0dfef1bf",
        "index": "df4f3b6..16ba2ff 100644",
        "commit_message": "Update export.py (#7301)\n\n* Update export.py\n\nSimplify code.\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "yolov5.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "def run(",
            "im, model = im.half(), model.half()  # to FP16",
            "model.train() if train else model.eval()  # training mode = no Detect() layer grid construction",
            "for k, m in model.named_modules():",
            "-        # if isinstance(m, Conv):  # assign export-friendly activations",
            "-        #     if isinstance(m.act, nn.SiLU):",
            "-        #         m.act = SiLU()",
            "if isinstance(m, Detect):",
            "m.inplace = inplace",
            "m.onnx_dynamic = dynamic",
            "m.export = True",
            "-            if hasattr(m, 'forward_export'):",
            "-                m.forward = m.forward_export  # assign custom forward (optional)",
            "",
            "for _ in range(2):",
            "y = model(im)  # dry runs"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=if, text=if))",
            "Delete(target_node=ASTNode(type=identifier, text=hasattr))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=m))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=string, text='forward_export'))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=identifier, text=m))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=forward))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=m))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=forward_export))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=block))",
            "Delete(target_node=ASTNode(type=if_statement))"
        ],
        "plus_line": 0,
        "minus_line": 5,
        "AST_diff_line": 23,
        "number": 8066,
        "neg_line": [
            "-# if isinstance(m, Conv):  # assign export-friendly activations",
            "-#     if isinstance(m.act, nn.SiLU):",
            "-#         m.act = SiLU()",
            "-if hasattr(m, 'forward_export'):",
            "-m.forward = m.forward_export  # assign custom forward (optional)"
        ],
        "pos_line": [],
        "core_change": "-# if isinstance(m, Conv):  # assign export-friendly activations -#     if isinstance(m.act, nn.SiLU): -#         m.act = SiLU() -if hasattr(m, 'forward_export'): -m.forward = m.forward_export  # assign custom forward (optional)",
        "core_API": "half"
    },
    {
        "commit_hash": "7c073a1b5a22ff939255fcbef947336659707033",
        "index": "34680f83..822d9adf 100644",
        "commit_message": "Fixed ragged sample weights\n\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def compute_weighted_loss(losses,",
            "losses = tf.convert_to_tensor(losses)",
            "input_dtype = losses.dtype",
            "",
            "-    if not isinstance(sample_weight, keras_tensor.KerasTensor):",
            "+    if not isinstance(sample_weight,",
            "+                      (keras_tensor.KerasTensor, tf.RaggedTensor)):",
            "sample_weight = tf.convert_to_tensor(sample_weight)",
            "",
            "# TODO(psv): Handle casting here in a better way, eg. if losses is float64"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('tuple', None), position=3, insert_id=2085865)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=4, insert_id=2085866)",
            "Insert(target_node=IN(type=tuple), node=('(', '('), position=0, insert_id=2085867)",
            "Move(target_node=IN(type=tuple), node=ASTNode(type=attribute), position=1)",
            "Insert(target_node=IN(type=tuple), node=(',', ','), position=2, insert_id=2085868)",
            "Insert(target_node=IN(type=tuple), node=('attribute', None), position=3, insert_id=2085869)",
            "Move(target_node=IN(type=tuple), node=ASTNode(type=), text=)), position=4)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2085870)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2085871)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'RaggedTensor'), position=2, insert_id=2085872)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 8067,
        "neg_line": [
            "-if not isinstance(sample_weight, keras_tensor.KerasTensor):"
        ],
        "pos_line": [
            "+if not isinstance(sample_weight,",
            "+(keras_tensor.KerasTensor, tf.RaggedTensor)):"
        ],
        "core_change": "-if not isinstance(sample_weight, keras_tensor.KerasTensor): +if not isinstance(sample_weight, +(keras_tensor.KerasTensor, tf.RaggedTensor)):",
        "core_API": "convert_to_tensor"
    },
    {
        "commit_hash": "56e1f49df918cff52707d64dcf894885250aaf89",
        "index": "c40e1935..7b2cb43f 100644",
        "commit_message": "Fix training Conditional Random Fields on GPU (#5313) (#5315)\n\nCo-authored-by: Pete <petew@allenai.org>\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ConditionalRandomField(torch.nn.Module):",
            "\"\"\"",
            "",
            "if mask is None:",
            "-            mask = torch.ones(*tags.size(), dtype=torch.bool)",
            "+            mask = torch.ones(*tags.size(), dtype=torch.bool, device=inputs.device)",
            "else:",
            "# The code below fails in weird ways if this isn't a bool tensor, so we make sure.",
            "mask = mask.to(torch.bool)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=1428)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=1429)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=1430)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1431)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=1432)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'inputs'), position=0, insert_id=1433)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1434)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=1435)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 8069,
        "neg_line": [
            "-mask = torch.ones(*tags.size(), dtype=torch.bool)"
        ],
        "pos_line": [
            "+mask = torch.ones(*tags.size(), dtype=torch.bool, device=inputs.device)"
        ],
        "core_change": "-mask = torch.ones(*tags.size(), dtype=torch.bool) +mask = torch.ones(*tags.size(), dtype=torch.bool, device=inputs.device)",
        "core_API": "ones"
    },
    {
        "commit_hash": "023f51fe16e34e0ca2b5598791ae508874d5b443",
        "index": "fac1a906e..2fd0e3c86 100644",
        "commit_message": "`blip` support for training (#21021)\n\n* `blip` support for training\n\n* remove labels creation\n\n* remove unneeded `decoder_input_ids` creation\n\n* final changes\n\n- add colab link to documentation\n- reduction = mean for loss\n\n* fix nits\n\n* update link\n\n* clearer error message\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class BlipTextModel(BlipTextPreTrainedModel):",
            "past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0",
            "",
            "if attention_mask is None:",
            "-            attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)))",
            "+            attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length))).to(device)",
            "",
            "# We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]",
            "# ourselves in which case we just need to make it broadcastable to all heads."
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1179451)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=1179452)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1179453)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to'), position=2, insert_id=1179454)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1179455)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'device'), position=1, insert_id=1179456)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1179457)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 8071,
        "neg_line": [
            "-attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)))"
        ],
        "pos_line": [
            "+attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length))).to(device)"
        ],
        "core_change": "-attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length))) +attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length))).to(device)",
        "core_API": "ones"
    },
    {
        "commit_hash": "c0ee88c37d7d019cde46d430ced23901fb0d1841",
        "index": "6a8c894..bbc5013 100644",
        "commit_message": "Some more small fixes\n",
        "file": "facenet.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def main(args):",
            "print('Total number of examples: %d' % len(label_list))",
            "",
            "# Placeholder for the learning rate",
            "-        learning_rate_placeholder = tf.placeholder(tf.float32, name='learing_rate')",
            "+        learning_rate_placeholder = tf.placeholder(tf.float32, name='learning_rate')",
            "",
            "# Build the inference graph",
            "prelogits, _ = network.inference(image_batch, args.keep_probability,"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text='learing_rate'), value=\"'learning_rate'\")"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 8074,
        "neg_line": [
            "-learning_rate_placeholder = tf.placeholder(tf.float32, name='learing_rate')"
        ],
        "pos_line": [
            "+learning_rate_placeholder = tf.placeholder(tf.float32, name='learning_rate')"
        ],
        "core_change": "-learning_rate_placeholder = tf.placeholder(tf.float32, name='learing_rate') +learning_rate_placeholder = tf.placeholder(tf.float32, name='learning_rate')",
        "core_API": "placeholder"
    },
    {
        "commit_hash": "c7fd1d9fb18862318b133c9474d37c5085850070",
        "index": "933792db..b3319b03 100644",
        "commit_message": "fix deprecations about casting & initializers in tf1.13\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class MaintainStepCounter(Callback):",
            "# ensure it exists",
            "gs_var = get_global_step_var()",
            "with tf.name_scope(None):",
            "-            with self.graph.colocate_with(gs_var):",
            "-                self.gs_incr_op = tf.assign_add(",
            "-                    gs_var, 1,",
            "-                    name=GLOBAL_STEP_INCR_OP_NAME).op",
            "+            self.gs_incr_op = tf.assign_add(",
            "+                gs_var, 1,",
            "+                name=GLOBAL_STEP_INCR_OP_NAME).op",
            "self._fetches = tf.train.SessionRunArgs(self.gs_incr_op)",
            "",
            "def _before_train(self):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=colocate_with))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=identifier, text=gs_var))"
        ],
        "plus_line": 3,
        "minus_line": 4,
        "AST_diff_line": 4,
        "number": 8078,
        "neg_line": [
            "-with self.graph.colocate_with(gs_var):",
            "-self.gs_incr_op = tf.assign_add(",
            "-gs_var, 1,",
            "-name=GLOBAL_STEP_INCR_OP_NAME).op"
        ],
        "pos_line": [
            "+self.gs_incr_op = tf.assign_add(",
            "+gs_var, 1,",
            "+name=GLOBAL_STEP_INCR_OP_NAME).op"
        ],
        "core_change": "-with self.graph.colocate_with(gs_var): -self.gs_incr_op = tf.assign_add( -gs_var, 1, -name=GLOBAL_STEP_INCR_OP_NAME).op +self.gs_incr_op = tf.assign_add( +gs_var, 1, +name=GLOBAL_STEP_INCR_OP_NAME).op",
        "core_API": "name_scope"
    },
    {
        "commit_hash": "f5f26a565b5f231e9b1f3d657c90c6d3018db551",
        "index": "d29721f25..08956aefb 100644",
        "commit_message": "Fix bug for ignore_id\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class E2E(STInterface, torch.nn.Module):",
            "hs, _, _ = self.enc(hs, ilens)",
            "return hs.squeeze(0)",
            "",
            "-    def translate(self, x, trans_args, char_list, rnnlm=None):",
            "+    def translate(self, x, trans_args, char_list, rnnlm=None, ensemble_models=[]):",
            "\"\"\"E2E beam search.",
            "",
            ":param ndarray x: input acoustic feature (T, D)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=10, insert_id=163837)",
            "Insert(target_node=ASTNode(type=parameters), node=('default_parameter', None), position=11, insert_id=163838)",
            "Insert(target_node=IN(type=default_parameter), node=('identifier', 'ensemble_models'), position=0, insert_id=163839)",
            "Insert(target_node=IN(type=default_parameter), node=('=', '='), position=1, insert_id=163840)",
            "Insert(target_node=IN(type=default_parameter), node=('list', None), position=2, insert_id=163841)",
            "Insert(target_node=IN(type=list), node=('[', '['), position=0, insert_id=163842)",
            "Insert(target_node=IN(type=list), node=(']', ']'), position=1, insert_id=163843)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 8088,
        "neg_line": [
            "-def translate(self, x, trans_args, char_list, rnnlm=None):"
        ],
        "pos_line": [
            "+def translate(self, x, trans_args, char_list, rnnlm=None, ensemble_models=[]):"
        ],
        "core_change": "-def translate(self, x, trans_args, char_list, rnnlm=None): +def translate(self, x, trans_args, char_list, rnnlm=None, ensemble_models=[]):",
        "core_API": "enc"
    },
    {
        "commit_hash": "aa750becf439224df59f80eb57aef5737cf11337",
        "index": "2b17ae07..bd493412 100644",
        "commit_message": "fix Average metric (#4624)\n\n* fix Average metric\n\n* try spawn\n\n* try again\n\n* oops\n\n* clean up, fix evalb too\n\n* use different start method for GPU vs CPU tests\n\n* add comment\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class PearsonCorrelation(Metric):",
            "A tensor of the same shape as `predictions`.",
            "\"\"\"",
            "predictions, gold_labels, mask = self.detach_tensors(predictions, gold_labels, mask)",
            "-        self._device = gold_labels.device",
            "if not is_distributed():",
            "self._predictions_labels_covariance(predictions, gold_labels, mask)",
            "self._predictions_variance(predictions, predictions, mask)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=_device))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=gold_labels))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=device))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 8091,
        "neg_line": [
            "-self._device = gold_labels.device"
        ],
        "pos_line": [],
        "core_change": "-self._device = gold_labels.device",
        "core_API": "detach_tensors"
    },
    {
        "commit_hash": "cf7b98b8077e91a40145986299c6d16f6dce2204",
        "index": "95d00de7a..b83e53006 100644",
        "commit_message": "Fix `run_clip.py` (#20234)\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Transform(torch.nn.Module):",
            "Normalize(mean, std),",
            ")",
            "",
            "-    def forward(self, x: Image) -> torch.Tensor:",
            "+    def forward(self, x) -> torch.Tensor:",
            "+        \"\"\"`x` should be an instance of `PIL.Image.Image`\"\"\"",
            "with torch.no_grad():",
            "x = self.transforms(x)",
            "return x"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=function_definition), node=('block', None), position=6, insert_id=1185515)",
            "Move(target_node=ASTNode(type=parameters), node=ASTNode(type=identifier, text=x), position=3)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=1185516)",
            "Insert(target_node=IN(type=expression_statement), node=('string', '\"\"\"`x` should be an instance of `PIL.Image.Image`\"\"\"'), position=0, insert_id=1185517)",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=identifier, text=Image))",
            "Delete(target_node=ASTNode(type=type))",
            "Delete(target_node=ASTNode(type=typed_parameter))",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 8092,
        "neg_line": [
            "-def forward(self, x: Image) -> torch.Tensor:"
        ],
        "pos_line": [
            "+def forward(self, x) -> torch.Tensor:",
            "+\"\"\"`x` should be an instance of `PIL.Image.Image`\"\"\""
        ],
        "core_change": "-def forward(self, x: Image) -> torch.Tensor: +def forward(self, x) -> torch.Tensor: +\"\"\"`x` should be an instance of `PIL.Image.Image`\"\"\"",
        "core_API": "no_grad"
    },
    {
        "commit_hash": "0b1e0fcf7aa9bb1fec62d04ddba1112d0fe0b184",
        "index": "d04560303..53dc690fd 100755",
        "commit_message": "Fix GPT-J onnx conversion (#16780)\n\n* add gptj to TOKENIZER_MAPPING_NAMES\n\n* fix int32 to float to avoid problem in onnx\n\n* Update src/transformers/models/gptj/modeling_gptj.py\n\nCo-authored-by: ChainYo <t.chaigneau.tc@gmail.com>\nCo-authored-by: lewtun <lewis.c.tunstall@gmail.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def fixed_pos_embedding(x, seq_dim=1, seq_len=None):",
            "if seq_len is None:",
            "seq_len = x.shape[seq_dim]",
            "inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2) / dim))",
            "-    sinusoid_inp = torch.einsum(\"i , j -> i j\", torch.arange(seq_len), inv_freq).to(x.device).float()",
            "+    sinusoid_inp = (",
            "+        torch.einsum(\"i , j -> i j\", torch.arange(seq_len, dtype=torch.float), inv_freq).to(x.device).float()",
            "+    )",
            "return torch.sin(sinusoid_inp), torch.cos(sinusoid_inp)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('parenthesized_expression', None), position=2, insert_id=1201416)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('(', '('), position=0, insert_id=1201417)",
            "Move(target_node=IN(type=parenthesized_expression), node=ASTNode(type=call), position=1)",
            "Insert(target_node=IN(type=parenthesized_expression), node=(')', ')'), position=2, insert_id=1201418)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1201419)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1201420)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=1201421)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1201422)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=1201423)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1201424)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1201425)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'float'), position=2, insert_id=1201426)"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 12,
        "number": 8097,
        "neg_line": [
            "-sinusoid_inp = torch.einsum(\"i , j -> i j\", torch.arange(seq_len), inv_freq).to(x.device).float()"
        ],
        "pos_line": [
            "+sinusoid_inp = (",
            "+torch.einsum(\"i , j -> i j\", torch.arange(seq_len, dtype=torch.float), inv_freq).to(x.device).float()",
            "+)"
        ],
        "core_change": "-sinusoid_inp = torch.einsum(\"i , j -> i j\", torch.arange(seq_len), inv_freq).to(x.device).float() +sinusoid_inp = ( +torch.einsum(\"i , j -> i j\", torch.arange(seq_len, dtype=torch.float), inv_freq).to(x.device).float() +)",
        "core_API": "arange"
    },
    {
        "commit_hash": "c4c532d25e012dbe6ab1ac14bca75e53e0acc621",
        "index": "c91d5868..770e4a1b 100644",
        "commit_message": "pylint -> flake8 (#3288)\n\n* pylint\n\n* update pylint\n\n* undo a lot of the raise / else\n\n* add bound on typed-ast\n\n* first mypy fixes\n\n* new flag\n\n* fix mypy errors\n\n* requirements.txt\n\n* pylint -> flake8\n\n* mypy 0.720 -> mypy 0.730\n\n* add back erroneously removed initial newline\n\n* remove .pylintrc\n\n* remove pylintrc from Dockerfile\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "from allennlp.training import NoOpTrainer",
            "",
            "class ConstantModel(Model):",
            "def forward(self, *inputs) -> Dict[str, torch.Tensor]:",
            "-        return {\"class\": torch.tensor(98)} # pylint: disable=not-callable",
            "+        return {\"class\": torch.tensor(98)}",
            "+",
            "",
            "class TestNoOpTrainer(AllenNlpTestCase):",
            "def setUp(self):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 8100,
        "neg_line": [
            "-return {\"class\": torch.tensor(98)} # pylint: disable=not-callable"
        ],
        "pos_line": [
            "+return {\"class\": torch.tensor(98)}",
            "+"
        ],
        "core_change": "-return {\"class\": torch.tensor(98)} # pylint: disable=not-callable +return {\"class\": torch.tensor(98)} +",
        "core_API": "tensor"
    },
    {
        "commit_hash": "ff98e7f83735ab2be6bd2ff614ef5329ca2e5524",
        "index": "43019e8e1..0cb0a6a86 100644",
        "commit_message": "Fix for keras\n\n",
        "file": "wandb.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tensorflow",
        "change": [
            "class Graph(object):",
            "if relevant_nodes and in_node not in relevant_nodes:",
            "# node is not part of the current network",
            "continue",
            "-                    for in_layer in nest.flatten(in_node.inbound_layers):",
            "+                    for in_layer in nest(in_node.inbound_layers):",
            "inbound_keras_node = Node.from_keras(in_layer)",
            "",
            "if (inbound_keras_node.id not in graph.nodes_by_id):"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=nest), position=0)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=flatten))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 8103,
        "neg_line": [
            "-for in_layer in nest.flatten(in_node.inbound_layers):"
        ],
        "pos_line": [
            "+for in_layer in nest(in_node.inbound_layers):"
        ],
        "core_change": "-for in_layer in nest.flatten(in_node.inbound_layers): +for in_layer in nest(in_node.inbound_layers):",
        "core_API": "flatten"
    },
    {
        "commit_hash": "0a3939883ad60db9a9ab85d7602cc3b471612799",
        "index": "040693d7..424b2b5a 100644",
        "commit_message": "Style fix\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def update_sub(x, decrement):",
            "",
            "",
            "def moving_average_update(variable, value, momentum):",
            "-    return tf.python.training.moving_averages.assign_moving_average(",
            "+    return moving_averages.assign_moving_average(",
            "variable, value, momentum)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=moving_averages), position=0)",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=python))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=training))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 8105,
        "neg_line": [
            "-return tf.python.training.moving_averages.assign_moving_average("
        ],
        "pos_line": [
            "+return moving_averages.assign_moving_average("
        ],
        "core_change": "-return tf.python.training.moving_averages.assign_moving_average( +return moving_averages.assign_moving_average(",
        "core_API": "assign_moving_average"
    },
    {
        "commit_hash": "c5cf1df9774f5052882344d37737bcd4ad189db0",
        "index": "0f0d456e30..6681828e97 100644",
        "commit_message": "small fixes for remainder in all backends, when modulus=False.\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def remainder(",
            "res_floored = tf.where(res >= 0, tf.math.floor(res), tf.math.ceil(res))",
            "diff = res - res_floored",
            "diff, x2 = ivy.promote_types_of_inputs(diff, x2)",
            "-        return tf.cast(diff * x2, x1.dtype)",
            "+        return tf.cast(tf.round(diff * x2), x1.dtype)",
            "return tf.experimental.numpy.remainder(x1, x2)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=1, insert_id=1998873)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1998874)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1998875)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=1998876)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1998877)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'round'), position=2, insert_id=1998878)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1998879)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=binary_operator), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1998880)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 8106,
        "neg_line": [
            "-return tf.cast(diff * x2, x1.dtype)"
        ],
        "pos_line": [
            "+return tf.cast(tf.round(diff * x2), x1.dtype)"
        ],
        "core_change": "-return tf.cast(diff * x2, x1.dtype) +return tf.cast(tf.round(diff * x2), x1.dtype)",
        "core_API": "where"
    },
    {
        "commit_hash": "be255de3063547df3cedbb0b98da1729a3b615f8",
        "index": "2a0b989e9..3460d85c6 100644",
        "commit_message": "Bugfix/all gather (#5221)\n\n* resolve bug\n\n* add tests\n\n* add tests\n\n* resolve flake8\n\n* update\n\n* update\n\n* remove globals\n\n* typo\n\n* Update pytorch_lightning/utilities/distributed.py\n\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\n\n* update\n\n* update\n\n* add suport int, float\n\n* update\n\n* resolve pep8\n\n* Update pytorch_lightning/core/lightning.py\n\nCo-authored-by: Adrian Wälchli <aedu.waelchli@gmail.com>\n\n* Update tests/utilities/test_all_gather_grad.py\n\nCo-authored-by: Adrian Wälchli <aedu.waelchli@gmail.com>\n\n* update doc\n\n* add bool and np.ndarray\n\n* resolve conflicts\n\n* resolve conflicts\n\n* resolve pep8\n\n* add changelog\n\n* Update pytorch_lightning/core/lightning.py\n\nCo-authored-by: Adrian Wälchli <aedu.waelchli@gmail.com>\n\nCo-authored-by: Ubuntu <ubuntu@ip-172-31-62-109.ec2.internal>\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\nCo-authored-by: Adrian Wälchli <aedu.waelchli@gmail.com>\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "import torch",
            "from pytorch_lightning import _logger as log",
            "",
            "if torch.distributed.is_available():",
            "-    from torch.distributed import ReduceOp, group",
            "+    from torch.distributed import group, ReduceOp",
            "else:",
            "class ReduceOp:",
            "SUM = None"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=dotted_name), node=ASTNode(type=import_from_statement), position=4)",
            "Move(target_node=ASTNode(type=dotted_name), node=ASTNode(type=import_from_statement), position=3)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 8110,
        "neg_line": [
            "-from torch.distributed import ReduceOp, group"
        ],
        "pos_line": [
            "+from torch.distributed import group, ReduceOp"
        ],
        "core_change": "-from torch.distributed import ReduceOp, group +from torch.distributed import group, ReduceOp",
        "core_API": "is_available"
    },
    {
        "commit_hash": "95b6d985ffb231d9a2047c039c95660913063b9d",
        "index": "e2705d35..b5d88239 100644",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestSelfAttentiveSpanExtractor:",
            "numpy.testing.assert_array_almost_equal(spans[1].data.numpy(), mean_embeddings.data.numpy())",
            "",
            "# Now test the case in which we have some masked spans in our indices.",
            "-        indices_mask = torch.LongTensor([[1, 1], [1, 0]])",
            "+        indices_mask = torch.BoolTensor([[True, True], [True, False]])",
            "span_representations = extractor(sequence_tensor, indices, span_indices_mask=indices_mask)",
            "",
            "# First element in the batch."
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=LongTensor), value='BoolTensor')",
            "Move(target_node=ASTNode(type=list), node=ASTNode(type=,, text=,), position=2)",
            "Insert(target_node=ASTNode(type=list), node=('true', 'True'), position=1, insert_id=19938)",
            "Move(target_node=ASTNode(type=list), node=ASTNode(type=,, text=,), position=2)",
            "Insert(target_node=ASTNode(type=list), node=('true', 'True'), position=3, insert_id=19939)",
            "Move(target_node=ASTNode(type=list), node=ASTNode(type=], text=]), position=4)",
            "Insert(target_node=ASTNode(type=list), node=('true', 'True'), position=1, insert_id=19940)",
            "Insert(target_node=ASTNode(type=list), node=(',', ','), position=2, insert_id=19941)",
            "Insert(target_node=ASTNode(type=list), node=('false', 'False'), position=3, insert_id=19942)",
            "Insert(target_node=ASTNode(type=list), node=(']', ']'), position=4, insert_id=19943)",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=integer, text=0))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 16,
        "number": 8115,
        "neg_line": [
            "-indices_mask = torch.LongTensor([[1, 1], [1, 0]])"
        ],
        "pos_line": [
            "+indices_mask = torch.BoolTensor([[True, True], [True, False]])"
        ],
        "core_change": "-indices_mask = torch.LongTensor([[1, 1], [1, 0]]) +indices_mask = torch.BoolTensor([[True, True], [True, False]])",
        "core_API": "assert_array_almost_equal"
    },
    {
        "commit_hash": "57c1aeb427fa116790a16c17777ce944ccb5666c",
        "index": "005428b00a..4f15b9c962 100644",
        "commit_message": "[rllib] Use suppress_output instead of run_silent.sh script for tests (#4386)\n\n* fix\n\n* enable custom loss\n\n* Update run_rllib_tests.sh\n\n* enable tests\n\n* fix action prob\n\n* Update suppress_output\n\n* fix example\n\n* fix\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class CustomLossModel(Model):",
            "input_ops = reader.tf_input_ops()",
            "",
            "# define a secondary loss by building a graph copy with weight sharing",
            "+        obs = tf.cast(input_ops[\"obs\"], tf.float32)",
            "logits, _ = self._build_layers_v2({",
            "-            \"obs\": restore_original_dimensions(input_ops[\"obs\"],",
            "-                                               self.obs_space)",
            "+            \"obs\": restore_original_dimensions(obs, self.obs_space)",
            "}, self.num_outputs, self.options)",
            "",
            "# You can also add self-supervised losses easily by referencing tensors"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=2, insert_id=2150190)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=2150191)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'obs'), position=0, insert_id=2150192)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=2150193)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=2150194)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2150195)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2150196)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2150197)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2150198)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cast'), position=2, insert_id=2150199)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2150200)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=subscript), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=2150201)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=3, insert_id=2150202)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=2150203)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2150204)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2150205)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'float32'), position=2, insert_id=2150206)",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'obs'), position=1, insert_id=2150207)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 19,
        "number": 8116,
        "neg_line": [
            "-\"obs\": restore_original_dimensions(input_ops[\"obs\"],",
            "-self.obs_space)"
        ],
        "pos_line": [
            "+obs = tf.cast(input_ops[\"obs\"], tf.float32)",
            "+\"obs\": restore_original_dimensions(obs, self.obs_space)"
        ],
        "core_change": "+obs = tf.cast(input_ops[\"obs\"], tf.float32) -\"obs\": restore_original_dimensions(input_ops[\"obs\"], -self.obs_space) +\"obs\": restore_original_dimensions(obs, self.obs_space)",
        "core_API": "tf_input_ops"
    },
    {
        "commit_hash": "5b572681fff6cb594705248f1904531814a3be9c",
        "index": "3c9dd98..7fc75c0 100644",
        "commit_message": "pseudo labeling bug fix\n\n",
        "file": "yolov3.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def detect(save_img=False):",
            "",
            "save_path = str(Path(out) / Path(p).name)",
            "s += '%gx%g ' % img.shape[2:]  # print string",
            "-            gn = torch.tensor(im0s.shape)[[1, 0, 1, 0]]  #  normalization gain whwh",
            "+            gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  #  normalization gain whwh",
            "if det is not None and len(det):",
            "# Rescale boxes from imgsz to im0 size",
            "det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=im0s), value='im0')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 8117,
        "neg_line": [
            "-gn = torch.tensor(im0s.shape)[[1, 0, 1, 0]]  #  normalization gain whwh"
        ],
        "pos_line": [
            "+gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  #  normalization gain whwh"
        ],
        "core_change": "-gn = torch.tensor(im0s.shape)[[1, 0, 1, 0]]  #  normalization gain whwh +gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  #  normalization gain whwh",
        "core_API": "tensor"
    },
    {
        "commit_hash": "16553fd759e70d93824407f18cdea419703d85d4",
        "index": "942c6a5..0a60edc 100644",
        "commit_message": "Fix typo for named argument\n",
        "file": "gcn.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "import tensorflow as tf",
            "",
            "def masked_softmax_cross_entropy(preds, labels, mask):",
            "\"\"\"Softmax cross-entropy loss with masking.\"\"\"",
            "-    loss = tf.nn.softmax_cross_entropy_with_logits(logits=preds, lables=labels)",
            "+    loss = tf.nn.softmax_cross_entropy_with_logits(logits=preds, labels=labels)",
            "mask = tf.cast(mask, dtype=tf.float32)",
            "mask /= tf.reduce_mean(mask)",
            "loss *= mask"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=lables), value='labels')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 8118,
        "neg_line": [
            "-loss = tf.nn.softmax_cross_entropy_with_logits(logits=preds, lables=labels)"
        ],
        "pos_line": [
            "+loss = tf.nn.softmax_cross_entropy_with_logits(logits=preds, labels=labels)"
        ],
        "core_change": "-loss = tf.nn.softmax_cross_entropy_with_logits(logits=preds, lables=labels) +loss = tf.nn.softmax_cross_entropy_with_logits(logits=preds, labels=labels)",
        "core_API": "softmax_cross_entropy_with_logits"
    },
    {
        "commit_hash": "c37bef6982eaeca677cb252a2b1b37bcc0053e4d",
        "index": "24cf8e63..da54bb67 100644",
        "commit_message": "feat: modelstore + some fixes (#1836)\n\n* fix: exceptions lazy loader\n\n* use pyyaml, clean up configurations, add LocalBentoMgr interface\n\n* build: add interface\n\n* build: prototype attrs\n\n* refactor: renaming\n\n* feat: list models\n\n* fix: logging\n\n* fix: ci\n\n* revert: cherry-pick chaoyu commits + finish base modelstore\n\n* feat: finish modelstore + model_yaml interface\n\nCo-authored-by: Chaoyu <paranoyang@gmail.com>\n",
        "file": "BentoML.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tensorflow",
        "change": [
            "class KerasModel(Model):",
            "",
            "return model",
            "",
            "-    @catch_exceptions(",
            "-        catch_exc=ModuleNotFoundError, throw_exc=MissingDependencyException, msg=_exc",
            "-    )",
            "def save(self, path: PathType) -> None:",
            "tf.compat.v1.keras.backend.get_session()"
        ],
        "hunk_index": 4,
        "AST_diff": [
            "Move(target_node=ASTNode(type=module), node=ASTNode(type=function_definition), position=2)",
            "Delete(target_node=ASTNode(type=@, text=@))",
            "Delete(target_node=ASTNode(type=identifier, text=catch_exceptions))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=catch_exc))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=ModuleNotFoundError))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=throw_exc))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=MissingDependencyException))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=msg))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=_exc))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=decorator))",
            "Delete(target_node=ASTNode(type=decorated_definition))"
        ],
        "plus_line": 0,
        "minus_line": 3,
        "AST_diff_line": 23,
        "number": 8125,
        "neg_line": [
            "-@catch_exceptions(",
            "-catch_exc=ModuleNotFoundError, throw_exc=MissingDependencyException, msg=_exc",
            "-)"
        ],
        "pos_line": [],
        "core_change": "-@catch_exceptions( -catch_exc=ModuleNotFoundError, throw_exc=MissingDependencyException, msg=_exc -)",
        "core_API": "get_session"
    },
    {
        "commit_hash": "c1c43238084bd85c668ce4b6252b3683244e319f",
        "index": "= 0",
        "commit_message": "Fixed style\n\n",
        "file": "PySyft.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def perform_analysis_torch(preds, indices, noise_eps=0.1, delta=1e-5, moments=8,",
            "data_ind_eps_list = (data_ind_log_mgf - math.log(delta)) / l_list",
            "",
            "return min(eps_list_nm), min(data_ind_eps_list)",
            "-",
            "-",
            "-",
            "-"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 0,
        "minus_line": 0,
        "AST_diff_line": 17,
        "number": 8128,
        "neg_line": [
            "-",
            "-",
            "-",
            "-"
        ],
        "pos_line": [],
        "core_change": "- - - -",
        "core_API": "log"
    },
    {
        "commit_hash": "9f47f203f0df39db1b8e87fa8145068dfc08774f",
        "index": "ca1b3eae0b..b60cd2a1bc 100644",
        "commit_message": "fix test_shape and lints\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def sum(",
            "]",
            "),",
            "dtype=dtype,",
            "-                out=out,",
            ")",
            "return torch.sum(input=x, dim=axis, dtype=dtype, keepdim=keepdims)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=out))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=out))",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 8129,
        "neg_line": [
            "-out=out,"
        ],
        "pos_line": [],
        "core_change": "-out=out,",
        "core_API": "sum"
    },
    {
        "commit_hash": "95b6d985ffb231d9a2047c039c95660913063b9d",
        "index": "ac871367..d449154a 100644",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class F1MeasureTest(AllenNlpTestCase):",
            "assert_allclose(f1, 0.8)",
            "",
            "# Test the same thing with a mask:",
            "-        mask = torch.tensor([[0, 1, 0], [1, 1, 1]], device=device)",
            "+        mask = torch.BoolTensor([[False, True, False], [True, True, True]], device=device)",
            "f1_measure(predictions, targets, mask)",
            "precision, recall, f1 = f1_measure.get_metric()",
            "assert f1_measure._true_positives == 1.0"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tensor), value='BoolTensor')",
            "Insert(target_node=ASTNode(type=list), node=('false', 'False'), position=1, insert_id=20067)",
            "Insert(target_node=ASTNode(type=list), node=('true', 'True'), position=4, insert_id=20068)",
            "Insert(target_node=ASTNode(type=list), node=('false', 'False'), position=6, insert_id=20069)",
            "Insert(target_node=ASTNode(type=list), node=('true', 'True'), position=1, insert_id=20070)",
            "Insert(target_node=ASTNode(type=list), node=('true', 'True'), position=4, insert_id=20071)",
            "Insert(target_node=ASTNode(type=list), node=('true', 'True'), position=6, insert_id=20072)",
            "Delete(target_node=ASTNode(type=integer, text=0))",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=integer, text=0))",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=integer, text=1))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 13,
        "number": 8130,
        "neg_line": [
            "-mask = torch.tensor([[0, 1, 0], [1, 1, 1]], device=device)"
        ],
        "pos_line": [
            "+mask = torch.BoolTensor([[False, True, False], [True, True, True]], device=device)"
        ],
        "core_change": "-mask = torch.tensor([[0, 1, 0], [1, 1, 1]], device=device) +mask = torch.BoolTensor([[False, True, False], [True, True, True]], device=device)",
        "core_API": "tensor"
    },
    {
        "commit_hash": "4963b88f64c4968061a04dc00d741168b506467c",
        "index": "172eac3eeb..3e4573e118 100644",
        "commit_message": "Fixed test discovery errors (#7136)\n\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tensorflow",
        "change": [
            "def test_tensorflow_l2_normalize(",
            "",
            "# trace",
            "@handle_frontend_test(",
            "+    fn_tree=\"tensorflow.linalg.trace\",",
            "dtype_and_input=_get_dtype_and_matrix(),",
            "num_positional_args=helpers.num_positional_args(",
            "fn_name=\"ivy.functional.frontends.tensorflow.linalg.trace\""
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=ERROR), node=('keyword_argument', None), position=3, insert_id=2425804)",
            "Insert(target_node=ASTNode(type=ERROR), node=(',', ','), position=4, insert_id=2425805)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'fn_tree'), position=0, insert_id=2425806)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2425807)",
            "Insert(target_node=IN(type=keyword_argument), node=('string', '\"tensorflow.linalg.trace\"'), position=2, insert_id=2425808)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 5,
        "number": 8138,
        "neg_line": [],
        "pos_line": [
            "+fn_tree=\"tensorflow.linalg.trace\","
        ],
        "core_change": "+fn_tree=\"tensorflow.linalg.trace\",",
        "core_API": "num_positional_args"
    },
    {
        "commit_hash": "de4159a318f9a4f2f474733306c00ac326e643a7",
        "index": "18455d5a5..4133dcba6 100644",
        "commit_message": "More TF int dtype fixes (#20384)\n\n* Add a test to ensure int dummy inputs are int64\n\n* Move the test into the existing int64 test and update a lot of existing dummies\n\n* Fix remaining dummies\n\n* Fix remaining dummies\n\n* Test for int64 serving sigs as well\n\n* Update core tests to use tf.int64\n\n* Add better messages to the assertions\n\n* Update all serving sigs to int64\n\n* More sneaky hiding tf.int32s\n\n* Add an optional int32 signature in save_pretrained\n\n* make fixup\n\n* Add Amy's suggestions\n\n* Switch all serving sigs back to tf.int32\n\n* Switch all dummies to tf.int32\n\n* Adjust tests to check for tf.int32 instead of tf.int64\n\n* Fix base dummy_inputs dtype\n\n* Start casting to tf.int32 in input_processing\n\n* Change dtype for unpack_inputs test\n\n* Add proper tf.int32 test\n\n* Make the alternate serving signature int64\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFVisionEncoderDecoderModel(TFPreTrainedModel, TFCausalLanguageModelingLos",
            "Returns:",
            "`Dict[str, tf.Tensor]`: The dummy inputs.",
            "\"\"\"",
            "-        decoder_input_ids = tf.constant(DUMMY_INPUTS)",
            "+        decoder_input_ids = tf.constant(DUMMY_INPUTS, dtype=tf.int32)",
            "batch_size, seq_len = decoder_input_ids.shape",
            "",
            "VISION_DUMMY_INPUTS = tf.random.uniform("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=2358207)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=2358208)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=2358209)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2358210)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=2358211)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2358212)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2358213)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'int32'), position=2, insert_id=2358214)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 8140,
        "neg_line": [
            "-decoder_input_ids = tf.constant(DUMMY_INPUTS)"
        ],
        "pos_line": [
            "+decoder_input_ids = tf.constant(DUMMY_INPUTS, dtype=tf.int32)"
        ],
        "core_change": "-decoder_input_ids = tf.constant(DUMMY_INPUTS) +decoder_input_ids = tf.constant(DUMMY_INPUTS, dtype=tf.int32)",
        "core_API": "constant"
    },
    {
        "commit_hash": "a0028d3f68410f05f42a0071cd441243c167a4bb",
        "index": "80aa636dbb..f484792e52 100644",
        "commit_message": "fixes to elementwise\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def subtract(",
            "x1, x2 = ivy.promote_types_of_inputs(x1, x2)",
            "if alpha not in (1, None):",
            "x2 = x2 * alpha",
            "-    return tf.experimental.numpy.subtract(x1, x2)",
            "+    return tf.subtract(x1, x2)",
            "",
            "",
            "def tan("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=alpha), position=0)",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=ERROR), position=1)",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=., text=.), position=2)",
            "Delete(target_node=ASTNode(type=identifier, text=experimental))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=numpy))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 8144,
        "neg_line": [
            "-return tf.experimental.numpy.subtract(x1, x2)"
        ],
        "pos_line": [
            "+return tf.subtract(x1, x2)"
        ],
        "core_change": "-return tf.experimental.numpy.subtract(x1, x2) +return tf.subtract(x1, x2)",
        "core_API": "promote_types_of_inputs"
    },
    {
        "commit_hash": "470b516884cc9a4cc2636537fcc6f6fc3689b17b",
        "index": "cbf32389..be683aa4 100644",
        "commit_message": "reworked runner module, fixed some test issues\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class MemoryModel(Model):",
            "# e.g. F T F F F F",
            "",
            "# Store the steps until end of the episode(s) determined by the input terminal signals (True starts new count).",
            "-        lengths = tf.scan(fn=len_, elems=terminal, initializer=0)",
            "+        lengths = tf.scan(",
            "+            fn=len_, elems=terminal,",
            "+            initializer=tf.zeros_like(tensor=terminal[0], dtype=tf.int32)",
            "+        )",
            "# e.g. 1 1 2 3 4 5",
            "off_horizon = tf.greater(lengths, tf.fill(dims=tf.shape(lengths), value=horizon))",
            "# e.g. F F F F T T"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=keyword_argument), node=('=', '='), position=1, insert_id=2233220)",
            "Insert(target_node=ASTNode(type=keyword_argument), node=('call', None), position=2, insert_id=2233221)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2233222)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2233223)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2233224)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2233225)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'zeros_like'), position=2, insert_id=2233226)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2233227)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=1, insert_id=2233228)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=2233229)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=3, insert_id=2233230)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=2233231)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'tensor'), position=0, insert_id=2233232)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type==, text==), position=1)",
            "Insert(target_node=IN(type=keyword_argument), node=('subscript', None), position=2, insert_id=2233233)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=2233234)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2233235)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=2233236)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'terminal'), position=0, insert_id=2233237)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=2233238)",
            "Move(target_node=IN(type=subscript), node=ASTNode(type=integer, text=0), position=2)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=2233239)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2233240)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2233241)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'int32'), position=2, insert_id=2233242)"
        ],
        "plus_line": 4,
        "minus_line": 1,
        "AST_diff_line": 25,
        "number": 8149,
        "neg_line": [
            "-lengths = tf.scan(fn=len_, elems=terminal, initializer=0)"
        ],
        "pos_line": [
            "+lengths = tf.scan(",
            "+fn=len_, elems=terminal,",
            "+initializer=tf.zeros_like(tensor=terminal[0], dtype=tf.int32)",
            "+)"
        ],
        "core_change": "-lengths = tf.scan(fn=len_, elems=terminal, initializer=0) +lengths = tf.scan( +fn=len_, elems=terminal, +initializer=tf.zeros_like(tensor=terminal[0], dtype=tf.int32) +)",
        "core_API": "scan"
    },
    {
        "commit_hash": "95b6d985ffb231d9a2047c039c95660913063b9d",
        "index": "5a66c4eb..4cfeb379 100644",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class IntraSentenceAttentionEncoder(Seq2SeqEncoder):",
            "return False",
            "",
            "@overrides",
            "-    def forward(self, tokens: torch.Tensor, mask: torch.Tensor):",
            "+    def forward(self, tokens: torch.Tensor, mask: torch.BoolTensor):",
            "batch_size, sequence_length, _ = tokens.size()",
            "# Shape: (batch_size, sequence_length, sequence_length)",
            "similarity_matrix = self._matrix_attention(tokens, tokens)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=Tensor), value='BoolTensor')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 8150,
        "neg_line": [
            "-def forward(self, tokens: torch.Tensor, mask: torch.Tensor):"
        ],
        "pos_line": [
            "+def forward(self, tokens: torch.Tensor, mask: torch.BoolTensor):"
        ],
        "core_change": "-def forward(self, tokens: torch.Tensor, mask: torch.Tensor): +def forward(self, tokens: torch.Tensor, mask: torch.BoolTensor):",
        "core_API": "size"
    },
    {
        "commit_hash": "d3cb28886ac68beba9a6646b422a4d727b056c0c",
        "index": "d261104ac..ea966b7e8 100644",
        "commit_message": "Not use -1e4 as attn mask (#17306)\n\n* Use torch.finfo(self.dtype).min\n\n* for GPTNeoX\n\n* for Albert\n\n* For Splinter\n\n* Update src/transformers/models/data2vec/modeling_data2vec_audio.py\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* fix -inf used in Bart-like models\n\n* Fix a few remaining -inf\n\n* more fix\n\n* clean up\n\n* For CLIP\n\n* For FSMT\n\n* clean up\n\n* fix test\n\n* Add dtype argument and use it for LayoutLMv3\n\n* update FlaxLongT5Attention\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DetrMHAttentionMap(nn.Module):",
            "weights = torch.einsum(\"bqnc,bnchw->bqnhw\", queries_per_head * self.normalize_fact, keys_per_head)",
            "",
            "if mask is not None:",
            "-            weights.masked_fill_(mask.unsqueeze(1).unsqueeze(1), float(\"-inf\"))",
            "+            weights.masked_fill_(mask.unsqueeze(1).unsqueeze(1), torch.finfo(weights.dtype).min)",
            "weights = nn.functional.softmax(weights.flatten(2), dim=-1).view(weights.size())",
            "weights = self.dropout(weights)",
            "return weights"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=3, insert_id=1196047)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1196048)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'min'), position=2, insert_id=1196049)",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1196050)",
            "Update(target_node=ASTNode(type=identifier, text=float), value='torch')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=float), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1196051)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'finfo'), position=2, insert_id=1196052)",
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=1, insert_id=1196053)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'weights'), position=0, insert_id=1196054)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1196055)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dtype'), position=2, insert_id=1196056)",
            "Delete(target_node=ASTNode(type=string, text=\"-inf\"))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 14,
        "number": 8153,
        "neg_line": [
            "-weights.masked_fill_(mask.unsqueeze(1).unsqueeze(1), float(\"-inf\"))"
        ],
        "pos_line": [
            "+weights.masked_fill_(mask.unsqueeze(1).unsqueeze(1), torch.finfo(weights.dtype).min)"
        ],
        "core_change": "-weights.masked_fill_(mask.unsqueeze(1).unsqueeze(1), float(\"-inf\")) +weights.masked_fill_(mask.unsqueeze(1).unsqueeze(1), torch.finfo(weights.dtype).min)",
        "core_API": "einsum"
    },
    {
        "commit_hash": "95ab1563a510771eeb7abff2645a826e9981d85f",
        "index": "e871d3a8..6382eba0 100644",
        "commit_message": "support prefix for FeedfreeInferenceRunner\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Trainer(object):",
            "summary (tf.Summary or str): a summary object, or a str which will",
            "be interpreted as a serialized tf.Summary protobuf.",
            "\"\"\"",
            "-        if isinstance(summary, six.string_types):",
            "+        if isinstance(summary, six.binary_type):",
            "summary = tf.Summary.FromString(summary)",
            "+        assert isinstance(summary, tf.Summary), type(summary)",
            "for val in summary.value:",
            "if val.WhichOneof('value') == 'simple_value':",
            "val.tag = re.sub('tower[p0-9]+/', '', val.tag)   # TODO move to subclasses"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('assert_statement', None), position=4, insert_id=2307622)",
            "Insert(target_node=IN(type=assert_statement), node=('assert', 'assert'), position=0, insert_id=2307623)",
            "Insert(target_node=IN(type=assert_statement), node=('call', None), position=1, insert_id=2307624)",
            "Insert(target_node=IN(type=assert_statement), node=(',', ','), position=2, insert_id=2307625)",
            "Insert(target_node=IN(type=assert_statement), node=('call', None), position=3, insert_id=2307626)",
            "Insert(target_node=IN(type=call), node=('identifier', 'isinstance'), position=0, insert_id=2307627)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2307628)",
            "Insert(target_node=IN(type=call), node=('identifier', 'type'), position=0, insert_id=2307629)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2307630)",
            "Insert(target_node=ASTNode(type=ERROR), node=('attribute', None), position=2, insert_id=2307631)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2307632)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'summary'), position=1, insert_id=2307633)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=2307634)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=attribute), position=3)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=2307635)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2307636)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'summary'), position=1, insert_id=2307637)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2307638)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2307639)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2307640)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Summary'), position=2, insert_id=2307641)",
            "Update(target_node=ASTNode(type=identifier, text=string_types), value='binary_type')"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 22,
        "number": 8155,
        "neg_line": [
            "-if isinstance(summary, six.string_types):"
        ],
        "pos_line": [
            "+if isinstance(summary, six.binary_type):",
            "+assert isinstance(summary, tf.Summary), type(summary)"
        ],
        "core_change": "-if isinstance(summary, six.string_types): +if isinstance(summary, six.binary_type): +assert isinstance(summary, tf.Summary), type(summary)",
        "core_API": "FromString"
    },
    {
        "commit_hash": "05625e3e6e660909e4df833de1f7b49019496a68",
        "index": "5889e08d..5f5c9859 100644",
        "commit_message": "fix #4708 (#4805)\n\nCo-authored-by: moslehpour <moslehpour@meta.com>\n",
        "file": "fairseq.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SinusoidalPositionalEmbedding(nn.Module):",
            "if self.onnx_trace:",
            "flat_embeddings = self.weights.detach().index_select(0, positions.view(-1))",
            "embedding_shape = torch.cat(",
            "-                (bsz.view(1), seq_len.view(1), torch.tensor([-1], dtype=torch.long))",
            "+                (bsz, seq_len, torch.tensor([-1], dtype=torch.long))",
            ")",
            "embeddings = torch.onnx.operators.reshape_from_tensor_shape(",
            "flat_embeddings, embedding_shape"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=tuple), node=ASTNode(type=identifier, text=bsz), position=1)",
            "Move(target_node=ASTNode(type=tuple), node=ASTNode(type=identifier, text=seq_len), position=4)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=view))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=view))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 18,
        "number": 8157,
        "neg_line": [
            "-(bsz.view(1), seq_len.view(1), torch.tensor([-1], dtype=torch.long))"
        ],
        "pos_line": [
            "+(bsz, seq_len, torch.tensor([-1], dtype=torch.long))"
        ],
        "core_change": "-(bsz.view(1), seq_len.view(1), torch.tensor([-1], dtype=torch.long)) +(bsz, seq_len, torch.tensor([-1], dtype=torch.long))",
        "core_API": "detach"
    },
    {
        "commit_hash": "d3cb28886ac68beba9a6646b422a4d727b056c0c",
        "index": "118ab1fe5..983efada2 100755",
        "commit_message": "Not use -1e4 as attn mask (#17306)\n\n* Use torch.finfo(self.dtype).min\n\n* for GPTNeoX\n\n* for Albert\n\n* For Splinter\n\n* Update src/transformers/models/data2vec/modeling_data2vec_audio.py\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* fix -inf used in Bart-like models\n\n* Fix a few remaining -inf\n\n* more fix\n\n* clean up\n\n* For CLIP\n\n* For FSMT\n\n* clean up\n\n* fix test\n\n* Add dtype argument and use it for LayoutLMv3\n\n* update FlaxLongT5Attention\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class VisualBertRegionToPhraseAttention(nn.Module):",
            "def forward(self, query, key, attention_mask):",
            "attention_mask = attention_mask.to(query.dtype)",
            "attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)",
            "-        attention_mask = (1.0 - attention_mask) * -10000.0",
            "+        attention_mask = (1.0 - attention_mask) * torch.finfo(query.dtype).min",
            "",
            "mixed_query_layer = self.query(query)",
            "mixed_key_layer = self.key(key)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=binary_operator), node=('attribute', None), position=2, insert_id=1197248)",
            "Insert(target_node=IN(type=attribute), node=('call', None), position=0, insert_id=1197249)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1197250)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'min'), position=2, insert_id=1197251)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1197252)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1197253)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1197254)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1197255)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'finfo'), position=2, insert_id=1197256)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1197257)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=1197258)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1197259)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'query'), position=0, insert_id=1197260)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1197261)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dtype'), position=2, insert_id=1197262)",
            "Delete(target_node=ASTNode(type=-, text=-))",
            "Delete(target_node=ASTNode(type=float, text=10000.0))",
            "Delete(target_node=ASTNode(type=unary_operator))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 18,
        "number": 8158,
        "neg_line": [
            "-attention_mask = (1.0 - attention_mask) * -10000.0"
        ],
        "pos_line": [
            "+attention_mask = (1.0 - attention_mask) * torch.finfo(query.dtype).min"
        ],
        "core_change": "-attention_mask = (1.0 - attention_mask) * -10000.0 +attention_mask = (1.0 - attention_mask) * torch.finfo(query.dtype).min",
        "core_API": "to"
    },
    {
        "commit_hash": "04e0a63d39dc57dfbc88ecf57b095acf1406768b",
        "index": "0fcb7650..468035e3 100644",
        "commit_message": "fix: typo miss codespelling documentation and commented code\n\n",
        "file": "tf-quant-finance.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class ParabolicEquationStepperTest(tf.test.TestCase, parameterized.TestCase):",
            "sizes=[100],",
            "dtype=np.float64))",
            "",
            "-    initial_values = tf.math.exp(grid[0])  # Initial condtion",
            "+    initial_values = tf.math.exp(grid[0])  # Initial condition",
            "time_step = 0.001",
            "final_t = 0.1",
            "if default_bc == 'left':"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 8162,
        "neg_line": [
            "-initial_values = tf.math.exp(grid[0])  # Initial condtion"
        ],
        "pos_line": [
            "+initial_values = tf.math.exp(grid[0])  # Initial condition"
        ],
        "core_change": "-initial_values = tf.math.exp(grid[0])  # Initial condtion +initial_values = tf.math.exp(grid[0])  # Initial condition",
        "core_API": "exp"
    },
    {
        "commit_hash": "74f65ab7a396f6e964fa514c8c84721d55059763",
        "index": "21d27ae3..89a45755 100644",
        "commit_message": "Fix checkpoint loading for HuggingFace encoders (#3010)\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "ludwig.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class InputFeature(BaseFeature, LudwigModule, ABC):",
            "def update_config_with_metadata(feature_config, feature_metadata, *args, **kwargs):",
            "pass",
            "",
            "+    def update_config_after_module_init(self, feature_config):",
            "+        \"\"\"Updates the config after the torch.nn.Module objects have been initialized.\"\"\"",
            "+        pass",
            "+",
            "def initialize_encoder(self, encoder_config):",
            "encoder_cls = get_encoder_cls(self.type(), encoder_config.type)",
            "encoder_schema = encoder_cls.get_schema_cls().Schema()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('pass_statement', None), position=2, insert_id=1418614)",
            "Insert(target_node=ASTNode(type=module), node=('function_definition', None), position=3, insert_id=1418615)",
            "Insert(target_node=IN(type=pass_statement), node=('pass', 'pass'), position=0, insert_id=1418616)",
            "Insert(target_node=IN(type=function_definition), node=('def', 'def'), position=0, insert_id=1418617)",
            "Insert(target_node=IN(type=function_definition), node=('identifier', 'update_config_after_module_init'), position=1, insert_id=1418618)",
            "Insert(target_node=IN(type=function_definition), node=('parameters', None), position=2, insert_id=1418619)",
            "Insert(target_node=IN(type=function_definition), node=(':', ':'), position=3, insert_id=1418620)",
            "Insert(target_node=IN(type=function_definition), node=('block', None), position=4, insert_id=1418621)",
            "Insert(target_node=IN(type=parameters), node=('(', '('), position=0, insert_id=1418622)",
            "Insert(target_node=IN(type=parameters), node=('identifier', 'self'), position=1, insert_id=1418623)",
            "Insert(target_node=IN(type=parameters), node=(',', ','), position=2, insert_id=1418624)",
            "Insert(target_node=IN(type=parameters), node=('identifier', 'feature_config'), position=3, insert_id=1418625)",
            "Insert(target_node=IN(type=parameters), node=(')', ')'), position=4, insert_id=1418626)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=1418627)",
            "Move(target_node=IN(type=block), node=ASTNode(type=pass_statement), position=1)",
            "Insert(target_node=IN(type=expression_statement), node=('string', '\"\"\"Updates the config after the torch.nn.Module objects have been initialized.\"\"\"'), position=0, insert_id=1418628)"
        ],
        "plus_line": 3,
        "minus_line": 0,
        "AST_diff_line": 16,
        "number": 8164,
        "neg_line": [],
        "pos_line": [
            "+def update_config_after_module_init(self, feature_config):",
            "+\"\"\"Updates the config after the torch.nn.Module objects have been initialized.\"\"\"",
            "+pass",
            "+"
        ],
        "core_change": "+def update_config_after_module_init(self, feature_config): +\"\"\"Updates the config after the torch.nn.Module objects have been initialized.\"\"\" +pass +",
        "core_API": "type"
    },
    {
        "commit_hash": "480469c8cf18649afad9ad8e2d376726ba811034",
        "index": "12d292fa..27220212 100644",
        "commit_message": "Fix: formulas should use spots and not forwards.\n\n",
        "file": "tf-quant-finance.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def bjerksund_stensland(*,",
            "volatilities=volatilities,",
            "strikes=strikes,",
            "expiries=expiries,",
            "-                forwards=forwards,",
            "+                spots=spots,",
            "discount_rates=discount_rates,",
            "cost_of_carries=cost_of_carries,",
            "is_call_options=is_call_options),",
            "# For put options, adjust inputs according to call-put transformation",
            "# function:  P(S, X, T, r, b, sigma) = C(X, S, T, r - b, -b, sigma)",
            "tf.where(is_call_options,",
            "-                bjerksund_stensland_model(forwards, strikes, expiries, discount_rates,",
            "+                bjerksund_stensland_model(spots, strikes, expiries, discount_rates,",
            "cost_of_carries, volatilities),",
            "-                bjerksund_stensland_model(strikes, forwards, expiries, discount_rates -",
            "+                bjerksund_stensland_model(strikes, spots, expiries, discount_rates -",
            "cost_of_carries, -cost_of_carries, volatilities)))",
            "",
            "return american_prices"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=forwards), value='spots')",
            "Update(target_node=ASTNode(type=identifier, text=forwards), value='spots')",
            "Update(target_node=ASTNode(type=identifier, text=forwards), value='spots')",
            "Update(target_node=ASTNode(type=identifier, text=forwards), value='spots')"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 4,
        "number": 8166,
        "neg_line": [
            "-forwards=forwards,",
            "-bjerksund_stensland_model(forwards, strikes, expiries, discount_rates,",
            "-bjerksund_stensland_model(strikes, forwards, expiries, discount_rates -"
        ],
        "pos_line": [
            "+spots=spots,",
            "+bjerksund_stensland_model(spots, strikes, expiries, discount_rates,",
            "+bjerksund_stensland_model(strikes, spots, expiries, discount_rates -"
        ],
        "core_change": "-forwards=forwards, +spots=spots, -bjerksund_stensland_model(forwards, strikes, expiries, discount_rates, +bjerksund_stensland_model(spots, strikes, expiries, discount_rates, -bjerksund_stensland_model(strikes, forwards, expiries, discount_rates - +bjerksund_stensland_model(strikes, spots, expiries, discount_rates -",
        "core_API": "where"
    },
    {
        "commit_hash": "f38aaa5be05104ddfc3452249eeb521f139ded99",
        "index": "fc7d7a1a..29e91726 100644",
        "commit_message": "improved global tensor handling, various other fixes\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def detectMinVal(input_mat, var, threshold=1e-6, name='', debug=False):",
            "input_mat_clipped = clipoutNeg(input_mat, threshold)",
            "",
            "if debug:",
            "-        input_mat_clipped = tf.cond(tf.logical_or(tf.greater(eigen_ratio, 0.), tf.less(eigen_ratio, -500)), lambda: input_mat_clipped, lambda: tf.Print(",
            "+        input_mat_clipped = self.cond(tf.logical_or(tf.greater(eigen_ratio, 0.), tf.less(eigen_ratio, -500)), lambda: input_mat_clipped, lambda: tf.Print(",
            "input_mat_clipped, [tf.convert_to_tensor('screwed ratio ' + name + ' eigen values!!!'), tf.convert_to_tensor(var.name), eigen_min, eigen_max, eigen_ratio]))",
            "",
            "return input_mat_clipped"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tf), value='self')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 8167,
        "neg_line": [
            "-input_mat_clipped = tf.cond(tf.logical_or(tf.greater(eigen_ratio, 0.), tf.less(eigen_ratio, -500)), lambda: input_mat_clipped, lambda: tf.Print("
        ],
        "pos_line": [
            "+input_mat_clipped = self.cond(tf.logical_or(tf.greater(eigen_ratio, 0.), tf.less(eigen_ratio, -500)), lambda: input_mat_clipped, lambda: tf.Print("
        ],
        "core_change": "-input_mat_clipped = tf.cond(tf.logical_or(tf.greater(eigen_ratio, 0.), tf.less(eigen_ratio, -500)), lambda: input_mat_clipped, lambda: tf.Print( +input_mat_clipped = self.cond(tf.logical_or(tf.greater(eigen_ratio, 0.), tf.less(eigen_ratio, -500)), lambda: input_mat_clipped, lambda: tf.Print(",
        "core_API": "cond"
    },
    {
        "commit_hash": "209da29e0a156494f1048f3526e2cc5fa7ad0e7c",
        "index": "77b5bcf7..dcd5acf2 100644",
        "commit_message": "[DQN] fix the layout when channel>1\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def maskrcnn_loss(mask_logits, fg_labels, fg_target_masks):",
            "\"\"\"",
            "Args:",
            "mask_logits: #fg x #category xhxw",
            "-        fg_labels: #fg, in 1~#class",
            "+        fg_labels: #fg, in 1~#class, int64",
            "fg_target_masks: #fgxhxw, int",
            "\"\"\"",
            "-    num_fg = tf.size(fg_labels)",
            "-    indices = tf.stack([tf.range(num_fg), tf.to_int32(fg_labels) - 1], axis=1)  # #fgx2",
            "+    num_fg = tf.size(fg_labels, out_type=tf.int64)",
            "+    indices = tf.stack([tf.range(num_fg), fg_labels - 1], axis=1)  # #fgx2",
            "mask_logits = tf.gather_nd(mask_logits, indices)  # #fgxhxw",
            "mask_probs = tf.sigmoid(mask_logits)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"\"\"\nArgs:\nmask_logits: #fg x #category xhxw\n        fg_labels: #fg, in 1~#class\nfg_target_masks: #fgxhxw, int\n\"\"\"), value='\"\"\"\\nArgs:\\nmask_logits: #fg x #category xhxw\\n        fg_labels: #fg, in 1~#class, int64\\nfg_target_masks: #fgxhxw, int\\n\"\"\"')",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=2281119)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=2281120)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'out_type'), position=0, insert_id=2281121)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2281122)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=2281123)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2281124)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2281125)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'int64'), position=2, insert_id=2281126)",
            "Move(target_node=ASTNode(type=binary_operator), node=ASTNode(type=identifier, text=fg_labels), position=0)",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=to_int32))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 18,
        "number": 8170,
        "neg_line": [
            "-fg_labels: #fg, in 1~#class",
            "-num_fg = tf.size(fg_labels)",
            "-indices = tf.stack([tf.range(num_fg), tf.to_int32(fg_labels) - 1], axis=1)  # #fgx2"
        ],
        "pos_line": [
            "+fg_labels: #fg, in 1~#class, int64",
            "+num_fg = tf.size(fg_labels, out_type=tf.int64)",
            "+indices = tf.stack([tf.range(num_fg), fg_labels - 1], axis=1)  # #fgx2"
        ],
        "core_change": "-fg_labels: #fg, in 1~#class +fg_labels: #fg, in 1~#class, int64 -num_fg = tf.size(fg_labels) -indices = tf.stack([tf.range(num_fg), tf.to_int32(fg_labels) - 1], axis=1)  # #fgx2 +num_fg = tf.size(fg_labels, out_type=tf.int64) +indices = tf.stack([tf.range(num_fg), fg_labels - 1], axis=1)  # #fgx2",
        "core_API": "size"
    },
    {
        "commit_hash": "e90555840658638832e284a1ffc3fba1a1ade350",
        "index": "4c3d3da60..43019e8e1 100644",
        "commit_message": "Resuming bug fixes, keras callback with tf2, lazy load fastai\n\n",
        "file": "wandb.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tensorflow",
        "change": [
            "class Graph(object):",
            "if relevant_nodes and in_node not in relevant_nodes:",
            "# node is not part of the current network",
            "continue",
            "-                    for in_layer in in_node.inbound_layers:",
            "+                    for in_layer in nest.flatten(in_node.inbound_layers):",
            "inbound_keras_node = Node.from_keras(in_layer)",
            "",
            "if (inbound_keras_node.id not in graph.nodes_by_id):"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=for_statement), node=('call', None), position=3, insert_id=2461970)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2461971)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2461972)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'nest'), position=0, insert_id=2461973)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2461974)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'flatten'), position=2, insert_id=2461975)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2461976)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=attribute), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2461977)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 8179,
        "neg_line": [
            "-for in_layer in in_node.inbound_layers:"
        ],
        "pos_line": [
            "+for in_layer in nest.flatten(in_node.inbound_layers):"
        ],
        "core_change": "-for in_layer in in_node.inbound_layers: +for in_layer in nest.flatten(in_node.inbound_layers):",
        "core_API": "flatten"
    },
    {
        "commit_hash": "e4354ef796013d86b0ffdf337bc552482c8367e7",
        "index": "6f85a9c..3f0b87c 100644",
        "commit_message": "refactor hdfs_path; fix more deprecation warnings; print label+preds\n\n",
        "file": "TensorFlowOnSpark.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def main_fun(argv, ctx):",
            "saver = tf.train.Saver(tf.global_variables())",
            "",
            "# Build the summary operation from the last tower summaries.",
            "-      summary_op = tf.merge_summary(summaries)",
            "+      summary_op = tf.summary.merge(summaries)",
            "",
            "# Build an initialization operation to run below.",
            "init = tf.global_variables_initializer()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=2213597)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=2213598)",
            "Insert(target_node=ASTNode(type=attribute), node=('identifier', 'merge'), position=2, insert_id=2213599)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=tf), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=merge_summary), value='summary')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=merge_summary), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 8185,
        "neg_line": [
            "-summary_op = tf.merge_summary(summaries)"
        ],
        "pos_line": [
            "+summary_op = tf.summary.merge(summaries)"
        ],
        "core_change": "-summary_op = tf.merge_summary(summaries) +summary_op = tf.summary.merge(summaries)",
        "core_API": "Saver"
    },
    {
        "commit_hash": "5f4c6e830f603830117877b8990a0e65a2386aa6",
        "index": "2969cdc49..a0b811508 100644",
        "commit_message": "Quick fix :) (#606)\n\n* Changing the name\n\n* style + quality\n\n* update doc and logo\n\n* clean up\n\n* circle-CI on the branche for now\n\n* fix daily dialog dataset\n\n* fix urls\n\nCo-authored-by: Quentin Lhoest <lhoest.q@gmail.com>\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class LibrispeechLm(nlp.GeneratorBasedBuilder):",
            "\"\"\"Returns SplitGenerators.\"\"\"",
            "archive_path = dl_manager.download_and_extract(_DL_URL)",
            "return [",
            "-            nlp.SplitGenerator(name=nlp.Split.TRAIN, gen_kwargs={\"archive_path\": archive_path}),",
            "+            datasets.SplitGenerator(name=datasets.Split.TRAIN, gen_kwargs={\"archive_path\": archive_path}),",
            "]",
            "",
            "def _generate_examples(self, archive_path):"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=nlp), value='datasets')",
            "Update(target_node=ASTNode(type=identifier, text=nlp), value='datasets')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 8194,
        "neg_line": [
            "-nlp.SplitGenerator(name=nlp.Split.TRAIN, gen_kwargs={\"archive_path\": archive_path}),"
        ],
        "pos_line": [
            "+datasets.SplitGenerator(name=datasets.Split.TRAIN, gen_kwargs={\"archive_path\": archive_path}),"
        ],
        "core_change": "-nlp.SplitGenerator(name=nlp.Split.TRAIN, gen_kwargs={\"archive_path\": archive_path}), +datasets.SplitGenerator(name=datasets.Split.TRAIN, gen_kwargs={\"archive_path\": archive_path}),",
        "core_API": "download_and_extract"
    },
    {
        "commit_hash": "1eb1810f2b2d90911302282979831fea948c2582",
        "index": "f8629145a0..fd11661e97 100644",
        "commit_message": "general.py::gather fixes (#4037)\n\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def to_list(x: Union[tf.Tensor, tf.Variable], /) -> list:",
            "def gather(",
            "params: Union[tf.Tensor, tf.Variable],",
            "indices: Union[tf.Tensor, tf.Variable],",
            "-    axis: Optional[int] = -1,",
            "+    /,",
            "*,",
            "+    axis: Optional[int] = -1,",
            "out: Optional[Union[tf.Tensor, tf.Variable]] = None,",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "axis = axis % len(indices.shape)",
            "-    return tf.gather(params, indices, axis=axis, batch_dims=axis)",
            "+    return tf.gather(params, indices, axis=axis, batch_dims=None)",
            "",
            "",
            "def gather_nd("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=keyword_separator), node=ASTNode(type=parameters), position=5)",
            "Insert(target_node=ASTNode(type=parameters), node=('positional_separator', None), position=5, insert_id=1993177)",
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=6, insert_id=1993178)",
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=10, insert_id=1993179)",
            "Insert(target_node=IN(type=positional_separator), node=('/', '/'), position=0, insert_id=1993180)",
            "Insert(target_node=ASTNode(type=keyword_argument), node=('none', 'None'), position=2, insert_id=1993181)",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=axis))"
        ],
        "plus_line": 3,
        "minus_line": 2,
        "AST_diff_line": 8,
        "number": 8195,
        "neg_line": [
            "-axis: Optional[int] = -1,",
            "-return tf.gather(params, indices, axis=axis, batch_dims=axis)"
        ],
        "pos_line": [
            "+/,",
            "+axis: Optional[int] = -1,",
            "+return tf.gather(params, indices, axis=axis, batch_dims=None)"
        ],
        "core_change": "-axis: Optional[int] = -1, +/, +axis: Optional[int] = -1, -return tf.gather(params, indices, axis=axis, batch_dims=axis) +return tf.gather(params, indices, axis=axis, batch_dims=None)",
        "core_API": "gather"
    },
    {
        "commit_hash": "a8cf0b83cf463d341c60d37176fa72dab67dc39f",
        "index": "812f8dc..c7b9898 100644",
        "commit_message": "Add map_location when loading model checkpoint (#1084)\n\nFixes #1078\n",
        "file": "examples.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Trainer:",
            "self.model = DDP(self.model, device_ids=[self.local_rank])",
            "",
            "def _load_snapshot(self, snapshot_path):",
            "-        snapshot = torch.load(snapshot_path)",
            "+        loc = f\"cuda:{self.gpu_id}\"",
            "+        snapshot = torch.load(snapshot_path, map_location=loc)",
            "self.model.load_state_dict(snapshot[\"MODEL_STATE\"])",
            "self.epochs_run = snapshot[\"EPOCHS_RUN\"]",
            "print(f\"Resuming training from snapshot at Epoch {self.epochs_run}\")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=block), node=('expression_statement', None), position=0, insert_id=187274)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=187275)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'loc'), position=0, insert_id=187276)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=187277)",
            "Insert(target_node=IN(type=assignment), node=('string', 'f\"cuda:{self.gpu_id}\"'), position=2, insert_id=187278)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=187279)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=187280)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'map_location'), position=0, insert_id=187281)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=187282)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'loc'), position=2, insert_id=187283)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 8199,
        "neg_line": [
            "-snapshot = torch.load(snapshot_path)"
        ],
        "pos_line": [
            "+loc = f\"cuda:{self.gpu_id}\"",
            "+snapshot = torch.load(snapshot_path, map_location=loc)"
        ],
        "core_change": "-snapshot = torch.load(snapshot_path) +loc = f\"cuda:{self.gpu_id}\" +snapshot = torch.load(snapshot_path, map_location=loc)",
        "core_API": "load"
    },
    {
        "commit_hash": "5a34b5559895c687a78c532f94824fcd73de765d",
        "index": "fb437358..302df868 100644",
        "commit_message": "Deprecate LKJCorrCholesky distribution (#2771)\n\n* Deprecate LKJCorrCholesky distribution\n\n* fix tests; add FutureWarning\n\n* Fix failing examples/lkj.py\n\n* Fix deprecation warning in bart example\n\n* Address review comment\n\nCo-authored-by: Neeraj Pradhan <neerajprad@fb.com>\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def model(y):",
            "# Vector of variances for each of the d variables",
            "theta = pyro.sample(\"theta\", dist.HalfCauchy(torch.ones(d, **options)))",
            "# Lower cholesky factor of a correlation matrix",
            "-    eta = torch.ones(1, **options)  # Implies a uniform distribution over correlation matrices",
            "-    L_omega = pyro.sample(\"L_omega\", dist.LKJCorrCholesky(d, eta))",
            "+    concentration = torch.ones((), **options)  # Implies a uniform distribution over correlation matrices",
            "+    L_omega = pyro.sample(\"L_omega\", dist.LKJCholesky(d, concentration))",
            "# Lower cholesky factor of the covariance matrix",
            "L_Omega = torch.mm(torch.diag(theta.sqrt()), L_omega)",
            "# For inference with SVI, one might prefer to use torch.bmm(theta.sqrt().diag_embed(), L_omega)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=eta), value='concentration')",
            "Insert(target_node=ASTNode(type=argument_list), node=('tuple', None), position=1, insert_id=685180)",
            "Insert(target_node=IN(type=tuple), node=('(', '('), position=0, insert_id=685181)",
            "Insert(target_node=IN(type=tuple), node=(')', ')'), position=1, insert_id=685182)",
            "Update(target_node=ASTNode(type=identifier, text=LKJCorrCholesky), value='LKJCholesky')",
            "Update(target_node=ASTNode(type=identifier, text=eta), value='concentration')",
            "Delete(target_node=ASTNode(type=integer, text=1))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 7,
        "number": 8205,
        "neg_line": [
            "-eta = torch.ones(1, **options)  # Implies a uniform distribution over correlation matrices",
            "-L_omega = pyro.sample(\"L_omega\", dist.LKJCorrCholesky(d, eta))"
        ],
        "pos_line": [
            "+concentration = torch.ones((), **options)  # Implies a uniform distribution over correlation matrices",
            "+L_omega = pyro.sample(\"L_omega\", dist.LKJCholesky(d, concentration))"
        ],
        "core_change": "-eta = torch.ones(1, **options)  # Implies a uniform distribution over correlation matrices -L_omega = pyro.sample(\"L_omega\", dist.LKJCorrCholesky(d, eta)) +concentration = torch.ones((), **options)  # Implies a uniform distribution over correlation matrices +L_omega = pyro.sample(\"L_omega\", dist.LKJCholesky(d, concentration))",
        "core_API": "sample"
    },
    {
        "commit_hash": "c7081f1b992591ffd99134c3961abbbd8467976d",
        "index": "d82d7cbb..a7fb7b57 100644",
        "commit_message": "[fix] AugmentationSequential bbox keypoints transformation fix (#1570)\n\n* bug fix\n\n* added test\n\n* bug fix\n\n* bug fix\n\n* Update kornia/augmentation/container/utils.py\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestAugmentationSequential:",
            "points = torch.tensor([[[0.0, 0.0], [1.0, 1.0]]], device=device, dtype=dtype).expand(3, -1, -1)",
            "aug = K.AugmentationSequential(",
            "K.RandomCrop((3, 3), padding=1, cropping_mode='resample', fill=0),",
            "+            K.RandomAffine((360., 360.), p=1.),",
            "data_keys=[\"input\", \"mask\", \"bbox_xyxy\", \"keypoints\"],",
            ")"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=3, insert_id=406332)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=406333)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=406334)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=406335)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'K'), position=0, insert_id=406336)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=406337)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'RandomAffine'), position=2, insert_id=406338)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=406339)",
            "Insert(target_node=IN(type=argument_list), node=('tuple', None), position=1, insert_id=406340)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=406341)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=3, insert_id=406342)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=406343)",
            "Insert(target_node=IN(type=tuple), node=('(', '('), position=0, insert_id=406344)",
            "Insert(target_node=IN(type=tuple), node=('float', '360.'), position=1, insert_id=406345)",
            "Insert(target_node=IN(type=tuple), node=(',', ','), position=2, insert_id=406346)",
            "Insert(target_node=IN(type=tuple), node=('float', '360.'), position=3, insert_id=406347)",
            "Insert(target_node=IN(type=tuple), node=(')', ')'), position=4, insert_id=406348)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'p'), position=0, insert_id=406349)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=406350)",
            "Insert(target_node=IN(type=keyword_argument), node=('float', '1.'), position=2, insert_id=406351)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 20,
        "number": 8207,
        "neg_line": [],
        "pos_line": [
            "+K.RandomAffine((360., 360.), p=1.),"
        ],
        "core_change": "+K.RandomAffine((360., 360.), p=1.),",
        "core_API": "tensor"
    },
    {
        "commit_hash": "da2f1325014a158be36d62bdd20293ef97e574f4",
        "index": "f44e8963..b43e9e47 100644",
        "commit_message": "enable black in the precommit (#1777)\n\n* enable black in the precommit\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add some fixes\n\n* update libface detection url\n\n* added url from kornia checkpoint\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class MS_SSIMLoss(nn.Module):",
            "loss_l1 = F.l1_loss(img1, img2, reduction='none')",
            "",
            "# Compute average l1 loss in 3 channels",
            "-        gaussian_l1 = F.conv2d(loss_l1, g_masks[-CH:], groups=CH, padding=self.pad).mean(",
            "-            1",
            "-        )",
            "+        gaussian_l1 = F.conv2d(loss_l1, g_masks[-CH:], groups=CH, padding=self.pad).mean(1)",
            "",
            "# Compute MS-SSIM + L1 loss",
            "loss = self.alpha * loss_ms_ssim + (1 - self.alpha) * gaussian_l1 / self.DR"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 17,
        "number": 8208,
        "neg_line": [
            "-gaussian_l1 = F.conv2d(loss_l1, g_masks[-CH:], groups=CH, padding=self.pad).mean(",
            "-1",
            "-)"
        ],
        "pos_line": [
            "+gaussian_l1 = F.conv2d(loss_l1, g_masks[-CH:], groups=CH, padding=self.pad).mean(1)"
        ],
        "core_change": "-gaussian_l1 = F.conv2d(loss_l1, g_masks[-CH:], groups=CH, padding=self.pad).mean( -1 -) +gaussian_l1 = F.conv2d(loss_l1, g_masks[-CH:], groups=CH, padding=self.pad).mean(1)",
        "core_API": "l1_loss"
    },
    {
        "commit_hash": "c5e05d7adcd337f8386f7dea381e03b79a9ce1d4",
        "index": "727ad63f..9484eacc 100644",
        "commit_message": "fix colocation problems\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def add_moving_summary(*args, **kwargs):",
            "ema_op = moving_averages.assign_moving_average(",
            "ema_var, c, decay,",
            "zero_debias=True, name=name + '_EMA_apply')",
            "-            tf.summary.scalar(name + '-summary', ema_op)    # write the EMA value as a summary",
            "ema_ops.append(ema_op)",
            "+        # cannot add it into colocate group -- will force everything to cpus",
            "+        tf.summary.scalar(name + '-summary', ema_op)    # write the EMA value as a summary",
            "if coll is not None:",
            "for op in ema_ops:",
            "# TODO a new collection to summary every step?"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=expression_statement), node=ASTNode(type=module), position=2)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 8213,
        "neg_line": [
            "-tf.summary.scalar(name + '-summary', ema_op)    # write the EMA value as a summary"
        ],
        "pos_line": [
            "+# cannot add it into colocate group -- will force everything to cpus",
            "+tf.summary.scalar(name + '-summary', ema_op)    # write the EMA value as a summary"
        ],
        "core_change": "-tf.summary.scalar(name + '-summary', ema_op)    # write the EMA value as a summary +# cannot add it into colocate group -- will force everything to cpus +tf.summary.scalar(name + '-summary', ema_op)    # write the EMA value as a summary",
        "core_API": "assign_moving_average"
    },
    {
        "commit_hash": "c7fd1d9fb18862318b133c9474d37c5085850070",
        "index": "9110f5ad..2a61def5 100755",
        "commit_message": "fix deprecations about casting & initializers in tf1.13\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Model(ModelDesc):",
            "cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)",
            "cost = tf.reduce_mean(cost, name='cross_entropy_loss')",
            "",
            "-        tf.reduce_mean(tf.to_float(tf.nn.in_top_k(logits, label, 1)), name='accuracy')",
            "+        tf.reduce_mean(tf.cast(tf.nn.in_top_k(logits, label, 1)), tf.float32, name='accuracy')",
            "",
            "wd_cost = tf.multiply(1e-5,",
            "regularize_cost('fc.*/W', tf.nn.l2_loss),"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=3, insert_id=2278973)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=2278974)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2278975)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2278976)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'float32'), position=2, insert_id=2278977)",
            "Update(target_node=ASTNode(type=identifier, text=to_float), value='cast')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 8215,
        "neg_line": [
            "-tf.reduce_mean(tf.to_float(tf.nn.in_top_k(logits, label, 1)), name='accuracy')"
        ],
        "pos_line": [
            "+tf.reduce_mean(tf.cast(tf.nn.in_top_k(logits, label, 1)), tf.float32, name='accuracy')"
        ],
        "core_change": "-tf.reduce_mean(tf.to_float(tf.nn.in_top_k(logits, label, 1)), name='accuracy') +tf.reduce_mean(tf.cast(tf.nn.in_top_k(logits, label, 1)), tf.float32, name='accuracy')",
        "core_API": "sparse_softmax_cross_entropy_with_logits"
    },
    {
        "commit_hash": "5146c3b38a0b2b4a639d5695c4ed945063a72c2c",
        "index": "d26dc734..34d51b14 100644",
        "commit_message": "[FIX] Enable YOLOX training on different devices (#7912)\n\n* Enable yolox training on different devices\n\n* Enable yolox resize test on cpu\n",
        "file": "mmdetection.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class YOLOX(SingleStageDetector):",
            "gt_bbox[..., 1::2] = gt_bbox[..., 1::2] * scale_y",
            "return img, gt_bboxes",
            "",
            "-    def _random_resize(self):",
            "-        tensor = torch.LongTensor(2).cuda()",
            "+    def _random_resize(self, device):",
            "+        tensor = torch.LongTensor(2).to(device)",
            "",
            "if self.rank == 0:",
            "size = random.randint(*self._random_size_range)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=2, insert_id=614288)",
            "Insert(target_node=ASTNode(type=parameters), node=('identifier', 'device'), position=3, insert_id=614289)",
            "Update(target_node=ASTNode(type=identifier, text=cuda), value='to')",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'device'), position=1, insert_id=614290)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 4,
        "number": 8216,
        "neg_line": [
            "-def _random_resize(self):",
            "-tensor = torch.LongTensor(2).cuda()"
        ],
        "pos_line": [
            "+def _random_resize(self, device):",
            "+tensor = torch.LongTensor(2).to(device)"
        ],
        "core_change": "-def _random_resize(self): -tensor = torch.LongTensor(2).cuda() +def _random_resize(self, device): +tensor = torch.LongTensor(2).to(device)",
        "core_API": "LongTensor"
    },
    {
        "commit_hash": "709cc2e206f384bfacc6f2732203c3d37f09b228",
        "index": "8bf6daf6..c830e7e7 100644",
        "commit_message": "[Fix] Missing device initializations (#672)\n\n* Fix device initialization in several variables\n\n* Fix device initialization in several variables\n\n* Skip CUDA tests for #666\n\n* Lint\n\n* Fix device initialization in several variables\n\n* Skip CUDA tests for #666\n\n* Lint\n\n* Remove half-precision tests (#649) and add a jit test\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestPerspective:",
            "",
            "def test_smoke(self, device):",
            "x_data = torch.rand(1, 2, 3, 4).to(device)",
            "-        batch_prob = torch.rand(1) < 0.5",
            "+        batch_prob = torch.rand(1, device=device) < 0.5",
            "start_points = torch.rand(1, 4, 2).to(device)",
            "end_points = torch.rand(1, 4, 2).to(device)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=436899)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=436900)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=436901)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=436902)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=2, insert_id=436903)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 8234,
        "neg_line": [
            "-batch_prob = torch.rand(1) < 0.5"
        ],
        "pos_line": [
            "+batch_prob = torch.rand(1, device=device) < 0.5"
        ],
        "core_change": "-batch_prob = torch.rand(1) < 0.5 +batch_prob = torch.rand(1, device=device) < 0.5",
        "core_API": "rand"
    },
    {
        "commit_hash": "18ab3ed072ab4fc952ee64ed98378c7684023b66",
        "index": "58be6036..a9cb6f53 100644",
        "commit_message": "fix: rename attention_v\n\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Attention(BaseDenseAttention):",
            "# Reshape into [batch_size, 1, Tv, dim].",
            "k_reshaped = tf.expand_dims(key, axis=-3)",
            "if self.scale is not None:",
            "-        scores = self.attention_v * tf.reduce_sum(",
            "+        scores = self.concat_score_weight * tf.reduce_sum(",
            "tf.tanh(self.scale * (q_reshaped + k_reshaped)), axis=-1)",
            "else:",
            "-        scores = self.attention_v * tf.reduce_sum(",
            "+        scores = self.concat_score_weight * tf.reduce_sum(",
            "tf.tanh(q_reshaped + k_reshaped), axis=-1)",
            "",
            "return scores"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=attention_v), value='concat_score_weight')",
            "Update(target_node=ASTNode(type=identifier, text=attention_v), value='concat_score_weight')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 8238,
        "neg_line": [
            "-scores = self.attention_v * tf.reduce_sum(",
            "-scores = self.attention_v * tf.reduce_sum("
        ],
        "pos_line": [
            "+scores = self.concat_score_weight * tf.reduce_sum(",
            "+scores = self.concat_score_weight * tf.reduce_sum("
        ],
        "core_change": "-scores = self.attention_v * tf.reduce_sum( +scores = self.concat_score_weight * tf.reduce_sum( -scores = self.attention_v * tf.reduce_sum( +scores = self.concat_score_weight * tf.reduce_sum(",
        "core_API": "expand_dims"
    },
    {
        "commit_hash": "d2fdf0a5ffa6ca1e7ed1cbba9f66435fa1530ee8",
        "index": "1f41e62aa..60c51e706 100644",
        "commit_message": "fix pytest to import torch\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "import pytest",
            "-",
            "-try:",
            "-    import torch",
            "-    from torch.autograd import Variable",
            "-except:",
            "-    pytest.skip(\"pytorch is not installed\")",
            "-",
            "+pytest.importorskip('torch')",
            "+import torch",
            "from e2e_asr_attctc_th import pad_list, mask_by_length"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=module), node=ASTNode(type=expression_statement), position=1)",
            "Move(target_node=ASTNode(type=module), node=ASTNode(type=import_statement), position=2)",
            "Update(target_node=ASTNode(type=identifier, text=skip), value='importorskip')",
            "Update(target_node=ASTNode(type=string, text=\"pytorch is not installed\"), value=\"'torch'\")",
            "Delete(target_node=ASTNode(type=try, text=try))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=from, text=from))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=autograd))",
            "Delete(target_node=ASTNode(type=dotted_name))",
            "Delete(target_node=ASTNode(type=import, text=import))",
            "Delete(target_node=ASTNode(type=identifier, text=Variable))",
            "Delete(target_node=ASTNode(type=dotted_name))",
            "Delete(target_node=ASTNode(type=import_from_statement))",
            "Delete(target_node=ASTNode(type=block))",
            "Delete(target_node=ASTNode(type=except, text=except))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=block))",
            "Delete(target_node=ASTNode(type=except_clause))",
            "Delete(target_node=ASTNode(type=try_statement))"
        ],
        "plus_line": 0,
        "minus_line": 3,
        "AST_diff_line": 21,
        "number": 8240,
        "neg_line": [
            "-",
            "-try:",
            "-import torch",
            "-from torch.autograd import Variable",
            "-except:",
            "-pytest.skip(\"pytorch is not installed\")",
            "-"
        ],
        "pos_line": [
            "+pytest.importorskip('torch')",
            "+import torch"
        ],
        "core_change": "- -try: -import torch -from torch.autograd import Variable -except: -pytest.skip(\"pytorch is not installed\") - +pytest.importorskip('torch') +import torch",
        "core_API": "skip"
    },
    {
        "commit_hash": "97f3beed3616d50c722fc9227205f8330048ca6c",
        "index": "8026f92db..ee8a8cbd3 100644",
        "commit_message": "Add `with torch.no_grad()` to DistilBERT integration test forward pass (#14979)\n\n* refactor: wrap forward pass around no_grad context\n\n* Update tests/test_modeling_distilbert.py\n\n* fix: rm `no_grad` from non-integration tests\n\n* chore: rm whitespace change\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DistilBertModelIntergrationTest(unittest.TestCase):",
            "model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")",
            "input_ids = torch.tensor([[0, 345, 232, 328, 740, 140, 1695, 69, 6078, 1588, 2]])",
            "attention_mask = torch.tensor([[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])",
            "-        output = model(input_ids, attention_mask=attention_mask)[0]",
            "+        with torch.no_grad():",
            "+            output = model(input_ids, attention_mask=attention_mask)[0]",
            "expected_shape = torch.Size((1, 11, 768))",
            "self.assertEqual(output.shape, expected_shape)",
            "expected_slice = torch.tensor("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('with_statement', None), position=4, insert_id=1207917)",
            "Insert(target_node=IN(type=with_statement), node=('with', 'with'), position=0, insert_id=1207918)",
            "Insert(target_node=IN(type=with_statement), node=('with_clause', None), position=1, insert_id=1207919)",
            "Insert(target_node=IN(type=with_statement), node=(':', ':'), position=2, insert_id=1207920)",
            "Insert(target_node=IN(type=with_statement), node=('block', None), position=3, insert_id=1207921)",
            "Insert(target_node=IN(type=with_clause), node=('with_item', None), position=0, insert_id=1207922)",
            "Move(target_node=IN(type=block), node=ASTNode(type=expression_statement), position=0)",
            "Insert(target_node=IN(type=with_item), node=('call', None), position=0, insert_id=1207923)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1207924)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1207925)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1207926)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1207927)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'no_grad'), position=2, insert_id=1207928)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1207929)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1207930)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 15,
        "number": 8242,
        "neg_line": [
            "-output = model(input_ids, attention_mask=attention_mask)[0]"
        ],
        "pos_line": [
            "+with torch.no_grad():",
            "+output = model(input_ids, attention_mask=attention_mask)[0]"
        ],
        "core_change": "-output = model(input_ids, attention_mask=attention_mask)[0] +with torch.no_grad(): +output = model(input_ids, attention_mask=attention_mask)[0]",
        "core_API": "from_pretrained"
    },
    {
        "commit_hash": "821bc227bae958168a5691824b6a2fc484dda6d6",
        "index": "aaf2f9e..3cc969b 100644",
        "commit_message": "fixed wild bfloat, implemented POC MOE model\n\n",
        "file": "gpt-neo.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def model_fn(features, labels, mode, params):",
            "replica_cache_size = 300 * 1000000  # 300M per replica",
            "# Worker 0 caches all the TPU binaries.",
            "worker0_mem = replica_cache_size * ctx.num_replicas",
            "-        devices_memory_usage = [worker0_mem] + [0] * (num_hosts - 1)",
            "+        devices_memeory_usage = [worker0_mem] + [0] * (num_hosts - 1)",
            "var_placer = mtf.utils.BalancedVariablePlacer(device_list,",
            "-                                                      devices_memory_usage)",
            "+                                                      devices_memeory_usage)",
            "mesh_devices = [''] * mesh_shape.size",
            "mesh_impl = mtf.simd_mesh_impl.SimdMeshImpl(",
            "mesh_shape, layout_rules, mesh_devices, ctx.device_assignment)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=devices_memory_usage), value='devices_memeory_usage')",
            "Update(target_node=ASTNode(type=identifier, text=devices_memory_usage), value='devices_memeory_usage')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 8243,
        "neg_line": [
            "-devices_memory_usage = [worker0_mem] + [0] * (num_hosts - 1)",
            "-devices_memory_usage)"
        ],
        "pos_line": [
            "+devices_memeory_usage = [worker0_mem] + [0] * (num_hosts - 1)",
            "+devices_memeory_usage)"
        ],
        "core_change": "-devices_memory_usage = [worker0_mem] + [0] * (num_hosts - 1) +devices_memeory_usage = [worker0_mem] + [0] * (num_hosts - 1) -devices_memory_usage) +devices_memeory_usage)",
        "core_API": "BalancedVariablePlacer"
    },
    {
        "commit_hash": "c7b38a35e5ed7115a47910facf47eeb57cce3a40",
        "index": "c02ddf02..2dbf90c2 100644",
        "commit_message": "some more fixes\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_batch():",
            "",
            "data_list = batch.to_data_list()",
            "assert len(data_list) == 2",
            "-    assert len(data_list[0]) == 4",
            "+    assert len(data_list[0]) == 3",
            "assert data_list[0].x.tolist() == [1, 2, 3]",
            "assert data_list[0].edge_index.tolist() == [[0, 1, 1, 2], [1, 0, 2, 1]]",
            "assert data_list[0].s == '1'"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=integer, text=4), value='3')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 8244,
        "neg_line": [
            "-assert len(data_list[0]) == 4"
        ],
        "pos_line": [
            "+assert len(data_list[0]) == 3"
        ],
        "core_change": "-assert len(data_list[0]) == 4 +assert len(data_list[0]) == 3",
        "core_API": "to_data_list"
    },
    {
        "commit_hash": "f747a7f3b785fbbcd26e31a6c84c3ba3593d6d09",
        "index": "9586fa051..89bbb94dc 100644",
        "commit_message": "fix dtype\n\n",
        "file": "PySyft.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "from ...proto.lib.torch.tensor_pb2 import TensorData",
            "",
            "",
            "def object2proto(obj: np.ndarray) -> TensorData:",
            "-    tensor = torch.Tensor(obj)",
            "+    tensor = torch.from_numpy(obj).clone()",
            "tensor_proto = protobuf_tensor_serializer(tensor)",
            "",
            "return tensor_proto"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=781446)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=781447)",
            "Insert(target_node=IN(type=attribute), node=('call', None), position=0, insert_id=781448)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=781449)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'clone'), position=2, insert_id=781450)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=781451)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=781452)",
            "Move(target_node=IN(type=call), node=ASTNode(type=attribute), position=0)",
            "Move(target_node=IN(type=call), node=ASTNode(type=argument_list), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=Tensor), value='from_numpy')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 8245,
        "neg_line": [
            "-tensor = torch.Tensor(obj)"
        ],
        "pos_line": [
            "+tensor = torch.from_numpy(obj).clone()"
        ],
        "core_change": "-tensor = torch.Tensor(obj) +tensor = torch.from_numpy(obj).clone()",
        "core_API": "Tensor"
    },
    {
        "commit_hash": "595aca27eab60bc899ac7c2283763e56534d5473",
        "index": "4d1b933..479cb70 100644",
        "commit_message": "use assertClose\n\nSummary: use assertClose in some tests, which enforces shape equality. Fixes some small problems, including graph_conv on an empty graph.\n\nReviewed By: nikhilaravi\n\nDifferential Revision: D20556912\n\nfbshipit-source-id: 60a61eafe3c03ce0f6c9c1a842685708fb10ac5b\n\n",
        "file": "pytorch3d.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestMeshes(TestCaseMixin, unittest.TestCase):",
            "# Multiple meshes in the batch with equal sized meshes",
            "meshes_extended = mesh.extend(3)",
            "for m in meshes_extended.verts_normals_list():",
            "-            self.assertTrue(torch.allclose(m, verts_normals_expected))",
            "+            self.assertClose(m, verts_normals_expected)",
            "for f in meshes_extended.faces_normals_list():",
            "-            self.assertTrue(torch.allclose(f, faces_normals_expected))",
            "+            self.assertClose(f, faces_normals_expected)",
            "",
            "# Multiple meshes in the batch with different sized meshes",
            "# Check padded and packed normals are the correct sizes."
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=expression_statement), node=ASTNode(type=call), position=0)",
            "Move(target_node=ASTNode(type=expression_statement), node=ASTNode(type=call), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=torch), value='self')",
            "Update(target_node=ASTNode(type=identifier, text=allclose), value='assertClose')",
            "Update(target_node=ASTNode(type=identifier, text=torch), value='self')",
            "Update(target_node=ASTNode(type=identifier, text=allclose), value='assertClose')",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=assertTrue))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=assertTrue))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 22,
        "number": 8246,
        "neg_line": [
            "-self.assertTrue(torch.allclose(m, verts_normals_expected))",
            "-self.assertTrue(torch.allclose(f, faces_normals_expected))"
        ],
        "pos_line": [
            "+self.assertClose(m, verts_normals_expected)",
            "+self.assertClose(f, faces_normals_expected)"
        ],
        "core_change": "-self.assertTrue(torch.allclose(m, verts_normals_expected)) +self.assertClose(m, verts_normals_expected) -self.assertTrue(torch.allclose(f, faces_normals_expected)) +self.assertClose(f, faces_normals_expected)",
        "core_API": "extend"
    },
    {
        "commit_hash": "abe697bba0d7f36ce6d5a1e1b750809a8a3c4b25",
        "index": "5a2abeca..7a68f14e 100644",
        "commit_message": "fix linter errors\n\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def homography_i_H_ref(pinhole_i, pinhole_ref):",
            "i_pose_base = get_optical_pose_base(pinhole_i)",
            "ref_pose_base = get_optical_pose_base(pinhole_ref)",
            "i_pose_ref = torch.matmul(i_pose_base, inverse_pose(ref_pose_base))",
            "-    return torch.matmul(pinhole_matrix(pinhole_i), \\",
            "-        torch.matmul(i_pose_ref, inv_pinhole_matrix(pinhole_ref)))",
            "+    return torch.matmul(",
            "+        pinhole_matrix(pinhole_i),",
            "+        torch.matmul(",
            "+            i_pose_ref,",
            "+            inv_pinhole_matrix(pinhole_ref)))"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 5,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 8248,
        "neg_line": [
            "-return torch.matmul(pinhole_matrix(pinhole_i), \\",
            "-torch.matmul(i_pose_ref, inv_pinhole_matrix(pinhole_ref)))"
        ],
        "pos_line": [
            "+return torch.matmul(",
            "+pinhole_matrix(pinhole_i),",
            "+torch.matmul(",
            "+i_pose_ref,",
            "+inv_pinhole_matrix(pinhole_ref)))"
        ],
        "core_change": "-return torch.matmul(pinhole_matrix(pinhole_i), \\ -torch.matmul(i_pose_ref, inv_pinhole_matrix(pinhole_ref))) +return torch.matmul( +pinhole_matrix(pinhole_i), +torch.matmul( +i_pose_ref, +inv_pinhole_matrix(pinhole_ref)))",
        "core_API": "matmul"
    },
    {
        "commit_hash": "a341c014412cbfc86a9dd9816ae228e398dff3a2",
        "index": "d80967ce..392120fc 100644",
        "commit_message": "Fix `batch_dot` of TensorFlow when `axes=None` (#9922)\n\n\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def batch_dot(x, y, axes=None):",
            "axes = (axes, axes)",
            "x_ndim = ndim(x)",
            "y_ndim = ndim(y)",
            "+    if axes is None:",
            "+        # behaves like tf.batch_matmul as default",
            "+        axes = [x_ndim - 1, y_ndim - 2]",
            "if x_ndim > y_ndim:",
            "diff = x_ndim - y_ndim",
            "y = tf.reshape(y, tf.concat([tf.shape(y), [1] * (diff)], axis=0))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=4, insert_id=2112684)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=2112685)",
            "Insert(target_node=IN(type=if_statement), node=('comparison_operator', None), position=1, insert_id=2112686)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=2112687)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=2112688)",
            "Insert(target_node=IN(type=comparison_operator), node=('identifier', 'axes'), position=0, insert_id=2112689)",
            "Insert(target_node=IN(type=comparison_operator), node=('is', 'is'), position=1, insert_id=2112690)",
            "Insert(target_node=IN(type=comparison_operator), node=('none', 'None'), position=2, insert_id=2112691)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=2112692)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=2112693)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'axes'), position=0, insert_id=2112694)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=2112695)",
            "Insert(target_node=IN(type=assignment), node=('list', None), position=2, insert_id=2112696)",
            "Insert(target_node=IN(type=list), node=('[', '['), position=0, insert_id=2112697)",
            "Insert(target_node=IN(type=list), node=('binary_operator', None), position=1, insert_id=2112698)",
            "Insert(target_node=IN(type=list), node=(',', ','), position=2, insert_id=2112699)",
            "Insert(target_node=IN(type=list), node=('binary_operator', None), position=3, insert_id=2112700)",
            "Insert(target_node=IN(type=list), node=(']', ']'), position=4, insert_id=2112701)",
            "Insert(target_node=IN(type=binary_operator), node=('identifier', 'x_ndim'), position=0, insert_id=2112702)",
            "Insert(target_node=IN(type=binary_operator), node=('-', '-'), position=1, insert_id=2112703)",
            "Insert(target_node=IN(type=binary_operator), node=('integer', '1'), position=2, insert_id=2112704)",
            "Insert(target_node=IN(type=binary_operator), node=('identifier', 'y_ndim'), position=0, insert_id=2112705)",
            "Insert(target_node=IN(type=binary_operator), node=('-', '-'), position=1, insert_id=2112706)",
            "Insert(target_node=IN(type=binary_operator), node=('integer', '2'), position=2, insert_id=2112707)"
        ],
        "plus_line": 3,
        "minus_line": 0,
        "AST_diff_line": 24,
        "number": 8249,
        "neg_line": [],
        "pos_line": [
            "+if axes is None:",
            "+# behaves like tf.batch_matmul as default",
            "+axes = [x_ndim - 1, y_ndim - 2]"
        ],
        "core_change": "+if axes is None: +# behaves like tf.batch_matmul as default +axes = [x_ndim - 1, y_ndim - 2]",
        "core_API": "reshape"
    },
    {
        "commit_hash": "4bf5042240d33286460b83f3dbf9be77500faab3",
        "index": "e9a3d5014..0724e18ef 100644",
        "commit_message": "Fix BART tests on GPU (#4298)\n\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestSinusoidalPositionalEmbeddings(unittest.TestCase):",
            "# test that forward pass is just a lookup, there is no ignore padding logic",
            "input_ids = torch.tensor([[4, 10, pad, pad, pad]], dtype=torch.long, device=torch_device)",
            "no_cache_pad_zero = emb1(input_ids)",
            "-        self.assertTrue(torch.allclose(torch.Tensor(self.desired_weights), no_cache_pad_zero[:3, :5], atol=1e-3))",
            "+        self.assertTrue(",
            "+            torch.allclose(",
            "+                torch.tensor(self.desired_weights, device=torch_device), no_cache_pad_zero[:3, :5], atol=1e-3",
            "+            )",
            "+        )"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=Tensor), value='tensor')",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1239226)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1239227)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=1239228)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1239229)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'torch_device'), position=2, insert_id=1239230)"
        ],
        "plus_line": 5,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 8251,
        "neg_line": [
            "-self.assertTrue(torch.allclose(torch.Tensor(self.desired_weights), no_cache_pad_zero[:3, :5], atol=1e-3))"
        ],
        "pos_line": [
            "+self.assertTrue(",
            "+torch.allclose(",
            "+torch.tensor(self.desired_weights, device=torch_device), no_cache_pad_zero[:3, :5], atol=1e-3",
            "+)",
            "+)"
        ],
        "core_change": "-self.assertTrue(torch.allclose(torch.Tensor(self.desired_weights), no_cache_pad_zero[:3, :5], atol=1e-3)) +self.assertTrue( +torch.allclose( +torch.tensor(self.desired_weights, device=torch_device), no_cache_pad_zero[:3, :5], atol=1e-3 +) +)",
        "core_API": "tensor"
    },
    {
        "commit_hash": "8be8ce7990773fe994206ab8329fa8d08b8cfc99",
        "index": "ef412c6062..df3040f498 100644",
        "commit_message": "Formatting fixes to meshgrid on all backends (#7579)\n\nCo-authored-by: @simonetgordon <simonegordon12@icloud.com>\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def linspace_helper(start, stop, num, axis=None, *, dtype=None, device):",
            "",
            "",
            "def meshgrid(",
            "-    *arrays: torch.Tensor, sparse: bool = False, indexing=\"xy\"",
            "+    *arrays: torch.Tensor,",
            "+    sparse: bool = False,",
            "+    indexing: str = \"xy\",",
            ") -> List[torch.Tensor]:",
            "if not sparse:",
            "return list(torch.meshgrid(*arrays, indexing=indexing))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=parameters), node=('typed_default_parameter', None), position=5, insert_id=275835)",
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=6, insert_id=275836)",
            "Move(target_node=IN(type=typed_default_parameter), node=ASTNode(type=identifier, text=indexing), position=0)",
            "Insert(target_node=IN(type=typed_default_parameter), node=(':', ':'), position=1, insert_id=275837)",
            "Insert(target_node=IN(type=typed_default_parameter), node=('type', None), position=2, insert_id=275838)",
            "Move(target_node=IN(type=typed_default_parameter), node=ASTNode(type==, text==), position=3)",
            "Move(target_node=IN(type=typed_default_parameter), node=ASTNode(type=string, text=\"xy\"), position=4)",
            "Insert(target_node=IN(type=type), node=('identifier', 'str'), position=0, insert_id=275839)",
            "Delete(target_node=ASTNode(type=default_parameter))"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 8255,
        "neg_line": [
            "-*arrays: torch.Tensor, sparse: bool = False, indexing=\"xy\""
        ],
        "pos_line": [
            "+*arrays: torch.Tensor,",
            "+sparse: bool = False,",
            "+indexing: str = \"xy\","
        ],
        "core_change": "-*arrays: torch.Tensor, sparse: bool = False, indexing=\"xy\" +*arrays: torch.Tensor, +sparse: bool = False, +indexing: str = \"xy\",",
        "core_API": "meshgrid"
    },
    {
        "commit_hash": "e67ce7e75acc6b1d893fb2bf1791b81e78f9aaca",
        "index": "bf1c2a95d..99932627d 100644",
        "commit_message": "fix rnnlm load bug\n\nthis should be `torch_load()` instead of `torch.load()`\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def train(args):",
            "rnnlm = lm_pytorch.ClassifierWithState(",
            "lm_pytorch.RNNLM(",
            "len(args.char_list), rnnlm_args.layer, rnnlm_args.unit))",
            "-        torch.load(args.rnnlm, rnnlm)",
            "+        torch_load(args.rnnlm, rnnlm)",
            "model.rnnlm = rnnlm",
            "",
            "# write model config"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=torch), value='torch_load')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=torch), position=0)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=load))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 8256,
        "neg_line": [
            "-torch.load(args.rnnlm, rnnlm)"
        ],
        "pos_line": [
            "+torch_load(args.rnnlm, rnnlm)"
        ],
        "core_change": "-torch.load(args.rnnlm, rnnlm) +torch_load(args.rnnlm, rnnlm)",
        "core_API": "ClassifierWithState"
    },
    {
        "commit_hash": "42c5967d5009891721cae0cb291d40cb7a9ea650",
        "index": "ced58c9a..ca9f13b0 100755",
        "commit_message": "agents and models base classes moved, various fixes\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Model(object):",
            "else:",
            "assert not config.global_model and config.session is None",
            "tf.reset_default_graph()",
            "-            self.session = tf.Session()",
            "+            self.session = config.session = tf.Session()",
            "",
            "if config.distributed and not config.global_model:",
            "# Global and local model for asynchronous updates"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('assignment', None), position=2, insert_id=2244404)",
            "Insert(target_node=IN(type=assignment), node=('attribute', None), position=0, insert_id=2244405)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=2244406)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=call), position=2)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'config'), position=0, insert_id=2244407)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2244408)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'session'), position=2, insert_id=2244409)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 8259,
        "neg_line": [
            "-self.session = tf.Session()"
        ],
        "pos_line": [
            "+self.session = config.session = tf.Session()"
        ],
        "core_change": "-self.session = tf.Session() +self.session = config.session = tf.Session()",
        "core_API": "reset_default_graph"
    },
    {
        "commit_hash": "2234877e01c2824dd2a5ea37e1f6c8cc1d34664e",
        "index": "332d75ed..b5f95e46 100755",
        "commit_message": "fix tests\n\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class PipelineTesterMixin(unittest.TestCase):",
            "image_slice = image[0, -1, -3:, -3:].cpu()",
            "",
            "assert image.shape == (1, 3, 32, 32)",
            "-        expected_slice = torch.tensor([-0.7688, -0.7690, -0.7597, -0.7660, -0.7713, -0.7531, -0.7009, -0.7098, -0.7350])",
            "+        expected_slice = torch.tensor([-0.7383, -0.7385, -0.7298, -0.7364, -0.7414, -0.7239, -0.6737, -0.6813, -0.7068])",
            "assert (image_slice.flatten() - expected_slice).abs().max() < 1e-2"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=float, text=0.7688), value='0.7383')",
            "Update(target_node=ASTNode(type=float, text=0.7690), value='0.7385')",
            "Update(target_node=ASTNode(type=float, text=0.7597), value='0.7298')",
            "Update(target_node=ASTNode(type=float, text=0.7660), value='0.7364')",
            "Update(target_node=ASTNode(type=float, text=0.7713), value='0.7414')",
            "Update(target_node=ASTNode(type=float, text=0.7531), value='0.7239')",
            "Update(target_node=ASTNode(type=float, text=0.7009), value='0.6737')",
            "Update(target_node=ASTNode(type=float, text=0.7098), value='0.6813')",
            "Update(target_node=ASTNode(type=float, text=0.7350), value='0.7068')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 8261,
        "neg_line": [
            "-expected_slice = torch.tensor([-0.7688, -0.7690, -0.7597, -0.7660, -0.7713, -0.7531, -0.7009, -0.7098, -0.7350])"
        ],
        "pos_line": [
            "+expected_slice = torch.tensor([-0.7383, -0.7385, -0.7298, -0.7364, -0.7414, -0.7239, -0.6737, -0.6813, -0.7068])"
        ],
        "core_change": "-expected_slice = torch.tensor([-0.7688, -0.7690, -0.7597, -0.7660, -0.7713, -0.7531, -0.7009, -0.7098, -0.7350]) +expected_slice = torch.tensor([-0.7383, -0.7385, -0.7298, -0.7364, -0.7414, -0.7239, -0.6737, -0.6813, -0.7068])",
        "core_API": "tensor"
    },
    {
        "commit_hash": "2a1af832ba1582b41377a4ce9694c61e39707218",
        "index": "11d524d3..4fefaea0 100644",
        "commit_message": "only freeze trainable variables (fix #351)\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def freeze_variables():",
            "with varreplace.freeze_variable():",
            "x = FullyConnected('fc', x, 1000)   # fc/* will not be trained",
            "\"\"\"",
            "-    return remap_variables(lambda v: tf.stop_gradient(v))",
            "+    def custom_getter(getter, *args, **kwargs):",
            "+        v = getter(*args, **kwargs)",
            "+        if kwargs.pop('trainable', True):",
            "+            v = tf.stop_gradient(v)",
            "+        return v",
            "+    return custom_getter_scope(custom_getter)",
            "",
            "",
            "@deprecated(\"Renamed to remap_variables\", \"2017-11-06\")"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 6,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 8270,
        "neg_line": [
            "-return remap_variables(lambda v: tf.stop_gradient(v))"
        ],
        "pos_line": [
            "+def custom_getter(getter, *args, **kwargs):",
            "+v = getter(*args, **kwargs)",
            "+if kwargs.pop('trainable', True):",
            "+v = tf.stop_gradient(v)",
            "+return v",
            "+return custom_getter_scope(custom_getter)"
        ],
        "core_change": "-return remap_variables(lambda v: tf.stop_gradient(v)) +def custom_getter(getter, *args, **kwargs): +v = getter(*args, **kwargs) +if kwargs.pop('trainable', True): +v = tf.stop_gradient(v) +return v +return custom_getter_scope(custom_getter)",
        "core_API": "freeze_variable"
    },
    {
        "commit_hash": "c2a406277cb5bf072c47ed081f5e04342ea0554c",
        "index": "b7d195f3..dac94693 100644",
        "commit_message": "Fix `bipartite_sage_unsup` example  (#6550)\n\nFixes #6549\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test(loader):",
            "target = batch['user', 'item'].edge_label.long().cpu()",
            "",
            "preds.append(pred)",
            "-        targets.append(pred)",
            "+        targets.append(target)",
            "",
            "pred = torch.cat(preds, dim=0).numpy()",
            "-    target = torch.cat(target, dim=0).numpy()",
            "+    target = torch.cat(targets, dim=0).numpy()",
            "",
            "+    pred = pred > 0.5",
            "acc = accuracy_score(target, pred)",
            "prec = precision_score(target, pred)",
            "rec = recall_score(target, pred)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=6, insert_id=939939)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=939940)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'pred'), position=0, insert_id=939941)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=939942)",
            "Insert(target_node=IN(type=assignment), node=('comparison_operator', None), position=2, insert_id=939943)",
            "Update(target_node=ASTNode(type=identifier, text=pred), value='target')",
            "Insert(target_node=IN(type=comparison_operator), node=('identifier', 'pred'), position=0, insert_id=939944)",
            "Insert(target_node=IN(type=comparison_operator), node=('>', '>'), position=1, insert_id=939945)",
            "Insert(target_node=IN(type=comparison_operator), node=('float', '0.5'), position=2, insert_id=939946)",
            "Update(target_node=ASTNode(type=identifier, text=target), value='targets')"
        ],
        "plus_line": 3,
        "minus_line": 2,
        "AST_diff_line": 10,
        "number": 8273,
        "neg_line": [
            "-targets.append(pred)",
            "-target = torch.cat(target, dim=0).numpy()"
        ],
        "pos_line": [
            "+targets.append(target)",
            "+target = torch.cat(targets, dim=0).numpy()",
            "+pred = pred > 0.5"
        ],
        "core_change": "-targets.append(pred) +targets.append(target) -target = torch.cat(target, dim=0).numpy() +target = torch.cat(targets, dim=0).numpy() +pred = pred > 0.5",
        "core_API": "long"
    },
    {
        "commit_hash": "ce8f42b12ecd522e3ef7251e0d5f5175075a3fb4",
        "index": "2a4006de..f793ac7e 100644",
        "commit_message": "Fixes to enable dev branch to move to pytorch-1.2 (#2001)\n\n* Fixes to enable dev branch to move to pytorch-1.2\n\n* change name for published cpu wheels\n\n* fix failing unit tests\n\n* change docs conf\n\n* change tracking example\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class MaskedMixture(TorchDistribution):",
            "arg_constraints = {}  # nothing can be constrained",
            "",
            "def __init__(self, mask, component0, component1, validate_args=None):",
            "-        if not torch.is_tensor(mask) or mask.dtype != torch.uint8:",
            "-            raise ValueError('Expected mask to be a ByteTensor but got {}'.format(type(mask)))",
            "+        if not torch.is_tensor(mask) or mask.dtype != torch.bool:",
            "+            raise ValueError('Expected mask to be a BoolTensor but got {}'.format(type(mask)))",
            "if component0.event_shape != component1.event_shape:",
            "raise ValueError('components event_shape disagree: {} vs {}'",
            ".format(component0.event_shape, component1.event_shape))"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=uint8), value='bool')",
            "Update(target_node=ASTNode(type=string, text='Expected mask to be a ByteTensor but got {}'), value=\"'Expected mask to be a BoolTensor but got {}'\")"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 8275,
        "neg_line": [
            "-if not torch.is_tensor(mask) or mask.dtype != torch.uint8:",
            "-raise ValueError('Expected mask to be a ByteTensor but got {}'.format(type(mask)))"
        ],
        "pos_line": [
            "+if not torch.is_tensor(mask) or mask.dtype != torch.bool:",
            "+raise ValueError('Expected mask to be a BoolTensor but got {}'.format(type(mask)))"
        ],
        "core_change": "-if not torch.is_tensor(mask) or mask.dtype != torch.uint8: -raise ValueError('Expected mask to be a ByteTensor but got {}'.format(type(mask))) +if not torch.is_tensor(mask) or mask.dtype != torch.bool: +raise ValueError('Expected mask to be a BoolTensor but got {}'.format(type(mask)))",
        "core_API": "is_tensor"
    },
    {
        "commit_hash": "a347aff8d4ec16365dcb3f570f0113c88cc4ccba",
        "index": "d52dac77..41b8aed5 100755",
        "commit_message": "[FasterRCNN] fix bug in last commit\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Model(ModelDesc):",
            "decoded_boxes = decode_bbox_target(",
            "fastrcnn_box_logits /",
            "tf.constant(config.FASTRCNN_BBOX_REG_WEIGHTS), anchors)",
            "-            decoded_boxes = clip_boxes(decoded_boxes, tf.shape(image)[:2], name='fastrcnn_all_boxes')",
            "+            decoded_boxes = clip_boxes(decoded_boxes, image_shape2d, name='fastrcnn_all_boxes')",
            "",
            "# indices: Nx2. Each index into (#proposal, #category)",
            "pred_indices, final_probs = fastrcnn_predictions(decoded_boxes, label_probs)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tf), value='image_shape2d')",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=tf), position=3)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=shape))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=image))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=integer, text=2))",
            "Delete(target_node=ASTNode(type=slice))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 16,
        "number": 8276,
        "neg_line": [
            "-decoded_boxes = clip_boxes(decoded_boxes, tf.shape(image)[:2], name='fastrcnn_all_boxes')"
        ],
        "pos_line": [
            "+decoded_boxes = clip_boxes(decoded_boxes, image_shape2d, name='fastrcnn_all_boxes')"
        ],
        "core_change": "-decoded_boxes = clip_boxes(decoded_boxes, tf.shape(image)[:2], name='fastrcnn_all_boxes') +decoded_boxes = clip_boxes(decoded_boxes, image_shape2d, name='fastrcnn_all_boxes')",
        "core_API": "constant"
    },
    {
        "commit_hash": "4ef3ecf37f3bdd677aca1f100206adfc0aa1c3f7",
        "index": "5e468cfb..31cd03b6 100644",
        "commit_message": "loca sens attn fix\n\n",
        "file": "TTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class AttentionRNNCell(nn.Module):",
            "out_dim (int): context vector feature dimension.",
            "rnn_dim (int): rnn hidden state dimension.",
            "annot_dim (int): annotation vector feature dimension.",
            "-            memory_dim (int): memory vector (decoder autogression) feature dimension.",
            "+            memory_dim (int): memory vector (decoder output) feature dimension.",
            "align_model (str): 'b' for Bahdanau, 'ls' Location Sensitive alignment.",
            "\"\"\"",
            "super(AttentionRNNCell, self).__init__()",
            "self.align_model = align_model",
            "-        self.rnn_cell = nn.GRUCell(out_dim + memory_dim, rnn_dim)",
            "+        self.rnn_cell = nn.GRUCell(annot_dim + memory_dim, rnn_dim)",
            "# pick bahdanau or location sensitive attention",
            "if align_model == 'b':",
            "-            self.alignment_model = BahdanauAttention(annot_dim, out_dim, out_dim)",
            "+            self.alignment_model = BahdanauAttention(annot_dim, rnn_dim, out_dim)",
            "if align_model == 'ls':",
            "self.alignment_model = LocationSensitiveAttention(annot_dim, rnn_dim, out_dim)",
            "else:"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'decoder'), position=1, insert_id=1559118)",
            "Update(target_node=ASTNode(type=identifier, text=decoder), value='output')",
            "Update(target_node=ASTNode(type=identifier, text=out_dim), value='annot_dim')",
            "Update(target_node=ASTNode(type=identifier, text=out_dim), value='rnn_dim')",
            "Delete(target_node=ASTNode(type=identifier, text=autogression))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 5,
        "number": 8280,
        "neg_line": [
            "-memory_dim (int): memory vector (decoder autogression) feature dimension.",
            "-self.rnn_cell = nn.GRUCell(out_dim + memory_dim, rnn_dim)",
            "-self.alignment_model = BahdanauAttention(annot_dim, out_dim, out_dim)"
        ],
        "pos_line": [
            "+memory_dim (int): memory vector (decoder output) feature dimension.",
            "+self.rnn_cell = nn.GRUCell(annot_dim + memory_dim, rnn_dim)",
            "+self.alignment_model = BahdanauAttention(annot_dim, rnn_dim, out_dim)"
        ],
        "core_change": "-memory_dim (int): memory vector (decoder autogression) feature dimension. +memory_dim (int): memory vector (decoder output) feature dimension. -self.rnn_cell = nn.GRUCell(out_dim + memory_dim, rnn_dim) +self.rnn_cell = nn.GRUCell(annot_dim + memory_dim, rnn_dim) -self.alignment_model = BahdanauAttention(annot_dim, out_dim, out_dim) +self.alignment_model = BahdanauAttention(annot_dim, rnn_dim, out_dim)",
        "core_API": "GRUCell"
    },
    {
        "commit_hash": "32a8daf6903eb0a00d147ee2aa62d85442d6fb0e",
        "index": "eb9db9e8..89698d57 100644",
        "commit_message": "fix test_homography_warper_gradcheck\n\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def create_random_homography(batch_size, eye_size, std_val=1e-3):",
            "return eye + std.uniform_(-std_val, std_val)",
            "",
            "",
            "-def tensor_to_gradcheck_var(tensor):",
            "+def tensor_to_gradcheck_var(tensor, dtype=torch.float64, requires_grad=True):",
            "\"\"\"Converts the input tensor to a valid variable to check the gradient.",
            "`gradcheck` needs 64-bit floating point and requires gradient.",
            "\"\"\"",
            "assert torch.is_tensor(tensor), type(tensor)",
            "-    return tensor.requires_grad_(True).type(torch.DoubleTensor)",
            "+    return tensor.requires_grad_(requires_grad).type(dtype)",
            "",
            "",
            "def compute_mse(x, y):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=2, insert_id=481458)",
            "Insert(target_node=ASTNode(type=parameters), node=('default_parameter', None), position=3, insert_id=481459)",
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=4, insert_id=481460)",
            "Insert(target_node=ASTNode(type=parameters), node=('default_parameter', None), position=5, insert_id=481461)",
            "Insert(target_node=IN(type=default_parameter), node=('identifier', 'dtype'), position=0, insert_id=481462)",
            "Insert(target_node=IN(type=default_parameter), node=('=', '='), position=1, insert_id=481463)",
            "Insert(target_node=IN(type=default_parameter), node=('attribute', None), position=2, insert_id=481464)",
            "Insert(target_node=IN(type=default_parameter), node=('identifier', 'requires_grad'), position=0, insert_id=481465)",
            "Insert(target_node=IN(type=default_parameter), node=('=', '='), position=1, insert_id=481466)",
            "Insert(target_node=IN(type=default_parameter), node=('true', 'True'), position=2, insert_id=481467)",
            "Update(target_node=ASTNode(type=identifier, text=torch), value='dtype')",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=torch), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=481468)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=481469)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'float64'), position=2, insert_id=481470)",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'requires_grad'), position=1, insert_id=481471)",
            "Delete(target_node=ASTNode(type=true, text=True))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=DoubleTensor))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 20,
        "number": 8282,
        "neg_line": [
            "-def tensor_to_gradcheck_var(tensor):",
            "-return tensor.requires_grad_(True).type(torch.DoubleTensor)"
        ],
        "pos_line": [
            "+def tensor_to_gradcheck_var(tensor, dtype=torch.float64, requires_grad=True):",
            "+return tensor.requires_grad_(requires_grad).type(dtype)"
        ],
        "core_change": "-def tensor_to_gradcheck_var(tensor): +def tensor_to_gradcheck_var(tensor, dtype=torch.float64, requires_grad=True): -return tensor.requires_grad_(True).type(torch.DoubleTensor) +return tensor.requires_grad_(requires_grad).type(dtype)",
        "core_API": "uniform_"
    },
    {
        "commit_hash": "22c7c87e1450a06bd0fdcfa7b6c1b15565b2f336",
        "index": "cc0e180d5f..833f4afaaf 100644",
        "commit_message": "[rllib] [tune] Custom preprocessors and models, various fixes (#1372)\n\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "models",
        "change": [
            "from ray.rllib.optimizers import SampleBatch, TFMultiGPUSupport",
            "class DQNEvaluator(TFMultiGPUSupport):",
            "\"\"\"The base DQN Evaluator that does not include the replay buffer.\"\"\"",
            "",
            "-    def __init__(self, env_creator, config, logdir):",
            "+    def __init__(self, registry, env_creator, config, logdir):",
            "env = env_creator()",
            "-        env = wrap_dqn(env, config[\"model\"])",
            "+        env = wrap_dqn(registry, env, config[\"model\"])",
            "self.env = env",
            "self.config = config",
            "",
            "tf_config = tf.ConfigProto(**config[\"tf_session_args\"])",
            "self.sess = tf.Session(config=tf_config)",
            "-        self.dqn_graph = models.DQNGraph(env, config, logdir)",
            "+        self.dqn_graph = models.DQNGraph(registry, env, config, logdir)",
            "",
            "# Create the schedule for exploration starting from 1.",
            "self.exploration = LinearSchedule("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=parameters), node=('identifier', 'registry'), position=3, insert_id=2674497)",
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=4, insert_id=2674498)",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'registry'), position=1, insert_id=2674499)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=2674500)",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'registry'), position=1, insert_id=2674501)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=2674502)"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 6,
        "number": 8283,
        "neg_line": [
            "-def __init__(self, env_creator, config, logdir):",
            "-env = wrap_dqn(env, config[\"model\"])",
            "-self.dqn_graph = models.DQNGraph(env, config, logdir)"
        ],
        "pos_line": [
            "+def __init__(self, registry, env_creator, config, logdir):",
            "+env = wrap_dqn(registry, env, config[\"model\"])",
            "+self.dqn_graph = models.DQNGraph(registry, env, config, logdir)"
        ],
        "core_change": "-def __init__(self, env_creator, config, logdir): +def __init__(self, registry, env_creator, config, logdir): -env = wrap_dqn(env, config[\"model\"]) +env = wrap_dqn(registry, env, config[\"model\"]) -self.dqn_graph = models.DQNGraph(env, config, logdir) +self.dqn_graph = models.DQNGraph(registry, env, config, logdir)",
        "core_API": "ConfigProto"
    },
    {
        "commit_hash": "d1edc7838d099d9b42e94945d0d46c2542bd854a",
        "index": "437b32a2..454326ec 100644",
        "commit_message": "Reimplement Sparse R-CNN (#4219)\n\n* draft version train and test\n\n* remove get_bbox in sparse_head\n\n* refactor SparseRCNN model, align inference eval mAP with official repo\n\n* add focal_loss in hungarian_assigner.py\n\n* update hungarian_assigner.py\n\n* update detr config\n\n* update doc and codes\n\n* fix unitest\n\n* fix unitest\n\n* add embedding_rpn and traing code\n\n* modify code format\n\n* fix proposalist in simpletest\n\n* add focal loss traing\n\n* fix acc\n\n* fix reduce mean\n\n* add embedding rpn doc and add show initproposals\n\n* add docstr\n\n* fix backbone lr\n\n* fix typo and change detach_proposal_list to proposal_list\n\n* remove debug assert\n\n* add readme\n\n* add config for random crop when use 300 proposals\n\n* add aug for 300 proposals\n\n* add 300 proposals results and fix docstr\n\n* add 1x results\n\n* fix lint\n\n* fix unitest\n\n* fix docstr of embeddingrpn\n\n* fix according to the comments\n\n* fix get_bbox and a bug of dynamic conv\n\n* support cpu focal loss and fix docstr\n\n* fix docstr\n\n* fix unitest\n\n* fix bug of  init_weight\n\n* fix to device in assign\n\n* fix all commemt\n\n* fix test_config\n\n* remove dim 1 in forward\n\n* fix shape in comment\n\n* fix the case that train_cfg is None\n\n* fix shape of docstr\n\n* fix docstr\n\n* fix typo\n\n* simplify test\n\n* fix docstr\n\n* move train_cfg and test_cfg to model as\n\n* fix unitest\n\n* add link of ckpt and log\n\n* fix 1x results\n\nCo-authored-by: tianyuandu <tianyuandu@gmail.com>\nCo-authored-by: fangkairen <fangkairen@sensetime.com>\n",
        "file": "mmdetection.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class HungarianAssigner(BaseAssigner):",
            "return AssignResult(",
            "num_gts, assigned_gt_inds, None, labels=assigned_labels)",
            "img_h, img_w, _ = img_meta['img_shape']",
            "-        factor = torch.Tensor([img_w, img_h, img_w,",
            "-                               img_h]).unsqueeze(0).to(gt_bboxes.device)",
            "+        factor = gt_bboxes.new_tensor([img_w, img_h, img_w,",
            "+                                       img_h]).unsqueeze(0)",
            "",
            "# 2. compute the weighted costs",
            "# classification and bboxcost."
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=IN(type=root), node=ASTNode(type=call), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=torch), value='gt_bboxes')",
            "Update(target_node=ASTNode(type=identifier, text=Tensor), value='new_tensor')",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=to))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=gt_bboxes))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=device))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 13,
        "number": 8286,
        "neg_line": [
            "-factor = torch.Tensor([img_w, img_h, img_w,",
            "-img_h]).unsqueeze(0).to(gt_bboxes.device)"
        ],
        "pos_line": [
            "+factor = gt_bboxes.new_tensor([img_w, img_h, img_w,",
            "+img_h]).unsqueeze(0)"
        ],
        "core_change": "-factor = torch.Tensor([img_w, img_h, img_w, -img_h]).unsqueeze(0).to(gt_bboxes.device) +factor = gt_bboxes.new_tensor([img_w, img_h, img_w, +img_h]).unsqueeze(0)",
        "core_API": "Tensor"
    },
    {
        "commit_hash": "eb25cd7f0b142d1d8e4ceba0207785858debb6ba",
        "index": "01810274..d60a1055 100644",
        "commit_message": "fix static batch size in Conv2DTranspose\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def Conv2DTranspose(",
            "shape4d(strides, data_format=data_format),",
            "padding=padding.upper(),",
            "data_format=data_format)",
            "-        conv.set_shape(tf.TensorShape([None] + out_shape3_sta))",
            "+        conv.set_shape(tf.TensorShape([shape_sta[0]] + out_shape3_sta))",
            "",
            "ret = tf.nn.bias_add(conv, b, data_format=data_format) if use_bias else conv",
            "if activation is not None:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=list), node=('subscript', None), position=1, insert_id=2274006)",
            "Insert(target_node=ASTNode(type=list), node=(']', ']'), position=2, insert_id=2274007)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'shape_sta'), position=0, insert_id=2274008)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=2274009)",
            "Insert(target_node=IN(type=subscript), node=('integer', '0'), position=2, insert_id=2274010)",
            "Move(target_node=IN(type=subscript), node=ASTNode(type=], text=]), position=3)",
            "Delete(target_node=ASTNode(type=none, text=None))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 8292,
        "neg_line": [
            "-conv.set_shape(tf.TensorShape([None] + out_shape3_sta))"
        ],
        "pos_line": [
            "+conv.set_shape(tf.TensorShape([shape_sta[0]] + out_shape3_sta))"
        ],
        "core_change": "-conv.set_shape(tf.TensorShape([None] + out_shape3_sta)) +conv.set_shape(tf.TensorShape([shape_sta[0]] + out_shape3_sta))",
        "core_API": "upper"
    },
    {
        "commit_hash": "6b18a908b8ee6e9b58c834a3f7b1944e002764a3",
        "index": "668b9edc..62b64a63 100644",
        "commit_message": "Fix shape inference error for newly version Tensorflow in ctc_label_dense_to_sparse (#3955)\n\n\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def ctc_label_dense_to_sparse(labels, label_lengths):",
            "max_num_labels_tns = tf.pack([label_shape[1]])",
            "",
            "def range_less_than(previous_state, current_input):",
            "-        return tf.expand_dims(tf.range(label_shape[1]), 0) < current_input",
            "+        return tf.expand_dims(tf.range(label_shape[1]), 0) < tf.fill(max_num_labels_tns, current_input)",
            "",
            "-    init = tf.cast(tf.fill(max_num_labels_tns, 0), tf.bool)",
            "+    init = tf.cast(tf.fill([1, label_shape[1]], 0), tf.bool)",
            "dense_mask = functional_ops.scan(range_less_than, label_lengths,",
            "initializer=init, parallel_iterations=1)",
            "dense_mask = dense_mask[:, 0, :]"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=comparison_operator), node=('call', None), position=2, insert_id=2115807)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2115808)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2115809)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2115810)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2115811)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'fill'), position=2, insert_id=2115812)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2115813)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'max_num_labels_tns'), position=1, insert_id=2115814)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=2115815)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=current_input), position=3)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=2115816)",
            "Insert(target_node=ASTNode(type=argument_list), node=('list', None), position=1, insert_id=2115817)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=2115818)",
            "Insert(target_node=IN(type=list), node=('[', '['), position=0, insert_id=2115819)",
            "Insert(target_node=IN(type=list), node=('integer', '1'), position=1, insert_id=2115820)",
            "Move(target_node=IN(type=list), node=ASTNode(type=,, text=,), position=2)",
            "Insert(target_node=IN(type=list), node=('subscript', None), position=3, insert_id=2115821)",
            "Insert(target_node=IN(type=list), node=(']', ']'), position=4, insert_id=2115822)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'label_shape'), position=0, insert_id=2115823)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=2115824)",
            "Insert(target_node=IN(type=subscript), node=('integer', '1'), position=2, insert_id=2115825)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=2115826)",
            "Delete(target_node=ASTNode(type=identifier, text=max_num_labels_tns))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 23,
        "number": 8296,
        "neg_line": [
            "-return tf.expand_dims(tf.range(label_shape[1]), 0) < current_input",
            "-init = tf.cast(tf.fill(max_num_labels_tns, 0), tf.bool)"
        ],
        "pos_line": [
            "+return tf.expand_dims(tf.range(label_shape[1]), 0) < tf.fill(max_num_labels_tns, current_input)",
            "+init = tf.cast(tf.fill([1, label_shape[1]], 0), tf.bool)"
        ],
        "core_change": "-return tf.expand_dims(tf.range(label_shape[1]), 0) < current_input +return tf.expand_dims(tf.range(label_shape[1]), 0) < tf.fill(max_num_labels_tns, current_input) -init = tf.cast(tf.fill(max_num_labels_tns, 0), tf.bool) +init = tf.cast(tf.fill([1, label_shape[1]], 0), tf.bool)",
        "core_API": "pack"
    },
    {
        "commit_hash": "91e423680a40e07346adc7e17f995e90632e44d1",
        "index": "41e395d3d8..bb859679fe 100644",
        "commit_message": "lintfixbot: Auto-commit fixed lint errors in codebase\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "functional",
        "change": [
            "def test_frontend_function(",
            "# strip the decorator to get an Ivy array",
            "# ToDo, fix testing for jax frontend for x32",
            "if frontend == \"jax\":",
            "-            importlib.import_module(",
            "-                'ivy.functional.frontends.jax').config.update('jax_enable_x64', True)",
            "+            importlib.import_module(\"ivy.functional.frontends.jax\").config.update(",
            "+                \"jax_enable_x64\", True",
            "+            )",
            "ret = get_frontend_ret(frontend_fn, *args_ivy, **kwargs_ivy)",
            "if with_out:",
            "if not inspect.isclass(ret):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text='jax_enable_x64'), value='\"jax_enable_x64\"')",
            "Update(target_node=ASTNode(type=string, text='ivy.functional.frontends.jax'), value='\"ivy.functional.frontends.jax\"')"
        ],
        "plus_line": 3,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 8298,
        "neg_line": [
            "-importlib.import_module(",
            "-'ivy.functional.frontends.jax').config.update('jax_enable_x64', True)"
        ],
        "pos_line": [
            "+importlib.import_module(\"ivy.functional.frontends.jax\").config.update(",
            "+\"jax_enable_x64\", True",
            "+)"
        ],
        "core_change": "-importlib.import_module( -'ivy.functional.frontends.jax').config.update('jax_enable_x64', True) +importlib.import_module(\"ivy.functional.frontends.jax\").config.update( +\"jax_enable_x64\", True +)",
        "core_API": "import_module"
    },
    {
        "commit_hash": "7c622482e824f9dd6fcb5d46edb3726ba587c466",
        "index": "66875a028..ee81a3adf 100755",
        "commit_message": "fix resize_token_embeddings (#11572)\n\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin, PushToHubMix",
            ")",
            "",
            "# Build new embeddings",
            "-        new_embeddings = nn.Embedding(new_num_tokens, old_embedding_dim).to(self.device)",
            "+        new_embeddings = nn.Embedding(new_num_tokens, old_embedding_dim).to(",
            "+            self.device, dtype=old_embeddings.weight.dtype",
            "+        )",
            "",
            "# initialize all new embeddings (in particular added tokens)",
            "self._init_weights(new_embeddings)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1539039)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1539040)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=1539041)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1539042)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=1539043)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1539044)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1539045)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dtype'), position=2, insert_id=1539046)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'old_embeddings'), position=0, insert_id=1539047)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1539048)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'weight'), position=2, insert_id=1539049)"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 8304,
        "neg_line": [
            "-new_embeddings = nn.Embedding(new_num_tokens, old_embedding_dim).to(self.device)"
        ],
        "pos_line": [
            "+new_embeddings = nn.Embedding(new_num_tokens, old_embedding_dim).to(",
            "+self.device, dtype=old_embeddings.weight.dtype",
            "+)"
        ],
        "core_change": "-new_embeddings = nn.Embedding(new_num_tokens, old_embedding_dim).to(self.device) +new_embeddings = nn.Embedding(new_num_tokens, old_embedding_dim).to( +self.device, dtype=old_embeddings.weight.dtype +)",
        "core_API": "Embedding"
    },
    {
        "commit_hash": "23efadd9a394d9d69e10e8539a7332665b118df0",
        "index": "3e409f9c..31fc0c6e 100644",
        "commit_message": "upgrade to pytorch 1.2 (#3182)\n\n* first attempt at pytorch 1.2\n\n* explicit is better than implicit\n\n* more explicit\n\n* attempt to fix flaky tests\n\n* pylint\n\n* no disable dropout\n\n* disable dropout by default\n\n* restore dropout, don't deepcopy\n\n* change batch size for biaffine_dependency_parser_multilang_test, maybe that will make it pass? :(\n\n* try batch size 10\n\n* ignore bad gradient parameter\n\n* cleanup\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class CrfTagger(Model):",
            "if tags is not None:",
            "# Add negative log-likelihood as loss",
            "log_likelihood = self.crf(logits, tags, mask)",
            "-            output[\"loss\"] = -log_likelihood",
            "+",
            "+            # It's not clear why, but pylint seems to think `log_likelihood` is tuple",
            "+            # (in fact, it's a torch.Tensor), so we need a disable.",
            "+            output[\"loss\"] = -log_likelihood  # pylint: disable=invalid-unary-operand-type",
            "",
            "# Represent viterbi tags as \"class probabilities\" that we can",
            "# feed into the metrics"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 8306,
        "neg_line": [
            "-output[\"loss\"] = -log_likelihood"
        ],
        "pos_line": [
            "+",
            "+# It's not clear why, but pylint seems to think `log_likelihood` is tuple",
            "+# (in fact, it's a torch.Tensor), so we need a disable.",
            "+output[\"loss\"] = -log_likelihood  # pylint: disable=invalid-unary-operand-type"
        ],
        "core_change": "-output[\"loss\"] = -log_likelihood + +# It's not clear why, but pylint seems to think `log_likelihood` is tuple +# (in fact, it's a torch.Tensor), so we need a disable. +output[\"loss\"] = -log_likelihood  # pylint: disable=invalid-unary-operand-type",
        "core_API": "crf"
    },
    {
        "commit_hash": "1b11b4eeb6bd4b163ce351af680c5a14d823443d",
        "index": "b7475495..d4499501 100644",
        "commit_message": "Fix shape inference issue with TF.resize_images\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def resize_images(X, height_factor, width_factor, dim_ordering):",
            "positive integers.",
            "'''",
            "if dim_ordering == 'th':",
            "+        original_shape = int_shape(X)",
            "new_shape = tf.shape(X)[2:]",
            "new_shape *= tf.constant(np.array([height_factor, width_factor]).astype('int32'))",
            "X = permute_dimensions(X, [0, 2, 3, 1])",
            "X = tf.image.resize_nearest_neighbor(X, new_shape)",
            "-        return permute_dimensions(X, [0, 3, 1, 2])",
            "+        X = permute_dimensions(X, [0, 3, 1, 2])",
            "+        X.set_shape((None, None, original_shape[2] * height_factor, original_shape[3] * width_factor))",
            "+        return X",
            "elif dim_ordering == 'tf':",
            "+        original_shape = int_shape(X)",
            "new_shape = tf.shape(X)[1:3]",
            "new_shape *= tf.constant(np.array([height_factor, width_factor]).astype('int32'))",
            "-        return tf.image.resize_nearest_neighbor(X, new_shape)",
            "+        X = tf.image.resize_nearest_neighbor(X, new_shape)",
            "+        X.set_shape((None, original_shape[1] * height_factor, original_shape[2] * width_factor, None))",
            "+        return X",
            "else:",
            "raise Exception('Invalid dim_ordering: ' + dim_ordering)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text='''\nif dim_ordering == 'th':\nnew_shape = tf.shape(X)[2:]\nnew_shape *= tf.constant(np.array([height_factor, width_factor]).astype('int32'))\nX = permute_dimensions(X, [0, 2, 3, 1])\nX = tf.image.resize_nearest_neighbor(X, new_shape)\n        return permute_dimensions(X, [0, 3, 1, 2])\nelif dim_ordering == 'tf':\nnew_shape = tf.shape(X)[1:3]\nnew_shape *= tf.constant(np.array([height_factor, width_factor]).astype('int32'))\n        return tf.image.resize_nearest_neighbor(X, new_shape)\nelse:\nraise Exception('Invalid dim_ordering: '), value=\"'''\\nif dim_ordering == 'th':\\n        original_shape = int_shape(X)\\nnew_shape = tf.shape(X)[2:]\\nnew_shape *= tf.constant(np.array([height_factor, width_factor]).astype('int32'))\\nX = permute_dimensions(X, [0, 2, 3, 1])\\nX = tf.image.resize_nearest_neighbor(X, new_shape)\\n        X = permute_dimensions(X, [0, 3, 1, 2])\\n        X.set_shape((None, None, original_shape[2] * height_factor, original_shape[3] * width_factor))\\n        return X\\nelif dim_ordering == 'tf':\\n        original_shape = int_shape(X)\\nnew_shape = tf.shape(X)[1:3]\\nnew_shape *= tf.constant(np.array([height_factor, width_factor]).astype('int32'))\\n        X = tf.image.resize_nearest_neighbor(X, new_shape)\\n        X.set_shape((None, original_shape[1] * height_factor, original_shape[2] * width_factor, None))\\n        return X\\nelse:\\nraise Exception('Invalid dim_ordering: '\")"
        ],
        "plus_line": 8,
        "minus_line": 2,
        "AST_diff_line": 1,
        "number": 8311,
        "neg_line": [
            "-return permute_dimensions(X, [0, 3, 1, 2])",
            "-return tf.image.resize_nearest_neighbor(X, new_shape)"
        ],
        "pos_line": [
            "+original_shape = int_shape(X)",
            "+X = permute_dimensions(X, [0, 3, 1, 2])",
            "+X.set_shape((None, None, original_shape[2] * height_factor, original_shape[3] * width_factor))",
            "+return X",
            "+original_shape = int_shape(X)",
            "+X = tf.image.resize_nearest_neighbor(X, new_shape)",
            "+X.set_shape((None, original_shape[1] * height_factor, original_shape[2] * width_factor, None))",
            "+return X"
        ],
        "core_change": "+original_shape = int_shape(X) -return permute_dimensions(X, [0, 3, 1, 2]) +X = permute_dimensions(X, [0, 3, 1, 2]) +X.set_shape((None, None, original_shape[2] * height_factor, original_shape[3] * width_factor)) +return X +original_shape = int_shape(X) -return tf.image.resize_nearest_neighbor(X, new_shape) +X = tf.image.resize_nearest_neighbor(X, new_shape) +X.set_shape((None, original_shape[1] * height_factor, original_shape[2] * width_factor, None)) +return X",
        "core_API": "shape"
    },
    {
        "commit_hash": "0afa5755b2212a783f861f03006a73f883100e81",
        "index": "8f8e42fd..5aee3728 100644",
        "commit_message": "Fixes warnings and add compatibility stub in torch solve (#1235)\n\n* add a compatibiliy stub for torch solve\n\n* change missing _torch_solve_cast calls\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* bump pytorch version to 1.7.1\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def _torch_solve_cast(input: torch.Tensor, A: torch.Tensor) -> Tuple[torch.Tenso",
            "if dtype not in (torch.float32, torch.float64):",
            "dtype = torch.float32",
            "",
            "-    out1, out2 = torch.solve(input.to(dtype), A.to(dtype))",
            "+    out = solve(A.to(dtype), input.to(dtype))",
            "",
            "-    return (out1.to(input.dtype), out2.to(input.dtype))",
            "+    return (out.to(input.dtype), out)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=out1), value='out')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=solve), position=0)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=argument_list), position=2)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=argument_list), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=out2), value='out')",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=out2), position=3)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=), text=)), position=4)",
            "Update(target_node=ASTNode(type=identifier, text=out1), value='out')",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=out2))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=to))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=input))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=dtype))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=), text=)))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 24,
        "number": 8314,
        "neg_line": [
            "-out1, out2 = torch.solve(input.to(dtype), A.to(dtype))",
            "-return (out1.to(input.dtype), out2.to(input.dtype))"
        ],
        "pos_line": [
            "+out = solve(A.to(dtype), input.to(dtype))",
            "+return (out.to(input.dtype), out)"
        ],
        "core_change": "-out1, out2 = torch.solve(input.to(dtype), A.to(dtype)) +out = solve(A.to(dtype), input.to(dtype)) -return (out1.to(input.dtype), out2.to(input.dtype)) +return (out.to(input.dtype), out)",
        "core_API": "solve"
    },
    {
        "commit_hash": "b168b0161987e95f9571c6fdaaef448c03ac396c",
        "index": "f08061f3..1e97ab21 100644",
        "commit_message": "Fix visualization for ENAS micro (#2813)\n\n\n",
        "file": "nni.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Mutator(BaseMutator):",
            "\"\"\"",
            "if self._connect_all:",
            "return self._all_connect_tensor_reduction(mutable.reduction, tensor_list), \\",
            "-                torch.ones(mutable.n_candidates)",
            "+                torch.ones(mutable.n_candidates).bool()",
            "mask = self._get_decision(mutable)",
            "assert len(mask) == mutable.n_candidates, \\",
            "\"Invalid mask, expected {} to be of length {}.\".format(mask, mutable.n_candidates)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"\"\"\nif self._connect_all:\nreturn self._all_connect_tensor_reduction(mutable.reduction, tensor_list), \\\n                torch.ones(mutable.n_candidates)\nmask = self._get_decision(mutable)\nassert len(mask) == mutable.n_candidates, \\\n\"Invalid mask, expected {} to be of length {}.\"), value='\"\"\"\\nif self._connect_all:\\nreturn self._all_connect_tensor_reduction(mutable.reduction, tensor_list), \\\\\\n                torch.ones(mutable.n_candidates).bool()\\nmask = self._get_decision(mutable)\\nassert len(mask) == mutable.n_candidates, \\\\\\n\"Invalid mask, expected {} to be of length {}.\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 8315,
        "neg_line": [
            "-torch.ones(mutable.n_candidates)"
        ],
        "pos_line": [
            "+torch.ones(mutable.n_candidates).bool()"
        ],
        "core_change": "-torch.ones(mutable.n_candidates) +torch.ones(mutable.n_candidates).bool()",
        "core_API": "_all_connect_tensor_reduction"
    },
    {
        "commit_hash": "6fdc016dc651550c9f41dff186b15fa0051636bf",
        "index": "70d0896..c947882 100644",
        "commit_message": "fix small bug\n\n",
        "file": "pytorch-CycleGAN-and-pix2pix.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class BaseModel():",
            "save_filename = '%s_net_%s.pth' % (which_epoch, name)",
            "save_path = os.path.join(self.save_dir, save_filename)",
            "net = getattr(self, 'net' + name)",
            "-                torch.module.save(net.cpu().state_dict(), save_path)",
            "+                torch.save(net.module.cpu().state_dict(), save_path)",
            "if len(self.gpu_ids) and torch.cuda.is_available():",
            "net.cuda(self.gpu_ids[0])"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=torch), position=0)",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=888012)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=888013)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=net), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'module'), position=2, insert_id=888014)",
            "Delete(target_node=ASTNode(type=identifier, text=module))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 8319,
        "neg_line": [
            "-torch.module.save(net.cpu().state_dict(), save_path)"
        ],
        "pos_line": [
            "+torch.save(net.module.cpu().state_dict(), save_path)"
        ],
        "core_change": "-torch.module.save(net.cpu().state_dict(), save_path) +torch.save(net.module.cpu().state_dict(), save_path)",
        "core_API": "join"
    },
    {
        "commit_hash": "ed63b7ee9e68ba120893aaaaca0d4525a56ed484",
        "index": "ae23ef87..1682f7e4 100644",
        "commit_message": "upgrade to pytorch 0.4.0 (#1126)\n\n* bump pytorch to 0.4 + fix sanitize\n\n* remove check for Variable in block_orthogonal\n\n* rename parameter split_size=\n\n* remove checks for Variable\n\n* fix more tests\n\n* fixes\n\n* get tests to pass\n\n* fix warnings\n\n* get rid of some of the Variables\n\n* more tests passing\n\n* more elimination of variables\n\n* finish removing all Variables\n\n* pylint and such\n\n* a few fixes\n\n* move torch.no_grad into model.forward_on_instances\n\n* more pytorch 0.4 changes\n\n* detach() -> data\n\n* pylint\n\n* fix bad tensor creation\n\n* fix types\n\n* remove print statement\n\n* more 0.4 goodness\n\n* factor out is_tensor\n\n* add no_grad to elmo command\n\n* cleanup\n\n* remove TODO\n\n* address PR feedback\n\n* replace all() with item()\n\n* more cleanup\n\n* further cleanup\n\n* remove Variable\n\n* really fix merge conflict\n\n* fix pylint\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SimpleSeq2SeqWithoutAttentionTest(ModelTestCase):",
            "batch_size = 5",
            "num_decoding_steps = 5",
            "num_classes = 10",
            "-        sample_logits = Variable(torch.randn(batch_size, num_decoding_steps-1, num_classes))",
            "-        sample_targets = Variable(torch.from_numpy(numpy.random.randint(0, num_classes,",
            "-                                                                        (batch_size, num_decoding_steps))))",
            "+        sample_logits = torch.randn(batch_size, num_decoding_steps-1, num_classes)",
            "+        sample_targets = torch.from_numpy(numpy.random.randint(0, num_classes,",
            "+                                                               (batch_size, num_decoding_steps)))",
            "# Mask should be either 0 or 1",
            "-        sample_mask = Variable(torch.from_numpy(numpy.random.randint(0, 2,",
            "-                                                                     (batch_size, num_decoding_steps))))",
            "+        sample_mask = torch.from_numpy(numpy.random.randint(0, 2,",
            "+                                                            (batch_size, num_decoding_steps)))",
            "expected_loss = sequence_cross_entropy_with_logits(sample_logits, sample_targets[:, 1:].contiguous(),",
            "sample_mask[:, 1:].contiguous())",
            "# pylint: disable=protected-access"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=call), position=2)",
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=call), position=2)",
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=call), position=2)",
            "Delete(target_node=ASTNode(type=identifier, text=Variable))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=identifier, text=Variable))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=identifier, text=Variable))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 5,
        "minus_line": 5,
        "AST_diff_line": 18,
        "number": 8321,
        "neg_line": [
            "-sample_logits = Variable(torch.randn(batch_size, num_decoding_steps-1, num_classes))",
            "-sample_targets = Variable(torch.from_numpy(numpy.random.randint(0, num_classes,",
            "-(batch_size, num_decoding_steps))))",
            "-sample_mask = Variable(torch.from_numpy(numpy.random.randint(0, 2,",
            "-(batch_size, num_decoding_steps))))"
        ],
        "pos_line": [
            "+sample_logits = torch.randn(batch_size, num_decoding_steps-1, num_classes)",
            "+sample_targets = torch.from_numpy(numpy.random.randint(0, num_classes,",
            "+(batch_size, num_decoding_steps)))",
            "+sample_mask = torch.from_numpy(numpy.random.randint(0, 2,",
            "+(batch_size, num_decoding_steps)))"
        ],
        "core_change": "-sample_logits = Variable(torch.randn(batch_size, num_decoding_steps-1, num_classes)) -sample_targets = Variable(torch.from_numpy(numpy.random.randint(0, num_classes, -(batch_size, num_decoding_steps)))) +sample_logits = torch.randn(batch_size, num_decoding_steps-1, num_classes) +sample_targets = torch.from_numpy(numpy.random.randint(0, num_classes, +(batch_size, num_decoding_steps))) -sample_mask = Variable(torch.from_numpy(numpy.random.randint(0, 2, -(batch_size, num_decoding_steps)))) +sample_mask = torch.from_numpy(numpy.random.randint(0, 2, +(batch_size, num_decoding_steps)))",
        "core_API": "randn"
    },
    {
        "commit_hash": "0c166dcfb8b9e9d79d013b7967a59dfb1ac4b804",
        "index": "443b15f7..8a7e26be 100644",
        "commit_message": "define and apply YAPF formatting  (#1039)\n\n* define YAPF\n\n* yapf kornia -rip\n\n* yapf examples -rip\n\n* yapf test -rip\n\n* Update CONTRIBUTING.rst\n\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\n\n* update config\n\n* fix 4lint\n\n* mypy\n\n* mypy\n\n* mypy\n\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def median_blur(input: torch.Tensor,",
            "b, c, h, w = input.shape",
            "",
            "# map the local window to single vector",
            "-    features: torch.Tensor = F.conv2d(",
            "-        input.reshape(b * c, 1, h, w), kernel, padding=padding, stride=1)",
            "+    features: torch.Tensor = F.conv2d(input.reshape(b * c, 1, h, w), kernel, padding=padding, stride=1)",
            "features = features.view(b, c, -1, h, w)  # BxCx(K_h * K_w)xHxW",
            "",
            "# compute the median along the feature axis"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 8327,
        "neg_line": [
            "-features: torch.Tensor = F.conv2d(",
            "-input.reshape(b * c, 1, h, w), kernel, padding=padding, stride=1)"
        ],
        "pos_line": [
            "+features: torch.Tensor = F.conv2d(input.reshape(b * c, 1, h, w), kernel, padding=padding, stride=1)"
        ],
        "core_change": "-features: torch.Tensor = F.conv2d( -input.reshape(b * c, 1, h, w), kernel, padding=padding, stride=1) +features: torch.Tensor = F.conv2d(input.reshape(b * c, 1, h, w), kernel, padding=padding, stride=1)",
        "core_API": "conv2d"
    },
    {
        "commit_hash": "75e6c025d651f7b52d44fcee54d64c74f88d3fac",
        "index": "114d980..d18722e 100644",
        "commit_message": "Fix for TF 0.12.1 and keras 1.2.1 (#65)\n\n* added fix for TF 0.12.1 and keras 1.2.1\n\n* updated readme\n\n",
        "file": "keras-rl.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def huber_loss(y_true, y_pred, clip_value):",
            "# Spacial case for infinity since Tensorflow does have problems",
            "# if we compare `K.abs(x) < np.inf`.",
            "return .5 * K.square(x)",
            "-",
            "+",
            "condition = K.abs(x) < clip_value",
            "squared_loss = .5 * K.square(x)",
            "linear_loss = clip_value * (K.abs(x) - .5 * clip_value)",
            "if K._BACKEND == 'tensorflow':",
            "import tensorflow as tf",
            "-        return tf.select(condition, squared_loss, linear_loss)  # condition, true, false",
            "+        return tf.where(condition, squared_loss, linear_loss)  # condition, true, false",
            "elif K._BACKEND == 'theano':",
            "from theano import tensor as T",
            "return T.switch(condition, squared_loss, linear_loss)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=select), value='where')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 8329,
        "neg_line": [
            "-",
            "-return tf.select(condition, squared_loss, linear_loss)  # condition, true, false"
        ],
        "pos_line": [
            "+",
            "+return tf.where(condition, squared_loss, linear_loss)  # condition, true, false"
        ],
        "core_change": "- + -return tf.select(condition, squared_loss, linear_loss)  # condition, true, false +return tf.where(condition, squared_loss, linear_loss)  # condition, true, false",
        "core_API": "abs"
    },
    {
        "commit_hash": "74efc21403477d103bd426ae64c37b7a30d8f4bf",
        "index": "e0dbdc60..be84a775 100644",
        "commit_message": "Fix attention order in unit tests (fixes #195) (#197)\n\n\n",
        "file": "fairseq.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestIncrementalDecoder(FairseqIncrementalDecoder):",
            "probs[:, i, self.dictionary.eos()] = 1.0",
            "",
            "# random attention",
            "-        attn = torch.rand(bbsz, src_len, tgt_len)",
            "+        attn = torch.rand(bbsz, tgt_len, src_len)",
            "",
            "return Variable(probs), Variable(attn)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=identifier, text=src_len), node=ASTNode(type=argument_list), position=5)",
            "Move(target_node=ASTNode(type=,, text=,), node=ASTNode(type=argument_list), position=6)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 8332,
        "neg_line": [
            "-attn = torch.rand(bbsz, src_len, tgt_len)"
        ],
        "pos_line": [
            "+attn = torch.rand(bbsz, tgt_len, src_len)"
        ],
        "core_change": "-attn = torch.rand(bbsz, src_len, tgt_len) +attn = torch.rand(bbsz, tgt_len, src_len)",
        "core_API": "eos"
    },
    {
        "commit_hash": "00250118f4d250f95b24ed61ba34b75450bfacb0",
        "index": "d954662c..4f0efb49 100644",
        "commit_message": "fix tu squeeze bug\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def read_file(folder, prefix, name, dtype=None):",
            "def cat(seq):",
            "seq = [item for item in seq if item is not None]",
            "seq = [item.unsqueeze(-1) if item.dim() == 1 else item for item in seq]",
            "-    return torch.cat(seq, dim=-1).squeeze() if len(seq) > 0 else None",
            "+    return torch.cat(seq, dim=-1) if len(seq) > 0 else None",
            "",
            "",
            "def split(data, batch):"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=IN(type=root), node=ASTNode(type=call), position=0)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=squeeze))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 8337,
        "neg_line": [
            "-return torch.cat(seq, dim=-1).squeeze() if len(seq) > 0 else None"
        ],
        "pos_line": [
            "+return torch.cat(seq, dim=-1) if len(seq) > 0 else None"
        ],
        "core_change": "-return torch.cat(seq, dim=-1).squeeze() if len(seq) > 0 else None +return torch.cat(seq, dim=-1) if len(seq) > 0 else None",
        "core_API": "unsqueeze"
    },
    {
        "commit_hash": "51acf779f2b9e52714fc617ceb81dd86b7b50788",
        "index": "e8e7ad4a..f22845cf 100644",
        "commit_message": "Add TableTextRetriever (#1529)\n\n* first draft / notes on new primitives\n\n* wip label / feedback refactor\n\n* rename doc.text -> doc.content. add doc.content_type\n\n* add datatype for content\n\n* remove faq_question_field from ES and weaviate. rename text_field -> content_field in docstores. update tutorials for content field\n\n* update converters for . Add warning for empty\n\n* renam label.question -> label.query. Allow sorting of Answers.\n\n* WIP primitives\n\n* update ui/reader for new Answer format\n\n* Improve Label. First refactoring of MultiLabel. Adjust eval code\n\n* fixed workflow conflict with introducing new one (#1472)\n\n* Add latest docstring and tutorial changes\n\n* make add_eval_data() work again\n\n* fix reader formats. WIP fix _extract_docs_and_labels_from_dict\n\n* fix test reader\n\n* Add latest docstring and tutorial changes\n\n* fix another test case for reader\n\n* fix mypy in farm reader.eval()\n\n* fix mypy in farm reader.eval()\n\n* WIP ORM refactor\n\n* Add latest docstring and tutorial changes\n\n* fix mypy weaviate\n\n* make label and multilabel dataclasses\n\n* bump mypy env in CI to python 3.8\n\n* WIP refactor Label ORM\n\n* WIP refactor Label ORM\n\n* simplify tests for individual doc stores\n\n* WIP refactoring markers of tests\n\n* test alternative approach for tests with existing parametrization\n\n* WIP refactor ORMs\n\n* fix skip logic of already parametrized tests\n\n* fix weaviate behaviour in tests - not parametrizing it in our general test cases.\n\n* Add latest docstring and tutorial changes\n\n* fix some tests\n\n* remove sql from document_store_types\n\n* fix markers for generator and pipeline test\n\n* remove inmemory marker\n\n* remove unneeded elasticsearch markers\n\n* add dataclasses-json dependency. adjust ORM to just store JSON repr\n\n* ignore type as dataclasses_json seems to miss functionality here\n\n* update readme and contributing.md\n\n* update contributing\n\n* adjust example\n\n* fix duplicate doc handling for custom index\n\n* Add latest docstring and tutorial changes\n\n* fix some ORM issues. fix get_all_labels_aggregated.\n\n* update drop flags where get_all_labels_aggregated() was used before\n\n* Add latest docstring and tutorial changes\n\n* add to_json(). add + fix tests\n\n* fix no_answer handling in label / multilabel\n\n* fix duplicate docs in memory doc store. change primary key for sql doc table\n\n* fix mypy issues\n\n* fix mypy issues\n\n* haystack/retriever/base.py\n\n* fix test_write_document_meta[elastic]\n\n* fix test_elasticsearch_custom_fields\n\n* fix test_labels[elastic]\n\n* fix crawler\n\n* fix converter\n\n* fix docx converter\n\n* fix preprocessor\n\n* fix test_utils\n\n* fix tfidf retriever. fix selection of docstore in tests with multiple fixtures / parameterizations\n\n* Add latest docstring and tutorial changes\n\n* fix crawler test. fix ocrconverter attribute\n\n* fix test_elasticsearch_custom_query\n\n* fix generator pipeline\n\n* fix ocr converter\n\n* fix ragenerator\n\n* Add latest docstring and tutorial changes\n\n* fix test_load_and_save_yaml for elasticsearch\n\n* fixes for pipeline tests\n\n* fix faq pipeline\n\n* fix pipeline tests\n\n* Add latest docstring and tutorial changes\n\n* Add MultimodalRetriever\n\n* Add latest docstring and tutorial changes\n\n* fix weaviate\n\n* Add latest docstring and tutorial changes\n\n* trigger CI\n\n* satisfy mypy\n\n* Add latest docstring and tutorial changes\n\n* satisfy mypy\n\n* Add latest docstring and tutorial changes\n\n* trigger CI\n\n* fix question generation test\n\n* fix ray. fix Q-generation\n\n* fix translator test\n\n* satisfy mypy\n\n* wip refactor feedback rest api\n\n* fix rest api feedback endpoint\n\n* fix doc classifier\n\n* remove relation of Labels -> Docs in SQL ORM\n\n* fix faiss/milvus tests\n\n* fix doc classifier test\n\n* fix eval test\n\n* fixing eval issues\n\n* Add latest docstring and tutorial changes\n\n* fix mypy\n\n* WIP replace dataclasses-json with manual serialization\n\n* Add methods to MultimodalRetriever\n\n* Add latest docstring and tutorial changes\n\n* revert to dataclass-json serialization for now. remove debug prints.\n\n* update docstrings\n\n* fix extractor. fix Answer Span init\n\n* fix api test\n\n* keep meta data of answers in reader.run()\n\n* fix meta handling\n\n* adress review feedback\n\n* Add latest docstring and tutorial changes\n\n* make document=None for open domain labels\n\n* add import\n\n* fix print utils\n\n* fix rest api\n\n* Add methods and tests\n\n* Add latest docstring and tutorial changes\n\n* Fix mypy\n\n* Add latest docstring and tutorial changes\n\n* Add type hints and doc strings\n\n* Make use of initialize_device_settings\n\n* Move serialization of pd.DataFrame to schema.py\n\n* Fix mypy\n\n* Adapt Document's from_dict method\n\n* Update docstrings\n\n* Add latest docstring and tutorial changes\n\n* Fix mypy\n\n* Fix mypy\n\n* Fix Document's from_dict method\n\n* Fix Document's to_dict method\n\n* Change handling of table metadata\n\n* Add latest docstring and tutorial changes\n\n* Change naming from Multimodal to TableText\n\n* Turn off tokenizers_parallelism in retriever tests\n\n* Add latest docstring and tutorial changes\n\n* Remove turning off tokenizers_parallelism in retriever tests\n\n* Adapt convert_es_hit_to_document\n\n* Change embed_surrounding_context to embed_meta_fields\n\n* Add latest docstring and tutorial changes\n\n* Add check if torch.distributed is available\n\n* Set n_gpu to 0 in training test\n\n* Set HIP_LAUNCH_BLOCKING to 1\n\n* Set HIP_LAUNCH_BLOCKING to \"1\"\n\n* Set use_gpu to False\n\n* Use DataParallel only if more than one device\n\n* Remove --find-links=https://download.pytorch.org/whl/torch_stable.html\n\nCo-authored-by: Malte Pietsch <malte.pietsch@deepset.ai>\nCo-authored-by: Markus Paff <markuspaff.mp@gmail.com>\nCo-authored-by: github-actions[bot] <41898282+github-actions[bot]@users.noreply.github.com>\n",
        "file": "haystack.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TextSimilarityHead(PredictionHead):",
            "",
            "# Check if DDP is initialized",
            "try:",
            "-            rank = torch.distributed.get_rank()",
            "+            if torch.distributed.is_available():",
            "+                rank = torch.distributed.get_rank()",
            "+            else:",
            "+                rank = -1",
            "except (AssertionError, RuntimeError):",
            "rank = -1"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=block), node=('if_statement', None), position=0, insert_id=246029)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=246030)",
            "Insert(target_node=IN(type=if_statement), node=('call', None), position=1, insert_id=246031)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=246032)",
            "Move(target_node=IN(type=if_statement), node=ASTNode(type=block), position=3)",
            "Insert(target_node=IN(type=if_statement), node=('else_clause', None), position=4, insert_id=246033)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=246034)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=246035)",
            "Insert(target_node=IN(type=else_clause), node=('else', 'else'), position=0, insert_id=246036)",
            "Insert(target_node=IN(type=else_clause), node=(':', ':'), position=1, insert_id=246037)",
            "Insert(target_node=IN(type=else_clause), node=('block', None), position=2, insert_id=246038)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=246039)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=246040)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'is_available'), position=2, insert_id=246041)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=246042)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=246043)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=246044)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=246045)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=246046)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'distributed'), position=2, insert_id=246047)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=246048)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'rank'), position=0, insert_id=246049)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=246050)",
            "Insert(target_node=IN(type=assignment), node=('unary_operator', '-1'), position=2, insert_id=246051)"
        ],
        "plus_line": 4,
        "minus_line": 1,
        "AST_diff_line": 24,
        "number": 8347,
        "neg_line": [
            "-rank = torch.distributed.get_rank()"
        ],
        "pos_line": [
            "+if torch.distributed.is_available():",
            "+rank = torch.distributed.get_rank()",
            "+else:",
            "+rank = -1"
        ],
        "core_change": "-rank = torch.distributed.get_rank() +if torch.distributed.is_available(): +rank = torch.distributed.get_rank() +else: +rank = -1",
        "core_API": "get_rank"
    },
    {
        "commit_hash": "09fc038e202c5411c4292d065f4ad04369bd87fc",
        "index": "88cc381d9..f4d368310 100644",
        "commit_message": "fix linter and add docs\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class DecoderLayer(nn.Module):",
            "self.norm1 = LayerNorm(size)",
            "self.norm2 = LayerNorm(size)",
            "self.norm3 = LayerNorm(size)",
            "-        self.dropout = nn.Dropout(dropout)",
            "+        self.dropout = nn.Dropout(dropout_rate)",
            "",
            "def forward(self, tgt, tgt_mask, memory, memory_mask):",
            "\"\"\"Compute decoded features"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=dropout), value='dropout_rate')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 8351,
        "neg_line": [
            "-self.dropout = nn.Dropout(dropout)"
        ],
        "pos_line": [
            "+self.dropout = nn.Dropout(dropout_rate)"
        ],
        "core_change": "-self.dropout = nn.Dropout(dropout) +self.dropout = nn.Dropout(dropout_rate)",
        "core_API": "Dropout"
    },
    {
        "commit_hash": "8d3c3ef45ce1d530aa3751f6187f18cfd9c40791",
        "index": "250342a..cf50a5d 100644",
        "commit_message": "Fix weight decay comment (#4228)\n\n\n",
        "file": "yolov5.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "def train(hyp,  # path/to/hyp.yaml or hyp dictionary",
            "for v in model.modules():",
            "if hasattr(v, 'bias') and isinstance(v.bias, nn.Parameter):  # bias",
            "g2.append(v.bias)",
            "-        if isinstance(v, nn.BatchNorm2d):  # weight with decay",
            "+        if isinstance(v, nn.BatchNorm2d):  # weight (no decay)",
            "g0.append(v.weight)",
            "-        elif hasattr(v, 'weight') and isinstance(v.weight, nn.Parameter):  # weight without decay",
            "+        elif hasattr(v, 'weight') and isinstance(v.weight, nn.Parameter):  # weight (with decay)",
            "g1.append(v.weight)",
            "",
            "if opt.adam:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 8355,
        "neg_line": [
            "-if isinstance(v, nn.BatchNorm2d):  # weight with decay",
            "-elif hasattr(v, 'weight') and isinstance(v.weight, nn.Parameter):  # weight without decay"
        ],
        "pos_line": [
            "+if isinstance(v, nn.BatchNorm2d):  # weight (no decay)",
            "+elif hasattr(v, 'weight') and isinstance(v.weight, nn.Parameter):  # weight (with decay)"
        ],
        "core_change": "-if isinstance(v, nn.BatchNorm2d):  # weight with decay +if isinstance(v, nn.BatchNorm2d):  # weight (no decay) -elif hasattr(v, 'weight') and isinstance(v.weight, nn.Parameter):  # weight without decay +elif hasattr(v, 'weight') and isinstance(v.weight, nn.Parameter):  # weight (with decay)",
        "core_API": "modules"
    },
    {
        "commit_hash": "deb6d5087f1962b10d848e495b7f26e49801392b",
        "index": "af40258dab..9432d9a5f0 100644",
        "commit_message": "add min and max assertion for clip to all backends. (#3753)\n\n* add min and max assertion for clip to all backends.\n\n* remove min, max assertion for clip from `Ivy` backend.\n\n* fix core test for clip.\n\n* small reformat for clip core test.\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def clip(",
            "*,",
            "out: Optional[Union[tf.Tensor, tf.Variable]] = None,",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "+    assert tf.reduce_all(tf.less(x_min, x_max)), \"Min value must be less than max.\"",
            "if hasattr(x_min, \"dtype\") and hasattr(x_max, \"dtype\"):",
            "promoted_type = tf.experimental.numpy.promote_types(x.dtype, x_min.dtype)",
            "promoted_type = tf.experimental.numpy.promote_types(promoted_type, x_max.dtype)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('block', None), position=0, insert_id=1998493)",
            "Insert(target_node=IN(type=block), node=('assert_statement', None), position=0, insert_id=1998494)",
            "Insert(target_node=IN(type=assert_statement), node=('assert', 'assert'), position=0, insert_id=1998495)",
            "Insert(target_node=IN(type=assert_statement), node=('call', None), position=1, insert_id=1998496)",
            "Insert(target_node=IN(type=assert_statement), node=(',', ','), position=2, insert_id=1998497)",
            "Insert(target_node=IN(type=assert_statement), node=('string', '\"Min value must be less than max.\"'), position=3, insert_id=1998498)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1998499)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1998500)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=1998501)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1998502)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'reduce_all'), position=2, insert_id=1998503)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1998504)",
            "Insert(target_node=IN(type=argument_list), node=('call', None), position=1, insert_id=1998505)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1998506)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1998507)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1998508)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=1998509)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1998510)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'less'), position=2, insert_id=1998511)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1998512)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'x_min'), position=1, insert_id=1998513)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=1998514)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'x_max'), position=3, insert_id=1998515)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=1998516)",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 25,
        "number": 8358,
        "neg_line": [],
        "pos_line": [
            "+assert tf.reduce_all(tf.less(x_min, x_max)), \"Min value must be less than max.\""
        ],
        "core_change": "+assert tf.reduce_all(tf.less(x_min, x_max)), \"Min value must be less than max.\"",
        "core_API": "reduce_all"
    },
    {
        "commit_hash": "c589eae2b83be5206dab7a899738a0995624cc82",
        "index": "dedafac94..559cc4a3f 100644",
        "commit_message": "[Longformer For Question Answering] Conversion script, doc, small fixes (#4593)\n\n* add new longformer for question answering model\n\n* add new config as well\n\n* fix links\n\n* fix links part 2\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "models",
        "change": [
            "logger = logging.getLogger(__name__)",
            "LONGFORMER_PRETRAINED_CONFIG_ARCHIVE_MAP = {",
            "\"longformer-base-4096\": \"https://s3.amazonaws.com/models.huggingface.co/bert/allenai/longformer-base-4096/config.json\",",
            "\"longformer-large-4096\": \"https://s3.amazonaws.com/models.huggingface.co/bert/allenai/longformer-large-4096/config.json\",",
            "+    \"longformer-large-4096-finetuned-triviaqa\": \"https://s3.amazonaws.com/models.huggingface.co/bert/allenai/longformer-large-4096-finetuned-triviaqa/config.json\",",
            "}"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=dictionary), node=('pair', None), position=5, insert_id=2689742)",
            "Insert(target_node=ASTNode(type=dictionary), node=(',', ','), position=6, insert_id=2689743)",
            "Insert(target_node=IN(type=pair), node=('string', '\"longformer-large-4096-finetuned-triviaqa\"'), position=0, insert_id=2689744)",
            "Insert(target_node=IN(type=pair), node=(':', ':'), position=1, insert_id=2689745)",
            "Insert(target_node=IN(type=pair), node=('string', '\"https://s3.amazonaws.com/models.huggingface.co/bert/allenai/longformer-large-4096-finetuned-triviaqa/config.json\"'), position=2, insert_id=2689746)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 5,
        "number": 8359,
        "neg_line": [],
        "pos_line": [
            "+\"longformer-large-4096-finetuned-triviaqa\": \"https://s3.amazonaws.com/models.huggingface.co/bert/allenai/longformer-large-4096-finetuned-triviaqa/config.json\","
        ],
        "core_change": "+\"longformer-large-4096-finetuned-triviaqa\": \"https://s3.amazonaws.com/models.huggingface.co/bert/allenai/longformer-large-4096-finetuned-triviaqa/config.json\",",
        "core_API": "getLogger"
    },
    {
        "commit_hash": "43043ee4d5c672f7fbc22ac9559e8164731fd053",
        "index": "031c5ff5c8..a6e14257ab 100644",
        "commit_message": "[RLlib] Tf2x preparation; part 2 (upgrading `try_import_tf()`). (#9136)\n\n* WIP.\n\n* Fixes.\n\n* LINT.\n\n* WIP.\n\n* WIP.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* WIP.\n\n* Fixes.\n\n* Test\n\n* Fix.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* LINT.\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Categorical(TFActionDistribution):",
            "",
            "@override(TFActionDistribution)",
            "def _build_sample_op(self):",
            "-        return tf.squeeze(tf.multinomial(self.inputs, 1), axis=1)",
            "+        return tf.squeeze(tf.random.categorical(self.inputs, 1), axis=1)",
            "",
            "@staticmethod",
            "@override(ActionDistribution)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=2145728)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=2145729)",
            "Insert(target_node=ASTNode(type=attribute), node=('identifier', 'categorical'), position=2, insert_id=2145730)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=tf), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=multinomial), value='random')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=multinomial), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 8362,
        "neg_line": [
            "-return tf.squeeze(tf.multinomial(self.inputs, 1), axis=1)"
        ],
        "pos_line": [
            "+return tf.squeeze(tf.random.categorical(self.inputs, 1), axis=1)"
        ],
        "core_change": "-return tf.squeeze(tf.multinomial(self.inputs, 1), axis=1) +return tf.squeeze(tf.random.categorical(self.inputs, 1), axis=1)",
        "core_API": "squeeze"
    },
    {
        "commit_hash": "1d4823c0ec446e93d00df8ca654db4b45b63b3d4",
        "index": "2751920ea4..a2e4b31bed 100644",
        "commit_message": "[RLlib] Add testing framework_iterator. (#7852)\n\n* Add testing framework_iterator.\n\n* LINT.\n\n* WIP.\n\n* Fix and LINT.\n\n* LINT fix.\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def fc(x, weights, biases=None):",
            "isinstance(weights, torch.Tensor) else weights",
            "biases = biases.detach().numpy() if \\",
            "isinstance(biases, torch.Tensor) else biases",
            "-    if tf:",
            "+    if tf and tf.executing_eagerly():",
            "x = x.numpy() if isinstance(x, tf.Variable) else x",
            "weights = weights.numpy() if isinstance(weights, tf.Variable) else \\",
            "weights"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=if_statement), node=('boolean_operator', None), position=1, insert_id=2146594)",
            "Move(target_node=IN(type=boolean_operator), node=ASTNode(type=identifier, text=tf), position=0)",
            "Insert(target_node=IN(type=boolean_operator), node=('and', 'and'), position=1, insert_id=2146595)",
            "Insert(target_node=IN(type=boolean_operator), node=('call', None), position=2, insert_id=2146596)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2146597)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2146598)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2146599)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2146600)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'executing_eagerly'), position=2, insert_id=2146601)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2146602)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=2146603)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 8363,
        "neg_line": [
            "-if tf:"
        ],
        "pos_line": [
            "+if tf and tf.executing_eagerly():"
        ],
        "core_change": "-if tf: +if tf and tf.executing_eagerly():",
        "core_API": "detach"
    },
    {
        "commit_hash": "348b5c96a2f696525e5c8ef32163c2a3d7208423",
        "index": "a6e15cdd..07076d90 100644",
        "commit_message": "Fix speaker encoder test\n\n",
        "file": "TTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SpeakerManager:",
            "waveform = self.speaker_encoder_ap.load_wav(wav_file, sr=self.speaker_encoder_ap.sample_rate)",
            "if not self.speaker_encoder_config.model_params.get(\"use_torch_spec\", False):",
            "m_input = self.speaker_encoder_ap.melspectrogram(waveform)",
            "-                m_input = torch.from_numpy(m_input.T)",
            "+                m_input = torch.from_numpy(m_input)",
            "else:",
            "m_input = torch.from_numpy(waveform)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=m_input), position=1)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=T))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 8365,
        "neg_line": [
            "-m_input = torch.from_numpy(m_input.T)"
        ],
        "pos_line": [
            "+m_input = torch.from_numpy(m_input)"
        ],
        "core_change": "-m_input = torch.from_numpy(m_input.T) +m_input = torch.from_numpy(m_input)",
        "core_API": "load_wav"
    },
    {
        "commit_hash": "0dfd6b624081fc4e1c72fc74ae0cd2de199c334c",
        "index": "dc73c662..aa0b5929 100644",
        "commit_message": "Add linting with black (#2678)\n\nSummary:\n# Before submitting\n\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\n- [ ] Did you make sure to update the docs?\n- [ ] Did you write any new necessary tests?\n\n## What does this PR do?\nFixes # (issue).\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding �\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2678\n\nReviewed By: Mortimerp9\n\nDifferential Revision: D32653381\n\nPulled By: dianaml0\n\nfbshipit-source-id: 2810d14867cd7d64f4d340740e2b590b82de47fe\n\n",
        "file": "fairseq.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class CheckpointFunction(torch.autograd.Function):",
            "tensor_inputs = checkpoint.detach_variable(tensor_inputs)",
            "if ctx.fwd_device is not None:",
            "tensor_inputs = [",
            "-                t.to(ctx.fwd_device[i], non_blocking=True) for i, t in enumerate(tensor_inputs)",
            "+                t.to(ctx.fwd_device[i], non_blocking=True)",
            "+                for i, t in enumerate(tensor_inputs)",
            "]",
            "for i, need_grad in enumerate(ctx.grad_requirements):",
            "tensor_inputs[i].requires_grad = need_grad"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 8374,
        "neg_line": [
            "-t.to(ctx.fwd_device[i], non_blocking=True) for i, t in enumerate(tensor_inputs)"
        ],
        "pos_line": [
            "+t.to(ctx.fwd_device[i], non_blocking=True)",
            "+for i, t in enumerate(tensor_inputs)"
        ],
        "core_change": "-t.to(ctx.fwd_device[i], non_blocking=True) for i, t in enumerate(tensor_inputs) +t.to(ctx.fwd_device[i], non_blocking=True) +for i, t in enumerate(tensor_inputs)",
        "core_API": "detach_variable"
    },
    {
        "commit_hash": "6153a6b4d5e5494f1821da60aabf4c2c7ce36075",
        "index": "b59906edc9..c762a4442a 100644",
        "commit_message": "fix code roles + move instance method examples to appropriate methods\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def kaiser_window(",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "if periodic is False:",
            "return tf.signal.kaiser_window(",
            "-            window_length, beta, dtype=tf.dtypes.float32, name=None)",
            "-    else:",
            "-        return tf.signal.kaiser_window(",
            "-            window_length + 1, beta, dtype=dtype, name=None)[:-1]",
            "+            window_length, beta, dtype=tf.dtypes.float32, name=None",
            "+        )",
            "+    else:",
            "+        return tf.signal.kaiser_window(window_length + 1, beta, dtype=dtype, name=None)[",
            "+            :-1",
            "+        ]",
            "",
            "",
            "def moveaxis("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 6,
        "minus_line": 4,
        "AST_diff_line": 17,
        "number": 8377,
        "neg_line": [
            "-window_length, beta, dtype=tf.dtypes.float32, name=None)",
            "-else:",
            "-return tf.signal.kaiser_window(",
            "-window_length + 1, beta, dtype=dtype, name=None)[:-1]"
        ],
        "pos_line": [
            "+window_length, beta, dtype=tf.dtypes.float32, name=None",
            "+)",
            "+else:",
            "+return tf.signal.kaiser_window(window_length + 1, beta, dtype=dtype, name=None)[",
            "+:-1",
            "+]"
        ],
        "core_change": "-window_length, beta, dtype=tf.dtypes.float32, name=None) -else: -return tf.signal.kaiser_window( -window_length + 1, beta, dtype=dtype, name=None)[:-1] +window_length, beta, dtype=tf.dtypes.float32, name=None +) +else: +return tf.signal.kaiser_window(window_length + 1, beta, dtype=dtype, name=None)[ +:-1 +]",
        "core_API": "kaiser_window"
    },
    {
        "commit_hash": "df71e743c33d2900c5006ac26b1c0764ac4d3a49",
        "index": "51d135d..0df7854 100644",
        "commit_message": "fix var name\n\n",
        "file": "deep-learning-for-image-processing.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def main():",
            "model = create_model(num_classes=num_classes + 1, box_thresh=box_thresh)",
            "",
            "# load train weights",
            "-    assert os.path.exists(weight_path), \"{} file dose not exist.\".format(weight_path)",
            "-    model.load_state_dict(torch.load(weight_path, map_location='cpu')[\"model\"])",
            "+    assert os.path.exists(weights_path), \"{} file dose not exist.\".format(weights_path)",
            "+    model.load_state_dict(torch.load(weights_path, map_location='cpu')[\"model\"])",
            "model.to(device)",
            "",
            "# read class_indict"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=weight_path), value='weights_path')",
            "Update(target_node=ASTNode(type=identifier, text=weight_path), value='weights_path')",
            "Update(target_node=ASTNode(type=identifier, text=weight_path), value='weights_path')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 3,
        "number": 8382,
        "neg_line": [
            "-assert os.path.exists(weight_path), \"{} file dose not exist.\".format(weight_path)",
            "-model.load_state_dict(torch.load(weight_path, map_location='cpu')[\"model\"])"
        ],
        "pos_line": [
            "+assert os.path.exists(weights_path), \"{} file dose not exist.\".format(weights_path)",
            "+model.load_state_dict(torch.load(weights_path, map_location='cpu')[\"model\"])"
        ],
        "core_change": "-assert os.path.exists(weight_path), \"{} file dose not exist.\".format(weight_path) -model.load_state_dict(torch.load(weight_path, map_location='cpu')[\"model\"]) +assert os.path.exists(weights_path), \"{} file dose not exist.\".format(weights_path) +model.load_state_dict(torch.load(weights_path, map_location='cpu')[\"model\"])",
        "core_API": "exists"
    },
    {
        "commit_hash": "1e8093de0398364cc65f3c7722b61a0f560df104",
        "index": "29a287d..fa2e987 100644",
        "commit_message": "Fixed misuse of epsilon in Dice loss (issue #24)\n",
        "file": "Pytorch-UNet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DiceCoeff(Function):",
            "",
            "def forward(self, input, target):",
            "self.save_for_backward(input, target)",
            "-        self.inter = torch.dot(input.view(-1), target.view(-1)) + 0.0001",
            "-        self.union = torch.sum(input) + torch.sum(target) + 0.0001",
            "+        eps = 0.0001",
            "+        self.inter = torch.dot(input.view(-1), target.view(-1))",
            "+        self.union = torch.sum(input) + torch.sum(target) + eps",
            "",
            "-        t = 2 * self.inter.float() / self.union.float()",
            "+        t = (2 * self.inter.float() + eps) / self.union.float()",
            "return t",
            "",
            "# This function has only a single output, so it gets only one gradient"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=904958)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=904959)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'eps'), position=0, insert_id=904960)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=904961)",
            "Insert(target_node=IN(type=assignment), node=('float', '0.0001'), position=2, insert_id=904962)",
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=call), position=2)",
            "Insert(target_node=ASTNode(type=binary_operator), node=('identifier', 'eps'), position=2, insert_id=904963)",
            "Insert(target_node=ASTNode(type=binary_operator), node=('parenthesized_expression', None), position=0, insert_id=904964)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('(', '('), position=0, insert_id=904965)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('binary_operator', None), position=1, insert_id=904966)",
            "Insert(target_node=IN(type=parenthesized_expression), node=(')', ')'), position=2, insert_id=904967)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=binary_operator), position=0)",
            "Insert(target_node=IN(type=binary_operator), node=('+', '+'), position=1, insert_id=904968)",
            "Insert(target_node=IN(type=binary_operator), node=('identifier', 'eps'), position=2, insert_id=904969)",
            "Delete(target_node=ASTNode(type=+, text=+))",
            "Delete(target_node=ASTNode(type=float, text=0.0001))",
            "Delete(target_node=ASTNode(type=binary_operator))",
            "Delete(target_node=ASTNode(type=float, text=0.0001))"
        ],
        "plus_line": 4,
        "minus_line": 3,
        "AST_diff_line": 18,
        "number": 8385,
        "neg_line": [
            "-self.inter = torch.dot(input.view(-1), target.view(-1)) + 0.0001",
            "-self.union = torch.sum(input) + torch.sum(target) + 0.0001",
            "-t = 2 * self.inter.float() / self.union.float()"
        ],
        "pos_line": [
            "+eps = 0.0001",
            "+self.inter = torch.dot(input.view(-1), target.view(-1))",
            "+self.union = torch.sum(input) + torch.sum(target) + eps",
            "+t = (2 * self.inter.float() + eps) / self.union.float()"
        ],
        "core_change": "-self.inter = torch.dot(input.view(-1), target.view(-1)) + 0.0001 -self.union = torch.sum(input) + torch.sum(target) + 0.0001 +eps = 0.0001 +self.inter = torch.dot(input.view(-1), target.view(-1)) +self.union = torch.sum(input) + torch.sum(target) + eps -t = 2 * self.inter.float() / self.union.float() +t = (2 * self.inter.float() + eps) / self.union.float()",
        "core_API": "save_for_backward"
    },
    {
        "commit_hash": "4185a222dc5c4166ce84a9403337b326eeae4b91",
        "index": "6ae0ad7b..e86c937e 100644",
        "commit_message": "fix some errors\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def FullyConnected(x, out_dim, W_init=None, b_init=None, nl=tf.nn.relu):",
            "",
            "W = tf.get_variable('W', [in_dim, out_dim], initializer=W_init)",
            "b = tf.get_variable('b', [out_dim], initializer=b_init)",
            "-    return nl(tf.matmul(x, W) + b)",
            "+    return nl(tf.matmul(x, W) + b, name=tf.get_variable_scope().name + '_output')"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=2321837)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=2321838)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=4, insert_id=2321839)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'name'), position=0, insert_id=2321840)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2321841)",
            "Insert(target_node=IN(type=keyword_argument), node=('binary_operator', None), position=2, insert_id=2321842)",
            "Insert(target_node=IN(type=binary_operator), node=('attribute', None), position=0, insert_id=2321843)",
            "Insert(target_node=IN(type=binary_operator), node=('+', '+'), position=1, insert_id=2321844)",
            "Insert(target_node=IN(type=binary_operator), node=('string', \"'_output'\"), position=2, insert_id=2321845)",
            "Insert(target_node=IN(type=attribute), node=('call', None), position=0, insert_id=2321846)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2321847)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'name'), position=2, insert_id=2321848)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2321849)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2321850)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2321851)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2321852)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'get_variable_scope'), position=2, insert_id=2321853)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2321854)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=), text=)), position=1)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 19,
        "number": 8388,
        "neg_line": [
            "-return nl(tf.matmul(x, W) + b)"
        ],
        "pos_line": [
            "+return nl(tf.matmul(x, W) + b, name=tf.get_variable_scope().name + '_output')"
        ],
        "core_change": "-return nl(tf.matmul(x, W) + b) +return nl(tf.matmul(x, W) + b, name=tf.get_variable_scope().name + '_output')",
        "core_API": "get_variable"
    },
    {
        "commit_hash": "de4159a318f9a4f2f474733306c00ac326e643a7",
        "index": "03a7ca273..5e684a72e 100644",
        "commit_message": "More TF int dtype fixes (#20384)\n\n* Add a test to ensure int dummy inputs are int64\n\n* Move the test into the existing int64 test and update a lot of existing dummies\n\n* Fix remaining dummies\n\n* Fix remaining dummies\n\n* Test for int64 serving sigs as well\n\n* Update core tests to use tf.int64\n\n* Add better messages to the assertions\n\n* Update all serving sigs to int64\n\n* More sneaky hiding tf.int32s\n\n* Add an optional int32 signature in save_pretrained\n\n* make fixup\n\n* Add Amy's suggestions\n\n* Switch all serving sigs back to tf.int32\n\n* Switch all dummies to tf.int32\n\n* Adjust tests to check for tf.int32 instead of tf.int64\n\n* Fix base dummy_inputs dtype\n\n* Start casting to tf.int32 in input_processing\n\n* Change dtype for unpack_inputs test\n\n* Add proper tf.int32 test\n\n* Make the alternate serving signature int64\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFWhisperPreTrainedModel(TFPreTrainedModel):",
            "self.main_input_name: tf.random.uniform(",
            "[2, self.config.num_mel_bins, self.config.max_source_positions * 2 - 1], dtype=tf.float32",
            "),",
            "-            \"decoder_input_ids\": tf.constant([[2, 3]], dtype=tf.int64),",
            "+            \"decoder_input_ids\": tf.constant([[2, 3]], dtype=tf.int32),",
            "}",
            "",
            "@tf.function(",
            "input_signature=[",
            "{",
            "\"input_features\": tf.TensorSpec((None, None, None), tf.float32, name=\"input_features\"),",
            "-                \"decoder_input_ids\": tf.TensorSpec((None, None), tf.int64, name=\"decoder_input_ids\"),",
            "-                \"decoder_attention_mask\": tf.TensorSpec((None, None), tf.int64, name=\"decoder_attention_mask\"),",
            "+                \"decoder_input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"decoder_input_ids\"),",
            "+                \"decoder_attention_mask\": tf.TensorSpec((None, None), tf.int32, name=\"decoder_attention_mask\"),",
            "}",
            "]",
            ")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=int64), value='int32')",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2358229)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2358230)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2358231)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'TensorSpec'), position=2, insert_id=2358232)",
            "Insert(target_node=ASTNode(type=argument_list), node=('tuple', None), position=1, insert_id=2358233)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=tuple), position=1)",
            "Insert(target_node=IN(type=tuple), node=('(', '('), position=0, insert_id=2358234)",
            "Insert(target_node=IN(type=tuple), node=('none', 'None'), position=1, insert_id=2358235)",
            "Insert(target_node=IN(type=tuple), node=(',', ','), position=2, insert_id=2358236)",
            "Insert(target_node=IN(type=tuple), node=('none', 'None'), position=3, insert_id=2358237)",
            "Insert(target_node=IN(type=tuple), node=(')', ')'), position=4, insert_id=2358238)",
            "Update(target_node=ASTNode(type=identifier, text=int64), value='int32')",
            "Update(target_node=ASTNode(type=identifier, text=int64), value='int32')",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=TensorSpec))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=none, text=None))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=none, text=None))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=tuple))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 25,
        "number": 8390,
        "neg_line": [
            "-\"decoder_input_ids\": tf.constant([[2, 3]], dtype=tf.int64),",
            "-\"decoder_input_ids\": tf.TensorSpec((None, None), tf.int64, name=\"decoder_input_ids\"),",
            "-\"decoder_attention_mask\": tf.TensorSpec((None, None), tf.int64, name=\"decoder_attention_mask\"),"
        ],
        "pos_line": [
            "+\"decoder_input_ids\": tf.constant([[2, 3]], dtype=tf.int32),",
            "+\"decoder_input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"decoder_input_ids\"),",
            "+\"decoder_attention_mask\": tf.TensorSpec((None, None), tf.int32, name=\"decoder_attention_mask\"),"
        ],
        "core_change": "-\"decoder_input_ids\": tf.constant([[2, 3]], dtype=tf.int64), +\"decoder_input_ids\": tf.constant([[2, 3]], dtype=tf.int32), -\"decoder_input_ids\": tf.TensorSpec((None, None), tf.int64, name=\"decoder_input_ids\"), -\"decoder_attention_mask\": tf.TensorSpec((None, None), tf.int64, name=\"decoder_attention_mask\"), +\"decoder_input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"decoder_input_ids\"), +\"decoder_attention_mask\": tf.TensorSpec((None, None), tf.int32, name=\"decoder_attention_mask\"),",
        "core_API": "uniform"
    },
    {
        "commit_hash": "8cf70b8493c13c33d766aac78a1652348e0e942a",
        "index": "5d7b434..6ba0a01 100644",
        "commit_message": "Fix eval bug (#161)\n\n* change to subprocess beckend\n\n* fix eval bug\n\n* Update launch.py\n\nmake flake8 happy\n\nCo-authored-by: Feng Wang <wangfeng19950315@163.com>\n",
        "file": "YOLOX.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ValTransform:",
            "# assume input is cv2 img for now",
            "def __call__(self, img, res, input_size):",
            "img, _ = preproc(img, input_size, self.means, self.std, self.swap)",
            "-        return torch.tensor_as(img), torch.zeros(1, 5)",
            "+        return torch.as_tensor(img), torch.zeros(1, 5)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tensor_as), value='as_tensor')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 8393,
        "neg_line": [
            "-return torch.tensor_as(img), torch.zeros(1, 5)"
        ],
        "pos_line": [
            "+return torch.as_tensor(img), torch.zeros(1, 5)"
        ],
        "core_change": "-return torch.tensor_as(img), torch.zeros(1, 5) +return torch.as_tensor(img), torch.zeros(1, 5)",
        "core_API": "tensor_as"
    },
    {
        "commit_hash": "7c8ea1acb2547e4ed6049ac95b8fe993224110f9",
        "index": "5ff6785d..cd5f6f2a 100644",
        "commit_message": "[cost] fix cross_entropy_seq\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def cross_entropy_seq(logits, target_seqs):#, batch_size=1, num_steps=None):",
            "loss = sequence_loss_by_example_fn(",
            "[logits],",
            "[tf.reshape(target_seqs, [-1])],",
            "-        [tf.ones_like(tf.reshape(targets, [-1]), dtype=tf.float32)])",
            "+        [tf.ones_like(tf.reshape(target_seqs, [-1]), dtype=tf.float32)])",
            "# [tf.ones([batch_size * num_steps])])",
            "-    cost = tf.reduce_sum(loss) / batch_size",
            "+    cost = tf.reduce_sum(loss) #/ batch_size",
            "return cost"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=call), position=2)",
            "Update(target_node=ASTNode(type=identifier, text=targets), value='target_seqs')",
            "Delete(target_node=ASTNode(type=/, text=/))",
            "Delete(target_node=ASTNode(type=identifier, text=batch_size))",
            "Delete(target_node=ASTNode(type=binary_operator))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 5,
        "number": 8394,
        "neg_line": [
            "-[tf.ones_like(tf.reshape(targets, [-1]), dtype=tf.float32)])",
            "-cost = tf.reduce_sum(loss) / batch_size"
        ],
        "pos_line": [
            "+[tf.ones_like(tf.reshape(target_seqs, [-1]), dtype=tf.float32)])",
            "+cost = tf.reduce_sum(loss) #/ batch_size"
        ],
        "core_change": "-[tf.ones_like(tf.reshape(targets, [-1]), dtype=tf.float32)]) +[tf.ones_like(tf.reshape(target_seqs, [-1]), dtype=tf.float32)]) -cost = tf.reduce_sum(loss) / batch_size +cost = tf.reduce_sum(loss) #/ batch_size",
        "core_API": "reshape"
    },
    {
        "commit_hash": "27b67b7945ca7cd8c7ad1ce60744892a4eb47716",
        "index": "db8fef2d..a1273f7f 100644",
        "commit_message": "Fix import\n\n",
        "file": "TTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class ForwardTTS(BaseTTS):",
            "# init speaker embedding layer",
            "if config.use_speaker_embedding and not config.use_d_vector_file:",
            "print(\" > Init speaker_embedding layer.\")",
            "-            self.emb_g = nn.Embedding(self.args.num_speakers, self.args.hidden_channels)",
            "+            self.emb_g = nn.Embedding(self.num_speakers, self.args.hidden_channels)",
            "nn.init.uniform_(self.emb_g.weight, -0.1, 0.1)",
            "",
            "@staticmethod"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=args))",
            "Delete(target_node=ASTNode(type=., text=.))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 8395,
        "neg_line": [
            "-self.emb_g = nn.Embedding(self.args.num_speakers, self.args.hidden_channels)"
        ],
        "pos_line": [
            "+self.emb_g = nn.Embedding(self.num_speakers, self.args.hidden_channels)"
        ],
        "core_change": "-self.emb_g = nn.Embedding(self.args.num_speakers, self.args.hidden_channels) +self.emb_g = nn.Embedding(self.num_speakers, self.args.hidden_channels)",
        "core_API": "Embedding"
    },
    {
        "commit_hash": "6aaeb70670d4540900e55f4ac532546656447bd2",
        "index": "e6b9a4f6..400bd42e 100644",
        "commit_message": "Add bilinear attention (#1349)\n\n* Add bilinear attention, some code cleanup, make semantic parsers use new attention\n\n* Fix pylint and docs\n\n* Increase beam size for wikitables ERM parser, so test doesn't fail\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class LegacyAttention(Attention):",
            "self._similarity_function = similarity_function or DotProductSimilarity()",
            "",
            "@overrides",
            "-    def _forward_internal(self,",
            "-                          vector: torch.Tensor,",
            "-                          matrix: torch.Tensor,",
            "-                          matrix_mask: torch.Tensor = None) -> torch.Tensor:",
            "+    def _forward_internal(self, vector: torch.Tensor, matrix: torch.Tensor) -> torch.Tensor:",
            "tiled_vector = vector.unsqueeze(1).expand(vector.size()[0],",
            "matrix.size()[1],",
            "vector.size()[1])"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=matrix_mask))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=Tensor))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=type))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=none, text=None))",
            "Delete(target_node=ASTNode(type=typed_default_parameter))"
        ],
        "plus_line": 1,
        "minus_line": 4,
        "AST_diff_line": 11,
        "number": 8396,
        "neg_line": [
            "-def _forward_internal(self,",
            "-vector: torch.Tensor,",
            "-matrix: torch.Tensor,",
            "-matrix_mask: torch.Tensor = None) -> torch.Tensor:"
        ],
        "pos_line": [
            "+def _forward_internal(self, vector: torch.Tensor, matrix: torch.Tensor) -> torch.Tensor:"
        ],
        "core_change": "-def _forward_internal(self, -vector: torch.Tensor, -matrix: torch.Tensor, -matrix_mask: torch.Tensor = None) -> torch.Tensor: +def _forward_internal(self, vector: torch.Tensor, matrix: torch.Tensor) -> torch.Tensor:",
        "core_API": "unsqueeze"
    },
    {
        "commit_hash": "2fc0211d30b808f049ab7e7f4990858cf2ac471f",
        "index": "b33cb0c..bce53db 100644",
        "commit_message": "fix fast_neural_style --export_onnx (#397)\n\n\n",
        "file": "examples.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def stylize(args):",
            "style_model.to(device)",
            "if args.export_onnx:",
            "assert args.export_onnx.endswith(\".onnx\"), \"Export model file should end with .onnx\"",
            "-                output = torch.onnx._export(style_model, content_image, args.export_onnx)",
            "+                output = torch.onnx._export(style_model, content_image, args.export_onnx).cpu()",
            "else:",
            "output = style_model(content_image).cpu()",
            "utils.save_image(args.output_image, output[0])"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=190447)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=190448)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=190449)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cpu'), position=2, insert_id=190450)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=190451)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=190452)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 8402,
        "neg_line": [
            "-output = torch.onnx._export(style_model, content_image, args.export_onnx)"
        ],
        "pos_line": [
            "+output = torch.onnx._export(style_model, content_image, args.export_onnx).cpu()"
        ],
        "core_change": "-output = torch.onnx._export(style_model, content_image, args.export_onnx) +output = torch.onnx._export(style_model, content_image, args.export_onnx).cpu()",
        "core_API": "to"
    },
    {
        "commit_hash": "5f4c6e830f603830117877b8990a0e65a2386aa6",
        "index": "ec308084f..c2d33c00a 100644",
        "commit_message": "Quick fix :) (#606)\n\n* Changing the name\n\n* style + quality\n\n* update doc and logo\n\n* clean up\n\n* circle-CI on the branche for now\n\n* fix daily dialog dataset\n\n* fix urls\n\nCo-authored-by: Quentin Lhoest <lhoest.q@gmail.com>\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class abstractclassmethod(classmethod):  # pylint: disable=invalid-name",
            "super(abstractclassmethod, self).__init__(fn)",
            "",
            "",
            "-def get_nlp_path(relative_path):",
            "-    \"\"\"Returns absolute path to file given path relative to nlp root.\"\"\"",
            "-    path = os.path.join(nlp_dir(), relative_path)",
            "+def get_datasets_path(relative_path):",
            "+    \"\"\"Returns absolute path to file given path relative to datasets root.\"\"\"",
            "+    path = os.path.join(datasets_dir(), relative_path)",
            "return path"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=get_nlp_path), value='get_datasets_path')",
            "Update(target_node=ASTNode(type=string, text=\"\"\"Returns absolute path to file given path relative to nlp root.\"\"\"), value='\"\"\"Returns absolute path to file given path relative to datasets root.\"\"\"')",
            "Update(target_node=ASTNode(type=identifier, text=nlp_dir), value='datasets_dir')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 3,
        "number": 8407,
        "neg_line": [
            "-def get_nlp_path(relative_path):",
            "-\"\"\"Returns absolute path to file given path relative to nlp root.\"\"\"",
            "-path = os.path.join(nlp_dir(), relative_path)"
        ],
        "pos_line": [
            "+def get_datasets_path(relative_path):",
            "+\"\"\"Returns absolute path to file given path relative to datasets root.\"\"\"",
            "+path = os.path.join(datasets_dir(), relative_path)"
        ],
        "core_change": "-def get_nlp_path(relative_path): -\"\"\"Returns absolute path to file given path relative to nlp root.\"\"\" -path = os.path.join(nlp_dir(), relative_path) +def get_datasets_path(relative_path): +\"\"\"Returns absolute path to file given path relative to datasets root.\"\"\" +path = os.path.join(datasets_dir(), relative_path)",
        "core_API": "join"
    },
    {
        "commit_hash": "f04257fdbcb6ecb5a9bef75f4c2a8d2e8b5a6209",
        "index": "d4bd3f53f..bc4921622 100644",
        "commit_message": "Add test to ensure models can take int64 inputs (#17210)\n\n* Add test to ensure models can take int64 inputs\n\n* is_integer is an attribute, not a method\n\n* Fix test when some inputs aren't tensors\n\n* Add casts to blenderbot and blenderbot-small\n\n* Add casts to the other failing models\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def get_masks(slen, lengths, causal, padding_mask=None):",
            "mask = padding_mask",
            "else:",
            "# assert lengths.max().item() <= slen",
            "-        alen = tf.range(slen)",
            "-        mask = tf.math.less(alen, tf.expand_dims(lengths, axis=1))",
            "+        alen = tf.range(slen, dtype=lengths.dtype)",
            "+        mask = alen < tf.expand_dims(lengths, axis=1)",
            "",
            "# attention mask is the same as mask, or triangular inferior attention (causal)",
            "if causal:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('comparison_operator', None), position=2, insert_id=2364352)",
            "Move(target_node=IN(type=comparison_operator), node=ASTNode(type=identifier, text=alen), position=0)",
            "Insert(target_node=IN(type=comparison_operator), node=('<', '<'), position=1, insert_id=2364353)",
            "Move(target_node=IN(type=comparison_operator), node=ASTNode(type=call), position=2)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=2364354)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=2364355)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=2364356)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2364357)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=2364358)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'lengths'), position=0, insert_id=2364359)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2364360)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dtype'), position=2, insert_id=2364361)",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=math))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=less))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 24,
        "number": 8410,
        "neg_line": [
            "-alen = tf.range(slen)",
            "-mask = tf.math.less(alen, tf.expand_dims(lengths, axis=1))"
        ],
        "pos_line": [
            "+alen = tf.range(slen, dtype=lengths.dtype)",
            "+mask = alen < tf.expand_dims(lengths, axis=1)"
        ],
        "core_change": "-alen = tf.range(slen) -mask = tf.math.less(alen, tf.expand_dims(lengths, axis=1)) +alen = tf.range(slen, dtype=lengths.dtype) +mask = alen < tf.expand_dims(lengths, axis=1)",
        "core_API": "max"
    },
    {
        "commit_hash": "455c6390938a5c737fa63e78396cedae41e4e87e",
        "index": "8f0f06872..511c4abf7 100644",
        "commit_message": "CDN urls (#4030)\n\n* [file_utils] use_cdn + documentation\n\n* Move to cdn. urls for weights\n\n* [urls] Hotfix for bert-base-japanese\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "models",
        "change": [
            "from .modeling_roberta import (",
            "logger = logging.getLogger(__name__)",
            "",
            "CAMEMBERT_PRETRAINED_MODEL_ARCHIVE_MAP = {",
            "-    \"camembert-base\": \"https://s3.amazonaws.com/models.huggingface.co/bert/camembert-base-pytorch_model.bin\",",
            "-    \"umberto-commoncrawl-cased-v1\": \"https://s3.amazonaws.com/models.huggingface.co/bert/Musixmatch/umberto-commoncrawl-cased-v1/pytorch_model.bin\",",
            "-    \"umberto-wikipedia-uncased-v1\": \"https://s3.amazonaws.com/models.huggingface.co/bert/Musixmatch/umberto-wikipedia-uncased-v1/pytorch_model.bin\",",
            "+    \"camembert-base\": \"https://cdn.huggingface.co/camembert-base-pytorch_model.bin\",",
            "+    \"umberto-commoncrawl-cased-v1\": \"https://cdn.huggingface.co/Musixmatch/umberto-commoncrawl-cased-v1/pytorch_model.bin\",",
            "+    \"umberto-wikipedia-uncased-v1\": \"https://cdn.huggingface.co/Musixmatch/umberto-wikipedia-uncased-v1/pytorch_model.bin\",",
            "}",
            "",
            "CAMEMBERT_START_DOCSTRING = r\"\"\""
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=,, text=,), node=ASTNode(type=dictionary), position=5)",
            "Insert(target_node=ASTNode(type=dictionary), node=(',', ','), position=2, insert_id=2689757)",
            "Insert(target_node=ASTNode(type=pair), node=(':', ':'), position=1, insert_id=2689758)",
            "Update(target_node=ASTNode(type=string, text=\"https://s3.amazonaws.com/models.huggingface.co/bert/camembert-base-pytorch_model.bin\"), value='\"https://cdn.huggingface.co/camembert-base-pytorch_model.bin\"')",
            "Update(target_node=ASTNode(type=string, text=\"https://s3.amazonaws.com/models.huggingface.co/bert/Musixmatch/umberto-commoncrawl-cased-v1/pytorch_model.bin\"), value='\"https://cdn.huggingface.co/Musixmatch/umberto-commoncrawl-cased-v1/pytorch_model.bin\"')",
            "Move(target_node=ASTNode(type=pair), node=ASTNode(type=:, text=:), position=1)",
            "Update(target_node=ASTNode(type=string, text=\"https://s3.amazonaws.com/models.huggingface.co/bert/Musixmatch/umberto-wikipedia-uncased-v1/pytorch_model.bin\"), value='\"https://cdn.huggingface.co/Musixmatch/umberto-wikipedia-uncased-v1/pytorch_model.bin\"')",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 9,
        "number": 8412,
        "neg_line": [
            "-\"camembert-base\": \"https://s3.amazonaws.com/models.huggingface.co/bert/camembert-base-pytorch_model.bin\",",
            "-\"umberto-commoncrawl-cased-v1\": \"https://s3.amazonaws.com/models.huggingface.co/bert/Musixmatch/umberto-commoncrawl-cased-v1/pytorch_model.bin\",",
            "-\"umberto-wikipedia-uncased-v1\": \"https://s3.amazonaws.com/models.huggingface.co/bert/Musixmatch/umberto-wikipedia-uncased-v1/pytorch_model.bin\","
        ],
        "pos_line": [
            "+\"camembert-base\": \"https://cdn.huggingface.co/camembert-base-pytorch_model.bin\",",
            "+\"umberto-commoncrawl-cased-v1\": \"https://cdn.huggingface.co/Musixmatch/umberto-commoncrawl-cased-v1/pytorch_model.bin\",",
            "+\"umberto-wikipedia-uncased-v1\": \"https://cdn.huggingface.co/Musixmatch/umberto-wikipedia-uncased-v1/pytorch_model.bin\","
        ],
        "core_change": "-\"camembert-base\": \"https://s3.amazonaws.com/models.huggingface.co/bert/camembert-base-pytorch_model.bin\", -\"umberto-commoncrawl-cased-v1\": \"https://s3.amazonaws.com/models.huggingface.co/bert/Musixmatch/umberto-commoncrawl-cased-v1/pytorch_model.bin\", -\"umberto-wikipedia-uncased-v1\": \"https://s3.amazonaws.com/models.huggingface.co/bert/Musixmatch/umberto-wikipedia-uncased-v1/pytorch_model.bin\", +\"camembert-base\": \"https://cdn.huggingface.co/camembert-base-pytorch_model.bin\", +\"umberto-commoncrawl-cased-v1\": \"https://cdn.huggingface.co/Musixmatch/umberto-commoncrawl-cased-v1/pytorch_model.bin\", +\"umberto-wikipedia-uncased-v1\": \"https://cdn.huggingface.co/Musixmatch/umberto-wikipedia-uncased-v1/pytorch_model.bin\",",
        "core_API": "getLogger"
    },
    {
        "commit_hash": "ed63b7ee9e68ba120893aaaaca0d4525a56ed484",
        "index": "38aafe7a..5eb5ce49 100644",
        "commit_message": "upgrade to pytorch 0.4.0 (#1126)\n\n* bump pytorch to 0.4 + fix sanitize\n\n* remove check for Variable in block_orthogonal\n\n* rename parameter split_size=\n\n* remove checks for Variable\n\n* fix more tests\n\n* fixes\n\n* get tests to pass\n\n* fix warnings\n\n* get rid of some of the Variables\n\n* more tests passing\n\n* more elimination of variables\n\n* finish removing all Variables\n\n* pylint and such\n\n* a few fixes\n\n* move torch.no_grad into model.forward_on_instances\n\n* more pytorch 0.4 changes\n\n* detach() -> data\n\n* pylint\n\n* fix bad tensor creation\n\n* fix types\n\n* remove print statement\n\n* more 0.4 goodness\n\n* factor out is_tensor\n\n* add no_grad to elmo command\n\n* cleanup\n\n* remove TODO\n\n* address PR feedback\n\n* replace all() with item()\n\n* more cleanup\n\n* further cleanup\n\n* remove Variable\n\n* really fix merge conflict\n\n* fix pylint\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SpanBasedF1Measure(Metric):",
            "possible roles associated with it).",
            "\"\"\"",
            "if mask is None:",
            "-            mask = ones_like(gold_labels)",
            "-        # Get the data from the Variables.",
            "+            mask = torch.ones_like(gold_labels)",
            "+",
            "predictions, gold_labels, mask, prediction_map = self.unwrap_to_tensors(predictions,",
            "gold_labels,",
            "mask, prediction_map)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=37354)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=37355)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=37356)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=ones_like), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 4,
        "number": 8428,
        "neg_line": [
            "-mask = ones_like(gold_labels)",
            "-# Get the data from the Variables."
        ],
        "pos_line": [
            "+mask = torch.ones_like(gold_labels)",
            "+"
        ],
        "core_change": "-mask = ones_like(gold_labels) -# Get the data from the Variables. +mask = torch.ones_like(gold_labels) +",
        "core_API": "ones_like"
    },
    {
        "commit_hash": "c990b3a59c4731cbde3b9fdd95e987ceb3051745",
        "index": "478d9c00..fb943a37 100644",
        "commit_message": "linter fixes and test fixes\n\n",
        "file": "TTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def interpolate_vocoder_input(scale_factor, spec):",
            "torch.tensor: interpolated spectrogram.",
            "\"\"\"",
            "print(\" > before interpolation :\", spec.shape)",
            "-    spec = torch.tensor(spec).unsqueeze(0).unsqueeze(0)",
            "+    spec = torch.tensor(spec).unsqueeze(0).unsqueeze(0)  # pylint: disable=not-callable",
            "spec = torch.nn.functional.interpolate(spec,",
            "scale_factor=scale_factor,",
            "recompute_scale_factor=True,"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 8429,
        "neg_line": [
            "-spec = torch.tensor(spec).unsqueeze(0).unsqueeze(0)"
        ],
        "pos_line": [
            "+spec = torch.tensor(spec).unsqueeze(0).unsqueeze(0)  # pylint: disable=not-callable"
        ],
        "core_change": "-spec = torch.tensor(spec).unsqueeze(0).unsqueeze(0) +spec = torch.tensor(spec).unsqueeze(0).unsqueeze(0)  # pylint: disable=not-callable",
        "core_API": "tensor"
    },
    {
        "commit_hash": "68427c9bebd1e4ff43d25b18bb9c7eb786303712",
        "index": "84a3e67ef..fac8cddc6 100644",
        "commit_message": "Fixing slow pipeline tests (#14260)\n\n* Fiixng slow pipeline tests\n\n* Remove the image-segmentaiton override.\n\n* Fixing clamping only in training.\n\n* Wav2vec2.\n\n* Remove last mention of `no_grad`.\n\n* Fixing copies.\n\n* Rename.\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ImageSegmentationPipeline(Pipeline):",
            "",
            "return super().__call__(*args, **kwargs)",
            "",
            "-    def get_inference_context(self):",
            "-        return torch.no_grad",
            "-",
            "def preprocess(self, image):",
            "image = load_image(image)",
            "target_size = torch.IntTensor([[image.height, image.width]])"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=def, text=def))",
            "Delete(target_node=ASTNode(type=identifier, text=get_inference_context))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=parameters))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=return, text=return))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=no_grad))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=return_statement))",
            "Delete(target_node=ASTNode(type=block))",
            "Delete(target_node=ASTNode(type=function_definition))"
        ],
        "plus_line": 0,
        "minus_line": 2,
        "AST_diff_line": 15,
        "number": 8431,
        "neg_line": [
            "-def get_inference_context(self):",
            "-return torch.no_grad",
            "-"
        ],
        "pos_line": [],
        "core_change": "-def get_inference_context(self): -return torch.no_grad -",
        "core_API": "IntTensor"
    },
    {
        "commit_hash": "94ecd2ae789c038e2e8ed9ed8e83f9adfe3eb447",
        "index": "0712beb3..d17a3e5c 100644",
        "commit_message": "fix softmax and agnn\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class AGNN(Module):",
            "self.beta.data.uniform_(0, 1)",
            "",
            "def forward(self, x, edge_index):",
            "-        beta = self.beta if self.requires_grad else Var(self._buffers['beta'])",
            "+        beta = self.beta if self.requires_grad else self._buffers['beta']",
            "return agnn(x, edge_index, beta)",
            "",
            "def __repr__(self):"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=ASTNode(type=conditional_expression), node=ASTNode(type=subscript), position=4)",
            "Delete(target_node=ASTNode(type=identifier, text=Var))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 8438,
        "neg_line": [
            "-beta = self.beta if self.requires_grad else Var(self._buffers['beta'])"
        ],
        "pos_line": [
            "+beta = self.beta if self.requires_grad else self._buffers['beta']"
        ],
        "core_change": "-beta = self.beta if self.requires_grad else Var(self._buffers['beta']) +beta = self.beta if self.requires_grad else self._buffers['beta']",
        "core_API": "uniform_"
    },
    {
        "commit_hash": "8ebc50f844f42e283f125792d630ea6d0a2a7000",
        "index": "d2249c29d..02960ae89 100644",
        "commit_message": "[RLlib] Issue 21334: Fix APPO when kl_loss is enabled. (#21855)\n\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def stats(policy: Policy, train_batch: SampleBatch):",
            "stats_dict[\"var_IS\"] = is_stat_var",
            "",
            "if policy.config[\"use_kl_loss\"]:",
            "-        stats_dict[\"kl\"] = policy.get_tower_stats(\"mean_kl_loss\")",
            "+        stats_dict[\"kl\"] = torch.mean(",
            "+            torch.stack(policy.get_tower_stats(\"mean_kl_loss\")))",
            "stats_dict[\"KL_Coeff\"] = policy.kl_coeff",
            "",
            "return stats_dict"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1113644)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=1113645)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1113646)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1113647)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'mean'), position=2, insert_id=1113648)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1113649)",
            "Insert(target_node=IN(type=argument_list), node=('call', None), position=1, insert_id=1113650)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1113651)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1113652)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1113653)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1113654)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1113655)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'stack'), position=2, insert_id=1113656)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1113657)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=call), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1113658)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 16,
        "number": 8440,
        "neg_line": [
            "-stats_dict[\"kl\"] = policy.get_tower_stats(\"mean_kl_loss\")"
        ],
        "pos_line": [
            "+stats_dict[\"kl\"] = torch.mean(",
            "+torch.stack(policy.get_tower_stats(\"mean_kl_loss\")))"
        ],
        "core_change": "-stats_dict[\"kl\"] = policy.get_tower_stats(\"mean_kl_loss\") +stats_dict[\"kl\"] = torch.mean( +torch.stack(policy.get_tower_stats(\"mean_kl_loss\")))",
        "core_API": "get_tower_stats"
    },
    {
        "commit_hash": "7a9a08c5d3ca4699fb439f691c40e1320b37507a",
        "index": "9b40f6d69..2c9eb1f76 100644",
        "commit_message": "Drop torch 1.6 testing (#10390)\n\n* Drop torch 1.6 support\n\n* Drop 1.6 support\n\n* Update CHANGELOG\n\n* Fixes\n\n* Split change\n\n* Undo change\n\n* 1.7 -> 1.7.1\n\nhttps://github.com/pytorch/pytorch/issues/47354\n\n* Force trigger nightly\n\n* Update .github/workflows/events-nightly.yml\n\nCo-authored-by: Aki Nitta <nitta@akihironitta.com>\n\n* Revert 1.7.1 change - try wildcard\n\n* Update adjust versions and test it\n\n* Undo test changes\n\n* Revert \"Undo test changes\"\n\nThis reverts commit 3a6acadd115e86f02d83a788f1978372ab6764f3.\n\n* Update CHANGELOG.md\n\nCo-authored-by: Aki Nitta <nitta@akihironitta.com>\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def atomic_save(checkpoint: Dict[str, Any], filepath: Union[str, Path]) -> None:",
            "\"\"\"",
            "",
            "bytesbuffer = io.BytesIO()",
            "-    # Can't use the new zipfile serialization for 1.6.0 because there's a bug in",
            "-    # torch.hub.load_state_dict_from_url() that prevents it from loading the new files.",
            "-    # More details can be found here: https://github.com/pytorch/pytorch/issues/42239",
            "-    if Version(torch.__version__).release[:3] == (1, 6, 0):",
            "-        torch.save(checkpoint, bytesbuffer, _use_new_zipfile_serialization=False)",
            "-    else:",
            "-        torch.save(checkpoint, bytesbuffer)",
            "+    torch.save(checkpoint, bytesbuffer)",
            "with fsspec.open(filepath, \"wb\") as f:",
            "f.write(bytesbuffer.getvalue())"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 7,
        "AST_diff_line": 17,
        "number": 8442,
        "neg_line": [
            "-# Can't use the new zipfile serialization for 1.6.0 because there's a bug in",
            "-# torch.hub.load_state_dict_from_url() that prevents it from loading the new files.",
            "-# More details can be found here: https://github.com/pytorch/pytorch/issues/42239",
            "-if Version(torch.__version__).release[:3] == (1, 6, 0):",
            "-torch.save(checkpoint, bytesbuffer, _use_new_zipfile_serialization=False)",
            "-else:",
            "-torch.save(checkpoint, bytesbuffer)"
        ],
        "pos_line": [
            "+torch.save(checkpoint, bytesbuffer)"
        ],
        "core_change": "-# Can't use the new zipfile serialization for 1.6.0 because there's a bug in -# torch.hub.load_state_dict_from_url() that prevents it from loading the new files. -# More details can be found here: https://github.com/pytorch/pytorch/issues/42239 -if Version(torch.__version__).release[:3] == (1, 6, 0): -torch.save(checkpoint, bytesbuffer, _use_new_zipfile_serialization=False) -else: -torch.save(checkpoint, bytesbuffer) +torch.save(checkpoint, bytesbuffer)",
        "core_API": "BytesIO"
    },
    {
        "commit_hash": "899f1d4bde374b0d98fe56c767cf1985d21cb72f",
        "index": "f545ae1..94ccf57 100644",
        "commit_message": "Fix DDP bug in single process multiple device use cases\n\n\n",
        "file": "yolov5.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def train(hyp):",
            "world_size=1,  # number of nodes",
            "rank=0)  # node rank",
            "model = torch.nn.parallel.DistributedDataParallel(model)",
            "+        # pip install torch==1.4.0+cu100 torchvision==0.5.0+cu100 -f https://download.pytorch.org/whl/torch_stable.html",
            "",
            "# Dataset",
            "dataset = LoadImagesAndLabels(train_path, imgsz, batch_size,"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 17,
        "number": 8448,
        "neg_line": [],
        "pos_line": [
            "+# pip install torch==1.4.0+cu100 torchvision==0.5.0+cu100 -f https://download.pytorch.org/whl/torch_stable.html"
        ],
        "core_change": "+# pip install torch==1.4.0+cu100 torchvision==0.5.0+cu100 -f https://download.pytorch.org/whl/torch_stable.html",
        "core_API": "DistributedDataParallel"
    },
    {
        "commit_hash": "588536de9905feb7f37c2c977d146a64c74ef28e",
        "index": "2b71de7a..adab60fd 100644",
        "commit_message": "Fix docstring formats (#3196)\n\n* update docstring formats\n\n* fix :func:\n\n* minor update\n",
        "file": "mmdetection.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Collect(object):",
            "",
            "@PIPELINES.register_module()",
            "class WrapFieldsToLists(object):",
            "-    \"\"\"",
            "-    Wrap fields of the data dictionary into lists for evaluation.",
            "+    \"\"\"Wrap fields of the data dictionary into lists for evaluation.",
            "",
            "This class can be used as a last step of a test or validation",
            "pipeline for single image evaluation or inference."
        ],
        "hunk_index": 3,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 8451,
        "neg_line": [
            "-\"\"\"",
            "-Wrap fields of the data dictionary into lists for evaluation."
        ],
        "pos_line": [
            "+\"\"\"Wrap fields of the data dictionary into lists for evaluation."
        ],
        "core_change": "-\"\"\" -Wrap fields of the data dictionary into lists for evaluation. +\"\"\"Wrap fields of the data dictionary into lists for evaluation.",
        "core_API": "register_module"
    },
    {
        "commit_hash": "67d140446bd0a23fcb433819902cf0c51a1bfb80",
        "index": "aeb713a..033ad69 100644",
        "commit_message": "Fix bug in classification head\n\n",
        "file": "pytorch-image-models.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SwinTransformerV2CR(nn.Module):",
            "for stage in self.stages:",
            "output: torch.Tensor = stage(output)",
            "# Perform average pooling",
            "-        output: torch.Tensor = self.average_pool(output)",
            "+        output: torch.Tensor = self.average_pool(output).flatten(start_dim=1)",
            "# Predict classification",
            "classification: torch.Tensor = self.head(output)",
            "return classification"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=893836)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=893837)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=893838)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'flatten'), position=2, insert_id=893839)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=893840)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=1, insert_id=893841)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=893842)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'start_dim'), position=0, insert_id=893843)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=893844)",
            "Insert(target_node=IN(type=keyword_argument), node=('integer', '1'), position=2, insert_id=893845)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 8453,
        "neg_line": [
            "-output: torch.Tensor = self.average_pool(output)"
        ],
        "pos_line": [
            "+output: torch.Tensor = self.average_pool(output).flatten(start_dim=1)"
        ],
        "core_change": "-output: torch.Tensor = self.average_pool(output) +output: torch.Tensor = self.average_pool(output).flatten(start_dim=1)",
        "core_API": "average_pool"
    },
    {
        "commit_hash": "e3990d137aee4728a1127f55cbcf76970df571ce",
        "index": "5484f2b71..56b68b92e 100644",
        "commit_message": "fix (#6946)\n\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class LxmertModelTest(ModelTesterMixin, unittest.TestCase):",
            "def test_model_from_pretrained(self):",
            "for model_name in LXMERT_PRETRAINED_MODEL_ARCHIVE_LIST[:1]:",
            "model = LxmertModel.from_pretrained(model_name)",
            "+            model.to(torch_device)",
            "self.assertIsNotNone(model)",
            "",
            "def test_attention_outputs(self):"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=4, insert_id=1233636)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=1233637)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1233638)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1233639)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'model'), position=0, insert_id=1233640)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1233641)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to'), position=2, insert_id=1233642)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1233643)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'torch_device'), position=1, insert_id=1233644)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1233645)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 10,
        "number": 8454,
        "neg_line": [],
        "pos_line": [
            "+model.to(torch_device)"
        ],
        "core_change": "+model.to(torch_device)",
        "core_API": "from_pretrained"
    },
    {
        "commit_hash": "8e00537b65bf20d2ede3c3e822d3dfefae9a6896",
        "index": "fd727c12f0..b5c3b86a9f 100644",
        "commit_message": "[RLlib] SlateQ: framework=tf fixes and SlateQ documentation update (#22543)\n\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class SlateEpsilonGreedy(EpsilonGreedy):",
            ") -> \"tf.Tensor\":",
            "",
            "per_slate_q_values = action_distribution.inputs",
            "-        all_slates = self.model.slates",
            "+        all_slates = action_distribution.all_slates",
            "",
            "-        exploit_indices = action_distribution.deterministic_sample()",
            "-        exploit_action = tf.gather(all_slates, exploit_indices)",
            "+        exploit_action = action_distribution.deterministic_sample()",
            "",
            "batch_size = tf.shape(per_slate_q_values)[0]",
            "action_logp = tf.zeros(batch_size, dtype=tf.float32)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=attribute), position=2)",
            "Update(target_node=ASTNode(type=identifier, text=exploit_indices), value='exploit_action')",
            "Update(target_node=ASTNode(type=identifier, text=self), value='action_distribution')",
            "Update(target_node=ASTNode(type=identifier, text=model), value='all_slates')",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=slates))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=identifier, text=exploit_action))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=gather))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=all_slates))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=exploit_indices))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 2,
        "minus_line": 3,
        "AST_diff_line": 22,
        "number": 8455,
        "neg_line": [
            "-all_slates = self.model.slates",
            "-exploit_indices = action_distribution.deterministic_sample()",
            "-exploit_action = tf.gather(all_slates, exploit_indices)"
        ],
        "pos_line": [
            "+all_slates = action_distribution.all_slates",
            "+exploit_action = action_distribution.deterministic_sample()"
        ],
        "core_change": "-all_slates = self.model.slates +all_slates = action_distribution.all_slates -exploit_indices = action_distribution.deterministic_sample() -exploit_action = tf.gather(all_slates, exploit_indices) +exploit_action = action_distribution.deterministic_sample()",
        "core_API": "deterministic_sample"
    },
    {
        "commit_hash": "3f52c685c135394e0a238bc821460ad4df891bd7",
        "index": "5f8388c57..09de5b3cb 100755",
        "commit_message": "Fix classifier dropout in AlbertForMultipleChoice (#13087)\n\nClassification head of AlbertForMultipleChoice uses `hidden_dropout_prob` instead of `classifier_dropout_prob`.  This\nis not desirable as we cannot change classifer head dropout probability without changing the dropout probabilities of\nthe whole model.\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class AlbertForMultipleChoice(AlbertPreTrainedModel):",
            "super().__init__(config)",
            "",
            "self.albert = AlbertModel(config)",
            "-        self.dropout = nn.Dropout(config.hidden_dropout_prob)",
            "+        self.dropout = nn.Dropout(config.classifier_dropout_prob)",
            "self.classifier = nn.Linear(config.hidden_size, 1)",
            "",
            "self.init_weights()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=hidden_dropout_prob), value='classifier_dropout_prob')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 8457,
        "neg_line": [
            "-self.dropout = nn.Dropout(config.hidden_dropout_prob)"
        ],
        "pos_line": [
            "+self.dropout = nn.Dropout(config.classifier_dropout_prob)"
        ],
        "core_change": "-self.dropout = nn.Dropout(config.hidden_dropout_prob) +self.dropout = nn.Dropout(config.classifier_dropout_prob)",
        "core_API": "Dropout"
    },
    {
        "commit_hash": "b6425ea3362791cf3aff146220e62ed8f84f1aa1",
        "index": "a7611061..625f0fdf 100644",
        "commit_message": "Bump version to 0.5.1 that is python 3.5 compatible (#2089)\n\n* Bump version to 0.5.1 that is python 3.5 compatible\n\n* more fixes to setup.py\n\n* update minimum python version to 3.5\n\n* address comments\n\n* fix tqdm issue\n\n* bump up pyro-api version\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def setup(app):",
            "",
            "# @jpchen's hack to get rtd builder to install latest pytorch",
            "if 'READTHEDOCS' in os.environ:",
            "-    os.system('pip install torch==1.2.0+cpu -f https://download.pytorch.org/whl/torch_stable.html')",
            "+    os.system('pip install torch==1.3.0+cpu -f https://download.pytorch.org/whl/torch_stable.html')"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text='pip install torch==1.2.0+cpu -f https://download.pytorch.org/whl/torch_stable.html'), value=\"'pip install torch==1.3.0+cpu -f https://download.pytorch.org/whl/torch_stable.html'\")"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 8458,
        "neg_line": [
            "-os.system('pip install torch==1.2.0+cpu -f https://download.pytorch.org/whl/torch_stable.html')"
        ],
        "pos_line": [
            "+os.system('pip install torch==1.3.0+cpu -f https://download.pytorch.org/whl/torch_stable.html')"
        ],
        "core_change": "-os.system('pip install torch==1.2.0+cpu -f https://download.pytorch.org/whl/torch_stable.html') +os.system('pip install torch==1.3.0+cpu -f https://download.pytorch.org/whl/torch_stable.html')",
        "core_API": "system"
    },
    {
        "commit_hash": "ee5de0ba449d638da704e1c03ffcc20a930f5589",
        "index": "5c032f05e..6cfbe3d00 100644",
        "commit_message": "BERT decoder: Fix causal mask dtype.\n\nPyTorch < 1.3 requires multiplication operands to be of the same type.\nThis was violated when using default attention mask (i.e.,\nattention_mask=None in arguments) given BERT in the decoder mode.\n\nIn particular, this was breaking Model2Model and made tutorial\nfrom the quickstart failing.\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class BertModel(BertPreTrainedModel):",
            "seq_ids = torch.arange(seq_length, device=device)",
            "causal_mask = seq_ids[None, None, :].repeat(batch_size, seq_length, 1) <= seq_ids[None, :, None]",
            "causal_mask = causal_mask.to(",
            "-                    torch.long",
            "-                )  # not converting to long will cause errors with pytorch version < 1.3",
            "+                    attention_mask.dtype",
            "+                )  # causal and attention masks must have same type with pytorch version < 1.3",
            "extended_attention_mask = causal_mask[:, None, :, :] * attention_mask[:, None, None, :]",
            "else:",
            "extended_attention_mask = attention_mask[:, None, None, :]"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=torch), value='attention_mask')",
            "Update(target_node=ASTNode(type=identifier, text=long), value='dtype')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 8475,
        "neg_line": [
            "-torch.long",
            "-)  # not converting to long will cause errors with pytorch version < 1.3"
        ],
        "pos_line": [
            "+attention_mask.dtype",
            "+)  # causal and attention masks must have same type with pytorch version < 1.3"
        ],
        "core_change": "-torch.long -)  # not converting to long will cause errors with pytorch version < 1.3 +attention_mask.dtype +)  # causal and attention masks must have same type with pytorch version < 1.3",
        "core_API": "arange"
    },
    {
        "commit_hash": "cd51772bbe4eec0addd4e149b2f47f999020e7a0",
        "index": "6d57fffb..5e6f092d 100644",
        "commit_message": "remove numpy import (#1116)\n\n* remove numpy import\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* undo and add setup develop in gh actions\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* remove create_checkerboard\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestDilate:",
            "None, None, :, :",
            "]",
            "assert_allclose(",
            "-            dilation(tensor, torch.ones_like(structural_element), structuring_element=structural_element), expected,",
            "-            atol=1e-4, rtol=1e-4",
            "+            dilation(tensor, torch.ones_like(structural_element), structuring_element=structural_element),",
            "+            expected,",
            "+            atol=1e-4,",
            "+            rtol=1e-4,",
            ")",
            "",
            "def test_exception(self, device, dtype):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=8, insert_id=423219)"
        ],
        "plus_line": 4,
        "minus_line": 2,
        "AST_diff_line": 1,
        "number": 8480,
        "neg_line": [
            "-dilation(tensor, torch.ones_like(structural_element), structuring_element=structural_element), expected,",
            "-atol=1e-4, rtol=1e-4"
        ],
        "pos_line": [
            "+dilation(tensor, torch.ones_like(structural_element), structuring_element=structural_element),",
            "+expected,",
            "+atol=1e-4,",
            "+rtol=1e-4,"
        ],
        "core_change": "-dilation(tensor, torch.ones_like(structural_element), structuring_element=structural_element), expected, -atol=1e-4, rtol=1e-4 +dilation(tensor, torch.ones_like(structural_element), structuring_element=structural_element), +expected, +atol=1e-4, +rtol=1e-4,",
        "core_API": "ones_like"
    },
    {
        "commit_hash": "18812e7478da1d4555f714d22e13ddc4bfaa2896",
        "index": "574371a..8b4e7b4 100755",
        "commit_message": "fix naming for griff created files\n\n",
        "file": "TensorFlowTTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class FastSpeech2Trainer(Seq2SeqBasedTrainer):",
            "grif_before = self.griffin_lim_tf(tf.reshape(mel_before, [-1, 80])[tf.newaxis, :], n_iter=32)",
            "grif_after = self.griffin_lim_tf(tf.reshape(mel_after, [-1, 80])[tf.newaxis, :], n_iter=32)",
            "grif_gt = self.griffin_lim_tf(tf.reshape(mel_gt, [-1, 80])[tf.newaxis, :], n_iter=32)",
            "-                self.griffin_lim_tf.save_wav(grif_before, griff_dir_name, f\"{idx}_before.wav\")",
            "-                self.griffin_lim_tf.save_wav(grif_after, griff_dir_name, f\"{idx}_after.wav\")",
            "-                self.griffin_lim_tf.save_wav(grif_gt, griff_dir_name, f\"{idx}_gt.wav\")",
            "+                self.griffin_lim_tf.save_wav(grif_before, griff_dir_name, f\"{idx}_before\")",
            "+                self.griffin_lim_tf.save_wav(grif_after, griff_dir_name, f\"{idx}_after\")",
            "+                self.griffin_lim_tf.save_wav(grif_gt, griff_dir_name, f\"{idx}_gt\")",
            "",
            "mel_gt = tf.reshape(mel_gt, (-1, 80)).numpy()  # [length, 80]",
            "mel_before = tf.reshape(mel_before, (-1, 80)).numpy()  # [length, 80]"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2214156)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2214157)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2214158)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'save_wav'), position=2, insert_id=2214159)",
            "Update(target_node=ASTNode(type=string, text=f\"{idx}_before.wav\"), value='f\"{idx}_before\"')",
            "Update(target_node=ASTNode(type=string, text=f\"{idx}_after.wav\"), value='f\"{idx}_after\"')",
            "Update(target_node=ASTNode(type=string, text=f\"{idx}_gt.wav\"), value='f\"{idx}_gt\"')",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=2214160)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2214161)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'griffin_lim_tf'), position=2, insert_id=2214162)",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=griffin_lim_tf))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=save_wav))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 18,
        "number": 8482,
        "neg_line": [
            "-self.griffin_lim_tf.save_wav(grif_before, griff_dir_name, f\"{idx}_before.wav\")",
            "-self.griffin_lim_tf.save_wav(grif_after, griff_dir_name, f\"{idx}_after.wav\")",
            "-self.griffin_lim_tf.save_wav(grif_gt, griff_dir_name, f\"{idx}_gt.wav\")"
        ],
        "pos_line": [
            "+self.griffin_lim_tf.save_wav(grif_before, griff_dir_name, f\"{idx}_before\")",
            "+self.griffin_lim_tf.save_wav(grif_after, griff_dir_name, f\"{idx}_after\")",
            "+self.griffin_lim_tf.save_wav(grif_gt, griff_dir_name, f\"{idx}_gt\")"
        ],
        "core_change": "-self.griffin_lim_tf.save_wav(grif_before, griff_dir_name, f\"{idx}_before.wav\") -self.griffin_lim_tf.save_wav(grif_after, griff_dir_name, f\"{idx}_after.wav\") -self.griffin_lim_tf.save_wav(grif_gt, griff_dir_name, f\"{idx}_gt.wav\") +self.griffin_lim_tf.save_wav(grif_before, griff_dir_name, f\"{idx}_before\") +self.griffin_lim_tf.save_wav(grif_after, griff_dir_name, f\"{idx}_after\") +self.griffin_lim_tf.save_wav(grif_gt, griff_dir_name, f\"{idx}_gt\")",
        "core_API": "griffin_lim_tf"
    },
    {
        "commit_hash": "e82759f1e006c2c5abed20998e208486ccaeae78",
        "index": "d17aef07f..38c369b87 100644",
        "commit_message": "Fix bugs. Change the time-domain output to SPK* [Batch, Sample].\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ESPnetFrontendModel(AbsESPnetModel):",
            "return loss.mean(), perm",
            "",
            "def collect_feats(",
            "-            self, speech_mix: torch.Tensor, speech_mix_lengths: torch.Tensor, **kwargs",
            "+        self, speech_mix: torch.Tensor, speech_mix_lengths: torch.Tensor, **kwargs",
            ") -> Dict[str, torch.Tensor]:",
            "# for data-parallel",
            "speech_mix = speech_mix[:, : speech_mix_lengths.max()]"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 8483,
        "neg_line": [
            "-self, speech_mix: torch.Tensor, speech_mix_lengths: torch.Tensor, **kwargs"
        ],
        "pos_line": [
            "+self, speech_mix: torch.Tensor, speech_mix_lengths: torch.Tensor, **kwargs"
        ],
        "core_change": "-self, speech_mix: torch.Tensor, speech_mix_lengths: torch.Tensor, **kwargs +self, speech_mix: torch.Tensor, speech_mix_lengths: torch.Tensor, **kwargs",
        "core_API": "mean"
    },
    {
        "commit_hash": "f6c76230938f8c5cd09ef85913a414c37862b206",
        "index": "49190e52..2f8ab4f1 100644",
        "commit_message": "Add Pre-commit bot fixes (#3722)\n\n* flake8\n\n* flake8\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "from torch_geometric.utils import remove_self_loops",
            "from typing import Optional",
            "",
            "",
            "-def pool_edge(cluster, edge_index,",
            "-              edge_attr: Optional[torch.Tensor] = None):",
            "+def pool_edge(cluster, edge_index, edge_attr: Optional[torch.Tensor] = None):",
            "num_nodes = cluster.size(0)",
            "edge_index = cluster[edge_index.view(-1)].view(2, -1)",
            "edge_index, edge_attr = remove_self_loops(edge_index, edge_attr)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 8484,
        "neg_line": [
            "-def pool_edge(cluster, edge_index,",
            "-edge_attr: Optional[torch.Tensor] = None):"
        ],
        "pos_line": [
            "+def pool_edge(cluster, edge_index, edge_attr: Optional[torch.Tensor] = None):"
        ],
        "core_change": "-def pool_edge(cluster, edge_index, -edge_attr: Optional[torch.Tensor] = None): +def pool_edge(cluster, edge_index, edge_attr: Optional[torch.Tensor] = None):",
        "core_API": "size"
    },
    {
        "commit_hash": "85d05682c21d5555bdd1b46c08994ae0fbd227a7",
        "index": "57ec67b5..81c607e0 100644",
        "commit_message": "fix LayerNorm (#4030)\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class LayerNorm(torch.nn.Module):",
            "self.eps = eps",
            "",
            "if affine:",
            "-            self.weight = Parameter(torch.Tensor([in_channels]))",
            "-            self.bias = Parameter(torch.Tensor([in_channels]))",
            "+            self.weight = Parameter(torch.Tensor(in_channels))",
            "+            self.bias = Parameter(torch.Tensor(in_channels))",
            "else:",
            "self.register_parameter('weight', None)",
            "self.register_parameter('bias', None)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=988284)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=988285)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=988286)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Tensor'), position=2, insert_id=988287)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=in_channels), position=1)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=in_channels), position=1)",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=list))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=Tensor))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=list))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 8485,
        "neg_line": [
            "-self.weight = Parameter(torch.Tensor([in_channels]))",
            "-self.bias = Parameter(torch.Tensor([in_channels]))"
        ],
        "pos_line": [
            "+self.weight = Parameter(torch.Tensor(in_channels))",
            "+self.bias = Parameter(torch.Tensor(in_channels))"
        ],
        "core_change": "-self.weight = Parameter(torch.Tensor([in_channels])) -self.bias = Parameter(torch.Tensor([in_channels])) +self.weight = Parameter(torch.Tensor(in_channels)) +self.bias = Parameter(torch.Tensor(in_channels))",
        "core_API": "Tensor"
    },
    {
        "commit_hash": "7237a1c865e7bf72406ed3afeb873772bd473adf",
        "index": "20e3b25a..067f5d54 100644",
        "commit_message": "summary / dataflow fix\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def summary_moving_average(cost_var):",
            "MOVING_SUMMARY_VARS_KEY, as well as the argument",
            "Return a op to maintain these average",
            "\"\"\"",
            "-    global_step_var = tf.get_default_graph().get_tensor_by_name(GLOBAL_STEP_VAR_NAME)",
            "+    global_step_var = get_global_step_var()",
            "averager = tf.train.ExponentialMovingAverage(",
            "0.99, num_updates=global_step_var, name='moving_averages')",
            "vars_to_summary = [cost_var] + \\"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 8486,
        "neg_line": [
            "-global_step_var = tf.get_default_graph().get_tensor_by_name(GLOBAL_STEP_VAR_NAME)"
        ],
        "pos_line": [
            "+global_step_var = get_global_step_var()"
        ],
        "core_change": "-global_step_var = tf.get_default_graph().get_tensor_by_name(GLOBAL_STEP_VAR_NAME) +global_step_var = get_global_step_var()",
        "core_API": "get_default_graph"
    },
    {
        "commit_hash": "08d66bf2d3cdfd0f1aa7df8b0e9a8f7f6c52eb9c",
        "index": "030e2f72..aae8741b 100644",
        "commit_message": "Fix typo in output parameter name in docstring (#2708)\n\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def sort_batch_by_length(tensor: torch.Tensor, sequence_lengths: torch.Tensor):",
            "restoration_indices : torch.LongTensor",
            "Indices into the sorted_tensor such that",
            "``sorted_tensor.index_select(0, restoration_indices) == original_tensor``",
            "-    permuation_index : torch.LongTensor",
            "+    permutation_index : torch.LongTensor",
            "The indices used to sort the tensor. This is useful if you want to sort many",
            "tensors using the same ordering.",
            "\"\"\""
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=permuation_index), value='permutation_index')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 8490,
        "neg_line": [
            "-permuation_index : torch.LongTensor"
        ],
        "pos_line": [
            "+permutation_index : torch.LongTensor"
        ],
        "core_change": "-permuation_index : torch.LongTensor +permutation_index : torch.LongTensor",
        "core_API": "index_select"
    },
    {
        "commit_hash": "24b68d7a271e36fa6ece7aa7f5158639f059af0f",
        "index": "db24708..a975a80 100644",
        "commit_message": "Avoid edge cases with quantization by setting a known seed (#1295)\n\nSummary:\nPull Request resolved: https://github.com/facebookresearch/pytext/pull/1295\n\nStress test reveals some cases where the model returns NaN (https://our.intern.facebook.com/intern/testinfra/diagnostics/3377699742969609.844424947441516.1585199924/).  My guess is that some initializations play badly with quantization, so forcing a known seed to try and fix it.\n\nReviewed By: AkshatSh\n\nDifferential Revision: D20673784\n\nfbshipit-source-id: a341e59b7636d798f557c477a81655eb0975b28a\n\n",
        "file": "pytext.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Seq2SeqModelEvalTests(unittest.TestCase):",
            "\"\"\"",
            "tensorizers = get_tensorizers()",
            "",
            "+        # Avoid numeric issues with quantization by setting a known seed.",
            "+        torch.manual_seed(42)",
            "+",
            "model = Seq2SeqModel.from_config(",
            "Seq2SeqModel.Config(",
            "source_embedding=WordEmbedding.Config(embed_dim=512),"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=876055)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=876056)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=876057)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=876058)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=876059)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=876060)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'manual_seed'), position=2, insert_id=876061)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=876062)",
            "Insert(target_node=IN(type=argument_list), node=('integer', '42'), position=1, insert_id=876063)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=876064)"
        ],
        "plus_line": 2,
        "minus_line": 0,
        "AST_diff_line": 10,
        "number": 8496,
        "neg_line": [],
        "pos_line": [
            "+# Avoid numeric issues with quantization by setting a known seed.",
            "+torch.manual_seed(42)",
            "+"
        ],
        "core_change": "+# Avoid numeric issues with quantization by setting a known seed. +torch.manual_seed(42) +",
        "core_API": "manual_seed"
    },
    {
        "commit_hash": "a85d6ab84fa701705b19a66ffdd0e83d8c8985fa",
        "index": "c2a3c70..a197021 100644",
        "commit_message": "fix typo\n\n",
        "file": "tutorials.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class RNN(object):",
            "l_in_y = tf.matmul(l_in_x, Wi) + bi",
            "l_in_y = tf.reshape(l_in_y, [-1, self._time_steps, self._cell_size], name='2_3D')",
            "",
            "-            with tf.variable_scope('lstm_cell'):",
            "+            with tf.variable_scope('cell'):",
            "cell = tf.nn.rnn_cell.BasicRNNCell(self._cell_size)",
            "with tf.name_scope('initial_state'):",
            "self._cell_initial_state = cell.zero_state(self._batch_size, dtype=tf.float32)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text='lstm_cell'), value=\"'cell'\")"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 8501,
        "neg_line": [
            "-with tf.variable_scope('lstm_cell'):"
        ],
        "pos_line": [
            "+with tf.variable_scope('cell'):"
        ],
        "core_change": "-with tf.variable_scope('lstm_cell'): +with tf.variable_scope('cell'):",
        "core_API": "matmul"
    },
    {
        "commit_hash": "9d83d9210ac675a8a51fd4d3e7f9b7d57ba88380",
        "index": "d01df1d9..49a1130a 100755",
        "commit_message": "imgaug & fix svhn-dorefa double fw\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Model(ModelDesc):",
            ".apply(fg).BatchNorm('bn6')",
            ".apply(cabs)",
            ".FullyConnected('fc1', 10, nl=tf.identity)())",
            "+        tf.get_variable = old_get_variable",
            "prob = tf.nn.softmax(logits, name='output')",
            "",
            "cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, label)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=4, insert_id=2314114)",
            "Insert(target_node=ASTNode(type=module), node=('ERROR', None), position=5, insert_id=2314115)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=2314116)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=identifier, text=prob), position=0)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type==, text==), position=1)",
            "Move(target_node=ASTNode(type=expression_statement), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=assignment), node=('attribute', None), position=0, insert_id=2314117)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=2314118)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'old_get_variable'), position=2, insert_id=2314119)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2314120)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2314121)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'get_variable'), position=2, insert_id=2314122)",
            "Delete(target_node=ASTNode(type=assignment))"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 13,
        "number": 8510,
        "neg_line": [],
        "pos_line": [
            "+tf.get_variable = old_get_variable"
        ],
        "core_change": "+tf.get_variable = old_get_variable",
        "core_API": "softmax"
    },
    {
        "commit_hash": "9e58b8237bce22e7c2bc625c6820b288c3887890",
        "index": "1729b467..760b177c 100644",
        "commit_message": "Enable colocate_gradients_with_ops=True (#3620)\n\nBy default TensorFlow allocates all gradient matricies on gpu:0, which makes it pretty much impossible to do parallelize a large model.\n\ncolocate_gradients_with_ops puts these matricies next to the operations, allowing you to split your model across multiple GPUs. I ran into this issue myself and this fixed it for me.\n\nI think it's also meant to set gradient computations to be done on the device where the operations are stored, but my belief about that comes from https://github.com/tensorflow/tensorflow/issues/2441\n\nI'm not sure why this isn't the default in TF, so I'm not sure if this should be behind a flag or something, but having to make my own patches to keras to do multi-GPU training seems like the wrong answer.\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def gradients(loss, variables):",
            "'''Returns the gradients of `variables` (list of tensor variables)",
            "with regard to `loss`.",
            "'''",
            "-    return tf.gradients(loss, variables)",
            "+    return tf.gradients(loss, variables, colocate_gradients_with_ops=True)",
            "",
            "",
            "def stop_gradient(variables):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=2115959)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=2115960)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'colocate_gradients_with_ops'), position=0, insert_id=2115961)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2115962)",
            "Insert(target_node=IN(type=keyword_argument), node=('true', 'True'), position=2, insert_id=2115963)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 8513,
        "neg_line": [
            "-return tf.gradients(loss, variables)"
        ],
        "pos_line": [
            "+return tf.gradients(loss, variables, colocate_gradients_with_ops=True)"
        ],
        "core_change": "-return tf.gradients(loss, variables) +return tf.gradients(loss, variables, colocate_gradients_with_ops=True)",
        "core_API": "gradients"
    },
    {
        "commit_hash": "6d86604980831cc6b7dd4b9a0fc65830a845c2f2",
        "index": "6f3d71ee..0335b5e6 100644",
        "commit_message": "fix num_nodes\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def split(data, batch):",
            "",
            "# Edge indices should start at zero for every graph.",
            "data.edge_index -= node_slice[batch[row]].unsqueeze(0)",
            "-    data.__num_nodes__ = torch.bincount(batch).tolist()",
            "",
            "slices = {'edge_index': edge_slice}",
            "if data.x is not None:",
            "slices['x'] = node_slice",
            "+    else:",
            "+        data.__num_nodes__ = torch.bincount(batch).tolist()",
            "if data.edge_attr is not None:",
            "slices['edge_attr'] = edge_slice",
            "if data.y is not None:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=expression_statement), node=ASTNode(type=module), position=6)",
            "Insert(target_node=ASTNode(type=assignment), node=('identifier', 'else'), position=0, insert_id=1002085)",
            "Insert(target_node=ASTNode(type=assignment), node=(':', ':'), position=1, insert_id=1002086)",
            "Insert(target_node=ASTNode(type=assignment), node=('type', None), position=2, insert_id=1002087)",
            "Move(target_node=IN(type=type), node=ASTNode(type=attribute), position=0)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 8514,
        "neg_line": [
            "-data.__num_nodes__ = torch.bincount(batch).tolist()"
        ],
        "pos_line": [
            "+else:",
            "+data.__num_nodes__ = torch.bincount(batch).tolist()"
        ],
        "core_change": "-data.__num_nodes__ = torch.bincount(batch).tolist() +else: +data.__num_nodes__ = torch.bincount(batch).tolist()",
        "core_API": "bincount"
    },
    {
        "commit_hash": "79bca2bf64da04e7e1e74a132eb54171f41638cc",
        "index": "90abdc5..fedc55d 100644",
        "commit_message": "`LOGGER` consolidation (#5569)\n\n* Logger consolidation\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "yolov5.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def train(hyp,  # path/to/hyp.yaml or hyp dictionary",
            "",
            "# DP mode",
            "if cuda and RANK == -1 and torch.cuda.device_count() > 1:",
            "-        logging.warning('DP not recommended, instead use torch.distributed.run for best DDP Multi-GPU results.\\n'",
            "-                        'See Multi-GPU Tutorial at https://github.com/ultralytics/yolov5/issues/475 to get started.')",
            "+        LOGGER.warning('WARNING: DP not recommended, use torch.distributed.run for best DDP Multi-GPU results.\\n'",
            "+                       'See Multi-GPU Tutorial at https://github.com/ultralytics/yolov5/issues/475 to get started.')",
            "model = torch.nn.DataParallel(model)",
            "",
            "# SyncBatchNorm"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=logging), value='LOGGER')",
            "Update(target_node=ASTNode(type=string, text='DP not recommended, instead use torch.distributed.run for best DDP Multi-GPU results.\\n'), value=\"'WARNING: DP not recommended, use torch.distributed.run for best DDP Multi-GPU results.\\\\n'\")"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 8517,
        "neg_line": [
            "-logging.warning('DP not recommended, instead use torch.distributed.run for best DDP Multi-GPU results.\\n'",
            "-'See Multi-GPU Tutorial at https://github.com/ultralytics/yolov5/issues/475 to get started.')"
        ],
        "pos_line": [
            "+LOGGER.warning('WARNING: DP not recommended, use torch.distributed.run for best DDP Multi-GPU results.\\n'",
            "+'See Multi-GPU Tutorial at https://github.com/ultralytics/yolov5/issues/475 to get started.')"
        ],
        "core_change": "-logging.warning('DP not recommended, instead use torch.distributed.run for best DDP Multi-GPU results.\\n' -'See Multi-GPU Tutorial at https://github.com/ultralytics/yolov5/issues/475 to get started.') +LOGGER.warning('WARNING: DP not recommended, use torch.distributed.run for best DDP Multi-GPU results.\\n' +'See Multi-GPU Tutorial at https://github.com/ultralytics/yolov5/issues/475 to get started.')",
        "core_API": "device_count"
    },
    {
        "commit_hash": "cdf23f31ad4dbd7676ec571a6ea9502c82cf2665",
        "index": "62d34d7a..33eb193a 100644",
        "commit_message": "dtype fix\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def add_remaining_self_loops(edge_index,",
            "loop_weight[row[inv_mask]] = edge_weight[inv_mask].view(-1)",
            "edge_weight = torch.cat([edge_weight[mask], loop_weight], dim=0)",
            "",
            "-    loop_index = torch.arange(0,",
            "-                              num_nodes,",
            "-                              dtype=torch.long,",
            "-                              device=row.device)",
            "+    loop_index = torch.arange(0, num_nodes, dtype=row.dtype, device=row.device)",
            "loop_index = loop_index.unsqueeze(0).repeat(2, 1)",
            "edge_index = torch.cat([edge_index[:, mask], loop_index], dim=1)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=torch), value='row')",
            "Update(target_node=ASTNode(type=identifier, text=long), value='dtype')"
        ],
        "plus_line": 1,
        "minus_line": 4,
        "AST_diff_line": 2,
        "number": 8522,
        "neg_line": [
            "-loop_index = torch.arange(0,",
            "-num_nodes,",
            "-dtype=torch.long,",
            "-device=row.device)"
        ],
        "pos_line": [
            "+loop_index = torch.arange(0, num_nodes, dtype=row.dtype, device=row.device)"
        ],
        "core_change": "-loop_index = torch.arange(0, -num_nodes, -dtype=torch.long, -device=row.device) +loop_index = torch.arange(0, num_nodes, dtype=row.dtype, device=row.device)",
        "core_API": "cat"
    },
    {
        "commit_hash": "03e1325cf3a82981f5000d98a75451d5c049d66a",
        "index": "4cbe3b54..69852ece 100644",
        "commit_message": "Fix loss_bbox in fovea_head.py when num_pos=0 (#2548)\n\n* fix bbox_loss in fovea_head.py when num_pos=0\n\n* Remove PIL from .isort.cfg\n\n* Make loss_bbox scalar in fovea_head.py when num_pos==0\n\n* Sum based reduction for loss_bbox in fovea_head\n\n* reverting to scalar implementation\n",
        "file": "mmdetection.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class FoveaHead(nn.Module):",
            "pos_weights,",
            "avg_factor=num_pos)",
            "else:",
            "-            loss_bbox = torch.tensor([0],",
            "-                                     dtype=flatten_bbox_preds.dtype,",
            "-                                     device=flatten_bbox_preds.device)",
            "+            loss_bbox = torch.tensor(",
            "+                0,",
            "+                dtype=flatten_bbox_preds.dtype,",
            "+                device=flatten_bbox_preds.device)",
            "return dict(loss_cls=loss_cls, loss_bbox=loss_bbox)",
            "",
            "def fovea_target(self, gt_bbox_list, gt_label_list, featmap_sizes, points):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=integer, text=0), position=1)",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=list))"
        ],
        "plus_line": 4,
        "minus_line": 3,
        "AST_diff_line": 4,
        "number": 8524,
        "neg_line": [
            "-loss_bbox = torch.tensor([0],",
            "-dtype=flatten_bbox_preds.dtype,",
            "-device=flatten_bbox_preds.device)"
        ],
        "pos_line": [
            "+loss_bbox = torch.tensor(",
            "+0,",
            "+dtype=flatten_bbox_preds.dtype,",
            "+device=flatten_bbox_preds.device)"
        ],
        "core_change": "-loss_bbox = torch.tensor([0], -dtype=flatten_bbox_preds.dtype, -device=flatten_bbox_preds.device) +loss_bbox = torch.tensor( +0, +dtype=flatten_bbox_preds.dtype, +device=flatten_bbox_preds.device)",
        "core_API": "tensor"
    },
    {
        "commit_hash": "e386caa07107a77c7dfb2301c8ba60f05d0211da",
        "index": "01a2f191..395a10ea 100644",
        "commit_message": "mass linter fix\n\n",
        "file": "TTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Decoder(nn.Module):",
            "self.attention.init_win_idx()",
            "self.attention.init_states(inputs)",
            "outputs, stop_tokens, alignments, t = [], [], [], 0",
            "-        stop_flags = [True, False, False]",
            "while True:",
            "memory = self.prenet(self.memory_truncated)",
            "decoder_output, alignment, stop_token = self.decode(memory)"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=stop_flags))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=true, text=True))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=false, text=False))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=false, text=False))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=list))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 12,
        "number": 8531,
        "neg_line": [
            "-stop_flags = [True, False, False]"
        ],
        "pos_line": [],
        "core_change": "-stop_flags = [True, False, False]",
        "core_API": "init_win_idx"
    },
    {
        "commit_hash": "c05ace221339102abb1c8b0933b9cbfd51ddd4a3",
        "index": "85b280678..f4ad1a8ea 100644",
        "commit_message": "Use f-strings in the dataset scripts (#3291)\n\n* Finishes #3257\n\nUsed f-strings to format the .py files in the dataset folder\n\n* Fix style\n\n* Fix hkcancor dataset\n\nCo-authored-by: Mario Šaško <mario@huggingface.co>\nCo-authored-by: mariosasko <mariosasko777@gmail.com>\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class MsrGenomicsKbcomp(datasets.GeneratorBasedBuilder):",
            "",
            "if not os.path.exists(data_dir):",
            "raise FileNotFoundError(",
            "-                \"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('msr_genomics_kbcomp', data_dir=...)` that includes files unzipped from the reclor zip. Manual download instructions: {}\".format(",
            "-                    data_dir, self.manual_download_instructions",
            "-                )",
            "+                f\"{data_dir} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('msr_genomics_kbcomp', data_dir=...)` that includes files unzipped from the reclor zip. Manual download instructions: {self.manual_download_instructions}\"",
            ")",
            "return [",
            "datasets.SplitGenerator("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('string', 'f\"{data_dir} does not exist. Make sure you insert a manual dir via `datasets.load_dataset(\\'msr_genomics_kbcomp\\', data_dir=...)` that includes files unzipped from the reclor zip. Manual download instructions: {self.manual_download_instructions}\"'), position=1, insert_id=1781619)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=), text=)), position=2)",
            "Delete(target_node=ASTNode(type=string, text=\"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('msr_genomics_kbcomp', data_dir=...)` that includes files unzipped from the reclor zip. Manual download instructions: {}\"))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=format))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=data_dir))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=manual_download_instructions))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=), text=)))"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 16,
        "number": 8532,
        "neg_line": [
            "-\"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('msr_genomics_kbcomp', data_dir=...)` that includes files unzipped from the reclor zip. Manual download instructions: {}\".format(",
            "-data_dir, self.manual_download_instructions",
            "-)"
        ],
        "pos_line": [
            "+f\"{data_dir} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('msr_genomics_kbcomp', data_dir=...)` that includes files unzipped from the reclor zip. Manual download instructions: {self.manual_download_instructions}\""
        ],
        "core_change": "-\"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('msr_genomics_kbcomp', data_dir=...)` that includes files unzipped from the reclor zip. Manual download instructions: {}\".format( -data_dir, self.manual_download_instructions -) +f\"{data_dir} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('msr_genomics_kbcomp', data_dir=...)` that includes files unzipped from the reclor zip. Manual download instructions: {self.manual_download_instructions}\"",
        "core_API": "exists"
    },
    {
        "commit_hash": "bf659621428ada80e36e34612942cdec7f908f2f",
        "index": "8c7b84c3..dcd680ee 100644",
        "commit_message": "Fix to ItoProcess and fwd_gradient docstring examples.\n\nPiperOrigin-RevId: 272845796\n\n",
        "file": "tf-quant-finance.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def fwd_gradient(func, x, grad_x=None, use_gradient_tape=False):",
            "op and the standard `tf.gradients`",
            "```python",
            "t = tf.range(1, 3, dtype=tf.float32)  # Shape [2]",
            "-    y = tf.stack([t, t ** 2, t ** 3], axis=0)  # Shape [3, 2]",
            "+    def fn(t):",
            "+      return tf.stack([t, t ** 2, t ** 3], axis=0)  # Shape [3, 2]",
            "# Produces shape [3, 2] with values [[1, 1], [2, 4], [3, 12]]",
            "-    fwd_grad_y = fwd_gradient(y, t)",
            "+    fwd_grad_y = fwd_gradient(fn, t)",
            "# Produces shape [2] with values [6, 17].",
            "bck_grad_y = tf.gradients(y, t)[0]",
            "```"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=module), position=6)",
            "Insert(target_node=ASTNode(type=module), node=('ERROR', None), position=2, insert_id=2347953)",
            "Insert(target_node=ASTNode(type=module), node=('function_definition', None), position=6, insert_id=2347954)",
            "Insert(target_node=IN(type=ERROR), node=('string', '``'), position=0, insert_id=2347955)",
            "Insert(target_node=IN(type=ERROR), node=('\"', '`'), position=1, insert_id=2347956)",
            "Insert(target_node=IN(type=function_definition), node=('def', 'def'), position=0, insert_id=2347957)",
            "Insert(target_node=IN(type=function_definition), node=('identifier', 'fn'), position=1, insert_id=2347958)",
            "Insert(target_node=IN(type=function_definition), node=('parameters', None), position=2, insert_id=2347959)",
            "Insert(target_node=IN(type=function_definition), node=(':', ':'), position=3, insert_id=2347960)",
            "Insert(target_node=IN(type=function_definition), node=('block', None), position=4, insert_id=2347961)",
            "Insert(target_node=IN(type=parameters), node=('(', '('), position=0, insert_id=2347962)",
            "Insert(target_node=IN(type=parameters), node=('identifier', 't'), position=1, insert_id=2347963)",
            "Insert(target_node=IN(type=parameters), node=(')', ')'), position=2, insert_id=2347964)",
            "Insert(target_node=IN(type=block), node=('return_statement', None), position=0, insert_id=2347965)",
            "Insert(target_node=IN(type=return_statement), node=('return', 'return'), position=0, insert_id=2347966)",
            "Move(target_node=IN(type=return_statement), node=ASTNode(type=call), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=y), value='fn')",
            "Delete(target_node=ASTNode(type=identifier, text=y))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=string, text=``))",
            "Delete(target_node=ASTNode(type=\", text=`))",
            "Delete(target_node=ASTNode(type=ERROR))"
        ],
        "plus_line": 3,
        "minus_line": 2,
        "AST_diff_line": 24,
        "number": 8535,
        "neg_line": [
            "-y = tf.stack([t, t ** 2, t ** 3], axis=0)  # Shape [3, 2]",
            "-fwd_grad_y = fwd_gradient(y, t)"
        ],
        "pos_line": [
            "+def fn(t):",
            "+return tf.stack([t, t ** 2, t ** 3], axis=0)  # Shape [3, 2]",
            "+fwd_grad_y = fwd_gradient(fn, t)"
        ],
        "core_change": "-y = tf.stack([t, t ** 2, t ** 3], axis=0)  # Shape [3, 2] +def fn(t): +return tf.stack([t, t ** 2, t ** 3], axis=0)  # Shape [3, 2] -fwd_grad_y = fwd_gradient(y, t) +fwd_grad_y = fwd_gradient(fn, t)",
        "core_API": "range"
    },
    {
        "commit_hash": "a039748e4b1e316d8310648da0b251fee1d33033",
        "index": "60deb2d3..24e4fdcf 100644",
        "commit_message": "retro env improvement, some fixes\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Module(object):",
            "use_while_v2=False",
            "):",
            "Module.global_scope.append('while')",
            "-        if maximum_iterations is not None and maximum_iterations.dtype not in (tf.int32, tf.int64):",
            "+        if maximum_iterations is not None and maximum_iterations.dtype is not tf.int32:",
            "maximum_iterations = tf.dtypes.cast(x=maximum_iterations, dtype=tf.int32)",
            "if use_while_v2:",
            "x = while_v2.while_loop("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=comparison_operator), node=('is not', 'is'), position=1, insert_id=2230044)",
            "Insert(target_node=ASTNode(type=comparison_operator), node=('is not', 'not'), position=2, insert_id=2230045)",
            "Move(target_node=ASTNode(type=comparison_operator), node=ASTNode(type=attribute), position=3)",
            "Delete(target_node=ASTNode(type=not in, text=not))",
            "Delete(target_node=ASTNode(type=not in, text=in))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=int64))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=tuple))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 13,
        "number": 8538,
        "neg_line": [
            "-if maximum_iterations is not None and maximum_iterations.dtype not in (tf.int32, tf.int64):"
        ],
        "pos_line": [
            "+if maximum_iterations is not None and maximum_iterations.dtype is not tf.int32:"
        ],
        "core_change": "-if maximum_iterations is not None and maximum_iterations.dtype not in (tf.int32, tf.int64): +if maximum_iterations is not None and maximum_iterations.dtype is not tf.int32:",
        "core_API": "append"
    },
    {
        "commit_hash": "d26b37e744ea980977e266adf48736451b73c583",
        "index": "2c9669306..77a2abeed 100644",
        "commit_message": "Speech2TextTransformer (#10175)\n\n* s2t\n\n* fix config\n\n* conversion script\n\n* fix import\n\n* add tokenizer\n\n* fix tok init\n\n* fix tokenizer\n\n* first version working\n\n* fix embeds\n\n* fix lm head\n\n* remove extra heads\n\n* fix convert script\n\n* handle encoder attn mask\n\n* style\n\n* better enc attn mask\n\n* override _prepare_attention_mask_for_generation\n\n* handle attn_maks in encoder and decoder\n\n* input_ids => input_features\n\n* enable use_cache\n\n* remove old code\n\n* expand embeddings if needed\n\n* remove logits bias\n\n* masked_lm_loss => loss\n\n* hack tokenizer to support feature processing\n\n* fix model_input_names\n\n* style\n\n* fix error message\n\n* doc\n\n* remove inputs_embeds\n\n* remove input_embeds\n\n* remove unnecessary docstring\n\n* quality\n\n* SpeechToText => Speech2Text\n\n* style\n\n* remove shared_embeds\n\n* subsample => conv\n\n* remove Speech2TextTransformerDecoderWrapper\n\n* update output_lengths formula\n\n* fix table\n\n* remove max_position_embeddings\n\n* update conversion scripts\n\n* add possibility to do upper case for now\n\n* add FeatureExtractor and Processor\n\n* add tests for extractor\n\n* require_torch_audio => require_torchaudio\n\n* add processor test\n\n* update import\n\n* remove classification head\n\n* attention mask is now 1D\n\n* update docstrings\n\n* attention mask should be of type long\n\n* handle attention mask from generate\n\n* alwyas return attention_mask\n\n* fix test\n\n* style\n\n* doc\n\n* Speech2TextTransformer => Speech2Text\n\n* Speech2TextTransformerConfig => Speech2TextConfig\n\n* remove dummy_inputs\n\n* nit\n\n* style\n\n* multilinguial tok\n\n* fix tokenizer\n\n* add tgt_lang setter\n\n* save lang_codes\n\n* fix tokenizer\n\n* add forced_bos_token_id to tokenizer\n\n* apply review suggestions\n\n* add torchaudio to extra deps\n\n* add speech deps to CI\n\n* fix dep\n\n* add libsndfile to ci\n\n* libsndfile1\n\n* add speech to extras all\n\n* libsndfile1 -> libsndfile1\n\n* libsndfile\n\n* libsndfile1-dev\n\n* apt update\n\n* add sudo to install\n\n* update deps table\n\n* install libsndfile1-dev on CI\n\n* tuple to list\n\n* init conv layer\n\n* add model tests\n\n* quality\n\n* add integration tests\n\n* skip_special_tokens\n\n* add speech_to_text_transformer in toctree\n\n* fix tokenizer\n\n* fix fp16 tests\n\n* add tokenizer tests\n\n* fix copyright\n\n* input_values => input_features\n\n* doc\n\n* add model in readme\n\n* doc\n\n* change checkpoint names\n\n* fix copyright\n\n* fix code example\n\n* add max_model_input_sizes in tokenizer\n\n* fix integration tests\n\n* add do_lower_case to tokenizer\n\n* remove clamp trick\n\n* fix \"Add modeling imports here\"\n\n* fix copyrights\n\n* fix tests\n\n* SpeechToTextTransformer => SpeechToText\n\n* fix naming\n\n* fix table formatting\n\n* fix typo\n\n* style\n\n* fix typos\n\n* remove speech dep from extras[testing]\n\n* fix copies\n\n* rename doc file,\n\n* put imports under is_torch_available\n\n* run feat extract tests when torch is available\n\n* dummy objects for processor and extractor\n\n* fix imports in tests\n\n* fix import in modeling test\n\n* fxi imports\n\n* fix torch import\n\n* fix imports again\n\n* fix positional embeddings\n\n* fix typo in import\n\n* adapt new extractor refactor\n\n* style\n\n* fix torchscript test\n\n* doc\n\n* doc\n\n* Apply suggestions from code review\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* fix docs, copied from, style\n\n* fix docstring\n\n* handle imports\n\n* remove speech from all extra deps\n\n* remove s2t from seq2seq lm mapping\n\n* better names\n\n* skip training tests\n\n* add install instructions\n\n* List => Tuple\n\n* doc\n\n* fix conversion script\n\n* fix urls\n\n* add instruction for libsndfile\n\n* fix fp16 test\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "if is_torch_available():",
            "class GenerationTesterMixin:",
            "model_tester = None",
            "all_generative_model_classes = ()",
            "+    input_name = \"input_ids\"",
            "",
            "def _get_input_ids_and_config(self):",
            "config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()",
            "",
            "-        input_ids = inputs_dict[\"input_ids\"]",
            "-        attention_mask = torch.ones_like(input_ids)",
            "+        input_ids = inputs_dict[self.input_name]",
            "+        attention_mask = torch.ones_like(input_ids, dtype=torch.long)",
            "",
            "# cut to half length & take max batch_size 3",
            "max_batch_size = 2"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=4, insert_id=1219756)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1219757)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'input_name'), position=0, insert_id=1219758)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1219759)",
            "Insert(target_node=IN(type=assignment), node=('string', '\"input_ids\"'), position=2, insert_id=1219760)",
            "Insert(target_node=ASTNode(type=subscript), node=('attribute', None), position=2, insert_id=1219761)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=1219762)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1219763)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'input_name'), position=2, insert_id=1219764)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1219765)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1219766)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=1219767)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1219768)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=1219769)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1219770)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1219771)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'long'), position=2, insert_id=1219772)",
            "Delete(target_node=ASTNode(type=string, text=\"input_ids\"))"
        ],
        "plus_line": 3,
        "minus_line": 2,
        "AST_diff_line": 18,
        "number": 8540,
        "neg_line": [
            "-input_ids = inputs_dict[\"input_ids\"]",
            "-attention_mask = torch.ones_like(input_ids)"
        ],
        "pos_line": [
            "+input_name = \"input_ids\"",
            "+input_ids = inputs_dict[self.input_name]",
            "+attention_mask = torch.ones_like(input_ids, dtype=torch.long)"
        ],
        "core_change": "+input_name = \"input_ids\" -input_ids = inputs_dict[\"input_ids\"] -attention_mask = torch.ones_like(input_ids) +input_ids = inputs_dict[self.input_name] +attention_mask = torch.ones_like(input_ids, dtype=torch.long)",
        "core_API": "prepare_config_and_inputs_for_common"
    },
    {
        "commit_hash": "0d4e97e765cc40245dfba663e223f89f0b588343",
        "index": "56c8dd6..c5cb02c 100644",
        "commit_message": "fix rnn dropout\n\n",
        "file": "text-classification-cnn-rnn.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "layers",
        "change": [
            "class TextCNN(object):",
            "with tf.name_scope(\"score\"):",
            "# 全连接层，后面接dropout以及relu激活",
            "fc = tf.layers.dense(gmp, self.config.hidden_dim, name='fc1')",
            "-            fc = tf.contrib.layers.dropout(fc,",
            "-                self.keep_prob)",
            "+            fc = tf.contrib.layers.dropout(fc, self.keep_prob)",
            "fc = tf.nn.relu(fc)",
            "",
            "# 分类器"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 8541,
        "neg_line": [
            "-fc = tf.contrib.layers.dropout(fc,",
            "-self.keep_prob)"
        ],
        "pos_line": [
            "+fc = tf.contrib.layers.dropout(fc, self.keep_prob)"
        ],
        "core_change": "-fc = tf.contrib.layers.dropout(fc, -self.keep_prob) +fc = tf.contrib.layers.dropout(fc, self.keep_prob)",
        "core_API": "name_scope"
    },
    {
        "commit_hash": "67a8be8e90a7fbd5e0bceff9f29fb89ccabb61be",
        "index": "ed6f950e2..cd4cf247a 100644",
        "commit_message": "fix backward in tests\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class CommonTestCases:",
            "inputs = inputs_dict.copy()",
            "inputs['head_mask'] = head_mask",
            "",
            "-                with torch.no_grad():",
            "-                    outputs = model(**inputs)",
            "+                outputs = model(**inputs)",
            "",
            "# Test that we can get a gradient back for importance score computation",
            "output = sum(t.sum() for t in outputs[0])"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=module), node=ASTNode(type=expression_statement), position=3)",
            "Delete(target_node=ASTNode(type=with, text=with))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=no_grad))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=with_item))",
            "Delete(target_node=ASTNode(type=with_clause))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=block))",
            "Delete(target_node=ASTNode(type=with_statement))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 15,
        "number": 8549,
        "neg_line": [
            "-with torch.no_grad():",
            "-outputs = model(**inputs)"
        ],
        "pos_line": [
            "+outputs = model(**inputs)"
        ],
        "core_change": "-with torch.no_grad(): -outputs = model(**inputs) +outputs = model(**inputs)",
        "core_API": "copy"
    },
    {
        "commit_hash": "cc4d6623023841a845ea71aa801c19af7266d1ad",
        "index": "172ea7c..9325b19 100644",
        "commit_message": "FIX BUG\n1. model parallel x_grad rescale\n\n",
        "file": "insightface.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class PartialFC(Module):",
            "logits.backward(grad)",
            "if total_features.grad is not None:",
            "total_features.grad.detach_()",
            "-        x_grad: torch.Tensor = torch.zeros_like(features)",
            "-        x_grad.mul_(self.world_size)",
            "-",
            "+        x_grad: torch.Tensor = torch.zeros_like(features, requires_grad=True)",
            "# feature gradient all-reduce",
            "dist.reduce_scatter(x_grad, list(total_features.grad.chunk(self.world_size, dim=0)))",
            "+        x_grad = x_grad * self.world_size",
            "# backward backbone",
            "return x_grad, loss_v"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=7, insert_id=255534)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=255535)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'x_grad'), position=0, insert_id=255536)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=255537)",
            "Insert(target_node=IN(type=assignment), node=('binary_operator', None), position=2, insert_id=255538)",
            "Insert(target_node=IN(type=binary_operator), node=('identifier', 'x_grad'), position=0, insert_id=255539)",
            "Insert(target_node=IN(type=binary_operator), node=('*', '*'), position=1, insert_id=255540)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=attribute), position=2)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=255541)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=255542)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'requires_grad'), position=0, insert_id=255543)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=255544)",
            "Insert(target_node=IN(type=keyword_argument), node=('true', 'True'), position=2, insert_id=255545)",
            "Delete(target_node=ASTNode(type=identifier, text=x_grad))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=mul_))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 22,
        "number": 8551,
        "neg_line": [
            "-x_grad: torch.Tensor = torch.zeros_like(features)",
            "-x_grad.mul_(self.world_size)",
            "-"
        ],
        "pos_line": [
            "+x_grad: torch.Tensor = torch.zeros_like(features, requires_grad=True)",
            "+x_grad = x_grad * self.world_size"
        ],
        "core_change": "-x_grad: torch.Tensor = torch.zeros_like(features) -x_grad.mul_(self.world_size) - +x_grad: torch.Tensor = torch.zeros_like(features, requires_grad=True) +x_grad = x_grad * self.world_size",
        "core_API": "backward"
    },
    {
        "commit_hash": "867a5efa3523fd2bd2fb7705e293f80f4bb15299",
        "index": "ee8fc53b..64a4e56f 100644",
        "commit_message": "Release 1.8.6rc0 (#660)\n\n* Release 1.8.6rc0\n\n* Update CHANGELOG.md\n\n* Typo Fix\n\n* Changelog Updated\n\n* tl_logging more readable\n\n* Typo Fixed\n\n* https enforced for badges\n\n* RTD badges pointing to Latest Commit\n\n* Decorators API Refactored\n\n* extra_requires `all`, `all_cpu` and `all_gpu` added\n\n* Error fix\n\n* YAPF Formating Correction\n\n* Test for private method decorator added\n\n* Test Logging Verbosity Fixed to Debug when runned individually\n\n* YAPF corrections applied\n\n* Changelog Added\n\n* Changelog updated\n\n* PR number changed\n\n* Changelog updated\n\n* Update .travis.yml\n\n* Update CONTRIBUTING.md\n\n* Update CONTRIBUTING.md\n\n* Update CHANGELOG.md\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TL_Logger_Test(CustomTestCase):",
            "",
            "if __name__ == '__main__':",
            "",
            "-    # tl.logging.set_verbosity(tl.logging.INFO)",
            "-    # tl.logging.set_verbosity(tl.logging.INFO)",
            "-    tl.logging.set_verbosity(tl.logging.DEBUG)",
            "+    tf.logging.set_verbosity(tf.logging.DEBUG)",
            "tl.logging.set_verbosity(tl.logging.DEBUG)",
            "",
            "unittest.main()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=2262724)",
            "Update(target_node=ASTNode(type=identifier, text=tl), value='tf')",
            "Insert(target_node=ASTNode(type=attribute), node=('identifier', 'logging'), position=2, insert_id=2262725)",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=tl), value='tf')",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=logging), position=2)",
            "Delete(target_node=ASTNode(type=identifier, text=logging))",
            "Delete(target_node=ASTNode(type=., text=.))"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 8,
        "number": 8554,
        "neg_line": [
            "-# tl.logging.set_verbosity(tl.logging.INFO)",
            "-# tl.logging.set_verbosity(tl.logging.INFO)",
            "-tl.logging.set_verbosity(tl.logging.DEBUG)"
        ],
        "pos_line": [
            "+tf.logging.set_verbosity(tf.logging.DEBUG)"
        ],
        "core_change": "-# tl.logging.set_verbosity(tl.logging.INFO) -# tl.logging.set_verbosity(tl.logging.INFO) -tl.logging.set_verbosity(tl.logging.DEBUG) +tf.logging.set_verbosity(tf.logging.DEBUG)",
        "core_API": "set_verbosity"
    },
    {
        "commit_hash": "fad27c004661692d715b31e8830122f93a09347f",
        "index": "b690dbe..2d53824 100644",
        "commit_message": "Update DDP for `torch.distributed.run` with `gloo` backend (#3680)\n\n* Update DDP for `torch.distributed.run`\n\n* Add LOCAL_RANK\n\n* remove opt.local_rank\n\n* backend=\"gloo|nccl\"\n\n* print\n\n* print\n\n* debug\n\n* debug\n\n* os.getenv\n\n* gloo\n\n* gloo\n\n* gloo\n\n* cleanup\n\n* fix getenv\n\n* cleanup\n\n* cleanup destroy\n\n* try nccl\n\n* return opt\n\n* add --local_rank\n\n* add timeout\n\n* add init_method\n\n* gloo\n\n* move destroy\n\n* move destroy\n\n* move print(opt) under if RANK\n\n* destroy only RANK 0\n\n* move destroy inside train()\n\n* restore destroy outside train()\n\n* update print(opt)\n\n* cleanup\n\n* nccl\n\n* gloo with 60 second timeout\n\n* update namespace printing\n",
        "file": "yolov5.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def torch_distributed_zero_first(local_rank: int):",
            "Decorator to make all processes in distributed training wait for each local_master to do something.",
            "\"\"\"",
            "if local_rank not in [-1, 0]:",
            "-        torch.distributed.barrier()",
            "+        dist.barrier()",
            "yield",
            "if local_rank == 0:",
            "-        torch.distributed.barrier()",
            "+        dist.barrier()",
            "",
            "",
            "def init_torch_seeds(seed=0):"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=1298422)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=argument_list), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=torch), value='dist')",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=torch), position=0)",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1298423)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1298424)",
            "Update(target_node=ASTNode(type=identifier, text=torch), value='dist')",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=torch), position=0)",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Delete(target_node=ASTNode(type=identifier, text=distributed))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=distributed))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 19,
        "number": 8561,
        "neg_line": [
            "-torch.distributed.barrier()",
            "-torch.distributed.barrier()"
        ],
        "pos_line": [
            "+dist.barrier()",
            "+dist.barrier()"
        ],
        "core_change": "-torch.distributed.barrier() +dist.barrier() -torch.distributed.barrier() +dist.barrier()",
        "core_API": "barrier"
    },
    {
        "commit_hash": "e491efb81f89f60ab98a9c8a343c55c327527900",
        "index": "5173aff..4daf69b 100644",
        "commit_message": "lint things\n\nSummary:\nLint related fixes: Improve internal/OSS consistency. Fix the fight between black and certain pyre-ignore markers by moving them to the line before.\nUse clang-format-8 automatically if present. Small number of pyre fixes.\n\narc doesn't run pyre at the moment, so I put back the explicit call to pyre. I don't know if there's an option somewhere to change this.\n\nReviewed By: nikhilaravi\n\nDifferential Revision: D19780518\n\nfbshipit-source-id: ef1c243392322fa074130f6cff2dd8a6f7738a7f\n\n",
        "file": "pytorch3d.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Textures(object):",
            "msg = \"Expected verts_rgb to be of shape (N, V, 3); got %r\"",
            "raise ValueError(msg % verts_rgb.shape)",
            "if maps is not None:",
            "-            if torch.is_tensor(map) and map.ndim != 4:",
            "+            if torch.is_tensor(maps) and maps.ndim != 4:",
            "msg = \"Expected maps to be of shape (N, H, W, 3); got %r\"",
            "raise ValueError(msg % repr(maps.shape))",
            "elif isinstance(maps, list):"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=map), value='maps')",
            "Update(target_node=ASTNode(type=identifier, text=map), value='maps')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 8562,
        "neg_line": [
            "-if torch.is_tensor(map) and map.ndim != 4:"
        ],
        "pos_line": [
            "+if torch.is_tensor(maps) and maps.ndim != 4:"
        ],
        "core_change": "-if torch.is_tensor(map) and map.ndim != 4: +if torch.is_tensor(maps) and maps.ndim != 4:",
        "core_API": "is_tensor"
    },
    {
        "commit_hash": "6fe6a4016237d29aeb540086788621c6a7c25f51",
        "index": "e98ce761bd..8a105f4000 100644",
        "commit_message": "Fixed failing test for statistical einsum (#6073)\n\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def einsum(",
            "*operands: Union[tf.Tensor, tf.Variable],",
            "out: Optional[Union[tf.Tensor, tf.Variable]] = None,",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "+    dtype = _get_promoted_type_of_operands(operands)",
            "operands = (tf.cast(operand, tf.float32) for operand in operands)",
            "-    return tf.einsum(equation, *operands)",
            "+    return tf.cast(tf.einsum(equation, *operands), dtype)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=function_definition), node=('block', None), position=6, insert_id=1983831)",
            "Insert(target_node=ASTNode(type=return_statement), node=('call', None), position=1, insert_id=1983832)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=1983833)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1983834)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1983835)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1983836)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=1983837)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1983838)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cast'), position=2, insert_id=1983839)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1983840)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=call), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=1983841)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'dtype'), position=3, insert_id=1983842)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=1983843)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'dtype'), position=0, insert_id=1983844)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1983845)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=1983846)",
            "Insert(target_node=IN(type=call), node=('identifier', '_get_promoted_type_of_operands'), position=0, insert_id=1983847)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1983848)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1983849)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'operands'), position=1, insert_id=1983850)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1983851)",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 23,
        "number": 8563,
        "neg_line": [
            "-return tf.einsum(equation, *operands)"
        ],
        "pos_line": [
            "+dtype = _get_promoted_type_of_operands(operands)",
            "+return tf.cast(tf.einsum(equation, *operands), dtype)"
        ],
        "core_change": "+dtype = _get_promoted_type_of_operands(operands) -return tf.einsum(equation, *operands) +return tf.cast(tf.einsum(equation, *operands), dtype)",
        "core_API": "cast"
    },
    {
        "commit_hash": "222dbdb2039678564444b51fdd35975fce468fe1",
        "index": "beff5ed2f..91966ba31 100644",
        "commit_message": "allow integer device for BatchEncoding (#9271)\n\nFixes #9244\n\nCo-authored-by: Jethro Kuan <jethro.kuan@bytedance.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class BatchEncoding(UserDict):",
            "# This check catches things like APEX blindly calling \"to\" on all inputs to a module",
            "# Otherwise it passes the casts down and casts the LongTensor containing the token idxs",
            "# into a HalfTensor",
            "-        if isinstance(device, str) or isinstance(device, torch.device):",
            "+        if isinstance(device, str) or isinstance(device, torch.device) or isinstance(device, int):",
            "self.data = {k: v.to(device=device) for k, v in self.data.items()}",
            "else:",
            "logger.warning("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=boolean_operator), node=ASTNode(type=boolean_operator), position=0)",
            "Insert(target_node=ASTNode(type=boolean_operator), node=('or', 'or'), position=1, insert_id=1224924)",
            "Insert(target_node=ASTNode(type=boolean_operator), node=('call', None), position=2, insert_id=1224925)",
            "Insert(target_node=IN(type=call), node=('identifier', 'isinstance'), position=0, insert_id=1224926)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1224927)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1224928)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'device'), position=1, insert_id=1224929)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=1224930)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'int'), position=3, insert_id=1224931)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=1224932)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 8566,
        "neg_line": [
            "-if isinstance(device, str) or isinstance(device, torch.device):"
        ],
        "pos_line": [
            "+if isinstance(device, str) or isinstance(device, torch.device) or isinstance(device, int):"
        ],
        "core_change": "-if isinstance(device, str) or isinstance(device, torch.device): +if isinstance(device, str) or isinstance(device, torch.device) or isinstance(device, int):",
        "core_API": "to"
    },
    {
        "commit_hash": "94c484f1931ff1e61f3ad9febedf36e261a79e4b",
        "index": "7246985b..74ebe959 100644",
        "commit_message": "fix bug in gradient computation for tail adaptive loss (#1805)\n\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TraceTailAdaptive_ELBO(Trace_ELBO):",
            "check_fully_reparametrized(site)",
            "",
            "# rank the particles according to p/q",
            "-        log_pq = torch.logsumexp(log_p - log_q, dim=0)",
            "+        log_pq = log_p - log_q",
            "rank = torch.argsort(log_pq, descending=False)",
            "rank = torch.index_select(torch.arange(self.num_particles) + 1, -1, rank).type_as(log_pq)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=binary_operator), position=2)",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=logsumexp))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=dim))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=integer, text=0))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 14,
        "number": 8568,
        "neg_line": [
            "-log_pq = torch.logsumexp(log_p - log_q, dim=0)"
        ],
        "pos_line": [
            "+log_pq = log_p - log_q"
        ],
        "core_change": "-log_pq = torch.logsumexp(log_p - log_q, dim=0) +log_pq = log_p - log_q",
        "core_API": "logsumexp"
    },
    {
        "commit_hash": "8f4183e742b40818758138c4c18ee50eeae5035f",
        "index": "2b5da844..ced96128 100644",
        "commit_message": "use tf.gfile in saver (fix #345)\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class ModelSaver(Callback):",
            "self.var_collections = var_collections",
            "if checkpoint_dir is None:",
            "checkpoint_dir = logger.LOG_DIR",
            "-        assert os.path.isdir(checkpoint_dir), checkpoint_dir",
            "+        assert tf.gfile.IsDirectory(checkpoint_dir), checkpoint_dir",
            "self.checkpoint_dir = checkpoint_dir",
            "",
            "def _setup_graph(self):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=isdir), value='IsDirectory')",
            "Update(target_node=ASTNode(type=identifier, text=os), value='tf')",
            "Update(target_node=ASTNode(type=identifier, text=path), value='gfile')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 3,
        "number": 8574,
        "neg_line": [
            "-assert os.path.isdir(checkpoint_dir), checkpoint_dir"
        ],
        "pos_line": [
            "+assert tf.gfile.IsDirectory(checkpoint_dir), checkpoint_dir"
        ],
        "core_change": "-assert os.path.isdir(checkpoint_dir), checkpoint_dir +assert tf.gfile.IsDirectory(checkpoint_dir), checkpoint_dir",
        "core_API": "isdir"
    },
    {
        "commit_hash": "6dd1083bbbc5d29643aafef3373853f03a317a92",
        "index": "1f2b467..82043b7 100644",
        "commit_message": "Tensorboard model visualization bug fix (#2758)\n\nThis fix should allow for visualizing YOLOv5 model graphs correctly in Tensorboard by uncommenting line 335 in train.py:\n```python\n                    if tb_writer:\n                        tb_writer.add_graph(torch.jit.trace(model, imgs, strict=False), [])  # add model graph\n```\n\nThe problem was that the detect() layer checks the input size to adapt the grid if required, and tracing does not seem to like this shape check (even if the shape is fine and no grid recomputation is required). The following will warn:\nhttps://github.com/ultralytics/yolov5/blob/0cae7576a9241110157cd154fc2237e703c2719e/train.py#L335\n\nSolution is below. This is a YOLOv5s model displayed in TensorBoard. You can see the Detect() layer merging the 3 layers into a single output for example, and everything appears to work and visualize correctly.\n```python\ntb_writer.add_graph(torch.jit.trace(model, imgs, strict=False), [])\n```\n<img width=\"893\" alt=\"Screenshot 2021-04-11 at 01 10 09\" src=\"https://user-images.githubusercontent.com/26833433/114286928-349bd600-9a63-11eb-941f-7139ee6cd602.png\">\n",
        "file": "yolov5.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def train(hyp, opt, device, tb_writer=None):",
            "Thread(target=plot_images, args=(imgs, targets, paths, f), daemon=True).start()",
            "# if tb_writer:",
            "#     tb_writer.add_image(f, result, dataformats='HWC', global_step=epoch)",
            "-                    #     tb_writer.add_graph(model, imgs)  # add model to tensorboard",
            "+                    #     tb_writer.add_graph(torch.jit.trace(model, imgs, strict=False), [])  # add model graph",
            "elif plots and ni == 10 and wandb_logger.wandb:",
            "wandb_logger.log({\"Mosaics\": [wandb_logger.wandb.Image(str(x), caption=x.name) for x in",
            "save_dir.glob('train*.jpg') if x.exists()]})"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 8578,
        "neg_line": [
            "-#     tb_writer.add_graph(model, imgs)  # add model to tensorboard"
        ],
        "pos_line": [
            "+#     tb_writer.add_graph(torch.jit.trace(model, imgs, strict=False), [])  # add model graph"
        ],
        "core_change": "-#     tb_writer.add_graph(model, imgs)  # add model to tensorboard +#     tb_writer.add_graph(torch.jit.trace(model, imgs, strict=False), [])  # add model graph",
        "core_API": "add_image"
    },
    {
        "commit_hash": "95b6d985ffb231d9a2047c039c95660913063b9d",
        "index": "9182d995..08e41e70 100644",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestMaskedLayerNorm(AllenNlpTestCase):",
            "mask_n = np.array([[1, 1, 0], [1, 1, 1]])",
            "",
            "x = torch.from_numpy(x_n).float()",
            "-        mask = torch.from_numpy(mask_n)",
            "+        mask = torch.from_numpy(mask_n).bool()",
            "",
            "layer_norm = MaskedLayerNorm(7, gamma0=0.2)",
            "normed_x = layer_norm(x, mask)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=19752)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=19753)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=19754)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'bool'), position=2, insert_id=19755)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=19756)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=19757)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 8580,
        "neg_line": [
            "-mask = torch.from_numpy(mask_n)"
        ],
        "pos_line": [
            "+mask = torch.from_numpy(mask_n).bool()"
        ],
        "core_change": "-mask = torch.from_numpy(mask_n) +mask = torch.from_numpy(mask_n).bool()",
        "core_API": "array"
    },
    {
        "commit_hash": "95b6d985ffb231d9a2047c039c95660913063b9d",
        "index": "3edee3f9..8ee2241b 100644",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class MultiHeadSelfAttention(Seq2SeqEncoder):",
            "",
            "batch_size, timesteps, _ = inputs.size()",
            "if mask is None:",
            "-            mask = inputs.new_ones(batch_size, timesteps)",
            "+            mask = inputs.new_ones(batch_size, timesteps).bool()",
            "",
            "# Shape (batch_size, timesteps, 2 * attention_dim + values_dim)",
            "combined_projection = self._combined_projection(inputs)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=19594)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=19595)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=19596)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'bool'), position=2, insert_id=19597)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=19598)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=19599)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 8585,
        "neg_line": [
            "-mask = inputs.new_ones(batch_size, timesteps)"
        ],
        "pos_line": [
            "+mask = inputs.new_ones(batch_size, timesteps).bool()"
        ],
        "core_change": "-mask = inputs.new_ones(batch_size, timesteps) +mask = inputs.new_ones(batch_size, timesteps).bool()",
        "core_API": "size"
    },
    {
        "commit_hash": "7a9a08c5d3ca4699fb439f691c40e1320b37507a",
        "index": "e15f7bb85..43eb65ce2 100644",
        "commit_message": "Drop torch 1.6 testing (#10390)\n\n* Drop torch 1.6 support\n\n* Drop 1.6 support\n\n* Update CHANGELOG\n\n* Fixes\n\n* Split change\n\n* Undo change\n\n* 1.7 -> 1.7.1\n\nhttps://github.com/pytorch/pytorch/issues/47354\n\n* Force trigger nightly\n\n* Update .github/workflows/events-nightly.yml\n\nCo-authored-by: Aki Nitta <nitta@akihironitta.com>\n\n* Revert 1.7.1 change - try wildcard\n\n* Update adjust versions and test it\n\n* Undo test changes\n\n* Revert \"Undo test changes\"\n\nThis reverts commit 3a6acadd115e86f02d83a788f1978372ab6764f3.\n\n* Update CHANGELOG.md\n\nCo-authored-by: Aki Nitta <nitta@akihironitta.com>\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class AcceleratorConnector:",
            "self.deterministic = deterministic",
            "if _TORCH_GREATER_EQUAL_1_8:",
            "torch.use_deterministic_algorithms(deterministic)",
            "-        elif _TORCH_GREATER_EQUAL_1_7:",
            "+        else:",
            "torch.set_deterministic(deterministic)",
            "-        else:  # the minimum version Lightning supports is PyTorch 1.6",
            "-            torch._set_deterministic(deterministic)",
            "if deterministic:",
            "# fixing non-deterministic part of horovod",
            "# https://github.com/PyTorchLightning/pytorch-lightning/pull/1572/files#r420279383"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=_TORCH_GREATER_EQUAL_1_7), value='else')",
            "Delete(target_node=ASTNode(type=identifier, text=elif))",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=identifier, text=else))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=_set_deterministic))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=deterministic))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=type))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 17,
        "number": 8591,
        "neg_line": [
            "-elif _TORCH_GREATER_EQUAL_1_7:",
            "-else:  # the minimum version Lightning supports is PyTorch 1.6",
            "-torch._set_deterministic(deterministic)"
        ],
        "pos_line": [
            "+else:"
        ],
        "core_change": "-elif _TORCH_GREATER_EQUAL_1_7: +else: -else:  # the minimum version Lightning supports is PyTorch 1.6 -torch._set_deterministic(deterministic)",
        "core_API": "use_deterministic_algorithms"
    },
    {
        "commit_hash": "c4ad814a03e1d3703b7fcdde389c4e74576a5291",
        "index": "69965a54..f1e8de67 100644",
        "commit_message": "fixed linting and tests\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "for i in range(split.size(0) - 1):",
            "model.conv1.reset_parameters()",
            "model.conv2.reset_parameters()",
            "model.fc1.reset_parameters()",
            "-        times = []",
            "for epoch in range(1, 301):",
            "train(epoch)",
            "-        times = torch.FloatTensor(times)",
            "acc = test(epoch, test_loader, ' Test Accuracy')",
            "accs_single.append(acc)",
            "accs.append(accs_single)"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=times))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=list))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=identifier, text=times))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=FloatTensor))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=times))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 0,
        "minus_line": 2,
        "AST_diff_line": 20,
        "number": 8598,
        "neg_line": [
            "-times = []",
            "-times = torch.FloatTensor(times)"
        ],
        "pos_line": [],
        "core_change": "-times = [] -times = torch.FloatTensor(times)",
        "core_API": "size"
    },
    {
        "commit_hash": "6ba19a977509e911daaea2367cd0aaa62936b37e",
        "index": "bd8cce42..d3e1cc36 100755",
        "commit_message": "fix ctc_loss API (#215)\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Model(ModelDesc):",
            "W_init=tf.truncated_normal_initializer(stddev=0.01))",
            "logits = tf.reshape(logits, (BATCH, -1, NR_CLASS))",
            "",
            "-        loss = tf.nn.ctc_loss(logits, label, seqlen, time_major=False)",
            "+        loss = tf.nn.ctc_loss(label, logits, seqlen, time_major=False)",
            "",
            "self.cost = tf.reduce_mean(loss, name='cost')"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=logits), value='label')",
            "Update(target_node=ASTNode(type=identifier, text=label), value='logits')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 8602,
        "neg_line": [
            "-loss = tf.nn.ctc_loss(logits, label, seqlen, time_major=False)"
        ],
        "pos_line": [
            "+loss = tf.nn.ctc_loss(label, logits, seqlen, time_major=False)"
        ],
        "core_change": "-loss = tf.nn.ctc_loss(logits, label, seqlen, time_major=False) +loss = tf.nn.ctc_loss(label, logits, seqlen, time_major=False)",
        "core_API": "truncated_normal_initializer"
    },
    {
        "commit_hash": "cd51772bbe4eec0addd4e149b2f47f999020e7a0",
        "index": "6830e54a..3df74f07 100644",
        "commit_message": "remove numpy import (#1116)\n\n* remove numpy import\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* undo and add setup develop in gh actions\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* remove create_checkerboard\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestGradient:",
            "None, None, :, :",
            "]",
            "assert_allclose(",
            "-            gradient(tensor, torch.ones_like(structural_element), structuring_element=structural_element), expected,",
            "-            atol=1e-3, rtol=1e-3",
            "+            gradient(tensor, torch.ones_like(structural_element), structuring_element=structural_element),",
            "+            expected,",
            "+            atol=1e-3,",
            "+            rtol=1e-3,",
            ")",
            "",
            "def test_exception(self, device, dtype):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=8, insert_id=423225)"
        ],
        "plus_line": 4,
        "minus_line": 2,
        "AST_diff_line": 1,
        "number": 8604,
        "neg_line": [
            "-gradient(tensor, torch.ones_like(structural_element), structuring_element=structural_element), expected,",
            "-atol=1e-3, rtol=1e-3"
        ],
        "pos_line": [
            "+gradient(tensor, torch.ones_like(structural_element), structuring_element=structural_element),",
            "+expected,",
            "+atol=1e-3,",
            "+rtol=1e-3,"
        ],
        "core_change": "-gradient(tensor, torch.ones_like(structural_element), structuring_element=structural_element), expected, -atol=1e-3, rtol=1e-3 +gradient(tensor, torch.ones_like(structural_element), structuring_element=structural_element), +expected, +atol=1e-3, +rtol=1e-3,",
        "core_API": "ones_like"
    },
    {
        "commit_hash": "2989d91d08332a58de9d933faf2d4d36af7c0a9a",
        "index": "95088a90..57c22bda 100644",
        "commit_message": "Fix issue for \"AttributeError: module 'tensorflow._api.v2.image' has no attribute 'resize_with_crop_or_pad'\" (#1049)\n\nThe API 'resize_with_crop_or_pad()' seems only for TF 1.0.\nFor TF 2.0.0-alpha0, the API should be resize_with_pad(). Modify Line 125, then this example can work fine.\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def _map_fn_train(img, target):",
            "",
            "def _map_fn_test(img, target):",
            "# 1. Crop the central [height, width] of the image.",
            "-    img = tf.image.resize_with_crop_or_pad(img, 24, 24)",
            "+    img = tf.image.resize_with_pad(img, 24, 24)",
            "# 2. Subtract off the mean and divide by the variance of the pixels.",
            "img = tf.image.per_image_standardization(img)",
            "img = tf.reshape(img, (24, 24, 3))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=resize_with_crop_or_pad), value='resize_with_pad')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 8608,
        "neg_line": [
            "-img = tf.image.resize_with_crop_or_pad(img, 24, 24)"
        ],
        "pos_line": [
            "+img = tf.image.resize_with_pad(img, 24, 24)"
        ],
        "core_change": "-img = tf.image.resize_with_crop_or_pad(img, 24, 24) +img = tf.image.resize_with_pad(img, 24, 24)",
        "core_API": "resize_with_crop_or_pad"
    },
    {
        "commit_hash": "ade4956483b34a295a0555886ee853f12998f8ff",
        "index": "0401e5c9..16127a02 100644",
        "commit_message": "breaking changes in independent act mode, other fixes\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Dropout(Layer):",
            "pass_tensors=dropout",
            ")",
            "",
            "-        skip_dropout = tf.math.logical_not(x=Module.retrieve_tensor(name='optimization'))",
            "+        skip_dropout = tf.math.logical_not(x=Module.retrieve_tensor(name='deterministic'))",
            "zero = tf.constant(value=0.0, dtype=util.tf_dtype(dtype='float'))",
            "skip_dropout = tf.math.logical_or(x=skip_dropout, y=tf.math.equal(x=rate, y=zero))",
            "return self.cond(pred=skip_dropout, true_fn=no_dropout, false_fn=apply_dropout)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text='optimization'), value=\"'deterministic'\")"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 8609,
        "neg_line": [
            "-skip_dropout = tf.math.logical_not(x=Module.retrieve_tensor(name='optimization'))"
        ],
        "pos_line": [
            "+skip_dropout = tf.math.logical_not(x=Module.retrieve_tensor(name='deterministic'))"
        ],
        "core_change": "-skip_dropout = tf.math.logical_not(x=Module.retrieve_tensor(name='optimization')) +skip_dropout = tf.math.logical_not(x=Module.retrieve_tensor(name='deterministic'))",
        "core_API": "logical_not"
    },
    {
        "commit_hash": "f2399a39aa272e0c380165d7c1133ba0583f4c30",
        "index": "0fc5d080..460c5e22 100644",
        "commit_message": "samplified-example-code (#385)\n\n* Update tutorial_mlp_dropout2.py\n\n* Update tutorial_mlp_dropout1.py\n\n* Update tutorial_mnist_simple.py\n\n* Update tutorial_mnist_float16.py\n\n* fix format\n\n* fix format\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "sess = tf.InteractiveSession()",
            "batch_size = 128",
            "",
            "x = tf.placeholder(LayersConfig.tf_dtype, shape=[batch_size, 28, 28, 1])",
            "-y_ = tf.placeholder(",
            "-    tf.int64, shape=[",
            "-        batch_size,",
            "-    ])",
            "+y_ = tf.placeholder(tf.int64, shape=[batch_size])",
            "",
            "",
            "def model(x, is_train=True, reuse=False):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 0,
        "minus_line": 3,
        "AST_diff_line": 1,
        "number": 8617,
        "neg_line": [
            "-y_ = tf.placeholder(",
            "-tf.int64, shape=[",
            "-batch_size,",
            "-])"
        ],
        "pos_line": [
            "+y_ = tf.placeholder(tf.int64, shape=[batch_size])"
        ],
        "core_change": "-y_ = tf.placeholder( -tf.int64, shape=[ -batch_size, -]) +y_ = tf.placeholder(tf.int64, shape=[batch_size])",
        "core_API": "InteractiveSession"
    },
    {
        "commit_hash": "2e0b961a0e3233eba39f9ea473cb4f56dcdf3bd8",
        "index": "4a441220..e7e02d82 100644",
        "commit_message": "fix import_user_module (#3144)\n\nSummary:\n## What does this PR do?\nAvoid throwing ValueError when attempting to load a user defined module from common.user_dir that has the same module name and same module path as some loaded module. This occurs when a job is preempted and restarts using submitit_slurm\n\nX-link: https://github.com/fairinternal/fairseq-py/pull/3144\n\nReviewed By: Abdel-rahmanMohamed\n\nDifferential Revision: D34521450\n\nPulled By: wnhsu\n\nfbshipit-source-id: eed00d4238a66dc524eee400a55ad2c011e1543c\n\n",
        "file": "fairseq.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SequenceGenerator(nn.Module):",
            "cum_unfin.append(prev)",
            "cum_fin_tensor = torch.tensor(cum_unfin, dtype=torch.int).to(bbsz_idx)",
            "",
            "-        unfin_idx = torch.div(bbsz_idx, beam_size, rounding_mode='trunc')",
            "+        unfin_idx = torch.div(bbsz_idx, beam_size, rounding_mode=\"trunc\")",
            "sent = unfin_idx + torch.index_select(cum_fin_tensor, 0, unfin_idx)",
            "",
            "# Create a set of \"{sent}{unfin_idx}\", where"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text='trunc'), value='\"trunc\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 8618,
        "neg_line": [
            "-unfin_idx = torch.div(bbsz_idx, beam_size, rounding_mode='trunc')"
        ],
        "pos_line": [
            "+unfin_idx = torch.div(bbsz_idx, beam_size, rounding_mode=\"trunc\")"
        ],
        "core_change": "-unfin_idx = torch.div(bbsz_idx, beam_size, rounding_mode='trunc') +unfin_idx = torch.div(bbsz_idx, beam_size, rounding_mode=\"trunc\")",
        "core_API": "append"
    },
    {
        "commit_hash": "e6d057c226de70fc9ddec5d8e0e241740d276580",
        "index": "2f1be117..ddc64ebb 100644",
        "commit_message": "fix loading jit and onnx embeddings\n\n",
        "file": "flair.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TransformerEmbeddings(TransformerBaseEmbeddings):",
            "if self.token_embedding:",
            "assert word_ids is not None",
            "assert token_lengths is not None",
            "-            all_token_embeddings = torch.zeros(",
            "-                word_ids.shape[0], token_lengths.max(), self.embedding_length_internal, device=flair.device  # type: ignore",
            "+            all_token_embeddings = torch.zeros(  # type: ignore",
            "+                word_ids.shape[0], token_lengths.max(), self.embedding_length_internal, device=flair.device",
            ")",
            "true_tensor = torch.ones_like(word_ids[:, :1], dtype=torch.bool)",
            "if self.subtoken_pooling == \"first\":"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 8621,
        "neg_line": [
            "-all_token_embeddings = torch.zeros(",
            "-word_ids.shape[0], token_lengths.max(), self.embedding_length_internal, device=flair.device  # type: ignore"
        ],
        "pos_line": [
            "+all_token_embeddings = torch.zeros(  # type: ignore",
            "+word_ids.shape[0], token_lengths.max(), self.embedding_length_internal, device=flair.device"
        ],
        "core_change": "-all_token_embeddings = torch.zeros( -word_ids.shape[0], token_lengths.max(), self.embedding_length_internal, device=flair.device  # type: ignore +all_token_embeddings = torch.zeros(  # type: ignore +word_ids.shape[0], token_lengths.max(), self.embedding_length_internal, device=flair.device",
        "core_API": "zeros"
    },
    {
        "commit_hash": "6a0d33d1cf6d5d71080b19c549c1fe5101a70ace",
        "index": "0b5b4c82..b9117f37 100644",
        "commit_message": "[MaskRCNN] bugfix on data loading\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def crop_and_resize(image, boxes, box_ind, crop_size, pad_border=True):",
            "boxes = transform_fpcoor_for_tf(boxes, image_shape, [crop_size, crop_size])",
            "image = tf.transpose(image, [0, 2, 3, 1])   # 1hwc",
            "ret = tf.image.crop_and_resize(",
            "-        image, boxes, box_ind,",
            "+        image, boxes, tf.to_int32(box_ind),",
            "crop_size=[crop_size, crop_size])",
            "ret = tf.transpose(ret, [0, 3, 1, 2])   # ncss",
            "return ret"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=5, insert_id=2283633)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2283634)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2283635)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2283636)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2283637)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to_int32'), position=2, insert_id=2283638)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2283639)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=box_ind), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2283640)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 8622,
        "neg_line": [
            "-image, boxes, box_ind,"
        ],
        "pos_line": [
            "+image, boxes, tf.to_int32(box_ind),"
        ],
        "core_change": "-image, boxes, box_ind, +image, boxes, tf.to_int32(box_ind),",
        "core_API": "transpose"
    },
    {
        "commit_hash": "92bf5cce4248a6c9bd4955f712bf37b95ae450c5",
        "index": "3c38567180..9857b58d99 100644",
        "commit_message": "Fix linting issues\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "var.support_native_out = True",
            "# ------#",
            "",
            "",
            "-def einsum(equation: str, *operands: torch.Tensor, out: Optional[torch.Tensor] = None) -> torch.Tensor:",
            "+def einsum(",
            "+    equation: str, *operands: torch.Tensor, out: Optional[torch.Tensor] = None",
            "+) -> torch.Tensor:",
            "return torch.einsum(equation, *operands)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 17,
        "number": 8623,
        "neg_line": [
            "-def einsum(equation: str, *operands: torch.Tensor, out: Optional[torch.Tensor] = None) -> torch.Tensor:"
        ],
        "pos_line": [
            "+def einsum(",
            "+equation: str, *operands: torch.Tensor, out: Optional[torch.Tensor] = None",
            "+) -> torch.Tensor:"
        ],
        "core_change": "-def einsum(equation: str, *operands: torch.Tensor, out: Optional[torch.Tensor] = None) -> torch.Tensor: +def einsum( +equation: str, *operands: torch.Tensor, out: Optional[torch.Tensor] = None +) -> torch.Tensor:",
        "core_API": "einsum"
    },
    {
        "commit_hash": "8e76796fd0535d8fbc9934e7f6604a1abcfb119e",
        "index": "ab184a2363..19b3d3472b 100644",
        "commit_message": "ci: Redo `format.sh --all` script & backfill lint fixes (#9956)\n\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def from_importance_weights(log_rhos,",
            "if clip_pg_rho_threshold is not None:",
            "clip_pg_rho_threshold.shape.assert_has_rank(0)",
            "",
            "-    with tf1.name_scope(name, values=[",
            "-        log_rhos, discounts, rewards, values, bootstrap_value",
            "-    ]):",
            "+    with tf1.name_scope(",
            "+            name,",
            "+            values=[log_rhos, discounts, rewards, values, bootstrap_value]):",
            "rhos = tf.math.exp(log_rhos)",
            "if clip_rho_threshold is not None:",
            "clipped_rhos = tf.minimum(",
            "clip_rho_threshold, rhos, name=\"clipped_rhos\")",
            "",
            "-            tf1.summary.histogram(",
            "-                    \"clipped_rhos_1000\", tf.minimum(1000.0, rhos))",
            "+            tf1.summary.histogram(\"clipped_rhos_1000\", tf.minimum(",
            "+                1000.0, rhos))",
            "tf1.summary.scalar(",
            "\"num_of_clipped_rhos\",",
            "tf.reduce_sum("
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 5,
        "minus_line": 5,
        "AST_diff_line": 17,
        "number": 8630,
        "neg_line": [
            "-with tf1.name_scope(name, values=[",
            "-log_rhos, discounts, rewards, values, bootstrap_value",
            "-]):",
            "-tf1.summary.histogram(",
            "-\"clipped_rhos_1000\", tf.minimum(1000.0, rhos))"
        ],
        "pos_line": [
            "+with tf1.name_scope(",
            "+name,",
            "+values=[log_rhos, discounts, rewards, values, bootstrap_value]):",
            "+tf1.summary.histogram(\"clipped_rhos_1000\", tf.minimum(",
            "+1000.0, rhos))"
        ],
        "core_change": "-with tf1.name_scope(name, values=[ -log_rhos, discounts, rewards, values, bootstrap_value -]): +with tf1.name_scope( +name, +values=[log_rhos, discounts, rewards, values, bootstrap_value]): -tf1.summary.histogram( -\"clipped_rhos_1000\", tf.minimum(1000.0, rhos)) +tf1.summary.histogram(\"clipped_rhos_1000\", tf.minimum( +1000.0, rhos))",
        "core_API": "assert_has_rank"
    },
    {
        "commit_hash": "51572b6d19e316766bb895a10c1c5450af0f5d2f",
        "index": "806db93b..382360a7 100644",
        "commit_message": "[BUG] remove float casting in image_to_tensor (#497)\n\n* remove float casting when onverting to tensor\n\n* test dtype is persistent\n\n* fixl linter issues\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def image_to_tensor(image: Union[np.ndarray, Image.Image], keepdim: bool = True)",
            "\"Input size must be a two, three or four dimensional array\")",
            "",
            "input_shape = image.shape",
            "-    tensor: torch.Tensor = torch.from_numpy(image).to(torch.float)",
            "+    tensor: torch.Tensor = torch.from_numpy(image)",
            "",
            "if len(input_shape) == 2:",
            "# (H, W) -> (1, H, W)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=to))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=float))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 8634,
        "neg_line": [
            "-tensor: torch.Tensor = torch.from_numpy(image).to(torch.float)"
        ],
        "pos_line": [
            "+tensor: torch.Tensor = torch.from_numpy(image)"
        ],
        "core_change": "-tensor: torch.Tensor = torch.from_numpy(image).to(torch.float) +tensor: torch.Tensor = torch.from_numpy(image)",
        "core_API": "from_numpy"
    },
    {
        "commit_hash": "abe795e2858b2d785a1ebce0ba66661f357330da",
        "index": "ba74a13c8..be02a80e3 100644",
        "commit_message": "Fix _module_available to detect horovod.torch properly (#12377)\n\n\nCo-authored-by: Adrian Wälchli <aedu.waelchli@gmail.com>\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\nCo-authored-by: Jirka <jirka.borovec@seznam.cz>\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class AcceleratorConnector:",
            "hvd.init()",
            "if isinstance(self.accelerator, GPUAccelerator):",
            "# Horovod assigns one local GPU per process",
            "-            self._parallel_devices = list(range(hvd.local_size()))",
            "+            self._parallel_devices = [torch.device(f\"cuda:{i}\") for i in range(hvd.local_size())]",
            "else:",
            "self._parallel_devices = [torch.device(\"cpu\")] * hvd.local_size()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('list_comprehension', None), position=2, insert_id=510198)",
            "Insert(target_node=IN(type=list_comprehension), node=('[', '['), position=0, insert_id=510199)",
            "Insert(target_node=IN(type=list_comprehension), node=('call', None), position=1, insert_id=510200)",
            "Insert(target_node=IN(type=list_comprehension), node=('for_in_clause', None), position=2, insert_id=510201)",
            "Insert(target_node=IN(type=list_comprehension), node=(']', ']'), position=3, insert_id=510202)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=510203)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=510204)",
            "Insert(target_node=IN(type=for_in_clause), node=('for', 'for'), position=0, insert_id=510205)",
            "Insert(target_node=IN(type=for_in_clause), node=('identifier', 'i'), position=1, insert_id=510206)",
            "Insert(target_node=IN(type=for_in_clause), node=('in', 'in'), position=2, insert_id=510207)",
            "Move(target_node=IN(type=for_in_clause), node=ASTNode(type=call), position=3)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=510208)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=510209)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=510210)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=(, text=(), position=0)",
            "Insert(target_node=IN(type=argument_list), node=('string', 'f\"cuda:{i}\"'), position=1, insert_id=510211)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=510212)",
            "Delete(target_node=ASTNode(type=identifier, text=list))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 21,
        "number": 8641,
        "neg_line": [
            "-self._parallel_devices = list(range(hvd.local_size()))"
        ],
        "pos_line": [
            "+self._parallel_devices = [torch.device(f\"cuda:{i}\") for i in range(hvd.local_size())]"
        ],
        "core_change": "-self._parallel_devices = list(range(hvd.local_size())) +self._parallel_devices = [torch.device(f\"cuda:{i}\") for i in range(hvd.local_size())]",
        "core_API": "init"
    },
    {
        "commit_hash": "e158e379080a89db591338805ffe08ed886ad5dd",
        "index": "fd789b3e1..d20aa0806 100644",
        "commit_message": "Allow not specifying feature cols other than `predictions`/`references` in `Metric.compute` (#3824)\n\n* Update SARI metric\n\n* Fix Metric.compute\n\n* Style fix\n\n* Add test\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class Sari(datasets.Metric):",
            "inputs_description=_KWARGS_DESCRIPTION,",
            "features=datasets.Features(",
            "{",
            "+                    \"sources\": datasets.Value(\"string\", id=\"sequence\"),",
            "\"predictions\": datasets.Value(\"string\", id=\"sequence\"),",
            "\"references\": datasets.Sequence(datasets.Value(\"string\", id=\"sequence\"), id=\"references\"),",
            "}"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=dictionary), node=('pair', None), position=1, insert_id=1780101)",
            "Insert(target_node=ASTNode(type=dictionary), node=(',', ','), position=2, insert_id=1780102)",
            "Insert(target_node=IN(type=pair), node=('string', '\"sources\"'), position=0, insert_id=1780103)",
            "Insert(target_node=IN(type=pair), node=(':', ':'), position=1, insert_id=1780104)",
            "Insert(target_node=IN(type=pair), node=('call', None), position=2, insert_id=1780105)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1780106)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1780107)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'datasets'), position=0, insert_id=1780108)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1780109)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Value'), position=2, insert_id=1780110)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1780111)",
            "Insert(target_node=IN(type=argument_list), node=('string', '\"string\"'), position=1, insert_id=1780112)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=1780113)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1780114)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=1780115)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'id'), position=0, insert_id=1780116)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1780117)",
            "Insert(target_node=IN(type=keyword_argument), node=('string', '\"sequence\"'), position=2, insert_id=1780118)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 18,
        "number": 8642,
        "neg_line": [],
        "pos_line": [
            "+\"sources\": datasets.Value(\"string\", id=\"sequence\"),"
        ],
        "core_change": "+\"sources\": datasets.Value(\"string\", id=\"sequence\"),",
        "core_API": "Features"
    },
    {
        "commit_hash": "0cb2c602db87a1e21fa22c9afe2008584659336f",
        "index": "38313dac5..9a7442b46 100644",
        "commit_message": "[rllib] Fixes typo in RolloutWorker.__init__ (#17583)\n\nFixes the typo in RolloutWorker.__init__, closes #17582\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class RolloutWorker(ParallelIteratorWorker):",
            "# Not all Operations support this.",
            "torch.use_deterministic_algorithms(True)",
            "else:",
            "-                        torch.set_determinstic(True)",
            "+                        torch.set_deterministic(True)",
            "# This is only for Convolution no problem.",
            "torch.backends.cudnn.deterministic = True",
            "# Tf2.x."
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=set_determinstic), value='set_deterministic')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 8647,
        "neg_line": [
            "-torch.set_determinstic(True)"
        ],
        "pos_line": [
            "+torch.set_deterministic(True)"
        ],
        "core_change": "-torch.set_determinstic(True) +torch.set_deterministic(True)",
        "core_API": "use_deterministic_algorithms"
    },
    {
        "commit_hash": "9c97baf79b92e93e8a83fbaca46268fe46f12433",
        "index": "cf1e1ad7..876de393 100644",
        "commit_message": "refactor of the optimization interface (#212)\n\n* initial commit\n\n* optim.py\n\n* basic thing working\n\n* got some tests to pass\n\n* fixedguidemodel test happy\n\n* more comments, default implementation for loss_and_grads, ...\n\n* change import\n\n* got a lot more tests to pass; a tiny bit inference test refactoring\n\n* flake8\n\n* fix more tests\n\n* made conjugate model tests faster; made example test pass; all tests pass now maybe?\n\n* made conjugate model tests faster; made example test pass; all tests pass now maybe?\n\n* xfail test_examples\n\n* xfail test_examples flake8\n\n* made tests/poutine/test_mapdata.py happy\n\n* fix test lambda test\n\n* more refactoring...\n\n* more refactoring...\n\n* progress on further rerefactoring\n\n* always more refactoring\n\n* linting\n\n* done with initial re-refactor?\n\n* fix bug\n\n* fl8\n\n* address tests\n\n*  make examples happy\n\n* fix tests. fixed everything?\n\n* final clean-up?\n\n* assert fix\n\n* sets not hashable error\n\n* exclusively doc string changes\n\n* batch rearrange\n\n* fix fake conflict\n\n* fl8\n\n* doc rebuild workaround\n\n* fix random module test (tags)\n\n* wrap loss\n\n* rebalance tests\n\n* whoops\n\n* fix more merge conflicts; fix test imports\n\n* fl8\n\n*  delete old tracegraph_kl_qp.py\n\n* travis workaround\n\n* better rng seeds/inits?\n\n* fix travis yml\n\n* these seeds/inits should be good\n\n* fix data\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "optim",
        "change": [
            "def main():",
            "",
            "# get batch",
            "batch_data = mnist_data[batch_start:batch_end]",
            "-            epoch_loss += kl_optim.step(batch_data)",
            "+            epoch_loss += svi.step(batch_data)",
            "",
            "sample = model_sample()",
            "vis.image(batch_data[0].contiguous().view(28, 28).data.numpy())"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=kl_optim), value='svi')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 8648,
        "neg_line": [
            "-epoch_loss += kl_optim.step(batch_data)"
        ],
        "pos_line": [
            "+epoch_loss += svi.step(batch_data)"
        ],
        "core_change": "-epoch_loss += kl_optim.step(batch_data) +epoch_loss += svi.step(batch_data)",
        "core_API": "step"
    },
    {
        "commit_hash": "2c744ee53aa50766499c3036b9f8489e60361352",
        "index": "1615884..a53dd7e 100644",
        "commit_message": "Bug(?) fix\n\n",
        "file": "apex.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DistributedFusedAdam(torch.optim.Optimizer):",
            "",
            "def _pipeline_block_step(self, block_id):",
            "if self._new_params is None:",
            "-            self._new_params = torch.zeros_like(self._flat_grads,dtype=uint8 if self._e5m2_allgather else self._flat_grads.dtype)",
            "+            self._new_params = torch.zeros_like(self._flat_grads,dtype=torch.uint8 if self._e5m2_allgather else self._flat_grads.dtype)",
            "",
            "start = block_id * self._block_size",
            "end = start + self._block_size"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=conditional_expression), node=('attribute', None), position=0, insert_id=52436)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=52437)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=52438)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=uint8), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 8651,
        "neg_line": [
            "-self._new_params = torch.zeros_like(self._flat_grads,dtype=uint8 if self._e5m2_allgather else self._flat_grads.dtype)"
        ],
        "pos_line": [
            "+self._new_params = torch.zeros_like(self._flat_grads,dtype=torch.uint8 if self._e5m2_allgather else self._flat_grads.dtype)"
        ],
        "core_change": "-self._new_params = torch.zeros_like(self._flat_grads,dtype=uint8 if self._e5m2_allgather else self._flat_grads.dtype) +self._new_params = torch.zeros_like(self._flat_grads,dtype=torch.uint8 if self._e5m2_allgather else self._flat_grads.dtype)",
        "core_API": "zeros_like"
    },
    {
        "commit_hash": "c993f767a8a5ce041921c1825ac407d9be847bb9",
        "index": "32ba2e27..218d6a2d 100644",
        "commit_message": "Add SPOS docs and improve NAS doc structure (#1907)\n\n* darts mutator docs\n\n* fix docs\n\n* update\n\n* add docs for SPOS\n\n* index SPOS\n\n* restore workers\n\n",
        "file": "nni.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Trainer(BaseTrainer):",
            "workers : int",
            "Number of workers used in data preprocessing.",
            "device : torch.device",
            "-            Device object. Either `torch.device(\"cuda\")` or torch.device(\"cpu\")`. When `None`, trainer will",
            "+            Device object. Either ``torch.device(\"cuda\")`` or ``torch.device(\"cpu\")``. When ``None``, trainer will",
            "automatic detects GPU and selects GPU first.",
            "log_frequency : int",
            "Number of mini-batches to log metrics."
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('ERROR', None), position=5, insert_id=668094)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=identifier, text=Device), position=0)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=attribute), position=1)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=call), position=2)",
            "Insert(target_node=IN(type=ERROR), node=('\"', '`'), position=3, insert_id=668095)",
            "Insert(target_node=ASTNode(type=expression_statement), node=('concatenated_string', None), position=0, insert_id=668096)",
            "Insert(target_node=IN(type=concatenated_string), node=('string', '` or `'), position=0, insert_id=668097)",
            "Insert(target_node=IN(type=concatenated_string), node=('string', '`torch.device(\"cpu\")`'), position=1, insert_id=668098)",
            "Move(target_node=IN(type=concatenated_string), node=ASTNode(type=string, text=`. When `), position=2)",
            "Insert(target_node=IN(type=concatenated_string), node=('string', '`None`'), position=3, insert_id=668099)",
            "Insert(target_node=ASTNode(type=attribute), node=('string', '``'), position=0, insert_id=668100)",
            "Insert(target_node=ASTNode(type=attribute), node=('ERROR', None), position=1, insert_id=668101)",
            "Update(target_node=ASTNode(type=string, text=\"cpu\"), value='\"cuda\"')",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=identifier, text=torch), position=0)",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=string, text=`torch.device(\"cuda\")`))",
            "Delete(target_node=ASTNode(type=or, text=or))",
            "Delete(target_node=ASTNode(type=boolean_operator))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=identifier, text=None))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 20,
        "number": 8652,
        "neg_line": [
            "-Device object. Either `torch.device(\"cuda\")` or torch.device(\"cpu\")`. When `None`, trainer will"
        ],
        "pos_line": [
            "+Device object. Either ``torch.device(\"cuda\")`` or ``torch.device(\"cpu\")``. When ``None``, trainer will"
        ],
        "core_change": "-Device object. Either `torch.device(\"cuda\")` or torch.device(\"cpu\")`. When `None`, trainer will +Device object. Either ``torch.device(\"cuda\")`` or ``torch.device(\"cpu\")``. When ``None``, trainer will",
        "core_API": "device"
    },
    {
        "commit_hash": "a2b898b713c8a6db80cab7feef6c9ecae9b417f7",
        "index": "2002ada..37ff095 100644",
        "commit_message": "Fix imports\n\n",
        "file": "gpt-neo.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def attn(x, scope, n_state, *, past, params, block_offset=0, train=False):",
            ")",
            "else:",
            "# HOWEVER, `attention` DOES NOT implement masking so we need to pass in `bias` on our own!",
            "-            a = mtf.transformer.attention.attention(",
            "+            a = mtf_transformer.attention.attention(",
            "q, k, v,",
            "memory_length_dim=dim_seq,",
            "key_dim=dim_embd,"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=mtf), value='mtf_transformer')",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=mtf), position=0)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=transformer))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 8656,
        "neg_line": [
            "-a = mtf.transformer.attention.attention("
        ],
        "pos_line": [
            "+a = mtf_transformer.attention.attention("
        ],
        "core_change": "-a = mtf.transformer.attention.attention( +a = mtf_transformer.attention.attention(",
        "core_API": "attention"
    },
    {
        "commit_hash": "f38aaa5be05104ddfc3452249eeb521f139ded99",
        "index": "42a73dd7..88a0ea42 100644",
        "commit_message": "improved global tensor handling, various other fixes\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Dropout(Layer):",
            ")",
            "",
            "update = Module.retrieve_tensor(name='update')",
            "-        return tf.cond(pred=update, true_fn=true_fn, false_fn=(lambda: x))",
            "+        return self.cond(pred=update, true_fn=true_fn, false_fn=(lambda: x))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tf), value='self')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 8657,
        "neg_line": [
            "-return tf.cond(pred=update, true_fn=true_fn, false_fn=(lambda: x))"
        ],
        "pos_line": [
            "+return self.cond(pred=update, true_fn=true_fn, false_fn=(lambda: x))"
        ],
        "core_change": "-return tf.cond(pred=update, true_fn=true_fn, false_fn=(lambda: x)) +return self.cond(pred=update, true_fn=true_fn, false_fn=(lambda: x))",
        "core_API": "retrieve_tensor"
    },
    {
        "commit_hash": "72d0d758da0c26e182a9f5ba0be1a838eeaadf10",
        "index": "a8ad651e06..3701a94a54 100644",
        "commit_message": "Fixed out argument usage in eye torch (#7002)\n\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def eye(",
            "elif 0 < k < n_cols:",
            "mat = torch.concat(",
            "[",
            "-                torch.zeros([n_rows, k], dtype=dtype, device=device, out=out),",
            "+                torch.zeros([n_rows, k], dtype=dtype, device=device),",
            "i[:, : n_cols - k],",
            "],",
            "1,",
            ")",
            "ret = torch.reshape(mat, reshape_dims).repeat(tile_dims)",
            "+        if out is not None:",
            "+            return ivy.inplace_update(out, ret)",
            "else:",
            "ret = torch.zeros(",
            "batch_shape + [n_rows, n_cols], dtype=dtype, device=device, out=out",
            ")",
            "-    if out is not None:",
            "-        return ivy.inplace_update(out, ret)",
            "return ret"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=if_statement), node=ASTNode(type=module), position=3)",
            "Insert(target_node=ASTNode(type=if_statement), node=('else_clause', None), position=4, insert_id=298134)",
            "Insert(target_node=IN(type=else_clause), node=('else', 'else'), position=0, insert_id=298135)",
            "Insert(target_node=IN(type=else_clause), node=(':', ':'), position=1, insert_id=298136)",
            "Insert(target_node=IN(type=else_clause), node=('block', ''), position=2, insert_id=298137)",
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=identifier, text=ret), position=0)",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=out))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=out))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=identifier, text=else))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=type))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 14,
        "number": 8659,
        "neg_line": [
            "-torch.zeros([n_rows, k], dtype=dtype, device=device, out=out),",
            "-if out is not None:",
            "-return ivy.inplace_update(out, ret)"
        ],
        "pos_line": [
            "+torch.zeros([n_rows, k], dtype=dtype, device=device),",
            "+if out is not None:",
            "+return ivy.inplace_update(out, ret)"
        ],
        "core_change": "-torch.zeros([n_rows, k], dtype=dtype, device=device, out=out), +torch.zeros([n_rows, k], dtype=dtype, device=device), +if out is not None: +return ivy.inplace_update(out, ret) -if out is not None: -return ivy.inplace_update(out, ret)",
        "core_API": "concat"
    },
    {
        "commit_hash": "2b3e7ab7f87051eaffafa3b860b77e43138bebe2",
        "index": "98286c99..0c1dbbb6 100644",
        "commit_message": "fixed py3 errors\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class GaussianPyramidTests(TestCase):",
            "def test_elbo_reparameterized(self):",
            "for N in [3, 4, 5]:",
            "self.setup_pyramid(N)",
            "-            self.do_elbo_test(True, N * 4000 - 4000)",
            "+            self.do_elbo_test(True, N * 3000 - 3000)",
            "",
            "# def test_elbo_nonreparameterized(self): XXX to add",
            "#     self.do_elbo_test(False, 5000)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=integer, text=4000), value='3000')",
            "Update(target_node=ASTNode(type=integer, text=4000), value='3000')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 8660,
        "neg_line": [
            "-self.do_elbo_test(True, N * 4000 - 4000)"
        ],
        "pos_line": [
            "+self.do_elbo_test(True, N * 3000 - 3000)"
        ],
        "core_change": "-self.do_elbo_test(True, N * 4000 - 4000) +self.do_elbo_test(True, N * 3000 - 3000)",
        "core_API": "setup_pyramid"
    },
    {
        "commit_hash": "40b049c701b4b500d9762f39ac0a2d128b585c6b",
        "index": "4505c9fc1..2ef53d8f2 100755",
        "commit_message": "Check copies blackify (#10775)\n\n* Apply black before checking copies\n\n* Fix for class methods\n\n* Deal with lonely brackets\n\n* Remove debug and add forward changes\n\n* Separate copies and fix test\n\n* Add black as a test dependency\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "models",
        "change": [
            "class M2M100Decoder(M2M100PreTrainedModel):",
            "",
            "self.init_weights()",
            "",
            "-    # Copied from transformers.models.mbart.modeling_mbart.MBartDecoder.forward with MBart->M2M100",
            "def forward(",
            "self,",
            "input_ids=None,"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 8673,
        "neg_line": [
            "-# Copied from transformers.models.mbart.modeling_mbart.MBartDecoder.forward with MBart->M2M100"
        ],
        "pos_line": [],
        "core_change": "-# Copied from transformers.models.mbart.modeling_mbart.MBartDecoder.forward with MBart->M2M100",
        "core_API": "init_weights"
    },
    {
        "commit_hash": "11183b4d905d14c6a0164a4d13675b89b1bf4ceb",
        "index": "1568b2b8..af9fbcf2 100644",
        "commit_message": "fix for #6700\n\n",
        "file": "stable-diffusion-webui.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class PersonalizedBase(Dataset):",
            "weight /= weight.mean()",
            "elif use_weight:",
            "#If an image does not have a alpha channel, add a ones weight map anyway so we can stack it later",
            "-                weight = torch.ones([channels] + latent_size)",
            "+                weight = torch.ones(latent_sample.shape)",
            "else:",
            "weight = None"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=1, insert_id=1132704)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'latent_sample'), position=0, insert_id=1132705)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1132706)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'shape'), position=2, insert_id=1132707)",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=identifier, text=channels))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=list))",
            "Delete(target_node=ASTNode(type=+, text=+))",
            "Delete(target_node=ASTNode(type=identifier, text=latent_size))",
            "Delete(target_node=ASTNode(type=binary_operator))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 8675,
        "neg_line": [
            "-weight = torch.ones([channels] + latent_size)"
        ],
        "pos_line": [
            "+weight = torch.ones(latent_sample.shape)"
        ],
        "core_change": "-weight = torch.ones([channels] + latent_size) +weight = torch.ones(latent_sample.shape)",
        "core_API": "mean"
    },
    {
        "commit_hash": "da2f1325014a158be36d62bdd20293ef97e574f4",
        "index": "86f319dc..19b0d338 100644",
        "commit_message": "enable black in the precommit (#1777)\n\n* enable black in the precommit\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add some fixes\n\n* update libface detection url\n\n* added url from kornia checkpoint\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def psnr(input: torch.Tensor, target: torch.Tensor, max_val: float) -> torch.Ten",
            "if input.shape != target.shape:",
            "raise TypeError(f\"Expected tensors of equal shapes, but got {input.shape} and {target.shape}\")",
            "",
            "-    return 10.0 * torch.log10(max_val ** 2 / mse(input, target, reduction='mean'))",
            "+    return 10.0 * torch.log10(max_val**2 / mse(input, target, reduction='mean'))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 8676,
        "neg_line": [
            "-return 10.0 * torch.log10(max_val ** 2 / mse(input, target, reduction='mean'))"
        ],
        "pos_line": [
            "+return 10.0 * torch.log10(max_val**2 / mse(input, target, reduction='mean'))"
        ],
        "core_change": "-return 10.0 * torch.log10(max_val ** 2 / mse(input, target, reduction='mean')) +return 10.0 * torch.log10(max_val**2 / mse(input, target, reduction='mean'))",
        "core_API": "log10"
    },
    {
        "commit_hash": "2322eb8e2f9765cb73f59b324cc46a0e9cfe803f",
        "index": "2bf59d0c3..da71be87d 100644",
        "commit_message": "Update serving signatures and make sure we actually use them (#19034)\n\n* Override save() to use the serving signature as the default\n\n* Replace int32 with int64 in all our serving signatures\n\n* Remember one very important line so as not to break every test at once\n\n* Dtype fix for TFLED\n\n* dtype fix for shift_tokens_right in general\n\n* Dtype fixes in mBART and RAG\n\n* Fix dtypes for test_unpack_inputs\n\n* More dtype fixes\n\n* Yet more mBART + RAG dtype fixes\n\n* Yet more mBART + RAG dtype fixes\n\n* Add a check that the model actually has a serving method\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFFunnelForMultipleChoice(TFFunnelPreTrainedModel, TFMultipleChoiceLoss):",
            "@tf.function(",
            "input_signature=[",
            "{",
            "-                \"input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"input_ids\"),",
            "+                \"input_ids\": tf.TensorSpec((None, None), tf.int64, name=\"input_ids\"),",
            "\"attention_mask\": tf.TensorSpec((None, None), tf.float32, name=\"attention_mask\"),",
            "-                \"token_type_ids\": tf.TensorSpec((None, None), tf.int32, name=\"token_type_ids\"),",
            "+                \"token_type_ids\": tf.TensorSpec((None, None), tf.int64, name=\"token_type_ids\"),",
            "}",
            "]",
            ")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2361301)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2361302)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2361303)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'TensorSpec'), position=2, insert_id=2361304)",
            "Insert(target_node=ASTNode(type=argument_list), node=('tuple', None), position=1, insert_id=2361305)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=tuple), position=1)",
            "Insert(target_node=IN(type=tuple), node=('(', '('), position=0, insert_id=2361306)",
            "Insert(target_node=IN(type=tuple), node=('none', 'None'), position=1, insert_id=2361307)",
            "Insert(target_node=IN(type=tuple), node=(',', ','), position=2, insert_id=2361308)",
            "Insert(target_node=IN(type=tuple), node=('none', 'None'), position=3, insert_id=2361309)",
            "Insert(target_node=IN(type=tuple), node=(')', ')'), position=4, insert_id=2361310)",
            "Update(target_node=ASTNode(type=identifier, text=int32), value='int64')",
            "Update(target_node=ASTNode(type=identifier, text=int32), value='int64')",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=TensorSpec))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=none, text=None))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=none, text=None))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=tuple))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 24,
        "number": 8682,
        "neg_line": [
            "-\"input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"input_ids\"),",
            "-\"token_type_ids\": tf.TensorSpec((None, None), tf.int32, name=\"token_type_ids\"),"
        ],
        "pos_line": [
            "+\"input_ids\": tf.TensorSpec((None, None), tf.int64, name=\"input_ids\"),",
            "+\"token_type_ids\": tf.TensorSpec((None, None), tf.int64, name=\"token_type_ids\"),"
        ],
        "core_change": "-\"input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"input_ids\"), +\"input_ids\": tf.TensorSpec((None, None), tf.int64, name=\"input_ids\"), -\"token_type_ids\": tf.TensorSpec((None, None), tf.int32, name=\"token_type_ids\"), +\"token_type_ids\": tf.TensorSpec((None, None), tf.int64, name=\"token_type_ids\"),",
        "core_API": "function"
    },
    {
        "commit_hash": "17e058b68f2e93c4bf183006b9269d93c7a38fcb",
        "index": "ddc9b530a..ee495ac64 100755",
        "commit_message": "Fixes wrong imports\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def main():",
            "# extract",
            "logging.info('backend = ' + args.backend)",
            "if args.backend == \"pytorch\":",
            "-        from tts.pytorch.tts_pytorch import decode",
            "+        fromespnet.lmpytorch.tts_pytorch import decode",
            "decode(args)",
            "else:",
            "raise NotImplementedError(\"Only pytorch is supported.\")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=if_statement), node=('ERROR', None), position=3, insert_id=178917)",
            "Insert(target_node=ASTNode(type=if_statement), node=('block', None), position=4, insert_id=178918)",
            "Insert(target_node=IN(type=ERROR), node=('attribute', None), position=0, insert_id=178919)",
            "Insert(target_node=IN(type=block), node=('import_statement', None), position=0, insert_id=178920)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=178921)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=tts_pytorch), position=2)",
            "Move(target_node=IN(type=import_statement), node=ASTNode(type=import, text=import), position=0)",
            "Move(target_node=IN(type=import_statement), node=ASTNode(type=dotted_name), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=tts), value='fromespnet')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=tts), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=pytorch), value='lmpytorch')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=pytorch), position=2)",
            "Delete(target_node=ASTNode(type=from, text=from))",
            "Delete(target_node=ASTNode(type=dotted_name))",
            "Delete(target_node=ASTNode(type=import_from_statement))",
            "Delete(target_node=ASTNode(type=block))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 18,
        "number": 8683,
        "neg_line": [
            "-from tts.pytorch.tts_pytorch import decode"
        ],
        "pos_line": [
            "+fromespnet.lmpytorch.tts_pytorch import decode"
        ],
        "core_change": "-from tts.pytorch.tts_pytorch import decode +fromespnet.lmpytorch.tts_pytorch import decode",
        "core_API": "info"
    },
    {
        "commit_hash": "6934f5c97e9fd4e4af07e240b70a88c21b2f5873",
        "index": "89a3979..233905f 100644",
        "commit_message": "quick fix\n\n",
        "file": "tflearn.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "with tf.Graph().as_default():",
            "return x",
            "",
            "net = dnn(X)",
            "-    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(net, Y))",
            "+    loss = tf.reduce_mean(",
            "+        tf.nn.softmax_cross_entropy_with_logits(logits=net, labels=Y))",
            "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)",
            "accuracy = tf.reduce_mean(",
            "tf.cast(tf.equal(tf.argmax(net, 1), tf.argmax(Y, 1)), tf.float32),"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=1, insert_id=2349911)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=4, insert_id=2349912)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'logits'), position=0, insert_id=2349913)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2349914)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=identifier, text=net), position=2)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'labels'), position=0, insert_id=2349915)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2349916)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=identifier, text=Y), position=2)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 8684,
        "neg_line": [
            "-loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(net, Y))"
        ],
        "pos_line": [
            "+loss = tf.reduce_mean(",
            "+tf.nn.softmax_cross_entropy_with_logits(logits=net, labels=Y))"
        ],
        "core_change": "-loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(net, Y)) +loss = tf.reduce_mean( +tf.nn.softmax_cross_entropy_with_logits(logits=net, labels=Y))",
        "core_API": "Graph"
    },
    {
        "commit_hash": "53282b5bd0cf78fae913d1d7e43f94c94620df0c",
        "index": "3cfdee38c..c4ca0be87 100644",
        "commit_message": "Change attention mask dtype to be bool. Fix #1119\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TransfoXLModel(TransfoXLPreTrainedModel):",
            "else:",
            "mask_shift_len = qlen",
            "dec_attn_mask = (torch.triu(all_ones, 1+mlen)",
            "-                    + torch.tril(all_ones, -mask_shift_len)).byte()[:, :, None] # -1",
            "+                    + torch.tril(all_ones, -mask_shift_len)).bool()[:, :, None] # -1",
            "else:",
            "dec_attn_mask = torch.triu(",
            "-                word_emb.new_ones(qlen, klen), diagonal=1+mlen).byte()[:,:,None]",
            "+                word_emb.new_ones(qlen, klen), diagonal=1+mlen).bool()[:,:,None]",
            "",
            "hids = []",
            "attentions = []"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=subscript), node=('slice', None), position=2, insert_id=1247703)",
            "Move(target_node=ASTNode(type=subscript), node=ASTNode(type=slice), position=4)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=1247704)",
            "Insert(target_node=IN(type=slice), node=(':', ':'), position=0, insert_id=1247705)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=argument_list), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=byte), value='bool')",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1247706)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1247707)",
            "Update(target_node=ASTNode(type=identifier, text=byte), value='bool')",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=slice))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 14,
        "number": 8686,
        "neg_line": [
            "-+ torch.tril(all_ones, -mask_shift_len)).byte()[:, :, None] # -1",
            "-word_emb.new_ones(qlen, klen), diagonal=1+mlen).byte()[:,:,None]"
        ],
        "pos_line": [
            "++ torch.tril(all_ones, -mask_shift_len)).bool()[:, :, None] # -1",
            "+word_emb.new_ones(qlen, klen), diagonal=1+mlen).bool()[:,:,None]"
        ],
        "core_change": "-+ torch.tril(all_ones, -mask_shift_len)).byte()[:, :, None] # -1 ++ torch.tril(all_ones, -mask_shift_len)).bool()[:, :, None] # -1 -word_emb.new_ones(qlen, klen), diagonal=1+mlen).byte()[:,:,None] +word_emb.new_ones(qlen, klen), diagonal=1+mlen).bool()[:,:,None]",
        "core_API": "triu"
    },
    {
        "commit_hash": "f03580fb0210240328fef73ae2f4ab48dcd91cea",
        "index": "093e75cf9..bfa03f977 100644",
        "commit_message": "Fix DETR integration test (#12734)\n\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DetrModelIntegrationTests(unittest.TestCase):",
            "expected_slice_masks = torch.tensor(",
            "[[-7.7558, -10.8788, -11.9797], [-11.8881, -16.4329, -17.7451], [-14.7316, -19.7383, -20.3004]]",
            ").to(torch_device)",
            "-        self.assertTrue(torch.allclose(outputs.pred_masks[0, 0, :3, :3], expected_slice_masks, atol=1e-4))",
            "+        self.assertTrue(torch.allclose(outputs.pred_masks[0, 0, :3, :3], expected_slice_masks, atol=1e-3))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=float, text=1e-4), value='1e-3')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 8688,
        "neg_line": [
            "-self.assertTrue(torch.allclose(outputs.pred_masks[0, 0, :3, :3], expected_slice_masks, atol=1e-4))"
        ],
        "pos_line": [
            "+self.assertTrue(torch.allclose(outputs.pred_masks[0, 0, :3, :3], expected_slice_masks, atol=1e-3))"
        ],
        "core_change": "-self.assertTrue(torch.allclose(outputs.pred_masks[0, 0, :3, :3], expected_slice_masks, atol=1e-4)) +self.assertTrue(torch.allclose(outputs.pred_masks[0, 0, :3, :3], expected_slice_masks, atol=1e-3))",
        "core_API": "tensor"
    },
    {
        "commit_hash": "1e1d1821d070d2ed02c64a718bc81dbc6468c46b",
        "index": "98675866b..c28e5cec5 100644",
        "commit_message": "fix best score on wrong device in EarlyStopping callback (#8295)\n\n\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class EarlyStopping(Callback):",
            "f\" {self.monitor} = {current} {self.order_dict[self.mode]} {self.divergence_threshold}.\"",
            "\" Signaling Trainer to stop.\"",
            ")",
            "-        elif self.monitor_op(current - self.min_delta, self.best_score.to(trainer.lightning_module.device)):",
            "+        elif self.monitor_op(current - self.min_delta, self.best_score.to(current.device)):",
            "should_stop = False",
            "reason = self._improvement_message(current)",
            "self.best_score = current"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=trainer), value='current')",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=trainer), position=0)",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Delete(target_node=ASTNode(type=identifier, text=lightning_module))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 8693,
        "neg_line": [
            "-elif self.monitor_op(current - self.min_delta, self.best_score.to(trainer.lightning_module.device)):"
        ],
        "pos_line": [
            "+elif self.monitor_op(current - self.min_delta, self.best_score.to(current.device)):"
        ],
        "core_change": "-elif self.monitor_op(current - self.min_delta, self.best_score.to(trainer.lightning_module.device)): +elif self.monitor_op(current - self.min_delta, self.best_score.to(current.device)):",
        "core_API": "monitor_op"
    },
    {
        "commit_hash": "7513a6a715051684a39884d438073d406ff63f49",
        "index": "b27bfeee9e..84f4f003b7 100644",
        "commit_message": "Fixed failing test for linalg tensordot (#4636)\n\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def tensordot(",
            "out: Optional[Union[tf.Tensor, tf.Variable]] = None,",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "# find type to promote to",
            "-    dtype = tf.experimental.numpy.promote_types(x1.dtype, x2.dtype)",
            "+    dtype = ivy.as_native_dtype(ivy.promote_types(x1.dtype, x2.dtype))",
            "",
            "# type casting to float32 which is acceptable for tf.tensordot",
            "x1, x2 = tf.cast(x1, tf.float32), tf.cast(x2, tf.float32)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=1993102)",
            "Update(target_node=ASTNode(type=identifier, text=tf), value='ivy')",
            "Update(target_node=ASTNode(type=identifier, text=experimental), value='as_native_dtype')",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1993103)",
            "Insert(target_node=IN(type=argument_list), node=('call', None), position=1, insert_id=1993104)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1993105)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1993106)",
            "Move(target_node=IN(type=call), node=ASTNode(type=argument_list), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=numpy), value='ivy')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=numpy), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=promote_types), position=2)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 16,
        "number": 8698,
        "neg_line": [
            "-dtype = tf.experimental.numpy.promote_types(x1.dtype, x2.dtype)"
        ],
        "pos_line": [
            "+dtype = ivy.as_native_dtype(ivy.promote_types(x1.dtype, x2.dtype))"
        ],
        "core_change": "-dtype = tf.experimental.numpy.promote_types(x1.dtype, x2.dtype) +dtype = ivy.as_native_dtype(ivy.promote_types(x1.dtype, x2.dtype))",
        "core_API": "promote_types"
    },
    {
        "commit_hash": "89879e2c76e86c685e44c47a6cdb82f7e645c142",
        "index": "3482dcf1..b351787d 100644",
        "commit_message": "Fix keras docstrings\n\nPiperOrigin-RevId: 424275818\n\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def log_cosh(y_true, y_pred):",
            ">>> x = y_pred - y_true",
            ">>> assert np.allclose(",
            "...     loss.numpy(),",
            "-  ...     np.mean(x + np.log(np.exp(-2. * x) + 1.) - math_ops.log(2.), axis=-1),",
            "+  ...     np.mean(x + np.log(np.exp(-2. * x) + 1.) - tf.math.log(2.), axis=-1),",
            "...     atol=1e-5)",
            "",
            "Args:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=2069386)",
            "Update(target_node=ASTNode(type=identifier, text=math_ops), value='tf')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=math_ops), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2069387)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'math'), position=2, insert_id=2069388)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 8699,
        "neg_line": [
            "-...     np.mean(x + np.log(np.exp(-2. * x) + 1.) - math_ops.log(2.), axis=-1),"
        ],
        "pos_line": [
            "+...     np.mean(x + np.log(np.exp(-2. * x) + 1.) - tf.math.log(2.), axis=-1),"
        ],
        "core_change": "-...     np.mean(x + np.log(np.exp(-2. * x) + 1.) - math_ops.log(2.), axis=-1), +...     np.mean(x + np.log(np.exp(-2. * x) + 1.) - tf.math.log(2.), axis=-1),",
        "core_API": "allclose"
    },
    {
        "commit_hash": "0c923be9eaf79c595791985671b6e1e8c8ee4a68",
        "index": "32c7c5b7d..73b75a956 100644",
        "commit_message": "Fix a bug about variable spelling errors\n\nThe variable subsampling_factors in class CustomConverterMulEnc was  incorrectly  written as subsamping_factors , resulting in inconsistency.\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class CustomConverterMulEnc(object):",
            "ys = batch[0][-1]",
            "",
            "# perform subsampling",
            "-        if np.sum(self.subsamping_factors) > self.num_encs:",
            "+        if np.sum(self.subsampling_factors) > self.num_encs:",
            "xs_list = [",
            "[x[:: self.subsampling_factors[i], :] for x in xs_list[i]]",
            "for i in range(self.num_encs)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=subsamping_factors), value='subsampling_factors')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 8705,
        "neg_line": [
            "-if np.sum(self.subsamping_factors) > self.num_encs:"
        ],
        "pos_line": [
            "+if np.sum(self.subsampling_factors) > self.num_encs:"
        ],
        "core_change": "-if np.sum(self.subsamping_factors) > self.num_encs: +if np.sum(self.subsampling_factors) > self.num_encs:",
        "core_API": "sum"
    },
    {
        "commit_hash": "dc12b098e8d7501f55811bce08d31079fc687394",
        "index": "2e9f06d2..78a4fe5e 100644",
        "commit_message": "fix the import path of msda (#5338)\n\n* fix the import path of msda\n\n* fix the import path of msda\n",
        "file": "mmdetection.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class DeformableDetrTransformer(Transformer):",
            "nn.init.xavier_uniform_(p)",
            "for m in self.modules():",
            "if isinstance(m, MultiScaleDeformableAttention):",
            "-                m.init_weight()",
            "+                m.init_weights()",
            "if not self.as_two_stage:",
            "xavier_init(self.reference_points, distribution='uniform', bias=0.)",
            "normal_(self.level_embeds)"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=init_weight), value='init_weights')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 8706,
        "neg_line": [
            "-m.init_weight()"
        ],
        "pos_line": [
            "+m.init_weights()"
        ],
        "core_change": "-m.init_weight() +m.init_weights()",
        "core_API": "xavier_uniform_"
    },
    {
        "commit_hash": "c51dc4f92755c67a83f3fc8a0bd6b3e64df199e4",
        "index": "cac930053..f7be47c00 100755",
        "commit_message": "[torch] remove deprecated uint8 in favor of bool (#21384)\n\n* uint8 -> bool\n\n* fix copies\n\n* style\n\n* update test modeling commen when checking attention buffers\n\n* style\n\n* use logical not on random mask instead of subtraction with 1\n\n* remove torch uint8\n\n* quality\n\n* remove modified modeling utils\n\n* Update based on review\n\nCo-authored-by: sgugger <sylvain.gugger@gmail.com>\n\n---------\n\nCo-authored-by: sgugger <sylvain.gugger@gmail.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def filter_logits(logits, top_k=0, top_p=0.0, filter_value=-float(\"Inf\")):",
            "sorted_indices_to_remove[..., 0] = 0",
            "",
            "# indices_to_remove = sorted_indices[sorted_indices_to_remove]",
            "-        indices_to_remove = torch.zeros_like(logits, dtype=torch.uint8).scatter_(",
            "+        indices_to_remove = torch.zeros_like(logits, dtype=torch.bool).scatter_(",
            "dim=-1, index=sorted_indices, src=sorted_indices_to_remove",
            ")",
            "logits[indices_to_remove] = filter_value"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=uint8), value='bool')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 8707,
        "neg_line": [
            "-indices_to_remove = torch.zeros_like(logits, dtype=torch.uint8).scatter_("
        ],
        "pos_line": [
            "+indices_to_remove = torch.zeros_like(logits, dtype=torch.bool).scatter_("
        ],
        "core_change": "-indices_to_remove = torch.zeros_like(logits, dtype=torch.uint8).scatter_( +indices_to_remove = torch.zeros_like(logits, dtype=torch.bool).scatter_(",
        "core_API": "zeros_like"
    },
    {
        "commit_hash": "c579e904f9d87214c40a53469e3c03604fd48319",
        "index": "e23384e..4e7f7fa 100644",
        "commit_message": "Fixed a bug with the vocoder's inference demo\n\n",
        "file": "Real-Time-Voice-Cloning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "optim",
        "change": [
            "if __name__ == '__main__':",
            "torch.save({'step': step, 'model_state': model.state_dict()}, model_fpath)",
            "print('<saved>')",
            "",
            "-    optimiser = optim.Adam(model.parameters())",
            "-    train(model, optimiser, epochs=60, batch_size=128, classes=2**bits,",
            "+    optimizer = optim.Adam(model.parameters())",
            "+    train(model, optimizer, epochs=60, batch_size=64, classes=2 ** bits,",
            "seq_len=seq_len, step=step, lr=1e-4)",
            "",
            "\\ No newline at end of file"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=optimiser), value='optimizer')",
            "Update(target_node=ASTNode(type=identifier, text=optimiser), value='optimizer')",
            "Update(target_node=ASTNode(type=integer, text=128), value='64')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 3,
        "number": 8709,
        "neg_line": [
            "-optimiser = optim.Adam(model.parameters())",
            "-train(model, optimiser, epochs=60, batch_size=128, classes=2**bits,"
        ],
        "pos_line": [
            "+optimizer = optim.Adam(model.parameters())",
            "+train(model, optimizer, epochs=60, batch_size=64, classes=2 ** bits,"
        ],
        "core_change": "-optimiser = optim.Adam(model.parameters()) -train(model, optimiser, epochs=60, batch_size=128, classes=2**bits, +optimizer = optim.Adam(model.parameters()) +train(model, optimizer, epochs=60, batch_size=64, classes=2 ** bits,",
        "core_API": "save"
    },
    {
        "commit_hash": "df3e2cb4cceb47a36ceed0a261cceeb27e7a6e00",
        "index": "6e906d3d..a9174dee 100644",
        "commit_message": "Layer API Refactoring (#675)\n\n* Layer API Refactoring\n\n* private method decorator added\n\n* Input Layers added\n\n* Lazy Import on ROI pooling\n\n* @zsdonghao change proposal added\n\n* UTF-8 header added\n\n* gitignore updated\n\n* Python2 error correction\n\n* Changelog Updated\n\n* Python Headers Uniformized + Codacy Error Fix + LSTMStateTuple missing import\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "layers",
        "change": [
            "class ROIPoolingLayer(Layer):",
            "",
            "logging.info(\"ROIPoolingLayer %s: (%d, %d)\" % (name, pool_height, pool_width))",
            "",
            "-        self.inputs = prev_layer.outputs",
            "-",
            "self.outputs = roi_pooling(self.inputs, rois, pool_height, pool_width)",
            "",
            "-        self.all_layers.append(self.outputs)",
            "+        self._add_layers(self.outputs)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=all_layers), value='_add_layers')",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=inputs))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=prev_layer))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=outputs))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=append))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 16,
        "number": 8711,
        "neg_line": [
            "-self.inputs = prev_layer.outputs",
            "-",
            "-self.all_layers.append(self.outputs)"
        ],
        "pos_line": [
            "+self._add_layers(self.outputs)"
        ],
        "core_change": "-self.inputs = prev_layer.outputs - -self.all_layers.append(self.outputs) +self._add_layers(self.outputs)",
        "core_API": "info"
    },
    {
        "commit_hash": "2c22d6a75f4052f4fcd08aa602947e388aa3460c",
        "index": "72984c57..124c586b 100755",
        "commit_message": "fixed problems with q_model internals, q_nstep_model disc cum reward, base_agent_test excludes in test_multi, evolutionary test params\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Optimizer(object):",
            "The optimization operation.",
            "\"\"\"",
            "deltas = self.step(time=time, variables=variables, **kwargs)",
            "-        # deltas[0] = tf.Print(deltas[0], (deltas[0],))",
            "with tf.control_dependencies(control_inputs=deltas):",
            "return tf.no_op()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 8713,
        "neg_line": [
            "-# deltas[0] = tf.Print(deltas[0], (deltas[0],))"
        ],
        "pos_line": [],
        "core_change": "-# deltas[0] = tf.Print(deltas[0], (deltas[0],))",
        "core_API": "step"
    },
    {
        "commit_hash": "5ebace28f847cdac0c5870a66690f8bb9a641276",
        "index": "67f30c23..4269e319 100644",
        "commit_message": "Fix `DegreeScaler` to not divide by zero (#6877)\n\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DegreeScalerAggregation(Aggregation):",
            "elif scaler == 'amplification':",
            "out_scaler = out * (torch.log(deg + 1) / self.avg_deg_log)",
            "elif scaler == 'attenuation':",
            "-                out_scaler = out * (self.avg_deg_log / torch.log(deg + 1))",
            "+                # Clamp minimum degree to one to avoid dividing by zero:",
            "+                out_scaler = out * (self.avg_deg_log /",
            "+                                    torch.log(deg.clamp(min=1) + 1))",
            "elif scaler == 'linear':",
            "out_scaler = out * (deg / self.avg_deg_lin)",
            "elif scaler == 'inverse_linear':",
            "-                # Clamps minimum degree into one to avoid dividing by zero",
            "-                out_scaler = out * (self.avg_deg_lin / deg.clamp(1))",
            "+                # Clamp minimum degree to one to avoid dividing by zero:",
            "+                out_scaler = out * (self.avg_deg_lin / deg.clamp(min=1))",
            "else:",
            "raise ValueError(f\"Unknown scaler '{scaler}'\")",
            "outs.append(out_scaler)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=1, insert_id=929600)",
            "Insert(target_node=ASTNode(type=binary_operator), node=('call', None), position=0, insert_id=929601)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'min'), position=0, insert_id=929602)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=929603)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=integer, text=1), position=2)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=929604)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=929605)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=deg), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=929606)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'clamp'), position=2, insert_id=929607)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=929608)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=1, insert_id=929609)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=929610)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'min'), position=0, insert_id=929611)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=929612)",
            "Insert(target_node=IN(type=keyword_argument), node=('integer', '1'), position=2, insert_id=929613)"
        ],
        "plus_line": 5,
        "minus_line": 3,
        "AST_diff_line": 16,
        "number": 8717,
        "neg_line": [
            "-out_scaler = out * (self.avg_deg_log / torch.log(deg + 1))",
            "-# Clamps minimum degree into one to avoid dividing by zero",
            "-out_scaler = out * (self.avg_deg_lin / deg.clamp(1))"
        ],
        "pos_line": [
            "+# Clamp minimum degree to one to avoid dividing by zero:",
            "+out_scaler = out * (self.avg_deg_log /",
            "+torch.log(deg.clamp(min=1) + 1))",
            "+# Clamp minimum degree to one to avoid dividing by zero:",
            "+out_scaler = out * (self.avg_deg_lin / deg.clamp(min=1))"
        ],
        "core_change": "-out_scaler = out * (self.avg_deg_log / torch.log(deg + 1)) +# Clamp minimum degree to one to avoid dividing by zero: +out_scaler = out * (self.avg_deg_log / +torch.log(deg.clamp(min=1) + 1)) -# Clamps minimum degree into one to avoid dividing by zero -out_scaler = out * (self.avg_deg_lin / deg.clamp(1)) +# Clamp minimum degree to one to avoid dividing by zero: +out_scaler = out * (self.avg_deg_lin / deg.clamp(min=1))",
        "core_API": "log"
    },
    {
        "commit_hash": "c0b2040e38c5135c65dd2b5e9b24f00ff3f31548",
        "index": "43f5130..9677e11 100644",
        "commit_message": "Fixed module resolution for tf.keras optimizers and added unit tests (#1935)\n\nSigned-off-by: Travis Addair <taddair@uber.com>\n",
        "file": "horovod.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "keras",
        "change": [
            "def broadcast(backend, value, root_rank, name):",
            "return _eval(backend, hvd.broadcast(tf.constant(value, name=name), root_rank))",
            "",
            "",
            "-def load_model(keras, wrap_optimizer, filepath, custom_optimizers, custom_objects):",
            "+def load_model(keras, wrap_optimizer, optimizer_modules, filepath, custom_optimizers, custom_objects):",
            "horovod_objects = {",
            "subclass.__name__.lower(): wrap_optimizer(subclass)",
            "for subclass in keras.optimizers.Optimizer.__subclasses__()",
            "-        if subclass.__module__ == keras.optimizers.Optimizer.__module__",
            "+        if subclass.__module__ in optimizer_modules",
            "}",
            "",
            "if custom_optimizers is not None:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=parameters), node=('identifier', 'optimizer_modules'), position=5, insert_id=2486133)",
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=6, insert_id=2486134)",
            "Insert(target_node=ASTNode(type=comparison_operator), node=('in', 'in'), position=1, insert_id=2486135)",
            "Update(target_node=ASTNode(type=identifier, text=keras), value='optimizer_modules')",
            "Move(target_node=ASTNode(type=comparison_operator), node=ASTNode(type=identifier, text=keras), position=2)",
            "Delete(target_node=ASTNode(type===, text===))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=optimizers))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=Optimizer))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=__module__))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 15,
        "number": 8718,
        "neg_line": [
            "-def load_model(keras, wrap_optimizer, filepath, custom_optimizers, custom_objects):",
            "-if subclass.__module__ == keras.optimizers.Optimizer.__module__"
        ],
        "pos_line": [
            "+def load_model(keras, wrap_optimizer, optimizer_modules, filepath, custom_optimizers, custom_objects):",
            "+if subclass.__module__ in optimizer_modules"
        ],
        "core_change": "-def load_model(keras, wrap_optimizer, filepath, custom_optimizers, custom_objects): +def load_model(keras, wrap_optimizer, optimizer_modules, filepath, custom_optimizers, custom_objects): -if subclass.__module__ == keras.optimizers.Optimizer.__module__ +if subclass.__module__ in optimizer_modules",
        "core_API": "broadcast"
    },
    {
        "commit_hash": "80aefc752c3e3a2dc59212ff9a87564bf727fc5f",
        "index": "26aef8002e..497a34b16f 100644",
        "commit_message": "small fixes, replacing as_str arg with as_native, and removing redundant setting of the default value.\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "from ivy.container import Container",
            "",
            "",
            "def variable(x):",
            "-    with tf.device(\"/\" + ivy.dev(x, as_str=True).upper()):",
            "+    with ivy.dev(x, as_native=True):",
            "return tf.Variable(x, trainable=True)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=argument_list), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=as_str), value='as_native')",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=device))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=string, text=\"/\"))",
            "Delete(target_node=ASTNode(type=+, text=+))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=upper))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=binary_operator))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 21,
        "number": 8719,
        "neg_line": [
            "-with tf.device(\"/\" + ivy.dev(x, as_str=True).upper()):"
        ],
        "pos_line": [
            "+with ivy.dev(x, as_native=True):"
        ],
        "core_change": "-with tf.device(\"/\" + ivy.dev(x, as_str=True).upper()): +with ivy.dev(x, as_native=True):",
        "core_API": "device"
    },
    {
        "commit_hash": "9bea48cd9b0e7a324d5bd596c68efe5a2efdd322",
        "index": "7eeef94..f736841 100644",
        "commit_message": "Revert \"Fix mtf broadcast error when lowering\"\n\nThis reverts commit 8b9631fc5a06149fb6fc5465305217bf5ae31c3e.\n\n",
        "file": "gpt-neo.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def expand_tile(value, newdim):",
            "print(value)",
            "print('############')",
            "",
            "-    return mtf.broadcast(mtf_expand_dims(value, newdim, 0),",
            "+    return mtf.broadcast(mtf_expand_dims(value, 'dummy_batch', 0),",
            "[newdim] + value.shape.dims)  # shape.dims gets us a list which we need in order to concat"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('string', \"'dummy_batch'\"), position=3, insert_id=1942765)",
            "Delete(target_node=ASTNode(type=identifier, text=newdim))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 8726,
        "neg_line": [
            "-return mtf.broadcast(mtf_expand_dims(value, newdim, 0),"
        ],
        "pos_line": [
            "+return mtf.broadcast(mtf_expand_dims(value, 'dummy_batch', 0),"
        ],
        "core_change": "-return mtf.broadcast(mtf_expand_dims(value, newdim, 0), +return mtf.broadcast(mtf_expand_dims(value, 'dummy_batch', 0),",
        "core_API": "broadcast"
    },
    {
        "commit_hash": "dfbb592566cf674ed48eed2a2ec30976ebb469d4",
        "index": "b7d438869..f418db2bd 100644",
        "commit_message": "fix test - reduce metric\n\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_metric_are_properly_reduced(tmpdir):",
            "trainer.fit(model)",
            "",
            "assert trainer.callback_metrics[\"val_acc\"] == 8 / 32.",
            "-    assert \"train_acc\" in trainer.callback_metrics",
            "+    assert \"train_loss\" in trainer.callback_metrics"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"train_acc\"), value='\"train_loss\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 8727,
        "neg_line": [
            "-assert \"train_acc\" in trainer.callback_metrics"
        ],
        "pos_line": [
            "+assert \"train_loss\" in trainer.callback_metrics"
        ],
        "core_change": "-assert \"train_acc\" in trainer.callback_metrics +assert \"train_loss\" in trainer.callback_metrics",
        "core_API": "fit"
    },
    {
        "commit_hash": "5f4c6e830f603830117877b8990a0e65a2386aa6",
        "index": "167dfdf64..81d98e6d4 100644",
        "commit_message": "Quick fix :) (#606)\n\n* Changing the name\n\n* style + quality\n\n* update doc and logo\n\n* clean up\n\n* circle-CI on the branche for now\n\n* fix daily dialog dataset\n\n* fix urls\n\nCo-authored-by: Quentin Lhoest <lhoest.q@gmail.com>\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class ObjectInfo:",
            "",
            "def __repr__(self):",
            "single_line_description = self.description.replace(\"\\n\", \"\") if self.description is not None else \"\"",
            "-        return f\"nlp.ObjectInfo(\\n\\tid='{self.id}',\\n\\tdescription='{single_line_description}',\\n\\tfiles={self.siblings}\\n)\"",
            "+        return f\"datasets.ObjectInfo(\\n\\tid='{self.id}',\\n\\tdescription='{single_line_description}',\\n\\tfiles={self.siblings}\\n)\"",
            "",
            "",
            "class HfApi:"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=f\"nlp.ObjectInfo(\\n\\tid='{self.id}',\\n\\tdescription='{single_line_description}',\\n\\tfiles={self.siblings}\\n)\"), value='f\"datasets.ObjectInfo(\\\\n\\\\tid=\\'{self.id}\\',\\\\n\\\\tdescription=\\'{single_line_description}\\',\\\\n\\\\tfiles={self.siblings}\\\\n)\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 8729,
        "neg_line": [
            "-return f\"nlp.ObjectInfo(\\n\\tid='{self.id}',\\n\\tdescription='{single_line_description}',\\n\\tfiles={self.siblings}\\n)\""
        ],
        "pos_line": [
            "+return f\"datasets.ObjectInfo(\\n\\tid='{self.id}',\\n\\tdescription='{single_line_description}',\\n\\tfiles={self.siblings}\\n)\""
        ],
        "core_change": "-return f\"nlp.ObjectInfo(\\n\\tid='{self.id}',\\n\\tdescription='{single_line_description}',\\n\\tfiles={self.siblings}\\n)\" +return f\"datasets.ObjectInfo(\\n\\tid='{self.id}',\\n\\tdescription='{single_line_description}',\\n\\tfiles={self.siblings}\\n)\"",
        "core_API": "replace"
    },
    {
        "commit_hash": "d863941b264b863900f9d107c0837b8ba43f713f",
        "index": "b3ac30c7..c317e5b5 100644",
        "commit_message": "fix the embedding averaging in the entity linker\n\n",
        "file": "flair.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class EntityLinker(flair.nn.DefaultClassifier[Sentence, Span]):",
            ")",
            "",
            "def emb_mean(self, span, embedding_names):",
            "-        return torch.mean(torch.cat([token.get_embedding(embedding_names) for token in span], 0), 0)",
            "+        return torch.mean(torch.stack([token.get_embedding(embedding_names) for token in span], 0), 0)",
            "",
            "def _get_data_points_from_sentence(self, sentence: Sentence) -> List[Span]:",
            "return sentence.get_spans(self.label_type)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=cat), value='stack')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 8738,
        "neg_line": [
            "-return torch.mean(torch.cat([token.get_embedding(embedding_names) for token in span], 0), 0)"
        ],
        "pos_line": [
            "+return torch.mean(torch.stack([token.get_embedding(embedding_names) for token in span], 0), 0)"
        ],
        "core_change": "-return torch.mean(torch.cat([token.get_embedding(embedding_names) for token in span], 0), 0) +return torch.mean(torch.stack([token.get_embedding(embedding_names) for token in span], 0), 0)",
        "core_API": "mean"
    },
    {
        "commit_hash": "f04257fdbcb6ecb5a9bef75f4c2a8d2e8b5a6209",
        "index": "af95f348e..dcfa84d0f 100644",
        "commit_message": "Add test to ensure models can take int64 inputs (#17210)\n\n* Add test to ensure models can take int64 inputs\n\n* is_integer is an attribute, not a method\n\n* Fix test when some inputs aren't tensors\n\n* Add casts to blenderbot and blenderbot-small\n\n* Add casts to the other failing models\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFAdaptiveSoftmaxMask(tf.keras.layers.Layer):",
            "@staticmethod",
            "def _gather_logprob(logprob, target):",
            "lp_size = shape_list(logprob)",
            "-        r = tf.range(lp_size[0])",
            "+        r = tf.range(lp_size[0], dtype=target.dtype)",
            "idx = tf.stack([r, target], 1)",
            "return tf.gather_nd(logprob, idx)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=2364428)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=2364429)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=2364430)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2364431)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=2364432)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'target'), position=0, insert_id=2364433)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2364434)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dtype'), position=2, insert_id=2364435)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 8743,
        "neg_line": [
            "-r = tf.range(lp_size[0])"
        ],
        "pos_line": [
            "+r = tf.range(lp_size[0], dtype=target.dtype)"
        ],
        "core_change": "-r = tf.range(lp_size[0]) +r = tf.range(lp_size[0], dtype=target.dtype)",
        "core_API": "range"
    },
    {
        "commit_hash": "9d0274c24e7f558f4466b94be05f4abe2eda6bb3",
        "index": "301ff5a7..ccfa47e6 100644",
        "commit_message": "Fix `spots` argument processing for approximate option price under Heston.\n\nPiperOrigin-RevId: 353640676\n\n",
        "file": "tf-quant-finance.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def european_option_price(*,",
            "sigmas = tf.convert_to_tensor(sigmas, dtype=dtype, name='sigmas')",
            "rhos = tf.convert_to_tensor(rhos, dtype=dtype, name='rhos')",
            "variances = tf.convert_to_tensor(variances, dtype=dtype, name='variances')",
            "-    forwards = tf.convert_to_tensor(forwards, dtype=dtype, name='forwards')",
            "",
            "if discount_factors is not None:",
            "discount_factors = tf.convert_to_tensor("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=IN(type=root), node=ASTNode(type=call), position=0)",
            "Delete(target_node=ASTNode(type=identifier, text=forwards))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=convert_to_tensor))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=forwards))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=dtype))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=dtype))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=name))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=string, text='forwards'))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 22,
        "number": 8744,
        "neg_line": [
            "-forwards = tf.convert_to_tensor(forwards, dtype=dtype, name='forwards')"
        ],
        "pos_line": [],
        "core_change": "-forwards = tf.convert_to_tensor(forwards, dtype=dtype, name='forwards')",
        "core_API": "convert_to_tensor"
    },
    {
        "commit_hash": "4ef2d4d0c8524372d0047e050065edcac665ce1a",
        "index": "0cc9a3b..104a8b4 100644",
        "commit_message": "Fix actor_critic example (#301)\n\n* smooth_l1_loss now requires shapes to match\n * once scalars are enabled we must torch.stack() instead of\n   torch.cat() a list of scalars\n",
        "file": "examples.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def finish_episode():",
            "rewards = torch.Tensor(rewards)",
            "rewards = (rewards - rewards.mean()) / (rewards.std() + np.finfo(np.float32).eps)",
            "for (log_prob, value), r in zip(saved_actions, rewards):",
            "-        reward = r - value.data[0, 0]",
            "+        reward = r - value.data[0]",
            "policy_losses.append(-log_prob * reward)",
            "value_losses.append(F.smooth_l1_loss(value, Variable(torch.Tensor([r]))))",
            "optimizer.zero_grad()",
            "-    loss = torch.cat(policy_losses).sum() + torch.cat(value_losses).sum()",
            "+    loss = torch.stack(policy_losses).sum() + torch.stack(value_losses).sum()",
            "loss.backward()",
            "optimizer.step()",
            "del model.rewards[:]"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=cat), value='stack')",
            "Update(target_node=ASTNode(type=identifier, text=cat), value='stack')",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=integer, text=0))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 4,
        "number": 8745,
        "neg_line": [
            "-reward = r - value.data[0, 0]",
            "-loss = torch.cat(policy_losses).sum() + torch.cat(value_losses).sum()"
        ],
        "pos_line": [
            "+reward = r - value.data[0]",
            "+loss = torch.stack(policy_losses).sum() + torch.stack(value_losses).sum()"
        ],
        "core_change": "-reward = r - value.data[0, 0] +reward = r - value.data[0] -loss = torch.cat(policy_losses).sum() + torch.cat(value_losses).sum() +loss = torch.stack(policy_losses).sum() + torch.stack(value_losses).sum()",
        "core_API": "Tensor"
    },
    {
        "commit_hash": "5d3d051c9b6bb25c45d254ceabab669c758ed72b",
        "index": "7a73083..fa05fcf 100644",
        "commit_message": "Inline `_make_grid()` meshgrid (#9170)\n\n* Inline _make_grid() meshgrid \n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "yolov5.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Detect(nn.Module):",
            "t = self.anchors[i].dtype",
            "shape = 1, self.na, ny, nx, 2  # grid shape",
            "y, x = torch.arange(ny, device=d, dtype=t), torch.arange(nx, device=d, dtype=t)",
            "-        if torch_1_10:  # torch>=1.10.0 meshgrid workaround for torch>=0.7 compatibility",
            "-            yv, xv = torch.meshgrid(y, x, indexing='ij')",
            "-        else:",
            "-            yv, xv = torch.meshgrid(y, x)",
            "+        yv, xv = torch.meshgrid(y, x, indexing='ij') if torch_1_10 else torch.meshgrid(y, x)  # torch>=0.7 compatibility",
            "grid = torch.stack((xv, yv), 2).expand(shape) - 0.5  # add grid offset, i.e. y = 2.0 * x - 0.5",
            "anchor_grid = (self.anchors[i] * self.stride[i]).view((1, self.na, 1, 1, 2)).expand(shape)",
            "return grid, anchor_grid"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=module), node=ASTNode(type=expression_statement), position=4)",
            "Insert(target_node=ASTNode(type=assignment), node=('conditional_expression', None), position=2, insert_id=1292478)",
            "Move(target_node=IN(type=conditional_expression), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=conditional_expression), node=('if', 'if'), position=1, insert_id=1292479)",
            "Insert(target_node=IN(type=conditional_expression), node=('identifier', 'torch_1_10'), position=2, insert_id=1292480)",
            "Insert(target_node=IN(type=conditional_expression), node=('else', 'else'), position=3, insert_id=1292481)",
            "Move(target_node=IN(type=conditional_expression), node=ASTNode(type=call), position=4)",
            "Delete(target_node=ASTNode(type=if, text=if))",
            "Delete(target_node=ASTNode(type=identifier, text=torch_1_10))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=block))",
            "Delete(target_node=ASTNode(type=else, text=else))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=identifier, text=yv))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=xv))",
            "Delete(target_node=ASTNode(type=pattern_list))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=block))",
            "Delete(target_node=ASTNode(type=else_clause))",
            "Delete(target_node=ASTNode(type=if_statement))"
        ],
        "plus_line": 1,
        "minus_line": 4,
        "AST_diff_line": 23,
        "number": 8746,
        "neg_line": [
            "-if torch_1_10:  # torch>=1.10.0 meshgrid workaround for torch>=0.7 compatibility",
            "-yv, xv = torch.meshgrid(y, x, indexing='ij')",
            "-else:",
            "-yv, xv = torch.meshgrid(y, x)"
        ],
        "pos_line": [
            "+yv, xv = torch.meshgrid(y, x, indexing='ij') if torch_1_10 else torch.meshgrid(y, x)  # torch>=0.7 compatibility"
        ],
        "core_change": "-if torch_1_10:  # torch>=1.10.0 meshgrid workaround for torch>=0.7 compatibility -yv, xv = torch.meshgrid(y, x, indexing='ij') -else: -yv, xv = torch.meshgrid(y, x) +yv, xv = torch.meshgrid(y, x, indexing='ij') if torch_1_10 else torch.meshgrid(y, x)  # torch>=0.7 compatibility",
        "core_API": "arange"
    },
    {
        "commit_hash": "0435e23a64c32ce3888b4d12a8c117eea2d4cf01",
        "index": "a476e30dc..13303b02f 100644",
        "commit_message": "deprecate enable_pl_optimizer as it is not restored properly (#5244)\n\n* update\n\n* clean test\n\n* still in progress\n\n* udpdate test\n\n* update\n\n* update\n\n* resolve flake\n\n* add test for zero_grad\n\n* update\n\n* works without accumulated_grad\n\n* update\n\n* update\n\n* resolve amp\n\n* revert back to True\n\n* update\n\n* clean tests\n\n* cleaned out\n\n* typo\n\n* update test\n\n* git repare bug\n\n* remove print\n\n* udpate\n\n* Fix formatting/optimizer imports\n\n* Refactor the test for cleanliness\n\n* Add vanilla model to the test, better var names\n\n* Fixed var names, let's clean up these mock tests\n\n* repare test\n\n* update test\n\n* resolve flake8\n\n* add manual_optimization\n\n* update tests\n\n* resolve flake8\n\n* add random accumulate_grad_batches\n\n* improve test\n\n* Update tests/trainer/optimization/test_parity_automatic_optimization.py\n\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\n\n* Update tests/trainer/optimization/test_parity_automatic_optimization.py\n\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\n\n* update\n\n* clean tests\n\n* correct bug\n\n* Apply suggestions from code review\n\n* format\n\n* adress comments\n\n* update on comments\n\n* wip\n\n* typo\n\n* depreceate enable_pl_optimizer\n\n* resolve latest bugs\n\n* update\n\n* resolve merge\n\n* add comment\n\n* Update pytorch_lightning/core/lightning.py\n\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\n\n* Update tests/deprecated_api/test_remove_1-3.py\n\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\n\n* Update pytorch_lightning/trainer/connectors/optimizer_connector.py\n\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\n\n* Update pytorch_lightning/trainer/trainer.py\n\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\n\n* Update pytorch_lightning/trainer/trainer.py\n\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\n\n* Update tests/trainer/optimization/test_parity_automatic_optimization.py\n\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\n\n* update on comments\n\n* update restore\n\n* add a property\n\n* remove setstate as not needed anymore\n\n* update test\n\n* provide optimizer to on_before_zero_grad\n\n* update on comments\n\n* update on comments\n\n* Update pytorch_lightning/trainer/trainer.py\n\nCo-authored-by: Adrian Wälchli <aedu.waelchli@gmail.com>\n\n* Update tests/trainer/optimization/test_parity_automatic_optimization.py\n\nCo-authored-by: Adrian Wälchli <aedu.waelchli@gmail.com>\n\n* Update tests/trainer/optimization/test_parity_automatic_optimization.py\n\nCo-authored-by: Adrian Wälchli <aedu.waelchli@gmail.com>\n\n* Update tests/trainer/optimization/test_parity_automatic_optimization.py\n\nCo-authored-by: Adrian Wälchli <aedu.waelchli@gmail.com>\n\n* mofidy import\n\n* update changelog\n\n* resolve flake8\n\n* update\n\n* update\n\n* clean doc\n\nCo-authored-by: SeanNaren <sean@grid.ai>\nCo-authored-by: Ubuntu <ubuntu@ip-172-31-62-109.ec2.internal>\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\nCo-authored-by: Jirka Borovec <jirka.borovec@seznam.cz>\nCo-authored-by: Adrian Wälchli <aedu.waelchli@gmail.com>\nCo-authored-by: Sean Naren <sean.narenthiran@gmail.com>\n\n(cherry picked from commit f2e99d617f05ec65fded81ccc6d0d59807c47573)\n\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "def register_optimizers(ctx, model):",
            "model.trainer.optimizers = optimizers",
            "model.trainer.lr_schedulers = lr_schedulers",
            "model.trainer.optimizer_frequencies = optimizer_frequencies",
            "-    model.trainer.convert_to_lightning_optimizers()",
            "",
            "",
            "def run_optimizer(ctx, model):"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=model))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=trainer))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=convert_to_lightning_optimizers))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 12,
        "number": 8747,
        "neg_line": [
            "-model.trainer.convert_to_lightning_optimizers()"
        ],
        "pos_line": [],
        "core_change": "-model.trainer.convert_to_lightning_optimizers()",
        "core_API": "convert_to_lightning_optimizers"
    },
    {
        "commit_hash": "b0d33577387bb0397492d0ee0af5b0d15d7c8385",
        "index": "136b2b9d..1a3427d9 100644",
        "commit_message": "Update keras RNG logic to use tf.random.Generator if possible.\n\nThis change also update the RNG behavior for initializer. The seeded initializer will no longer produce same random value across multiple calls. Instead, it will produce different value, and multiple initializer created with same seed will produce same sequences. This change will the make the seeded initializer behavior align between v1 and v2.\n\nKeras was using stateful RNG op in various place when seed is not provided. The recommended approach in v2 is using tf.random.Generator which can be treat as a variable (seed) with stateless RNG op. This change make sure we use this new approach when seed is provided in v2, and also leave a flag to enforce the new approach, which has not been turn on yet. The new approach will be turned on when all the internal tests are fixed. V1 graph mode, the behavior is not change.\n\nPiperOrigin-RevId: 392092094\n\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def _get_random_features_initializer(initializer, shape):",
            "random_features_initializer = initializer",
            "if isinstance(initializer, str):",
            "if initializer.lower() == 'gaussian':",
            "-      random_features_initializer = tf.compat.v1.random_normal_initializer(",
            "-          stddev=1.0)",
            "+      random_features_initializer = initializers.RandomNormal(stddev=1.0)",
            "elif initializer.lower() == 'laplacian':",
            "-      random_features_initializer = tf.compat.v1.constant_initializer(",
            "+      random_features_initializer = initializers.Constant(",
            "_get_cauchy_samples(loc=0.0, scale=1.0, shape=shape))",
            "",
            "else:"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=tf), value='initializers')",
            "Update(target_node=ASTNode(type=identifier, text=compat), value='RandomNormal')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=tf), value='initializers')",
            "Update(target_node=ASTNode(type=identifier, text=compat), value='Constant')",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=v1))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=random_normal_initializer))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=v1))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=constant_initializer))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 3,
        "AST_diff_line": 18,
        "number": 8748,
        "neg_line": [
            "-random_features_initializer = tf.compat.v1.random_normal_initializer(",
            "-stddev=1.0)",
            "-random_features_initializer = tf.compat.v1.constant_initializer("
        ],
        "pos_line": [
            "+random_features_initializer = initializers.RandomNormal(stddev=1.0)",
            "+random_features_initializer = initializers.Constant("
        ],
        "core_change": "-random_features_initializer = tf.compat.v1.random_normal_initializer( -stddev=1.0) +random_features_initializer = initializers.RandomNormal(stddev=1.0) -random_features_initializer = tf.compat.v1.constant_initializer( +random_features_initializer = initializers.Constant(",
        "core_API": "lower"
    },
    {
        "commit_hash": "a039748e4b1e316d8310648da0b251fee1d33033",
        "index": "6d082a90..91419aac 100644",
        "commit_message": "retro env improvement, some fixes\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Deltafier(PreprocessingLayer):",
            "value=tf.constant(value=True, dtype=util.tf_dtype(dtype='bool')), read_value=False",
            ")",
            "with tf.control_dependencies(control_inputs=(assignment,)):",
            "-                return tf.concat(values=(tf.zeros_like(tensor=x[:1]), x[1:] - x[:-1]), axis=0)",
            "+                return tf.concat(values=(tf.zeros_like(tensor=x[:1]), x[1:] - x[:-1]), axis=0)  # dtype=util.tf_dtype(dtype='???'))",
            "",
            "def later_delta():",
            "return x - tf.concat(values=(self.previous, x[:-1]), axis=0)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 8753,
        "neg_line": [
            "-return tf.concat(values=(tf.zeros_like(tensor=x[:1]), x[1:] - x[:-1]), axis=0)"
        ],
        "pos_line": [
            "+return tf.concat(values=(tf.zeros_like(tensor=x[:1]), x[1:] - x[:-1]), axis=0)  # dtype=util.tf_dtype(dtype='???'))"
        ],
        "core_change": "-return tf.concat(values=(tf.zeros_like(tensor=x[:1]), x[1:] - x[:-1]), axis=0) +return tf.concat(values=(tf.zeros_like(tensor=x[:1]), x[1:] - x[:-1]), axis=0)  # dtype=util.tf_dtype(dtype='???'))",
        "core_API": "constant"
    },
    {
        "commit_hash": "b7f0ce5b39acc2e0d879c55c56b374738be7c027",
        "index": "2cb81968..ea94f486 100755",
        "commit_message": "fix test_ldm_text2img_fast\n\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class PipelineTesterMixin(unittest.TestCase):",
            "image_slice = image[0, -1, -3:, -3:].cpu()",
            "",
            "assert image.shape == (1, 3, 256, 256)",
            "-        expected_slice = torch.rensor([0.3163, 0.8670, 0.6465, 0.1865, 0.6291, 0.5139, 0.2824, 0.3723, 0.4344])",
            "+        expected_slice = torch.tensor([0.3163, 0.8670, 0.6465, 0.1865, 0.6291, 0.5139, 0.2824, 0.3723, 0.4344])",
            "assert (image_slice.flatten() - expected_slice).abs().max() < 1e-2",
            "",
            "@slow"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=rensor), value='tensor')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 8760,
        "neg_line": [
            "-expected_slice = torch.rensor([0.3163, 0.8670, 0.6465, 0.1865, 0.6291, 0.5139, 0.2824, 0.3723, 0.4344])"
        ],
        "pos_line": [
            "+expected_slice = torch.tensor([0.3163, 0.8670, 0.6465, 0.1865, 0.6291, 0.5139, 0.2824, 0.3723, 0.4344])"
        ],
        "core_change": "-expected_slice = torch.rensor([0.3163, 0.8670, 0.6465, 0.1865, 0.6291, 0.5139, 0.2824, 0.3723, 0.4344]) +expected_slice = torch.tensor([0.3163, 0.8670, 0.6465, 0.1865, 0.6291, 0.5139, 0.2824, 0.3723, 0.4344])",
        "core_API": "rensor"
    },
    {
        "commit_hash": "5f4c6e830f603830117877b8990a0e65a2386aa6",
        "index": "3b989034a..7c4a81b4f 100644",
        "commit_message": "Quick fix :) (#606)\n\n* Changing the name\n\n* style + quality\n\n* update doc and logo\n\n* clean up\n\n* circle-CI on the branche for now\n\n* fix daily dialog dataset\n\n* fix urls\n\nCo-authored-by: Quentin Lhoest <lhoest.q@gmail.com>\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class Reddit(nlp.GeneratorBasedBuilder):",
            "\"\"\"Returns SplitGenerators.\"\"\"",
            "dl_path = dl_manager.download_and_extract(_URL)",
            "return [",
            "-            nlp.SplitGenerator(",
            "-                name=nlp.Split.TRAIN,",
            "+            datasets.SplitGenerator(",
            "+                name=datasets.Split.TRAIN,",
            "gen_kwargs={\"path\": os.path.join(dl_path, \"corpus-webis-tldr-17.json\")},",
            ")",
            "]"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=nlp), value='datasets')",
            "Update(target_node=ASTNode(type=identifier, text=nlp), value='datasets')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 8761,
        "neg_line": [
            "-nlp.SplitGenerator(",
            "-name=nlp.Split.TRAIN,"
        ],
        "pos_line": [
            "+datasets.SplitGenerator(",
            "+name=datasets.Split.TRAIN,"
        ],
        "core_change": "-nlp.SplitGenerator( -name=nlp.Split.TRAIN, +datasets.SplitGenerator( +name=datasets.Split.TRAIN,",
        "core_API": "download_and_extract"
    },
    {
        "commit_hash": "b2ac1dae529f0c3a9c6f3c4601bd1aee0bbbd507",
        "index": "b0586eac..7ac84c12 100644",
        "commit_message": "fix unpooling with none shape\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def UnPooling2x2ZeroFilled(x):",
            "out_size = [-1, sh[1] * 2, sh[2] * 2, sh[3]]",
            "return tf.reshape(out, out_size)",
            "else:",
            "-        sh = tf.shape(x)",
            "-        ret = tf.reshape(out, tf.pack([-1, sh[1] * 2, sh[2] * 2, sh[3]]))",
            "+        shv = tf.shape(x)",
            "+        ret = tf.reshape(out, tf.pack([-1, shv[1] * 2, shv[2] * 2, sh[3]]))",
            "ret.set_shape([None, None, None, sh[3]])",
            "return ret"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=sh), value='shv')",
            "Update(target_node=ASTNode(type=identifier, text=sh), value='shv')",
            "Update(target_node=ASTNode(type=identifier, text=sh), value='shv')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 3,
        "number": 8762,
        "neg_line": [
            "-sh = tf.shape(x)",
            "-ret = tf.reshape(out, tf.pack([-1, sh[1] * 2, sh[2] * 2, sh[3]]))"
        ],
        "pos_line": [
            "+shv = tf.shape(x)",
            "+ret = tf.reshape(out, tf.pack([-1, shv[1] * 2, shv[2] * 2, sh[3]]))"
        ],
        "core_change": "-sh = tf.shape(x) -ret = tf.reshape(out, tf.pack([-1, sh[1] * 2, sh[2] * 2, sh[3]])) +shv = tf.shape(x) +ret = tf.reshape(out, tf.pack([-1, shv[1] * 2, shv[2] * 2, sh[3]]))",
        "core_API": "reshape"
    },
    {
        "commit_hash": "c2a8735b98a90c996a508128b11c204659d2f1aa",
        "index": "d6b83384f..6644714a8 100644",
        "commit_message": "Ast refactor (#4941)\n\n* Ast refactor that includes:\n* property support + action for properties\n* static methods\n* ast refactor\n* globals support\n* settable properties\n\n* removing props\n\n* removing protobuf\n\n* protobug changes\n\n* reverting protobuf\n\n* protoc changes\n\n* Some fixes\n\n* Added sy_serialize back for situations where serialize is taken\n\n* Add copy method back\n\n* Restored change to send method which requires tags, description\n\n* More fixes\n\n* Lib ast fix\n\n* Type hint and tests fix\n\nSigned-off-by: George-Cristian Muraru <murarugeorgec@gmail.com>\n\n* Add util lost file\n\n* Remove type hint\n\n* docs\n\n* removing protobuf\n\n* proto removal\n\n* mypy fixes\n\n* pytest hack\n\n* mypy fix\n\n* skipping duet test\n\n* fixing mypy err\n\n* Disable python 3.9 tests because torchcsprng==0.1.4 not available yet\n\n* Linting and small fixes\n\n- Adding missing break to loopback=True _server_exchange\n- Renamed exception variable in logging to prevent shadowing\n- Added if not None check to Metaclass to satisfy linter\n- Replaced Logger code with template and added print=True kwarg\n- isort and pre_commit.sh checks\n\n* Reverting back to python 3.6 compatible logging code\n\nCo-authored-by: George Muraru <murarugeorgec@gmail.com>\nCo-authored-by: Madhava Jay <me@madhavajay.com>\n",
        "file": "PySyft.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "def test_path_cache() -> None:",
            "",
            "refs: TypeList[TypeType] = []",
            "for path in conv2d_paths:",
            "-        klass = sy.lib_ast(path, return_callable=True, obj_type=th.nn.Conv2d)",
            "+        klass = sy.lib_ast.query(path, obj_type=th.nn.Conv2d)",
            "assert klass == sy.lib_ast.torch.nn.Conv2d",
            "assert klass.name == \"Conv2d\"",
            "assert klass.path_and_name == short_fqn"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1448499)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1448500)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'query'), position=2, insert_id=1448501)",
            "Delete(target_node=ASTNode(type=identifier, text=return_callable))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=true, text=True))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 8763,
        "neg_line": [
            "-klass = sy.lib_ast(path, return_callable=True, obj_type=th.nn.Conv2d)"
        ],
        "pos_line": [
            "+klass = sy.lib_ast.query(path, obj_type=th.nn.Conv2d)"
        ],
        "core_change": "-klass = sy.lib_ast(path, return_callable=True, obj_type=th.nn.Conv2d) +klass = sy.lib_ast.query(path, obj_type=th.nn.Conv2d)",
        "core_API": "lib_ast"
    },
    {
        "commit_hash": "9b1d0907486af6ee16e9e614839afbbe9d583a37",
        "index": "065627b3..838c1636 100644",
        "commit_message": "fix bn performance\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class SyncMultiGPUTrainer(MultiGPUTrainer,",
            "super(SyncMultiGPUTrainer, self)._setup()",
            "grad_list = MultiGPUTrainer._multi_tower_grads(",
            "self.config.tower, lambda: self._get_cost_and_grad()[1])",
            "+",
            "+        # debug tower performance:",
            "+        #ops = [k[0] for k in grad_list[1]] + [k[0] for k in grad_list[0]]",
            "+        #self.train_op = tf.group(*ops)",
            "+        #return",
            "+",
            "grads = SyncMultiGPUTrainer._average_grads(grad_list)",
            "grads = apply_grad_processors(grads, self.model.get_gradient_processor())"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 4,
        "minus_line": 0,
        "AST_diff_line": 17,
        "number": 8765,
        "neg_line": [],
        "pos_line": [
            "+",
            "+# debug tower performance:",
            "+#ops = [k[0] for k in grad_list[1]] + [k[0] for k in grad_list[0]]",
            "+#self.train_op = tf.group(*ops)",
            "+#return",
            "+"
        ],
        "core_change": "+ +# debug tower performance: +#ops = [k[0] for k in grad_list[1]] + [k[0] for k in grad_list[0]] +#self.train_op = tf.group(*ops) +#return +",
        "core_API": "_multi_tower_grads"
    },
    {
        "commit_hash": "9de1add073378ad60bfe5fbab88d0c74d86793f5",
        "index": "83ea81231..c5cd08ae4 100644",
        "commit_message": "[Datasets] Autodetect dataset parallelism based on available resources and data size (#25883)\n\nThis PR defaults the parallelism of Dataset reads to `-1`. The parallelism is determined according to the following rule in this case:\n- The number of available CPUs is estimated. If in a placement group, the number of CPUs in the cluster is scaled by the size of the placement group compared to the cluster size. If not in a placement group, this is the number of CPUs in the cluster. If the estimated CPUs is less than 8, it is set to 8.\n- The parallelism is set to the estimated number of CPUs multiplied by 2.\n- The in-memory data size is estimated. If the parallelism would create in-memory blocks larger than the target block size (512MiB), the parallelism is increased until the blocks are < 512MiB in size.\n\nThese rules fix two common user problems:\n1. Insufficient parallelism in a large cluster, or too much parallelism on a small cluster.\n2. Overly large block sizes leading to OOMs when processing a single block.\n\nTODO:\n- [x] Unit tests\n- [x] Docs update\n\nSupercedes part of: https://github.com/ray-project/ray/pull/25708\n\nCo-authored-by: Ubuntu <ubuntu@ip-172-31-32-136.us-west-2.compute.internal>\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_zip_arrow(ray_start_regular_shared):",
            "def test_batch_tensors(ray_start_regular_shared):",
            "import torch",
            "",
            "-    ds = ray.data.from_items([torch.tensor([0, 0]) for _ in range(40)])",
            "+    ds = ray.data.from_items([torch.tensor([0, 0]) for _ in range(40)], parallelism=40)",
            "res = \"Dataset(num_blocks=40, num_rows=40, schema=<class 'torch.Tensor'>)\"",
            "assert str(ds) == res, str(ds)",
            "with pytest.raises(pa.lib.ArrowInvalid):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1110100)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1110101)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'parallelism'), position=0, insert_id=1110102)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1110103)",
            "Insert(target_node=IN(type=keyword_argument), node=('integer', '40'), position=2, insert_id=1110104)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 8766,
        "neg_line": [
            "-ds = ray.data.from_items([torch.tensor([0, 0]) for _ in range(40)])"
        ],
        "pos_line": [
            "+ds = ray.data.from_items([torch.tensor([0, 0]) for _ in range(40)], parallelism=40)"
        ],
        "core_change": "-ds = ray.data.from_items([torch.tensor([0, 0]) for _ in range(40)]) +ds = ray.data.from_items([torch.tensor([0, 0]) for _ in range(40)], parallelism=40)",
        "core_API": "from_items"
    },
    {
        "commit_hash": "1a79d595f7eda9dc9dc8428f4461680ed2222ab6",
        "index": "b671a26f..e0dbbaa7 100644",
        "commit_message": "TensorFlow Version 1 Compatibility Fix (#1538)\n\n* tf v1 compat fix\n\n* fix lint error for unused module\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class EvalCallback(Callback):",
            "self.dataflow = get_eval_dataflow(self._eval_dataset,",
            "shard=hvd.local_rank(), num_shards=hvd.local_size())",
            "",
            "-            self.barrier = hvd.allreduce(tf.random_normal(shape=[1]))",
            "+            self.barrier = hvd.allreduce(tfv1.random_normal(shape=[1]))",
            "",
            "def _build_predictor(self, idx):",
            "return self.trainer.get_predictor(self._in_names, self._out_names, device=idx)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tf), value='tfv1')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 8771,
        "neg_line": [
            "-self.barrier = hvd.allreduce(tf.random_normal(shape=[1]))"
        ],
        "pos_line": [
            "+self.barrier = hvd.allreduce(tfv1.random_normal(shape=[1]))"
        ],
        "core_change": "-self.barrier = hvd.allreduce(tf.random_normal(shape=[1])) +self.barrier = hvd.allreduce(tfv1.random_normal(shape=[1]))",
        "core_API": "local_rank"
    },
    {
        "commit_hash": "a53843317f2027a3d6333752d9cc64a36209f4fb",
        "index": "8a74d056..17caa0f9 100755",
        "commit_message": "fixed conv2d\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def dense_layer(x, size, l2_regularization=0.0):",
            "",
            "def conv2d_layer(x, size, l2_regularization=0.0, window=3, stride=1):",
            "with tf.variable_scope('conv2d'):",
            "-        filters = tf.Variable(initial_value=tf.random_normal(shape=(window, window, x.get_shape()[2].value, size), stddev=sqrt(2.0 / size))),",
            "+        filters = tf.Variable(initial_value=tf.random_normal(shape=(window, window, x.get_shape()[3].value, size), stddev=sqrt(2.0 / size)))",
            "+",
            "if l2_regularization > 0.0:",
            "tf.losses.add_loss(l2_regularization * tf.nn.l2_loss(t=filters))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=call), position=2)",
            "Update(target_node=ASTNode(type=integer, text=2), value='3')",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=expression_list))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 8774,
        "neg_line": [
            "-filters = tf.Variable(initial_value=tf.random_normal(shape=(window, window, x.get_shape()[2].value, size), stddev=sqrt(2.0 / size))),"
        ],
        "pos_line": [
            "+filters = tf.Variable(initial_value=tf.random_normal(shape=(window, window, x.get_shape()[3].value, size), stddev=sqrt(2.0 / size)))",
            "+"
        ],
        "core_change": "-filters = tf.Variable(initial_value=tf.random_normal(shape=(window, window, x.get_shape()[2].value, size), stddev=sqrt(2.0 / size))), +filters = tf.Variable(initial_value=tf.random_normal(shape=(window, window, x.get_shape()[3].value, size), stddev=sqrt(2.0 / size))) +",
        "core_API": "variable_scope"
    },
    {
        "commit_hash": "ed63b7ee9e68ba120893aaaaca0d4525a56ed484",
        "index": "cab87939..494a83c4 100644",
        "commit_message": "upgrade to pytorch 0.4.0 (#1126)\n\n* bump pytorch to 0.4 + fix sanitize\n\n* remove check for Variable in block_orthogonal\n\n* rename parameter split_size=\n\n* remove checks for Variable\n\n* fix more tests\n\n* fixes\n\n* get tests to pass\n\n* fix warnings\n\n* get rid of some of the Variables\n\n* more tests passing\n\n* more elimination of variables\n\n* finish removing all Variables\n\n* pylint and such\n\n* a few fixes\n\n* move torch.no_grad into model.forward_on_instances\n\n* more pytorch 0.4 changes\n\n* detach() -> data\n\n* pylint\n\n* fix bad tensor creation\n\n* fix types\n\n* remove print statement\n\n* more 0.4 goodness\n\n* factor out is_tensor\n\n* add no_grad to elmo command\n\n* cleanup\n\n* remove TODO\n\n* address PR feedback\n\n* replace all() with item()\n\n* more cleanup\n\n* further cleanup\n\n* remove Variable\n\n* really fix merge conflict\n\n* fix pylint\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestMultiHeadedSimilarityFunction(AllenNlpTestCase):",
            "similarity = MultiHeadedSimilarity(num_heads=3, tensor_1_dim=6)",
            "similarity._tensor_1_projection = Parameter(torch.eye(6))",
            "similarity._tensor_2_projection = Parameter(torch.eye(6))",
            "-        a_vectors = Variable(torch.FloatTensor([[[[1, 1, -1, -1, 0, 1], [-2, 5, 9, -1, 3, 4]]]]))",
            "-        b_vectors = Variable(torch.FloatTensor([[[[1, 1, 1, 0, 2, 5], [0, 1, -1, -7, 1, 2]]]]))",
            "+        a_vectors = torch.FloatTensor([[[[1, 1, -1, -1, 0, 1], [-2, 5, 9, -1, 3, 4]]]])",
            "+        b_vectors = torch.FloatTensor([[[[1, 1, 1, 0, 2, 5], [0, 1, -1, -7, 1, 2]]]])",
            "result = similarity(a_vectors, b_vectors).data.numpy()",
            "assert result.shape == (1, 1, 2, 3)",
            "assert_almost_equal(result, [[[[2, -1, 5], [5, -2, 11]]]])"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=call), position=2)",
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=call), position=2)",
            "Delete(target_node=ASTNode(type=identifier, text=Variable))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=identifier, text=Variable))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 12,
        "number": 8779,
        "neg_line": [
            "-a_vectors = Variable(torch.FloatTensor([[[[1, 1, -1, -1, 0, 1], [-2, 5, 9, -1, 3, 4]]]]))",
            "-b_vectors = Variable(torch.FloatTensor([[[[1, 1, 1, 0, 2, 5], [0, 1, -1, -7, 1, 2]]]]))"
        ],
        "pos_line": [
            "+a_vectors = torch.FloatTensor([[[[1, 1, -1, -1, 0, 1], [-2, 5, 9, -1, 3, 4]]]])",
            "+b_vectors = torch.FloatTensor([[[[1, 1, 1, 0, 2, 5], [0, 1, -1, -7, 1, 2]]]])"
        ],
        "core_change": "-a_vectors = Variable(torch.FloatTensor([[[[1, 1, -1, -1, 0, 1], [-2, 5, 9, -1, 3, 4]]]])) -b_vectors = Variable(torch.FloatTensor([[[[1, 1, 1, 0, 2, 5], [0, 1, -1, -7, 1, 2]]]])) +a_vectors = torch.FloatTensor([[[[1, 1, -1, -1, 0, 1], [-2, 5, 9, -1, 3, 4]]]]) +b_vectors = torch.FloatTensor([[[[1, 1, 1, 0, 2, 5], [0, 1, -1, -7, 1, 2]]]])",
        "core_API": "eye"
    },
    {
        "commit_hash": "1cd25ccf0d3842fc954263f5d538827cfb82e040",
        "index": "ae3a7df7..188a3acf 100644",
        "commit_message": "bug fix\n\n",
        "file": "TTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def text_to_seqvec(text, CONFIG, use_cuda):",
            "def numpy_to_torch(np_array, dtype, cuda=False):",
            "if np_array is None:",
            "return None",
            "-    tensor = torch.Tensor(np_array, dtype=dtype)",
            "+    tensor = torch.as_tensor(np_array, dtype=dtype)",
            "if cuda:",
            "return tensor.cuda()",
            "return tensor"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=Tensor), value='as_tensor')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 8784,
        "neg_line": [
            "-tensor = torch.Tensor(np_array, dtype=dtype)"
        ],
        "pos_line": [
            "+tensor = torch.as_tensor(np_array, dtype=dtype)"
        ],
        "core_change": "-tensor = torch.Tensor(np_array, dtype=dtype) +tensor = torch.as_tensor(np_array, dtype=dtype)",
        "core_API": "Tensor"
    },
    {
        "commit_hash": "43043ee4d5c672f7fbc22ac9559e8164731fd053",
        "index": "0a536c7d26..a0fa41c2bb 100644",
        "commit_message": "[RLlib] Tf2x preparation; part 2 (upgrading `try_import_tf()`). (#9136)\n\n* WIP.\n\n* Fixes.\n\n* LINT.\n\n* WIP.\n\n* WIP.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* WIP.\n\n* Fixes.\n\n* Test\n\n* Fix.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* LINT.\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class DeprecatedCustomLossModelV1(Model):",
            "",
            "def _build_layers_v2(self, input_dict, num_outputs, options):",
            "self.obs_in = input_dict[\"obs\"]",
            "-        with tf.variable_scope(\"shared\", reuse=tf.AUTO_REUSE):",
            "+        with tf1.variable_scope(\"shared\", reuse=tf1.AUTO_REUSE):",
            "self.fcnet = FullyConnectedNetwork(input_dict, self.obs_space,",
            "self.action_space, num_outputs,",
            "options)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tf), value='tf1')",
            "Update(target_node=ASTNode(type=identifier, text=tf), value='tf1')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 8787,
        "neg_line": [
            "-with tf.variable_scope(\"shared\", reuse=tf.AUTO_REUSE):"
        ],
        "pos_line": [
            "+with tf1.variable_scope(\"shared\", reuse=tf1.AUTO_REUSE):"
        ],
        "core_change": "-with tf.variable_scope(\"shared\", reuse=tf.AUTO_REUSE): +with tf1.variable_scope(\"shared\", reuse=tf1.AUTO_REUSE):",
        "core_API": "variable_scope"
    },
    {
        "commit_hash": "7608192a4c1b47cb8594092561fac9877004b917",
        "index": "e3ecd261..8fb66df2 100644",
        "commit_message": "fix tests for PyTorch 1.10\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_pdn_conv():",
            "",
            "t = '(Tensor, SparseTensor, OptTensor) -> Tensor'",
            "jit = torch.jit.script(conv.jittable(t))",
            "-    assert torch.allclose(jit(x, adj.t()), out)",
            "+    assert torch.allclose(jit(x, adj.t()), out, atol=1e-6)",
            "",
            "",
            "def test_pdn_conv_with_sparse_node_input_feature():"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=994629)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=994630)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'atol'), position=0, insert_id=994631)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=994632)",
            "Insert(target_node=IN(type=keyword_argument), node=('float', '1e-6'), position=2, insert_id=994633)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 8789,
        "neg_line": [
            "-assert torch.allclose(jit(x, adj.t()), out)"
        ],
        "pos_line": [
            "+assert torch.allclose(jit(x, adj.t()), out, atol=1e-6)"
        ],
        "core_change": "-assert torch.allclose(jit(x, adj.t()), out) +assert torch.allclose(jit(x, adj.t()), out, atol=1e-6)",
        "core_API": "script"
    },
    {
        "commit_hash": "6d5b199c791e608a99fa3e313624e2c4c7bba736",
        "index": "e3af5789..afdd4b91 100644",
        "commit_message": "fix many pylint issues.\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class SpatialTransformer2dAffineLayer(Layer):",
            "else:",
            "from tensorflow.python.ops import array_ops",
            "batch_size = array_ops.shape(self.inputs)[0]",
            "-            size = self.inputs.get_shape().as_list()",
            "+",
            "n_channels = self.inputs.get_shape().as_list()[-1]",
            "# logging.info(self.outputs)",
            "self.outputs = tf.reshape(self.outputs, shape=[batch_size, out_size[0], out_size[1], n_channels])"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=size))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=inputs))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=get_shape))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=as_list))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 22,
        "number": 8790,
        "neg_line": [
            "-size = self.inputs.get_shape().as_list()"
        ],
        "pos_line": [
            "+"
        ],
        "core_change": "-size = self.inputs.get_shape().as_list() +",
        "core_API": "shape"
    },
    {
        "commit_hash": "462b1694ea95e09f46695dc30c2587f85d196806",
        "index": "4dd1bde..1c47668 100644",
        "commit_message": "fix little bug, the correct tf.concat api first parameter is dim, not values\n\n",
        "file": "tflearn.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def reshape(incoming, new_shape, name=\"Reshape\"):",
            "with tf.name_scope(name) as scope:",
            "inference = incoming",
            "if isinstance(inference, list):",
            "-            inference = tf.concat(inference, 0)",
            "+            inference = tf.concat(0, inference)",
            "inference = tf.cast(inference, tf.float32)",
            "inference = tf.reshape(inference, shape=new_shape)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=identifier, text=inference), node=ASTNode(type=argument_list), position=3)",
            "Move(target_node=ASTNode(type=,, text=,), node=ASTNode(type=argument_list), position=4)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 8791,
        "neg_line": [
            "-inference = tf.concat(inference, 0)"
        ],
        "pos_line": [
            "+inference = tf.concat(0, inference)"
        ],
        "core_change": "-inference = tf.concat(inference, 0) +inference = tf.concat(0, inference)",
        "core_API": "name_scope"
    },
    {
        "commit_hash": "5d7388d43bb59b005e9e6fbdbceb9ee691d54484",
        "index": "70d61b696..e902151ec 100644",
        "commit_message": "Fix when _stable_1d_sort to work when n >= N (#6177)\n\n* Fix when _stable_1d_sort to work when n >= N\n\n* Apply suggestions\n\nCo-authored-by: Carlos Mocholi <carlossmocholi@gmail.com>\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestAUC(MetricTester):",
            "])",
            "def test_auc(x, y, expected):",
            "# Test Area Under Curve (AUC) computation",
            "-    assert auc(torch.tensor(x), torch.tensor(y)) == expected",
            "+    assert auc(torch.tensor(x), torch.tensor(y), reorder=True) == expected"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=544670)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=544671)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'reorder'), position=0, insert_id=544672)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=544673)",
            "Insert(target_node=IN(type=keyword_argument), node=('true', 'True'), position=2, insert_id=544674)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 8794,
        "neg_line": [
            "-assert auc(torch.tensor(x), torch.tensor(y)) == expected"
        ],
        "pos_line": [
            "+assert auc(torch.tensor(x), torch.tensor(y), reorder=True) == expected"
        ],
        "core_change": "-assert auc(torch.tensor(x), torch.tensor(y)) == expected +assert auc(torch.tensor(x), torch.tensor(y), reorder=True) == expected",
        "core_API": "tensor"
    },
    {
        "commit_hash": "00112c35393e4d81ef7593a3763dc626c0403e7b",
        "index": "c6e71b9d4..1e330f4cc 100644",
        "commit_message": "Replace swish with silu (#8166)\n\n* Replace swish with silu\n\n* revert nn.silu to nn.swish due to older version\n\n* simplify optimized silu conditional and fix format\n\n* Update activations.py\n\n* Update activations_tf.py\n\n* Update modeling_flax_utils.py\n\n* Update modeling_openai.py\n\n* add swish testcase\n\n* add pytorch swish testcase\n\n* Add more robust python version check\n\n* more formatting fixes\n\nCo-authored-by: TFUsers <TFUsers@gmail.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "keras",
        "change": [
            "ACT2FN = {",
            "\"gelu\": tf.keras.layers.Activation(gelu),",
            "\"relu\": tf.keras.activations.relu,",
            "\"swish\": tf.keras.activations.swish,",
            "+    \"silu\": tf.keras.activations.swish,",
            "\"gelu_new\": tf.keras.layers.Activation(gelu_new),",
            "\"mish\": tf.keras.layers.Activation(mish),",
            "\"tanh\": tf.keras.activations.tanh,"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=ERROR), node=('pair', None), position=9, insert_id=2555852)",
            "Insert(target_node=ASTNode(type=ERROR), node=(',', ','), position=10, insert_id=2555853)",
            "Insert(target_node=IN(type=pair), node=('string', '\"silu\"'), position=0, insert_id=2555854)",
            "Insert(target_node=IN(type=pair), node=(':', ':'), position=1, insert_id=2555855)",
            "Insert(target_node=IN(type=pair), node=('attribute', None), position=2, insert_id=2555856)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2555857)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2555858)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'swish'), position=2, insert_id=2555859)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2555860)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2555861)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'activations'), position=2, insert_id=2555862)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2555863)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2555864)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'keras'), position=2, insert_id=2555865)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 14,
        "number": 8800,
        "neg_line": [],
        "pos_line": [
            "+\"silu\": tf.keras.activations.swish,"
        ],
        "core_change": "+\"silu\": tf.keras.activations.swish,",
        "core_API": "Activation"
    },
    {
        "commit_hash": "c10f0d12bbdb7e8b9dcf674d2feab50ece5815db",
        "index": "aa338b3..c7d27e2 100644",
        "commit_message": "Fix typo when checking nans\n",
        "file": "Pytorch-UNet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def train_model(",
            "tag = tag.replace('/', '.')",
            "if not (torch.isinf(value) | torch.isnan(value)).any():",
            "histograms['Weights/' + tag] = wandb.Histogram(value.data.cpu())",
            "-                            if not (torch.isinf(value.grad) | torch.isnan(value)).any():",
            "+                            if not (torch.isinf(value.grad) | torch.isnan(value.grad)).any():",
            "histograms['Gradients/' + tag] = wandb.Histogram(value.grad.data.cpu())",
            "",
            "val_score = evaluate(model, val_loader, device, amp)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=1, insert_id=903892)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=value), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=903893)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'grad'), position=2, insert_id=903894)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 8801,
        "neg_line": [
            "-if not (torch.isinf(value.grad) | torch.isnan(value)).any():"
        ],
        "pos_line": [
            "+if not (torch.isinf(value.grad) | torch.isnan(value.grad)).any():"
        ],
        "core_change": "-if not (torch.isinf(value.grad) | torch.isnan(value)).any(): +if not (torch.isinf(value.grad) | torch.isnan(value.grad)).any():",
        "core_API": "replace"
    },
    {
        "commit_hash": "bc4552463085a97639d3a7b03363d57c63907c1d",
        "index": "12501118..09cfe0e0 100755",
        "commit_message": "fixed TF seed problem, improved seed unittest\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Agent(object):",
            "assert isinstance(seed, int)",
            "random.seed(a=seed)",
            "np.random.seed(seed=seed)",
            "-            tf.random.set_random_seed(seed=seed)",
            "",
            "# States/actions specification",
            "self.states_spec = util.valid_values_spec("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=random))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=set_random_seed))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=seed))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=seed))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 16,
        "number": 8803,
        "neg_line": [
            "-tf.random.set_random_seed(seed=seed)"
        ],
        "pos_line": [],
        "core_change": "-tf.random.set_random_seed(seed=seed)",
        "core_API": "seed"
    },
    {
        "commit_hash": "731ea53c803ce00fbfd7531b4f1c6bf263a1f6b4",
        "index": "c4e180f..a273fa5 100644",
        "commit_message": "Llff & blender convention fix\n\nSummary: Images were coming out in the wrong format.\n\nReviewed By: shapovalov\n\nDifferential Revision: D37291278\n\nfbshipit-source-id: c10871c37dd186982e7abf2071ac66ed583df2e6\n\n",
        "file": "pytorch3d.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class LlffDatasetMapProvider(SingleSceneDatasetMapProviderBase):",
            "i_split = (i_train, i_test, i_test)",
            "H, W, focal = hwf",
            "H, W = int(H), int(W)",
            "-        images = torch.from_numpy(images)",
            "+        images = torch.from_numpy(images).permute(0, 3, 1, 2)",
            "poses = torch.from_numpy(poses)",
            "",
            "# pyre-ignore[16]"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=912246)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=912247)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=912248)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'permute'), position=2, insert_id=912249)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=912250)",
            "Insert(target_node=IN(type=argument_list), node=('integer', '0'), position=1, insert_id=912251)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=912252)",
            "Insert(target_node=IN(type=argument_list), node=('integer', '3'), position=3, insert_id=912253)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=4, insert_id=912254)",
            "Insert(target_node=IN(type=argument_list), node=('integer', '1'), position=5, insert_id=912255)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=6, insert_id=912256)",
            "Insert(target_node=IN(type=argument_list), node=('integer', '2'), position=7, insert_id=912257)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=8, insert_id=912258)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 14,
        "number": 8804,
        "neg_line": [
            "-images = torch.from_numpy(images)"
        ],
        "pos_line": [
            "+images = torch.from_numpy(images).permute(0, 3, 1, 2)"
        ],
        "core_change": "-images = torch.from_numpy(images) +images = torch.from_numpy(images).permute(0, 3, 1, 2)",
        "core_API": "from_numpy"
    },
    {
        "commit_hash": "0ba7472da92a89af9aca84de5b01a228d18340a2",
        "index": "f059eefb9..12c4eebb3 100644",
        "commit_message": "[Testing] Fix LINT/sphinx errors. (#8874)\n\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "torch, nn = try_import_torch()",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "-class FullyConnectedNetwork(TorchModelV2):",
            "+class FullyConnectedNetwork(TorchModelV2, nn.Module):",
            "\"\"\"Generic fully connected network.\"\"\"",
            "",
            "def __init__(self, obs_space, action_space, num_outputs, model_config,",
            "name):",
            "TorchModelV2.__init__(self, obs_space, action_space, num_outputs,",
            "model_config, name)",
            "+        nn.Module.__init__(self)",
            "",
            "activation = get_activation_fn(",
            "model_config.get(\"fcnet_activation\"), framework=\"torch\")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=6, insert_id=1506396)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=1506397)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1506398)",
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=3, insert_id=1506399)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1506400)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1506401)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'nn'), position=0, insert_id=1506402)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1506403)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Module'), position=2, insert_id=1506404)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1506405)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1506406)",
            "Insert(target_node=IN(type=attribute), node=('identifier', '__init__'), position=2, insert_id=1506407)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1506408)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'self'), position=1, insert_id=1506409)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1506410)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'nn'), position=0, insert_id=1506411)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1506412)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Module'), position=2, insert_id=1506413)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 18,
        "number": 8805,
        "neg_line": [
            "-class FullyConnectedNetwork(TorchModelV2):"
        ],
        "pos_line": [
            "+class FullyConnectedNetwork(TorchModelV2, nn.Module):",
            "+nn.Module.__init__(self)"
        ],
        "core_change": "-class FullyConnectedNetwork(TorchModelV2): +class FullyConnectedNetwork(TorchModelV2, nn.Module): +nn.Module.__init__(self)",
        "core_API": "getLogger"
    },
    {
        "commit_hash": "0a8d873cc3538904d92b0ad65eb7bdfb08abcad0",
        "index": "6b28df370c..f56349e100 100644",
        "commit_message": "type hints and small fixes in conversions\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def slogdet(",
            "/,",
            ") -> NamedTuple:",
            "results = NamedTuple(",
            "-        \"slogdet\",",
            "-        [(\"sign\", torch.Tensor), (\"logabsdet\", torch.Tensor)]",
            "+        \"slogdet\", [(\"sign\", torch.Tensor), (\"logabsdet\", torch.Tensor)]",
            ")",
            "sign, logabsdet = torch.linalg.slogdet(x)",
            "return results(sign, logabsdet)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 8810,
        "neg_line": [
            "-\"slogdet\",",
            "-[(\"sign\", torch.Tensor), (\"logabsdet\", torch.Tensor)]"
        ],
        "pos_line": [
            "+\"slogdet\", [(\"sign\", torch.Tensor), (\"logabsdet\", torch.Tensor)]"
        ],
        "core_change": "-\"slogdet\", -[(\"sign\", torch.Tensor), (\"logabsdet\", torch.Tensor)] +\"slogdet\", [(\"sign\", torch.Tensor), (\"logabsdet\", torch.Tensor)]",
        "core_API": "slogdet"
    },
    {
        "commit_hash": "ff98d7df1d8c90dc2841064dd576ec9488c9f534",
        "index": "07133841..a9236b02 100644",
        "commit_message": "fix linear layer into tag space predictions\n\n",
        "file": "flair.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ViterbiDecoder:",
            "self.start_tag = tag_dictionary.get_idx_for_item(START_TAG)",
            "self.stop_tag = tag_dictionary.get_idx_for_item(STOP_TAG)",
            "",
            "-    def decode(self, features: torch.Tensor, lengths: torch.Tensor) -> List:",
            "+    def decode(self, features_tuple: tuple) -> List:",
            "\"\"\"",
            "Decoding function returning the most likely sequence of tags.",
            ":param features: CRF scores from CRF forward method in shape (batch size, seq len, tagset size, tagset size)",
            ":param lengths: lengths tuple containing sorted lengths and indices from unsorted list",
            ":return: decoded sequences",
            "\"\"\"",
            "+        features, lengths = features_tuple",
            "+",
            "tags = []",
            "batch_size = features.size(0)",
            "seq_len = features.size(1)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=5, insert_id=235377)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=235378)",
            "Insert(target_node=IN(type=assignment), node=('pattern_list', None), position=0, insert_id=235379)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=235380)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'features_tuple'), position=2, insert_id=235381)",
            "Update(target_node=ASTNode(type=identifier, text=features), value='features_tuple')",
            "Insert(target_node=IN(type=pattern_list), node=('identifier', 'features'), position=0, insert_id=235382)",
            "Insert(target_node=IN(type=pattern_list), node=(',', ','), position=1, insert_id=235383)",
            "Insert(target_node=IN(type=pattern_list), node=('identifier', 'lengths'), position=2, insert_id=235384)",
            "Update(target_node=ASTNode(type=identifier, text=torch), value='tuple')",
            "Move(target_node=ASTNode(type=type), node=ASTNode(type=identifier, text=torch), position=0)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=Tensor))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=lengths))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=Tensor))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=type))",
            "Delete(target_node=ASTNode(type=typed_parameter))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 23,
        "number": 8814,
        "neg_line": [
            "-def decode(self, features: torch.Tensor, lengths: torch.Tensor) -> List:"
        ],
        "pos_line": [
            "+def decode(self, features_tuple: tuple) -> List:",
            "+features, lengths = features_tuple",
            "+"
        ],
        "core_change": "-def decode(self, features: torch.Tensor, lengths: torch.Tensor) -> List: +def decode(self, features_tuple: tuple) -> List: +features, lengths = features_tuple +",
        "core_API": "get_idx_for_item"
    },
    {
        "commit_hash": "81b73f97a33118f7c3a6b7996a4dabaea6221b50",
        "index": "da0abda..424584b 100644",
        "commit_message": "Optiimzation (#355)\n\n* constant folding into kernels\n\n* that opt worth it?\n\n* fix mypy\n\n* ast one kernel\n\n* save 2 lines in conv kernel\n\n* debug print kernel count\n\n* cl debugging\n\n* early realize inputs\n\n* refactor Device\n",
        "file": "tinygrad.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TorchBuffer(torch.Tensor):",
            "def custompad(x, padding): return torch.nn.functional.pad(x, [item for sublist in padding[::-1] for item in sublist])",
            "",
            "@staticmethod",
            "-  def fromCPU(data):",
            "-    return TorchBuffer(torch.from_numpy(data).requires_grad_(False)).to(device)",
            "-  def toCPU(x):",
            "-    return x.cpu().numpy()",
            "+  def fromCPU(data): return TorchBuffer(torch.from_numpy(data).requires_grad_(False)).to(device)",
            "+  def toCPU(x): return x.cpu().numpy()",
            "",
            "unary_op, binary_op, reduce_op, movement_op = CPUBuffer.unary_op, CPUBuffer.binary_op, CPUBuffer.reduce_op, CPUBuffer.movement_op"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 4,
        "AST_diff_line": 17,
        "number": 8815,
        "neg_line": [
            "-def fromCPU(data):",
            "-return TorchBuffer(torch.from_numpy(data).requires_grad_(False)).to(device)",
            "-def toCPU(x):",
            "-return x.cpu().numpy()"
        ],
        "pos_line": [
            "+def fromCPU(data): return TorchBuffer(torch.from_numpy(data).requires_grad_(False)).to(device)",
            "+def toCPU(x): return x.cpu().numpy()"
        ],
        "core_change": "-def fromCPU(data): -return TorchBuffer(torch.from_numpy(data).requires_grad_(False)).to(device) -def toCPU(x): -return x.cpu().numpy() +def fromCPU(data): return TorchBuffer(torch.from_numpy(data).requires_grad_(False)).to(device) +def toCPU(x): return x.cpu().numpy()",
        "core_API": "pad"
    },
    {
        "commit_hash": "7556cc1e2b8eea86fda8025b489e303f5ba8c6ad",
        "index": "d71a25b5..efda3db6 100644",
        "commit_message": "[FasterRCNN] remove unused arg; fix bug in decode bbox\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def decode_bbox_target(box_predictions, anchors):",
            "xbyb = box_pred_txty * waha + xaya",
            "x1y1 = xbyb - wbhb * 0.5",
            "x2y2 = xbyb + wbhb * 0.5    # (...)x1x2",
            "-    out = tf.concat([x1y1, x2y2], axis=1)",
            "+    out = tf.concat([x1y1, x2y2], axis=-2)",
            "return tf.reshape(out, orig_shape)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=keyword_argument), node=('unary_operator', '-2'), position=2, insert_id=2292660)",
            "Delete(target_node=ASTNode(type=integer, text=1))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 8823,
        "neg_line": [
            "-out = tf.concat([x1y1, x2y2], axis=1)"
        ],
        "pos_line": [
            "+out = tf.concat([x1y1, x2y2], axis=-2)"
        ],
        "core_change": "-out = tf.concat([x1y1, x2y2], axis=1) +out = tf.concat([x1y1, x2y2], axis=-2)",
        "core_API": "concat"
    },
    {
        "commit_hash": "edb78974b84b324df08ec198e6ff7f267ee23d33",
        "index": "721197fe..365f4712 100644",
        "commit_message": "upgrade detection / some v2 compat fix\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def _replace_global_by_local(kwargs):",
            "if 'collections' in kwargs:",
            "collections = kwargs['collections']",
            "if not collections:",
            "-        collections = {tf.GraphKeys.GLOBAL_VARIABLES}",
            "+        collections = {tfv1.GraphKeys.GLOBAL_VARIABLES}",
            "else:",
            "collections = set(collections.copy())",
            "-    collections.remove(tf.GraphKeys.GLOBAL_VARIABLES)",
            "-    collections.add(tf.GraphKeys.LOCAL_VARIABLES)",
            "+    collections.remove(tfv1.GraphKeys.GLOBAL_VARIABLES)",
            "+    collections.add(tfv1.GraphKeys.LOCAL_VARIABLES)",
            "kwargs['collections'] = list(collections)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tf), value='tfv1')",
            "Update(target_node=ASTNode(type=identifier, text=tf), value='tfv1')",
            "Update(target_node=ASTNode(type=identifier, text=tf), value='tfv1')"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 3,
        "number": 8824,
        "neg_line": [
            "-collections = {tf.GraphKeys.GLOBAL_VARIABLES}",
            "-collections.remove(tf.GraphKeys.GLOBAL_VARIABLES)",
            "-collections.add(tf.GraphKeys.LOCAL_VARIABLES)"
        ],
        "pos_line": [
            "+collections = {tfv1.GraphKeys.GLOBAL_VARIABLES}",
            "+collections.remove(tfv1.GraphKeys.GLOBAL_VARIABLES)",
            "+collections.add(tfv1.GraphKeys.LOCAL_VARIABLES)"
        ],
        "core_change": "-collections = {tf.GraphKeys.GLOBAL_VARIABLES} +collections = {tfv1.GraphKeys.GLOBAL_VARIABLES} -collections.remove(tf.GraphKeys.GLOBAL_VARIABLES) -collections.add(tf.GraphKeys.LOCAL_VARIABLES) +collections.remove(tfv1.GraphKeys.GLOBAL_VARIABLES) +collections.add(tfv1.GraphKeys.LOCAL_VARIABLES)",
        "core_API": "copy"
    }
]