{"number": 4, "change": "class TrainerIntegrationTest(unittest.TestCase):\n\n# Adding one column not used by the model should have no impact\nz = np.random.normal(size=(64,)).astype(np.float32)\n-        train_dataset = nlp.Dataset.from_dict({\"input_x\": x, \"label\": y, \"extra\": z})\n+        train_dataset = datasets.Dataset.from_dict({\"input_x\": x, \"label\": y, \"extra\": z})\nmodel = RegressionModel()\ntrainer = Trainer(model, args, train_dataset=train_dataset)\ntrainer.train()\n", "fix_pattern": "Condition: No specific condition is identified in the given context. \nPattern: The code previously used `nlp.Dataset.from_dict` to create a dataset, and it was causing API misuse. \nCode One: `train_dataset = nlp.Dataset.from_dict({\"input_x\": x, \"label\": y, \"extra\": z})`\nCode Two: `train_dataset = datasets.Dataset.from_dict({\"input_x\": x, \"label\": y, \"extra\": z})`\nFix Pattern: In the condition of no specific condition, if the pattern of using `nlp.Dataset.from_dict` is detected, then change the code from `nlp.Dataset.from_dict` to `datasets.Dataset.from_dict` to fix the API misuse."}

{"number": 7, "change": "def test_quantile():\n\n\ndef test_pi():\n-    x = torch.empty(1000).log_normal_(0, 1)\n+    x = torch.randn(1000).exp()\nassert_equal(pi(x, prob=0.8), quantile(x, probs=[0.1, 0.9]))\n", "fix_pattern": "Condition: The condition is not clear in the given context.\n\nPattern: The pattern is to replace the code that creates an empty tensor with a log-normal distribution with a new code that creates a tensor with a standard normal distribution and applies the exponential function to it.\n\nCode One: The code that creates an empty tensor with a log-normal distribution: `x = torch.empty(1000).log_normal_(0, 1)`\n\nCode Two: The code that creates a tensor with a standard normal distribution and applies the exponential function to it: `x = torch.randn(1000).exp()`\n\nFix Pattern: In the condition of the unknown condition, if the pattern of creating an empty tensor with a log-normal distribution is detected, then replace the code that creates the empty tensor with a new code that creates a tensor with a standard normal distribution and applies the exponential function to it to fix the API misuse."}

{"number": 8, "change": "class TPUAccelerator(Accelerator):\nReturn:\nA tensor of shape (world_size, batch, ...)\n\"\"\"\n-        return xm.all_gather(tensor, group=group, sync_grads=sync_grads)\n+        # todo: Add support for backward with all_gather\n+        if torch.distributed.is_initialized():\n+            return xm.all_gather(tensor, group=group, sync_grads=sync_grads)\n+        return tensor\n", "fix_pattern": "Condition: The code is in a class named \"TPUAccelerator\" that is a subclass of \"Accelerator\".\n\nPattern: The pattern is to remove a specific line of code that calls \"xm.all_gather\" with two arguments, \"group\" and \"sync_grads\".\n\nCode One: The code, \"return xm.all_gather(tensor, group=group, sync_grads=sync_grads)\".\n\nCode Two: The code, \"if torch.distributed.is_initialized():\\n    return xm.all_gather(tensor, group=group, sync_grads=sync_grads)\\nreturn tensor\".\n\nFix Pattern: In the condition of being in the \"TPUAccelerator\" class, if the pattern of calling \"xm.all_gather\" with \"group\" and \"sync_grads\" is detected, then the code one is removed and replaced with the code two to fix the API misuse."}

{"number": 9, "change": "class Swinv2SelfAttention(nn.Module):\nquery_layer = self.transpose_for_scores(mixed_query_layer)\n\n# cosine attention\n-        attention_scores = F.normalize(query_layer, dim=-1) @ F.normalize(key_layer, dim=-1).transpose(-2, -1)\n+        attention_scores = nn.functional.normalize(query_layer, dim=-1) @ nn.functional.normalize(\n+            key_layer, dim=-1\n+        ).transpose(-2, -1)\nlogit_scale = torch.clamp(self.logit_scale, max=math.log(1.0 / 0.01)).exp()\nattention_scores = attention_scores * logit_scale\nrelative_position_bias_table = self.continuous_position_bias_mlp(self.relative_coords_table).view(\n", "fix_pattern": "<condition>: There is a need to normalize the query_layer and key_layer before calculating the attention_scores.\n<pattern>: The pattern is to replace the F.normalize function with nn.functional.normalize function.\n<code_one>: The code that was removed is \"attention_scores = F.normalize(query_layer, dim=-1) @ F.normalize(key_layer, dim=-1).transpose(-2, -1)\".\n<code_two>: The code that was added is \"attention_scores = nn.functional.normalize(query_layer, dim=-1) @ nn.functional.normalize(key_layer, dim=-1).transpose(-2, -1)\".\nFix_pattern: In the condition of normalizing the query_layer and key_layer, if the code \"attention_scores = F.normalize(query_layer, dim=-1) @ F.normalize(key_layer, dim=-1).transpose(-2, -1)\" is detected, then replace it with \"attention_scores = nn.functional.normalize(query_layer, dim=-1) @ nn.functional.normalize(key_layer, dim=-1).transpose(-2, -1)\" to fix the API misuse."}

{"number": 12, "change": "def get_rotation_matrix2d(\n\n# create output tensor\nbatch_size: int = center.shape[0]\n-    one = torch.tensor(1.).to(center.device)\n+    one = torch.tensor(1., device=center.device, dtype=center.dtype)\nM: torch.Tensor = torch.zeros(\nbatch_size, 2, 3, device=center.device, dtype=center.dtype)\nM[..., 0:2, 0:2] = scaled_rotation\n", "fix_pattern": "<condition>: The condition in this case is when creating an output tensor in the function \"get_rotation_matrix2d\".\n<pattern>: The pattern is the incorrect use of the torch.tensor function without specifying the device and dtype.\n<code_one>: The code removed is \"one = torch.tensor(1.).to(center.device)\".\n<code_two>: The code added is \"one = torch.tensor(1., device=center.device, dtype=center.dtype)\".\nFix_pattern: In the condition of creating an output tensor in \"get_rotation_matrix2d\", if the pattern of an incorrect torch.tensor usage is detected, then the code \"one = torch.tensor(1.).to(center.device)\" should be changed to \"one = torch.tensor(1., device=center.device, dtype=center.dtype)\" to fix the API misuse."}

{"number": 15, "change": "class Trainer(TrainerBase):\nbreak\nsys.stdout.flush()\n\n-        model.load_state_dict(torch.load(best_model_path))\n+        if rank == 0:\n+            model.load_state_dict(torch.load(best_model_path))\nreturn model, best_metric\n\ndef _run_epoch(\n", "fix_pattern": "<condition>: No clear condition is needed.\n<pattern>: The code is modified to conditionally load the model state dictionary based on the value of the variable \"rank\".\n<code_one>: model.load_state_dict(torch.load(best_model_path))\n<code_two>: if rank == 0: model.load_state_dict(torch.load(best_model_path))\nFix_pattern: In the condition of no predefined condition, if the code for loading the model state dictionary is detected, then the code is modified to only load the model state dictionary when the value of the variable \"rank\" is 0 to fix the API misuse."}

{"number": 16, "change": "def test_preprocess_weights_for_loading_gru_incompatible():\n\ndef assert_not_compatible(src, dest, message):\nwith pytest.raises(ValueError) as ex:\n-            keras.engine.topology.preprocess_weights_for_loading(\n+            keras.engine.saving.preprocess_weights_for_loading(\ndest, initialize_weights(src).get_weights())\nassert message in ex.value.message\n", "fix_pattern": "Condition: When we want to preprocess weights for loading in a GRU model.\nPattern: Using the wrong function for preprocessing weights for loading.\nCode_one: keras.engine.topology.preprocess_weights_for_loading.\nCode_two: keras.engine.saving.preprocess_weights_for_loading.\nFix_pattern: In the condition of preprocessing weights for loading in a GRU model, if the wrong function for preprocessing weights is detected, then change the code from \"keras.engine.topology.preprocess_weights_for_loading\" to \"keras.engine.saving.preprocess_weights_for_loading\" to fix the API misuse."}

{"number": 17, "change": "Returns:\n\"\"\"\n\n\n-class BERTScore(nlp.Metric):\n+class BERTScore(datasets.Metric):\ndef _info(self):\n-        return nlp.MetricInfo(\n+        return datasets.MetricInfo(\ndescription=_DESCRIPTION,\ncitation=_CITATION,\nhomepage=\"https://github.com/Tiiiger/bert_score\",\ninputs_description=_KWARGS_DESCRIPTION,\n-            features=nlp.Features(\n+            features=datasets.Features(\n{\n-                    \"predictions\": nlp.Value(\"string\", id=\"sequence\"),\n-                    \"references\": nlp.Sequence(nlp.Value(\"string\", id=\"sequence\"), id=\"references\"),\n+                    \"predictions\": datasets.Value(\"string\", id=\"sequence\"),\n+                    \"references\": datasets.Sequence(datasets.Value(\"string\", id=\"sequence\"), id=\"references\"),\n}\n),\ncodebase_urls=[\"https://github.com/Tiiiger/bert_score\"],\n", "fix_pattern": "<condition>: There is a need to change the codebase from nlp module to datasets module.\n<pattern>: The class BERTScore needs to be updated.\n<code_one>: nlp.MetricInfo(...)\n<code_two>: datasets.MetricInfo(...)\nFix_pattern: In the condition of changing the codebase from nlp module to datasets module, the class BERTScore needs to be updated by changing nlp.MetricInfo(...) to datasets.MetricInfo(...)."}

{"number": 18, "change": "class CoarseMaskHead(FCNMaskHead):\nfor i in range(num_fcs):\nfc_in_channels = (\nlast_layer_dim if i == 0 else self.fc_out_channels)\n-            self.fcs.append(nn.Linear(fc_in_channels, self.fc_out_channels))\n+            self.fcs.append(Linear(fc_in_channels, self.fc_out_channels))\nlast_layer_dim = self.fc_out_channels\noutput_channels = self.num_classes * self.output_area\n-        self.fc_logits = nn.Linear(last_layer_dim, output_channels)\n+        self.fc_logits = Linear(last_layer_dim, output_channels)\n\ndef init_weights(self):\nfor m in self.fcs.modules():\n", "fix_pattern": "<condition>: The condition is in the context section, which is \"class CoarseMaskHead(FCNMaskHead)\".\n<pattern>: The pattern is the removal of \"nn.\" before \"Linear\" in the code removed section.\n<code_one>: The code one is \"nn.Linear\".\n<code_two>: The code two is \"Linear\".\nFix_pattern: In the condition of \"class CoarseMaskHead(FCNMaskHead)\", if the pattern of removing \"nn.\" from \"nn.Linear\" is detected, then the code \"nn.Linear\" should be changed to \"Linear\" to fix the API misuse."}

{"number": 12, "change": "def get_rotation_matrix2d(\n\n# create output tensor\nbatch_size: int = center.shape[0]\n-    one = torch.tensor(1.).to(center.device)\n+    one = torch.tensor(1., device=center.device, dtype=center.dtype)\nM: torch.Tensor = torch.zeros(\nbatch_size, 2, 3, device=center.device, dtype=center.dtype)\nM[..., 0:2, 0:2] = scaled_rotation\n", "fix_pattern": "<condition>: there is no clear condition identified in the context.\n<pattern>: the pattern is detecting the creation of a tensor with value 1.\n<code_one>: the code removed is the line of code that creates a tensor with the value 1.\n<code_two>: the code added is the line of code that creates a tensor with the value 1, specifying the device and dtype.\nFix_pattern: In the condition of no clear condition needed, if the creation of a tensor with the value 1 is detected, then the line of code that creates the tensor should be changed to specify the device and dtype."}
{"number": 15, "change": "class Trainer(TrainerBase):\nbreak\nsys.stdout.flush()\n\n-        model.load_state_dict(torch.load(best_model_path))\n+        if rank == 0:\n+            model.load_state_dict(torch.load(best_model_path))\nreturn model, best_metric\n\ndef _run_epoch(\n", "fix_pattern": "Condition: The condition is when the program is running on rank 0.\nPattern: The pattern is that the model's state dictionary is loaded from a file.\nCode One: The code that is removed is \"model.load_state_dict(torch.load(best_model_path))\".\nCode Two: The code that is added is \"if rank == 0: model.load_state_dict(torch.load(best_model_path))\".\nFix Pattern: In the condition of running on rank 0, if the pattern of loading the model's state dictionary is detected, then the code \"model.load_state_dict(torch.load(best_model_path))\" is changed to \"if rank == 0: model.load_state_dict(torch.load(best_model_path))\" to fix the API misuse."}
{"number": 16, "change": "def test_preprocess_weights_for_loading_gru_incompatible():\n\ndef assert_not_compatible(src, dest, message):\nwith pytest.raises(ValueError) as ex:\n-            keras.engine.topology.preprocess_weights_for_loading(\n+            keras.engine.saving.preprocess_weights_for_loading(\ndest, initialize_weights(src).get_weights())\nassert message in ex.value.message\n", "fix_pattern": "<condition>: When preparing weights for loading a GRU model.\n<pattern>: The function for preprocessing weights was called from the wrong module.\n<code_one>: keras.engine.topology.preprocess_weights_for_loading(\n<code_two>: keras.engine.saving.preprocess_weights_for_loading(\nFix_pattern: In the condition of preparing weights for loading a GRU model, if the function \"keras.engine.topology.preprocess_weights_for_loading(\" is detected, then it should be changed to \"keras.engine.saving.preprocess_weights_for_loading(\" to fix the API misuse."}
{"number": 17, "change": "Returns:\n\"\"\"\n\n\n-class BERTScore(nlp.Metric):\n+class BERTScore(datasets.Metric):\ndef _info(self):\n-        return nlp.MetricInfo(\n+        return datasets.MetricInfo(\ndescription=_DESCRIPTION,\ncitation=_CITATION,\nhomepage=\"https://github.com/Tiiiger/bert_score\",\ninputs_description=_KWARGS_DESCRIPTION,\n-            features=nlp.Features(\n+            features=datasets.Features(\n{\n-                    \"predictions\": nlp.Value(\"string\", id=\"sequence\"),\n-                    \"references\": nlp.Sequence(nlp.Value(\"string\", id=\"sequence\"), id=\"references\"),\n+                    \"predictions\": datasets.Value(\"string\", id=\"sequence\"),\n+                    \"references\": datasets.Sequence(datasets.Value(\"string\", id=\"sequence\"), id=\"references\"),\n}\n),\ncodebase_urls=[\"https://github.com/Tiiiger/bert_score\"],\n", "fix_pattern": "Condition: The code is a part of the BERTScore module for natural language processing.\nPattern: The features and sequences are defined using the nlp module.\nCode One: The code uses nlp.MetricInfo() and nlp.Features() to define features and sequences.\nCode Two: The code should be using datasets.MetricInfo() and datasets.Features() instead.\nFix Pattern: In the condition of being a part of the BERTScore module, if the features and sequences are defined using the nlp module, then change the code from using nlp.MetricInfo() and nlp.Features() to using datasets.MetricInfo() and datasets.Features() to fix the API misuse."}
{"number": 18, "change": "class CoarseMaskHead(FCNMaskHead):\nfor i in range(num_fcs):\nfc_in_channels = (\nlast_layer_dim if i == 0 else self.fc_out_channels)\n-            self.fcs.append(nn.Linear(fc_in_channels, self.fc_out_channels))\n+            self.fcs.append(Linear(fc_in_channels, self.fc_out_channels))\nlast_layer_dim = self.fc_out_channels\noutput_channels = self.num_classes * self.output_area\n-        self.fc_logits = nn.Linear(last_layer_dim, output_channels)\n+        self.fc_logits = Linear(last_layer_dim, output_channels)\n\ndef init_weights(self):\nfor m in self.fcs.modules():\n", "fix_pattern": "<condition>: The code is within the \"init_weights\" method of the \"CoarseMaskHead\" class.\n<pattern>: The code is changing the type of the linear layers used from \"nn.Linear\" to \"Linear\".\n<code_one>: \"nn.Linear(fc_in_channels, self.fc_out_channels)\".\n<code_two>: \"Linear(fc_in_channels, self.fc_out_channels)\".\nFix_pattern: In the condition of \"init_weights\" method, if the code \"nn.Linear(fc_in_channels, self.fc_out_channels)\" is detected, then change it to \"Linear(fc_in_channels, self.fc_out_channels)\" to fix the API misuse."}
{"number": 21, "change": "class SequenceTagger(flair.nn.DefaultClassifier):\nfor sentence in batch:\nsentence.remove_labels(label_name)\n\n-            loss = self._calculate_loss(features, gold_labels)\n-\nif return_loss:\n+                loss = self._calculate_loss(features, gold_labels)\noverall_loss += loss[0]\nlabel_count += loss[1]\n", "fix_pattern": "Condition: If the variable 'return_loss' is true.\nPattern: Calling the function '_calculate_loss' with 'features' and 'gold_labels' as parameters.\nCode_one: No code to remove.\nCode_two: Re-add the line 'loss = self._calculate_loss(features, gold_labels)'.\nFix_pattern: In the condition of 'return_loss', if the pattern of calling '_calculate_loss' is detected, then re-add the line for calculating the loss to fix the API misuse."}
{"number": 23, "change": "class TacotronTrainTest(unittest.TestCase):\noptimizer = optim.Adam(model.parameters(), lr=c.lr)\nfor i in range(5):\nmel_out, linear_out, align, stop_tokens = model.forward(\n-                input, input_lengths, mel_spec)\n+                input, input_lengths, mel_spec, speaker_ids)\noptimizer.zero_grad()\nloss = criterion(mel_out, mel_spec, mel_lengths)\nstop_loss = criterion_st(stop_tokens, stop_targets)\n", "fix_pattern": "Condition: The condition is not explicitly mentioned in the given code snippet. \nPattern: The pattern detected is that an additional argument \"speaker_ids\" is added to the function \"model.forward()\". \nCode One: The code \"input, input_lengths, mel_spec\" is removed. \nCode Two: The code \"input, input_lengths, mel_spec, speaker_ids\" is added. \nFix Pattern: In the condition of no specific condition, if an additional argument \"speaker_ids\" is detected, then remove the code \"input, input_lengths, mel_spec\" and add the code \"input, input_lengths, mel_spec, speaker_ids\" to fix the API misuse."}
{"number": 24, "change": "def evaluate(args, model, tokenizer, prefix=\"\", test=False):\neval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n\n# multi-gpu evaluate\n-        if args.n_gpu > 1:\n+        if args.n_gpu > 1 and not isinstance(model, torch.nn.DataParallel):\nmodel = torch.nn.DataParallel(model)\n\n# Eval!\n", "fix_pattern": "<condition>: No clear condition is needed.\n<pattern>: N/A\n<code_one>: if args.n_gpu > 1\n<code_two>: if args.n_gpu > 1 and not isinstance(model, torch.nn.DataParallel)\nFix_pattern: In the condition of if args.n_gpu > 1, if the code pattern of args.n_gpu > 1 is detected, then change the condition to if args.n_gpu > 1 and not isinstance(model, torch.nn.DataParallel) to fix the API misuse."}
{"number": 31, "change": "class StableDiffusionProcessingTxt2Img(StableDiffusionProcessing):\nreturn samples\n\nx = create_random_tensors([opt_C, self.firstphase_height // opt_f, self.firstphase_width // opt_f], seeds=seeds, subseeds=subseeds, subseed_strength=self.subseed_strength, seed_resize_from_h=self.seed_resize_from_h, seed_resize_from_w=self.seed_resize_from_w, p=self)\n-        samples = self.sampler.sample(self, x, conditioning, unconditional_conditioning, image_conditioning=self.create_dummy_mask(x))\n+        samples = self.sampler.sample(self, x, conditioning, unconditional_conditioning, image_conditioning=self.create_dummy_mask(x, first_phase=True))\n\nsamples = samples[:, :, self.truncate_y//2:samples.shape[2]-self.truncate_y//2, self.truncate_x//2:samples.shape[3]-self.truncate_x//2]\n", "fix_pattern": "<condition>: No clear condition needed.\n<pattern>: The code was changed to include the argument \"first_phase=True\" in the function \"create_dummy_mask\".\n<code_one>: \"self.create_dummy_mask(x)\"\n<code_two>: \"self.create_dummy_mask(x, first_phase=True)\"\nFix_pattern: In the condition of no clear condition needed, if the code \"self.create_dummy_mask(x)\" is detected, then it should be changed to \"self.create_dummy_mask(x, first_phase=True)\" to fix the API misuse."}
{"number": 32, "change": "def sort_batch_by_length(tensor: torch.autograd.Variable, sequence_lengths: torc\nsorted_tensor = tensor.index_select(0, permutation_index)\n# This is the equivalent of zipping with index, sorting by the original\n# sequence lengths and returning the now sorted indices.\n-    index_range = Variable(torch.range(0, len(sequence_lengths) - 1).long())\n+    index_range = Variable(torch.arange(0, len(sequence_lengths)).long())\n_, reverse_mapping = permutation_index.sort(0, descending=False)\nrestoration_indices = index_range.index_select(0, reverse_mapping)\nreturn sorted_tensor, sorted_sequence_lengths, restoration_indices\n", "fix_pattern": "<condition>: Condition for the fix pattern is not clearly identified in the context.\n<pattern>: The pattern is to replace the code in <code_one> with the code in <code_two>.\n<code_one>: The code removed is 'index_range = Variable(torch.range(0, len(sequence_lengths) - 1).long())'.\n<code_two>: The code added is 'index_range = Variable(torch.arange(0, len(sequence_lengths)).long())'.\nFix_pattern: In this fix pattern, if the code 'index_range = Variable(torch.range(0, len(sequence_lengths) - 1).long())' is detected, it should be replaced with 'index_range = Variable(torch.arange(0, len(sequence_lengths)).long())' to fix the API misuse."}
{"number": 40, "change": "def makenp(x, modality=None):\n\ndef pytorch_np(x, modality):\nimport torch\n-    if isinstance(x, torch.autograd.variable.Variable):\n+    if isinstance(x, torch.autograd.Variable):\nx = x.data\nx = x.cpu().numpy()\nif modality == 'IMG':\n", "fix_pattern": "<condition>: The condition is when the variable \"x\" is an instance of the torch.autograd.variable.Variable class.\n<pattern>: The pattern is the use of the \"isinstance()\" function to check if \"x\" is an instance of the specified class.\n<code_one>: The code that is removed is \"if isinstance(x, torch.autograd.variable.Variable):\".\n<code_two>: The code that is added is \"if isinstance(x, torch.autograd.Variable):\".\nFix_pattern: In the condition of checking if \"x\" is an instance of a specific class, if the pattern of using \"isinstance()\" is detected, then the \"code_one\" is removed and the \"code_two\" is added to fix the API misuse."}
{"number": 43, "change": "class TrainingTypePlugin(ABC):\nself.lr_schedulers = schedulers\n\ndef _move_optimizer_state(self, device: Optional[torch.device] = None) -> None:\n-        \"\"\"Moves the state of the optimizers to the GPU if needed.\"\"\"\n-        device = device or self.root_device\n+        \"\"\"Moves the state of the optimizers to the appropriate device if needed.\"\"\"\nfor opt in self.optimizers:\nfor p, v in opt.state.items():\n-                opt.state[p] = apply_to_collection(v, torch.Tensor, move_data_to_device, device)\n+                # `self.root_device` would raise error if called outside the spawn process\n+                # while training on 8 and more cores.\n+                opt.state[p] = apply_to_collection(v, torch.Tensor, move_data_to_device, device or self.root_device)\n\ndef optimizer_state(self, optimizer: Optimizer) -> Dict[str, Tensor]:\n\"\"\"Returns state of an optimizer.\n", "fix_pattern": "Condition: The condition for the fix pattern is not specified in the context.\n\nPattern: The pattern is to move the state of the optimizers to the appropriate device if needed.\n\nCode_one: The code that needs to be removed is a statement that moves the state of the optimizers to the GPU.\n\nCode_two: The code that needs to be added is an improved version of the code that moves the optimizer state to the appropriate device.\n\nFix Pattern: In the condition of unspecified, if the pattern of moving optimizer state is detected, then remove the code that moves the state to the GPU and add the code that moves the state to the appropriate device to fix the API misuse."}
{"number": 50, "change": "class GraphConv(MessagePassing):\nself.lin.reset_parameters()\n\ndef forward(self, x, edge_index):\n+        if isinstance(x, Tensor):\n+            x = (x, x)\nreturn self.propagate(edge_index, x=(self.lin(x[0]), x[1]))\n", "fix_pattern": "Condition: There is a condition checking if x is an instance of Tensor.\nPattern: There is a missing check for whether x is an instance of Tensor.\nCode one: No code is removed.\nCode two: The missing check for whether x is an instance of Tensor is added.\nFix_pattern: In the condition of checking if x is an instance of Tensor, if the check is not present, then the missing check for whether x is an instance of Tensor is added to fix the API misuse."}
{"number": 53, "change": "class Trainer:\n).to(self.args.device)\n\nelif is_sagemaker_dp_enabled():\n-            model = DDP(model, device_ids=[dist.get_local_rank()], broadcast_buffers=False)\n+            model = nn.parallel.DistributedDataParallel(\n+                model, device_ids=[int(os.getenv(\"SMDATAPARALLEL_LOCAL_RANK\"))]\n+            )\nelif self.args.local_rank != -1:\nkwargs = {}\nif self.args.ddp_find_unused_parameters is not None:\n", "fix_pattern": "<condition>: self.args.local_rank != -1\n<pattern>: DDP(model, device_ids=[dist.get_local_rank()], broadcast_buffers=False)\n<code_one>: DDP(model, device_ids=[dist.get_local_rank()], broadcast_buffers=False)\n<code_two>: nn.parallel.DistributedDataParallel(model, device_ids=[int(os.getenv(\"SMDATAPARALLEL_LOCAL_RANK\"))])\n\nFix_pattern: In the condition where self.args.local_rank is not equal to -1, if the pattern of DDP(model, device_ids=[dist.get_local_rank()], broadcast_buffers=False) is detected, then remove the code DDP(model, device_ids=[dist.get_local_rank()], broadcast_buffers=False) and add the code nn.parallel.DistributedDataParallel(model, device_ids=[int(os.getenv(\"SMDATAPARALLEL_LOCAL_RANK\"))]) to fix the API misuse."}
{"number": 54, "change": "class Network(object):\n\"\"\"\n@layer\ndef softmax(self, target, axis, name=None):\n-        max_axis = tf.reduce_max(target, axis, keepdims=True)\n+        max_axis = tf.reduce_max(target, axis, keep_dims=True)\ntarget_exp = tf.exp(target-max_axis)\n-        normalize = tf.reduce_sum(target_exp, axis, keepdims=True)\n+        normalize = tf.reduce_sum(target_exp, axis, keep_dims=True)\nsoftmax = tf.div(target_exp, normalize, name)\nreturn softmax\n", "fix_pattern": "Condition: The condition is not clearly identified in the provided context.\n\nPattern: The pattern is the change in the function arguments of the `tf.reduce_max` and `tf.reduce_sum` functions.\n\nCode One: The original code used the argument `keepdims=True`.\n\nCode Two: The fixed code replaced `keepdims=True` with `keep_dims=True`.\n\nFix Pattern: In the condition of the network's `softmax` function, if the pattern of using `keepdims=True` is detected in the `tf.reduce_max` and `tf.reduce_sum` calls, the fix is to change it to `keep_dims=True` to correctly fix the API misuse."}
{"number": 55, "change": "class RNN(torch.nn.Module):\nif not isinstance(ilens, torch.Tensor):\nilens = torch.tensor(ilens)\nxs_pack = pack_padded_sequence(xs_pad, ilens.cpu(), batch_first=True)\n-        self.nbrnn.flatten_parameters()\n+        if self.training:\n+            self.nbrnn.flatten_parameters()\nif prev_state is not None and self.nbrnn.bidirectional:\n# We assume that when previous state is passed,\n# it means that we're streaming the input\n", "fix_pattern": "<condition>: prev_state is not None and self.nbrnn.bidirectional is True\n<pattern>: self.nbrnn.flatten_parameters() is missing in the conditional block.\n<code_one>: self.nbrnn.flatten_parameters()\n<code_two>: if self.training: self.nbrnn.flatten_parameters()\nFix_pattern: In the condition of \"prev_state is not None and self.nbrnn.bidirectional is True\", if the \"self.nbrnn.flatten_parameters()\" is missing, then add \"if self.training: self.nbrnn.flatten_parameters()\" to fix the API misuse."}
{"number": 56, "change": "class TFXLNetModelTest(TFModelTesterMixin, unittest.TestCase):\n# Send to model\nloss = model(tuple_input[:-1])[0]\n\n-                self.assertEqual(loss.shape, [loss_size])\n+                self.assertEqual(loss.shape.as_list(), expected_loss_size)\n\n\n@require_tf\n", "fix_pattern": "Condition: The condition in this fix pattern is not mentioned in the given context.\n\nPattern: The pattern in this fix pattern is to change the method from `shape` to `shape.as_list()`.\n\nCode One: The code that is removed is `self.assertEqual(loss.shape, [loss_size])`.\n\nCode Two: The code that is added is `self.assertEqual(loss.shape.as_list(), expected_loss_size)`.\n\nFix Pattern: In the condition of (no pre-condition is needed), if the pattern of comparing `loss.shape` is detected, then change the code from `self.assertEqual(loss.shape, [loss_size])` to `self.assertEqual(loss.shape.as_list(), expected_loss_size)` to fix the API misuse."}
{"number": 58, "change": "def sigmoid_example(design):\ntorch.tensor([[-1.5, 0.5], [1.5, 0.]])\n),\n(\n-        known_covariance_linear_model(torch.tensor([1., -1.]), torch.tensor(10.), torch.tensor(1.)),\n+        known_covariance_linear_model(torch.tensor([1., -1.]), torch.tensor([10., 10.]), torch.tensor(1.)),\nnz_lm_2p_10_10_1,\ntorch.tensor([[-1., 0.5], [2.5, -2.]])\n),\n", "fix_pattern": "<condition>: This fix pattern is applicable when there is a call to the function \"known_covariance_linear_model\" with specific input arguments in the code.\n<pattern>: The pattern to be detected is the incorrect use of a scalar input for the second argument, instead of a tensor.\n<code_one>: The code that was removed was the incorrect usage of torch.tensor(10.) as the second argument.\n<code_two>: The code that was added is the correct usage of torch.tensor([10., 10.]) as the second argument.\nFix_pattern: In the condition of calling \"known_covariance_linear_model\", if the incorrect input pattern of a scalar instead of a tensor is detected, then the fix is to change the code_one to code_two to correctly use a tensor input."}
{"number": 59, "change": "class DetaModel(DetaPreTrainedModel):\nscale = 2 * math.pi\n\ndim_t = torch.arange(num_pos_feats, dtype=torch.float32, device=proposals.device)\n-        dim_t = temperature ** (2 * (dim_t // 2) / num_pos_feats)\n+        dim_t = temperature ** (2 * torch.div(dim_t, 2) / num_pos_feats)\n# batch_size, num_queries, 4\nproposals = proposals.sigmoid() * scale\n# batch_size, num_queries, 4, 128\n", "fix_pattern": "<condition>: The code is using the sigmoid activation function.\n<pattern>: The code is raising the temperature to the power of a mathematical expression involving dim_t.\n<code_one>: dim_t // 2\n<code_two>: torch.div(dim_t, 2)\nFix_pattern: In the condition of using the sigmoid activation function, if raising the temperature to the power of a mathematical expression involving dim_t is detected, then change dim_t // 2 to torch.div(dim_t, 2) to fix the API misuse."}
{"number": 61, "change": "class LxmertAttention(nn.Module):\nattention_scores = attention_scores + attention_mask\n\n# Normalize the attention scores to probabilities.\n-        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n+        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n\n# This is actually dropping out entire tokens to attend to, which might\n# seem a bit unusual, but is taken from the original Transformer paper.\n", "fix_pattern": "<condition>: There is a need to normalize attention scores to probabilities.\n<pattern>: The code for normalizing attention scores using the nn.Softmax function is removed.\n<code_one>: nn.Softmax(dim=-1)(attention_scores)\n<code_two>: nn.functional.softmax(attention_scores, dim=-1)\nFix_pattern: In the condition of needing to normalize attention scores to probabilities, if the code for normalizing attention scores using nn.Softmax is detected, then it should be replaced with the code nn.functional.softmax(attention_scores, dim=-1) to fix the API misuse."}
{"number": 66, "change": "class DynamicConvolution2D(nn.Module):\nweight = self.linear_weight(x)  # B x T x kH\nweight = F.dropout(weight, self.dropout_rate, training=self.training)\nweight = weight.view(B, T, H, k).transpose(1, 2).contiguous()  # B x H x T x k\n-        weight_new = torch.zeros(B * H * T * (T + k - 1)).view(B, H, T, T + k - 1).fill_(float('-inf'))\n+        weight_new = torch.zeros(B * H * T * (T + k - 1), dtype=weight.dtype)\n+        weight_new = weight_new.view(B, H, T, T + k - 1).fill_(float('-inf'))\nweight_new = weight_new.to(x.device)  # B x H x T x T+k-1\nweight_new.as_strided((B, H, T, k), ((T + k - 1) * T * H, (T + k - 1) * T, T + k, 1)).copy_(weight)\nweight_new = weight_new.narrow(-1, int((k - 1) / 2), T)  # B x H x T x T(k)\n", "fix_pattern": "<condition>: No clear condition is needed.\n<pattern>: Initialize and fill the 'weight_new' tensor with '-inf' values.\n<code_one>: weight_new = torch.zeros(B * H * T * (T + k - 1)).view(B, H, T, T + k - 1).fill_(float('-inf'))\n<code_two>: weight_new = torch.zeros(B * H * T * (T + k - 1), dtype=weight.dtype); weight_new = weight_new.view(B, H, T, T + k - 1).fill_(float('-inf'))\nFix_pattern: In the condition of no clear condition, if initializing and filling 'weight_new' tensor with '-inf' values is detected, then remove 'weight_new = torch.zeros(B * H * T * (T + k - 1)).view(B, H, T, T + k - 1).fill_(float('-inf'))' and add 'weight_new = torch.zeros(B * H * T * (T + k - 1), dtype=weight.dtype); weight_new = weight_new.view(B, H, T, T + k - 1).fill_(float('-inf'))' to fix the API misuse."}
{"number": 68, "change": "class Gru(TransformationBase):\n\ndef tf_apply(self, x, sequence_length=None):\nx, state = tf.nn.dynamic_rnn(\n-            cell=self.cell, inputs=x, sequence_length=sequence_length, dtype=tf.float32,\n+            cell=self.cell, inputs=x, sequence_length=sequence_length, initial_state=None,\n+            dtype=util.tf_dtype(dtype='float'),\n# Weird TensorFlow behavior? (https://github.com/tensorflow/tensorflow/issues/15874)\nparallel_iterations=(self.input_spec['shape'][0] + 1)\n)\n", "fix_pattern": "<condition>: There is a parallel_iterations argument in the tf.nn.dynamic_rnn function call.\n<pattern>: The value of parallel_iterations is being set to self.input_spec['shape'][0] + 1.\n<code_one>: cell=self.cell, inputs=x, sequence_length=sequence_length, dtype=tf.float32\n<code_two>: cell=self.cell, inputs=x, sequence_length=sequence_length, initial_state=None, dtype=util.tf_dtype(dtype='float')\nFix_pattern: In the condition of having the parallel_iterations argument set in the tf.nn.dynamic_rnn function call, if the value is self.input_spec['shape'][0] + 1, then change the cell=self.cell, inputs=x, sequence_length=sequence_length, dtype=tf.float32 to cell=self.cell, inputs=x, sequence_length=sequence_length, initial_state=None, dtype=util.tf_dtype(dtype='float') to fix the API misuse."}
{"number": 69, "change": "class Optimizer(Component):\nFor those we treat model as max_norm.\neg. optimizer.clip_grad_norm(max_norm)\n\"\"\"\n-            return torch.nn.utils.clip_grad_norm_(self.params, max_norm)\n+            return clip_grad_norm_(self.params, max_norm)\nelse:\n-            return torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n+            return clip_grad_norm_(model.parameters(), max_norm)\n\ndef pre_export(self, model):\npass\n", "fix_pattern": "<condition>: The condition is when the code is using the method \"torch.nn.utils.clip_grad_norm_\" to clip gradient norms.\n\n<pattern>: The pattern is detecting the usage of \"torch.nn.utils.clip_grad_norm_\" method.\n\n<code_one>: The code that is being removed is \"torch.nn.utils.clip_grad_norm_(self.params, max_norm)\" and \"torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\".\n\n<code_two>: The code that is being added is \"clip_grad_norm_(self.params, max_norm)\" and \"clip_grad_norm_(model.parameters(), max_norm)\".\n\nFix_pattern: In the condition of using the \"torch.nn.utils.clip_grad_norm_\" method, the fix pattern is to remove \"torch.nn.utils.\" from the code and replace it with \"clip_grad_norm_\"."}
{"number": 76, "change": "def main(args):\nbob_decision = Marginal(Search(bob))\n\n# Here Alice and Bob slightly prefer one location over the other a priori\n-    shared_preference = Variable(torch.Tensor([args.preference]))\n+    shared_preference = torch.tensor([args.preference])\n\nbob_depth = args.depth\nnum_samples = args.num_samples\n", "fix_pattern": "<condition>: The condition is not clearly specified in the context.\n<pattern>: The pattern is detecting the use of \"Variable()\" function.\n<code_one>: The code removed is \"shared_preference = Variable(torch.Tensor([args.preference]))\".\n<code_two>: The code added is \"shared_preference = torch.tensor([args.preference])\".\nFix_pattern: In the condition of <condition>, if the use of \"Variable()\" is detected, then the code \"shared_preference = Variable(torch.Tensor([args.preference]))\" should be changed to \"shared_preference = torch.tensor([args.preference])\" to fix the API misuse."}
{"number": 81, "change": "class Planetoid(Dataset):\n# Create unweighted sparse adjacency matrix.\nweight = torch.ones(index.size(1))\nn = input.size(0)\n-        adj = torch.sparse.FloatTensor(index, weight, torch.Size([n, n]))\n+        adj = SparseTensor(index, weight, torch.Size([n, n]))\n\n# Bundle graph to data object.\n-        self.data = Data(input, adj, position=None, target=target)\n+        self.data = Data(input, adj, position=None, target=target.long())\n\ndef __getitem__(self, index):\ndata = self.data\n", "fix_pattern": "<condition>: No clear condition is needed.\n<pattern>: The pattern is to change the creation of a torch sparse tensor to a SparseTensor object.\n<code_one>: The code that was removed is \"adj = torch.sparse.FloatTensor(index, weight, torch.Size([n, n]))\".\n<code_two>: The code that was added is \"adj = SparseTensor(index, weight, torch.Size([n, n]))\".\nFix_pattern: In the condition of no clear condition, if the pattern of creating a torch sparse tensor is detected, then change the code that creates the tensor from torch sparse tensor to SparseTensor object to fix the API misuse."}
{"number": 85, "change": "\"import sys\\n\",\n\"sys.path.append(f'{os.getcwd()}/SentEval')\\n\",\n\"\\n\",\n-        \"import tensorflow_hub as hub\\n\",\n\"import tensorflow as tf\\n\",\n+        \"\\n\",\n+        \"# Prevent TF from claiming all GPU memory so there is some left for pytorch.\\n\",\n+        \"gpus = tf.config.list_physical_devices('GPU')\\n\",\n+        \"if gpus:\\n\",\n+        \"  # Memory growth needs to be the same across GPUs.\\n\",\n+        \"  for gpu in gpus:\\n\",\n+        \"    tf.config.experimental.set_memory_growth(gpu, True)\\n\",\n+        \"\\n\",\n+        \"import tensorflow_hub as hub\\n\",\n\"import tensorflow_text\\n\",\n\"import senteval\\n\",\n\"import time\\n\",\n", "fix_pattern": "<condition>: The code is attempting to import the module \"tensorflow_hub\".\n<pattern>: The import statement for \"tensorflow_hub\" is removed.\n<code_one>: \"import tensorflow_hub as hub\" is removed.\n<code_two>: \"import tensorflow_hub as hub\" is added.\nFix_pattern: In the condition of attempting to import \"tensorflow_hub\", if the import statement for \"tensorflow_hub\" is removed, then \"import tensorflow_hub as hub\" should be added to fix the API misuse."}
{"number": 88, "change": "def create_checkerboard(h, w, nw):\n\n\n# TODO: Isn't this function duplicated with eye_like?\n-def create_eye_batch(batch_size, eye_size):\n+def create_eye_batch(batch_size, eye_size, device=None, dtype=None):\n\"\"\"Creates a batch of identity matrices of shape Bx3x3\n\"\"\"\n-    return torch.eye(eye_size).view(\n+    return torch.eye(eye_size, device=device, dtype=dtype).view(\n1, eye_size, eye_size).expand(batch_size, -1, -1)\n", "fix_pattern": "Condition: The function create_checkerboard is created but it is mentioned that it might be duplicated with another function called eye_like. \nPattern: The code removed is a function called create_eye_batch which takes batch_size and eye_size as arguments. \nCode one: The code removed takes the eye_size and return a tensor with the same size. \nCode two: The code added takes the batch_size, eye_size, device, and dtype as arguments and returns a tensor with the same size. \nFix_pattern: In the condition of eye_like, if create_eye_batch is detected, then remove the code_one and add the code_two to fix the API misuse."}
{"number": 93, "change": "class DNAConv(MessagePassing):\nnum_edges = edge_index.size(1)\n\nedge_index, edge_weight = gcn_norm(edge_index, x.size(self.node_dim),\n-                                           edge_weight, self.improved, x.dtype)\n+                                           edge_weight, dtype=x.dtype)\n\nif self.cached:\nself._cache = (num_edges, edge_index, edge_weight)\n", "fix_pattern": "<condition>: The condition is that the API `gcn_norm` is being used with the variable `edge_weight` as one of its arguments.\n<pattern>: The pattern is that the `edge_weight` parameter is being passed as an argument to `gcn_norm`.\n<code_one>: The code being removed is `edge_weight, self.improved, x.dtype`.\n<code_two>: The code being added is `edge_weight, dtype=x.dtype`.\nFix_pattern: In the condition where `gcn_norm` is used with `edge_weight` as an argument, the `edge_weight` parameter is being changed from `<code_one>` to `<code_two>` to fix the API misuse."}
{"number": 94, "change": "class T5Attention(nn.Module):\nposition_bias = position_bias + mask  # (batch_size, n_heads, seq_length, key_length)\n\nscores += position_bias\n-        attn_weights = F.softmax(scores.float(), dim=-1).type_as(\n+        attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(\nscores\n)  # (batch_size, n_heads, seq_length, key_length)\n-        attn_weights = F.dropout(\n+        attn_weights = nn.functional.dropout(\nattn_weights, p=self.dropout, training=self.training\n)  # (batch_size, n_heads, seq_length, key_length)\n", "fix_pattern": "<condition>: The condition is not clearly mentioned in the given code context.\n<pattern>: The pattern is to replace the usage of the function \"F.softmax\" with the function \"nn.functional.softmax\", and the usage of the function \"F.dropout\" with the function \"nn.functional.dropout\".\n<code_one>: F.softmax(scores.float(), dim=-1).type_as(attn_weights = F.dropout(\n<code_two>: nn.functional.softmax(scores.float(), dim=-1).type_as(attn_weights = nn.functional.dropout(\nFix_pattern: In the condition where the code is performing an API misuse, the pattern is detected by replacing the usage of certain functions with their corresponding alternatives to fix the issue."}
{"number": 97, "change": "def _get_ort_session_options() -> ort.SessionOptions:\nif not torch.cuda.is_available():\nsess_options.execution_mode = ort.ExecutionMode.ORT_PARALLEL\nsess_options.inter_op_num_threads = 1\n-        sess_options.intra_op_num_threads = max(torch.get_num_threads(), 1)\n+        sess_options.intra_op_num_threads = max(\n+            int(\n+                os.environ.get(\"NEBULLVM_THREADS_PER_MODEL\")\n+                or torch.get_num_threads()\n+            ),\n+            1,\n+        )\nreturn sess_options\n", "fix_pattern": "<condition>: When the condition of torch.cuda.is_available() is not met.\n<pattern>: The pattern of setting sess_options.intra_op_num_threads to max(torch.get_num_threads(), 1).\n<code_one>: The code setting sess_options.intra_op_num_threads = max(torch.get_num_threads(), 1).\n<code_two>: The code setting sess_options.intra_op_num_threads = max(int(os.environ.get(\"NEBULLVM_THREADS_PER_MODEL\") or torch.get_num_threads()), 1).\nFix_pattern: In the condition where torch.cuda.is_available() is not met, the fix pattern is to replace the code sess_options.intra_op_num_threads = max(torch.get_num_threads(), 1) with sess_options.intra_op_num_threads = max(int(os.environ.get(\"NEBULLVM_THREADS_PER_MODEL\") or torch.get_num_threads()), 1) to fix the API misuse."}
{"number": 98, "change": "def save_best_model(model, optimizer, model_loss, best_loss, out_path,\ndef check_update(model, grad_clip, grad_top):\nr'''Check model gradient against unexpected jumps and failures'''\nskip_flag = False\n-    grad_norm = torch.nn.utils.clip_grad_norm(model.parameters(), grad_clip)\n+    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\nif np.isinf(grad_norm):\nprint(\" | > Gradient is INF !!\")\nskip_flag = True\n", "fix_pattern": "Condition: The condition is when the gradient norm is infinite (np.isinf(grad_norm)).\n\nPattern: The pattern is the misuse of the \"clip_grad_norm\" function, which is not used correctly in the original code.\n\nCode one: The code one is \"grad_norm = torch.nn.utils.clip_grad_norm(model.parameters(), grad_clip)\".\n\nCode two: The code two is \"grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\".\n\nFix pattern: In the condition of the gradient norm being infinite, the fix is to change the code one to code two in order to correctly use the \"clip_grad_norm_\" function and fix the API misuse."}
{"number": 99, "change": "def quaternion_exp_to_log(quaternion: torch.Tensor,\n>>> kornia.quaternion_exp_to_log(quaternion)\ntensor([0., 0., 0.])\n\"\"\"\n-    if not torch.is_tensor(quaternion):\n+    if not isinstance(quaternion, torch.Tensor):\nraise TypeError(\"Input type is not a torch.Tensor. Got {}\".format(\ntype(quaternion)))\n", "fix_pattern": "Condition: The condition is checking if the input parameter 'quaternion' is a tensor or not.\nPattern: The pattern is checking if 'quaternion' is not a tensor.\nCode one: The code that is being removed is the check using the 'torch.is_tensor()' function.\nCode two: The code that is being added is the check using the 'isinstance()' function.\nFix_pattern: In the condition of checking if 'quaternion' is not a tensor, remove the code using 'torch.is_tensor()' and add the code using 'isinstance()' to fix the API misuse."}
{"number": 103, "change": "class ReformerLayer(nn.Module):\n\"\"\"\n# randomize seeds\n# use cuda generator if available\n-        if len(torch.cuda.default_generators) > 0:\n+        if hasattr(torch.cuda, \"default_generators\") and len(torch.cuda.default_generators) > 0:\n# GPU\ndevice_idx = torch.cuda.current_device()\nself.feed_forward_seed = torch.cuda.default_generators[device_idx].seed()\n", "fix_pattern": "<condition>: The condition is checking if there are any CUDA generators available.\n<pattern>: The pattern is checking the length of the list \"torch.cuda.default_generators\" and if it is greater than 0.\n<code_one>: The code being removed is \"if len(torch.cuda.default_generators) > 0:\"\n<code_two>: The code being added is \"if hasattr(torch.cuda, \"default_generators\") and len(torch.cuda.default_generators) > 0:\"\nFix_pattern: In the condition of checking if CUDA generators are available, if the length of \"torch.cuda.default_generators\" is greater than 0, then remove the code checking the length and replace it with a new code that also checks if the attribute \"default_generators\" is present using \"hasattr\"."}
{"number": 104, "change": "def get_projective_transform(center: torch.Tensor, angles: torch.Tensor) -> torc\n\n# create rotation matrix\nangle_axis_rad: torch.Tensor = K.deg2rad(angles)\n-    rmat: torch.Tensor = K.angle_axis_to_rotation_matrix(angle_axis_rad)  # Bx3x3\n+    rmat: torch.Tensor = K.angle_axis_to_rotation_matrix(angle_axis_rad) * scales.view(-1, 1, 1)  # Bx3x3\n\n# define matrix to move forth and back to origin\nfrom_origin_mat = torch.eye(4)[None].repeat(rmat.shape[0], 1, 1).type_as(center)  # Bx4x4\n", "fix_pattern": "<condition>: When creating a rotation matrix.\n<pattern>: The original code only created the rotation matrix without scaling it.\n<code_one>: rmat: torch.Tensor = K.angle_axis_to_rotation_matrix(angle_axis_rad)  # Bx3x3\n<code_two>: rmat: torch.Tensor = K.angle_axis_to_rotation_matrix(angle_axis_rad) * scales.view(-1, 1, 1)  # Bx3x3\nFix_pattern: In the condition of creating a rotation matrix, if no scaling is applied, then multiply the rotation matrix by the scales to fix the API misuse."}
{"number": 105, "change": "class FQETorchModel:\nq_values, _ = self.q_model({\"obs\": obs}, [], None)\nif actions is not None:\nactions = torch.tensor(actions, device=self.device, dtype=int)\n-            q_values = torch.gather(q_values, -1, actions.unsqueeze(-1)).squeeze()\n+            q_values = torch.gather(q_values, -1, actions.unsqueeze(-1)).squeeze(-1)\nreturn q_values.detach()\n\ndef estimate_v(\n", "fix_pattern": "<condition>: The condition is that <actions> is not None.\n<pattern>: The pattern is that <q_values> is gathered based on <actions> and reshaped.\n<code_one>: The code that is removed is \"q_values = torch.gather(q_values, -1, actions.unsqueeze(-1)).squeeze()\".\n<code_two>: The code that is added is \"q_values = torch.gather(q_values, -1, actions.unsqueeze(-1)).squeeze(-1)\".\nFix_pattern: In the condition of <actions> not being None, if the pattern of gathering and reshaping <q_values> based on <actions> is detected, then the code \"q_values = torch.gather(q_values, -1, actions.unsqueeze(-1)).squeeze()\" should be changed to \"q_values = torch.gather(q_values, -1, actions.unsqueeze(-1)).squeeze(-1)\" to fix the API misuse."}
{"number": 107, "change": "class ModelCatalog:\nmodel_name (str): Name to register the model under.\nmodel_class (type): Python class of the model.\n\"\"\"\n-        if issubclass(model_class, tf.keras.Model):\n-            deprecation_warning(old=\"register_custom_model\", error=False)\n+        if tf is not None:\n+            if issubclass(model_class, tf.keras.Model):\n+                deprecation_warning(old=\"register_custom_model\", error=False)\n_global_registry.register(RLLIB_MODEL, model_name, model_class)\n\n@staticmethod\n", "fix_pattern": "<condition>: If `model_class` is a subclass of `tf.keras.Model`.\n<pattern>: `issubclass(model_class, tf.keras.Model)`\n<code_one>: The condition in the `if` statement that checks if `model_class` is a subclass of `tf.keras.Model` and raises a deprecation warning if it is.\n<code_two>: The code that adds an additional check if `tf` is not `None` before checking the condition for `model_class` and raising the deprecation warning.\nFix_pattern: In the condition of `model_class` being a subclass of `tf.keras.Model`, the fix is to add an additional check for `tf` being not `None` before raising the deprecation warning."}
{"number": 109, "change": "class DLA(nn.Module):\nif self.drop_rate > 0.:\nx = F.dropout(x, p=self.drop_rate, training=self.training)\nx = self.fc(x)\n-        if not self.global_pool.is_identity():\n-            x = x.flatten(1)  # conv classifier, flatten if pooling isn't pass-through (disabled)\n+        x = self.flatten(x)\nreturn x\n", "fix_pattern": "Condition: The condition is \"if self.drop_rate > 0.\".\nPattern: The pattern is \"not self.global_pool.is_identity()\".\nCode one: The code that was removed is \"x = x.flatten(1)\".\nCode two: The code that was added is \"x = self.flatten(x)\".\nFix_pattern: In the condition of \"if self.drop_rate > 0.\", if the pattern \"not self.global_pool.is_identity()\" is detected, then remove \"x = x.flatten(1)\" and add \"x = self.flatten(x)\" to fix the API misuse."}
{"number": 112, "change": "class Trainer(\n\nresults = self.predict_loop.on_predict_epoch_end()\nself.predict_loop.on_predict_end()\n+\n+        # re-enable grads\n+        torch.set_grad_enabled(True)\n+\nreturn results\n\ndef run_sanity_check(self, ref_model):\n", "fix_pattern": "Condition: In the context of the Trainer class.\nPattern: No clear pattern can be identified in the given context.\nCode_one: No relevant code can be identified in the given context.\nCode_two: The code added is to re-enable gradients using the torch.set_grad_enabled(True) function.\nFix_pattern: No pre-condition is needed."}
{"number": 113, "change": "def filter2d(\ninput = input.view(-1, tmp_kernel.size(0), input.size(-2), input.size(-1))\n\n# convolve the tensor with the kernel.\n-    output = F.conv2d(input, tmp_kernel, groups=tmp_kernel.size(0), padding=0, stride=1)\n+    # NOTE: type(...) to fix getting `torch.bfloat16` type.\n+    # TODO: @johnnv1, fix it through the Augmentation Base.\n+    output = F.conv2d(input, tmp_kernel, groups=tmp_kernel.size(0), padding=0, stride=1).type(input.dtype)\n\nif padding == 'same':\nout = output.view(b, c, h, w)\n", "fix_pattern": "<condition>: The condition is when the padding is set to 'same'.\n<pattern>: The pattern detected is an API misuse, specifically in the type of the output tensor.\n<code_one>: The code that is removed is the line where the output tensor is computed using the F.conv2d() function.\n<code_two>: The code that is added is the line where the output tensor is cast to the same data type as the input tensor.\nFix_pattern: In the condition of padding being 'same', if there is an API misuse of output tensor type, then the F.conv2d() code is replaced with a new line to cast the output tensor to the same data type as the input tensor."}
{"number": 114, "change": "def remainder(\nres_floored = torch.where(res >= 0, torch.floor(res), torch.ceil(res))\ndiff = res - res_floored\ndiff, x2 = ivy.promote_types_of_inputs(diff, x2)\n-        return torch.mul(diff, x2, out=out)\n+        return torch.mul(diff, x2, out=out).to(x1.dtype)\nreturn torch.remainder(x1, x2, out=out)\n", "fix_pattern": "<condition>: There is no specific condition identified in the context section.\n<pattern>: In the code removed section, the pattern is to return the result of multiplying diff and x2.\n<code_one>: The code that was removed is \"return torch.mul(diff, x2, out=out)\".\n<code_two>: The code that was added is \".to(x1.dtype)\".\nFix_pattern: In the condition of no specific condition, if the pattern of returning the multiplication result of diff and x2 is detected, then the code \"return torch.mul(diff, x2, out=out)\" should be changed to \"return torch.mul(diff, x2, out=out).to(x1.dtype)\" to fix the API misuse."}
{"number": 116, "change": "class EpsilonGreedy(Exploration):\ntorch.multinomial(random_valid_action_logits, 1), axis=1)\n# Pick either random or greedy.\naction = torch.where(\n-                torch.empty((batch_size, )).uniform_() < epsilon,\n+                torch.empty(\n+                    (batch_size, )).uniform_().to(self.device) < epsilon,\nrandom_actions, exploit_action)\n\nreturn action, action_logp\n", "fix_pattern": "<condition>: The condition is checking if random_actions is True.\n<pattern>: The pattern is checking if the empty tensor multiplied by epsilon is less than a random uniform tensor.\n<code_one>: The code that was removed is \"torch.empty((batch_size, )).uniform_() < epsilon\".\n<code_two>: The code that was added is \"torch.empty((batch_size, )).uniform_().to(self.device) < epsilon\".\nFix_pattern: In the condition of checking if random_actions is True, if the empty tensor multiplied by epsilon is less than a random uniform tensor, then change the code \"torch.empty((batch_size, )).uniform_() < epsilon\" to \"torch.empty((batch_size, )).uniform_().to(self.device) < epsilon\" to fix the API misuse."}
{"number": 118, "change": "class Delta(TorchDistribution):\n\ndef expand(self, batch_shape):\nvalidate_args = self.__dict__.get('_validate_args')\n+        batch_shape = torch.Size(batch_shape)\nv = self.v.expand(batch_shape + self.event_shape)\nlog_density = self.log_density.expand(batch_shape)\nreturn Delta(v, log_density, self.event_dim, validate_args=validate_args)\n", "fix_pattern": "Condition: The condition is that the API `expand()` is called on an instance of the `Delta` class.\nPattern: No specific pattern is detected.\nCode one: No code is removed.\nCode two: The code `batch_shape = torch.Size(batch_shape)` is added.\nFix pattern: In the condition of the `expand()` method being called on the `Delta` class, the code `batch_shape = torch.Size(batch_shape)` is added to fix the API misuse."}
{"number": 119, "change": "def main():\n\npruner = AGP_Pruner(model, configure_list)\nmodel = pruner.compress()\n-\n+    model = model.to(device)\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\nfor epoch in range(10):\npruner.update_epoch(epoch)\nprint('# Epoch {} #'.format(epoch))\ntrain(model, device, train_loader, optimizer)\ntest(model, device, test_loader)\n-    pruner.export_model('model.pth', 'mask.pth', 'model.onnx', [1, 1, 28, 28])\n+    pruner.export_model('model.pth', 'mask.pth', 'model.onnx', [1, 1, 28, 28], device)\n\n\nif __name__ == '__main__':\n", "fix_pattern": "<condition>: The fix pattern does not have a specific condition.\n<pattern>: The pattern is the removal of the code that exports the model with specific file names and sizes.\n<code_one>: The code that is removed is \"pruner.export_model('model.pth', 'mask.pth', 'model.onnx', [1, 1, 28, 28])\".\n<code_two>: The code that is added is \"model = model.to(device) \\npruner.export_model('model.pth', 'mask.pth', 'model.onnx', [1, 1, 28, 28], device)\".\nFix_pattern: In the code, if the pattern of exporting the model with specific file names and sizes is detected, then the code \"pruner.export_model('model.pth', 'mask.pth', 'model.onnx', [1, 1, 28, 28])\" should be removed and replaced with \"model = model.to(device) \\npruner.export_model('model.pth', 'mask.pth', 'model.onnx', [1, 1, 28, 28], device)\" to fix the API misuse."}
{"number": 121, "change": "from ray.air.config import ScalingConfig\n\n\ndef mnist_dataset(batch_size: int) -> tf.data.Dataset:\n-    (x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n+    with FileLock(os.path.expanduser(\"~/.mnist_lock\")):\n+        (x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n# The `x` arrays are in uint8 and have values in the [0, 255] range.\n# You need to convert them to float32 with values in the [0, 1] range.\nx_train = x_train / np.float32(255)\n", "fix_pattern": "Condition: The code is accessing the variable \"x_train\" without initializing it first.\nPattern: Trying to divide \"x_train\" by np.float32(255) to convert it to float32 with values in the range [0, 1].\nCode One: (x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\nCode Two: with FileLock(os.path.expanduser(\"~/.mnist_lock\")): (x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\nFix_pattern: In the condition of accessing \"x_train\" without initialization, if trying to convert \"x_train\" to float32 by dividing it with np.float32(255) is detected, then add the code \"with FileLock(os.path.expanduser(\"~/.mnist_lock\")):\" and (x_train, y_train), _ = tf.keras.datasets.mnist.load_data() to fix the API misuse."}
{"number": 126, "change": "def main_fun(argv, ctx):\ngrads = average_gradients(tower_grads)\n\n# Add a summary to track the learning rate.\n-      summaries.append(tf.scalar_summary('learning_rate', lr))\n+      summaries.append(tf.summary.scalar('learning_rate', lr))\n\n# Add histograms for gradients.\nfor grad, var in grads:\nif grad is not None:\nsummaries.append(\n-              tf.histogram_summary(var.op.name + '/gradients', grad))\n+              tf.summary.histogram(var.op.name + '/gradients', grad))\n\n# Apply the gradients to adjust the shared variables.\napply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n\n# Add histograms for trainable variables.\nfor var in tf.trainable_variables():\n-        summaries.append(tf.histogram_summary(var.op.name, var))\n+        summaries.append(tf.summary.histogram(var.op.name, var))\n\n# Track the moving averages of all trainable variables.\nvariable_averages = tf.train.ExponentialMovingAverage(\n", "fix_pattern": "<condition>: The condition is that the code is used to add summaries and histograms for learning rate and gradients.\n<pattern>: The pattern is \"tf.scalar_summary('learning_rate', lr)\" and \"tf.histogram_summary(var.op.name + '/gradients', grad)\".\n<code_one>: The code that is removed is \"tf.scalar_summary('learning_rate', lr)\" and \"tf.histogram_summary(var.op.name, var)\".\n<code_two>: The code that is added is \"tf.summary.scalar('learning_rate', lr)\" and \"tf.summary.histogram(var.op.name + '/gradients', grad)\".\nFix_pattern: In the condition of adding summaries and histograms, if the pattern \"tf.scalar_summary('learning_rate', lr)\" and \"tf.histogram_summary(var.op.name + '/gradients', grad)\" is detected, then remove the code \"tf.scalar_summary('learning_rate', lr)\" and \"tf.histogram_summary(var.op.name, var)\" and add the code \"tf.summary.scalar('learning_rate', lr)\" and \"tf.summary.histogram(var.op.name + '/gradients', grad)\" to fix the API misuse."}
{"number": 127, "change": "class Trainer(TrainerBase):\n\n@timing.time(\"Trainer.test\")\ndef test(self, test_iter, model, metric_reporter: MetricReporter):\n+        if cuda.CUDA_ENABLED:\n+            model = model.cuda()\n+\nmodel.eval()\nwith torch.no_grad():\ntest_metric = self._run_epoch(\n", "fix_pattern": "<condition>: The condition is that CUDA is enabled.\n<pattern>: The pattern is the API misuse related to the use of CUDA.\n<code_one>: No specific code is mentioned in the code removed section.\n<code_two>: The code added is \"model = model.cuda()\".\nFix_pattern: In the condition of CUDA being enabled, if there is an API misuse related to the use of CUDA, the fix is to add the code \"model = model.cuda()\"."}
{"number": 128, "change": "class TrainingArguments:\n@torch_required\ndef _setup_devices(self) -> \"torch.device\":\nlogger.info(\"PyTorch: setting up devices\")\n-        if torch.distributed.is_initialized() and self.local_rank == -1:\n+        if torch.distributed.is_available() and torch.distributed.is_initialized() and self.local_rank == -1:\nlogger.warning(\n\"torch.distributed process group is initialized, but local_rank == -1. \"\n\"In order to use Torch DDP, launch your script with `python -m torch.distributed.launch\"\n", "fix_pattern": "<condition>: There is a check for torch.distributed.is_initialized() and self.local_rank == -1.\n<pattern>: The API torch.distributed.is_initialized() is replaced with torch.distributed.is_available() in the condition.\n<code_one>: if torch.distributed.is_initialized() and self.local_rank == -1\n<code_two>: if torch.distributed.is_available() and torch.distributed.is_initialized() and self.local_rank == -1\nFix_pattern: In the condition of checking if torch.distributed is initialized and local_rank is -1, the API torch.distributed.is_initialized() is changed to torch.distributed.is_available() to fix the API misuse."}
{"number": 131, "change": "with tf.Graph().as_default():\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"model\")\nif not os.path.exists(checkpoint_dir):\nos.makedirs(checkpoint_dir)\n-        saver = tf.train.Saver(tf.all_variables())\n+        saver = tf.train.Saver(tf.global_variables())\n\n# Write vocabulary\nvocab_processor.save(os.path.join(out_dir, \"vocab\"))\n\n# Initialize all variables\n-        sess.run(tf.initialize_all_variables())\n+        sess.run(tf.global_variables_initializer())\n\ndef train_step(x_batch, y_batch):\n\"\"\"\n", "fix_pattern": "<condition>: No clear condition identified.\n<pattern>: The code that initializes all variables and creates a saver object using `tf.all_variables()` is removed.\n<code_one>: `saver = tf.train.Saver(tf.all_variables())\\nsess.run(tf.initialize_all_variables())`\n<code_two>: `saver = tf.train.Saver(tf.global_variables())\\nsess.run(tf.global_variables_initializer())`\nFix pattern: In the given code, the initialization of all variables and creation of a saver object should be changed from `tf.all_variables()` and `tf.initialize_all_variables()` to `tf.global_variables()` and `tf.global_variables_initializer()` respectively."}
{"number": 132, "change": "class CLIPTextTransformer(nn.Module):\nattentions=encoder_outputs.attentions,\n)\n\n-    def _build_causal_attention_mask(self, bsz, seq_len):\n+    def _build_causal_attention_mask(self, bsz, seq_len, dtype):\n# lazily create causal attention mask, with full attention between the vision tokens\n# pytorch uses additive attention mask; fill with -inf\n-        mask = torch.empty(bsz, seq_len, seq_len)\n-        mask.fill_(torch.tensor(float(\"-inf\")))\n+        mask = torch.empty(bsz, seq_len, seq_len, dtype=dtype)\n+        mask.fill_(torch.tensor(torch.finfo(dtype).min))\nmask.triu_(1)  # zero out the lower diagonal\nmask = mask.unsqueeze(1)  # expand mask\nreturn mask\n", "fix_pattern": "Condition: The code is a part of a class called \"CLIPTextTransformer\".\nPattern: The pattern is the creation of a causal attention mask.\nCode One: The original code initializes the mask tensor and fills it with \"-inf\".\nCode Two: The fixed code initializes the mask tensor with a specified data type and fills it with the minimum value of that data type.\nFix Pattern: In the condition of the \"CLIPTextTransformer\" class, if the creation of a causal attention mask is detected, then remove the original code that sets the mask tensor to \"-inf\" and add the fixed code that initializes the mask tensor with a specified data type and fills it with the minimum value of that data type to fix the API misuse."}
{"number": 135, "change": "def get_mini_batch(mini_batch_indices, sequences, seq_lengths, cuda=False):\n# get mask for mini-batch\nmini_batch_mask = get_mini_batch_mask(mini_batch, sorted_seq_lengths)\n\n-    # wrap in PyTorch Variables\n-    mini_batch = Variable(torch.Tensor(mini_batch))\n-    mini_batch_reversed = Variable(torch.Tensor(mini_batch_reversed))\n-    mini_batch_mask = Variable(torch.Tensor(mini_batch_mask))\n+    # wrap in PyTorch Tensors\n+    mini_batch = torch.tensor(mini_batch)\n+    mini_batch_reversed = torch.tensor(mini_batch_reversed)\n+    mini_batch_mask = torch.tensor(mini_batch_mask)\n\n# cuda() here because need to cuda() before packing\nif cuda:\n", "fix_pattern": "<condition>: When the 'cuda' flag is True.\n<pattern>: The code is converting variables to PyTorch Variables.\n<code_one>: The code that wraps variables in PyTorch Variables.\n<code_two>: The code that wraps variables in PyTorch Tensors.\nFix_pattern: In the condition of 'cuda' being True, the code that wraps variables in PyTorch Variables is removed and replaced with code that wraps variables in PyTorch Tensors to fix the API misuse."}
{"number": 137, "change": "class FlopsProfiler(object):\nstart_time_hook)\n\ndef end_time_hook(module, input, output):\n-                torch.cuda.synchronize()\n+                get_accelerator().synchronize()\nmodule.__duration__ += time.time() - module.__start_time__\n\nif not hasattr(module, \"__end_time_hook_handle__\"):\n", "fix_pattern": "Condition: There is a check for the existence of a specific attribute in the 'module' object.\nPattern: The code 'torch.cuda.synchronize()' is removed.\nCode One: torch.cuda.synchronize()\nCode Two: get_accelerator().synchronize()\nFix Pattern: In the condition of checking the existence of the '__end_time_hook_handle__' attribute in the module object, if the code 'torch.cuda.synchronize()' is detected, then it is replaced with 'get_accelerator().synchronize()' to fix the API misuse."}
{"number": 140, "change": "class PaintByExample(DiffusionInpaintModel):\nmask: [H, W, 1] 255 means area to repaint\nreturn: BGR IMAGE\n\"\"\"\n-        set_seed(config.paint_by_example_seed)\n-\noutput = self.model(\nimage=PIL.Image.fromarray(image),\nmask_image=PIL.Image.fromarray(mask[:, :, -1], mode=\"L\"),\nexample_image=config.paint_by_example_example_image,\nnum_inference_steps=config.paint_by_example_steps,\noutput_type='np.array',\n+            generator=torch.manual_seed(config.paint_by_example_seed)\n).images[0]\n\noutput = (output * 255).round().astype(\"uint8\")\n", "fix_pattern": "condition: The condition is to set the seed for the paint by example operation.\npattern: The pattern is to use the set_seed method to set the seed.\ncode_one: The code that is removed is \"set_seed(config.paint_by_example_seed)\".\ncode_two: The code that is added is \"generator=torch.manual_seed(config.paint_by_example_seed)\".\nFix_pattern: In the condition of setting the seed for the paint by example operation, if the set_seed method is detected, then remove \"set_seed(config.paint_by_example_seed)\" and add \"generator=torch.manual_seed(config.paint_by_example_seed)\" to fix the API misuse."}
{"number": 141, "change": "class BigBirdPegasusBlockSparseAttention(nn.Module):\nnum_indices_to_gather = indices.shape[-2] * indices.shape[-1]\nnum_indices_to_pick_from = params.shape[2]\n\n-        indices_shift = (\n-            torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)\n-            // num_indices_to_gather\n-            * num_indices_to_pick_from\n-        )\n+        shift = torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)\n+        indices_shift = torch_int_div(shift, num_indices_to_gather) * num_indices_to_pick_from\n\nflattened_indices = indices.view(-1) + indices_shift\nflattened_params = params.reshape(-1, params.shape[-2], params.shape[-1])\n", "fix_pattern": "Condition: There is a need to create indices_shift based on the indices and num_indices_to_gather.\nPattern: Compute indices_shift using torch.arange and relational operators.\nCode One: indices_shift = torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device) // num_indices_to_gather * num_indices_to_pick_from\nCode Two: shift = torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device) indices_shift = torch_int_div(shift, num_indices_to_gather) * num_indices_to_pick_from\nFix Pattern: In the condition of needing to compute indices_shift, if the pattern of using torch.arange and relational operators is detected, then the code one (indices_shift computation) needs to be removed and replaced with code two (shift computation, torch_int_div, and multiplication)."}
{"number": 151, "change": "def main(args):\n# CNN will transform a high dimension image into a low dimension 2D tensors for RBF kernel.\n# This kernel accepts inputs are inputs of CNN and gives outputs are covariance matrix of RBF on\n# outputs of CNN.\n-    kernel = gp.kernels.RBF(input_dim=10, lengthscale=torch.ones(10)).warp(iwarping_fn=cnn_fn)\n+    kernel = gp.kernels.Warp(gp.kernels.RBF(input_dim=10, lengthscale=torch.ones(10)),\n+                             iwarping_fn=cnn_fn)\n\n# init inducing points (taken randomly from dataset)\nXu = next(iter(train_loader))[0][:args.num_inducing]\n", "fix_pattern": "<condition>: The code is using the wrong type of kernel for the API.\n<pattern>: The incorrect kernel is being created using the RBF kernel with warping function.\n<code_one>: kernel = gp.kernels.RBF(input_dim=10, lengthscale=torch.ones(10)).warp(iwarping_fn=cnn_fn)\n<code_two>: kernel = gp.kernels.Warp(gp.kernels.RBF(input_dim=10, lengthscale=torch.ones(10)), iwarping_fn=cnn_fn)\nFix_pattern: In the condition of using the warping function with the RBF kernel, remove the .warp(iwarping_fn=cnn_fn) from the code and create the kernel using the Warp() function."}
{"number": 153, "change": "def linspace_helper(start, stop, num, axis=None, *, device):\nelse:\nres = [linspace_method(start, stp, num, device=device) for stp in stop]\nelse:\n-        return linspace_method(start, stop, num, device=device)\n+        return linspace_method(start, stop, num, dtype=torch.float64, device=device)\nres = torch.cat(res, -1).reshape(sos_shape + [num])\nif axis is not None:\nres = torch.transpose(res, axis, -1)\n", "fix_pattern": "<condition>: The condition is when the variable \"axis\" is not None.\n<pattern>: The pattern is the method call to \"linspace_method\" with the given parameters.\n<code_one>: The code removed is the return statement that calls \"linspace_method\" with the original parameters.\n<code_two>: The code added is the return statement that calls \"linspace_method\" with the original parameters and includes the dtype parameter set to torch.float64.\nFix_pattern: In the condition of \"axis is not None\", if the method call to \"linspace_method\" is detected, then change the returned code to include the dtype parameter set to torch.float64 to fix the API misuse."}
{"number": 159, "change": "class PatchAffineShapeEstimator(nn.Module):\n\"input shape should be must be [Bx1x{}x{}]. \"\n\"Got {}\".format(self.patch_size, self.patch_size, patch.size()))\nself.weighting = self.weighting.to(patch.dtype).to(patch.device)\n-        grads: torch.Tensor = self.gradient(patch)\n+        grads: torch.Tensor = self.gradient(patch) * self.weighting\n# unpack the edges\ngx: torch.Tensor = grads[:, :, 0]\ngy: torch.Tensor = grads[:, :, 1]\n", "fix_pattern": "<condition>: The condition is that the input shape should be [Bx1x{}x{}].\n<pattern>: The pattern is that the gradient calculation is missing a weighting factor.\n<code_one>: The code removed is \"grads: torch.Tensor = self.gradient(patch)\".\n<code_two>: The code added is \"grads: torch.Tensor = self.gradient(patch) * self.weighting\".\nFix_pattern: In the condition of the input shape requirement, if the gradient calculation is detected without the weighting factor, then the code \"grads: torch.Tensor = self.gradient(patch)\" should be changed to \"grads: torch.Tensor = self.gradient(patch) * self.weighting\" to fix the API misuse."}
{"number": 163, "change": "def non_max_suppression(prediction, conf_thres=0.1, iou_thres=0.6, classes=None,\nmerge = False  # use merge-NMS\n\nt = time.time()\n-    output = [torch.zeros(0, 6)] * prediction.shape[0]\n+    output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\nfor xi, x in enumerate(prediction):  # image index, image inference\n# Apply constraints\n# x[((x[..., 2:4] < min_wh) | (x[..., 2:4] > max_wh)).any(1), 4] = 0  # width-height\n", "fix_pattern": "<condition>: N/A (no pre condition is needed)\n<pattern>: If the condition to apply constraints is met in the code, remove the line of code that initializes the 'output' variable to a list of zeros.\n<code_one>: output = [torch.zeros(0, 6)] * prediction.shape[0]\n<code_two>: output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\nFix_pattern: In the condition of applying constraints, if the line of code initializing the 'output' variable to a list of zeros is detected, then remove it and replace it with a new line of code that initializes 'output' to a list of zeros on the device specified by 'prediction.device'."}
{"number": 166, "change": "class DartsTrainer(BaseOneShotTrainer):\np += e * d\n\n_, loss = self._logits_and_loss(trn_X, trn_y)\n-            dalphas.append(torch.autograd.grad(loss, [c.alpha for c in self.nas_modules]))\n+            dalphas.append(torch.autograd.grad(loss, [c.alpha for _, c in self.nas_modules]))\n\ndalpha_pos, dalpha_neg = dalphas  # dalpha { L_trn(w+) }, # dalpha { L_trn(w-) }\nhessian = [(p - n) / (2. * eps) for p, n in zip(dalpha_pos, dalpha_neg)]\n", "fix_pattern": "<condition>: The condition is not clearly stated in the provided code snippet.\n<pattern>: The pattern is that the code is modified to include an underscore (_) before the variable c in the list comprehension.\n<code_one>: The code that is removed is torch.autograd.grad(loss, [c.alpha for c in self.nas_modules]) which calculates gradients using the loss and the alpha variable.\n<code_two>: The code that is added is torch.autograd.grad(loss, [c.alpha for _, c in self.nas_modules]) which calculates gradients using the loss and the alpha variable, but now includes an underscore (_) before the variable c in the list comprehension.\nFix_pattern: In the condition where an underscore (_) is needed before the variable c in a list comprehension, replace code_one with code_two to correctly calculate gradients using the loss and the alpha variable."}
{"number": 167, "change": "def subtract(\nout: Optional[Union[tf.Tensor, tf.Variable]] = None,\n) -> Union[tf.Tensor, tf.Variable]:\nx1, x2 = ivy.promote_types_of_inputs(x1, x2)\n-    return tf.subtract(x1, x2)\n+    return tf.experimental.numpy.subtract(x1, x2)\n\n\ndef tan(\n", "fix_pattern": "<condition>: No pre condition is needed.\n<pattern>: Calling the `subtract` function using TensorFlow's `tf.subtract` method.\n<code_one>: `return tf.subtract(x1, x2)`\n<code_two>: `return tf.experimental.numpy.subtract(x1, x2)`\nFix_pattern: In the condition of calling the `subtract` function using TensorFlow's `tf.subtract` method, if this pattern is detected, then change the code `return tf.subtract(x1, x2)` to `return tf.experimental.numpy.subtract(x1, x2)` to fix the API misuse."}
{"number": 169, "change": "class XDropout(torch.autograd.Function):\n# Once https://github.com/pytorch/pytorch/issues/78391 is fixed, do something like:\n# if opset_version < 12:\n#   return torch.onnx.symbolic_opset9.dropout(g, input, dropout_p, train)\n-        return torch.onnx.symbolic_opset12.dropout(g, input, dropout_p, train)\n+        return symbolic_opset12.dropout(g, input, dropout_p, train)\n\n\n# Copied from transformers.models.deberta.modeling_deberta.StableDropout\n", "fix_pattern": "Condition: The condition is that opset_version should be less than 12.\nPattern: The pattern is the call to torch.onnx.symbolic_opset12.dropout() function.\nCode one: The code being removed is \"return torch.onnx.symbolic_opset12.dropout(g, input, dropout_p, train)\".\nCode two: The code being added is \"return symbolic_opset12.dropout(g, input, dropout_p, train)\".\nFix pattern: In the condition of opset_version being less than 12, if the call to torch.onnx.symbolic_opset12.dropout() is detected, then remove the code_one and add code_two to fix the API misuse."}
{"number": 177, "change": "class IvyModule(ivy.Module):\nif ivy.array_mode():\na, kw = ivy.args_to_native(*a, **kw)\n# noinspection PyUnresolvedReferences\n-        params_hk = self._native_module.init(ivy.functional.core.random.RNG, *a, **kw)\n+        params_hk = self._native_module.init(ivy.random.RNG, *a, **kw)\nparams_dict = _hk_flat_map_to_dict(params_hk)\nself._hk_params = ivy.Container(params_dict)\nparam_iterator = self._hk_params.to_iterator()\n", "fix_pattern": "Condition: The condition is when the array mode of \"ivy\" is enabled.\nPattern: The pattern is detecting the usage of \"ivy.functional.core.random.RNG\" in the code.\nCode one: The code being removed is \"params_hk = self._native_module.init(ivy.functional.core.random.RNG, *a, **kw)\".\nCode two: The code being added is \"params_hk = self._native_module.init(ivy.random.RNG, *a, **kw)\".\nFix Pattern: In the condition of \"ivy\" array mode being enabled, if the usage of \"ivy.functional.core.random.RNG\" is detected, then the code \"params_hk = self._native_module.init(ivy.functional.core.random.RNG, *a, **kw)\" should be changed to \"params_hk = self._native_module.init(ivy.random.RNG, *a, **kw)\" to fix the API misuse."}
{"number": 181, "change": "class TensorFlowEstimator(BaseEstimator):\nraise NotFittedError()\npredict_data_feeder = setup_predict_data_feeder(X)\npreds = []\n-        dropouts = tf.get_collection(DROPOUTS)\n-        feed_dict = {prob: 0.0 for prob in dropouts}\n+        dropouts = self._graph.get_collection(DROPOUTS)\n+        feed_dict = {prob: 1.0 for prob in dropouts}\nfor data in predict_data_feeder:\nfeed_dict[self._inp] = data\npreds.append(self._session.run(\n", "fix_pattern": "Condition: This fix pattern applies when the code encounters a NotFittedError.\nPattern: The pattern is to replace the code that sets all dropouts to 0.0 with code that sets all dropouts to 1.0.\nCode_one: The code that sets all dropouts to 0.0.\nCode_two: The code that sets all dropouts to 1.0.\nFix_pattern: In the condition of encountering a NotFittedError, if the code contains the pattern of setting dropouts to 0.0, then the code should be changed to set dropouts to 1.0 to fix the API misuse."}
{"number": 182, "change": "class GradientsTest(tf.test.TestCase):\nself.assertAllClose(eager_result, function_result)\nbackprop_result, numeric_result = tf.test.compute_gradient(\nm, [inp], delta=1e-3)\n-    self.assertAllClose(numeric_result, backprop_result, rtol=1e-2)\n+    self.assertAllClose(numeric_result, backprop_result, atol=1e-3)\nself.assertAllClose(tf.reshape(numeric_result, [-1]),\n-                        tf.reshape(eager_result, [-1]), rtol=1e-2)\n+                        tf.reshape(eager_result, [-1]), atol=1e-3)\n\ndef testEmbeddingLookupGradientsHaveKnownShape(self):\n", "fix_pattern": "<condition>: In the testEmbeddingLookupGradientsHaveKnownShape method.\n<pattern>: Remove the rtol=1e-2 from the self.assertAllClose() function call.\n<code_one>: self.assertAllClose(numeric_result, backprop_result, rtol=1e-2) tf.reshape(eager_result, [-1]), rtol=1e-2)\n<code_two>: self.assertAllClose(numeric_result, backprop_result, atol=1e-3) tf.reshape(eager_result, [-1]), atol=1e-3)\nFix_pattern: In the condition of the testEmbeddingLookupGradientsHaveKnownShape method, if the rtol=1e-2 is detected in the self.assertAllClose() function call, then change it to atol=1e-3 to fix the API misuse."}
{"number": 187, "change": "class Pix2PixModel(BaseModel):\ndef backward_D(self):\n# Fake\n# stop backprop to the generator by detaching fake_B\n-        fake_AB = self.fake_AB_pool.query(torch.cat((self.real_A, self.fake_B), 1))\n+        fake_AB = self.fake_AB_pool.query(torch.cat((self.real_A, self.fake_B), 1).data)\npred_fake = self.netD.forward(fake_AB.detach())\nself.loss_D_fake = self.criterionGAN(pred_fake, False)\n\n# Real\nreal_AB = torch.cat((self.real_A, self.real_B), 1)\npred_real = self.netD.forward(real_AB)\n-        self.loss_D_real = self.criterionGAN(self.pred_real, True)\n+        self.loss_D_real = self.criterionGAN(pred_real, True)\n\n# Combined loss\nself.loss_D = (self.loss_D_fake + self.loss_D_real) * 0.5\n", "fix_pattern": "<condition>: Inside the backward_D method of the Pix2PixModel class.\n<pattern>: The variable pred_real is used but not defined in the code.\n<code_one>: self.loss_D_real = self.criterionGAN(self.pred_real, True)\n<code_two>: self.loss_D_real = self.criterionGAN(pred_real, True)\nFix_pattern: In the condition of backward_D method, if the variable pred_real is detected without being defined, then change self.loss_D_real = self.criterionGAN(self.pred_real, True) to self.loss_D_real = self.criterionGAN(pred_real, True) to fix the API misuse."}
{"number": 193, "change": "class tensorflow_extractor(base_extractor):\nwriter.close()\nsess.run(init)\nsaver = tf.train.Saver()\n+            tf.train.export_meta_graph(\"kit.meta\", as_text=True)\nsaver.restore(sess, path + cls.architecture_map[architecture]['filename'])\nsave_path = saver.save(sess, path + \"imagenet_{}.ckpt\".format(architecture))\nprint(\"Model saved in file: %s\" % save_path)\n", "fix_pattern": "Condition: The condition is not clear in the given context.\nPattern: There is no specific pattern identified in the given code.\nCode One: No code was removed in the given context.\nCode Two: The added code is \"tf.train.export_meta_graph(\"kit.meta\", as_text=True)\".\nFix Pattern: No pre condition is needed. In the code, \"tf.train.export_meta_graph(\"kit.meta\", as_text=True)\" is added to fix the API misuse."}
{"number": 194, "change": "def test_auto_diagonal_gaussians(auto_class, Elbo):\nguide = auto_class(model, rank=1)\nelse:\nguide = auto_class(model)\n-    adam = optim.Adam({\"lr\": .001, \"betas\": (0.95, 0.999)})\n+    adam = optim.ClippedAdam({\"lr\": .01, \"betas\": (0.95, 0.999),\n+                              \"lrd\": 0.1 ** (1 / n_steps)})\nsvi = SVI(model, guide, adam, loss=Elbo())\n\nfor k in range(n_steps):\n", "fix_pattern": "Condition: The fix pattern applies when using the Adam optimizer with specific parameters.\nPattern: The pattern detects the use of the Adam optimizer with the default parameters.\nCode one: The code that is removed is the line that initializes the Adam optimizer with the default parameters.\nCode two: The code that is added is the line that initializes the ClippedAdam optimizer with custom parameters.\nFix pattern: In the condition of using the Adam optimizer, if the pattern of using default parameters is detected, then the code initializing the Adam optimizer needs to be changed to the code that initializes the ClippedAdam optimizer with custom parameters to fix the API misuse."}
{"number": 195, "change": "def test_frontend_backward_multi_channel(train, use_wpe, use_beamformer):\nfrontend.train()\nelse:\nfrontend.eval()\n+    torch.random.manual_seed(14)\nx = torch.randn(2, 1000, 2, requires_grad=True)\nx_lengths = torch.LongTensor([1000, 980])\ny, y_lengths = frontend(x, x_lengths)\n", "fix_pattern": "<condition>: No pre condition needed.\n<pattern>: No pattern detected.\n<code_one>: No code removed.\n<code_two>: torch.random.manual_seed(14)\nFix_pattern: In this fix, there is no specific condition or pattern detected. The code change is adding the line \"torch.random.manual_seed(14)\" to fix the API misuse."}
{"number": 197, "change": "class TestGradientScaling(unittest.TestCase):\noptimizer = FP16Optimizer.build_optimizer(self.namespace_dls, params)\n\nself.run_iter(model, params, optimizer)\n-        self.assertTrue(torch.all(optimizer.fp32_params.eq(torch.tensor([3.1000, 5.1000], device='cuda:0', requires_grad=True))))\n+        self.assertTrue(all(\n+            torch.all(fp32_params.eq(torch.tensor([3.1000, 5.1000], device='cuda:0', requires_grad=True)))\n+            for fp32_params in optimizer.fp32_params.values()\n+        ))\n\ndef test_memory_efficient(self):\nmodel = copy.deepcopy(self.model)\n", "fix_pattern": "<condition>: This fix does not have a clear condition in the given context.\n<pattern>: The pattern is to replace a specific assertion with a new assertion that iterates through all fp32_params and checks their values.\n<code_one>: The code that is being removed is the specific assertion that checks the equality of optimizer.fp32_params.\n<code_two>: The code that is being added is the new assertion that iterates through all fp32_params and checks their values.\nFix_pattern: In the condition of <condition>, if <pattern> is detected, then remove the <code_one> and add the new <code_two> to fix the API misuse."}
{"number": 198, "change": "class TestLuvToRgb(BaseTester):\n[0.06325728, 0.78878325, 0.74280596, 0.99514300, 0.47176042]]\n]], device=device, dtype=dtype)\n\n-        assert_allclose(kornia.color.luv_to_rgb(data), expected)\n+        assert_allclose(kornia.color.luv_to_rgb(data), expected, rtol=1e-4, atol=1e-4)\n\ndef test_forth_and_back(self, device, dtype):\ndata = torch.rand(3, 4, 5, device=device, dtype=dtype)\n", "fix_pattern": "Condition: There is a test method called \"test_forth_and_back\" in a class called \"TestLuvToRgb\".\nPattern: There is an assert statement that checks the output of the \"kornia.color.luv_to_rgb\" function.\nCode one: The original test had an assert statement with only two arguments.\nCode two: The fix added two additional arguments to the assert statement, \"rtol=1e-4\" and \"atol=1e-4\".\nFix pattern: In the condition of the \"test_forth_and_back\" method, if there is an assert statement checking the output of \"kornia.color.luv_to_rgb\", then the \"assert_allclose\" function should be called with two additional arguments, \"rtol=1e-4\" and \"atol=1e-4\", to fix the API misuse."}
{"number": 199, "change": "class EulerSamplingTest(tf.test.TestCase, parameterized.TestCase):\ntimes=times,\nnum_samples=num_samples,\ninitial_state=x0,\n-            random_type=tff.math.random.RandomType.SOBOL,\n+            random_type=tff.math.random.RandomType.HALTON,\ntime_step=0.01,\n-            seed=12134))\n+            seed=12134,\n+            skip=100,\n+            dtype=tf.float32))\n\n-    self.assertAllClose(paths.shape, (num_samples, 5, 2), atol=0)\n+    self.assertAllClose(paths.shape, (num_samples, 3, 2), atol=0)\nmeans = np.mean(paths, axis=0)\ntimes = np.reshape(times, [-1, 1])\nexpected_means = x0 + (2.0 / 3.0) * mu * np.power(times, 1.5)\n", "fix_pattern": "<condition>: The condition is not provided in the context section.\n<pattern>: There is no clear pattern identified in the code removed section.\n<code_one>: The code removed is related to the random seed and the type of random number generator used.\n<code_two>: The code added changes the random seed, the type of random number generator, and skips the first 100 samples.\nFix_pattern: In this fix, the code modifies the random seed, changes the random number generator type to HALTON, and skips the first 100 samples to fix the API misuse."}
{"number": 203, "change": "class BatchNorm(TransformModule):\nif self.training:\nmean, var = y.mean(0), y.var(0)\n\n-            # NOTE: The momentum variable agrees with the definition in e.g. `torch.nn.BatchNorm1d`\n-            self.moving_mean.mul_(1 - self.momentum).add_(mean * self.momentum)\n-            self.moving_variance.mul_(1 - self.momentum).add_(var * self.momentum)\n+            with torch.no_grad():\n+                # NOTE: The momentum variable agrees with the definition in e.g. `torch.nn.BatchNorm1d`\n+                self.moving_mean.mul_(1 - self.momentum).add_(mean * self.momentum)\n+                self.moving_variance.mul_(1 - self.momentum).add_(var * self.momentum)\n\n# During test time, use smoothed averages rather than the sample ones\nelse:\n", "fix_pattern": "<condition>: During test time\n<pattern>: Updating the moving mean and variance using exponential moving average with momentum\n<code_one>: self.moving_mean.mul_(1 - self.momentum).add_(mean * self.momentum)\n<code_two>: with torch.no_grad(): self.moving_mean.mul_(1 - self.momentum).add_(mean * self.momentum)\nFix_pattern: In the condition of \"During test time\", if the pattern of updating the moving mean and variance using exponential moving average with momentum is detected, then change the code \"self.moving_mean.mul_(1 - self.momentum).add_(mean * self.momentum)\" to \"with torch.no_grad(): self.moving_mean.mul_(1 - self.momentum).add_(mean * self.momentum)\" to fix the API misuse."}
{"number": 210, "change": "def train_model(params: Params, serialization_dir: str) -> Model:\n\nlogger.info(\"Creating a vocabulary using %s data.\", \", \".join(datasets_for_vocab_creation))\nvocab = Vocabulary.from_params(params.pop(\"vocabulary\", {}),\n-                                   Dataset([instance for key, dataset in all_datasets.items()\n-                                            for instance in dataset.instances\n-                                            if key in datasets_for_vocab_creation]))\n+                                   (instance for key, dataset in all_datasets.items()\n+                                    for instance in dataset\n+                                    if key in datasets_for_vocab_creation))\nvocab.save_to_files(os.path.join(serialization_dir, \"vocabulary\"))\n\nmodel = Model.from_params(vocab, params.pop('model'))\n", "fix_pattern": "<condition>: The condition is that the code is creating a vocabulary using certain datasets.\n<pattern>: The pattern is to iterate through the instances of the datasets and filter them based on a condition.\n<code_one>: The code that is removed is Dataset([instance for key, dataset in all_datasets.items() for instance in dataset.instances if key in datasets_for_vocab_creation]).\n<code_two>: The code that is added is (instance for key, dataset in all_datasets.items() for instance in dataset if key in datasets_for_vocab_creation).\nFix_pattern: In the condition of creating a vocabulary using specific datasets, if the pattern of iterating through instances and filtering based on a condition is detected, then the code that creates the dataset should be changed to iterate through dataset instances and filter them based on the condition, by removing the \"Dataset()\" wrapper."}
{"number": 214, "change": "class KerasBackend(AbstractBackend):\nreturn keras\n\ndef einsum(self, pattern, *x):\n-        return self.tf.einsum(pattern, *x)\n+        return self.tf.vectorized_map(\n+            functools.partial(self.tf.einsum, pattern),\n+            *x\n+        )\n\n\nclass OneFlowBackend(AbstractBackend):\n", "fix_pattern": "<condition>: The condition is when using the Keras backend. \n<pattern>: The pattern is to call the `einsum` function with a specific pattern. \n<code_one>: The original code was calling the `einsum` function from the Keras backend. \n<code_two>: The fixed code replaces the `einsum` function call with the `vectorized_map` function, which takes a partial function of the `einsum` function with the desired pattern as an argument. \nFix_pattern: In the condition of using the Keras backend, if calling the `einsum` function with a specific pattern is detected, then the code should be changed to use the `vectorized_map` function with a partial function of `einsum` with the desired pattern to fix the API misuse."}
{"number": 215, "change": "def run_modelmerger(primary_model_name, secondary_model_name, teritary_model_nam\nt2 = theta_2.get(key, torch.zeros_like(theta_1[key]))\ntheta_1[key] = theta_func1(theta_1[key], t2)\nelse:\n-                    theta_1[key] = 0\n+                    theta_1[key] = torch.zeros_like(theta_1[key])\ndel theta_2, teritary_model\n\nfor key in tqdm.tqdm(theta_0.keys()):\n", "fix_pattern": "<condition>: The condition is that the variable \"key\" is present in the list of keys in the dictionary \"theta_0\".\n<pattern>: The pattern is that the variable \"theta_1\" is set to 0.\n<code_one>: The code that was removed is \"theta_1[key] = 0\".\n<code_two>: The code that was added is \"theta_1[key] = torch.zeros_like(theta_1[key])\".\nFix_pattern: In the condition of the variable \"key\" being present in \"theta_0\" keys, if the variable \"theta_1\" is set to 0, then the code is changed to \"theta_1[key] = torch.zeros_like(theta_1[key])\" to fix the API misuse."}
{"number": 216, "change": "class DefaultClassifier(Classifier):\n\ndef _calculate_loss(self, scores, labels):\n\n-        if len(labels) == 0: return torch.tensor(0., requires_grad=True), 1\n+        if len(labels) == 0: return torch.tensor(0., requires_grad=True, device=flair.device), 1\n\nif self.multi_label:\nlabels = torch.tensor([[1 if l in all_labels_for_point else 0 for l in self.label_dictionary.get_items()]\n", "fix_pattern": "<condition>: The condition is identified as self.multi_label being True.\n<pattern>: The pattern is that if len(labels) == 0, then the code is returning torch.tensor(0., requires_grad=True), 1.\n<code_one>: The code that is being removed is \"return torch.tensor(0., requires_grad=True), 1\".\n<code_two>: The code that is being added is \"return torch.tensor(0., requires_grad=True, device=flair.device), 1\".\nFix_pattern: In the condition of self.multi_label being True, if len(labels) == 0 is detected, then the code is changed from \"return torch.tensor(0., requires_grad=True), 1\" to \"return torch.tensor(0., requires_grad=True, device=flair.device), 1\" to fix the API misuse."}
{"number": 217, "change": "class EmbeddingLayer(nn.Module):\ntorch.empty(weight_shape[0],\nweight_shape[1],\ndtype=dtype,\n-                        device=torch.cuda.current_device()))\n+                        device=get_accelerator().current_device_name()))\n\ndef forward(self, input):\nreturn F.embedding(input, self.weight)\n", "fix_pattern": "Condition: The code is using the torch.empty() function with a \"dtype\" argument.\nPattern: The code is specifying a specific device using \"torch.cuda.current_device()\".\nCode One: \"torch.cuda.current_device()\"\nCode Two: \"get_accelerator().current_device_name()\"\nFix Pattern: In the condition of using torch.empty() with a specified \"dtype\", if the code is using \"torch.cuda.current_device()\" to specify the device, then change it to \"get_accelerator().current_device_name()\" to fix the API misuse."}
{"number": 218, "change": "class MultiActionDistribution(ActionDistribution):\n\ndef logp(self, x):\n\"\"\"The log-likelihood of the action distribution.\"\"\"\n-        split_list = self.reshaper.split_tensor(x)\n+        split_list = tf.split(x, len(self.input_lens), axis=1)\nfor i, distribution in enumerate(self.child_distributions):\n# Remove extra categorical dimension\nif isinstance(distribution, Categorical):\n", "fix_pattern": "Condition: The condition is checking if the distribution object is an instance of the Categorical class.\nPattern: The pattern is the removal of the code that splits the tensor using the reshaper object.\nCode One: split_list = self.reshaper.split_tensor(x)\nCode Two: split_list = tf.split(x, len(self.input_lens), axis=1)\nFix Pattern: In the condition of checking if the distribution is Categorical, the code for splitting the tensor is changed from using the reshaper object to using the tf.split() function to fix the API misuse."}
{"number": 219, "change": "class CategoricalOneHotPolicy(StochasticPolicy):\ndef __init__(self, network, session, state, random, action_count=1, scope='policy'):\nwith tf.variable_scope(scope):\naction_layer = linear(layer_input=network.output, config={'num_outputs': action_count}, scope='outputs')\n+            action_layer = tf.reshape(action_layer, [-1, action_count])\n+\ndistribution = tf.nn.softmax(action_layer)\nsample = tf.multinomial(distribution, 1)\n", "fix_pattern": "<condition>: The condition is a situation where the output of the \"action_layer\" needs to be reshaped.\n<pattern>: The pattern is detecting that the \"action_layer\" needs to be reshaped.\n<code_one>: There is no code one mentioned in the code removed section.\n<code_two>: The code added is \"action_layer = tf.reshape(action_layer, [-1, action_count])\".\nFix_pattern: In the condition where the output of \"action_layer\" needs to be reshaped, the fix is to add the code \"action_layer = tf.reshape(action_layer, [-1, action_count])\" to reshape the \"action_layer\"."}
{"number": 221, "change": "class SingleRoIExtractor(nn.Module):\nout_size = self.roi_layers[0].out_size\nnum_levels = len(feats)\ntarget_lvls = self.map_roi_levels(rois, num_levels)\n-        roi_feats = torch.cuda.FloatTensor(rois.size()[0], self.out_channels,\n-                                           out_size, out_size).fill_(0)\n+        roi_feats = feats[0].new_zeros(rois.size()[0], self.out_channels,\n+                                       out_size, out_size)\nfor i in range(num_levels):\ninds = target_lvls == i\nif inds.any():\n", "fix_pattern": "<condition>: The condition is that the variable \"inds\" is True.\n<pattern>: The pattern is that the tensor \"roi_feats\" is initialized with zeros using the function \"fill_\" on a CUDA tensor.\n<code_one>: The code that was removed is \"roi_feats = torch.cuda.FloatTensor(rois.size()[0], self.out_channels, out_size, out_size).fill_(0)\".\n<code_two>: The code that was added is \"roi_feats = feats[0].new_zeros(rois.size()[0], self.out_channels, out_size, out_size)\".\nFix_pattern: In the condition where \"inds\" is True, the previous code that initialized \"roi_feats\" with zeros using a CUDA tensor is replaced with the new code that initializes \"roi_feats\" with zeros using \"new_zeros()\" function."}
{"number": 223, "change": "class MobileNetV3LargeEncoder(MobileNetV3):\n)\n\nif pretrained:\n-            self.load_state_dict(load_state_dict_from_url(\n+            self.load_state_dict(torch.hub.load_state_dict_from_url(\n'https://download.pytorch.org/models/mobilenet_v3_large-8738ca79.pth'))\n\ndel self.avgpool\n", "fix_pattern": "<condition>: The condition is that the variable \"pretrained\" must be true.\n<pattern>: The pattern is the misuse of the \"load_state_dict\" function.\n<code_one>: The code that is removed is \"self.load_state_dict(load_state_dict_from_url(\".\n<code_two>: The code that is added is \"self.load_state_dict(torch.hub.load_state_dict_from_url(\".\nFix_pattern: In the condition of \"pretrained\" being true, if the \"load_state_dict\" function is detected, then change the code from \"self.load_state_dict(load_state_dict_from_url(\" to \"self.load_state_dict(torch.hub.load_state_dict_from_url(\" to fix the API misuse."}
{"number": 224, "change": "def make_non_pad_mask(lengths):\n\"\"\"\nbs = int(len(lengths))\nmaxlen = int(max(lengths))\n-    mask = torch.zeros(bs, maxlen).byte()\n+    mask = torch.zeros(bs, maxlen, dtype=torch.uint8)\nfor i, l in enumerate(lengths):\nmask[i, :l] = 1\n", "fix_pattern": "<condition>: The function is creating a non-padding mask based on input lengths.\n<pattern>: The original code initializes the mask as a byte tensor, but it is modified to be a uint8 tensor.\n<code_one>: mask = torch.zeros(bs, maxlen).byte()\n<code_two>: mask = torch.zeros(bs, maxlen, dtype=torch.uint8)\nFix_pattern: In the condition of creating a non-padding mask based on input lengths, if the mask is initialized as a byte tensor, then change it to be initialized as a uint8 tensor to fix the API misuse."}
{"number": 229, "change": "class TFXGLMPreTrainedModel(TFPreTrainedModel):\n@tf.function(\ninput_signature=[\n{\n-                \"input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"input_ids\"),\n-                \"attention_mask\": tf.TensorSpec((None, None), tf.int32, name=\"attention_mask\"),\n+                \"input_ids\": tf.TensorSpec((None, None), tf.int64, name=\"input_ids\"),\n+                \"attention_mask\": tf.TensorSpec((None, None), tf.int64, name=\"attention_mask\"),\n}\n]\n)\n", "fix_pattern": "<condition>: The code is using TensorFlow's tf.function decorator with an input signature.\n<pattern>: The code is using tf.TensorSpec to define the data type and shape of input tensors in the input signature.\n<code_one>: \"input_ids\" and \"attention_mask\" tensors are specified with tf.int32 data type.\n<code_two>: \"input_ids\" and \"attention_mask\" tensors are changed to tf.int64 data type.\nFix_pattern: In the condition of using tf.function input signature, if \"input_ids\" and \"attention_mask\" tensors are specified with tf.int32 data type, then change them to tf.int64 data type to fix the API misuse."}
{"number": 231, "change": "class SpeedyResNet:\nnn.Linear(512, num_classes, bias=False)\n]\n\n-  # note, pytorch just uses https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html instead of logsoftmax\n-  def __call__(self, x): return x.sequential(self.net).logsoftmax()\n+  # note, pytorch just uses https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html instead of log_softmax\n+  def __call__(self, x): return x.sequential(self.net).log_softmax()\n\nfrom extra.jit import TinyJit\n@TinyJit\n", "fix_pattern": "<condition>: No clear condition is needed.\n<pattern>: The pattern detected is that the code used \"logsoftmax()\" instead of \"log_softmax()\".\n<code_one>: The code that was removed was \"return x.sequential(self.net).logsoftmax()\".\n<code_two>: The code that was added was \"return x.sequential(self.net).log_softmax()\".\nFix_pattern: In the condition of no clear condition, if the pattern of using \"logsoftmax()\" is detected, then change the code \"logsoftmax()\" to \"log_softmax()\" to fix the API misuse."}
{"number": 234, "change": "class EulerAncestralDiscreteScheduler(SchedulerMixin, ConfigMixin):\n\nprev_sample = sample + derivative * dt\n\n-        device = model_output.device if torch.is_tensor(model_output) else torch.device(\"cpu\")\n+        device = model_output.device\nif device.type == \"mps\":\n# randn does not work reproducibly on mps\nnoise = torch.randn(model_output.shape, dtype=model_output.dtype, device=\"cpu\", generator=generator).to(\n", "fix_pattern": "<condition>: The condition is checking if the device type is \"mps\".\n<pattern>: The pattern is that the device assignment is unnecessarily complex.\n<code_one>: The code that was removed is assigning the device based on the model output device or using the CPU device if the model output is not a tensor.\n<code_two>: The code that was added is simplifying the device assignment by directly assigning the device based on the model output.\nFix_pattern: In the condition of checking if the device type is \"mps\", if the device assignment using the model output device or the CPU device is detected, then remove the complex device assignment and directly assign the device based on the model output to fix the API misuse."}
{"number": 236, "change": "def _create_fc(num_features, num_classes, use_conv=False):\nelif use_conv:\nfc = nn.Conv2d(num_features, num_classes, 1, bias=True)\nelse:\n-        # NOTE: using my Linear wrapper that fixes AMP + torchscript casting issue\n-        fc = Linear(num_features, num_classes, bias=True)\n+        fc = nn.Linear(num_features, num_classes, bias=True)\nreturn fc\n", "fix_pattern": "Condition: \nThe condition is that the variable \"use_conv\" needs to be true.\n\nPattern: \nThe pattern is the incorrect usage of the \"Linear\" class as a replacement for \"nn.Conv2d\" when \"use_conv\" is true.\n\nCode one: \nThe code that needs to be remove is \"fc = Linear(num_features, num_classes, bias=True)\".\n\nCode two: \nThe code that needs to be added is \"fc = nn.Linear(num_features, num_classes, bias=True)\".\n\nFix pattern: \nIn the condition of \"use_conv\" being true, if the incorrect usage of \"Linear\" is detected, then replace it with \"nn.Linear\" to fix the API misuse."}
{"number": 241, "change": "class GenerationMixin:\ncontinue  # don't waste resources running the code we don't need\n\nnext_token_logits = outputs.logits[:, -1, :]\n-\n-            # hack: adjust tokens for Marian. For Marian we have to make sure that the `pad_token_id`\n-            # cannot be generated both before and after the `nn.functional.log_softmax` operation.\n-            next_token_logits = outputs.logits[:, -1, :]\n# hack: adjust tokens for Marian. For Marian we have to make sure that the `pad_token_id`\n# cannot be generated both before and after the `nn.functional.log_softmax` operation.\nnext_token_logits = self.adjust_logits_during_generation(next_token_logits, cur_len=cur_len)\n", "fix_pattern": "Condition: The code segment is using the \"GenerationMixin\" class.\nPattern: The adjustment of tokens for the \"Marian\" model needs to be performed before the \"nn.functional.log_softmax\" operation.\nCode One: next_token_logits = outputs.logits[:, -1, :]\nCode Two: next_token_logits = self.adjust_logits_during_generation(next_token_logits, cur_len=cur_len)\nFix Pattern: In the condition of using the GenerationMixin class, if the need to adjust tokens for the Marian model is detected, then change the code segment \"next_token_logits = outputs.logits[:, -1, :]\" to \"next_token_logits = self.adjust_logits_during_generation(next_token_logits, cur_len=cur_len)\" to fix the API misuse."}
{"number": 244, "change": "def test_runway_sd_1_5_negative_prompt(sd_device, strategy, sampler):\ndef test_cv2(strategy, cv2_flag, cv2_radius):\nmodel = ModelManager(\nname=\"cv2\",\n-        device=device,\n+        device=torch.device(device),\n)\ncfg = get_config(strategy, cv2_flag=cv2_flag, cv2_radius=cv2_radius)\nassert_equal(\n", "fix_pattern": "Condition: The condition is not mentioned in the given code snippet.\nPattern: The pattern is to replace \"device=device\" with \"device=torch.device(device)\".\nCode One: The code section \"device=device\".\nCode Two: The code section \"device=torch.device(device)\".\nFix Pattern: In the condition of <condition>, if <pattern> is detected, then remove the <code_one> and add/change it to <code_two> to fix the API misuse."}
{"number": 245, "change": "class Model(ModelDesc):\nsummary.add_moving_summary(self.cost)\n\ndef _get_optimizer(self):\n-        lr = tf.get_variable('learning_rate', initializer=1e-3, trainable=False)\n+        lr = tf.get_variable('learning_rate', initializer=self.learning_rate, trainable=False)\nopt = tf.train.AdamOptimizer(lr, epsilon=1e-3)\nreturn optimizer.apply_grad_processors(\nopt, [gradproc.GlobalNormClip(10), gradproc.SummaryGradient()])\n", "fix_pattern": "Condition: The code is setting a learning rate variable.\nPattern: The learning rate variable is being initialized with a hardcoded value.\nCode one: `lr = tf.get_variable('learning_rate', initializer=1e-3, trainable=False)`\nCode two: `lr = tf.get_variable('learning_rate', initializer=self.learning_rate, trainable=False)`\nFix pattern: In the condition of setting the learning rate variable, if it is initialized with a hardcoded value, then change the code to initialize it with `self.learning_rate` to fix the API misuse."}
{"number": 246, "change": "class Network(object):\nweights = self.make_var('weights', shape=[dim, num_out])\nbiases = self.make_var('biases', [num_out])\nop = tf.nn.relu_layer if relu else tf.nn.xw_plus_b\n-            fc = op(feed_in, weights, biases, name=scope.name)\n+            #fc = op(feed_in, weights, biases, name=scope.name)\n+            fc = op(feed_in, weights, biases, name=name)\nreturn fc\n", "fix_pattern": "<condition>: The condition is whether the API misuse involves the use of the `tf.nn.relu_layer` function or the `tf.nn.xw_plus_b` function.\n<pattern>: The pattern is detecting the use of `op` function with the `tf.nn.relu_layer` or `tf.nn.xw_plus_b` function.\n<code_one>: The code that was removed is `fc = op(feed_in, weights, biases, name=scope.name)`.\n<code_two>: The code that was added is `fc = op(feed_in, weights, biases, name=name)`.\nFix_pattern: In the condition of using either `tf.nn.relu_layer` or `tf.nn.xw_plus_b` functions, the `fc` variable should be assigned using the `op` function with the `name` parameter instead of the `scope.name` parameter to fix the API misuse."}
{"number": 250, "change": "def train_cnn_and_compute_accuracy(params, steps, train_images, train_labels, va\n# Do the training and evaluation.\nwith tf.Session() as sess:\n# Initialize the network weights.\n-    sess.run(tf.initialize_all_variables())\n+    sess.run(tf.global_variables_initializer())\nfor i in range(1, steps + 1):\n# Fetch the next batch of data.\nimage_batch = get_batch(train_images, i, batch_size)\n", "fix_pattern": "<condition>: No pre condition needed.\n<pattern>: tf.initialize_all_variables() is deprecated and should be replaced.\n<code_one>: sess.run(tf.initialize_all_variables())\n<code_two>: sess.run(tf.global_variables_initializer())\nFix_pattern: In the condition of tf.Session(), if tf.initialize_all_variables() is detected, then remove tf.initialize_all_variables() and add sess.run(tf.global_variables_initializer()) to fix the API misuse."}
{"number": 252, "change": "class DeepSpeedDataLoader(object):\nelse:\nif data_sampler is None:\ndata_sampler = RandomSampler(dataset)\n-                device_count = torch.cuda.device_count()\n+                device_count = get_accelerator().device_count()\nbatch_size *= device_count\n\nif num_local_io_workers is None:\n", "fix_pattern": "<condition>: When \"data_sampler\" is None.\n<pattern>: When the condition is met, the code is changed from \"device_count = torch.cuda.device_count()\" to \"device_count = get_accelerator().device_count()\".\n<code_one>: \"device_count = torch.cuda.device_count()\"\n<code_two>: \"device_count = get_accelerator().device_count()\"\nFix_pattern: In the condition of \"data_sampler\" being None, the code is changed from \"device_count = torch.cuda.device_count()\" to \"device_count = get_accelerator().device_count()\" to fix the API misuse."}
{"number": 257, "change": "def asin(x: Union[tf.Tensor, tf.Variable]) -> Union[tf.Tensor, tf.Variable]:\nreturn tf.asin(x)\n\n\n-def asinh(\n-        x: Union[tf.Tensor, tf.Variable]\n-) -> Union[tf.Tensor, tf.Variable]:\n-    x = tf.cast(x, tf.float32)\n+def asinh(x: Union[tf.Tensor, tf.Variable]) -> Union[tf.Tensor, tf.Variable]:\nreturn tf.asinh(x)\n", "fix_pattern": "Condition: The condition in this fix pattern is the use of the `asin` function.\nPattern: The pattern is the replacement of the `asinh` function with the `asin` function.\nCode one: The code removed is the implementation of the `asinh` function.\nCode two: The code added is the usage of the `asin` function.\n\nFix pattern: In the condition of using the `asin` function, if the `asinh` function is detected, then remove the `asinh` function implementation and use the `asin` function instead to fix the API misuse."}
{"number": 259, "change": "class RagTokenForGeneration(RagPreTrainedModel):\nn_docs = n_docs if n_docs is not None else self.config.n_docs\n\n# RAG-token marginalization\n-        seq_logprobs = torch.nn.functional.log_softmax(seq_logits, dim=-1).view(\n+        seq_logprobs = nn.functional.log_softmax(seq_logits, dim=-1).view(\nseq_logits.shape[0] // n_docs, n_docs, -1, seq_logits.size(-1)\n)\ndoc_logprobs = torch.log_softmax(doc_scores, dim=1)\n", "fix_pattern": "<condition>: The condition is that the variable \"n_docs\" is not None.\n<pattern>: The pattern is that the \"seq_logprobs\" tensor is being converted to log probabilities using torch.nn.functional.log_softmax and then reshaped.\n<code_one>: The code being removed is \"torch.nn.functional.log_softmax(seq_logits, dim=-1).view(\"\n<code_two>: The code being added is \"nn.functional.log_softmax(seq_logits, dim=-1).view(\"\nFix_pattern: In the condition of \"n_docs\" not being None, if \"seq_logprobs\" needs to be converted to log probabilities and reshaped, then change \"torch.nn.functional.log_softmax\" to \"nn.functional.log_softmax\"."}
{"number": 266, "change": "class TestStackedSelfAttention(AllenNlpTestCase):\nfeedforward_hidden_dim=5,\nnum_layers=3,\nnum_attention_heads=3)\n-        inputs = Variable(torch.randn([3, 5, 9]))\n+        inputs = torch.randn([3, 5, 9])\nencoder_output = encoder(inputs, None)\nassert list(encoder_output.size()) == [3, 5, 12]\n", "fix_pattern": "<condition>: No pre condition is needed.\n<pattern>: If a variable `inputs` is defined as `Variable(torch.randn([3, 5, 9]))`.\n<code_one>: `inputs = Variable(torch.randn([3, 5, 9]))`.\n<code_two>: `inputs = torch.randn([3, 5, 9])`.\nFix_pattern: In the condition of no specific pre condition, if the pattern of defining `inputs` as a `Variable` with the shape `[3, 5, 9]` is detected, then remove the `Variable` wrapper to fix the API misuse."}
{"number": 267, "change": "def qr(A: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:  # pragma: no cove\n\"\"\"\nLike torch.linalg.qr.\n\"\"\"\n-    if hasattr(torch.linalg, \"qr\"):\n+    if hasattr(torch, \"linalg\") and hasattr(torch.linalg, \"qr\"):\n# PyTorch version >= 1.9\nreturn torch.linalg.qr(A)\nreturn torch.qr(A)\n", "fix_pattern": "<condition>: Check if the attribute \"torch.linalg.qr\" exists.\n<pattern>: Remove the condition \"if hasattr(torch.linalg, \"qr\"):\".\n<code_one>: if hasattr(torch.linalg, \"qr\"):\n<code_two>: if hasattr(torch, \"linalg\") and hasattr(torch.linalg, \"qr\"):\nFix_pattern: In the condition of checking if the attribute \"torch.linalg.qr\" exists, if the condition is detected, remove the code \"if hasattr(torch.linalg, \"qr\"):\" and replace it with \"if hasattr(torch, \"linalg\") and hasattr(torch.linalg, \"qr\"):\" to fix the API misuse."}
{"number": 268, "change": "def prepare_bart_inputs_dict(\nif decoder_attention_mask is None:\ndecoder_attention_mask = decoder_input_ids.ne(config.pad_token_id)\nif head_mask is None:\n-        head_mask = torch.ones(config.encoder_layers, config.encoder_attention_heads)\n+        head_mask = torch.ones(config.encoder_layers, config.encoder_attention_heads, device=torch_device)\nif decoder_head_mask is None:\n-        decoder_head_mask = torch.ones(config.decoder_layers, config.decoder_attention_heads)\n+        decoder_head_mask = torch.ones(config.decoder_layers, config.decoder_attention_heads, device=torch_device)\nreturn {\n\"input_ids\": input_ids,\n\"decoder_input_ids\": decoder_input_ids,\n", "fix_pattern": "<condition>: `decoder_attention_mask` is None\n<pattern>: `head_mask` and `decoder_head_mask` are assigned `torch.ones()` with specific parameters\n<code_one>: `head_mask = torch.ones(config.encoder_layers, config.encoder_attention_heads)`\n<code_two>: `head_mask = torch.ones(config.encoder_layers, config.encoder_attention_heads, device=torch_device)`\nFix_pattern: In the condition where `decoder_attention_mask` is None, the fix pattern is to add the `device=torch_device` parameter to the `torch.ones()` assignment for `head_mask` to fix the API misuse."}
{"number": 269, "change": "class PNDMSchedulerTest(SchedulerCommonTest):\nscheduler_config = self.get_scheduler_config(steps_offset=1)\nscheduler = scheduler_class(**scheduler_config)\nscheduler.set_timesteps(10)\n-        assert np.equal(\n+        assert torch.equal(\nscheduler.timesteps,\n-            np.array([901, 851, 851, 801, 801, 751, 751, 701, 701, 651, 651, 601, 601, 501, 401, 301, 201, 101, 1]),\n-        ).all()\n+            torch.LongTensor(\n+                [901, 851, 851, 801, 801, 751, 751, 701, 701, 651, 651, 601, 601, 501, 401, 301, 201, 101, 1]\n+            ),\n+        )\n\ndef test_betas(self):\nfor beta_start, beta_end in zip([0.0001, 0.001], [0.002, 0.02]):\n", "fix_pattern": "<condition>: No clear condition can be identified.\n<pattern>: The code is using the numpy array's `equal` method to compare arrays.\n<code_one>: `assert np.equal(np.array([901, 851, 851, 801, 801, 751, 751, 701, 701, 651, 651, 601, 601, 501, 401, 301, 201, 101, 1]),).all()`\n<code_two>: `assert torch.equal(torch.LongTensor([901, 851, 851, 801, 801, 751, 751, 701, 701, 651, 651, 601, 601, 501, 401, 301, 201, 101, 1]))`\nFix_pattern: In the condition of no clear condition, if the code is using the numpy array's `equal` method to compare arrays, then remove the `all()` method and replace the numpy array and method with a torch tensor and `torch.equal()` method."}
{"number": 272, "change": "class GradTTS(DiffusionPipeline):\nmu_y = mu_y.transpose(1, 2)\n\n# Sample latent representation from terminal distribution N(mu_y, I)\n-        z = mu_y + torch.randn_like(mu_y, device=mu_y.device) / temperature\n+        z = mu_y + torch.randn(mu_y.shape, device=mu_y.device, generator=generator) / temperature\n\nxt = z * y_mask\nh = 1.0 / num_inference_steps\n", "fix_pattern": "<condition>: No clear condition is needed.\n<pattern>: In the condition of GradTTS class, if a code that adds random noise to mu_y divided by temperature is detected, then remove it.\n<code_one>: z = mu_y + torch.randn_like(mu_y, device=mu_y.device) / temperature\n<code_two>: z = mu_y + torch.randn(mu_y.shape, device=mu_y.device, generator=generator) / temperature\nFix_pattern: In the condition of GradTTS class, if the code that adds random noise to mu_y divided by temperature is detected, then replace it with the code that adds random noise to mu_y taking into account the shape and generator."}
{"number": 273, "change": "class NanDetector:\ngradients = {}\nfor name, param in self.named_parameters:\nif param.grad is not None:\n-                grad_norm = torch.norm(param.grad.data, p=2, dtype=torch.float32)\n+                grad_norm = torch.norm(param.grad.data.float(), p=2)\nnorm[name] = grad_norm.item()\nif torch.isnan(grad_norm).any() or torch.isinf(grad_norm).any():\ngradients[name] = param.grad.data\n", "fix_pattern": "<condition>: The condition is when there is a detection of any NaN or infinity values in the grad_norm tensor.\n<pattern>: The pattern is to replace the code of calculating grad_norm with a fixed pattern that converts the data type to float before calculating the norm.\n<code_one>: The code that was removed is `grad_norm = torch.norm(param.grad.data, p=2, dtype=torch.float32)`.\n<code_two>: The code that was added is `grad_norm = torch.norm(param.grad.data.float(), p=2)`.\nFix_pattern: In the condition of detecting NaN or infinity values in the grad_norm tensor, the fix involves removing the original code for calculating grad_norm and replacing it with a new code that converts the data type to float before calculating the norm."}
{"number": 277, "change": "def fpn_map_rois_to_levels(boxes):\nBe careful that the returned tensor could be empty.\n\"\"\"\nsqrtarea = tf.sqrt(tf_area(boxes))\n-    level = tf.to_int32(tf.floor(\n-        4 + tf.log(sqrtarea * (1. / 224) + 1e-6) * (1.0 / np.log(2))))\n+    level = tf.cast(tf.floor(\n+        4 + tf.log(sqrtarea * (1. / 224) + 1e-6) * (1.0 / np.log(2))), tf.int32)\n\n# RoI levels range from 2~5 (not 6)\nlevel_ids = [\n", "fix_pattern": "<condition>: The code is calculating the level of RoIs based on the sqrtarea of the boxes.\n<pattern>: The pattern detected is the use of tf.to_int32 to cast the result of the calculation to an integer.\n<code_one>: The code being removed is the tf.to_int32() function.\n<code_two>: The code being added is tf.cast() function, with tf.int32 as the argument.\nFix_pattern: In the condition of calculating the RoI levels based on sqrtarea of the boxes, if the tf.to_int32() function is used to cast the result to an integer, then remove tf.to_int32() and add tf.cast() with tf.int32 as the argument to fix the API misuse."}
{"number": 278, "change": "def mesh_laplacian_smoothing(meshes, method: str = \"uniform\"):\nelif method == \"cot\":\nloss = L.mm(verts_packed) * norm_w - verts_packed\nelif method == \"cotcurv\":\n-        loss = (L.mm(verts_packed) - verts_packed) * norm_w\n+        loss = (L.mm(verts_packed) - L_sum * verts_packed) * norm_w\nloss = loss.norm(dim=1)\n\nloss = loss * weights\n", "fix_pattern": "<condition>: The condition is that the method parameter is set to \"cot\".\n\n<pattern>: The pattern detected is the calculation of the loss using matrix multiplication, subtraction, and multiplication.\n\n<code_one>: The code that was removed is \"(L.mm(verts_packed) - verts_packed) * norm_w\".\n\n<code_two>: The code that was added is \"(L.mm(verts_packed) - L_sum * verts_packed) * norm_w\".\n\nFix_pattern: In the condition of \"cot\", if the pattern of \"(L.mm(verts_packed) - verts_packed) * norm_w\" is detected, then change the code to \"(L.mm(verts_packed) - L_sum * verts_packed) * norm_w\" to fix the API misuse."}
{"number": 281, "change": "def test_hub_oneshot(space_type, strategy_type):\nNDS_SPACES = ['amoeba', 'darts', 'pnas', 'enas', 'nasnet']\nif strategy_type == 'proxyless':\nif 'width' in space_type or 'depth' in space_type or \\\n-                any(space_type.startswith(prefix) for prefix in NDS_SPACES + ['proxylessnas', 'mobilenetv3']):\n+                any(space_type.startswith(prefix) for prefix in NDS_SPACES + ['proxylessnas', 'mobilenetv3', 'autoformer']):\npytest.skip('The space has used unsupported APIs.')\nif strategy_type in ['darts', 'gumbel'] and space_type == 'mobilenetv3':\npytest.skip('Skip as it consumes too much memory.')\n", "fix_pattern": "Condition: The condition is when `strategy_type` is equal to 'proxyless'.\nPattern: The pattern is checking if `space_type` starts with any of the prefixes in `NDS_SPACES` or 'proxylessnas' or 'mobilenetv3'.\nCode One: The code being removed is any(space_type.startswith(prefix) for prefix in NDS_SPACES + ['proxylessnas', 'mobilenetv3']).\nCode Two: The code being added is any(space_type.startswith(prefix) for prefix in NDS_SPACES + ['proxylessnas', 'mobilenetv3', 'autoformer']).\nFix Pattern: In the condition of `strategy_type` being 'proxyless', if `space_type` starts with any of the prefixes in `NDS_SPACES` or 'proxylessnas' or 'mobilenetv3', then change the removed code to the added code to fix the API misuse."}
{"number": 282, "change": "class GCNConv(MessagePassing):\nx = torch.matmul(x, self.weight)\n\nif not self.cached or self.cached_result is None:\n-            edge_index, norm = GCNConv.norm(edge_index,\n-                                            x.size(0), edge_weight,\n+            edge_index, norm = GCNConv.norm(edge_index, x.size(0), edge_weight,\nself.improved, x.dtype)\nself.cached_result = edge_index, norm\n", "fix_pattern": "Condition: The condition is \"if not self.cached or self.cached_result is None\".\nPattern: The pattern is \"GCNConv.norm(edge_index, x.size(0), edge_weight\".\nCode one: The code one is \"edge_index, norm = GCNConv.norm(edge_index, x.size(0), edge_weight\".\nCode two: The code two is \"edge_index, norm = GCNConv.norm(edge_index, x.size(0), edge_weight\".\nFix pattern: In the condition of \"if not self.cached or self.cached_result is None\", if the pattern \"GCNConv.norm(edge_index, x.size(0), edge_weight\" is detected, then remove the code \"edge_index, norm = GCNConv.norm(edge_index, x.size(0), edge_weight\" and add the code \"edge_index, norm = GCNConv.norm(edge_index, x.size(0), edge_weight\" to fix the API misuse."}
{"number": 283, "change": "class CTC(torch.nn.Module):\nif self.ctc_type == \"builtin\":\nolens = to_device(ys_hat, torch.LongTensor([len(s) for s in ys]))\nhlens = hlens.long()\n+            ys_pad = torch.cat(ys)  # without this the code breaks for asr_mix\nself.loss = self.loss_fn(ys_hat, ys_pad, hlens, olens)\nelse:\nself.loss = None\n", "fix_pattern": "<condition>: The condition is if the value of self.ctc_type is not equal to \"builtin\".\n<pattern>: The pattern is the absence of the line \"ys_pad = torch.cat(ys)\".\n<code_one>: No code is removed.\n<code_two>: The added code is \"ys_pad = torch.cat(ys)\".\nFix_pattern: In the condition where self.ctc_type is not \"builtin\", the code \"ys_pad = torch.cat(ys)\" is added to fix the API misuse and prevent a code breakage."}
{"number": 296, "change": "class DeformableDetrModelIntegrationTests(unittest.TestCase):\nresults = feature_extractor.post_process_object_detection(\noutputs, threshold=0.3, target_sizes=[image.size[::-1]]\n)[0]\n-        expected_scores = torch.tensor([0.7999, 0.7894, 0.6331, 0.4720, 0.4382])\n+        expected_scores = torch.tensor([0.7999, 0.7894, 0.6331, 0.4720, 0.4382]).to(torch_device)\nexpected_labels = [17, 17, 75, 75, 63]\n-        expected_slice_boxes = torch.tensor([16.5028, 52.8390, 318.2544, 470.7841])\n+        expected_slice_boxes = torch.tensor([16.5028, 52.8390, 318.2544, 470.7841]).to(torch_device)\n\nself.assertEqual(len(results[\"scores\"]), 5)\nself.assertTrue(torch.allclose(results[\"scores\"], expected_scores, atol=1e-4))\n", "fix_pattern": "<condition>: The condition is not clear in the given context.\n<pattern>: No clear pattern can be identified in the given code.\n<code_one>: The code that was removed is the initialization of the expected_scores and expected_slice_boxes variables.\n<code_two>: The code that was added is the addition of the .to(torch_device) method to the tensors.\nFix_pattern: In this fix, the initialization of the expected_scores and expected_slice_boxes variables was removed and the .to(torch_device) method was added to these tensors to fix the API misuse."}
{"number": 298, "change": "class TensorflowBackendInferenceLearner(TensorflowBaseInferenceLearner):\nmetadata = LearnerMetadata.read(path)\nnetwork_parameters = ModelParams(**metadata.network_parameters)\ninput_tfms = metadata.input_tfms\n-        model = nebullvm.operations.inference_learners.utils.load_model(\n+        model = tf.keras.models.load_model(\npath / TENSORFLOW_BACKEND_FILENAMES[\"tf_model\"]\n)\ndevice = Device(metadata.device)\n", "fix_pattern": "Condition: There is a need to load a TensorFlow model using the correct API.\nPattern: The model is loaded using a function from a different API - nebullvm.operations.inference_learners.utils.load_model()\nCode one: model = nebullvm.operations.inference_learners.utils.load_model(\nCode two: model = tf.keras.models.load_model(\nFix pattern: In the condition of needing to load a TensorFlow model, if the incorrect nebullvm function is detected, then it is replaced with the correct tf.keras.models.load_model() function to fix the API misuse."}
{"number": 302, "change": "def rmsle(\n>>> x = torch.tensor([0., 1, 2, 3])\n>>> y = torch.tensor([0., 1, 2, 2])\n>>> rmsle(x, y)\n-        tensor(0.0207)\n+        tensor(0.1438)\n\n\"\"\"\n-    rmsle = mse(torch.log(pred + 1), torch.log(target + 1), reduction=reduction)\n+    rmsle = rmse(torch.log(pred + 1), torch.log(target + 1), reduction=reduction)\nreturn rmsle\n", "fix_pattern": "<condition>: No pre condition is needed.\n<pattern>: In the previous implementation, the `mse` function was used to calculate RMSLE.\n<code_one>: `mse(torch.log(pred + 1), torch.log(target + 1), reduction=reduction)`\n<code_two>: `rmse(torch.log(pred + 1), torch.log(target + 1), reduction=reduction)`\nFix_pattern: In the condition of no pre condition, if the usage of `mse` to calculate RMSLE is detected, then replace `mse` with `rmse` to fix the API misuse."}
{"number": 305, "change": "def ones_like(x, name=None):\n[ 1.,  1.,  1.]], dtype=float32)\n```\n\"\"\"\n-    return tf.ones_like(x, name=name)\n+    return tf.ones_like(x, dtype=dtype, name=name)\n\n\ndef random_uniform_variable(shape, low, high, dtype=None,\n", "fix_pattern": "Condition: The function `ones_like` is being used in the code.\nPattern: The `dtype` parameter is missing in the call to `tf.ones_like`.\nCode One: `return tf.ones_like(x, name=name)`\nCode Two: `return tf.ones_like(x, dtype=dtype, name=name)`\nFix Pattern: In the condition where the `ones_like` function is being used, if the `dtype` parameter is missing, then add `dtype=dtype` to the call to `tf.ones_like` to fix the API misuse."}
{"number": 306, "change": "class Ensemble(nn.ModuleList):\nreturn y, None  # inference, train output\n\n\n-def attempt_load(weights, map_location=None, inplace=True, fuse=True):\n+def attempt_load(weights, device=None, inplace=True, fuse=True):\nfrom models.yolo import Detect, Model\n\n# Loads an ensemble of models weights=[a,b,c] or a single model weights=[a] or weights=a\nmodel = Ensemble()\nfor w in weights if isinstance(weights, list) else [weights]:\n-        ckpt = torch.load(attempt_download(w), map_location=map_location)  # load\n-        ckpt = (ckpt.get('ema') or ckpt['model']).float()  # FP32 model\n+        ckpt = torch.load(attempt_download(w))\n+        ckpt = (ckpt.get('ema') or ckpt['model']).to(device).float()  # FP32 model\nmodel.append(ckpt.fuse().eval() if fuse else ckpt.eval())  # fused or un-fused model in eval mode\n\n# Compatibility updates\n", "fix_pattern": "<condition>: Checking whether `weights` is a list or a single value.\n<pattern>: Loading and processing the weights based on the type of `weights`.\n<code_one>: The removed code was loading the weights and converting the model to FP32.\n<code_two>: The added code is loading the weights and converting the model to FP32, while also considering the device.\nFix_pattern: In the condition of checking `weights`, if the pattern of loading and converting the model to FP32 is detected, then change the loading code to also consider the device."}
{"number": 307, "change": "from allennlp.common.params import Params\n\nclass TestStackedBidirectionalLstm(AllenNlpTestCase):\ndef test_stacked_bidirectional_lstm_completes_forward_pass(self):\n-        input_tensor = torch.autograd.Variable(torch.rand(4, 5, 3))\n+        input_tensor = torch.rand(4, 5, 3)\ninput_tensor[1, 4:, :] = 0.\ninput_tensor[2, 2:, :] = 0.\ninput_tensor[3, 1:, :] = 0.\n", "fix_pattern": "<condition>: No clear condition can be identified.\n<pattern>: N/A\n<code_one>: input_tensor = torch.autograd.Variable(torch.rand(4, 5, 3))\n<code_two>: input_tensor = torch.rand(4, 5, 3)\nFix_pattern: In this case, the fix involves removing the torch.autograd.Variable() function when initializing the input_tensor variable."}
{"number": 309, "change": "class TFCTRLMainLayer(tf.keras.layers.Layer):\ntoken_type_embeds = 0\nposition_ids = tf.reshape(position_ids, [-1, shape_list(position_ids)[-1]])\n\n-        inputs_embeds = self.w(input_ids)\n+        inputs_embeds = self.w(input_ids, mode='embedding')\n# x = embedded.unsqueeze(0) if len(input_ids.shape)<2 else embedded\nseq_len = input_shape[-1]\nmask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n", "fix_pattern": "Condition: The condition in this fix pattern is not clearly stated in the given context.\n\nPattern: The pattern in this fix is to change the function call from \"self.w(input_ids)\" to \"self.w(input_ids, mode='embedding')\".\n\nCode One: The code that was removed is \"inputs_embeds = self.w(input_ids)\".\n\nCode Two: The code that was added is \"inputs_embeds = self.w(input_ids, mode='embedding')\".\n\nFix Pattern: In the condition of <condition>, if <pattern> is detected, then change the function call from <code_one> to <code_two> to fix the API misuse."}
{"number": 310, "change": "def _preprocess_conv3d_input(x, data_format):\nA tensor.\n\"\"\"\n# tensorflow doesn't support float64 for conv layer before 1.8.0\n-    if (dtype(x) == 'float64'\n-            and StrictVersion(tf.__version__) < StrictVersion('1.8.0')):\n+    if (dtype(x) == 'float64' and\n+            StrictVersion(tf.__version__.split('-')[0]) < StrictVersion('1.8.0')):\nx = tf.cast(x, 'float32')\ntf_data_format = 'NDHWC'\nif data_format == 'channels_first':\n", "fix_pattern": "Condition: If the data format is 'channels_first'.\nPattern: Check if the data type of x is 'float64' and the TensorFlow version is less than '1.8.0'.\nCode One: Check if (dtype(x) == 'float64' and StrictVersion(tf.__version__) < StrictVersion('1.8.0')).\nCode Two: Check if (dtype(x) == 'float64' and StrictVersion(tf.__version__.split('-')[0]) < StrictVersion('1.8.0')).\nFix Pattern: In the condition of 'channels_first', if the data type of x is 'float64' and the TensorFlow version is less than '1.8.0', then change the code from \"if (dtype(x) == 'float64' and StrictVersion(tf.__version__) < StrictVersion('1.8.0'))\" to \"if (dtype(x) == 'float64' and StrictVersion(tf.__version__.split('-')[0]) < StrictVersion('1.8.0'))\" to fix the API misuse."}
{"number": 314, "change": "class _EagerVariableStore(tf.Module):\nlayer = create_layer_method()\nself._layers[name] = layer\nif isinstance(layer, base_layer.Layer):\n-        self._regularizers[name] = lambda: layer.losses\n+        self._regularizers[name] = lambda: tf.math.reduce_sum(layer.losses)\nreturn self._layers[name]\n\ndef add_regularizer(self, var, regularizer):\n", "fix_pattern": "<condition>: The condition is if `layer` is an instance of `base_layer.Layer`.\n<pattern>: The pattern detected is that the lambda function assigned to `self._regularizers[name]` is changed.\n<code_one>: The code removed is `self._regularizers[name] = lambda: layer.losses`.\n<code_two>: The code added is `self._regularizers[name] = lambda: tf.math.reduce_sum(layer.losses)`.\nFix_pattern: In the condition of `layer` being an instance of `base_layer.Layer`, if the lambda function for `self._regularizers[name]` is `lambda: layer.losses`, then change it to `lambda: tf.math.reduce_sum(layer.losses)` to fix the API misuse."}
{"number": 315, "change": "class TFWav2Vec2MainLayer(tf.keras.layers.Layer):\nif inputs[\"attention_mask\"] is not None:\n# compute real output lengths according to convolution formula\noutput_lengths = self._get_feat_extract_output_lengths(tf.reduce_sum(inputs[\"attention_mask\"], -1))\n-            attention_mask = tf.sequence_mask(output_lengths, dtype=hidden_states.dtype)\n+\n+            attention_mask = tf.sequence_mask(\n+                output_lengths, maxlen=shape_list(hidden_states)[1], dtype=hidden_states.dtype\n+            )\n\nhidden_states = self.feature_projection(hidden_states, training=inputs[\"training\"])\n", "fix_pattern": "<condition>: The condition is checking if the \"attention_mask\" input is not equal to None.\n<pattern>: The pattern that is detected is that the \"attention_mask\" is used to compute the output lengths.\n<code_one>: The code that is removed is \"attention_mask = tf.sequence_mask(output_lengths, dtype=hidden_states.dtype)\"\n<code_two>: The code that is added is \"attention_mask = tf.sequence_mask(output_lengths, maxlen=shape_list(hidden_states)[1], dtype=hidden_states.dtype)\"\nFix_pattern: In the condition of the \"attention_mask\" not being None, if the pattern of using \"attention_mask\" to compute output lengths is detected, then the code \"attention_mask = tf.sequence_mask(output_lengths, dtype=hidden_states.dtype)\" is replaced with \"attention_mask = tf.sequence_mask(output_lengths, maxlen=shape_list(hidden_states)[1], dtype=hidden_states.dtype)\" to fix the API misuse."}
{"number": 317, "change": "class DiagNormal(Distribution):\n# when the data is a ragged tensor. also useful for KL annealing. this entire logic\n# will likely be done in a better/cleaner way in the future\nif log_pdf_mask is not None:\n-            # TODO fix this to broadcasting as below, e.g. by instead:\n-            # log_pxs *= log_pdf_mask  # Then continue with broadcasting logic below.\n-            return torch.sum(log_pdf_mask * log_pxs, -1)\n+            log_pxs = log_pxs * log_pdf_mask\nbatch_log_pdf = torch.sum(log_pxs, -1)\nbatch_log_pdf_shape = x.size()[:-1] + (1,)\nreturn batch_log_pdf.contiguous().view(batch_log_pdf_shape)\n", "fix_pattern": "<condition>: The condition is that the variable \"log_pdf_mask\" is not None.\n<pattern>: The pattern is that the code was using broadcasting incorrectly.\n<code_one>: The code that was removed was \"log_pxs *= log_pdf_mask\".\n<code_two>: The code that was added is \"log_pxs = log_pxs * log_pdf_mask\".\nFix_pattern: In the condition of \"log_pdf_mask is not None\", if broadcasting misuse is detected, then change the code \"log_pxs *= log_pdf_mask\" to \"log_pxs = log_pxs * log_pdf_mask\" to fix the API misuse."}
{"number": 323, "change": "class BaseModel():\nsave_filename = '%s_net_%s.pth' % (which_epoch, name)\nsave_path = os.path.join(self.save_dir, save_filename)\nnet = getattr(self, 'net' + name)\n-                net.load_state_dict(torch.load(save_path))\n+                net.module.load_state_dict(torch.load(save_path))\n\n# print network information\ndef print_networks(self, verbose):\n", "fix_pattern": "Condition: The code is trying to load the state dictionary of a network.\n\nPattern: The code is using the \"load_state_dict\" function on the \"net\" variable.\n\nCode_one: \"net.load_state_dict(torch.load(save_path))\"\n\nCode_two: \"net.module.load_state_dict(torch.load(save_path))\"\n\nFix_pattern: In the condition of loading the state dictionary of a network, if the code is using the \"load_state_dict\" function on the \"net\" variable, then change \"net.load_state_dict(torch.load(save_path))\" to \"net.module.load_state_dict(torch.load(save_path))\" to fix the API misuse."}
{"number": 324, "change": "class SpeedsterRootOp(Operation):\n) -> List[BaseInferenceLearner]:\nif self.orig_latency_measure_op.get_result() is not None:\nmodel_outputs = self.orig_latency_measure_op.get_result()[0]\n-            if isinstance(model, Module):\n+            if isinstance(model, torch.nn.Module):\noptimization_op = self.torch_optimization_op\nelif isinstance(model, tf.Module) and model is not None:\noptimization_op = self.tensorflow_optimization_op\n", "fix_pattern": "<condition>: If the model is an instance of tf.Module and is not None. \n<pattern>: The code checks if the model is an instance of torch.nn.Module.\n<code_one>: if isinstance(model, Module):\n<code_two>: if isinstance(model, torch.nn.Module):\nFix_pattern: In the condition of checking the model instance, if the model is an instance of tf.Module, then change the code from 'if isinstance(model, Module):' to 'if isinstance(model, torch.nn.Module):' to fix the API misuse."}
{"number": 325, "change": "def run(\n):\n# PyTorch model\nim = torch.zeros((batch_size, 3, *imgsz))  # BCHW image\n-    model = attempt_load(weights, map_location=torch.device('cpu'), inplace=True, fuse=False)\n+    model = attempt_load(weights, device=torch.device('cpu'), inplace=True, fuse=False)\n_ = model(im)  # inference\nmodel.info()\n", "fix_pattern": "Condition: There is an API misuse in the code.\nPattern: The argument \"map_location\" was used in the \"attempt_load\" function call but it needed to be changed to \"device\".\nCode one: map_location=torch.device('cpu')\nCode two: device=torch.device('cpu')\nFix pattern: In the condition of API misuse, if the \"map_location\" argument is detected, then change \"map_location=torch.device('cpu')\" to \"device=torch.device('cpu')\" to fix the API misuse."}
{"number": 328, "change": "class UNet2DConditionModel(ModelMixin, ConfigMixin):\nif not torch.is_tensor(timesteps):\ntimesteps = torch.tensor([timesteps], dtype=torch.long, device=sample.device)\nelif torch.is_tensor(timesteps) and len(timesteps.shape) == 0:\n-            timesteps = timesteps[None].to(sample.device)\n+            timesteps = timesteps.to(dtype=torch.float32)\n+            timesteps = timesteps[None].to(device=sample.device)\n\n# broadcast to batch dimension in a way that's compatible with ONNX/Core ML\ntimesteps = timesteps.expand(sample.shape[0])\n", "fix_pattern": "<condition>: The condition is checking if the variable \"timesteps\" is not a Torch tensor or if it is a tensor of length 0.\n<pattern>: The pattern is detecting the code where \"timesteps\" is reassigned with broadcasting and converting it to the appropriate data type.\n<code_one>: The code that was removed is \"timesteps = timesteps[None].to(sample.device)\".\n<code_two>: The code that was added is \"timesteps = timesteps.to(dtype=torch.float32); timesteps = timesteps[None].to(device=sample.device)\".\nFix_pattern: In the condition of checking \"timesteps\", if the code that assigns \"timesteps\" with broadcasting is detected, then the code is removed and replaced with converting \"timesteps\" to the appropriate data type before assigning it with broadcasting to fix the API misuse."}
{"number": 335, "change": "class AdaptiveEmbedding(nn.Module):\n\ninp_i = inp_flat.index_select(0, indices_i) - l_idx\nemb_i = self.emb_layers[i](inp_i)\n-                emb_i = F.linear(emb_i, self.emb_projs[i])\n+                emb_i = nn.functional.linear(emb_i, self.emb_projs[i])\n\nemb_flat.index_copy_(0, indices_i, emb_i)\n", "fix_pattern": "<condition>: The condition is not clear in this context.\n<pattern>: The pattern is to change the function call from F.linear() to nn.functional.linear().\n<code_one>: The code that is removed is F.linear(emb_i, self.emb_projs[i]).\n<code_two>: The code that is added is nn.functional.linear(emb_i, self.emb_projs[i]).\nFix_pattern: In the condition of unclear condition, if the pattern of calling the function F.linear() is detected, then replace the code F.linear(emb_i, self.emb_projs[i]) with nn.functional.linear(emb_i, self.emb_projs[i]) to fix the API misuse."}
{"number": 341, "change": "for m in model_list:\ndata_root=os.environ.get('IMAGENET_DIR', './imagenet')\n)\n\n+    torch.cuda.empty_cache()\n+\n", "fix_pattern": "Condition: There is a loop that iterates over a model_list variable.\nPattern: There is a missing import statement for the os module.\nCode one: None\nCode two: Adding the import statement for the os module.\nFix pattern: In the condition of iterating over model_list, if there is a missing import for the os module, add the import statement to fix the API misuse."}
{"number": 348, "change": "def main():\nmodel = MMDataParallel(model, device_ids=[0])\noutputs = single_gpu_test(model, data_loader, args.show)\nelse:\n-        model = MMDistributedDataParallel(model.cuda())\n+        model = MMDistributedDataParallel(\n+            model.cuda(),\n+            device_ids=[torch.cuda.current_device()],\n+            broadcast_buffers=False)\noutputs = multi_gpu_test(model, data_loader, args.tmpdir,\nargs.gpu_collect)\n", "fix_pattern": "<condition>: When the code is not in the 'if' condition. \n<pattern>: A model is being wrapped with a different parallelization class. \n<code_one>: 'model = MMDistributedDataParallel(model.cuda())'\n<code_two>: 'model = MMDistributedDataParallel(model.cuda(), device_ids=[torch.cuda.current_device()], broadcast_buffers=False)'\nFix_pattern: In the condition of the 'else' statement, if the model is being wrapped with 'MMDistributedDataParallel', then replace it with 'MMDistributedDataParallel(model.cuda(), device_ids=[torch.cuda.current_device()], broadcast_buffers=False)' to fix the API misuse."}
{"number": 357, "change": "class Tester(unittest.TestCase):\n# generate input data\nbatch_size = 1\ncenter = torch.zeros(batch_size, 2)\n-        angle = torch.ones(batch_size, 1)\n-        scale = torch.ones(batch_size, 1)\n+        angle = torch.ones(batch_size)\n+        scale = torch.ones(batch_size)\n\ncenter = utils.tensor_to_gradcheck_var(center)  # to var\nangle = utils.tensor_to_gradcheck_var(angle)  # to var\n", "fix_pattern": "<condition>: No clear condition is needed.\n<pattern>: In this case, the pattern is to remove the unnecessary second dimension from tensors by changing <code_one> to <code_two>.\n<code_one>: angle = torch.ones(batch_size, 1)\n<code_two>: angle = torch.ones(batch_size)\nFix_pattern: In the condition where unnecessary second dimension is detected, remove the second dimension from tensors by changing <code_one> to <code_two> to fix the API misuse."}
{"number": 360, "change": "class Trainer:\nreturn type(data)(self._prepare_input(v) for v in data)\nelif isinstance(data, torch.Tensor):\nkwargs = {\"device\": self.args.device}\n-            if self.deepspeed and data.dtype != torch.int64:\n-                # NLP models inputs are int64 and those get adjusted to the right dtype of the\n+            if self.deepspeed and (torch.is_floating_point(data) or torch.is_complex(data)):\n+                # NLP models inputs are int/uint and those get adjusted to the right dtype of the\n# embedding. Other models such as wav2vec2's inputs are already float and thus\n# may need special handling to match the dtypes of the model\nkwargs.update({\"dtype\": self.args.hf_deepspeed_config.dtype()})\n", "fix_pattern": "<condition>: If self.deepspeed is true and the dtype of the data is not torch.int64.\n<pattern>: If the data is a floating-point or complex number.\n<code_one>: if isinstance(data, torch.Tensor) and self.deepspeed and data.dtype != torch.int64:\n<code_two>: if self.deepspeed and (torch.is_floating_point(data) or torch.is_complex(data)):\nFix_pattern: In the condition of self.deepspeed and data.dtype not being torch.int64, the code that checks for non-floating-point or non-complex number data types is removed and replaced with code that checks for floating-point or complex number data types to fix the API misuse."}
{"number": 362, "change": "class DeformableDetrImageProcessor(BaseImageProcessor):\nimg_w = torch.Tensor([i[1] for i in target_sizes])\nelse:\nimg_h, img_w = target_sizes.unbind(1)\n-        scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1)\n+        scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1).to(boxes.device)\nboxes = boxes * scale_fct[:, None, :]\n\nresults = []\n", "fix_pattern": "<condition>: The condition is when the variable \"scale_fct\" is being used in the code.\n<pattern>: The pattern that is detected is that \"scale_fct\" needs to be moved to the same device as the variable \"boxes\".\n<code_one>: The code that needs to be removed is \"scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1)\".\n<code_two>: The code that needs to be added is \"scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1).to(boxes.device)\".\nFix_pattern: In the condition of using \"scale_fct\" in the code, if the pattern of not having it on the same device as \"boxes\" is detected, then the code \"scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1)\" needs to be changed to \"scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1).to(boxes.device)\" to fix the API misuse."}
{"number": 363, "change": "class SpanBasedF1Test(AllenNlpTestCase):\ngold_indices = [self.vocab.get_token_index(x, \"tags\") for x in bio_tags]\ngold_tensor = torch.tensor([gold_indices], device=device)\nprediction_tensor = torch.rand([1, 6, self.vocab.get_vocab_size(\"tags\")], device=device)\n-        mask = torch.tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]], device=device)\n+        mask = torch.BoolTensor(\n+            [[True, True, True, True, True, True, True, True, True]], device=device\n+        )\n\n# Make prediction so that it is exactly correct.\nfor i, tag_index in enumerate(gold_indices):\n", "fix_pattern": "Condition: The code is trying to make a prediction using a tensor.\nPattern: The code is using a torch.tensor() function to create a tensor.\nCode one: The code is creating a tensor using torch.tensor().\nCode two: The code is creating a tensor using torch.BoolTensor().\nFix pattern: In the condition of making a prediction using a tensor, if the torch.tensor() function is used, then change it to torch.BoolTensor() to fix the API misuse."}
{"number": 366, "change": "class DistributedFusedLAMB(torch.optim.Optimizer):\nl2_norm = torch.zeros(size=[self._model_params_num], dtype=torch.float32, device='cuda')\nlocal_contrib_l2_norm = multi_tensor_applier(self.multi_tensor_l2norm, self._overflow_buf, [self._contrib_update_frag_for_norm], True)[1] ** 2\nl2_norm.masked_scatter_(self._model_param_is_contrib, local_contrib_l2_norm)\n-        torch.distributed.allreduce(l2_norm, group=self._ag_pg[0])\n+        torch.distributed.all_reduce(l2_norm, group=self._ag_pg[0])\nreturn l2_norm.masked_select(self._model_param_is_contrib)\n\ndef _pipeline_step(self):\n", "fix_pattern": "<condition>: There is a condition where the `l2_norm` variable is being used.\n<pattern>: The pattern being detected is the usage of the `torch.distributed.allreduce` function on the `l2_norm` variable.\n<code_one>: The code being removed is `torch.distributed.allreduce(l2_norm, group=self._ag_pg[0])`.\n<code_two>: The code being added is `torch.distributed.all_reduce(l2_norm, group=self._ag_pg[0])`.\nFix_pattern: In the condition of using `l2_norm`, if `torch.distributed.allreduce` is detected, then change the code `torch.distributed.allreduce` to `torch.distributed.all_reduce` to fix the API misuse."}
{"number": 367, "change": "class _BinaryPostprocessing(torch.nn.Module):\npredictions = [self.bool2str.get(pred, self.bool2str[0]) for pred in predictions]\n\nprobs = preds[self.probabilities_key]\n-        probs = torch.dstack(1 - probs, probs)\n+        probs = torch.stack([1 - probs, probs], dim=-1)\n\nreturn {\nself.predictions_key: predictions,\n", "fix_pattern": "Condition: The condition is not clearly stated in the given context.\n\nPattern: The pattern detected in the code change is a modification of the code for stacking tensors.\n\nCode One: The original code was `probs = torch.dstack(1 - probs, probs)`, which was removed.\n\nCode Two: The new code added is `probs = torch.stack([1 - probs, probs], dim=-1)`.\n\nFix Pattern: In the condition of unknown, if the pattern of modifying the code for stacking tensors is detected, then the code `probs = torch.dstack(1 - probs, probs)` should be changed to `probs = torch.stack([1 - probs, probs], dim=-1)` to fix the API misuse."}
{"number": 368, "change": "if __name__ == \"__main__\":\nexp = get_exp(args.exp_file, args.name)\nexp.merge(args.opts)\n\n-    num_gpu = get_num_devices() if args.devices is None else args.devices\n-    assert num_gpu <= get_num_devices()\n+    num_gpu = torch.cuda.device_count() if args.devices is None else args.devices\n+    assert num_gpu <= torch.cuda.device_count()\n\ndist_url = \"auto\" if args.dist_url is None else args.dist_url\nlaunch(\n", "fix_pattern": "<condition>: The variable args.devices is None.\n<pattern>: A function get_num_devices() is called to determine the number of devices.\n<code_one>: num_gpu = get_num_devices()\n<code_two>: num_gpu = torch.cuda.device_count()\nFix_pattern: In the condition of args.devices being None, replace the call to get_num_devices() with torch.cuda.device_count() to fix the API misuse."}
{"number": 372, "change": "class Critic(object):\nself.train_op = tf.train.AdamOptimizer(self.lr).minimize(self.loss)\n\nwith tf.variable_scope('a_grad'):\n-            self.a_grads = tf.gradients(self.q, a)[0]   # tensor of gradients of each sample (None, a_dim)\n+            self.a_grads = tf.gradients(self.q, self.a)[0]   # tensor of gradients of each sample (None, a_dim)\n\nif self.replacement['name'] == 'hard':\nself.t_replace_counter = 0\n", "fix_pattern": "Condition: The condition states that when the replacement name is 'hard', a fix needs to be applied.\nPattern: The pattern is the incorrect use of the variable 'a' instead of 'self.a' in the tf.gradients() function.\nCode_one: The code that needs to be removed is \"self.a_grads = tf.gradients(self.q, a)[0]\".\nCode_two: The code that needs to be added is \"self.a_grads = tf.gradients(self.q, self.a)[0]\".\nFix pattern: In the condition where the replacement name is 'hard', the fix involves replacing the variable 'a' with 'self.a' in the tf.gradients() function."}
{"number": 375, "change": "def corr2d(X, K):  #@save\n\n# Defined in file: ./chapter_convolutional-neural-networks/lenet.md\ndef evaluate_accuracy_gpu(net, data_iter, device=None): #@save\n+    net.eval()  # Set the model to evaluation mode\nif not device:\ndevice = next(iter(net.parameters())).device\nmetric = d2l.Accumulator(2)  # num_corrected_examples, num_examples\n", "fix_pattern": "<condition>: The condition is that the 'device' parameter is not set.\n<pattern>: The pattern is to add the line 'net.eval()' to set the model to evaluation mode.\n<code_one>: There is no code removed in this case.\n<code_two>: The code added is 'net.eval()'.\nFix_pattern: In the condition of 'device' not set, add the line 'net.eval()' to fix the API misuse."}
{"number": 376, "change": "class VideoSequential(ImageSequential):\n# Size of T\nframe_num = input.size(self._temporal_channel)\n# Got param generation shape to (B, C, H, W). Ignoring T.\n-        batch_shape = self.__infer_channel_exclusive_batch_shape__(input)\n+        batch_shape = self.__infer_channel_exclusive_batch_shape__(input, self._temporal_channel)\ninput = self._input_shape_convert_in(input)\ninput = input.reshape(-1, *batch_shape[1:])\nif not self.same_on_frame:\n", "fix_pattern": "<condition>: The condition is \"if not self.same_on_frame\".\n<pattern>: The pattern is to add an argument \"self._temporal_channel\" to the method \"__infer_channel_exclusive_batch_shape__\".\n<code_one>: The code that was removed is \"batch_shape = self.__infer_channel_exclusive_batch_shape__(input)\".\n<code_two>: The code that was added is \"batch_shape = self.__infer_channel_exclusive_batch_shape__(input, self._temporal_channel)\".\nFix_pattern: In the condition of \"if not self.same_on_frame\", if the pattern of not passing the argument \"self._temporal_channel\" in the method \"__infer_channel_exclusive_batch_shape__\" is detected, then add the argument \"self._temporal_channel\" to fix the API misuse."}
{"number": 377, "change": "def create_dataloader(path, imgsz, batch_size, stride, single_cls=False, hyp=Non\nprefix=prefix)\n\nbatch_size = min(batch_size, len(dataset))\n-    nw = min([os.cpu_count() // DEVICE_COUNT, batch_size if batch_size > 1 else 0, workers])  # number of workers\n+    nd = torch.cuda.device_count()  # number of CUDA devices\n+    nw = min([os.cpu_count() // max(nd, 1), batch_size if batch_size > 1 else 0, workers])  # number of workers\nsampler = None if rank == -1 else distributed.DistributedSampler(dataset, shuffle=shuffle)\nloader = DataLoader if image_weights else InfiniteDataLoader  # only DataLoader allows for attribute updates\nreturn loader(dataset,\n", "fix_pattern": "<condition>: When creating a data loader for a dataset.\n<pattern>: A calculation of the number of workers based on CPU count and batch size.\n<code_one>: nw = min([os.cpu_count() // DEVICE_COUNT, batch_size if batch_size > 1 else 0, workers])\n<code_two>: nw = min([os.cpu_count() // max(nd, 1), batch_size if batch_size > 1 else 0, workers])\nFix_pattern: In the condition of creating a data loader for a dataset, if the calculation of the number of workers (nw) is detected, then change the code of the calculation from <code_one> to <code_two> to fix the API misuse."}
{"number": 381, "change": "if dependency_check.crypten_available:\n\nframework_packages[\"crypten\"] = crypten\nframework_tensors.append(crypten.mpc.MPCTensor)\n+    framework_tensors.append(crypten.nn.Module)\n+\n\nframework_tensors = tuple(framework_tensors)\nFrameworkTensorType = Union[framework_tensors]\n", "fix_pattern": "<condition>: The condition is \"dependency_check.crypten_available\" which checks if the Crypten package is available.\n<pattern>: The pattern is the addition of \"crypten.mpc.MPCTensor\" to the \"framework_tensors\" list.\n<code_one>: No code was removed.\n<code_two>: The added code is \"framework_tensors.append(crypten.nn.Module)\".\nFix_pattern: In the condition of \"dependency_check.crypten_available\", if the pattern of adding \"crypten.mpc.MPCTensor\" is detected, then add \"crypten.nn.Module\" to the \"framework_tensors\" list to fix the API misuse."}
{"number": 384, "change": "def array(object_in, dtype: Optional[str] = None, dev: Optional[str] = None):\ndev = default_device(dev)\ndtype = dtype_from_str(default_dtype(dtype, object_in))\nif isinstance(object_in, np.ndarray):\n-        return _torch.Tensor(object_in).to(dev_from_str(dev))\n+        return torch.Tensor(object_in).to(dev_from_str(dev))\nif dtype is not None:\n-        return _torch.tensor(object_in, dtype=dtype, device=dev_from_str(dev))\n-    elif isinstance(object_in, _torch.Tensor):\n+        return torch.tensor(object_in, dtype=dtype, device=dev_from_str(dev))\n+    elif isinstance(object_in, torch.Tensor):\nreturn object_in.to(dev_from_str(dev))\nelse:\n-        return _torch.tensor(object_in, device=dev_from_str(dev))\n+        return torch.tensor(object_in, device=dev_from_str(dev))\n\nasarray = array\n", "fix_pattern": "<condition>: The condition is checking if the input object is an instance of numpy ndarray.\n<pattern>: The pattern is that the code is using the \"_torch\" module instead of the \"torch\" module for Tensor operations.\n<code_one>: The code that is removed is \"_torch.Tensor(object_in).to(dev_from_str(dev))\" and \"_torch.tensor(object_in, dtype=dtype, device=dev_from_str(dev))\" and \"_torch.tensor(object_in, device=dev_from_str(dev))\".\n<code_two>: The code that is added is \"torch.Tensor(object_in).to(dev_from_str(dev))\" and \"torch.tensor(object_in, dtype=dtype, device=dev_from_str(dev))\" and \"torch.tensor(object_in, device=dev_from_str(dev))\".\nFix_pattern: In the condition of checking if the input object is an instance of numpy ndarray, if the code uses \"_torch\" module for Tensor operations, then it should be changed to \"torch\" module to fix the API misuse."}
{"number": 390, "change": "def _calculate_expected_result(\naggregation_op_only_probs = gumbel_dist.sample()\nelse:\n# <float32>[batch_size, num_aggregation_labels - 1]\n-        aggregation_op_only_probs = torch.nn.functional.softmax(\n+        aggregation_op_only_probs = nn.functional.softmax(\nlogits_aggregation[:, 1:] / config.aggregation_temperature, dim=-1\n)\n", "fix_pattern": "Condition: The condition for the fix is not clear from the provided context.\nPattern: The pattern is the replacement of `torch.nn.functional.softmax` with `nn.functional.softmax`.\nCode One: `aggregation_op_only_probs = torch.nn.functional.softmax(`\nCode Two: `aggregation_op_only_probs = nn.functional.softmax(`\nFix Pattern: In the condition of <condition>, if <pattern> is detected, then remove the code_one and add code_two to fix the API misuse."}
{"number": 392, "change": "class TensorforceModel(Model):\ndiscounts = tf.math.pow(x=discount, y=exponent)\nif not self.predict_terminal_values:\ndiscounts = tf.where(\n-                    condition=tf.math.greater(x=_terminal, y=one),\n-                    x=discounts, y=tf.zeros_like(input=discounts)\n+                    condition=tf.math.equal(x=_terminal, y=one),\n+                    x=tf.zeros_like(input=discounts), y=discounts\n)\n\n-            reward += discounts * horizon_values\n+            reward = reward + discounts * horizon_values\n\ndependencies = [reward]\nif self.summaries == 'all' or 'reward' in self.summaries:\n", "fix_pattern": "Condition: The condition is checking if the variable \"_terminal\" is greater than \"one\". \nPattern: The pattern that is detected is the misuse of API, where the \"tf.where()\" function is used incorrectly.\nCode_one: The code that is removed is \"condition=tf.math.greater(x=_terminal, y=one), x=discounts, y=tf.zeros_like(input=discounts)\".\nCode_two: The code that is added is \"condition=tf.math.equal(x=_terminal, y=one), x=tf.zeros_like(input=discounts), y=discounts\".\nFix_pattern: In the condition of \"if _terminal is greater than one\", then change the \"tf.where()\" block to \"condition=tf.math.equal(x=_terminal, y=one), x=tf.zeros_like(input=discounts), y=discounts\" to fix the API misuse."}
{"number": 395, "change": "def test_cgcnn_conv():\nedge_index = torch.tensor([[0, 0, 0, 1, 2, 3], [1, 2, 3, 0, 0, 0]])\nnum_nodes = edge_index.max().item() + 1\nx = torch.randn((num_nodes, node_dim))\n-    pseudo = torch.rand((edge_index.size(1), 3))\n+    pseudo = torch.rand((edge_index.size(1), edge_dim))\n\nconv = CGCNNConv(node_dim, edge_dim)\nassert conv.__repr__() == 'CGCNNConv(16, 16)'\n", "fix_pattern": "<condition>: No clear condition is needed.\n<pattern>: The code originally generated a random tensor of size (edge_index.size(1), 3).\n<code_one>: The code `pseudo = torch.rand((edge_index.size(1), 3))` was removed.\n<code_two>: The code `pseudo = torch.rand((edge_index.size(1), edge_dim))` was added.\nFix_pattern: In the condition of no clear condition, if the code `pseudo = torch.rand((edge_index.size(1), 3))` is detected, then remove the code `pseudo = torch.rand((edge_index.size(1), 3))` and add the code `pseudo = torch.rand((edge_index.size(1), edge_dim))` to fix the API misuse."}
{"number": 408, "change": "\"        # compute the gating function and one minus the gating function\\n\",\n\"        gate_intermediate = self.relu(self.lin_gate_z_to_hidden(z_t_1))\\n\",\n\"        gate = self.sigmoid(self.lin_gate_hidden_to_z(gate_intermediate))\\n\",\n-    \"        one_minus_gate = ng_ones(gate.size()).type_as(gate) - gate\\n\",\n+    \"        one_minus_gate = torch.ones(gate.size()).type_as(gate) - gate\\n\",\n\"        # compute the 'proposed mean'\\n\",\n\"        proposed_mean_intermediate = self.relu(self.lin_proposed_mean_z_to_hidden(z_t_1))\\n\",\n\"        proposed_mean = self.lin_proposed_mean_hidden_to_z(proposed_mean_intermediate)\\n\",\n", "fix_pattern": "Condition: In the code snippet, there is a computation of the gating function and one minus the gating function.\nPattern: The code is using the function ng_ones(), which is not recognized by the current API.\nCode one: The line \"one_minus_gate = ng_ones(gate.size()).type_as(gate) - gate\" is removed.\nCode two: The line \"one_minus_gate = torch.ones(gate.size()).type_as(gate) - gate\" is added.\nFix pattern: In the condition of computing the gating function, if the use of ng_ones() is detected, then remove the line using ng_ones() and add a line using torch.ones() to fix the API misuse."}
{"number": 412, "change": "def crop_by_boxes(tensor, src_box, dst_box,\ndst_trans_src = dst_trans_src.expand(tensor.shape[0], -1, -1)\n\nbbox = _infer_bounding_box(dst_box)\n-    patches: torch.Tensor = warp_perspective(\n-        tensor, dst_trans_src, (int(bbox[0].int().data.item()), int(bbox[1].int().data.item())))\n+    patches: torch.Tensor = warp_affine(\n+        tensor, dst_trans_src[:, :2, :], (int(bbox[0].int().data.item()), int(bbox[1].int().data.item())))\n\n# return in the original shape\nif is_unbatched:\n", "fix_pattern": "Condition: The condition is not clearly identified in the given context.\nPattern: The pattern is not clearly identified in the given code removed section.\nCode One: The code removed is \"patches: torch.Tensor = warp_perspective(tensor, dst_trans_src, (int(bbox[0].int().data.item()), int(bbox[1].int().data.item())))\".\nCode Two: The code added is \"patches: torch.Tensor = warp_affine(tensor, dst_trans_src[:, :2, :], (int(bbox[0].int().data.item()), int(bbox[1].int().data.item())))\".\nFix Pattern: In the condition of unknown, if unknown pattern is detected, then change the code_one to code_two to fix the API misuse."}
{"number": 417, "change": "class Model(ModelDesc):\nif get_current_tower_context().is_training:\nwd_w = tf.train.exponential_decay(2e-4, get_global_step_var(),\n80000, 0.7, True)\n-            wd_cost = tf.mul(wd_w, regularize_cost('.*/W', tf.nn.l2_loss), name='wd_cost')\n+            wd_cost = tf.multiply(wd_w, regularize_cost('.*/W', tf.nn.l2_loss), name='wd_cost')\ncosts.append(wd_cost)\n\nadd_param_summary(('.*/W', ['histogram']))   # monitor W\n", "fix_pattern": "Condition: The condition is that the current tower context is for training.\nPattern: The pattern is using the tf.mul() function to multiply wd_w and regularize_cost() to calculate wd_cost. \nCode one: The code being removed is \"wd_cost = tf.mul(wd_w, regularize_cost('.*/W', tf.nn.l2_loss), name='wd_cost')\".\nCode two: The code being added is \"wd_cost = tf.multiply(wd_w, regularize_cost('.*/W', tf.nn.l2_loss), name='wd_cost')\".\nFix pattern: In the condition of the current tower context being for training, if the pattern of multiplying wd_w and regularize_cost() is detected, then change the code to use tf.multiply() instead of tf.mul() to fix the API misuse."}
{"number": 418, "change": "if __name__ == '__main__':\nloss_values.clear()\naccuracies.clear()\nif step % 100 == 0:\n-            vis.draw_projections(embeds.detach().cpu().numpy(), utterances_per_speaker, step)\n+            vis.draw_projections(embeds.detach().numpy(), utterances_per_speaker, step)\n", "fix_pattern": "Condition: The code is running when the current file is the main module.\nPattern: A visualization function is being called.\nCode one: vis.draw_projections(embeds.detach().cpu().numpy(), utterances_per_speaker, step)\nCode two: vis.draw_projections(embeds.detach().numpy(), utterances_per_speaker, step)\nFix Pattern: In the condition of running as the main module, if a visualization function is called, the API misuse is fixed by changing code_one to code_two."}
{"number": 419, "change": "class ScoreSdeVpScheduler(SchedulerMixin, ConfigMixin):\nx_mean = x + drift * dt\n\n# add noise\n-        noise = torch.randn(x.shape, layout=x.layout, generator=generator).to(x.device)\n+        noise = randn_tensor(x.shape, layout=x.layout, generator=generator, device=x.device, dtype=x.dtype)\nx = x_mean + diffusion * math.sqrt(-dt) * noise\n\nreturn x, x_mean\n", "fix_pattern": "Condition: The code is attempting to add noise to the variable \"x\" in the ScoreSdeVpScheduler class.\nPattern: The code is using the torch.randn() function to generate the noise.\nCode_one: \"noise = torch.randn(x.shape, layout=x.layout, generator=generator).to(x.device)\"\nCode_two: \"noise = randn_tensor(x.shape, layout=x.layout, generator=generator, device=x.device, dtype=x.dtype)\"\nFix_pattern: In the condition of attempting to add noise to \"x\", if the pattern of using torch.randn() is detected, then remove the code_one and add code_two to fix the API misuse."}
{"number": 422, "change": "def get_keras_model():\nM.add(KL.Conv2D(32, 3, padding='same', activation='relu'))\nM.add(KL.Flatten())\nM.add(KL.Dense(512, activation='relu', kernel_regularizer=keras.regularizers.l2(1e-5)))\n-        M.add(KL.Dropout(0.5))\n+        M.add(KL.Dropout(rate=0.5))\nM.add(KL.Dense(10, activation=None, kernel_regularizer=keras.regularizers.l2(1e-5)))\nreturn M\n", "fix_pattern": "Condition: The condition is not clearly specified in the given context.\n\nPattern: The pattern detected is the incorrect usage of the `Dropout` function without explicitly specifying the parameter name.\n\nCode one: The code that is removed is `M.add(KL.Dropout(0.5))`.\n\nCode two: The code that is added is `M.add(KL.Dropout(rate=0.5))`.\n\nFix_pattern: In the condition where the incorrect usage of `Dropout` function is detected, the code `M.add(KL.Dropout(0.5))` is changed to `M.add(KL.Dropout(rate=0.5))` to fix the API misuse."}
{"number": 424, "change": "def main(parsed_args):\n\ndef cli_main():\nparser = options.get_eval_lm_parser()\n+    add_distributed_training_args(parser)\nargs = options.parse_args_and_arch(parser)\n-    main(args)\n+    distributed_utils.call_main(args, main)\n\n\nif __name__ == '__main__':\n", "fix_pattern": "<condition>: The condition is when the script is running as the main script.\n<pattern>: The pattern is calling the main function with the parsed arguments.\n<code_one>: The code that has been removed is \"main(args)\".\n<code_two>: The code that has been added is \"add_distributed_training_args(parser)\\ndistributed_utils.call_main(args, main)\".\nFix_pattern: In the condition of the script running as the main script, if the main function is called with the parsed arguments, then remove the \"main(args)\" code and add \"add_distributed_training_args(parser)\\ndistributed_utils.call_main(args, main)\" code to fix the API misuse."}
{"number": 425, "change": "def degree(index, num_nodes=None, dtype=None, device=None):\ntensor([3., 1., 1.])\n\"\"\"\nnum_nodes = maybe_num_nodes(index, num_nodes)\n-    out = torch.zeros((num_nodes), dtype=dtype, device=device)\n+    out = torch.zeros((num_nodes), dtype=dtype, device=index.device)\nreturn out.scatter_add_(0, index, out.new_ones((index.size(0))))\n", "fix_pattern": "Condition: The condition is when the variable \"num_nodes\" is being used in the code.\n\nPattern: The pattern being detected is the incorrect assignment of the \"out\" variable.\n\nCode_one: The code being removed is the line that initializes the \"out\" variable with zeros.\n\nCode_two: The code being added is the line that initializes the \"out\" variable with zeros but also specifies the device based on the \"index\" variable.\n\nFix pattern: In the condition of using the \"num_nodes\" variable, if the pattern of wrongly initializing the \"out\" variable with zeros is detected, then the code should be changed to properly initialize the \"out\" variable with zeros and also specify the device based on the \"index\" variable to fix the API misuse."}
{"number": 429, "change": "class RandomThinPlateSpline(AugmentationBase2D):\n\ndef generate_parameters(self, shape: torch.Size) -> Dict[str, Tensor]:\nB, _, _, _ = shape\n-        src = torch.tensor([[[-1.0, -1.0], [-1.0, 1.0], [1.0, -1.0], [1.0, 1.0], [0.0, 0.0]]]).expand(B, 5, 2)  # Bx5x2\n+        src = tensor([[[-1.0, -1.0], [-1.0, 1.0], [1.0, -1.0], [1.0, 1.0], [0.0, 0.0]]]).expand(B, 5, 2)  # Bx5x2\ndst = src + self.dist.rsample(src.shape)\nreturn dict(src=src, dst=dst)\n", "fix_pattern": "<condition>: There is no clear condition in the context for the fix pattern.\n<pattern>: The pattern is removing the torch.tensor() function and replacing it with tensor(), as the torch.tensor() function is not necessary.\n<code_one>: The code that is removed is 'src = torch.tensor([[[-1.0, -1.0], [-1.0, 1.0], [1.0, -1.0], [1.0, 1.0], [0.0, 0.0]]]).expand(B, 5, 2)  # Bx5x2'\n<code_two>: The code that is added is 'src = tensor([[[-1.0, -1.0], [-1.0, 1.0], [1.0, -1.0], [1.0, 1.0], [0.0, 0.0]]]).expand(B, 5, 2)  # Bx5x2'\nFix_pattern: In the condition of no pre condition, if the pattern of removing torch.tensor() is detected, then change the code 'torch.tensor()' to 'tensor()' to fix the API misuse."}
{"number": 432, "change": "class PNDMScheduler(SchedulerMixin, ConfigMixin):\n::-1\n].copy()  # we copy to avoid having negative strides which are not supported by torch.from_numpy\n\n-        self.timesteps = np.concatenate([self.prk_timesteps, self.plms_timesteps]).astype(np.int64)\n+        timesteps = np.concatenate([self.prk_timesteps, self.plms_timesteps]).astype(np.int64)\n+        self.timesteps = torch.from_numpy(timesteps).to(device)\n\nself.ets = []\nself.counter = 0\n", "fix_pattern": "Condition: There is a need to convert a numpy array to a Torch tensor.\nPattern: Concatenate two numpy arrays and convert them to a Torch tensor.\nCode one: self.timesteps = np.concatenate([self.prk_timesteps, self.plms_timesteps]).astype(np.int64)\nCode two: timesteps = np.concatenate([self.prk_timesteps, self.plms_timesteps]).astype(np.int64)\nFix_pattern: In the condition of needing to convert a numpy array to a Torch tensor, if concatenation and type conversion are required, then change the code to concatenate the numpy arrays and convert them to a Torch tensor."}
{"number": 433, "change": "def HomographyRegressionApp():\n[-1, 1],  # top-right\n]]).to(dst_homo_src.device)\n# transform points\n-            pts_dst = dgm.transform_points(dgm.inverse(dst_homo_src), pts_src)\n+            pts_dst = dgm.transform_points(torch.inverse(dst_homo_src), pts_src)\n\ndef compute_factor(size):\nreturn 1.0 * size / 2\n", "fix_pattern": "<condition>: No pre condition is needed.\n<pattern>: The code \"dgm.transform_points(dgm.inverse(dst_homo_src), pts_src)\" \n<code_one>: was removed. \n<code_two>: It was replaced with \"torch.inverse(dst_homo_src)\".\nFix_pattern: In the code, if the \"dgm.transform_points(dgm.inverse(dst_homo_src), pts_src)\" pattern is detected, then remove it and replace it with \"torch.inverse(dst_homo_src)\" to fix the API misuse."}
{"number": 436, "change": "class Highway(torch.nn.Module):\n# above, too.\nnonlinear_part, gate = projected_input.chunk(2, dim=-1)\nnonlinear_part = self._activation(nonlinear_part)\n-            gate = torch.nn.functional.sigmoid(gate)\n+            gate = torch.sigmoid(gate)\ncurrent_input = gate * linear_part + (1 - gate) * nonlinear_part\nreturn current_input\n", "fix_pattern": "Condition: The code is using the torch.nn.functional.sigmoid function.\nPattern: The code is using the deprecated torch.nn.functional.sigmoid function.\nCode one: gate = torch.nn.functional.sigmoid(gate)\nCode two: gate = torch.sigmoid(gate)\nFix pattern: In the condition of using the deprecated torch.nn.functional.sigmoid function, replace it with torch.sigmoid to fix the API misuse."}
{"number": 437, "change": "class Model(object):\n\"It should be either Tensor or a list of Tensor.\"\n)\nfor idx in range(len(check_argu)):\n-                        if not isinstance(check_argu[idx], tf_ops._TensorLike) or not tf_ops.is_dense_tensor_like(\n+                        if not isinstance(check_argu[idx], [tf.Tensor, tf.SparseTensor, tf.Variable]) or not tf_ops.is_dense_tensor_like(\ncheck_argu[idx]):\nraise TypeError(\n\"The argument `%s` should be either Tensor or a list of Tensor \" % (check_order[co]) +\n", "fix_pattern": "<condition>: The condition is that the argument should be either a Tensor or a list of Tensor.\n<pattern>: The pattern is checking if the argument is not an instance of tf_ops._TensorLike or if it is not a dense tensor like object using tf_ops.is_dense_tensor_like.\n<code_one>: The code being removed is the check for tf_ops._TensorLike and tf_ops.is_dense_tensor_like.\n<code_two>: The code being added is the check for [tf.Tensor, tf.SparseTensor, tf.Variable] and tf_ops.is_dense_tensor_like.\nFix_pattern: In the condition of checking if the argument is either a Tensor or a list of Tensor, the fix pattern is to remove the check for tf_ops._TensorLike and tf_ops.is_dense_tensor_like, and instead add a check for [tf.Tensor, tf.SparseTensor, tf.Variable] and tf_ops.is_dense_tensor_like."}
{"number": 439, "change": "class TestBasicTextFieldEmbedder(AllenNlpTestCase):\n})\ntoken_embedder = BasicTextFieldEmbedder.from_params(self.vocab, params)\ninputs = {\n-                'words': Variable(torch.rand(3, 4, 5, 6) * 20).long(),\n-                'characters': Variable(torch.rand(3, 4, 5, 6, 7) * 15).long(),\n+                'words': (torch.rand(3, 4, 5, 6) * 20).long(),\n+                'characters': (torch.rand(3, 4, 5, 6, 7) * 15).long(),\n}\nassert token_embedder(inputs, num_wrapping_dims=2).size() == (3, 4, 5, 6, 12)\n", "fix_pattern": "<condition>: The condition is not specified in the context.\n<pattern>: No clear pattern is identified.\n<code_one>: The code that is removed is `'words': Variable(torch.rand(3, 4, 5, 6) * 20).long(),'characters': Variable(torch.rand(3, 4, 5, 6, 7) * 15).long(),`\n<code_two>: The code that is added is `'words': (torch.rand(3, 4, 5, 6) * 20).long(),'characters': (torch.rand(3, 4, 5, 6, 7) * 15).long(),`\nFix_pattern: In this fix, the code that instantiates the 'words' and 'characters' variables is changed from using `Variable` to not using `Variable`."}
{"number": 446, "change": "class Csv(datasets.ArrowBasedBuilder):\nif schema is not None\nelse None\n)\n-        for file_idx, file in enumerate(files):\n+        for file_idx, file in enumerate(itertools.chain.from_iterable(files)):\ncsv_file_reader = pd.read_csv(file, iterator=True, dtype=dtype, **self.config.read_csv_kwargs)\ntry:\nfor batch_idx, df in enumerate(csv_file_reader):\n", "fix_pattern": "<condition>: The condition is that the variable \"schema\" should not be None.\n<pattern>: The pattern that is detected is a for loop iterating over a list of files.\n<code_one>: The code \"for file_idx, file in enumerate(files):\" is removed.\n<code_two>: The code \"for file_idx, file in enumerate(itertools.chain.from_iterable(files)):\" is added.\nFix_pattern: In the condition of \"schema is not None\", if the pattern of iterating over a list of files is detected, then the code \"for file_idx, file in enumerate(files):\" is removed and replaced with \"for file_idx, file in enumerate(itertools.chain.from_iterable(files))\" to fix the API misuse."}
{"number": 448, "change": "class ARMAConv(MessagePassing):\nif self.bias is not None:\nout += self.bias[0 if self.shared_weights else t]\n\n-            if t < self.num_layers - 1:\n+            if self.act is not None and t < self.num_layers - 1:\nout = self.act(out)\n\nreturn out.mean(dim=-3)\n", "fix_pattern": "<condition>: Checking if the bias is not None.\n<pattern>: Adding the condition \"self.act is not None\" in the code.\n<code_one>: \"if t < self.num_layers - 1: \"\n<code_two>: \"if self.act is not None and t < self.num_layers - 1: \"\nFix_pattern: In the condition of checking if the bias is not None, if the condition \"if t < self.num_layers - 1:\" is detected, then change it to \"if self.act is not None and t < self.num_layers - 1:\" to fix the API misuse."}
{"number": 449, "change": "class DependencyParser(flair.nn.Model):\nsentence_tensor = self.word_dropout(sentence_tensor)\n\nif self.use_rnn:\n-            sentence_tensor = pack_padded_sequence(sentence_tensor, lengths, True, False)\n+            sentence_sequence = pack_padded_sequence(sentence_tensor, torch.IntTensor(lengths), True, False)\n\n-            sentence_tensor, _ = self.lstm(sentence_tensor)\n-            sentence_tensor, _ = pad_packed_sequence(sentence_tensor, True, total_length=seq_len)\n+            sentence_sequence, _ = self.lstm(sentence_sequence)\n+            sentence_tensor, _ = pad_packed_sequence(sentence_sequence, True, total_length=seq_len)\n\n# apply MLPs for arc and relations to the BiLSTM output states\narc_h = self.mlp_arc_h(sentence_tensor)\n", "fix_pattern": "Condition: The condition is not clearly indicated in the given context.\n\nPattern: The pattern is the code segment that was removed. In this case, the pattern is:\n\n```\nsentence_tensor = pack_padded_sequence(sentence_tensor, lengths, True, False)\nsentence_tensor, _ = self.lstm(sentence_tensor)\nsentence_tensor, _ = pad_packed_sequence(sentence_tensor, True, total_length=seq_len)\n```\n\nCode One: The code segment that was removed is:\n\n```\nsentence_tensor = pack_padded_sequence(sentence_tensor, lengths, True, False)\nsentence_tensor, _ = self.lstm(sentence_tensor)\nsentence_tensor, _ = pad_packed_sequence(sentence_tensor, True, total_length=seq_len)\n```\n\nCode Two: The code segment that was added is:\n\n```\nsentence_sequence = pack_padded_sequence(sentence_tensor, torch.IntTensor(lengths), True, False)\nsentence_sequence, _ = self.lstm(sentence_sequence)\nsentence_tensor, _ = pad_packed_sequence(sentence_sequence, True, total_length=seq_len)\n```\n\nFix Pattern: In the condition of unknown, if the code segment `sentence_tensor = pack_padded_sequence(sentence_tensor, lengths, True, False) sentence_tensor, _ = self.lstm(sentence_tensor) sentence_tensor, _ = pad_packed_sequence(sentence_tensor, True, total_length=seq_len)` is detected, then change the `sentence_tensor` to `sentence_sequence` to fix the API misuse."}
{"number": 451, "change": "def testtanh():\n\nPtensor = PolynomialTensor()\n\n-    x = torch.linspace(-3, 3, steps=10)\n+    x = torch.tensor(np.linspace(-3, 3, 10))\nexpected = torch.tensor(\n[\n-3.3883e02,\n", "fix_pattern": "Condition: No clear condition can be identified.\nPattern: Remove '3.3883e02,' from the code.\nCode one: '3.3883e02,'\nCode two: 'x = torch.tensor(np.linspace(-3, 3, 10))'\nFix pattern: In the condition with no clear pre-condition, remove '3.3883e02,' from the code and add 'x = torch.tensor(np.linspace(-3, 3, 10))' to fix the API misuse."}
{"number": 452, "change": "class BartTranslationTests(unittest.TestCase):\nwith torch.no_grad():\nlogits, *other_stuff = model(**self.net_input)\n\n-        expected_slice = torch.tensor([9.0078, 10.1113, 14.4787])\n+        expected_slice = torch.tensor([9.0078, 10.1113, 14.4787], device=torch_device)\nresult_slice = logits[0][0][:3]\nself.assertTrue(torch.allclose(expected_slice, result_slice, atol=TOLERANCE))\n", "fix_pattern": "<condition>: The code is checking if the expected slice matches the result slice with a certain tolerance.\n<pattern>: The expected slice is missing the device specification.\n<code_one>: expected_slice = torch.tensor([9.0078, 10.1113, 14.4787])\n<code_two>: expected_slice = torch.tensor([9.0078, 10.1113, 14.4787], device=torch_device)\nFix_pattern: In the condition of checking if the expected slice matches the result slice, a missing device specification is detected, therefore the expected slice should be assigned with the device specification added to fix the API misuse."}
{"number": 453, "change": "def test_dc_crn_separator_invalid_type():\ndef test_dc_crn_separator_output():\nreal = torch.rand(2, 10, 17)\nimag = torch.rand(2, 10, 17)\n-    x = ComplexTensor(real, imag) if is_torch_1_9_plus else torch.complex(real, imag)\n+    x = torch.complex(real, imag) if is_torch_1_9_plus else ComplexTensor(real, imag)\nx_lens = torch.tensor([10, 8], dtype=torch.long)\n\nfor num_spk in range(1, 3):\n", "fix_pattern": "Condition: No specific condition can be identified in the given context.\nPattern: The pattern that is detected is the swapping of the order of the code for creating a ComplexTensor object.\nCode One: The code that is removed is \"ComplexTensor(real, imag) if is_torch_1_9_plus else torch.complex(real, imag)\".\nCode Two: The code that is added is \"torch.complex(real, imag) if is_torch_1_9_plus else ComplexTensor(real, imag)\".\nFix Pattern: In the condition of no specific condition, if the pattern of code for creating a ComplexTensor object is detected, then swap the order of the code from Code One to Code Two to fix the API misuse."}
{"number": 457, "change": "def multilevel_roi_align(features, rcnn_boxes, resolution):\nall_rois = tf.concat(all_rois, axis=0)  # NCHW\n# Unshuffle to the original order, to match the original samples\nlevel_id_perm = tf.concat(level_ids, axis=0)  # A permutation of 1~N\n-    level_id_invert_perm = tf.invert_permutation(level_id_perm)\n+    level_id_invert_perm = tf.math.invert_permutation(level_id_perm)\nall_rois = tf.gather(all_rois, level_id_invert_perm, name=\"output\")\nreturn all_rois\n", "fix_pattern": "Condition: The code is trying to perform a permutation operation on a tensor.\n\nPattern: The tf.invert_permutation function is being used.\n\nCode One: level_id_invert_perm = tf.invert_permutation(level_id_perm)\n\nCode Two: level_id_invert_perm = tf.math.invert_permutation(level_id_perm)\n\nFix Pattern: In the condition of performing a permutation operation on a tensor, if tf.invert_permutation is detected, then change the code from tf.invert_permutation to tf.math.invert_permutation to fix the API misuse."}
{"number": 459, "change": "def _preprocess_deconv_output_shape(x, shape, dim_ordering):\nshape = (shape[0], shape[2], shape[3], shape[1])\n\nif shape[0] is None:\n-        shape = (tf.shape(x)[0], ) + shape[1:]\n+        shape = (tf.shape(x)[0], ) + tuple(shape[1:])\nreturn shape\n", "fix_pattern": "<condition>: The condition is that the value of shape[0] should not be None.\n<pattern>: The pattern is to modify the shape tuple by adding an element from the x tensor shape.\n<code_one>: The code that was removed is shape = (tf.shape(x)[0], ) + shape[1:].\n<code_two>: The code that was added is shape = (tf.shape(x)[0], ) + tuple(shape[1:]).\nFix_pattern: In the condition of shape[0] not being None, the fix pattern is to modify the shape tuple by adding an element from the x tensor shape to ensure correct API usage."}
{"number": 462, "change": "class RGCNConv(MessagePassing):\nreturn out if edge_norm is None else out * edge_norm.view(-1, 1)\n\ndef update(self, aggr_out, x):\n-        if x.dtype == torch.long:\n+        if x is None:\nout = aggr_out + self.root\nelse:\nout = aggr_out + torch.matmul(x, self.root)\n", "fix_pattern": "<condition>: There is a check for the data type of variable x.\n<pattern>: The pattern is that if the data type of x is torch.long.\n<code_one>: The code that has been removed is \"if x.dtype == torch.long:\"\n<code_two>: The code that has been added is \"if x is None:\"\nFix_pattern: In the condition of checking the data type of x, if x is None, then remove the code \"if x.dtype == torch.long:\" to fix the API misuse."}
{"number": 470, "change": "class DenseGCNConv(torch.nn.Module):\nidx = torch.arange(N, dtype=torch.long, device=adj.device)\nadj[:, idx, idx] = 1 if not self.improved else 2\n\n-        out = self.lin(x)\n+        out = torch.matmul(x, self.weight)\ndeg_inv_sqrt = adj.sum(dim=-1).clamp(min=1).pow(-0.5)\n\nadj = deg_inv_sqrt.unsqueeze(-1) * adj * deg_inv_sqrt.unsqueeze(-2)\n", "fix_pattern": "Condition: The condition is that if the variable \"self.improved\" is False.\n\nPattern: The pattern is the replacement of \"self.lin(x)\" with \"torch.matmul(x, self.weight)\".\n\nCode One: The code that was removed is \"out = self.lin(x)\".\n\nCode Two: The code that was added is \"out = torch.matmul(x, self.weight)\".\n\nFix Pattern: In the condition of \"self.improved\" being False, the fix pattern is to replace the line \"out = self.lin(x)\" with \"out = torch.matmul(x, self.weight)\" to fix the API misuse."}
{"number": 472, "change": "class GradientsTest(tf.test.TestCase):\n\n\nif __name__ == \"__main__\":\n-  tf.test.main()\n+  if tf.__internal__.tf2.enabled():\n+    tf.test.main()\n", "fix_pattern": "Condition: The fix is applied when the code is being executed in the main block of the file.\nPattern: The pattern is the absence of a specific condition before the code being removed.\nCode One: tf.test.main()\nCode Two: if tf.__internal__.tf2.enabled(): tf.test.main()\nFix Pattern: In the condition of being executed in the main block, if tf.__internal__.tf2.enabled() is detected, then add the code tf.test.main() to fix the API misuse."}
{"number": 473, "change": "class Model(ModelDesc):\ncost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)\ncost = tf.reduce_mean(cost, name='cross_entropy_loss')\n\n-        acc = tf.to_float(tf.nn.in_top_k(logits, label, 1))\n+        acc = tf.cast(tf.nn.in_top_k(logits, label, 1), tf.float32)\n\nacc = tf.reduce_mean(acc, name='accuracy')\nsummary.add_moving_summary(acc)\n", "fix_pattern": "<condition>: No clear condition is needed.\n<pattern>: A conversion from integer to float is necessary.\n<code_one>: `tf.to_float(tf.nn.in_top_k(logits, label, 1))`\n<code_two>: `tf.cast(tf.nn.in_top_k(logits, label, 1), tf.float32)`\nFix_pattern: In the condition of no clear condition, if a conversion from integer to float is detected, then change the code from `tf.to_float(tf.nn.in_top_k(logits, label, 1))` to `tf.cast(tf.nn.in_top_k(logits, label, 1), tf.float32)` to fix the API misuse."}
{"number": 475, "change": "def block(params, scope, layer_num, bias, sequence_dim, memory_length_dim, varia\nif use_moe:\nmoe_params = mtf.transformer.moe.HParams()\nmtf.transformer.moe.set_default_moe_hparams(moe_params)\n+\n+                # override defaults\nfor k, v in params[\"moe_params\"].items():\nmoe_params.add_hparam(k, v)\n-                mtf.transformer.moe.set_default_moe_hparams(moe_params)\n+\nmoe_train = params[\"mode\"] == \"train\"\n\nm, aux_loss = mtf.transformer.moe.transformer_moe_layer_v1(res_x, x.shape[-1], moe_params,\n", "fix_pattern": "<condition>: The condition is when the variable \"use_moe\" is true. \n<pattern>: The pattern is the call to the function \"mtf.transformer.moe.set_default_moe_hparams(moe_params)\".\n<code_one>: The code that is removed is \"mtf.transformer.moe.set_default_moe_hparams(moe_params)\".\n<code_two>: The code that is added is \"# override defaults\".\nFix_pattern: In the condition of \"use_moe\" being true, if the call to \"mtf.transformer.moe.set_default_moe_hparams(moe_params)\" is detected, then remove the code \"mtf.transformer.moe.set_default_moe_hparams(moe_params)\" and add \"# override defaults\" to fix the API misuse."}
{"number": 476, "change": "class TFCoreModelTesterMixin:\n\nself.assertIsNotNone(outputs)\n\n-        tf.keras.mixed_precision.experimental.set_policy(\"float32\")\n+        tf.keras.mixed_precision.set_global_policy(\"float32\")\n\n@slow\ndef test_train_pipeline_custom_model(self):\n", "fix_pattern": "Condition: There is no pre-condition needed.\nPattern: The code \"tf.keras.mixed_precision.experimental.set_policy(\"float32\")\" was present in the code.\nCode one: \"tf.keras.mixed_precision.experimental.set_policy(\"float32\")\"\nCode two: \"tf.keras.mixed_precision.set_global_policy(\"float32\")\"\nFix Pattern: In the condition of no pre-condition, if the pattern \"tf.keras.mixed_precision.experimental.set_policy(\"float32\")\" is detected, then change the code \"tf.keras.mixed_precision.experimental.set_policy(\"float32\")\" to \"tf.keras.mixed_precision.set_global_policy(\"float32\")\" to fix the API misuse."}
{"number": 479, "change": "class TFMBartForConditionalGeneration(TFMBartPreTrainedModel, TFCausalLanguageMo\nif labels is not None:\nlabels = tf.where(\nlabels == self.config.pad_token_id,\n-                tf.fill(shape_list(labels), -100),\n+                tf.cast(tf.fill(shape_list(labels), -100), labels.dtype),\nlabels,\n)\nuse_cache = False\n", "fix_pattern": "<condition>: The condition in this context is the presence of the \"labels\" variable being not None.\n<pattern>: The pattern that is detected is the usage of \"tf.fill(shape_list(labels), -100)\".\n<code_one>: The code that is being removed is \"tf.fill(shape_list(labels), -100)\".\n<code_two>: The code that is being added is \"tf.cast(tf.fill(shape_list(labels), -100), labels.dtype)\".\nFix_pattern: In the condition of the \"labels\" variable being not None, if the pattern of using \"tf.fill(shape_list(labels), -100)\" is detected, then the code is changed from \"tf.fill(shape_list(labels), -100)\" to \"tf.cast(tf.fill(shape_list(labels), -100), labels.dtype)\" to fix the API misuse."}
{"number": 480, "change": "class TFModelTesterMixin:\ndef _prepare_for_class(self, inputs_dict, model_class, return_labels=False):\nif model_class in TF_MODEL_FOR_MULTIPLE_CHOICE_MAPPING.values():\ninputs_dict = {\n-                k: tf.tile(tf.expand_dims(v, 1), (1, self.model_tester.num_choices, 1))\n-                if isinstance(v, tf.Tensor) and v.ndim != 0\n+                k: tf.tile(tf.expand_dims(v, 1), (1, self.model_tester.num_choices) + (1,) * (v.ndim - 1))\n+                if isinstance(v, tf.Tensor) and v.ndim > 0\nelse v\nfor k, v in inputs_dict.items()\n}\n", "fix_pattern": "<condition>: The condition is checking if the model class is present in a specific mapping (TF_MODEL_FOR_MULTIPLE_CHOICE_MAPPING.values()).\n<pattern>: The pattern is detecting if a specific variable (v) is a TensorFlow tensor and has a dimension not equal to zero.\n<code_one>: The code being removed is the original code that tiled and expanded the dimensions of the variable (v) if it matched the pattern.\n<code_two>: The code being added is an updated version that tiles and expands the dimensions of the variable (v) if it matches the pattern.\nFix_pattern: In the condition of checking the model class, if the variable (v) is a TensorFlow tensor with a non-zero dimension, update the code to tile and expand the dimensions accordingly to fix the API misuse."}
{"number": 482, "change": "class Pandas(datasets.ArrowBasedBuilder):\nreturn pa_table\n\ndef _generate_tables(self, files):\n-        for i, file in enumerate(files):\n+        for i, file in enumerate(itertools.chain.from_iterable(files)):\nwith open(file, \"rb\") as f:\npa_table = pa.Table.from_pandas(pd.read_pickle(f))\nyield i, self._cast_table(pa_table)\n", "fix_pattern": "Condition: No clear condition is needed.\nPattern: The pattern identified is a simple change in code structure.\nCode One: The code line \"for i, file in enumerate(files):\"\nCode Two: The code line \"for i, file in enumerate(itertools.chain.from_iterable(files)):\"\nFix Pattern: In the condition of no clear condition is needed, if the pattern of the code line \"for i, file in enumerate(files):\" is detected, then change the code line to \"for i, file in enumerate(itertools.chain.from_iterable(files))\" to fix the API misuse."}
{"number": 483, "change": "class DiceLoss(nn.Module):\ncardinality = torch.sum(input_soft + target_one_hot, dims)\n\ndice_score = 2. * intersection / (cardinality + self.eps)\n-        return torch.mean(1. - dice_score)\n+        return torch.mean(torch.tensor(1.) - dice_score)\n\n\n######################\n", "fix_pattern": "<condition>: The condition is the calculation of the dice score in the DiceLoss class.\n<pattern>: The pattern is the subtraction of the dice score from 1.0 in the return statement.\n<code_one>: The code removed is \"return torch.mean(1. - dice_score)\".\n<code_two>: The code added is \"return torch.mean(torch.tensor(1.) - dice_score)\".\nFix_pattern: In the condition of calculating the dice score in the DiceLoss class, if the pattern of subtracting the dice score from 1.0 is detected, then change the code \"return torch.mean(1. - dice_score)\" to \"return torch.mean(torch.tensor(1.) - dice_score)\" to fix the API misuse."}
{"number": 488, "change": "class SageMakerTrainingArguments(TrainingArguments):\n# Here, we'll use torch.distributed.\n# Initializes the distributed backend which will take care of synchronizing nodes/GPUs\nif not torch.distributed.is_initialized():\n-                torch.distributed.init_process_group(backend=\"nccl\")\n+                torch.distributed.init_process_group(backend=\"nccl\", timeout=self.ddp_timeout_delta)\ndevice = torch.device(\"cuda\", self.local_rank)\nself._n_gpu = 1\n", "fix_pattern": "Condition: The condition is that the torch.distributed.is_initialized() function returns False.\nPattern: The pattern is the initialization of the distributed process group using torch.distributed.init_process_group(backend=\"nccl\").\nCode one: The code that was removed is \"torch.distributed.init_process_group(backend=\"nccl\")\".\nCode two: The code that was added is \"torch.distributed.init_process_group(backend=\"nccl\", timeout=self.ddp_timeout_delta)\".\nFix_pattern: In the condition of torch.distributed.is_initialized() returning False, the code \"torch.distributed.init_process_group(backend=\"nccl\")\" is removed and replaced with \"torch.distributed.init_process_group(backend=\"nccl\", timeout=self.ddp_timeout_delta)\" to fix the API misuse."}
{"number": 497, "change": "class EvalbBracketingScorer(Metric):\nshutil.rmtree(tempdir)\n\nif is_distributed():\n-            # Setting the device to CPU since this metric is not expected to run on GPUs.\n-            device = torch.device(\"cpu\")\n+            device = torch.device(\"cuda\" if dist.get_backend() == \"nccl\" else \"cpu\")\ncorrect_predicted_brackets = torch.tensor(_correct_predicted_brackets).to(device)\npredicted_brackets = torch.tensor(_predicted_brackets).to(device)\ngold_brackets = torch.tensor(_gold_brackets).to(device)\n", "fix_pattern": "<condition>: The code is checking if the execution is running in a distributed environment.\n<pattern>: The pattern is to change the device of execution depending on whether the distributed backend is \"nccl\" or not.\n<code_one>: The code is setting the device to CPU.\n<code_two>: The code is setting the device to \"cuda\" if the distributed backend is \"nccl\", otherwise it sets it to \"cpu\".\nFix_pattern: In the condition of checking if the execution is running in a distributed environment, if the distributed backend is \"nccl\" change the device of execution to \"cuda\", otherwise set it to \"cpu\" to fix the API misuse."}
{"number": 501, "change": "class E2E(torch.nn.Module):\n# Neither CPUTensor nor float/int value can be used\n# because NCCL communicates between GPU devices.\ndevice = next(self.parameters()).device\n-        acc = torch.tensor([acc], device=device)\n+\n+        acc = torch.tensor([acc], device=device) if acc is not None else None\ncer = torch.tensor([cer], device=device)\nwer = torch.tensor([wer], device=device)\nreturn self.loss, loss_ctc, loss_att, acc, cer, wer\n", "fix_pattern": "<condition>: The condition is that the value of \"acc\" may be None.\n<pattern>: The pattern is to check if \"acc\" is None and assign it to None if it is.\n<code_one>: The code that is removed is the assignment of \"acc\" to a tensor.\n<code_two>: The code that is added is a conditional assignment of \"acc\" to a tensor if it is not None, otherwise it is assigned to None.\nFix_pattern: In the condition of \"acc\" being potentially None, the fix is to remove the assignment of \"acc\" to a tensor and instead add a conditional assignment of \"acc\" to a tensor if it is not None, otherwise assign it to None."}
{"number": 502, "change": "class DeepQNetwork(ValueFunction):\n\"\"\"\n\n# Compute estimated future value\n-        float_terminals = tf.to_float(batch['terminals'])\n+        float_terminals = batch['terminals'].astype(float)\nq_targets = batch['rewards'] + (1. - float_terminals) \\\n* self.gamma * self.get_target_values(batch['next_states'])\n", "fix_pattern": "Condition: The code is dealing with the computation of estimated future value in a deep Q-network.\nPattern: The code is converting a 'terminals' variable to a float using the 'tf.to_float' function.\nCode One: float_terminals = tf.to_float(batch['terminals'])\nCode Two: float_terminals = batch['terminals'].astype(float)\nFix Pattern: In the condition of computing estimated future value, if the 'terminals' variable needs to be converted to a float, then the code should be changed from using 'tf.to_float' to 'astype(float)' to fix the API misuse."}
{"number": 508, "change": "class DistributedFusedAdam(torch.optim.Optimizer):\ngrp = torch.distributed.new_group(ranks=ranks)\nif torch.distributed.get_rank() in ranks:\nself._rs_pg.append(grp)\n-            if self._compute_L2_grad_norm and torch.distributed.get_rank() in ranks:\n-                self._l2_grad_norm_pg = torch.distributed.new_group(ranks=ranks)\n-                torch.distributed.all_reduce(self._overflow_buf,group=self._l2_grad_norm_pg)\n+            if self._compute_L2_grad_norm:\n+                l2_grad_norm_pg = torch.distributed.new_group(ranks=ranks)\n+                if torch.distributed.get_rank() in ranks:\n+                    self._l2_grad_norm_pg = l2_grad_norm_pg\n+                    torch.distributed.all_reduce(self._overflow_buf,group=self._l2_grad_norm_pg)\nself._rs_st = [torch.cuda.Stream() for _ in range(self._num_rs_pg)]\nfor rs_pg in self._rs_pg:\ntorch.distributed.all_reduce(self._overflow_buf,group=rs_pg)\n", "fix_pattern": "<condition>: If the distributed rank is in the specified ranks.\n<pattern>: Creating a new distributed group and performing an all_reduce operation using that group.\n<code_one>: The code that creates a new distributed group for computing L2 gradient norm and performs the all_reduce operation using that group.\n<code_two>: The code that creates a new distributed group for computing L2 gradient norm and assigns it to self._l2_grad_norm_pg only if the distributed rank is in the specified ranks, and then performs the all_reduce operation using that group.\nFix_pattern: In the condition of the distributed rank being in the specified ranks, if the pattern of creating a new distributed group and performing an all_reduce operation is detected, then the code_one is removed and the code_two is added to fix the API misuse."}
{"number": 510, "change": "def cartesian_product_of_parameters(**possible_parameters):\n\n\ndef default_with_one_parameter_changed(*, default={}, **possible_parameters):\n-    assert isinstance(default, dict), f\"default should be a dict not a {type(default)}\"\n+    if not isinstance(default, dict):\n+        raise AssertionError(f\"default should be a dict not a {type(default)}\")\n\nfor parameter_name, possible_values in possible_parameters.items():\nfor v in possible_values:\n", "fix_pattern": "<condition>: The condition is that the \"default\" parameter should be a dictionary.\n<pattern>: The pattern being detected is that the \"default\" parameter is not a dictionary.\n<code_one>: The code being removed is the assertion that checks the type of \"default\".\n<code_two>: The code being added is a raise AssertionError statement that checks the type of \"default\".\nFix_pattern: In the condition of checking the \"default\" parameter, if it is not a dictionary, then raise an AssertionError to fix the API misuse."}
{"number": 516, "change": "class Speech2Text2SinusoidalPositionalEmbedding(nn.Module):\nemb = torch.cat([emb, torch.zeros(num_embeddings, 1)], dim=1)\nif padding_idx is not None:\nemb[padding_idx, :] = 0\n-        return emb\n+        return emb.to(torch.get_default_dtype())\n\n@torch.no_grad()\ndef forward(self, input_ids: torch.Tensor, past_key_values_length: int = 0):\n", "fix_pattern": "Condition: The condition is when there is a need to fix API misuse related to the variable \"emb\" in the code.\nPattern: The pattern is that the return statement for the variable \"emb\" is removed.\nCode One: The code that was removed is \"return emb\".\nCode Two: The code that was added is \"return emb.to(torch.get_default_dtype())\".\nFix Pattern: In the condition of API misuse related to the variable \"emb\", if the pattern of removing the return statement \"return emb\" is detected, then the code should be changed to \"return emb.to(torch.get_default_dtype())\" to fix the API misuse."}
{"number": 518, "change": "class EulerAncestralDiscreteScheduler(SchedulerMixin, ConfigMixin):\n\nprev_sample = sample + derivative * dt\n\n-        device = model_output.device if torch.is_tensor(model_output) else \"cpu\"\n-        if str(device) == \"mps\":\n+        device = model_output.device if torch.is_tensor(model_output) else torch.device(\"cpu\")\n+        if device.type == \"mps\":\n# randn does not work reproducibly on mps\nnoise = torch.randn(model_output.shape, dtype=model_output.dtype, device=\"cpu\", generator=generator).to(\ndevice\n", "fix_pattern": "<condition>: If the device is \"mps\" in the code.\n<pattern>: Comparing the device string with \"mps\".\n<code_one>: The original code checking if the device is \"mps\".\n<code_two>: The updated code checking if the device type is \"mps\".\nFix_pattern: In the condition of checking if the device is \"mps\", the code was changed from comparing the device string to checking the device type to fix the API misuse."}
{"number": 519, "change": "class AutoRegressiveNN(nn.Module):\n\nif permutation is None:\n# By default set a random permutation of variables, which is important for performance with multiple steps\n-            self.permutation = torch.randperm(input_dim)\n+            self.permutation = torch.randperm(input_dim, device='cpu').to(torch.Tensor().device)\nelse:\n# The permutation is chosen by the user\nself.permutation = permutation.type(dtype=torch.int64)\n", "fix_pattern": "Condition: The condition is if permutation is None.\nPattern: The pattern is detecting the code line self.permutation = torch.randperm(input_dim).\nCode one: The code one is self.permutation = torch.randperm(input_dim).\nCode two: The code two is self.permutation = torch.randperm(input_dim, device='cpu').to(torch.Tensor().device).\nFix pattern: In the condition of permutation being None, if the line of code self.permutation = torch.randperm(input_dim) is detected, then change the code to self.permutation = torch.randperm(input_dim, device='cpu').to(torch.Tensor().device) to fix the API misuse."}
{"number": 522, "change": "class StableDiffusionInpaintPipeline(DiffusionPipeline):\nelse:\nraise ImportError(\"Please install accelerate via `pip install accelerate`\")\n\n-        device = torch.device(\"cuda\")\n+        device = torch.device(f\"cuda:{gpu_id}\")\n\nfor cpu_offloaded_model in [self.unet, self.text_encoder, self.vae, self.safety_checker]:\nif cpu_offloaded_model is not None:\n", "fix_pattern": "<condition>: The condition is that the \"cpu_offloaded_model\" variable is not None.\n<pattern>: The pattern is that the \"device\" is being set to \"cuda\".\n<code_one>: The code being removed is \"device = torch.device(\"cuda\")\".\n<code_two>: The code being added is \"device = torch.device(f\"cuda:{gpu_id}\")\".\nFix_pattern: In the condition of \"cpu_offloaded_model\" not being None, if the \"device\" is set to \"cuda\", then change the code to set the \"device\" to \"cuda:{gpu_id}\" to fix the API misuse."}
{"number": 524, "change": "class TwoStageDetector(BaseDetector, RPNTestMixin, BBoxTestMixin,\nif self.with_rpn:\nrpn_outs = self.rpn_head(x)\nouts = outs + (rpn_outs, )\n-        proposals = torch.randn(1000, 4).cuda()\n+        proposals = torch.randn(1000, 4).to(device=img.device)\n# bbox head\nrois = bbox2roi([proposals])\nif self.with_bbox:\n", "fix_pattern": "<condition>: The condition is `if self.with_bbox`.\n<pattern>: The pattern is that the `proposals` variable is being assigned a tensor.\n<code_one>: The code that was removed is `proposals = torch.randn(1000, 4).cuda()`.\n<code_two>: The code that was added is `proposals = torch.randn(1000, 4).to(device=img.device)`.\nFix_pattern: In the condition of `if self.with_bbox`, if the `proposals` variable is assigned a tensor, then remove `proposals = torch.randn(1000, 4).cuda()` and add `proposals = torch.randn(1000, 4).to(device=img.device)` to fix the API misuse."}
{"number": 526, "change": "class PGModel(Model):\nactions = np.concatenate([path['actions'] for path in batch])\nbatch_advantage = np.concatenate([path[\"advantage\"] for path in batch])\nbatch_advantage = zero_mean_unit_variance(batch_advantage)\n+        batch_advantage = np.expand_dims(batch_advantage, axis=1)\nstates = np.concatenate([path['states'] for path in batch])\n\nreturn action_log_stds, action_means, actions, batch_advantage, states\n", "fix_pattern": "Condition: The condition is not provided in the given context.\n\nPattern: There is no pattern found in the code.\n\nCode One: No code is mentioned in the code removed section.\n\nCode Two: The code added is \"batch_advantage = np.expand_dims(batch_advantage, axis=1)\".\n\nFix Pattern: In this fix, the code is modified by adding \"batch_advantage = np.expand_dims(batch_advantage, axis=1)\" to fix the API misuse."}
{"number": 529, "change": "class Categorical(Distribution):\nelif one_hot:\nboolean_mask = x\nelse:\n-                boolean_mask = torch.zeros(ps.size()).scatter_(-1, x.data.long(), 1)\n+                boolean_mask = torch_zeros_like(ps.data).scatter_(-1, x.data.long(), 1)\n# apply log function to masked probability tensor\nreturn torch.log(ps.masked_select(boolean_mask.byte()).contiguous().view(*batch_pdf_size))\n", "fix_pattern": "<condition>: The condition is when the variable \"one_hot\" is true.\n<pattern>: The pattern detected is incorrect initialization of the \"boolean_mask\" variable.\n<code_one>: The code that was removed is \"boolean_mask = torch.zeros(ps.size()).scatter_(-1, x.data.long(), 1)\".\n<code_two>: The code that was added is \"boolean_mask = torch_zeros_like(ps.data).scatter_(-1, x.data.long(), 1)\".\nFix_pattern: In the condition of \"one_hot\" being true, if incorrect initialization of the \"boolean_mask\" variable is detected, then the code \"boolean_mask = torch.zeros(ps.size()).scatter_(-1, x.data.long(), 1)\" should be changed to \"boolean_mask = torch_zeros_like(ps.data).scatter_(-1, x.data.long(), 1)\" to fix the API misuse."}
{"number": 530, "change": "class ViTMAEModelIntegrationTest(unittest.TestCase):\n\n# forward pass\nwith torch.no_grad():\n-            outputs = model(**inputs, noise=torch.from_numpy(noise))\n+            outputs = model(**inputs, noise=torch.from_numpy(noise).to(device=torch_device))\n\n# verify the logits\nexpected_shape = torch.Size((1, 196, 768))\n", "fix_pattern": "<condition>: The code is running a forward pass with no gradient calculation.\n<pattern>: A missing device specification for the noise tensor.\n<code_one>: `noise=torch.from_numpy(noise)`\n<code_two>: `noise=torch.from_numpy(noise).to(device=torch_device)`\nFix_pattern: In the condition of performing a forward pass without gradient calculation, if a missing device specification for the noise tensor is detected, then add `.to(device=torch_device)` to fix the API misuse."}
{"number": 532, "change": "def initialize_vocabulary(vocabulary_path):\nrev_vocab = []\nwith gfile.GFile(vocabulary_path, mode=\"rb\") as f:\nrev_vocab.extend(f.readlines())\n-    rev_vocab = [line.strip() for line in rev_vocab]\n+    rev_vocab = [tf.compat.as_bytes(line.strip()) for line in rev_vocab]\nvocab = dict([(x, y) for (y, x) in enumerate(rev_vocab)])\nreturn vocab, rev_vocab\nelse:\n", "fix_pattern": "Condition: The function is being used to initialize a vocabulary.\n\nPattern: The values in the \"rev_vocab\" list are being stripped of leading and trailing whitespace.\n\nCode one: rev_vocab = [line.strip() for line in rev_vocab]\n\nCode two: rev_vocab = [tf.compat.as_bytes(line.strip()) for line in rev_vocab]\n\nFix_pattern: In the condition of initializing the vocabulary, if leading and trailing whitespace is detected in the values of \"rev_vocab\", then the code that strips the whitespace from the values should be replaced with code that converts the values to a byte string using tf.compat.as_bytes()."}
{"number": 534, "change": "class ConformerSeparator(AbsSeparator):\n\"\"\"\n\n# if complex spectrum,\n-        if isinstance(input, ComplexTensor):\n+        if isinstance(input, ComplexTensor) or (\n+            is_torch_1_8_plus and torch.is_complex(input)\n+        ):\nfeature = abs(input)\nelse:\nfeature = input\n", "fix_pattern": "Condition: The condition is that the input object must be an instance of the ComplexTensor class.\nPattern: The pattern is the removal of the condition check for isinstance(input, ComplexTensor).\nCode one: The code one is the removed condition check: if isinstance(input, ComplexTensor).\nCode two: The code two is the added condition check: if isinstance(input, ComplexTensor) or (is_torch_1_8_plus and torch.is_complex(input)).\nFix pattern: In the condition of input being an instance of ComplexTensor, if the condition check for isinstance(input, ComplexTensor) is detected, then remove the code_one to fix the API misuse and replace it with the code_two."}
{"number": 537, "change": "def batch_flatten(x):\n'''Turn a n-D tensor into a 2D tensor where\nthe first dimension is conserved.\n'''\n-    x = tf.reshape(x, [-1, prod(shape(x)[1:])])\n+    x = tf.reshape(x, tf.pack([-1, prod(shape(x)[1:])]))\nreturn x\n", "fix_pattern": "Condition: No pre condition is needed.\nPattern: The code contains a call to the tf.reshape() function with a fixed shape argument.\nCode One: x = tf.reshape(x, [-1, prod(shape(x)[1:])])\nCode Two: x = tf.reshape(x, tf.pack([-1, prod(shape(x)[1:])]))\nFix Pattern: In the condition of no pre condition is needed, if the pattern of calling tf.reshape() with a fixed shape argument is detected, then change the code to call tf.reshape() with a variable shape argument using tf.pack()."}
{"number": 538, "change": "def distributed_train(local_rank: int, main_address: str, main_port: int, num_no\n# 2. PREPARE DISTRIBUTED MODEL\nmodel = torch.nn.Linear(32, 2)\ndevice = torch.device(f\"cuda:{local_rank}\") if torch.cuda.is_available() else torch.device(\"cpu\")\n-    model = DistributedDataParallel(model, device_ids=[local_rank]).to(device)\n+    model = DistributedDataParallel(model, device_ids=[local_rank] if torch.cuda.is_available() else None).to(device)\n\n# 3. SETUP LOSS AND OPTIMIZER\ncriterion = torch.nn.MSELoss()\n", "fix_pattern": "<condition>: The condition is whether the CUDA is available or not.\n<pattern>: The pattern is the incorrect distribution of the model using `DistributedDataParallel`.\n<code_one>: The code that needs to be removed is `model = DistributedDataParallel(model, device_ids=[local_rank]).to(device)`.\n<code_two>: The code that needs to be added is `model = DistributedDataParallel(model, device_ids=[local_rank] if torch.cuda.is_available() else None).to(device)`.\nFix_pattern: In the condition of CUDA availability, if the incorrect distribution of the model using `DistributedDataParallel` is detected, then remove the code `model = DistributedDataParallel(model, device_ids=[local_rank])` and add the code `model = DistributedDataParallel(model, device_ids=[local_rank] if torch.cuda.is_available() else None)` to fix the API misuse."}
{"number": 548, "change": "class VisionTransformer(nn.Module):\n\ndef forward(self, x):\nx = self.forward_features(x)\n-        if isinstance(x, tuple):\n-            x, x_dist = self.head(x[0]), self.head_dist(x[1])\n+        if self.head_dist is not None:\n+            x, x_dist = self.head(x[0]), self.head_dist(x[1])  # x must be a tuple\nif self.training and not torch.jit.is_scripting():\n# during inference, return the average of both classifier predictions\nreturn x, x_dist\n", "fix_pattern": "<condition>: The condition is \"self.training and not torch.jit.is_scripting()\".\n\n<pattern>: The pattern is \"isinstance(x, tuple)\".\n\n<code_one>: The removed code is \"x, x_dist = self.head(x[0]), self.head_dist(x[1])\".\n\n<code_two>: The added code is \"x, x_dist = self.head(x[0]), self.head_dist(x[1])  # x must be a tuple\".\n\nFix_pattern: In the condition of \"self.training and not torch.jit.is_scripting()\", if the pattern \"isinstance(x, tuple)\" is detected, then remove the code \"x, x_dist = self.head(x[0]), self.head_dist(x[1])\" and add the code \"x, x_dist = self.head(x[0]), self.head_dist(x[1])  # x must be a tuple\" to fix the API misuse."}
{"number": 549, "change": "class TFKerasUtil(object):\n\ndataset = dataset.batch(batch_size).map(prep_data_tf_keras)\nreturn dataset\n-        return fn\n+        return tf.autograph.experimental.do_not_convert(fn) if _HAS_AUTOGRAPH else fn\n\n@staticmethod\ndef get_horovod():\n", "fix_pattern": "Condition: The function `get_horovod()` is called in the code.\nPattern: The function `get_horovod()` is being returned directly.\nCode one: `return fn` \nCode two: `return tf.autograph.experimental.do_not_convert(fn) if _HAS_AUTOGRAPH else fn`\nFix pattern: In the condition of `get_horovod()` being called, if the function `get_horovod()` is detected, then change the code `return fn` to `return tf.autograph.experimental.do_not_convert(fn) if _HAS_AUTOGRAPH else fn` to fix the API misuse."}
{"number": 554, "change": "with tf.device('/cpu:0'):\nnet = FlattenLayer(net, name='flatten')\nnet = DenseLayer(net, 384, act=tf.nn.relu, W_init=W_init2, b_init=b_init2, name='d1relu')\nnet = DenseLayer(net, 192, act=tf.nn.relu, W_init=W_init2, b_init=b_init2, name='d2relu')\n-            net = DenseLayer(net, n_units=10, act=tf.identity, W_init=W_init2, name='output')\n+            net = DenseLayer(net, n_units=10, act=None, W_init=W_init2, name='output')\ny = net.outputs\n\nce = tl.cost.cross_entropy(y, y_, name='cost')\n", "fix_pattern": "Condition: There is a `FlattenLayer` followed by two `DenseLayer` operations in the code.\nPattern: The `DenseLayer` with `n_units=10` and `act=tf.identity` is removed.\nCode One: `net = DenseLayer(net, n_units=10, act=tf.identity, W_init=W_init2, name='output')`.\nCode Two: `net = DenseLayer(net, n_units=10, act=None, W_init=W_init2, name='output')`.\nFix Pattern: In the condition of having a `FlattenLayer` followed by two `DenseLayer` operations, if a `DenseLayer` with `n_units=10` and `act=tf.identity` is detected, then remove the code line and add a new code line `net = DenseLayer(net, n_units=10, act=None, W_init=W_init2, name='output')` to fix the API misuse."}
{"number": 557, "change": "class up(nn.Module):\nif bilinear:\nself.up = nn.UpsamplingBilinear2d(scale_factor=2)\nelse:\n-            self.up = nn.ConvTranspose2d(in_ch, out_ch, 2, stride=2)\n+            self.up = nn.ConvTranspose2d(in_ch//2, in_ch//2, 2, stride=2)\n\nself.conv = double_conv(in_ch, out_ch)\n", "fix_pattern": "<condition>: The condition is when the variable \"bilinear\" is true.\n<pattern>: The pattern is that the nn.UpsamplingBilinear2d function is used instead of nn.ConvTranspose2d.\n<code_one>: The code removed is \"self.up = nn.ConvTranspose2d(in_ch, out_ch, 2, stride=2)\".\n<code_two>: The code added is \"self.up = nn.ConvTranspose2d(in_ch//2, in_ch//2, 2, stride=2)\".\nFix_pattern: In the condition of \"bilinear\" being true, the code \"self.up = nn.ConvTranspose2d(in_ch, out_ch, 2, stride=2)\" is replaced with \"self.up = nn.ConvTranspose2d(in_ch//2, in_ch//2, 2, stride=2)\" to fix the API misuse."}
{"number": 559, "change": "class Metric(nn.Module, ABC):\nAutomatically calls ``update()``. Returns the metric value over inputs if ``compute_on_step`` is True.\n\"\"\"\n# add current step\n-        self.update(*args, **kwargs)\n+        with torch.no_grad():\n+            self.update(*args, **kwargs)\nself._forward_cache = None\n\nif self.compute_on_step:\n", "fix_pattern": "Condition: The condition is that `compute_on_step` is True.\nPattern: The pattern is that the `update()` method is called.\nCode one: The code that is removed is `self.update(*args, **kwargs)`.\nCode two: The code that is added is `with torch.no_grad():` before calling `self.update(*args, **kwargs)`.\n\nFix_pattern: In the condition where `compute_on_step` is True, if the pattern of calling `self.update(*args, **kwargs)` is detected, then add `with torch.no_grad():` before the call to `self.update(*args, **kwargs)` to fix the API misuse."}
{"number": 560, "change": "temperature = max(args.temperature, 1e-3)\nwith open(args.outf, 'w') as outf:\nfor i in range(args.nwords):\n\n-        output, hidden = model(Variable(input, requires_grad=False), hidden)\n-        gen = torch.multinomial(output[0].data.cpu().div(temperature).exp(), 1)[0][0] # FIXME: no multinomial on GPU?\n+        output, hidden = model(Variable(input, volatile=True), hidden)\n+        gen = torch.multinomial(output[0].data.div(temperature).exp().cpu(), 1)[0][0] # FIXME: multinomial is only for CPU\ninput.fill_(gen)\nword = corpus.dic.idx2word[gen]\noutf.write(word)\n", "fix_pattern": "<condition>: There is an API misuse involving the use of the \"multinomial\" function on the GPU.\n<pattern>: The \"multinomial\" function should only be used on the CPU.\n<code_one>: gen = torch.multinomial(output[0].data.cpu().div(temperature).exp(), 1)[0][0] # FIXME: no multinomial on GPU?\n<code_two>: gen = torch.multinomial(output[0].data.div(temperature).exp().cpu(), 1)[0][0] # FIXME: multinomial is only for CPU\nFix_pattern: In the condition of \"no multinomial on GPU\", if the pattern of using the \"multinomial\" function on the GPU is detected, then change the code from <code_one> to <code_two> to fix the API misuse."}
{"number": 561, "change": "class TFFunnelForMultipleChoice(TFFunnelPreTrainedModel, TFMultipleChoiceLoss):\n@tf.function(\ninput_signature=[\n{\n-                \"input_ids\": tf.TensorSpec((None, None), tf.int64, name=\"input_ids\"),\n+                \"input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"input_ids\"),\n\"attention_mask\": tf.TensorSpec((None, None), tf.float32, name=\"attention_mask\"),\n-                \"token_type_ids\": tf.TensorSpec((None, None), tf.int64, name=\"token_type_ids\"),\n+                \"token_type_ids\": tf.TensorSpec((None, None), tf.int32, name=\"token_type_ids\"),\n}\n]\n)\n", "fix_pattern": "<condition>: The condition is that the model class `TFFunnelForMultipleChoice` is using a specific input signature with a `attention_mask` tensor.\n<pattern>: The pattern is an incorrect data type for the `input_ids` and `token_type_ids` tensors, which are defined as `tf.int64` instead of `tf.int32`.\n<code_one>: The code that was removed is the incorrect data type definition for `input_ids` and `token_type_ids` tensors.\n<code_two>: The code that was added is the correct data type definition for `input_ids` and `token_type_ids` tensors.\nFix_pattern: In the condition of `TFFunnelForMultipleChoice` using the `attention_mask` tensor, if the incorrect `input_ids` and `token_type_ids` data types are detected, then remove the incorrect data type definitions and add the correct data type definitions of `input_ids` and `token_type_ids` tensors to fix the API misuse."}
{"number": 566, "change": "class ModelSaver(Callback):\nself.var_collections = var_collections\nif checkpoint_dir is None:\ncheckpoint_dir = logger.get_logger_dir()\n-        assert checkpoint_dir is not None\n-        if not tf.gfile.IsDirectory(checkpoint_dir):\n-            tf.gfile.MakeDirs(checkpoint_dir)\n+        if checkpoint_dir is not None:\n+            if not tf.gfile.IsDirectory(checkpoint_dir):\n+                tf.gfile.MakeDirs(checkpoint_dir)\nself.checkpoint_dir = checkpoint_dir\n\ndef _setup_graph(self):\n+        assert self.checkpoint_dir is not None, \\\n+            \"ModelSaver() doesn't have a valid checkpoint directory.\"\nvars = []\nfor key in self.var_collections:\nvars.extend(tf.get_collection(key))\n", "fix_pattern": "Condition: The condition is that the variable \"checkpoint_dir\" is None.\nPattern: The pattern being detected is the absence of a check for the existence of the directory specified by \"checkpoint_dir\" and the creation of the directory if it does not exist.\nCode One: The code that is being removed is the assertion statement that checks if \"checkpoint_dir\" is not None, and the code that creates the directory if it does not exist.\nCode Two: The code that is being added is the check for the existence of the directory specified by \"checkpoint_dir\" and the creation of the directory if it does not exist, and an assertion statement to ensure that \"self.checkpoint_dir\" is not None.\nFix Pattern: In the condition of \"checkpoint_dir\" being None, if the absence of a check for the existence of the directory specified by \"checkpoint_dir\" is detected, then the assertion statement to check for \"checkpoint_dir\" and the code to create the directory should be added/changed to fix the API misuse."}
{"number": 571, "change": "class ModelCheckpoint(Callback):\nself.best_k_models.pop(del_filepath)\n\n# do not save nan, replace with +/- inf\n-        if torch.isnan(current):\n+        if isinstance(current, torch.Tensor) and torch.isnan(current):\ncurrent = torch.tensor(float('inf' if self.mode == \"min\" else '-inf'))\n\nfilepath = self._get_metric_interpolated_filepath_name(ckpt_name_metrics, epoch, step, del_filepath)\n", "fix_pattern": "<condition>: current is a tensor variable.\n<pattern>: Checking if current is NaN.\n<code_one>: if torch.isnan(current)\n<code_two>: if isinstance(current, torch.Tensor) and torch.isnan(current)\nFix_pattern: In the condition of current being a tensor variable, if current is NaN, then replace the code \"if torch.isnan(current)\" with \"if isinstance(current, torch.Tensor) and torch.isnan(current)\" to fix the API misuse."}
{"number": 574, "change": "class Graph(kerastuner.HyperModel, serializable.Serializable):\n\ndef build(self, hp):\n\"\"\"Build the HyperModel into a Keras Model.\"\"\"\n-        tf.keras.backend.clear_session()\nself._register_hps(hp)\nself.compile()\nreal_nodes = {}\n", "fix_pattern": "Condition: There is no specific condition mentioned in the context section, so no pre-condition is needed.\nPattern: The pattern is the removal of the line \"tf.keras.backend.clear_session()\".\nCode One: tf.keras.backend.clear_session()\nCode Two: \nFix_pattern: In the condition of no specific condition, if the pattern of removing \"tf.keras.backend.clear_session()\" is detected, then remove the line to fix the API misuse."}
{"number": 578, "change": "class GroupViTVisionTransformer(nn.Module):\n\nself.embeddings = GroupViTVisionEmbeddings(config)\nself.encoder = GroupViTVisionEncoder(config)\n-        self.layernorm = nn.LayerNorm(embed_dim)\n+        self.layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n\n@add_start_docstrings_to_model_forward(GROUPVIT_VISION_INPUTS_DOCSTRING)\n@replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=GroupViTVisionConfig)\n", "fix_pattern": "Condition: There is a need to add an epsilon value to the LayerNorm initialization.\nPattern: The constructor of LayerNorm should have an additional parameter for epsilon value.\nCode One: self.layernorm = nn.LayerNorm(embed_dim)\nCode Two: self.layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\nFix Pattern: In the condition of initializing a LayerNorm object, if no epsilon value is detected, then add the parameter \"eps\" with the value \"config.layer_norm_eps\" to fix the API misuse."}
{"number": 585, "change": "class TestTrainSampleHook(tf.test.TestCase):\npred_dict = {}\npred_dict[\"predicted_tokens\"] = tf.constant([[\"Hello\", \"World\", \"\u7b11w\"]])\npred_dict[\"labels.target_tokens\"] = tf.constant([[\"Hello\", \"World\", \"\u7b11w\"]])\n-    pred_dict[\"labels.target_len\"] = tf.constant([2]),\n+    pred_dict[\"labels.target_len\"] = tf.constant(2),\ngraph_utils.add_dict_to_collection(pred_dict, \"predictions\")\n\ndef tearDown(self):\n", "fix_pattern": "Condition: There is a dictionary called \"pred_dict\" containing \"labels.target_len\" as a key.\nPattern: \"labels.target_len\" is assigned a tf.constant([2]) value.\nCode one: pred_dict[\"labels.target_len\"] = tf.constant([2])\nCode two: pred_dict[\"labels.target_len\"] = tf.constant(2)\nFix_pattern: In the condition where \"labels.target_len\" is present in the \"pred_dict\" dictionary, the value assigned to it is changed from tf.constant([2]) to tf.constant(2) in order to fix the API misuse."}
{"number": 587, "change": "class VonMises(TorchDistribution):\n\"\"\"\nshape = self._extended_shape(sample_shape)\nx = torch.empty(shape, dtype=self.loc.dtype, device=self.loc.device)\n-        done = torch.zeros(shape, dtype=self.loc.dtype, device=self.loc.device).byte()\n+        done = torch.zeros(shape, dtype=self.loc.dtype, device=self.loc.device).bool()\nwhile not done.all():\nu = torch.rand((3,) + shape, dtype=self.loc.dtype, device=self.loc.device)\nu1, u2, u3 = u.unbind()\n", "fix_pattern": "Condition: There is a while loop that continues until all elements in the \"done\" tensor are true.\nPattern: The \"done\" tensor is initialized as a byte tensor using torch.zeros().\nCode One: done = torch.zeros(shape, dtype=self.loc.dtype, device=self.loc.device).byte()\nCode Two: done = torch.zeros(shape, dtype=self.loc.dtype, device=self.loc.device).bool()\nFix Pattern: In the condition of the while loop, if the \"done\" tensor is mistakenly initialized as a byte tensor, then it should be changed to a boolean tensor using .bool() to fix the API misuse."}
{"number": 588, "change": "class GridTest(TestCase):\nassert_equal(adj.to_dense().numpy(), expected_adj)\n\ndef test_grid_with_connectivity_8(self):\n-        adj = grid(torch.Size([3, 2]), connectivity=8)\n+        adj = grid_3x3(torch.Size([3, 2]), connectivity=8)\n\nexpected_adj = [\n[0, 1, 1, 2, 0, 0],\n", "fix_pattern": "<condition>: The condition is not explicitly mentioned in the given context.\n<pattern>: The pattern is to replace the grid function call with grid_3x3 function call.\n<code_one>: The code being removed is \"adj = grid(torch.Size([3, 2]), connectivity=8)\".\n<code_two>: The code being added is \"adj = grid_3x3(torch.Size([3, 2]), connectivity=8)\".\nFix_pattern: In the condition of unknown condition, if the grid function call is detected, then the code \"adj = grid(torch.Size([3, 2]), connectivity=8)\" should be removed and replaced with \"adj = grid_3x3(torch.Size([3, 2]), connectivity=8)\" to fix the API misuse."}
{"number": 591, "change": "class Model(ModelDesc):\nself.last_state = tf.identity(last_state, 'last_state')\n\n# seqlen x (Bxrnnsize)\n-        output = tf.reshape(tf.concat(1, outputs), [-1, param.rnn_size])  # (Bxseqlen) x rnnsize\n+        output = tf.reshape(tf.concat_v2(outputs, 1), [-1, param.rnn_size])  # (Bxseqlen) x rnnsize\nlogits = FullyConnected('fc', output, param.vocab_size, nl=tf.identity)\nself.prob = tf.nn.softmax(logits / param.softmax_temprature)\n", "fix_pattern": "<condition>: There is a need to reshape the output tensor.\n<pattern>: Reshaping the output by concatenating the outputs along axis 1 and then reshaping it to [-1, param.rnn_size].\n<code_one>: output = tf.reshape(tf.concat(1, outputs), [-1, param.rnn_size])\n<code_two>: output = tf.reshape(tf.concat_v2(outputs, 1), [-1, param.rnn_size])\nFix_pattern: In the condition of needing to reshape the output tensor, if the pattern of concatenating the outputs along axis 1 is detected, then change the code of concatenation from tf.concat(1, outputs) to tf.concat_v2(outputs, 1) to fix the API misuse."}
{"number": 595, "change": "def fit_line(points: Tensor, weights: Optional[Tensor] = None) -> ParametrizedLi\nA = A.transpose(-2, -1) @ A\n\n# NOTE: not optimal for 2d points, but for now works for other dimensions\n-    _, _, V = torch.linalg.svd(A)\n+    _, _, V = _torch_svd_cast(A)\n+    V = V.transpose(-2, -1)\n\n# the first left eigenvector is the direction on the fited line\ndirection = V[..., 0, :]  # BxD\n", "fix_pattern": "<condition>: There is a need to compute the left eigenvector of a matrix.\n<pattern>: Using the torch.linalg.svd() function to compute the left eigenvector.\n<code_one>: _, _, V = torch.linalg.svd(A)\n<code_two>: _, _, V = _torch_svd_cast(A); V = V.transpose(-2, -1)\nFix_pattern: In the condition of needing to compute the left eigenvector of a matrix, if the torch.linalg.svd() function is being used, then replace it with _torch_svd_cast() and transpose the resulting V tensor to fix the API misuse."}
{"number": 596, "change": "def ndim(x):\n'''Returns the number of axes in a tensor, as an integer.\n'''\nif is_sparse(x):\n-        return int(x.shape.get_shape()[0])\n+        return x._dims\n\ndims = x.get_shape()._dims\nif dims is not None:\n", "fix_pattern": "Condition: If the input tensor is sparse.\nPattern: Accessing the shape of the tensor.\nCode one: Returning the shape of the tensor using \"x.shape.get_shape()[0]\".\nCode two: Returning the dimensions of the tensor using \"x._dims\".\nFix pattern: In the condition of being a sparse tensor, if the shape of the tensor is accessed, then change the code to return the dimensions of the tensor instead."}
{"number": 601, "change": "class Model(ModelDesc):\ninput_list = tf.unstack(input_feature, axis=1)  # seqlen x (Bxrnnsize)\n\n# seqlen is 1 in inference. don't need loop_function\n-        outputs, last_state = tf.nn.rnn(cell, input_list, initial, scope='rnnlm')\n+        outputs, last_state = tf.contrib.rnn.static_rnn(cell, input_list, initial, scope='rnnlm')\nself.last_state = tf.identity(last_state, 'last_state')\n\n# seqlen x (Bxrnnsize)\n", "fix_pattern": "<condition>: The code is using the tf.nn.rnn function.\n<pattern>: The code needs to be changed to use the tf.contrib.rnn.static_rnn function instead.\n<code_one>: tf.nn.rnn(cell, input_list, initial, scope='rnnlm')\n<code_two>: tf.contrib.rnn.static_rnn(cell, input_list, initial, scope='rnnlm')\nFix_pattern: In the condition of using tf.nn.rnn, if the code tf.nn.rnn(cell, input_list, initial, scope='rnnlm') is detected, then change it to tf.contrib.rnn.static_rnn(cell, input_list, initial, scope='rnnlm') to fix the API misuse."}
{"number": 611, "change": "class Attention(nn.Module):\nquery, processed_inputs)\n# apply masking\nif mask is not None:\n-            attention.data.masked_fill_(torch.bitwise_not(mask), self._mask_value)\n+            attention.data.masked_fill_(~mask, self._mask_value)\n# apply windowing - only in eval mode\nif not self.training and self.windowing:\nattention = self.apply_windowing(attention, inputs)\n", "fix_pattern": "<condition>: The condition is if `mask` is not None.\n<pattern>: The pattern is to replace `torch.bitwise_not(mask)` with `~mask`.\n<code_one>: The code that was removed is `attention.data.masked_fill_(torch.bitwise_not(mask), self._mask_value)`.\n<code_two>: The code that was added is `attention.data.masked_fill_(~mask, self._mask_value)`.\nFix_pattern: In the condition of `mask is not None`, if `torch.bitwise_not(mask)` is detected, then change `attention.data.masked_fill_(torch.bitwise_not(mask), self._mask_value)` to `attention.data.masked_fill_(~mask, self._mask_value)` to fix the API misuse."}
{"number": 613, "change": "class EarlyStopping(Callback):\nf\" {self.monitor} = {current} {self.order_dict[self.mode]} {self.divergence_threshold}.\"\n\" Signaling Trainer to stop.\"\n)\n-        elif self.monitor_op(current - self.min_delta, self.best_score):\n+        elif self.monitor_op(current - self.min_delta, self.best_score.to(trainer.lightning_module.device)):\nshould_stop = False\nreason = self._improvement_message(current)\nself.best_score = current\n", "fix_pattern": "<condition>: The condition is when the monitor operation of the current value minus the minimum delta compared to the best score is True.\n<pattern>: The pattern is that the best score needs to be converted to the device used by the trainer.\n<code_one>: The code that was removed is \"elif self.monitor_op(current - self.min_delta, self.best_score)\".\n<code_two>: The code that was added is \"elif self.monitor_op(current - self.min_delta, self.best_score.to(trainer.lightning_module.device))\".\nFix_pattern: In the condition of checking if the monitor operation result is True, the fix requires converting the best score to the same device as the trainer to avoid API misuse."}
{"number": 614, "change": "class LSTM(Model):\nlast_layer = add_time_dimension(features, self.seq_lens)\n\n# Setup the LSTM cell\n-        lstm = rnn.BasicLSTMCell(cell_size, state_is_tuple=True)\n+        lstm = tf.nn.rnn_cell.LSTMCell(cell_size, state_is_tuple=True)\nself.state_init = [\nnp.zeros(lstm.state_size.c, np.float32),\nnp.zeros(lstm.state_size.h, np.float32)\n", "fix_pattern": "Condition: The code is using the rnn module from the tensorflow package.\nPattern: The BasicLSTMCell function is being used to declare the lstm cell.\nCode_one: rnn.BasicLSTMCell(cell_size, state_is_tuple=True)\nCode_two: tf.nn.rnn_cell.LSTMCell(cell_size, state_is_tuple=True)\nFix_pattern: In the condition where the code is using the rnn module, if the BasicLSTMCell function is detected, then it should be changed to LSTMCell to fix the API misuse."}
{"number": 615, "change": "class XGLMModel(XGLMPreTrainedModel):\n\nhidden_states = inputs_embeds + positions\n\n-        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n+        hidden_states = nn.functional.dropout(hidden_states, p=float(self.dropout), training=self.training)\n\n# decoder layers\nall_hidden_states = () if output_hidden_states else None\n", "fix_pattern": "Condition: The condition is not clearly stated in the given context.\n\nPattern: The pattern is to remove the use of the `nn.functional.dropout()` method with a variable `self.dropout`.\n\nCode one: `hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)`\n\nCode two: `hidden_states = nn.functional.dropout(hidden_states, p=float(self.dropout), training=self.training)`\n\nFix pattern: In the condition of [no pre condition is needed], if the pattern of `nn.functional.dropout()` method used with a variable `self.dropout` is detected, then change the code from `hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)` to `hidden_states = nn.functional.dropout(hidden_states, p=float(self.dropout), training=self.training)` to fix the API misuse."}
{"number": 618, "change": "class CategoricalAccuracy(Metric):\ncorrect.unsqueeze_(-1)\n\nif mask is not None:\n-            correct *= mask.view(-1, 1).float()\n+            correct *= mask.view(-1, 1)\nself.total_count += mask.sum()\nelse:\nself.total_count += gold_labels.numel()\n", "fix_pattern": "<condition>: The condition is checking if the variable mask is not None.\n<pattern>: The pattern detected is multiplying the variable correct by mask.\n<code_one>: The code that was removed is \"correct *= mask.view(-1, 1).float()\".\n<code_two>: The code that was added is \"correct *= mask.view(-1, 1)\".\nFix_pattern: In the condition of checking if mask is not None, if the pattern of multiplying correct by mask is detected, then remove the code \"correct *= mask.view(-1, 1).float()\" and replace it with \"correct *= mask.view(-1, 1)\" to fix the API misuse."}
{"number": 621, "change": "def conditional(\nif f_scale_tril is not None:\npack = torch.cat((pack, f_scale_tril_2D), dim=1)\n\n-        Lffinv_pack = pack.triangular_solve(Lff, upper=False)[0]\n+        Lffinv_pack = torch.linalg.solve_triangular(Lff, pack, upper=False)\n# unpack\nv_2D = Lffinv_pack[:, : f_loc_2D.size(1)]\nW = Lffinv_pack[:, f_loc_2D.size(1) : f_loc_2D.size(1) + M].t()\n", "fix_pattern": "<condition>: If the variable f_scale_tril is not None.\n<pattern>: In the code, there is a call to the triangular_solve() function to solve a linear equation using triangular matrices.\n<code_one>: The code implements the triangular_solve() function to solve the equation. \n<code_two>: The code changes the implementation to use the solve_triangular() function from the torch.linalg module.\nFix_pattern: In the condition of f_scale_tril not being None, if the code is using the triangular_solve() function, then change it to use the solve_triangular() function to fix the API misuse."}
{"number": 623, "change": "class Model(ModelDesc):\nwrong = prediction_incorrect(logits, label, 5, name='wrong-top5')\nadd_moving_summary(tf.reduce_mean(wrong, name='train-error-top5'))\n\n-        wd_cost = tf.mul(1e-4, regularize_cost('.*/W', tf.nn.l2_loss), name='l2_regularize_loss')\n+        wd_cost = regularize_cost('.*/W', l2_regularizer(1e-4), name='l2_regularize_loss')\nadd_moving_summary(loss, wd_cost)\nself.cost = tf.add_n([loss, wd_cost], name='cost')\n", "fix_pattern": "<condition>: The condition is not clear in the given context.\n<pattern>: The pattern detected is the use of tf.mul() to multiply a constant value with the result of regularize_cost().\n<code_one>: The code that was removed is tf.mul(1e-4, regularize_cost('.*/W', tf.nn.l2_loss), name='l2_regularize_loss').\n<code_two>: The code that was added is regularize_cost('.*/W', l2_regularizer(1e-4), name='l2_regularize_loss').\nFix_pattern: In the condition of unknown, if the pattern of tf.mul() with a constant value and regularize_cost() is detected, then the code tf.mul() is removed and replaced with regularize_cost() called with l2_regularizer()."}
{"number": 627, "change": "class Optimizer:\ng = [dev_grads[dev][var_idx][0] for dev in devices]\n\nif np.prod(grad_shape):  # nccl does not support zero-sized tensors\n-                            g = tf.contrib.nccl.all_sum(g)\n+                            g = nccl_ops.all_sum(g)\n\nfor dev, gg in zip(devices, g):\ndev_grads[dev][var_idx] = (gg, dev_grads[dev][var_idx][1])\n", "fix_pattern": "<condition>: The condition is \"if np.prod(grad_shape):\".\n<pattern>: The pattern is \"tf.contrib.nccl.all_sum(g)\".\n<code_one>: The code being removed is \"g = tf.contrib.nccl.all_sum(g)\".\n<code_two>: The code being added is \"g = nccl_ops.all_sum(g)\".\nFix_pattern: In the condition of \"if np.prod(grad_shape)\", if the pattern \"tf.contrib.nccl.all_sum(g)\" is detected, then change the \"g = tf.contrib.nccl.all_sum(g)\" to \"g = nccl_ops.all_sum(g)\" to fix the API misuse."}
{"number": 639, "change": "class LinearModel(object):\nreturn self.sess.run(self.cross_entropy_grads, feed_dict={self.x: xs, self.y_: ys})\n\ndef net_initialization():\n-  return LinearModel([784,10])\n+  with tf.Graph().as_default():\n+    return LinearModel([784,10])\n\n# By default, when an environment variable is used by a remote function, the\n# initialization code will be rerun at the end of the remote task to ensure\n", "fix_pattern": "<condition>: No clear condition can be identified.\n<pattern>: The code that initializes the LinearModel object is being changed.\n<code_one>: return LinearModel([784,10])\n<code_two>: with tf.Graph().as_default(): return LinearModel([784,10])\nFix_pattern: In the net_initialization() function, the code that initializes the LinearModel object is changed to include a tf.Graph().as_default() block."}
{"number": 640, "change": "class EpochResultStore:\n# attach capture batch_size\nResult.attach_batch_size(self._batch_size, hook_result)\n\n-            hook_result.detach()\n+            hook_result = hook_result.detach()\nif self.trainer.move_metrics_to_cpu:\n-                hook_result.cpu()\n+                hook_result = hook_result.cpu()\nelif self.trainer._distrib_type == DistributedType.DP:\n-                hook_result.to(torch.device(\"cuda\", self.trainer.root_gpu))\n+                hook_result = hook_result.to(torch.device(\"cuda\", self.trainer.root_gpu))\n\nself._internals[fx_name].append(hook_result, info)\n", "fix_pattern": "<condition>: The condition is that the trainer's move_metrics_to_cpu attribute is true and the trainer's distrib_type attribute is DistributedType.DP.\n<pattern>: The pattern detected is that the hook_result is being detached, moved to the CPU, and then moved to a CUDA device.\n<code_one>: The code being removed is \"hook_result.detach()\\nhook_result.cpu()\\nhook_result.to(torch.device(\"cuda\", self.trainer.root_gpu))\".\n<code_two>: The code being added is \"hook_result = hook_result.detach()\\nhook_result = hook_result.cpu()\\nhook_result = hook_result.to(torch.device(\"cuda\", self.trainer.root_gpu))\".\nFix_pattern: To fix the API misuse, the code snippets for detaching, moving to CPU, and moving to CUDA are replaced with reassignment statements that update the hook_result variable accordingly."}
{"number": 641, "change": "class LinearRegression(d2l.Module):\ndef __init__(self, lr):\nsuper().__init__()\nself.save_hyperparameters()\n-        self.net = tf.keras.layers.Dense(1)\n+        initializer = tf.initializers.RandomNormal(stddev=0.01)\n+        self.net = tf.keras.layers.Dense(1, kernel_initializer=initializer)\n\ndef forward(self, X):\n\"\"\"The linear regression model.\n", "fix_pattern": "Condition: There is a need to provide an initializer for the Dense layer in the LinearRegression model.\nPattern: The code for initializing the Dense layer has been removed.\nCode One: \"self.net = tf.keras.layers.Dense(1)\"\nCode Two: \"initializer = tf.initializers.RandomNormal(stddev=0.01)\\nself.net = tf.keras.layers.Dense(1, kernel_initializer=initializer)\"\nFix Pattern: In the condition of needing to initialize the Dense layer, the code for initializing the layer is removed and replaced with a new line that initializes the layer with a RandomNormal initializer."}
{"number": 643, "change": "class TestPretrainedTransformerEmbedder(AllenNlpTestCase):\ndef test_xlnet_token_type_ids(self):\ntoken_embedder = PretrainedTransformerEmbedder(\"xlnet-base-cased\")\ntoken_ids = torch.LongTensor([[1, 2, 3], [2, 3, 4]])\n-        mask = torch.ones_like(token_ids)\n+        mask = torch.ones_like(token_ids).bool()\ntype_ids = torch.zeros_like(token_ids)\ntype_ids[1, 1] = 1\ntoken_embedder(token_ids, mask, type_ids)\n", "fix_pattern": "<condition>: The condition is when using the `PretrainedTransformerEmbedder` class.\n<pattern>: The pattern is that the `mask` variable needs to be modified to have a boolean data type.\n<code_one>: The code that was removed is `mask = torch.ones_like(token_ids)`.\n<code_two>: The code that was added is `mask = torch.ones_like(token_ids).bool()`.\nFix_pattern: In the condition of using the `PretrainedTransformerEmbedder`, if the `mask` variable is detected without a boolean data type, then it is modified by changing `mask = torch.ones_like(token_ids)` to `mask = torch.ones_like(token_ids).bool()` to fix the API misuse."}
{"number": 645, "change": "class Block(Layer):\nlayer_counter[layer_type] += 1\n\n# layer_name = self.name + '-' + layer_name\n-            self.layers[n] = self.submodule(\n+            layer = self.submodule(\nname=layer_name, module=layer_spec, modules=tensorforce.core.layer_modules,\ninput_spec=self._input_spec\n)\n-            self._input_spec = self.layers[n].output_spec()\n-\n+            self.layers.append(layer)\n+            self._input_spec = layer.output_spec()\n\nreturn self.layers[0].input_spec.copy()\n", "fix_pattern": "<condition>: This fix pattern is applicable when the layer counter needs to be updated.\n<pattern>: If the condition is met, then the code for updating the input specification of a layer is removed.\n<code_one>: self.layers[n] = self.submodule(\n<code_two>: self._input_spec = layer.output_spec()\nFix_pattern: In the condition of updating the layer counter, if the code for updating the input specification of a layer is detected, then it is removed and replaced with the code to update the input specification with the output specification of the layer."}
{"number": 648, "change": "def model():\n\nif sd_vae_approx_model is None:\nsd_vae_approx_model = VAEApprox()\n-        sd_vae_approx_model.load_state_dict(torch.load(os.path.join(paths.models_path, \"VAE-approx\", \"model.pt\")))\n+        sd_vae_approx_model.load_state_dict(torch.load(os.path.join(paths.models_path, \"VAE-approx\", \"model.pt\"), map_location='cpu' if devices.device.type != 'cuda' else None))\nsd_vae_approx_model.eval()\nsd_vae_approx_model.to(devices.device, devices.dtype)\n", "fix_pattern": "Condition: The condition is that the variable sd_vae_approx_model is None.\nPattern: The pattern detected is that the state dictionary of sd_vae_approx_model is loaded without specifying the map_location.\nCode one: The code that is removed is \"sd_vae_approx_model.load_state_dict(torch.load(os.path.join(paths.models_path, \"VAE-approx\", \"model.pt\")))\".\nCode two: The code that is added is \"sd_vae_approx_model.load_state_dict(torch.load(os.path.join(paths.models_path, \"VAE-approx\", \"model.pt\"), map_location='cpu' if devices.device.type != 'cuda' else None))\".\nFix pattern: In the condition of sd_vae_approx_model being None, the pattern of loading the state dictionary of sd_vae_approx_model without specifying the map_location is detected, so the code \"sd_vae_approx_model.load_state_dict(torch.load(os.path.join(paths.models_path, \"VAE-approx\", \"model.pt\")))\" is changed to \"sd_vae_approx_model.load_state_dict(torch.load(os.path.join(paths.models_path, \"VAE-approx\", \"model.pt\"), map_location='cpu' if devices.device.type != 'cuda' else None))\" to fix the API misuse."}
{"number": 655, "change": "class {{cookiecutter.camelcase_modelname}}EncoderLayer(nn.Module):\nhidden_states = residual + hidden_states\nhidden_states = self.final_layer_norm(hidden_states)\n\n-        if torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any():\n+        if hidden_states.dtype == torch.float16 and (torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any()):\nclamp_value = torch.finfo(hidden_states.dtype).max - 1000\nhidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n", "fix_pattern": "<condition>: No clear condition is needed.\n<pattern>: Checking for infinite or NaN values in the tensor.\n<code_one>: Checking for infinite or NaN values using the `torch.isinf` and `torch.isnan` functions.\n<code_two>: Checking for infinite or NaN values using the `torch.isinf` and `torch.isnan` functions, only if the tensor dtype is `torch.float16`.\nFix_pattern: In the condition of no pre condition, if the pattern of checking for infinite or NaN values is detected, then remove the code for checking without considering the tensor dtype, and add the code for checking only if the tensor dtype is `torch.float16`, to fix the API misuse."}
{"number": 656, "change": "class LabelSmoother:\n\ndef __call__(self, model_output, labels):\nlogits = model_output[\"logits\"] if isinstance(model_output, dict) else model_output[0]\n-        log_probs = -torch.nn.functional.log_softmax(logits, dim=-1)\n+        log_probs = -nn.functional.log_softmax(logits, dim=-1)\nif labels.dim() == log_probs.dim() - 1:\nlabels = labels.unsqueeze(-1)\n", "fix_pattern": "Condition: The condition is checking if the dimension of labels is one less than the dimension of logits. \nPattern: The pattern is that log_probs is computed by applying the log_softmax function on logits. \nCode one: The code that was removed is \"-torch.nn.functional.log_softmax(logits, dim=-1)\". \nCode two: The code that was added is \"-nn.functional.log_softmax(logits, dim=-1)\". \nFix pattern: In the condition of checking the dimensions, if the pattern of computing log_probs using log_softmax is detected, then change the code \"-torch.nn.functional.log_softmax(logits, dim=-1)\" to \"-nn.functional.log_softmax(logits, dim=-1)\" to fix the API misuse."}
{"number": 661, "change": "class Model(ModelDesc):\n.apply(fg)\n.BatchNorm('bn5').apply(activate)\n# 5\n-                      .tf.nn.dropout(0.5 if is_training else 1.0)\n+                      .Dropout(rate=0.5 if is_training else 0.0)\n.Conv2D('conv6', 512, 5, padding='VALID')\n.apply(fg).BatchNorm('bn6')\n.apply(nonlin)\n", "fix_pattern": "<condition>: The condition is not explicitly mentioned in the context section. \n<pattern>: The pattern is to replace the dropout method from TensorFlow (tf.nn.dropout) with Ze"}
{"number": 665, "change": "class BLEU(Metric):\nreturn math.exp(1.0 - self._reference_lengths / self._prediction_lengths)\n\ndef _get_valid_tokens_mask(self, tensor: torch.LongTensor) -> torch.ByteTensor:\n-        valid_tokens_mask = torch.ones(tensor.size(), dtype=torch.uint8)\n+        valid_tokens_mask = torch.ones(tensor.size(), dtype=torch.bool)\nfor index in self._exclude_indices:\nvalid_tokens_mask = valid_tokens_mask & (tensor != index)\nreturn valid_tokens_mask\n", "fix_pattern": "<condition>: When calculating the BLEU metric in the class BLEU.\n<pattern>: Valid tokens mask is created using torch ones with dtype torch.uint8.\n<code_one>: valid_tokens_mask = torch.ones(tensor.size(), dtype=torch.uint8)\n<code_two>: valid_tokens_mask = torch.ones(tensor.size(), dtype=torch.bool)\nFix_pattern: In the condition of calculating the BLEU metric, if the valid tokens mask is created using torch ones with dtype torch.uint8, then change the code to create the valid tokens mask using torch ones with dtype torch.bool to fix the API misuse."}
{"number": 672, "change": "class MessagePassing(torch.nn.Module):\nthe_size: List[Optional[int]] = [None, None]\n\nif isinstance(edge_index, Tensor):\n-            assert edge_index.dtype == torch.long\n-            assert edge_index.dim() == 2\n-            assert edge_index.size(0) == 2\n+            assert edge_index.dtype == torch.long, \\\n+                \"edge_index.dtype is not of torch.long\"\n+            assert edge_index.dim() == 2, \\\n+                \"edge_index.dim() is not equal to 2\"\n+            assert edge_index.size(0) == 2, \\\n+                \"edge_index.size(0) is not equal to 2\"\nif size is not None:\nthe_size[0] = size[0]\nthe_size[1] = size[1]\n", "fix_pattern": "<condition>: The condition is that `edge_index` should be an instance of `Tensor`.\n<pattern>: The pattern is that certain assertions are used to check the properties of `edge_index`.\n<code_one>: The code that is being changed is the set of assertions that check `edge_index`.\n<code_two>: The code that is added is a modified set of assertions that check `edge_index` and provide error messages.\nFix_pattern: In the condition of `edge_index` being a `Tensor`, if the pattern of assertions is detected, then the assertions are removed and replaced with modified assertions that provide error messages to fix the API misuse."}
{"number": 674, "change": "class DecoderLayer(nn.Module):\nif self.normalize_before:\nx = self.norm2(x)\nif self.concate_after:\n-            x_concat = torch.cat(x, self.src_attn(x, memory, memory, memory_mask))\n+            x_concat = torch.cat((x, self.src_attn(x, memory, memory, memory_mask)), dim=-1)\nx = residual + self.concate_linear2(x_concat)\nelse:\nx = residual + self.dropout(self.src_attn(x, memory, memory, memory_mask))\n", "fix_pattern": "condition: The condition is missing, there is no clear condition identified.\n\npattern: The pattern is to concatenate two tensors using the torch.cat() function.\n\ncode_one: The code removed is the existing concatenation code.\n\ncode_two: The code added is the fixed concatenation code.\n\nfix_pattern: In the condition of no clear condition, if the pattern of concatenating tensors using torch.cat() is detected, then remove the existing concatenation code and add the fixed concatenation code to fix the API misuse."}
{"number": 677, "change": "def clip(\n*,\nout: Optional[torch.Tensor] = None,\n) -> torch.Tensor:\n-    assert torch.all(torch.less(x_min, x_max)), \"Min value must be less than max.\"\n+    assert torch.all(\n+        torch.less(torch.tensor(x_min), x_max)\n+    ), \"Min value must be less than max.\"\nif hasattr(x_min, \"dtype\"):\npromoted_type = torch.promote_types(x_min.dtype, x_max.dtype)\npromoted_type = torch.promote_types(promoted_type, x.dtype)\n", "fix_pattern": "<condition>: The condition is checking if the variable x_min has a \"dtype\" attribute.\n<pattern>: The pattern is an API misuse where the \"torch.all(torch.less(x_min, x_max))\" is used as an assertion for \"Min value must be less than max.\"\n<code_one>: The code that was removed is the assertion statement \"assert torch.all(torch.less(x_min, x_max)), \"Min value must be less than max.\"\"\n<code_two>: The code that was added is a modified version of the assertion statement using the torch.tensor() function to convert x_min to a tensor, \"assert torch.all(torch.less(torch.tensor(x_min), x_max)), \"Min value must be less than max.\"\"\nFix_pattern: In the condition of checking if x_min has a \"dtype\" attribute, if the pattern of using \"torch.all(torch.less(x_min, x_max))\" as an assertion is detected, then the code is modified to use \"assert torch.all(torch.less(torch.tensor(x_min), x_max)), \"Min value must be less than max.\"\""}
{"number": 681, "change": "class LayoutLMv3Model(LayoutLMv3PreTrainedModel):\nposition_ids = position_ids.expand_as(input_ids)\nfinal_position_ids = position_ids\n\n-        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, None, device)\n+        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(\n+            attention_mask, None, device, dtype=embedding_output.dtype\n+        )\n\n# Prepare head mask if needed\n# 1.0 in head_mask indicate we keep the head\n", "fix_pattern": "<condition>: There is a need to prepare a head mask.\n<pattern>: The code for obtaining the extended attention mask is changed.\n<code_one>: extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, None, device)\n<code_two>: extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, None, device, dtype=embedding_output.dtype)\nFix_pattern: In the condition of needing a head mask, if the existing code for obtaining the extended attention mask is detected, then change the code to include the dtype parameter for fixing the API misuse."}
{"number": 683, "change": "class StableDiffusionKDiffusionPipeline(DiffusionPipeline):\nreturn_tensors=\"pt\",\n)\ntext_input_ids = text_inputs.input_ids\n-        untruncated_ids = self.tokenizer(prompt, padding=\"max_length\", return_tensors=\"pt\").input_ids\n+        untruncated_ids = self.tokenizer(prompt, padding=\"longest\", return_tensors=\"pt\").input_ids\n\n-        if not torch.equal(text_input_ids, untruncated_ids):\n+        if untruncated_ids.shape[-1] >= text_input_ids.shape[-1] and not torch.equal(text_input_ids, untruncated_ids):\nremoved_text = self.tokenizer.batch_decode(untruncated_ids[:, self.tokenizer.model_max_length - 1 : -1])\nlogger.warning(\n\"The following part of your input was truncated because CLIP can only handle sequences up to\"\n", "fix_pattern": "<condition>: There is a check for equality between `text_input_ids` and `untruncated_ids`.\n<pattern>: The pattern is to change the padding strategy to \"longest\" and add an additional condition to check if `untruncated_ids` has a length greater than or equal to `text_input_ids`.\n<code_one>: `padding=\"max_length\"`\n<code_two>: `padding=\"longest\"`\nFix_pattern: In the condition of checking for equality between `text_input_ids` and `untruncated_ids`, change the padding strategy from \"max_length\" to \"longest\" and also add a condition to check if `untruncated_ids` has a length greater than or equal to `text_input_ids` to fix the API misuse."}
{"number": 693, "change": "class Ensemble(nn.ModuleList):\n\n\ndef attempt_load(weights, device=None, inplace=True, fuse=True):\n+    # Loads an ensemble of models weights=[a,b,c] or a single model weights=[a] or weights=a\nfrom models.yolo import Detect, Model\n\n-    # Loads an ensemble of models weights=[a,b,c] or a single model weights=[a] or weights=a\nmodel = Ensemble()\nfor w in weights if isinstance(weights, list) else [weights]:\n-        ckpt = torch.load(attempt_download(w), map_location=device)\n-        ckpt = (ckpt.get('ema') or ckpt['model']).float()  # FP32 model\n+        ckpt = torch.load(attempt_download(w), map_location='cpu')  # load\n+        ckpt = (ckpt.get('ema') or ckpt['model']).to(device).float()  # FP32 model\nmodel.append(ckpt.fuse().eval() if fuse else ckpt.eval())  # fused or un-fused model in eval mode\n\n# Compatibility updates\n", "fix_pattern": "<condition>: No specific condition identified.\n<pattern>: The pattern is to change the loading and conversion of the model weights.\n<code_one>: ckpt = torch.load(attempt_download(w), map_location=device)\n<code_two>: ckpt = torch.load(attempt_download(w), map_location='cpu')  # load\nFix_pattern: In the condition of no specific condition, if the code line to load the model weights is detected, then change the device in the map_location argument to 'cpu' to fix the API misuse."}
{"number": 707, "change": "class TFLiteBackendInferenceLearner(TensorflowBaseInferenceLearner):\nself.interpreter.set_tensor(i, input_tensor)\nself.interpreter.invoke()\nreturn tuple(\n-            self.interpreter.get_tensor(output_detail[\"index\"])\n+            tf.convert_to_tensor(\n+                self.interpreter.get_tensor(output_detail[\"index\"])\n+            )\nfor output_detail in output_details\n)\n", "fix_pattern": "Condition: There is an API misuse in the code.\nPattern: A call to \"self.interpreter.get_tensor(output_detail[\"index\"])\" is made without converting the returned value to a TensorFlow tensor.\nCode One: \"self.interpreter.get_tensor(output_detail[\"index\"])\"\nCode Two: \"tf.convert_to_tensor(self.interpreter.get_tensor(output_detail[\"index\"]))\"\nFix pattern: In the condition of API misuse, if a call to \"self.interpreter.get_tensor(output_detail[\"index\"])\" is detected, then the code should be changed to \"tf.convert_to_tensor(self.interpreter.get_tensor(output_detail[\"index\"]))\" to fix the API misuse."}
{"number": 710, "change": "class Model(ModelDesc):\n.Conv2D('conv3.1', filters=128, padding='VALID') \\\n.Conv2D('conv3.2', filters=128, padding='VALID') \\\n.FullyConnected('fc0', 1024 + 512, activation=tf.nn.relu) \\\n-                .tf.nn.dropout(keep_prob) \\\n+                .Dropout(rate=drop_rate) \\\n.FullyConnected('fc1', 512, activation=tf.nn.relu) \\\n.FullyConnected('linear', out_dim=self.cifar_classnum)()\n", "fix_pattern": "<condition>: The code is using the tf.nn.dropout() function to apply dropout during training.\n<pattern>: The pattern detected is that the tf.nn.dropout() function is being used.\n<code_one>: The code that is being removed is \".tf.nn.dropout(keep_prob)\".\n<code_two>: The code that is being added is \".Dropout(rate=drop_rate)\".\nFix_pattern: In the condition of using the tf.nn.dropout() function, if the pattern \".tf.nn.dropout(keep_prob)\" is detected, then it is replaced with \".Dropout(rate=drop_rate)\" to fix the API misuse."}
{"number": 720, "change": "class CustomConverter(object):\nxs_pad = pad_list([torch.from_numpy(x).float() for x in xs], 0).to(device, dtype=self.dtype)\n\nilens = torch.from_numpy(ilens).to(device)\n-        # NOTE: this is for multi-task learning (e.g., speech translation)\n-        ys_pad = pad_list([torch.from_numpy(np.array(y[0]) if isinstance(y, tuple) else y).long()\n+        # NOTE: this is for multi-output (e.g., speech translation)\n+        ys_pad = pad_list([torch.from_numpy(np.array(y[0][:]) if isinstance(y, tuple) else y).long()\nfor y in ys], self.ignore_id).to(device)\n\nreturn xs_pad, ilens, ys_pad\n", "fix_pattern": "<condition>: No clear pre condition is needed.\n<pattern>: In the code, the variable 'ys_pad' is being initialized and padded using 'pad_list' function, which takes a list of tensors as input.\n<code_one>: ys_pad = pad_list([torch.from_numpy(np.array(y[0]) if isinstance(y, tuple) else y).long()\n<code_two>: ys_pad = pad_list([torch.from_numpy(np.array(y[0][:]) if isinstance(y, tuple) else y).long()\nFix_pattern: In the condition of initializing 'ys_pad', if the input 'y' is a tuple, then the code should add an extra indexing [:] to correctly access the data before padding."}
{"number": 725, "change": "def run_benchmark(state):\n\n\ndef on_state_reset():\n-    tf.keras.backend.set_value(model.optimizer.lr, lr * hvd.size())\n+    opt.lr.assign(lr * hvd.size())\n\n\nstate = hvd.elastic.TensorFlowKerasState(model, opt, img_secs=[], iter=0, batch=0, warm=False)\n", "fix_pattern": "<condition>: The condition is not clearly identified in the given context.\n<pattern>: The pattern is to change the code `tf.keras.backend.set_value(model.optimizer.lr, lr * hvd.size())` to `opt.lr.assign(lr * hvd.size())`.\n<code_one>: `tf.keras.backend.set_value(model.optimizer.lr, lr * hvd.size())`\n<code_two>: `opt.lr.assign(lr * hvd.size())`\nFix pattern: In the condition (if any), if the code `tf.keras.backend.set_value(model.optimizer.lr, lr * hvd.size())` is detected, then change it to `opt.lr.assign(lr * hvd.size())` to fix the API misuse."}
{"number": 728, "change": "def vector_to_skew_symmetric_matrix(\na2s = vector_expanded[..., 1:2, :]\na3s = vector_expanded[..., 2:3, :]\n# BS x 1 x 1\n-    zs = torch.zeros(batch_shape + [1, 1], device=vector.device)\n+    zs = torch.zeros(batch_shape + [1, 1], device=vector.device, dtype=vector.dtype)\n# BS x 1 x 3\nrow1 = torch.cat((zs, -a3s, a2s), -1)\nrow2 = torch.cat((a3s, zs, -a1s), -1)\n", "fix_pattern": "<condition>: The condition is not specified in the given code snippet. No pre-condition is needed.\n<pattern>: The pattern is to change the dtype of the tensor being created.\n<code_one>: The code that was removed is creating a tensor with the default dtype.\n<code_two>: The code that was added includes specifying the dtype of the tensor as the dtype of the input vector.\nFix_pattern: In the condition of not specified, if the pattern of creating a tensor without specifying the dtype is detected, then change the code of creating the tensor to include specifying the dtype as the dtype of the input vector to fix the API misuse."}
{"number": 730, "change": "from pyro.ops.einsum import contract\ndef _finfo(tensor):\n# This can be replaced with torch.finfo once it is available\n# https://github.com/pytorch/pytorch/issues/10742\n-    return np.finfo(torch.empty(torch.Size(), dtype=tensor.dtype).numpy().dtype)\n+    return np.finfo(torch.empty(torch.Size(), dtype=tensor.dtype, device=\"cpu\").numpy().dtype)\n\n\ndef _check_batch_dims_are_sensible(output_dims, nonoutput_ordinal):\n", "fix_pattern": "Condition: The code is using numpy to get the dtype of a torch tensor.\nPattern: The code is obtaining the dtype of a torch tensor using np.finfo(torch.empty(torch.Size(), dtype=tensor.dtype).numpy().dtype).\nCode one: np.finfo(torch.empty(torch.Size(), dtype=tensor.dtype).numpy().dtype)\nCode two: np.finfo(torch.empty(torch.Size(), dtype=tensor.dtype, device=\"cpu\").numpy().dtype)\nFix pattern: In the condition of using numpy to get the dtype of a torch tensor, if the pattern np.finfo(torch.empty(torch.Size(), dtype=tensor.dtype).numpy().dtype) is detected, then change the code np.finfo(torch.empty(torch.Size(), dtype=tensor.dtype).numpy().dtype) to np.finfo(torch.empty(torch.Size(), dtype=tensor.dtype, device=\"cpu\").numpy().dtype) to fix the API misuse."}
{"number": 742, "change": "def stats(policy, train_batch):\n\"cur_lr\": tf.cast(policy.cur_lr, tf.float64),\n\"policy_loss\": policy.loss.pi_loss,\n\"entropy\": policy.loss.entropy,\n-        \"var_gnorm\": tf.global_norm(policy.model.trainable_variables()),\n+        \"var_gnorm\": tf.linalg.global_norm(policy.model.trainable_variables()),\n\"vf_loss\": policy.loss.vf_loss,\n\"vf_explained_var\": explained_variance(\ntf.reshape(policy.loss.value_targets, [-1]),\n", "fix_pattern": "<condition>: The condition is not explicitly mentioned in the context section. \n<pattern>: The pattern is the usage of \"tf.global_norm\" API.\n<code_one>: The code that needs to be removed is \"tf.global_norm(policy.model.trainable_variables())\".\n<code_two>: The code that needs to be added is \"tf.linalg.global_norm(policy.model.trainable_variables())\".\nFix_pattern: In the condition of no clear condition, if the usage of \"tf.global_norm\" API is detected, then remove the code \"tf.global_norm(policy.model.trainable_variables())\" and add the code \"tf.linalg.global_norm(policy.model.trainable_variables())\" to fix the API misuse."}
{"number": 743, "change": "def test_tensorrt_torch(\nres_orig = tuple(model(*inputs_example))\nassert all(\n[\n-                    torch.allclose(res_tensor, res_orig_tensor, rtol=1e-01)\n+                    torch.allclose(\n+                        res_tensor.float(), res_orig_tensor, rtol=1e-01\n+                    )\nfor (res_tensor, res_orig_tensor) in zip(res, res_orig)\n]\n)\n", "fix_pattern": "<condition>: No clear condition can be identified.\n<pattern>: The code is checking if all elements in two tensors are close within a given tolerance.\n<code_one>: torch.allclose(res_tensor, res_orig_tensor, rtol=1e-01)\n<code_two>: torch.allclose(res_tensor.float(), res_orig_tensor, rtol=1e-01)\nFix_pattern: In the condition of no clear condition, if the code checking the closeness of all elements in two tensors is detected, then change torch.allclose(res_tensor, res_orig_tensor, rtol=1e-01) to torch.allclose(res_tensor.float(), res_orig_tensor, rtol=1e-01) to fix the API misuse."}
{"number": 744, "change": "class Attention(nn.Module):\n# Apply the attention mask\nw = w + attention_mask\n\n-        w = nn.Softmax(dim=-1)(w)\n+        w = nn.functional.softmax(w, dim=-1)\nw = self.attn_dropout(w)\n\n# Mask heads if we want to\n", "fix_pattern": "Condition: There is a need to modify the softmax operation in the code.\nPattern: The code is using nn.Softmax(dim=-1) to apply the softmax operation.\nCode one: nn.Softmax(dim=-1)\nCode two: nn.functional.softmax(w, dim=-1)\nFix pattern: In the condition of needing to modify the softmax operation, if nn.Softmax(dim=-1) is detected, then change nn.Softmax(dim=-1) to nn.functional.softmax(w, dim=-1) to fix the API misuse."}
{"number": 752, "change": "class DistributedGroupSampler(Sampler):\nif size > 0:\nindice = np.where(self.flag == i)[0]\nassert len(indice) == size\n-                indice = indice[list(torch.randperm(int(size),\n-                                                    generator=g))].tolist()\n+                # add .numpy() to avoid bug when selecting indice in parrots.\n+                # TODO: check whether torch.randperm() can be replaced by\n+                # numpy.random.permutation().\n+                indice = indice[list(\n+                    torch.randperm(int(size), generator=g).numpy())].tolist()\nextra = int(\nmath.ceil(\nsize * 1.0 / self.samples_per_gpu / self.num_replicas)\n", "fix_pattern": "<condition>: The condition is \"if size > 0\" in the class method.\n<pattern>: The pattern is the code line \"indice = indice[list(torch.randperm(int(size), generator=g))].tolist()\" being removed.\n<code_one>: The removed code is \"indice = indice[list(torch.randperm(int(size), generator=g))].tolist()\".\n<code_two>: The added code is \"indice = indice[list(torch.randperm(int(size), generator=g).numpy())].tolist()\".\nFix_pattern: In the condition of \"if size > 0\", if the code line \"indice = indice[list(torch.randperm(int(size), generator=g))].tolist()\" is detected, then remove it and replace it with \"indice = indice[list(torch.randperm(int(size), generator=g).numpy())].tolist()\" to fix the API misuse."}
{"number": 759, "change": "def test_pair_norm(scale_individually):\nassert out1.size() == (100, 16)\n\nout2 = norm(torch.cat([x, x], dim=0), torch.cat([batch, batch + 1], dim=0))\n-    assert torch.allclose(out1, out2[:100])\n-    assert torch.allclose(out1, out2[100:])\n+    assert torch.allclose(out1, out2[:100], atol=1e-6)\n+    assert torch.allclose(out1, out2[100:], atol=1e-6)\n", "fix_pattern": "<condition>: The condition is not explicitly mentioned in the provided context.\n<pattern>: The pattern is to add an additional parameter \"atol=1e-6\" to the assert statements.\n<code_one>: The code that was removed is \"assert torch.allclose(out1, out2[:100])\" and \"assert torch.allclose(out1, out2[100:])\".\n<code_two>: The code that was added is \"assert torch.allclose(out1, out2[:100], atol=1e-6)\" and \"assert torch.allclose(out1, out2[100:], atol=1e-6)\".\nFix_pattern: In the condition of API misuse, if the pattern of using `torch.allclose()` without specifying the tolerance is detected, then add the \"atol=1e-6\" parameter to fix the issue."}
{"number": 764, "change": "class BlenderbotSmallEncoderLayer(nn.Module):\nhidden_states = residual + hidden_states\nhidden_states = self.final_layer_norm(hidden_states)\n\n-        if torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any():\n+        if hidden_states.dtype == torch.float16 and (\n+            torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any()\n+        ):\nclamp_value = torch.finfo(hidden_states.dtype).max - 1000\nhidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n", "fix_pattern": "<condition>: There is a condition checking for infinity or NaN values in the hidden_states tensor.\n<pattern>: The pattern is to check for the presence of infinity or NaN values.\n<code_one>: The code checking for infinity or NaN values is removed.\n<code_two>: The code checking for infinity or NaN values is added back, but with an additional condition to check if the dtype is torch.float16.\nFix_pattern: In the condition of checking for infinity or NaN values, if the dtype is torch.float16, then add the removed code back to fix the API misuse."}
{"number": 774, "change": "class QuantLinear(nn.Module):\nx_int = x / prev_act_scaling_factor\n\nreturn (\n-            F.linear(x_int, weight=self.weight_integer, bias=self.bias_integer) * bias_scaling_factor,\n+            nn.functional.linear(x_int, weight=self.weight_integer, bias=self.bias_integer) * bias_scaling_factor,\nbias_scaling_factor,\n)\n", "fix_pattern": "<condition>: The code is using the deprecated function `F.linear` for linear transformation.\n<pattern>: The code is using `F.linear` to perform linear transformation on the input tensor `x_int`.\n<code_one>: `F.linear(x_int, weight=self.weight_integer, bias=self.bias_integer) * bias_scaling_factor`\n<code_two>: `nn.functional.linear(x_int, weight=self.weight_integer, bias=self.bias_integer) * bias_scaling_factor`\nFix_pattern: In the condition of using `F.linear` for linear transformation, if the code pattern of `F.linear(x_int, weight=self.weight_integer, bias=self.bias_integer) * bias_scaling_factor` is detected, then change it to `nn.functional.linear(x_int, weight=self.weight_integer, bias=self.bias_integer) * bias_scaling_factor` to fix the API misuse."}
{"number": 779, "change": "class Importance(TracePosterior):\n\"\"\"\nif self.log_weights:\nlog_w_norm = self.get_normalized_weights(log_scale=True)\n-            ess = torch.exp(-logsumexp(2*log_w_norm, 0))\n+            ess = torch.exp(-torch.logsumexp(2*log_w_norm, 0))\nelse:\nwarnings.warn(\"The log_weights list is empty, effective sample size is zero.\")\ness = 0\n", "fix_pattern": "<condition>: The condition is that the \"self.log_weights\" attribute should be true.\n<pattern>: The pattern is that the calculation of \"ess\" using \"torch.exp(-logsumexp(2*log_w_norm, 0))\" needs to be changed.\n<code_one>: The code being removed is \"ess = torch.exp(-logsumexp(2*log_w_norm, 0))\".\n<code_two>: The code being added is \"ess = torch.exp(-torch.logsumexp(2*log_w_norm, 0))\".\nFix_pattern: In the condition of \"self.log_weights\" being true, if the pattern of \"ess = torch.exp(-logsumexp(2*log_w_norm, 0))\" is detected, then change it to \"ess = torch.exp(-torch.logsumexp(2*log_w_norm, 0))\" to fix the API misuse."}
{"number": 782, "change": "def dnn(tensor_in, hidden_units, activation=tf.nn.relu, keep_prob=None):\nwith tf.variable_scope('dnn'):\nfor i, n_units in enumerate(hidden_units):\nwith tf.variable_scope('layer%d' % i):\n-                tensor_in = linear.linear(tensor_in, n_units, True)\n+                tensor_in = linear(tensor_in, n_units, True)\ntensor_in = activation(tensor_in)\nif keep_prob:\ntensor_in = tf.nn.dropout(tensor_in, keep_prob)\n", "fix_pattern": "<condition>: The condition is that there is a need to apply an activation function to the input tensor before linear transformation.\n<pattern>: The pattern is that the linear transformation is being performed using the \"linear.linear\" function.\n<code_one>: The code being removed is \"linear.linear(tensor_in, n_units, True)\".\n<code_two>: The code being added is \"linear(tensor_in, n_units, True)\".\nFix_pattern: In the condition of applying an activation function to the input tensor, if the linear transformation is being performed using the \"linear.linear\" function, then remove the code \"linear.linear(tensor_in, n_units, True)\" and replace it with \"linear(tensor_in, n_units, True)\" to fix the API misuse."}
{"number": 783, "change": "class DSClipEncoder(torch.nn.Module):\nseq_len,\nseq_len,\ndtype=dtype,\n-                           device=torch.cuda.current_device())\n+                           device=get_accelerator().current_device_name())\nmask.fill_(torch.tensor(torch.finfo(dtype).min))\nmask.triu_(1)\nmask = mask.unsqueeze(1)\n", "fix_pattern": "<condition>: The condition is unclear in this context.\n\n<pattern>: The pattern is replacing the code that sets the device with a different method.\n\n<code_one>: The code being removed sets the device using 'torch.cuda.current_device()'.\n\n<code_two>: The code being added sets the device using 'get_accelerator().current_device_name()'.\n\nFix_pattern: In this fix pattern, if the code 'torch.cuda.current_device()' is detected in the device setting, it should be replaced with 'get_accelerator().current_device_name()' to fix the API misuse."}
{"number": 789, "change": "class AttentionDecoder(DecoderBase):\n])\nelse:\nattention_context = output.attention_context\n-    return tf.concat(1, [next_input, attention_context])\n+    return tf.concat_v2([next_input, attention_context], 1)\n\ndef _pad_att_scores(self, scores):\n\"\"\"Pads attention scores to fixed length. This is a hack because raw_rnn\n", "fix_pattern": "Condition: There is a function called `_pad_att_scores()` that pads attention scores to a fixed length.\nPattern: A specific line of code that concatenates `next_input` and `attention_context` using `tf.concat()`.\nCode_one: The original line of code that uses `tf.concat(1, [next_input, attention_context])`.\nCode_two: The updated line of code that uses `tf.concat_v2([next_input, attention_context], 1)`.\nFix_pattern: In the condition of `_pad_att_scores()` function, if the specific line of code using `tf.concat()` is detected, then change the code to use `tf.concat_v2()` to fix the API misuse."}
{"number": 791, "change": "def test_graph_saint():\nassert sample.node_norm.numel() == sample.num_nodes\nassert sample.edge_norm.numel() == sample.num_edges\n\n+    torch.manual_seed(12345)\nloader = GraphSAINTRandomWalkSampler(data, batch_size=2, walk_length=1,\n-                                         num_steps=4, log=False)\n+                                         num_steps=4, sample_coverage=10,\n+                                         log=False)\n\nfor sample in loader:\nassert len(sample) == 4\n", "fix_pattern": "<condition>: The fix pattern does not have a clear condition.\n<pattern>: The pattern that is detected is the absence of a specific code line.\n<code_one>: The code line that is removed is \"num_steps=4, log=False\".\n<code_two>: The code line that is added is \"torch.manual_seed(12345) num_steps=4, sample_coverage=10, log=False\".\nFix_pattern: In the condition of no specific condition, if the code line \"num_steps=4, log=False\" is detected, then remove it and add the code line \"torch.manual_seed(12345) num_steps=4, sample_coverage=10, log=False\" to fix the API misuse."}
{"number": 792, "change": "class GPTJAttention(nn.Module):\n):\n# compute causal mask from causal mask buffer\nquery_length, key_length = query.size(-2), key.size(-2)\n-        causal_mask = self.bias[:, :, key_length - query_length : key_length, :key_length].to(torch.bool)\n+        causal_mask = self.bias[:, :, key_length - query_length : key_length, :key_length]\n\n# Keep the attention weights computation in fp32 to avoid overflow issues\nquery = query.to(torch.float32)\n", "fix_pattern": "<condition>: The condition is that the attention weights computation needs to be kept in fp32 to avoid overflow issues.\n<pattern>: The pattern is the removal of the \".to(torch.bool)\" method from the \"causal_mask\" variable.\n<code_one>: The code removed is \"causal_mask = self.bias[:, :, key_length - query_length : key_length, :key_length].to(torch.bool)\".\n<code_two>: The code added is \"causal_mask = self.bias[:, :, key_length - query_length : key_length, :key_length]\".\nFix_pattern: In the condition of keeping the attention weights computation in fp32, if the pattern of removing the \".to(torch.bool)\" method from the \"causal_mask\" variable is detected, then the code \"causal_mask = self.bias[:, :, key_length - query_length : key_length, :key_length].to(torch.bool)\" should be changed to \"causal_mask = self.bias[:, :, key_length - query_length : key_length, :key_length]\" to fix the API misuse."}
{"number": 800, "change": "class GeneralizedChannelPermute(ConditionedGeneralizedChannelPermute, TransformM\nself.__delattr__('permutation')\n\n# Sample a random orthogonal matrix\n-        W, _ = torch.qr(torch.randn(channels, channels))\n+        W, _ = torch.linalg.qr(torch.randn(channels, channels))\n\n# Construct the partially pivoted LU-form and the pivots\nLU, pivots = W.lu()\n", "fix_pattern": "Condition: The code is using the torch.qr() function to sample a random orthogonal matrix.\nPattern: The code is replaced with torch.linalg.qr() function.\nCode One: torch.qr(torch.randn(channels, channels))\nCode Two: torch.linalg.qr(torch.randn(channels, channels))\nFix Pattern: In the condition where the code is using torch.qr() to sample a random orthogonal matrix, it is replaced with torch.linalg.qr() to fix the API misuse."}
{"number": 803, "change": "def model(x, is_train, reuse):\n# nt = Conv2d(nin, 16, (3, 3), (2, 2), act=tf.nn.relu, padding='SAME', name='tc1')\n# nt = Conv2d(nt, 8, (3, 3), (2, 2), act=tf.nn.relu, padding='SAME', name='tc2')\n## 2. Spatial transformer module (sampler)\n-        n = tl.layers.SpatialTransformer2dAffineLayer(nin, theta_layer=nt, out_size=[40, 40], name='spatial')\n+        n = tl.layers.SpatialTransformer2dAffineLayer(nin, theta_layer=nt, out_size=(40, 40), name='spatial')\ns = n\n## 3. Classifier\nn = tl.layers.Conv2d(\n", "fix_pattern": "<condition>: It is necessary to fix the API misuse in the code.\n<pattern>: When using the tl.layers.SpatialTransformer2dAffineLayer, the out_size argument should be passed as a tuple, not a list.\n<code_one>: `out_size=[40, 40]`\n<code_two>: `out_size=(40, 40)`\nFix_pattern: In the condition of API misuse, if the out_size argument is detected as a list, then change `out_size=[40, 40]` to `out_size=(40, 40)` to fix the API misuse."}
{"number": 804, "change": "class DeiTPreTrainedModel(PreTrainedModel):\ndef _init_weights(self, module: Union[nn.Linear, nn.Conv2d, nn.LayerNorm]) -> None:\n\"\"\"Initialize the weights\"\"\"\nif isinstance(module, (nn.Linear, nn.Conv2d)):\n-            # Slightly different from the TF version which uses truncated_normal for initialization\n-            # cf https://github.com/pytorch/pytorch/pull/5617\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.data = nn.init.trunc_normal_(module.weight.data, mean=0.0, std=self.config.initializer_range)\nif module.bias is not None:\nmodule.bias.data.zero_()\nelif isinstance(module, nn.LayerNorm):\n", "fix_pattern": "<condition>: The condition is that the module is an instance of nn.Linear, nn.Conv2d, or nn.LayerNorm.\n<pattern>: The pattern is that the weight data of the module is initialized using module.weight.data.normal_() in the removed code.\n<code_one>: The code to be removed is module.weight.data.normal_(mean=0.0, std=self.config.initializer_range).\n<code_two>: The code to be added is module.weight.data = nn.init.trunc_normal_(module.weight.data, mean=0.0, std=self.config.initializer_range).\nFix_pattern: In the condition of the module being an instance of nn.Linear, nn.Conv2d, or nn.LayerNorm, if the pattern of initializing weight data using module.weight.data.normal_() is detected, then the code to be removed is module.weight.data.normal_(mean=0.0, std=self.config.initializer_range) and it should be replaced with module.weight.data = nn.init.trunc_normal_(module.weight.data, mean=0.0, std=self.config.initializer_range) to fix the API misuse."}
{"number": 808, "change": "class DisentangledSelfAttention(nn.Module):\ndim=-1,\nindex=p2c_pos.squeeze(0).expand([query_layer.size(0), key_layer.size(-2), key_layer.size(-2)]),\n).transpose(-1, -2)\n-            score += p2c_att / scale\n+            score += p2c_att / torch.tensor(scale, dtype=p2c_att.dtype)\n\nreturn score\n", "fix_pattern": "Condition: The code is inside a method or function of a class called DisentangledSelfAttention.\nPattern: There is a calculation involving the variable \"score\" that includes the variable \"p2c_att\" divided by \"scale\".\nCode_one: \"score += p2c_att / scale\"\nCode_two: \"score += p2c_att / torch.tensor(scale, dtype=p2c_att.dtype)\"\nFix_pattern: In the condition of DisentangledSelfAttention class, if the calculation \"score += p2c_att / scale\" is detected, then change the code to \"score += p2c_att / torch.tensor(scale, dtype=p2c_att.dtype)\" to fix the API misuse."}
{"number": 812, "change": "def evaluate(model, data_loader, device):\nimage = list(img.to(device) for img in image)\ntargets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n-        torch.cuda.synchronize(device)\n+        # \u5f53\u4f7f\u7528CPU\u65f6\uff0c\u8df3\u8fc7GPU\u76f8\u5173\u6307\u4ee4\n+        if device != torch.device(\"cpu\"):\n+            torch.cuda.synchronize(device)\n+\nmodel_time = time.time()\noutputs = model(image)\n", "fix_pattern": "<condition>: When evaluating a model on a data loader using a specific device.\n<pattern>: Removing a synchronizing command for GPU.\n<code_one>: torch.cuda.synchronize(device)\n<code_two>: if device != torch.device(\"cpu\"): torch.cuda.synchronize(device)\nFix_pattern: In the condition of evaluating a model on a data loader using a specific device, if a synchronizing command for GPU is detected, then remove the command and add a check to skip GPU-related instructions when using the CPU to fix the API misuse."}
{"number": 816, "change": "def dnn(tensor_in, hidden_units, activation=tf.nn.relu, keep_prob=None):\nfor i, n_units in enumerate(hidden_units):\nwith tf.variable_scope('layer%d' % i):\ntensor_in = linear(tensor_in, n_units, True)\n-            tensor_in = activation(tensor_in)\n-            if keep_prob:\n-                tensor_in = tf.nn.dropout(tensor_in, keep_prob)\n+                tensor_in = activation(tensor_in)\n+                if keep_prob:\n+                    tensor_in = skflow.ops.dropout(tensor_in, keep_prob)\nreturn tensor_in\n", "fix_pattern": "<condition>: No clear condition is needed.\n<pattern>: Activation function and dropout are being applied to the input tensor.\n<code_one>: tensor_in = activation(tensor_in)\n<code_two>: tensor_in = skflow.ops.dropout(tensor_in, keep_prob)\nFix_pattern: In the condition of applying the activation function and dropout to the input tensor, the code \"tensor_in = activation(tensor_in)\" is replaced with \"tensor_in = skflow.ops.dropout(tensor_in, keep_prob)\" to fix the API misuse."}
{"number": 828, "change": "def main():\nif utils.is_primary(args):\n_logger.info('Using NVIDIA APEX AMP. Training in mixed precision.')\nelif use_amp == 'native':\n-        amp_autocast = partial(torch.autocast, device_type=device.type, dtype=amp_dtype)\n-        if device.type == 'cuda':\n+        try:\n+            amp_autocast = partial(torch.autocast, device_type=device.type, dtype=amp_dtype)\n+        except (AttributeError, TypeError):\n+            # fallback to CUDA only AMP for PyTorch < 1.10\n+            assert device.type == 'cuda'\n+            amp_autocast = torch.cuda.amp.autocast\n+        if device.type == 'cuda' and amp_dtype == torch.float16:\n+            # loss scaler only used for float16 (half) dtype, bfloat16 does not need it\nloss_scaler = NativeScaler()\nif utils.is_primary(args):\n_logger.info('Using native Torch AMP. Training in mixed precision.')\n", "fix_pattern": "<condition>: The condition is when `utils.is_primary(args)` evaluates to true.\n\n<pattern>: The pattern is the presence of the code block `amp_autocast = partial(torch.autocast, device_type=device.type, dtype=amp_dtype) if device.type == 'cuda'`.\n\n<code_one>: The code being removed is the line `amp_autocast = partial(torch.autocast, device_type=device.type, dtype=amp_dtype)`.\n\n<code_two>: The code being added is the try-except block that handles the fallback case and the additional condition `if device.type == 'cuda' and amp_dtype == torch.float16`.\n\nFix_pattern: In the condition of `utils.is_primary(args)`, if the pattern of `amp_autocast = partial(torch.autocast, device_type=device.type, dtype=amp_dtype) if device.type == 'cuda'` is detected, then remove the line `amp_autocast = partial(torch.autocast, device_type=device.type, dtype=amp_dtype)` and replace it with the try-except block that handles the fallback case and the additional condition `if device.type == 'cuda' and amp_dtype == torch.float16` to fix the API misuse."}
{"number": 837, "change": "class Encoder(torch.nn.Module):\npos_enc_class(attention_dim, positional_dropout_rate),\n)\nelif input_layer is None:\n-            self.embed = pos_enc_class(attention_dim, positional_dropout_rate)\n+            self.embed = torch.nn.Sequential(\n+                pos_enc_class(attention_dim, positional_dropout_rate)\n+            )\nelse:\nraise ValueError(\"unknown input_layer: \" + input_layer)\nself.normalize_before = normalize_before\n", "fix_pattern": "<condition>: The condition is when the input layer is not None.\n<pattern>: The pattern is the incorrect initialization of the \"self.embed\" attribute.\n<code_one>: The code that was removed is \"self.embed = pos_enc_class(attention_dim, positional_dropout_rate)\".\n<code_two>: The code that was added is \"self.embed = torch.nn.Sequential(pos_enc_class(attention_dim, positional_dropout_rate))\".\nFix_pattern: In the condition of the input layer not being None, if the incorrect initialization of \"self.embed\" is detected, then change the \"self.embed\" assignment to use a torch.nn.Sequential with the correct initialization."}
{"number": 838, "change": "class ChineseCLIPVisionTransformer(nn.Module):\nembed_dim = config.hidden_size\n\nself.embeddings = ChineseCLIPVisionEmbeddings(config)\n-        self.pre_layrnorm = nn.LayerNorm(embed_dim)\n+        self.pre_layrnorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\nself.encoder = ChineseCLIPVisionEncoder(config)\n-        self.post_layernorm = nn.LayerNorm(embed_dim)\n+        self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n\n@add_start_docstrings_to_model_forward(CHINESE_CLIP_VISION_INPUTS_DOCSTRING)\n@replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=ChineseCLIPVisionConfig)\n", "fix_pattern": "Condition: The condition is not clearly stated in the given context.\n\nPattern: The pattern is to change the initialization of the `nn.LayerNorm` modules by adding the `eps` parameter with the value `config.layer_norm_eps`.\n\nCode One: `self.pre_layrnorm = nn.LayerNorm(embed_dim)`, `self.post_layernorm = nn.LayerNorm(embed_dim)`\n\nCode Two: `self.pre_layrnorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)`, `self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)`\n\nFix Pattern: In the condition of the given context, if the pattern of initializing `nn.LayerNorm` modules without specifying `eps` is detected, then the code should be changed by adding the `eps` parameter with the value `config.layer_norm_eps` to fix the API misuse."}
{"number": 840, "change": "class TestSolveCast:\n\nclass TestSolveWithMask:\ndef test_smoke(self, device, dtype):\n+        torch.manual_seed(0)  # issue kornia#2027\nA = torch.randn(2, 3, 1, 4, 4, device=device, dtype=dtype)\nB = torch.randn(2, 3, 1, 4, 6, device=device, dtype=dtype)\n", "fix_pattern": "<condition>: No pre-condition is needed.\n<pattern>: N/A\n<code_one>: N/A\n<code_two>: torch.manual_seed(0)  # issue kornia#2027\nFix_pattern: In the condition of no pre-condition, if the code torch.manual_seed(0)  # issue kornia#2027 is added, it fixes the API misuse."}
{"number": 847, "change": "def _add_gradients_summaries(grads_and_vars):\ngrad_values = grad.values\nelse:\ngrad_values = grad\n-      summaries.append(tf.histogram_summary(var.op.name + ':gradient',\n+      summaries.append(tf.summary.histogram(var.op.name + ':gradient',\ngrad_values))\n-      summaries.append(tf.histogram_summary(var.op.name + ':gradient_norm',\n+      summaries.append(tf.summary.histogram(var.op.name + ':gradient_norm',\ntf.global_norm([grad_values])))\nelse:\ntf.logging.info('Var %s has no gradient', var.op.name)\n", "fix_pattern": "<condition>: If the condition is not satisfied, which is determined by the 'else' statement.\n<pattern>: The pattern detected is the use of 'tf.histogram_summary' in the code removed section.\n<code_one>: The code that needs to be changed is 'tf.histogram_summary'.\n<code_two>: The code to fix the API misuse is 'tf.summary.histogram'.\nFix_pattern: In the condition of an 'else' statement, if 'tf.histogram_summary' is detected, then the code needs to be changed from 'tf.histogram_summary' to 'tf.summary.histogram' to fix the API misuse."}
{"number": 849, "change": "class QM9(InMemoryDataset):\nedge_type += 2 * [self.bonds[bond.GetBondType()]]\n\nedge_index = torch.tensor([row, col], dtype=torch.long)\n-            edge_type = torch.tensor(edge_type)\n-            edge_attr = F.one_hot(torch.tensor(edge_type),\n+            edge_type = torch.tensor(edge_type, dtype=torch.long)\n+            edge_attr = F.one_hot(edge_type,\nnum_classes=len(self.bonds)).to(torch.float)\n\nperm = (edge_index[0] * N + edge_index[1]).argsort()\n", "fix_pattern": "<condition>: The condition in this code fix is when the variable 'edge_type' is being assigned a tensor value.\n<pattern>: The pattern is that the 'edge_type' tensor is being created but the data type is not specified.\n<code_one>: The code being removed is the conversion of 'edge_type' to a tensor without specifying the data type.\n<code_two>: The code being added is the specification of the data type of 'edge_type' as 'torch.long' when creating the tensor.\nFix_pattern: In the condition of assigning a tensor value to 'edge_type', if the pattern of not specifying the data type is detected, then the code removing the conversion to a tensor is changed to add the specification of 'torch.long' data type to fix the API misuse."}
{"number": 854, "change": "class MinSaver(Callback):\nnewname = os.path.join(logger.LOG_DIR,\nself.filename or\n('max-' + self.monitor_stat if self.reverse else 'min-' + self.monitor_stat))\n-        files_to_copy = glob.glob(path + '*')\n+        files_to_copy = tf.gfile.Glob(path + '*')\nfor file_to_copy in files_to_copy:\n-            shutil.copy(file_to_copy, file_to_copy.replace(path, newname))\n+            tf.gfile.Copy(file_to_copy, file_to_copy.replace(path, newname), overwrite=True)\nlogger.info(\"Model with {} '{}' saved.\".format(\n'maximum' if self.reverse else 'minimum', self.monitor_stat))\n", "fix_pattern": "<condition>: The condition is not specified in the context.\n\n<pattern>: The pattern is detecting the use of the \"glob\" module for file copying.\n\n<code_one>: The code that is removed is using \"shutil.copy\" for file copying.\n\n<code_two>: The code that is added is using \"tf.gfile.Copy\" for file copying, with the \"overwrite\" argument set to True.\n\nFix_pattern: In this fix, when copying files, the \"shutil.copy\" method is replaced with \"tf.gfile.Copy\" to use the correct module for file copying."}
{"number": 855, "change": "class Evaluator(object):\nThe mean average result per tensor over the entire dataset.\n\n\"\"\"\n+        tflearn.is_training(False, self.session)\ncoord = tf.train.Coordinator()\ninputs = tf.get_collection(tf.GraphKeys.INPUTS)\n# Data Preprocessing\n", "fix_pattern": "<condition>: No pre condition is needed.\n<pattern>: No specific pattern is detected.\n<code_one>: No code is removed.\n<code_two>: \"tflearn.is_training(False, self.session)\"\nFix_pattern: In the condition of no specific pattern, add \"tflearn.is_training(False, self.session)\" to fix the API misuse."}
{"number": 857, "change": "class Lamb(Optimizer):\nglobal_grad_norm.add_(grad.pow(2).sum())\n\nglobal_grad_norm = torch.sqrt(global_grad_norm)\n-        max_grad_norm = self.defaults['max_grad_norm']\n+        # FIXME it'd be nice to remove explicit tensor conversion of scalars when torch.where promotes\n+        # scalar types properly https://github.com/pytorch/pytorch/issues/9190\n+        max_grad_norm = torch.tensor(self.defaults['max_grad_norm'], device=device)\nclip_global_grad_norm = torch.where(\nglobal_grad_norm > max_grad_norm,\nglobal_grad_norm / max_grad_norm,\n", "fix_pattern": "<condition>: The condition is that the \"max_grad_norm\" variable is needed in the code.\n<pattern>: The pattern is that the \"max_grad_norm\" variable is missing and needs to be added.\n<code_one>: The code that was removed is \"max_grad_norm = self.defaults['max_grad_norm']\".\n<code_two>: The code that was added is \"max_grad_norm = torch.tensor(self.defaults['max_grad_norm'], device=device)\".\nFix_pattern: In the condition of needing the \"max_grad_norm\" variable, if it is missing, then add the code to initialize it using \"torch.tensor\"."}
{"number": 860, "change": "class ModelCatalogTest(unittest.TestCase):\ndef testCustomModel(self):\nray.init()\nModelCatalog.register_custom_model(\"foo\", CustomModel)\n-        p1 = ModelCatalog.get_model(1, 5, {\"custom_model\": \"foo\"})\n+        p1 = ModelCatalog.get_model(\n+            tf.constant([1, 2, 3]), 5, {\"custom_model\": \"foo\"})\nself.assertEqual(str(type(p1)), str(CustomModel))\n", "fix_pattern": "<condition>: The condition is that a custom model needs to be registered in the ModelCatalog. \n\n<pattern>: The pattern to be detected is the use of the deprecated `ModelCatalog.get_model()` API.\n\n<code_one>: The code being removed is `p1 = ModelCatalog.get_model(1, 5, {\"custom_model\": \"foo\"})`.\n\n<code_two>: The code being added is `p1 = ModelCatalog.get_model(tf.constant([1, 2, 3]), 5, {\"custom_model\": \"foo\"})`.\n\nFix_pattern: In the condition of a registered custom model, if the deprecated `ModelCatalog.get_model()` API is detected, then change the old API call to `ModelCatalog.get_model(tf.constant([1, 2, 3]), 5, {\"custom_model\": \"foo\"})` to fix the API misuse."}
{"number": 862, "change": "class _netD(nn.Module):\n\ndef forward(self, input):\ngpu_ids = None\n-        if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > 1:\n+        if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > =1:\ngpu_ids = range(self.ngpu)\noutput = nn.parallel.data_parallel(self.main, input, gpu_ids)\nreturn output.view(-1, 1)\n", "fix_pattern": "Condition: The condition is when the input data is an instance of torch.cuda.FloatTensor and self.ngpu is greater than 1.\n\nPattern: The pattern being detected is the use of \"and\" in the condition.\n\nCode One: The code being removed is the condition that checks if the input data is an instance of torch.cuda.FloatTensor and self.ngpu is greater than 1.\n\nCode Two: The code being added is the condition that checks if the input data is an instance of torch.cuda.FloatTensor and self.ngpu is greater than or equal to 1.\n\nFix Pattern: In the condition of checking if the input data is an instance of torch.cuda.FloatTensor and self.ngpu is greater than 1, the code one is being removed and code two is being added to fix the API misuse."}
{"number": 865, "change": "from tests import utils\ndef test_image_classifier(tmp_path):\ntrain_x = utils.generate_data(num_instances=320, shape=(32, 32, 3))\ntrain_y = utils.generate_one_hot_labels(num_instances=320, num_classes=10)\n-    clf = ak.ImageClassifier(directory=tmp_path, max_trials=2, seed=utils.SEED)\n+    clf = ak.ImageClassifier(\n+        directory=tmp_path,\n+        max_trials=2,\n+        seed=utils.SEED,\n+        distribution_strategy=tf.distribute.MirroredStrategy(),\n+    )\nclf.fit(train_x, train_y, epochs=1, validation_split=0.2)\nkeras_model = clf.export_model()\nclf.evaluate(train_x, train_y)\n", "fix_pattern": "<condition>: The condition is not explicitly stated in the context.\n\n<pattern>: The pattern is the misuse of the API when initializing the ImageClassifier.\n\n<code_one>: The code that was removed is the initialization of the ImageClassifier with only the 'directory', 'max_trials', and 'seed' arguments.\n\n<code_two>: The code that was added includes the additional argument 'distribution_strategy=tf.distribute.MirroredStrategy()'.\n\nFix_pattern: In the condition of this code section, if the API misuse pattern of initializing the ImageClassifier without the 'distribution_strategy' argument is detected, then the code is changed by adding the 'distribution_strategy=tf.distribute.MirroredStrategy()' argument to fix the issue."}
{"number": 867, "change": "def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\noutput_shape[3],\noutput_shape[1])\nif output_shape[0] is None:\n-        output_shape = (tf.shape(x)[0],) + tuple(output_shape[1:])\n-        output_shape = tf.stack(list(output_shape))\n+        output_shape = (shape(x)[0],) + tuple(output_shape[1:])\n+\n+    output_shape = tf.stack(list(output_shape))\n\npadding = _preprocess_padding(padding)\nif tf_data_format == 'NHWC':\n", "fix_pattern": "<condition>: The condition is \"if output_shape[0] is None\".\n<pattern>: The pattern is to update the output_shape by replacing tf.shape(x)[0] with shape(x)[0].\n<code_one>: The code that was removed is \"output_shape = (tf.shape(x)[0],) + tuple(output_shape[1:])\\noutput_shape = tf.stack(list(output_shape))\".\n<code_two>: The code that was added is \"output_shape = (shape(x)[0],) + tuple(output_shape[1:])\\n\\noutput_shape = tf.stack(list(output_shape))\".\nFix_pattern: In the condition of \"if output_shape[0] is None\", if the pattern of using tf.shape(x)[0] is detected, then replace it with shape(x)[0] to fix the API misuse."}
{"number": 868, "change": "class TestRasterizeMeshes(TestCaseMixin, unittest.TestCase):\n], dtype=torch.int64, device=device)\n# fmt: on\n\n-        pix_to_face_padded = -torch.ones_like(pix_to_face_frontface)\n+        pix_to_face_padded = -(torch.ones_like(pix_to_face_frontface))\n# Run with and without culling\n# Without culling, for k=0, the front face (i.e. face 2) is\n# rasterized and for k=1, the back face (i.e. face 3) is\n", "fix_pattern": "<condition>: No clear condition is needed.\n<pattern>: `pix_to_face_padded` is set to a negative value.\n<code_one>: `pix_to_face_padded = -torch.ones_like(pix_to_face_frontface)`\n<code_two>: `pix_to_face_padded = -(torch.ones_like(pix_to_face_frontface))`\nFix_pattern: In the code, if `pix_to_face_padded` is detected to be set to a negative value, then change the line `pix_to_face_padded = -torch.ones_like(pix_to_face_frontface)` to `pix_to_face_padded = -(torch.ones_like(pix_to_face_frontface))` to fix the API misuse."}
{"number": 875, "change": "class SparkKerasTests(tf.test.TestCase):\n\ndef test_fit_model_multiclass(self):\nmodel = create_mnist_model()\n-        if version.parse(tf.keras.__version__) < version.parse(\"2.11\"):\n+        if version.parse(tf.keras.__version__.replace(\"-tf\", \"+tf\")) < version.parse(\"2.11\"):\noptimizer = tf.keras.optimizers.Adadelta(1.0)\nelse:\noptimizer = tf.keras.optimizers.legacy.Adadelta(1.0)\n", "fix_pattern": "Condition: The fix pattern is applied when the version of TensorFlow Keras is less than 2.11.\nPattern: The pattern detected is checking if the TensorFlow Keras version is less than 2.11 using version.parse.\nCode_one: The code being removed is the condition \"if version.parse(tf.keras.__version__) < version.parse(\"2.11\"):\".\nCode_two: The code being added is the condition \"if version.parse(tf.keras.__version__.replace(\"-tf\", \"+tf\")) < version.parse(\"2.11\"):\".\nFix_pattern: In the condition of checking the TensorFlow Keras version, if it is detected to be less than 2.11, then change the code condition to replace \"-tf\" with \"+tf\" in the TensorFlow Keras version to fix the API misuse."}
{"number": 878, "change": "def train(args):\ndtype = torch.float32\nmodel = model_class(args.n_vocab, args).to(dtype=dtype)\nif args.ngpu > 0:\n-        model.to(\"cuda:0\")\n+        model.to(\"cuda\")\ngpu_id = list(range(args.ngpu))\nelse:\ngpu_id = [-1]\n", "fix_pattern": "<condition>: The condition is when the argument \"args.ngpu\" is greater than zero.\n<pattern>: The pattern detected is calling the \"model.to\" function with the argument \"cuda:0\".\n<code_one>: The code removed is \"model.to(\"cuda:0\")\".\n<code_two>: The code added is \"model.to(\"cuda\")\".\nFix_pattern: In the condition of \"args.ngpu\" being greater than zero, if the pattern of calling \"model.to\" with the argument \"cuda:0\" is detected, then remove the code \"model.to(\"cuda:0\")\" and add the code \"model.to(\"cuda\")\" to fix the API misuse."}
{"number": 883, "change": "def triangular_solve(x, y, upper=False, transpose=False):\n\n\ndef precision_to_scale_tril(P):\n-    Lf = torch.cholesky(torch.flip(P, (-2, -1)))\n+    Lf = torch.linalg.cholesky(torch.flip(P, (-2, -1)))\nL_inv = torch.transpose(torch.flip(Lf, (-2, -1)), -2, -1)\nL = torch.triangular_solve(torch.eye(P.shape[-1], dtype=P.dtype, device=P.device),\nL_inv, upper=False)[0]\n", "fix_pattern": "<condition>: No clear condition is needed.\n<pattern>: In the code, torch.cholesky is replaced with torch.linalg.cholesky.\n<code_one>: Lf = torch.cholesky(torch.flip(P, (-2, -1)))\n<code_two>: Lf = torch.linalg.cholesky(torch.flip(P, (-2, -1)))\nFix_pattern: In the condition where torch.cholesky is used, it is replaced with torch.linalg.cholesky to fix the API misuse."}
{"number": 884, "change": "class NaturalGradient(Optimizer):\napplied = self.apply_step(variables=variables, diffs=estimated_diffs)\n\nwith tf.control_dependencies(control_inputs=(applied,)):\n-                return [tf.identity(input=estimated_diff) for estimated_diff in estimated_diffs]\n+                return [estimated_diff + 0.0 for estimated_diff in estimated_diffs]\n\ndef false_fn():\nreturn [tf.zeros_like(tensor=diff) for diff in diffs]\n", "fix_pattern": "<condition>: The condition is that the optimizer is used with a control dependency. \n<pattern>: The pattern detected is returning a list of tensors after applying the control dependency.\n<code_one>: The code that is removed is the use of tf.identity() on the estimated_diff.\n<code_two>: The code that is added is the addition of 0.0 to each estimated_diff in the list.\nFix_pattern: In the condition of using an optimizer with a control dependency, if returning a list of tensors using tf.identity() is detected, then remove tf.identity() and add 0.0 to each element in the list to fix the API misuse."}
{"number": 886, "change": "def argsort(\nret = tf.argsort(\ntf.convert_to_tensor(x), axis=axis, direction=\"ASCENDING\", stable=stable\n)\n-    return ret\n+    return tf.cast(ret, dtype=tf.int64)\n\n\ndef sort(\n", "fix_pattern": "Condition: The code is calling the function tf.argsort() with the arguments axis, direction, and stable.\nPattern: The return value of the function is not being cast to the correct data type.\nCode One: \"return ret\"\nCode Two: \"return tf.cast(ret, dtype=tf.int64)\"\nFix Pattern: In the condition of calling tf.argsort() with the specified arguments, change the code \"return ret\" to \"return tf.cast(ret, dtype=tf.int64)\" to fix the API misuse."}
{"number": 887, "change": "def _to_ivy(x: Any) -> Any:\n\n\ndef _to_ivy_array(x: Any) -> ivy.Array:\n-    if isinstance(x, (torch.Tensor, tf.Tensor, jnp.numpy.DeviceArray, numpy.ndarray)):\n+    if isinstance(x, (torch.Tensor, tf.Tensor, jnp.DeviceArray, numpy.ndarray)):\nreturn ivy.array(numpy.array(x))\nreturn x\n", "fix_pattern": "<condition>: The code needs to check if the variable 'x' belongs to certain data types.\n<pattern>: The code checks if 'x' is an instance of any of the specified data types.\n<code_one>: The code uses 'jnp.numpy.DeviceArray' as one of the data types in the condition.\n<code_two>: The code replaces 'jnp.numpy.DeviceArray' with 'jnp.DeviceArray'.\nFix_pattern: In the condition of checking if 'x' belongs to certain data types, if 'jnp.numpy.DeviceArray' is detected, then change it to 'jnp.DeviceArray' to fix the API misuse."}
{"number": 888, "change": "def vecdot(\nout: Optional[Union[tf.Tensor, tf.Variable]] = None,\n) -> Union[tf.Tensor, tf.Variable]:\ndtype = ivy.as_native_dtype(ivy.promote_types(x1.dtype, x2.dtype))\n-    x1, x2 = tf.cast(x1, tf.float32), tf.cast(x2, tf.float32)\n+    if dtype != \"float64\":\n+        x1, x2 = tf.cast(x1, tf.float32), tf.cast(x2, tf.float32)\nret = tf.cast(tf.tensordot(x1, x2, axes=(axis, axis)), dtype)\nreturn ret\n", "fix_pattern": "<condition>: There is no clear condition identified in the context section.\n<pattern>: The pattern is to check if the dtype is not equal to \"float64\".\n<code_one>: The code that needs to be removed is \"x1, x2 = tf.cast(x1, tf.float32), tf.cast(x2, tf.float32)\".\n<code_two>: The code that needs to be added is \"if dtype != \"float64\": x1, x2 = tf.cast(x1, tf.float32), tf.cast(x2, tf.float32)\".\nFix_pattern: In the condition of no clear condition, if the dtype is not equal to \"float64\", then remove the code \"x1, x2 = tf.cast(x1, tf.float32), tf.cast(x2, tf.float32)\" and add the code \"if dtype != \"float64\": x1, x2 = tf.cast(x1, tf.float32), tf.cast(x2, tf.float32)\" to fix the API misuse."}
{"number": 902, "change": "class PipelineEngine(DeepSpeedEngine):\nmem_cached = new_cached\nmem_alloced = new_alloced\n\n-        max_alloced = torch.cuda.max_memory_allocated()\n-        max_cached = torch.cuda.max_memory_cached()\n+        max_alloced = get_accelerator().max_memory_allocated()\n+        max_cached = get_accelerator().max_memory_cached()\n\n# convert to GB for printing\nnew_alloced /= 1024**3\n", "fix_pattern": "Condition: The condition is not explicitly mentioned in the given context.\nPattern: The pattern is the replacement of torch.cuda with get_accelerator() for accessing memory metrics.\nCode one: torch.cuda.max_memory_allocated()\nCode two: get_accelerator().max_memory_allocated()\nFix pattern: In the condition where memory metrics are accessed using torch.cuda, replace it with get_accelerator() to fix the API misuse."}
{"number": 903, "change": "def torch_multinomial(input, num_samples, replacement=False):\nDoes not support keyword argument `out`.\n\"\"\"\nif input.is_cuda:\n-        return torch_multinomial(input.cpu(), num_samples, replacement).cuda()\n+        return torch.multinomial(input.cpu(), num_samples, replacement).cuda(input.get_device())\nelse:\nreturn torch.multinomial(input, num_samples, replacement)\n", "fix_pattern": "Condition: The condition is whether the input is on the CUDA device.\nPattern: The pattern is that the input needs to be moved to the CPU device before applying the `torch.multinomial` function.\nCode_one: The code that was removed is `input.cpu()`.\nCode_two: The code that was added is `input.get_device()`.\nFix_pattern: In the condition of checking if the input is on the CUDA device, if the pattern of moving the input to the CPU device is detected, then remove `input.cpu()` and add `input.get_device()` to fix the API misuse."}
{"number": 904, "change": "def test_delete_entire_dataset(domain_owner, cleanup_storage):\nassert domain_owner.datasets[0].name == \"Dataset_1\"\nassert domain_owner.datasets[1].name == \"Dataset_2\"\n\n-    domain_owner.datasets.delete(dataset_id=domain_owner.datasets[0].id)\n+    domain_owner.datasets.delete(\n+        dataset_id=domain_owner.datasets[0].id, skip_checks=True\n+    )\n\n# Check if the number of available datasets has been decreased\nassert len(domain_owner.datasets) == 1\n", "fix_pattern": "Condition: There is no specific condition identified in the context section.\nPattern: The pattern is to add the argument \"skip_checks=True\" when calling the \"domain_owner.datasets.delete()\" function.\nCode One: \"domain_owner.datasets.delete(dataset_id=domain_owner.datasets[0].id)\"\nCode Two: \"domain_owner.datasets.delete(dataset_id=domain_owner.datasets[0].id, skip_checks=True)\"\nFix Pattern: In the condition where no specific condition is needed, if the pattern of calling \"domain_owner.datasets.delete()\" without \"skip_checks=True\" is detected, then change the code to include \"skip_checks=True\" to fix the API misuse."}
{"number": 908, "change": "def pack(\ntry:\nimport torch\n\n-        meta_objs.update(torch=torch.__version__)\n+        meta_objs.update(torch=str(torch.__version__))\nexcept ImportError:\npass\ntry:\n", "fix_pattern": "<condition>: ImportError\n<pattern>: `torch` version information is being added to `meta_objs` by updating the value corresponding to the key 'torch' with `torch.__version__`.\n<code_one>: `meta_objs.update(torch=torch.__version__)`\n<code_two>: `meta_objs.update(torch=str(torch.__version__))`\nFix_pattern: In the condition of an ImportError, if the pattern of updating the value of `meta_objs` key 'torch' with `torch.__version__` is detected, then change `torch=torch.__version__` to `torch=str(torch.__version__)` to fix the API misuse."}
{"number": 910, "change": "class Schedule(metaclass=ABCMeta):\nraise NotImplementedError\n\ndef value(self, t):\n-        if self.framework == \"tf\" and tf.executing_eagerly() is False:\n+        if self.framework == \"tf\":\nreturn tf.cast(\n-                tf.py_func(self._value, [t], tf.float64),\n+                tf.py_function(self._value, [t], tf.float64),\ntf.float32,\n-                name=\"schedule-value\")\n+                name=\"schedule_value\")\nreturn self._value(t)\n\ndef __call__(self, t):\n", "fix_pattern": "<condition>: The condition is that the framework variable should be \"tf\".\n<pattern>: The pattern is the use of the py_func method to call the _value function.\n<code_one>: The code that was removed is \"tf.py_func(self._value, [t], tf.float64), name=\"schedule-value\")\".\n<code_two>: The code that was added is \"tf.py_function(self._value, [t], tf.float64), name=\"schedule_value\")\".\nFix_pattern: In the condition of framework variable being \"tf\", if the py_func method is used to call the _value function, then remove the code \"tf.py_func(self._value, [t], tf.float64), name=\"schedule-value\")\" and add the code \"tf.py_function(self._value, [t], tf.float64), name=\"schedule_value\")\" to fix the API misuse."}
{"number": 912, "change": "class FeedForwardTransformer(TTSInterface, torch.nn.Module):\n\n# concat speaker embedding\nif self.spk_embed_dim is not None:\n-            spembs = torch.nn.functional.normalize(spembs).unsqueeze(1).expand(-1, hs.size(1), -1)\n-            hs = self.projection(torch.cat([hs, spembs], dim=-1))\n+            spembs_ = F.normalize(spembs).unsqueeze(1).expand(-1, hs.size(1), -1)\n+            hs = self.projection(torch.cat([hs, spembs_], dim=-1))\n\n# forward duration predictor and length regulator\nd_masks = make_pad_mask(ilens).to(xs.device)\n", "fix_pattern": "<condition>: If the `spk_embed_dim` is not `None`.\n<pattern>: Remove the code that normalizes and expands `spembs` and concatenates it with `hs`.\n<code_one>: `spembs = torch.nn.functional.normalize(spembs).unsqueeze(1).expand(-1, hs.size(1), -1) hs = self.projection(torch.cat([hs, spembs], dim=-1))`\n<code_two>: `spembs_ = F.normalize(spembs).unsqueeze(1).expand(-1, hs.size(1), -1) hs = self.projection(torch.cat([hs, spembs_], dim=-1))`\nFix_pattern: In the condition where `spk_embed_dim` is not `None`, replace the code that normalizes and expands `spembs` with the corrected version."}
{"number": 919, "change": "def image_histogram2d(\nhist = hist.squeeze()\nelif image.dim() == 3:\nhist = hist.squeeze(0)\n-    return hist, torch.zeros_like(hist, dtype=hist.dtype, device=device)\n+    return hist, torch.zeros_like(hist, dtype=hist.dtype, device=hist.device)\n", "fix_pattern": "<condition>: The condition is that the image dimension should be equal to 3.\n<pattern>: The pattern is that the \"squeeze()\" function is being used on the \"hist\" variable.\n<code_one>: The \"squeeze()\" function is being called without any arguments.\n<code_two>: The \"squeeze(0)\" function is being called instead.\nFix_pattern: In the condition of the image dimension being equal to 3, if the \"squeeze()\" function is detected, then change the \"squeeze()\" function to \"squeeze(0)\" to fix the API misuse."}
{"number": 920, "change": "class FeedForwardEncoder(Seq2SeqEncoder):\nreturn self._feedforward(inputs)\nelse:\noutputs = self._feedforward(inputs)\n-            return outputs * mask.unsqueeze(dim=-1).float()\n+            return outputs * mask.unsqueeze(dim=-1)\n", "fix_pattern": "<condition>: The condition is that there is an \"if\" statement in the code.\n<pattern>: The pattern is the multiplication of \"outputs\" with \"mask.unsqueeze(dim=-1).float()\".\n<code_one>: The code that needs to be removed is \" * mask.unsqueeze(dim=-1).float()\".\n<code_two>: The code that needs to be added is \" * mask.unsqueeze(dim=-1)\".\nFix_pattern: In the condition of \"if\" statement, if the pattern of multiplying \"outputs\" with \"mask.unsqueeze(dim=-1).float()\" is detected, then remove \" * mask.unsqueeze(dim=-1).float()\" to fix the API misuse."}
{"number": 929, "change": "def rand_like_with_shape(shape, ori_t):\nhigher_bound = torch.max(ori_t)\n\nif dtype in [torch.uint8, torch.int16, torch.short, torch.int16, torch.long, torch.bool]:\n-        return torch.randint(lower_bound, higher_bound+1, shape, dtype=dtype, device=device)\n+        return torch.randint(lower_bound.long(), higher_bound.long() + 1, shape, dtype=dtype, device=device)\nelse:\nreturn torch.rand(shape, dtype=dtype, device=device, requires_grad=require_grad)\n", "fix_pattern": "<condition>: The condition is that the variable \"dtype\" is checked to see if it is in a list of specific data types.\n<pattern>: The pattern is that a specific line of code is removed and replaced with a modified version.\n<code_one>: The code that is removed is \"return torch.randint(lower_bound, higher_bound+1, shape, dtype=dtype, device=device)\".\n<code_two>: The code that is added is \"return torch.randint(lower_bound.long(), higher_bound.long() + 1, shape, dtype=dtype, device=device)\".\nFix_pattern: In the condition of checking \"dtype\", if the specific line of code is detected, then remove the code and replace it with the modified version to fix the API misuse."}
{"number": 930, "change": "def map_data_vector_model(subsample_size):\npyro.sample(\"x\", dist.normal, mu[batch], sigma[batch])\nreturn batch\n\n-    ind = Variable(torch.LongTensor(range(20)))\n+    LongTensor = torch.cuda.LongTensor if torch.Tensor.is_cuda else torch.LongTensor\n+    ind = Variable(LongTensor(range(20)))\nbatch = pyro.map_data('mapdata', ind, local_model, batch_size=subsample_size)\nreturn list(batch.data)\n", "fix_pattern": "<condition>: The code is using a Pyro API function called \"map_data\" to process a batch of data.\n<pattern>: The variable \"ind\" is being created using the \"Variable\" function with a tensor created from a range of numbers.\n<code_one>: ind = Variable(torch.LongTensor(range(20)))\n<code_two>: ind = Variable(LongTensor(range(20)))\nFix_pattern: In the condition of using the \"map_data\" function, if the variable \"ind\" is created using the \"Variable\" function with a tensor, it should be replaced with a tensor created using the \"LongTensor\" function to fix the API misuse."}
{"number": 933, "change": "class SelfAttnFunc(torch.autograd.Function):\nvalues_grads   = torch.bmm(dropout_results.transpose(1,2), output_lin_grads, out=values_grads.transpose(0,1))\n\n# Mask and Scaling for Dropout (not a publically documented op)\n-        dropout_grads = torch._masked_scale(matmul2_dgrad1, dropout_mask, dropout_prob_t[0])\n+        dropout_grads = torch._masked_scale(matmul2_dgrad1, dropout_mask, 1.0/(1.0-dropout_prob_t[0]))\n\n# Softmax Grad (not a publically documented op)\nsoftmax_grads = torch._softmax_backward_data(dropout_grads, softmax_results, -1, softmax_results)\n", "fix_pattern": "Condition: The condition is not clearly mentioned in the given context. No clear condition can be identified.\n\nPattern: The pattern is the detection of <dropout_grads = torch._masked_scale(matmul2_dgrad1, dropout_mask, dropout_prob_t[0])> in the code.\n\nCode One: <dropout_grads = torch._masked_scale(matmul2_dgrad1, dropout_mask, dropout_prob_t[0])>\n\nCode Two: <dropout_grads = torch._masked_scale(matmul2_dgrad1, dropout_mask, 1.0/(1.0-dropout_prob_t[0]))>\n\nFix Pattern: In the condition of no specific condition, if <dropout_grads = torch._masked_scale(matmul2_dgrad1, dropout_mask, dropout_prob_t[0])> is detected, then change <code_one> to <code_two> to fix the API misuse."}
{"number": 935, "change": "class AutoShape(nn.Module):\n#   multiple:        = [Image.open('image1.jpg'), Image.open('image2.jpg'), ...]  # list of images\n\nt = [time_sync()]\n-        p = next(self.model.parameters()) if self.pt else torch.zeros(1)  # for device and type\n+        p = next(self.model.parameters()) if self.pt else torch.zeros(1, device=self.model.device)  # for device, type\nautocast = self.amp and (p.device.type != 'cpu')  # Automatic Mixed Precision (AMP) inference\nif isinstance(imgs, torch.Tensor):  # torch\nwith amp.autocast(autocast):\n", "fix_pattern": "<condition>: The condition is checking if the variable \"imgs\" is an instance of the torch.Tensor class.\n<pattern>: The pattern is to remove the line of code that initializes a tensor \"p\" using torch.zeros(1) and add a new line of code that initializes it with torch.zeros(1, device=self.model.device).\n<code_one>: The code being removed is \"p = next(self.model.parameters()) if self.pt else torch.zeros(1)  # for device and type\".\n<code_two>: The code being added is \"p = next(self.model.parameters()) if self.pt else torch.zeros(1, device=self.model.device)  # for device, type\".\nFix_pattern: In the condition of checking if \"imgs\" is a torch.Tensor, if the code initializing \"p\" with torch.zeros(1) is detected, then it should be changed to initialize \"p\" with torch.zeros(1, device=self.model.device) to fix the API misuse."}
{"number": 936, "change": "class CycleDiffusionPipeline(DiffusionPipeline):\n\ndevice = torch.device(f\"cuda:{gpu_id}\")\n\n-        for cpu_offloaded_model in [self.unet, self.text_encoder, self.vae, self.safety_checker]:\n+        for cpu_offloaded_model in [self.unet, self.text_encoder, self.vae]:\nif cpu_offloaded_model is not None:\ncpu_offload(cpu_offloaded_model, device)\n\n+        if self.safety_checker is not None:\n+            # TODO(Patrick) - there is currently a bug with cpu offload of nn.Parameter in accelerate\n+            # fix by only offloading self.safety_checker for now\n+            cpu_offload(self.safety_checker.vision_model)\n+\n@property\n# Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline._execution_device\ndef _execution_device(self):\n", "fix_pattern": "<condition>: The condition is that the variable \"cpu_offloaded_model\" is not None.\n<pattern>: The pattern is that the \"cpu_offloaded_model\" is being iterated over in a for loop.\n<code_one>: The code \"self.safety_checker\" is being removed from the for loop iteration.\n<code_two>: The code \"if self.safety_checker is not None: cpu_offload(self.safety_checker.vision_model)\" is added after the for loop.\nFix_pattern: In the condition of \"cpu_offloaded_model\" not being None, if the iteration includes \"self.safety_checker\", then remove \"self.safety_checker\" from the iteration and add the code \"if self.safety_checker is not None: cpu_offload(self.safety_checker.vision_model)\" after the for loop."}
{"number": 937, "change": "def shift_tokens_right(input_ids: tf.Tensor, pad_token_id: int, decoder_start_to\n\nif tf.executing_eagerly():\n# \"Verify that `labels` has only positive values and -100\"\n-        assert_gte0 = tf.debugging.assert_greater_equal(shifted_input_ids, tf.constant(0))\n+        assert_gte0 = tf.debugging.assert_greater_equal(shifted_input_ids, tf.constant(0, dtype=input_ids.dtype))\n\n# Make sure the assertion op is called by wrapping the result in an identity no-op\nwith tf.control_dependencies([assert_gte0]):\n", "fix_pattern": "<condition>: `tf.executing_eagerly()` is true. \n<pattern>: `assert_gte0` is used to verify that `shifted_input_ids` is greater than or equal to 0. \n<code_one>: `assert_gte0 = tf.debugging.assert_greater_equal(shifted_input_ids, tf.constant(0))`\n<code_two>: `assert_gte0 = tf.debugging.assert_greater_equal(shifted_input_ids, tf.constant(0, dtype=input_ids.dtype))`\nFix_pattern: In the condition of `tf.executing_eagerly()` being true, if the pattern `assert_gte0` is detected, then change `<code_one>` to `<code_two>` to fix the API misuse."}
{"number": 940, "change": "def arange(start, stop=None, step=1, dtype=None, dev=None):\nif dtype in [torch.int8, torch.uint8, torch.int16]:\nreturn torch.arange(start, stop, step=step, dtype=torch.int64, device=dev).to(dtype)\nelse:\n-            return torch.range(start, stop, step=step, dtype=dtype, device=dev)\n+            return torch.arange(start, stop, step=step, dtype=dtype, device=dev)\n", "fix_pattern": "<condition>: The condition is checking if the dtype is one of the specified torch types.\n<pattern>: The pattern is a method call to `torch.range`.\n<code_one>: The code being removed is `torch.range(start, stop, step=step, dtype=dtype, device=dev)`.\n<code_two>: The code being added is `torch.arange(start, stop, step=step, dtype=dtype, device=dev)`.\nFix_pattern: In the condition of checking the dtype, if a call to `torch.range` is detected, then change it to `torch.arange` to fix the API misuse."}
{"number": 944, "change": "def _apply_affine(input: torch.Tensor,\n\nheight, width = x_data.shape[-2:]\ntransform: torch.Tensor = params['transform'].to(device, dtype)\n-\n-    out_data: torch.Tensor = warp_perspective(x_data, transform, (height, width))\n+    out_data: torch.Tensor = warp_affine(x_data, transform[:, :2, :], (height, width))\n\nif return_transform:\nreturn out_data.view_as(input), transform\n", "fix_pattern": "<condition>: The code is checking if the variable \"return_transform\" is true.\n<pattern>: When the condition is true, the code is using the \"warp_perspective\" function. \n<code_one>: The code is using \"warp_perspective\" function.\n<code_two>: The code is changed to use \"warp_affine\" function instead of \"warp_perspective\".\nFix_pattern: In the condition of \"return_transform\" being true, the code replaces the use of \"warp_perspective\" with \"warp_affine\" to fix the API misuse."}
{"number": 946, "change": "class GPTNeoAttentionMixin:\nelse:\nraise ValueError(f\"Input tensor rank should be one of [2, 3], but is: {len(tensor.shape)}\")\n\n-        padded_tensor = F.pad(tensor, padding_side, value=pad_value)\n+        padded_tensor = nn.functional.pad(tensor, padding_side, value=pad_value)\npadded_tensor = padded_tensor.unfold(dimension=1, size=window_size + block_length, step=block_length)\n\nif is_key_value:\n", "fix_pattern": "<condition>: The condition is that the input tensor rank should be one of [2, 3].\n<pattern>: The pattern detected is the use of the F.pad() function to pad the tensor.\n<code_one>: The code that was removed is \"padded_tensor = F.pad(tensor, padding_side, value=pad_value)\".\n<code_two>: The code that was added is \"padded_tensor = nn.functional.pad(tensor, padding_side, value=pad_value)\".\nFix_pattern: In the condition of the input tensor rank being one of [2, 3], if the use of the F.pad() function is detected, then change the code from \"padded_tensor = F.pad(tensor, padding_side, value=pad_value)\" to \"padded_tensor = nn.functional.pad(tensor, padding_side, value=pad_value)\" to fix the API misuse."}
{"number": 953, "change": "def test_tagged_corpus_downsample():\n\nassert 10 == len(corpus.train)\n\n-    corpus.downsample(percentage=0.3, only_downsample_train=True)\n+    corpus.downsample(percentage=0.3, downsample_dev=False, downsample_test=False)\n\nassert 3 == len(corpus.train)\n", "fix_pattern": "Condition: The condition is not clearly stated in the given context.\n\nPattern: The pattern is to change the arguments of the `corpus.downsample()` method from `only_downsample_train=True` to `downsample_dev=False, downsample_test=False`.\n\nCode one: `only_downsample_train=True`\n\nCode two: `downsample_dev=False, downsample_test=False`\n\nFix pattern: In the condition of <condition>, if <pattern> is detected, then change the <code_one> to <code_two> to fix the API misuse."}
{"number": 962, "change": "class DecisionTransformerGPT2Attention(nn.Module):\n# Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.\n# Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`\nmask_value = torch.full([], mask_value, dtype=attn_weights.dtype).to(attn_weights.device)\n-            attn_weights = torch.where(causal_mask, attn_weights, mask_value)\n+            attn_weights = torch.where(causal_mask, attn_weights.to(attn_weights.dtype), mask_value)\n\nif attention_mask is not None:\n# Apply the attention mask\n", "fix_pattern": "<condition>: The code is checking if the attention_mask is not None.\n<pattern>: The code is using torch.where() to apply an attention mask to attn_weights.\n<code_one>: The code is using attn_weights directly in torch.where().\n<code_two>: The code is adding a conversion of attn_weights to the same data type as attn_weights before using it in torch.where().\nFix_pattern: In the condition of checking if the attention_mask is not None, if the code is using attn_weights in torch.where(), then the fix is to add a conversion of attn_weights to the same data type as attn_weights before using it in torch.where()."}
{"number": 963, "change": "def load_tf_graph(graph_file):\n\"\"\"\n# We load the protobuf file from the disk and parse it to retrieve the\n# unserialized graph_def\n-    with tf.gfile.GFile(graph_file, \"rb\") as f:\n-        graph_def = tf.GraphDef()\n+    with tf.io.gfile.GFile(graph_file, \"rb\") as f:\n+        graph_def = tf.compat.v1.GraphDef()\ngraph_def.ParseFromString(f.read())\n\n# Then, we import the graph_def into a new Graph and returns it\n", "fix_pattern": "Condition: No clear condition is needed.\nPattern: The code uses `tf.gfile.GFile()` to read a graph file.\nCode One: `with tf.gfile.GFile(graph_file, \"rb\") as f:`\nCode Two: `with tf.io.gfile.GFile(graph_file, \"rb\") as f:`\nFix Pattern: In the condition of no clear condition, if the pattern of using `tf.gfile.GFile()` is detected, then change `tf.gfile.GFile()` to `tf.io.gfile.GFile()` to fix the API misuse."}
{"number": 967, "change": "class TestScalarMix(AllenNlpTestCase):\ntensors = [torch.randn([3, 4, 5]) for _ in range(3)]\nnumpy_mask = numpy.ones((3, 4), dtype=\"int32\")\nnumpy_mask[1, 2:] = 0\n-        mask = torch.from_numpy(numpy_mask)\n+        mask = torch.from_numpy(numpy_mask).bool()\n\nweights = [0.1, 0.2, 0.3]\nfor k in range(3):\n", "fix_pattern": "<condition>: \nIn the context of the TestScalarMix class.\n\n<pattern>: \nThere is a need to convert a numpy array (numpy_mask) to a torch tensor (mask).\n\n<code_one>: \nThe original code removed the conversion from numpy to torch using torch.from_numpy().\n\n<code_two>: \nThe fix involved adding the .bool() function after the torch.from_numpy() conversion to ensure the correct tensor type.\n\nFix_pattern: \nIn the condition of TestScalarMix, if there is a need to convert a numpy array to a torch tensor, then the code should include .bool() after the torch.from_numpy() conversion to fix the API misuse."}
{"number": 969, "change": "class SequenceFeatureExtractionTestMixin(FeatureExtractionSavingTestMixin):\ninput_np = feat_extract.pad(processed_features, padding=\"longest\", return_tensors=\"np\")[input_name]\ninput_tf = feat_extract.pad(processed_features, padding=\"longest\", return_tensors=\"tf\")[input_name]\n\n-        self.assertTrue(abs(input_np.sum() - input_tf.numpy().sum()) < 1e-2)\n+        self.assertTrue(abs(input_np.astype(np.float32).sum() - input_tf.numpy().sum()) < 1e-2)\n\ndef test_attention_mask(self):\nfeat_dict = self.feat_extract_dict\n", "fix_pattern": "Condition: In the test_attention_mask method.\nPattern: The sum of input_np and input_tf.numpy() should be within a certain tolerance.\nCode One: self.assertTrue(abs(input_np.sum() - input_tf.numpy().sum()) < 1e-2)\nCode Two: self.assertTrue(abs(input_np.astype(np.float32).sum() - input_tf.numpy().sum()) < 1e-2)\nFix Pattern: In the condition of test_attention_mask, if the pattern of the sum difference between input_np and input_tf.numpy() being within a tolerance is detected, then change the code to compare the sum difference after converting input_np to np.float32."}
{"number": 971, "change": "class ScoreSdeVeScheduler(SchedulerMixin, ConfigMixin):\n\"\"\"\nsampling_eps = sampling_eps if sampling_eps is not None else self.config.sampling_eps\n\n-        self.timesteps = torch.linspace(1, sampling_eps, num_inference_steps)\n+        self.timesteps = torch.linspace(1, sampling_eps, num_inference_steps, device=device)\n\ndef set_sigmas(\nself, num_inference_steps: int, sigma_min: float = None, sigma_max: float = None, sampling_eps: float = None\n", "fix_pattern": "<condition>: When `sampling_eps` is not None in the `set_sigmas` method.\n<pattern>: The code `self.timesteps = torch.linspace(1, sampling_eps, num_inference_steps)` is removed.\n<code_one>: `self.timesteps = torch.linspace(1, sampling_eps, num_inference_steps)`\n<code_two>: `self.timesteps = torch.linspace(1, sampling_eps, num_inference_steps, device=device)`\nFix_pattern: In the condition of `sampling_eps` being not None, if the pattern of `self.timesteps = torch.linspace(1, sampling_eps, num_inference_steps)` is detected, then change the `self.timesteps` to `torch.linspace(1, sampling_eps, num_inference_steps, device=device)` to fix the API misuse."}
{"number": 972, "change": "class SSIM(nn.Module):\nssim_map = ((2 * mu1_mu2 + self.C1) * (2 * sigma12 + self.C2)) / \\\n((mu1_sq + mu2_sq + self.C1) * (sigma1_sq + sigma2_sq + self.C2))\n\n-        loss = torch.clamp(1. - ssim_map, min=0, max=1) / 2.\n+        loss = torch.clamp(torch.tensor(1.) - ssim_map, min=0, max=1) / 2.\n\nif self.reduction == 'mean':\nloss = torch.mean(loss)\n", "fix_pattern": "<condition>: The condition is when the 'reduction' variable is set to 'mean'.\n<pattern>: The pattern is 'loss = torch.clamp(1. - ssim_map, min=0, max=1) / 2.'.\n<code_one>: The code that needs to be removed is 'loss = torch.clamp(1. - ssim_map, min=0, max=1) / 2.'.\n<code_two>: The code that needs to replace <code_one> is 'loss = torch.clamp(torch.tensor(1.) - ssim_map, min=0, max=1) / 2.'.\nFix_pattern: In the condition of 'reduction' being set to 'mean', if the pattern 'loss = torch.clamp(1. - ssim_map, min=0, max=1) / 2.' is detected, then replace it with 'loss = torch.clamp(torch.tensor(1.) - ssim_map, min=0, max=1) / 2.' to fix the API misuse."}
{"number": 974, "change": "def rpn_losses(anchor_labels, anchor_boxes, label_logits, box_logits):\nplaceholder = 1.\nlabel_loss = tf.nn.sigmoid_cross_entropy_with_logits(\nlabels=tf.to_float(valid_anchor_labels), logits=valid_label_logits)\n-    label_loss = label_loss * (1. / config.RPN_BATCH_PER_IM)\n+    label_loss = tf.reduce_sum(label_loss) * (1. / config.RPN_BATCH_PER_IM)\nlabel_loss = tf.where(tf.equal(nr_valid, 0), placeholder, label_loss, name='label_loss')\n\npos_anchor_boxes = tf.boolean_mask(anchor_boxes, pos_mask)\n", "fix_pattern": "<condition>: The condition is if the number of valid anchors is equal to 0.\n<pattern>: The pattern is multiplying the label_loss by 1 divided by a constant value.\n<code_one>: The code that was removed is \"label_loss = label_loss * (1. / config.RPN_BATCH_PER_IM)\".\n<code_two>: The code that was added is \"label_loss = tf.reduce_sum(label_loss) * (1. / config.RPN_BATCH_PER_IM)\".\nFix_pattern: In the condition of the number of valid anchors being equal to 0, if \"label_loss\" is detected, then remove \"label_loss = label_loss * (1. / config.RPN_BATCH_PER_IM)\" and add \"label_loss = tf.reduce_sum(label_loss) * (1. / config.RPN_BATCH_PER_IM)\" to fix the API misuse."}
{"number": 976, "change": "from allennlp.common.testing import AllenNlpTestCase\n\nclass TestElmoLstmCell(AllenNlpTestCase):\ndef test_elmo_lstm(self):\n-        input_tensor = Variable(torch.rand(4, 5, 3))\n+        input_tensor = torch.rand(4, 5, 3)\ninput_tensor[1, 4:, :] = 0.\ninput_tensor[2, 2:, :] = 0.\ninput_tensor[3, 1:, :] = 0.\n-        mask = Variable(torch.ones([4, 5]))\n+        mask = torch.ones([4, 5])\nmask[1, 4:] = 0.\nmask[2, 2:] = 0.\nmask[3, 1:] = 0.\n", "fix_pattern": "<condition>: No clear condition is needed.\n<pattern>: Variable is removed from the initialization of input_tensor and mask.\n<code_one>: input_tensor = Variable(torch.rand(4, 5, 3))\n               mask = Variable(torch.ones([4, 5]))\n<code_two>: input_tensor = torch.rand(4, 5, 3)\n               mask = torch.ones([4, 5])\nFix_pattern: In the API misuse fix, the code initializes input_tensor and mask using Variable() function, but it is changed to direct initialization using torch.rand()."}
{"number": 977, "change": "class TpuStrategyTest(tf.test.TestCase):\nserving_fn = create_serving_signature(model)\n\nsaved_model_dir = tempfile.mkdtemp(dir=self.get_temp_dir())\n-      tf.saved_model.save(\n-          model, saved_model_dir, signatures={\"serving_default\": serving_fn})\n+      model.save(saved_model_dir, save_format=\"tf\",\n+                 signatures={\"serving_default\": serving_fn})\n\n# Test the saved_model.\nloaded_serving_fn = tf.keras.models.load_model(\n", "fix_pattern": "<condition>: There is a need to fix an API misuse in the code.\n<pattern>: The code is using the \"tf.saved_model.save\" function to save the model with serving signatures.\n<code_one>: tf.saved_model.save(model, saved_model_dir, signatures={\"serving_default\": serving_fn})\n<code_two>: model.save(saved_model_dir, save_format=\"tf\", signatures={\"serving_default\": serving_fn})\nFix_pattern: In the condition of using \"tf.saved_model.save\" to save the model with serving signatures, the fix is to change the code from \"tf.saved_model.save\" to \"model.save\" with the appropriate parameters to achieve the desired result."}
{"number": 984, "change": "def _preprocess_conv3d_input(x, data_format):\n# Returns\nA tensor.\n\"\"\"\n-    if dtype(x) == 'float64':\n+    # tensorflow doesn't support float64 for conv layer before 1.8.0\n+    if (dtype(x) == 'float64'\n+            and StrictVersion(tf.__version__) < StrictVersion('1.8.0')):\nx = tf.cast(x, 'float32')\ntf_data_format = 'NDHWC'\nif data_format == 'channels_first':\n", "fix_pattern": "<condition>: The condition is when the data format is 'channels_first'.\n<pattern>: The pattern is if the input tensor has a dtype of 'float64'.\n<code_one>: The code that was removed is the check for the input tensor's dtype.\n<code_two>: The code that was added is a condition that checks for both the dtype and the version of TensorFlow.\nFix_pattern: In the condition of 'channels_first', if the input tensor has a dtype of 'float64', then remove the check for the dtype and add a new condition that checks for both the dtype and the version of TensorFlow to fix the API misuse."}
{"number": 988, "change": "def main(args):\naccelerator.clip_grad_norm_(params_to_clip, args.max_grad_norm)\noptimizer.step()\nlr_scheduler.step()\n-                optimizer.zero_grad()\n+                optimizer.zero_grad(set_to_none=args.set_grads_to_none)\n\n# Checks if the accelerator has performed an optimization step behind the scenes\nif accelerator.sync_gradients:\n", "fix_pattern": "<condition>: The condition is checking if the accelerator has performed an optimization step behind the scenes.\n<pattern>: The pattern is the removal of the line `optimizer.zero_grad()`.\n<code_one>: The code being removed is `optimizer.zero_grad()`.\n<code_two>: The code being added is `optimizer.zero_grad(set_to_none=args.set_grads_to_none)`.\nFix_pattern: In the condition of checking if the accelerator has performed an optimization step, if the removal of `optimizer.zero_grad()` is detected, then the code should be changed to `optimizer.zero_grad(set_to_none=args.set_grads_to_none)` to fix the API misuse."}
{"number": 989, "change": "class Tagger(nn.Module):\n# criterion\nself.crit = nn.CrossEntropyLoss(ignore_index=0) # ignore padding\n\n-        self.drop = Dropout(args['dropout'])\n+        self.drop = nn.Dropout(args['dropout'])\nself.worddrop = WordDropout(args['word_dropout'])\n\ndef forward(self, word, word_mask, wordchars, wordchars_mask, upos, xpos, ufeats, pretrained, word_orig_idx, sentlens, wordlens):\n", "fix_pattern": "Condition: The fix pattern is applied when there is a need to change the dropout implementation in the Tagger class.\nPattern: The code removed is a dropout layer created using the Dropout class.\nCode One: self.drop = Dropout(args['dropout'])\nCode Two: self.drop = nn.Dropout(args['dropout'])\nFix Pattern: In the condition of requiring a dropout layer change, the code_one is removed and replaced with code_two to fix the API misuse."}
{"number": 990, "change": "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\ndef train():\nmodel.train()\noptimizer.zero_grad()\n-    pos_z, neg_z, summary = model(data.x, data.edge_index, data.edge_attr)\n-    loss = model.loss(pos_z, neg_z, summary)\n+    y = model(data.x, data.edge_index, data.edge_attr)\n+    loss = torch.sum(y) #TODO: actual loss function\nloss.backward()\noptimizer.step()\nreturn loss.item()\n", "fix_pattern": "<condition>:\nThere is a need to fix an API misuse in the code.\n\n<pattern>:\nThe pattern is detecting the usage of model.loss() method.\n\n<code_one>:\nThe code that needs to be removed is \"model.loss(pos_z, neg_z, summary)\".\n\n<code_two>:\nThe code that needs to be added is \"y = model(data.x, data.edge_index, data.edge_attr)\" and \"loss = torch.sum(y)\".\n\nFix_pattern:\nIn the condition of API misuse, if the usage of model.loss() method is detected, then remove the code with model.loss() and replace it with \"y = model(data.x, data.edge_index, data.edge_attr)\" and \"loss = torch.sum(y)\"."}
{"number": 991, "change": "def build_linear_positions(index_dims, output_range=(-1.0, 1.0)):\nreturn torch.linspace(start=output_range[0], end=output_range[1], steps=n_xels_per_dim, dtype=torch.float32)\n\ndim_ranges = [_linspace(n_xels_per_dim) for n_xels_per_dim in index_dims]\n-    array_index_grid = torch.meshgrid(*dim_ranges)\n+    array_index_grid = meshgrid(*dim_ranges, indexing=\"ij\")\n\nreturn torch.stack(array_index_grid, dim=-1)\n", "fix_pattern": "Condition: There was no specific condition mentioned in the given context.\nPattern: The pattern was to change the function used for creating a meshgrid.\nCode_one: array_index_grid = torch.meshgrid(*dim_ranges)\nCode_two: array_index_grid = meshgrid(*dim_ranges, indexing=\"ij\")\nFix_pattern: In the condition of no specific condition, if the code using 'torch.meshgrid' is detected, then change it to 'meshgrid' with the additional parameter 'indexing=\"ij\"' to fix the API misuse."}
{"number": 1007, "change": "class GPT2Attention(nn.Module):\n# Apply the attention mask\nattn_weights = attn_weights + attention_mask\n\n-        attn_weights = nn.Softmax(dim=-1)(attn_weights)\n+        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n\n# Downcast (if necessary) back to V's dtype (if in mixed-precision) -- No-Op if otherwise\nif attn_weights.dtype != torch.float32:\n", "fix_pattern": "<condition>: The condition is that the variable \"attn_weights\" has a data type that is not torch.float32.\n<pattern>: The pattern is using nn.Softmax(dim=-1) to apply softmax to attn_weights.\n<code_one>: The code that is removed is \"attn_weights = nn.Softmax(dim=-1)(attn_weights)\".\n<code_two>: The code that is added is \"attn_weights = nn.functional.softmax(attn_weights, dim=-1)\".\nFix_pattern: In the condition of \"attn_weights\" having a data type that is not torch.float32, the pattern of using nn.Softmax(dim=-1) to apply softmax to \"attn_weights\" was detected and the code \"attn_weights = nn.Softmax(dim=-1)(attn_weights)\" is being replaced with \"attn_weights = nn.functional.softmax(attn_weights, dim=-1)\" to fix the API misuse."}
{"number": 1008, "change": "class BidirectionalEndpointSpanExtractor(SpanExtractor):\nsequence_lengths = util.get_lengths_from_binary_sequence_mask(sequence_mask)\nelse:\n# shape (batch_size), filled with the sequence length size of the sequence_tensor.\n-            sequence_lengths = util.ones_like(sequence_tensor[:, 0, 0]).long() * sequence_tensor.size(1)\n+            sequence_lengths = (torch.ones_like(sequence_tensor[:, 0, 0], dtype=torch.long) *\n+                                sequence_tensor.size(1))\n\n# shape (batch_size, num_spans, 1)\nend_sentinel_mask = (exclusive_span_ends == sequence_lengths.unsqueeze(-1)).long().unsqueeze(-1)\n", "fix_pattern": "<condition>: The code is checking for the existence of a certain condition. \n<pattern>: The pattern that is being detected is the incorrect usage of the \"sequence_lengths\" variable. \n<code_one>: The incorrect code is setting \"sequence_lengths\" to a specific value. \n<code_two>: The correct code is using torch.ones_like() to set \"sequence_lengths\". \nFix_pattern: In the condition of checking a specific condition, if the incorrect usage of \"sequence_lengths\" is detected, then the incorrect code of setting it to a specific value should be changed to using torch.ones_like() to fix the API misuse."}
{"number": 1016, "change": "class LanguageModel(nn.Module):\n\nfor i in range(number_of_characters):\n\n-                if torch.cuda.is_available():\n-                    input = input.cuda()\n+                input = input.to(flair.device)\n\n# get predicted weights\nprediction, _, hidden = self.forward(input, hidden)\n", "fix_pattern": "Condition: The condition is checking if CUDA is available.\n\nPattern: The pattern is using the condition to check if the input should be moved to the GPU.\n\nCode_one: The code being removed is checking if CUDA is available and then moving the input to the GPU.\n\nCode_two: The code being added is using the \"to\" method to move the input to the device specified by flair.\n\nFix_pattern: In the condition of checking if CUDA is available, if the condition is true, then remove the code that moves the input to the GPU and add the code that uses the \"to\" method to move the input to the device specified by flair to fix the API misuse."}
{"number": 1030, "change": "def Conv2DTranspose(\nif get_tf_version_tuple() <= (1, 12):\nkernel_initializer = tf.contrib.layers.variance_scaling_initializer(2.0),\nelse:\n-            kernel_initializer = tf.keras.initializers.VarianceScaling(2.0)\n+            kernel_initializer = tf.keras.initializers.VarianceScaling(2.0, distribution='untruncated_normal')\n\nwith rename_get_variable({'kernel': 'W', 'bias': 'b'}):\nlayer = tf.layers.Conv2DTranspose(\n", "fix_pattern": "<condition>: The condition is that the TensorFlow version should be less than or equal to (1, 12).\n<pattern>: The pattern is the misuse of the API, specifically the `kernel_initializer` argument.\n<code_one>: The code that was removed is `kernel_initializer = tf.keras.initializers.VarianceScaling(2.0)`.\n<code_two>: The code that was added is `kernel_initializer = tf.keras.initializers.VarianceScaling(2.0, distribution='untruncated_normal')`.\nFix_pattern: In the condition of the TensorFlow version being less than or equal to (1, 12), if the misuse of the `kernel_initializer` argument is detected, then the `kernel_initializer` code is changed from `kernel_initializer = tf.keras.initializers.VarianceScaling(2.0)` to `kernel_initializer = tf.keras.initializers.VarianceScaling(2.0, distribution='untruncated_normal')` in order to fix the API misuse."}
{"number": 1033, "change": "def make_batches(lines, args, task, max_positions, encode_fn):\n).long()\nfor src_str in lines\n]\n-    lengths = torch.LongTensor([t.numel() for t in tokens])\n+    lengths = [t.numel() for t in tokens]\nitr = task.get_batch_iterator(\ndataset=task.build_dataset_for_inference(tokens, lengths),\nmax_tokens=args.max_tokens,\n", "fix_pattern": "<condition>: When building a dataset for inference in a task.\n<pattern>: It was using a torch.LongTensor to calculate the lengths of tokens.\n<code_one>: lengths = torch.LongTensor([t.numel() for t in tokens])\n<code_two>: lengths = [t.numel() for t in tokens]\nFix_pattern: In the condition of building a dataset for inference, if the pattern of using torch.LongTensor to calculate token lengths is detected, then the fix is to remove torch.LongTensor and instead use a list comprehension to calculate lengths."}
{"number": 1034, "change": "class EarlyStopping(Callback):\n\nif trainer.use_tpu:\nstop = torch.tensor(int(trainer.should_stop), device=pl_module.device, dtype=torch.int32)\n-            stop = xm.mesh_reduce(\"stop_signal\", stop, torch.cat)\n+            stop = xm.mesh_reduce(\"stop_signal\", stop, sum)\ntorch_xla.core.xla_model.rendezvous(\"pl.EarlyStoppingCallback.stop_distributed_training_check\")\ntrainer.should_stop = int(stop.item()) == trainer.world_size\n", "fix_pattern": "<condition>: The condition is \"if trainer.use_tpu\".\n<pattern>: The pattern is \"stop = xm.mesh_reduce(\"stop_signal\", stop, torch.cat)\".\n<code_one>: The code being removed is \"stop = xm.mesh_reduce(\"stop_signal\", stop, torch.cat)\".\n<code_two>: The code being added is \"stop = xm.mesh_reduce(\"stop_signal\", stop, sum)\".\nFix_pattern: In the condition of \"if trainer.use_tpu\", if the pattern \"stop = xm.mesh_reduce(\"stop_signal\", stop, torch.cat)\" is detected, then remove the code \"stop = xm.mesh_reduce(\"stop_signal\", stop, torch.cat)\" and change it to \"stop = xm.mesh_reduce(\"stop_signal\", stop, sum)\" to fix the API misuse."}
{"number": 1035, "change": "class Entropy(Metric):\nmask : `torch.Tensor`, optional (default = None).\nA masking tensor of shape (batch_size, ...).\n\"\"\"\n-        logits, mask = self.unwrap_to_tensors(logits, mask)\n+        logits, mask = self.detach_tensors(logits, mask)\n\nif mask is None:\n-            mask = torch.ones(logits.size()[:-1])\n+            mask = torch.ones(logits.size()[:-1], device=logits.device)\n\nlog_probs = torch.nn.functional.log_softmax(logits, dim=-1)\nprobabilities = torch.exp(log_probs) * mask.unsqueeze(-1)\n", "fix_pattern": "<condition>: If the masking tensor \"mask\" is None.\n<pattern>: Set \"mask\" to a tensor of ones with the same shape as \"logits.size()[:-1]\".\n<code_one>: logits, mask = self.unwrap_to_tensors(logits, mask)\n            mask = torch.ones(logits.size()[:-1])\n<code_two>: logits, mask = self.detach_tensors(logits, mask)\n            mask = torch.ones(logits.size()[:-1], device=logits.device)\nFix_pattern: In the condition of \"mask is None\", if the pattern \"logits, mask = self.unwrap_to_tensors(logits, mask)\\nmask = torch.ones(logits.size()[:-1])\" is detected, then change the code to \"logits, mask = self.detach_tensors(logits, mask)\\nmask = torch.ones(logits.size()[:-1], device=logits.device)\" to fix the API misuse."}
{"number": 1044, "change": "def fmod(\nout: Optional[Union[tf.Tensor, tf.Variable]] = None,\n) -> Union[tf.Tensor, tf.Variable]:\nresult = tf.math.floormod(x1, x2, name=None)\n-    temp = (result, x1)\n-    return tf.map_fn(lambda x: x[0] if (x[0] * x[1] >= 0) else (-1 * x[0]), temp)\n+    temp = [result, x1]\n+    return tf.map_fn(lambda x: x[0] if (x[0] * x[1] >= 0) else (-1 * x[0]), temp, fn_output_signature=result.dtype)\n\n\ndef fmax(\n", "fix_pattern": "Condition: There is no clear condition identified in the context.\nPattern: The pattern is to change the data type of the input parameter from a tuple to a list.\nCode One: The code that was removed is: temp = (result, x1)\\nreturn tf.map_fn(lambda x: x[0] if (x[0] * x[1] >= 0) else (-1 * x[0]), temp)\nCode Two: The code that was added is: temp = [result, x1]\\nreturn tf.map_fn(lambda x: x[0] if (x[0] * x[1] >= 0) else (-1 * x[0]), temp, fn_output_signature=result.dtype)\nFix Pattern: In the condition where there is no clear condition, the pattern to fix the API misuse is to change the data type of the input parameter from a tuple to a list in order to correctly execute the map_fn function."}
{"number": 1046, "change": "class MT5DenseGatedActDense(nn.Module):\n# To make 8bit quantization work for google/flan-t5-xxl, self.wo is kept in float32.\n# See https://github.com/huggingface/transformers/issues/20287\n# we also make sure the weights are not in `int8` in case users will force `_keep_in_fp32_modules` to be `None``\n-        if hidden_states.dtype != self.wo.weight.dtype and self.wo.weight.dtype != torch.int8:\n+        if (\n+            isinstance(self.wo.weight, torch.Tensor)\n+            and hidden_states.dtype != self.wo.weight.dtype\n+            and self.wo.weight.dtype != torch.int8\n+        ):\nhidden_states = hidden_states.to(self.wo.weight.dtype)\n\nhidden_states = self.wo(hidden_states)\n", "fix_pattern": "Condition: The condition is when the data type of `hidden_states` is not equal to the data type of `self.wo.weight` and the data type of `self.wo.weight` is not equal to `torch.int8`.\nPattern: The pattern is to check the data type of `self.wo.weight` and perform an additional check to validate if it is an instance of `torch.Tensor`.\nCode One: The code being removed is the if condition checking for the data types.\nCode Two: The code being added is an updated if condition that checks for the data types and also checks if `self.wo.weight` is an instance of `torch.Tensor`.\nFix Pattern: In the condition of `hidden_states.dtype != self.wo.weight.dtype and self.wo.weight.dtype != torch.int8`, if the pattern is detected, then change the `code_one` to `code_two` to fix the API misuse."}
{"number": 1048, "change": "def vector_to_skew_symmetric_matrix(vector):\na2s = vector_expanded[..., 1:2, :]\na3s = vector_expanded[..., 2:3, :]\n# BS x 1 x 1\n-    zs = torch.zeros(batch_shape + [1, 1])\n+    zs = torch.zeros(batch_shape + [1, 1], device=vector.device)\n# BS x 1 x 3\nrow1 = torch.cat((zs, -a3s, a2s), -1)\nrow2 = torch.cat((a3s, zs, -a1s), -1)\n", "fix_pattern": "<condition>: The condition is not specified in the given code.\n<pattern>: The pattern detected is the addition of the \"device=vector.device\" argument to the torch.zeros() function.\n<code_one>: The code that was removed is \"zs = torch.zeros(batch_shape + [1, 1])\".\n<code_two>: The code that was added is \"zs = torch.zeros(batch_shape + [1, 1], device=vector.device)\".\nFix_pattern: In the condition of no specific condition needed, if the pattern of adding the \"device=vector.device\" argument is detected, then the code \"zs = torch.zeros(batch_shape + [1, 1])\" should be changed to \"zs = torch.zeros(batch_shape + [1, 1], device=vector.device)\" to fix the API misuse."}
{"number": 1050, "change": "class PointAssigner(BaseAssigner):\n\nif gt_labels is not None:\nassigned_labels = assigned_gt_inds.new_full((num_points, ), -1)\n-            pos_inds = torch.nonzero(assigned_gt_inds > 0).squeeze()\n+            pos_inds = torch.nonzero(\n+                assigned_gt_inds > 0, as_tuple=False).squeeze()\nif pos_inds.numel() > 0:\nassigned_labels[pos_inds] = gt_labels[\nassigned_gt_inds[pos_inds] - 1]\n", "fix_pattern": "<condition>: assigned_gt_inds is not None.\n<pattern>: pos_inds is created by applying a condition on assigned_gt_inds.\n<code_one>: pos_inds = torch.nonzero(assigned_gt_inds > 0).squeeze().\n<code_two>: pos_inds = torch.nonzero(assigned_gt_inds > 0, as_tuple=False).squeeze().\nFix_pattern: In the condition of assigned_gt_inds being not None, if pos_inds is detected, then change the code_one to code_two to fix the API misuse."}
{"number": 1052, "change": "def att_to_numpy(att_ws, att):\natt_ws = torch.stack([aw[:, -1] for aw in att_ws], dim=1).cpu().numpy()\nelif isinstance(att, (AttCov, AttCovLoc)):\n# att_ws => list of list of previous attentions\n-        att_ws = torch.stack([aw[-1] for aw in att_ws], dim=1).cpu().numpy()\n+        att_ws = torch.stack([aw[idx] for idx, aw in enumerate(att_ws)], dim=1).cpu().numpy()\nelif isinstance(att, AttLocRec):\n# att_ws => list of tuple of attention and hidden states\natt_ws = torch.stack([aw[0] for aw in att_ws], dim=1).cpu().numpy()\n", "fix_pattern": "<condition>: The condition is checking if the variable \"att\" is an instance of either \"AttCov\", \"AttCovLoc\", or \"AttLocRec\".\n<pattern>: The pattern detected is accessing the last element of each list within \"att_ws\".\n<code_one>: The code that was removed is \"aw[-1]\".\n<code_two>: The code that was added is \"aw[idx]\".\nFix_pattern: In the condition of checking the type of \"att\", if the pattern of accessing the last element of each list within \"att_ws\" is detected, then change \"aw[-1]\" to \"aw[idx]\" to fix the API misuse."}
{"number": 1059, "change": "class TFOPTDecoder(tf.keras.layers.Layer):\nif output_attentions:\nall_self_attns += (layer_self_attn,)\n\n+        if self.final_layer_norm is not None:\n+            hidden_states = self.final_layer_norm(hidden_states)\n+\nif self.project_out is not None:\nhidden_states = self.project_out(hidden_states)\n", "fix_pattern": "condition: If the final layer normalization is not None\npattern: Add the final layer normalization to the hidden states\ncode_one: None\ncode_two: self.final_layer_norm(hidden_states)\nFix_pattern: In the condition that the final layer normalization is not None, add the final layer normalization to the hidden states to fix the API misuse."}
{"number": 1065, "change": "class ParabolicEquationStepperTest(tf.test.TestCase, parameterized.TestCase):\nsecond_order_coeff_fn=second_order_coeff_fn,\ninner_first_order_coeff_fn=inner_first_order_coeff_fn)[0]\n\n-    true_values = tf.math.exp(final_t + grid[0])\n+    true_values = tf.expand_dims(tf.math.exp(final_t + grid[0]), axis=0)\nself.assertAllClose(\nest_values, true_values, atol=1e-2, rtol=1e-2)\n", "fix_pattern": "<condition>: There is no specific condition in the context section.\n<pattern>: The code `true_values = tf.math.exp(final_t + grid[0])` is detected in the code removed section.\n<code_one>: The code `true_values = tf.math.exp(final_t + grid[0])`.\n<code_two>: The code `true_values = tf.expand_dims(tf.math.exp(final_t + grid[0]), axis=0)`.\nFix_pattern: In the condition of no specific condition, if the code `true_values = tf.math.exp(final_t + grid[0])` is detected, then the code `true_values = tf.math.exp(final_t + grid[0])` should be changed to `true_values = tf.expand_dims(tf.math.exp(final_t + grid[0]), axis=0)` to fix the API misuse."}
{"number": 1066, "change": "class LocalMultiGPUOptimizer(PolicyOptimizer):\nelse:\nrnn_inputs = []\nself.par_opt = LocalSyncParallelOptimizer(\n-                        tf.train.AdamOptimizer(\n-                            self.sgd_stepsize), self.devices,\n+                        self.policy.optimizer(), self.devices,\n[v for _, v in self.policy.loss_inputs()], rnn_inputs,\nself.per_device_batch_size, self.policy.copy,\nos.getcwd())\n", "fix_pattern": "Condition: The condition is that the code is inside an \"else\" statement.\nPattern: The pattern is the use of tf.train.AdamOptimizer(self.sgd_stepsize) as the optimizer.\nCode one: tf.train.AdamOptimizer(self.sgd_stepsize), self.devices,\nCode two: self.policy.optimizer(), self.devices,\nFix pattern: In the condition of being inside an \"else\" statement, if the pattern of using tf.train.AdamOptimizer(self.sgd_stepsize) is detected, then remove the code tf.train.AdamOptimizer(self.sgd_stepsize), self.devices, and replace it with self.policy.optimizer(), self.devices, to fix the API misuse."}
{"number": 1067, "change": "class DistributedReplicatedBuilder(DataParallelBuilder):\nreturn grads\n\n# Ngpu * Nvar * 2\n-        grad_list = self.build_on_multi_tower(\n-            get_grads,\n+        grad_list = DataParallelBuilder.build_on_towers(\n+            self.towers, get_grads,\ndevices=self.raw_devices,\nuse_vs=[True] * len(self.towers))  # open vs at each tower\nDataParallelBuilder._check_grad_list(grad_list)\n", "fix_pattern": "<condition>: No clear condition is needed.\n<pattern>: In the code, the method \"build_on_multi_tower\" is being called.\n<code_one>: get_grads\n<code_two>: DataParallelBuilder.build_on_towers(self.towers, get_grads,\nFix_pattern: In the condition of calling \"build_on_multi_tower\" method, if \"get_grads\" is detected, then change it to \"DataParallelBuilder.build_on_towers(self.towers, get_grads,\" to fix the API misuse."}
{"number": 1069, "change": "class OPTForSequenceClassification(OPTPreTrainedModel):\nsequence_lengths = -1\nelse:\nif input_ids is not None:\n-                sequence_lengths = torch.ne(input_ids, self.config.pad_token_id).sum(-1) - 1\n+                sequence_lengths = (torch.ne(input_ids, self.config.pad_token_id).sum(-1) - 1).to(logits.device)\nelse:\nsequence_lengths = -1\nlogger.warning(\n", "fix_pattern": "<condition>: The condition is not clearly identified in the given code snippet.\n<pattern>: The pattern is to calculate the sequence lengths based on the input_ids and the pad_token_id.\n<code_one>: The code removed is \"sequence_lengths = torch.ne(input_ids, self.config.pad_token_id).sum(-1) - 1\".\n<code_two>: The code added is \"sequence_lengths = (torch.ne(input_ids, self.config.pad_token_id).sum(-1) - 1).to(logits.device)\".\nFix_pattern: In the condition where a certain condition is met, if the calculation of sequence lengths using input_ids and pad_token_id is incorrect, the fix is to change the code_one to code_two in order to fix the API misuse."}
{"number": 1072, "change": "def test_discrete_parallel(continuous_class):\n\ndef model(data):\nweights = pyro.sample('weights', dist.Dirichlet(0.5 * torch.ones(K)))\n-        locs = pyro.sample('locs', dist.Normal(0, 10).reshape([K], extra_event_dims=1))\n+        locs = pyro.sample('locs', dist.Normal(0, 10).expand_by([K]).independent(1))\nscale = pyro.sample('scale', dist.LogNormal(0, 1))\n\nwith pyro.iarange('data', len(data)):\n", "fix_pattern": "<condition>: No clear condition can be identified.\n<pattern>: The pattern detected is that the distribution object is being reshaped.\n<code_one>: The code being removed is `dist.Normal(0, 10).reshape([K], extra_event_dims=1)`.\n<code_two>: The code being added is `dist.Normal(0, 10).expand_by([K]).independent(1)`.\nFix_pattern: In the condition where a distribution object is being reshaped, the `reshape()` method is removed and replaced with `expand_by()` and `independent()` methods to fix the API misuse."}
{"number": 1075, "change": "class RenyiELBO(ELBO):\nsurrogate_elbo_particles = torch.stack(surrogate_elbo_particles)\n\nlog_weights = (1. - self.alpha) * elbo_particles\n-        log_mean_weight = logsumexp(log_weights, dim=0) - math.log(self.num_particles)\n+        log_mean_weight = torch.logsumexp(log_weights, dim=0) - math.log(self.num_particles)\nelbo = log_mean_weight.sum().item() / (1. - self.alpha)\n\n# collect parameters to train from model and guide\n", "fix_pattern": "<condition>: There is a variable named \"log_weights\" in the code.\n<pattern>: The variable \"logsumexp\" is used to calculate the logarithm of the weighted sum of the \"log_weights\".\n<code_one>: \"log_mean_weight = logsumexp(log_weights, dim=0) - math.log(self.num_particles)\"\n<code_two>: \"log_mean_weight = torch.logsumexp(log_weights, dim=0) - math.log(self.num_particles)\"\nFix_pattern: In the condition where the variable \"log_weights\" is present, replace the API function \"logsumexp\" with \"torch.logsumexp\" to fix the API misuse."}
{"number": 1077, "change": "class MaskTokensDataset(BaseWrapperDataset):\nif self.mask_whole_words is not None:\nmask = np.repeat(mask, word_lens)\nnew_item = np.full(len(mask), self.pad_idx)\n-                new_item[mask] = item[torch.from_numpy(mask)]\n+                new_item[mask] = item[torch.from_numpy(mask.astype(np.uint8))]\nreturn torch.from_numpy(new_item)\n\n# decide unmasking and random replacement\n", "fix_pattern": "<condition>: self.mask_whole_words is not None \n<pattern>: new_item[mask] = item[torch.from_numpy(mask)]\n<code_one>: new_item[mask]\n<code_two>: new_item[mask.astype(np.uint8)]\nFix_pattern: In the condition where self.mask_whole_words is not None, if the pattern of assigning values to new_item[mask] is detected, then change new_item[mask] to new_item[mask.astype(np.uint8)] to fix the API misuse."}
{"number": 1083, "change": "class CategoricalAccuracy(Metric):\n# ith entry in gold_labels points to index (0-num_classes) for ith row in max_predictions\n# For each row check if index pointed by gold_label is was 1 or not (among max scored classes)\ncorrect = max_predictions_mask[\n-                torch.arange(gold_labels.numel()).long(), gold_labels\n+                torch.arange(gold_labels.numel(), device=gold_labels.device).long(), gold_labels\n].float()\ntie_counts = max_predictions_mask.sum(-1)\ncorrect /= tie_counts.float()\n", "fix_pattern": "<condition>: The code was not correctly handling the indices for gold_labels.\n<pattern>: Using the torch.arange() function to generate the correct indices.\n<code_one>: torch.arange(gold_labels.numel()).long(), gold_labels\n<code_two>: torch.arange(gold_labels.numel(), device=gold_labels.device).long(), gold_labels\nFix_pattern: In the condition of incorrect handling of gold_labels indices, if the pattern using torch.arange() is detected, then the code_one torch.arange(gold_labels.numel()).long(), gold_labels should be changed to code_two torch.arange(gold_labels.numel(), device=gold_labels.device).long(), gold_labels to fix the API misuse."}
{"number": 1091, "change": "class Model(ModelDesc):\n.GlobalAvgPooling('gap')\n.FullyConnected('linear', 1000, nl=tf.identity)())\n\n-        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, label)\n+        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)\nloss = tf.reduce_mean(loss, name='xentropy-loss')\n\nwrong = prediction_incorrect(logits, label, 1, name='wrong-top1')\n", "fix_pattern": "<condition>: No clear condition is needed.\n<pattern>: The pattern is to change the argument name from \"label\" to \"labels\".\n<code_one>: tf.nn.sparse_softmax_cross_entropy_with_logits(logits, label)\n<code_two>: tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)\nFix_pattern: In the condition of no clear condition needed, if the pattern of using \"label\" as argument name in tf.nn.sparse_softmax_cross_entropy_with_logits is detected, then change the argument name from \"label\" to \"labels\" to fix the API misuse."}
{"number": 1094, "change": "def batchnorm_example(optimizer_fn,\nfor z in range(batch_per_epoch)]).repeat()\n\noptimizer = optimizer_fn()\n-  batchnorm = tf.compat.v1.layers.BatchNormalization(\n+  batchnorm = normalization.BatchNormalization(\nrenorm=renorm, momentum=momentum, fused=False)\n-  layer = tf.compat.v1.layers.Dense(1, use_bias=False)\n+  layer = core.Dense(1, use_bias=False)\n\ndef model_fn(x):\n\"\"\"A model that uses batchnorm.\"\"\"\n", "fix_pattern": "<condition>: The condition is not clearly mentioned in the provided code.\n<pattern>: The pattern is to replace the usage of \"tf.compat.v1.layers.BatchNormalization\" with \"normalization.BatchNormalization\".\n<code_one>: \"tf.compat.v1.layers.BatchNormalization\".\n<code_two>: \"normalization.BatchNormalization\".\nFix_pattern: In the condition of no specific condition, if the pattern of using \"tf.compat.v1.layers.BatchNormalization\" is detected, then remove it and add \"normalization.BatchNormalization\" to fix the API misuse."}
{"number": 1100, "change": "class TorchHook(object):\n\nself._hook_torch_module()\n\n+        if torch.torch_hooked > 0:\n+            raise Exception('Torch was already hooked')\n+\ndef _hook_native_tensors_and_variables(self, tensor_type):\n\"\"\"Overloading a given tensor_type\"\"\"\n# Overload 'special' methods here\n", "fix_pattern": "<condition>: No pre condition is needed.\n<pattern>: N/A\n<code_one>: N/A\n<code_two>: if torch.torch_hooked > 0: raise Exception('Torch was already hooked')\nFix_pattern: In the condition of no pre condition needed, if torch.torch_hooked is greater than 0, then raise an Exception with the message 'Torch was already hooked' to fix the API misuse."}
{"number": 1104, "change": "class EpsilonDecay(Exploration):\n\npred = tf.logical_or(x=(timestep < self.start_timestep),\ny=(timestep > self.start_timestep + int(self.timesteps)))\n-        return tf.cond(pred=pred, true_fn=true_fn, false_fn=false_fn)\n+        return tf.fill(dims=shape, value=tf.cond(pred=pred, true_fn=true_fn, false_fn=false_fn))\n", "fix_pattern": "<condition>: The condition is the logical OR operation of comparing the 'timestep' variable with the 'start_timestep' variable and the sum of 'start_timestep' and 'self.timesteps'.\n<pattern>: The pattern is a misuse of the conditional operator 'tf.cond', where it is being used to return a tensor based on the 'pred' condition.\n<code_one>: The code that was removed is 'return tf.cond(pred=pred, true_fn=true_fn, false_fn=false_fn)'.\n<code_two>: The code that was added is 'return tf.fill(dims=shape, value=tf.cond(pred=pred, true_fn=true_fn, false_fn=false_fn))'.\nFix_pattern: In the condition of comparing 'timestep' with 'start_timestep' and 'start_timestep' plus 'self.timesteps', if the pattern of using 'tf.cond' to return a tensor is detected, then change the code from 'return tf.cond(pred=pred, true_fn=true_fn, false_fn=false_fn)' to 'return tf.fill(dims=shape, value=tf.cond(pred=pred, true_fn=true_fn, false_fn=false_fn))' to fix the API misuse."}
{"number": 1118, "change": "def _scale_channel(im: torch.Tensor) -> torch.Tensor:\n# and then normalization by step.\nlut = (torch.cumsum(histo, 0) + (step // 2)) // step\n# Shift lut, prepending with 0.\n-        lut = torch.cat([torch.zeros(1, device=lut.device), lut[:-1]])\n+        lut = torch.cat([torch.zeros(1, device=lut.device, dtype=lut.dtype), lut[:-1]])\n# Clip the counts to be in range.  This is done\n# in the C code for image.point.\nreturn torch.clamp(lut, 0, 255)\n", "fix_pattern": "<condition>: There is no clear condition identified in the given context.\n<pattern>: The pattern detected is the absence of a dtype parameter in the torch.zeros() function.\n<code_one>: The code that was removed is \"torch.zeros(1, device=lut.device)\"\n<code_two>: The code that was added is \"dtype=lut.dtype\"\nFix_pattern: In the condition of no specific condition, if the absence of the dtype parameter is detected in the torch.zeros() function, then add \"dtype=lut.dtype\" to fix the API misuse."}
{"number": 1122, "change": "class DonutSwinLayer(nn.Module):\n# partition windows\nhidden_states_windows = window_partition(shifted_hidden_states, self.window_size)\nhidden_states_windows = hidden_states_windows.view(-1, self.window_size * self.window_size, channels)\n-        attn_mask = self.get_attn_mask(height_pad, width_pad)\n+        attn_mask = self.get_attn_mask(height_pad, width_pad, dtype=hidden_states.dtype)\nif attn_mask is not None:\nattn_mask = attn_mask.to(hidden_states_windows.device)\n", "fix_pattern": "<condition>: The condition is that there is an existing attn_mask.\n<pattern>: The pattern is that the attn_mask is being modified by adding a dtype parameter.\n<code_one>: The code being removed is the original line where attn_mask is obtained using the get_attn_mask() function without specifying the dtype of the hidden_states.\n<code_two>: The code being added is the modified line where attn_mask is obtained using the get_attn_mask() function with the dtype parameter set to hidden_states.dtype.\nFix_pattern: In the condition of an existing attn_mask, if the pattern of not specifying the dtype of hidden_states is detected, then the code that obtains attn_mask should be changed to include the dtype parameter to fix the API misuse."}
{"number": 1127, "change": "class VisualBertEmbeddings(nn.Module):\ninputs_embeds = self.word_embeddings(input_ids)\n\nif token_type_ids is None:\n-            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.input_embeds.device)\n+            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)\n\ntoken_type_embeddings = self.token_type_embeddings(token_type_ids)\n", "fix_pattern": "<condition>: When the token_type_ids is not provided.\n<pattern>: Setting the token_type_ids to all zeros.\n<code_one>: token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.input_embeds.device)\n<code_two>: token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)\nFix_pattern: In the condition of missing token_type_ids, the code sets token_type_ids to all zeros in order to fix the API misuse."}
{"number": 1128, "change": "class BCELossMasked(nn.Module):\nReturns:\nloss: An average loss value in range [0, 1] masked by the length.\n\"\"\"\n-        # mask: (batch, max_len, 1)\ntarget.requires_grad = False\nif length is not None:\n-            mask = sequence_mask(sequence_length=length, max_len=target.size(1)).float()\n-            x = x * mask\n-            target = target * mask\n+            # mask: (batch, max_len, 1)\n+            mask = sequence_mask(sequence_length=length, max_len=target.size(1))\nnum_items = mask.sum()\n+            loss = functional.binary_cross_entropy_with_logits(x.masked_select(mask), target.masked_select(mask), pos_weight=self.pos_weight, reduction=\"sum\")\nelse:\n+            loss = functional.binary_cross_entropy_with_logits(x, target, pos_weight=self.pos_weight, reduction=\"sum\")\nnum_items = torch.numel(x)\n-        loss = functional.binary_cross_entropy_with_logits(x, target, pos_weight=self.pos_weight, reduction=\"sum\")\nloss = loss / num_items\nreturn loss\n", "fix_pattern": "<condition>: The condition is whether the variable \"length\" is not None.\n<pattern>: The pattern is that the code is using the \"mask\" tensor to mask the inputs before calculating the loss.\n<code_one>: The code that was removed is the multiplication of \"x\" and \"target\" by the \"mask\" tensor.\n<code_two>: The code that was added is using the \"masked_select\" function to select only the masked elements of \"x\" and \"target\" before calculating the loss.\nFix_pattern: In the condition of \"length is not None\", if the \"mask\" pattern is detected, then remove the multiplication of \"x\" and \"target\" by the \"mask\" tensor and instead use the \"masked_select\" function to select only the masked elements of \"x\" and \"target\" to fix the API misuse."}
{"number": 1129, "change": "class PrioritizedReplay(Memory):\n))\n\nwith tf.control_dependencies(control_inputs=assignments):\n-            return tf.no_op()\n+            return util.no_operation()\n\n# These are not supported for prioritized replay currently.\ndef tf_retrieve_episodes(self, n):\n", "fix_pattern": "Condition: The code is checking for prioritized replay in a class.\nPattern: The code is using \"return tf.no_op()\" to retrieve episodes.\nCode one: \"return tf.no_op()\"\nCode two: \"return util.no_operation()\"\nFix pattern: In the condition of prioritized replay, if the code is using \"return tf.no_op()\" to retrieve episodes, then change \"return tf.no_op()\" to \"return util.no_operation()\" to fix the API misuse."}
{"number": 1132, "change": "class TFFastSpeech(tf.keras.Model):\n== config.decoder_self_attention_params.hidden_size,\nname=\"decoder\",\n)\n-        self.mel_dense = tf.keras.layers.Dense(units=config.num_mels, name=\"mel_before\")\n-        self.postnet = TFTacotronPostnet(config=config, name=\"postnet\")\n+        self.mel_dense = tf.keras.layers.Dense(units=config.num_mels, dtype=tf.float32, name=\"mel_before\")\n+        self.postnet = TFTacotronPostnet(config=config, dtype=tf.float32, name=\"postnet\")\n\nself.setup_inference_fn()\n", "fix_pattern": "<condition>: The condition is the instantiation of the `TFFastSpeech` class.\n<pattern>: The pattern is the missing `dtype=tf.float32` argument in the `Dense` layer instantiation.\n<code_one>: The `Dense` layer instantiation without the `dtype=tf.float32` argument.\n<code_two>: The `Dense` layer instantiation with the `dtype=tf.float32` argument.\nFix_pattern: In the condition of instantiating the `TFFastSpeech` class, if the missing `dtype=tf.float32` pattern is detected in the `Dense` layer instantiation, then the `Dense` layer is updated to include the `dtype=tf.float32` argument to fix the API misuse."}
{"number": 1141, "change": "class H3FeatureMixin(BaseFeatureMixin):\n):\ncolumn = input_df[feature_config[COLUMN]]\nif column.dtype == object:\n-            column = column.map(int)\n-        column = column.map(H3FeatureMixin.h3_to_list)\n+            column = backend.df_engine.map_objects(column, int)\n+        column = backend.df_engine.map_objects(column, H3FeatureMixin.h3_to_list)\n\nproc_df[feature_config[PROC_COLUMN]] = backend.df_engine.map_objects(\ncolumn, lambda x: np.array(x, dtype=np.uint8)\n", "fix_pattern": "<condition>: The condition is that the dtype of the 'column' variable is an object.\n<pattern>: The pattern is that the 'column' variable is being mapped to int and then to H3FeatureMixin.h3_to_list.\n<code_one>: The code that is removed is 'column = column.map(int)' followed by 'column = column.map(H3FeatureMixin.h3_to_list)'.\n<code_two>: The code that is added is 'column = backend.df_engine.map_objects(column, int)' followed by 'column = backend.df_engine.map_objects(column, H3FeatureMixin.h3_to_list)'.\nFix_pattern: In the condition of 'column.dtype == object', if the 'column' variable is detected, then change 'column.map(int)' to 'backend.df_engine.map_objects(column, int)' and change 'column.map(H3FeatureMixin.h3_to_list)' to 'backend.df_engine.map_objects(column, H3FeatureMixin.h3_to_list)' to fix the API misuse."}
{"number": 1144, "change": "class VariationalSparseGP(GPModel):\nM = self.Xu.size(0)\nKuu = self.kernel(self.Xu).contiguous()\nKuu.view(-1)[::M + 1] += self.jitter  # add jitter to the diagonal\n-        Luu = Kuu.cholesky()\n+        Luu = torch.linalg.cholesky(Kuu)\n\nzero_loc = self.Xu.new_zeros(self.u_loc.shape)\nif self.whiten:\n", "fix_pattern": "Condition: The condition is that the variable \"self.whiten\" is true.\nPattern: The pattern is the usage of the function \"cholesky()\" on the variable \"Kuu\".\nCode One: The code \"Luu = Kuu.cholesky()\" is removed.\nCode Two: The code \"Luu = torch.linalg.cholesky(Kuu)\" is added.\n\nFix Pattern: In the condition where \"self.whiten\" is true, if the pattern of using \"cholesky()\" on \"Kuu\" is detected, then the code \"Luu = Kuu.cholesky()\" should be removed and replaced with \"Luu = torch.linalg.cholesky(Kuu)\" to fix the API misuse."}
{"number": 1148, "change": "def _matvecmul(x, y):\n\n\ndef _cholesky(x):\n-    return x.sqrt() if x.dim() == 1 else x.cholesky()\n+    return x.sqrt() if x.dim() == 1 else torch.linalg.cholesky(x)\n\n\ndef _transpose(x):\n", "fix_pattern": "Condition: The condition is not specified in the given code. No clear condition can be identified.\nPattern: The pattern detected is an API misuse where the function `cholesky()` is being called from the wrong module.\nCode One: The code being removed is `x.cholesky()`.\nCode Two: The code being added is `torch.linalg.cholesky(x)`.\nFix Pattern: In the condition of no pre condition needed, if an API misuse is detected where `cholesky()` is called from the wrong module, then the code `x.cholesky()` should be changed to `torch.linalg.cholesky(x)` to fix the issue."}
{"number": 1155, "change": "def convert_to_numpy(x: TensorStructType, reduce_floats: bool = False):\nif torch and isinstance(item, torch.Tensor):\nret = item.cpu().item() if len(item.size()) == 0 else \\\nitem.detach().cpu().numpy()\n-        elif tf and isinstance(item, (tf.Tensor, tf.Variable)):\n+        elif tf and isinstance(item, (tf.Tensor, tf.Variable)) and \\\n+                hasattr(item, \"numpy\"):\nassert tf.executing_eagerly()\nret = item.numpy()\nelse:\n", "fix_pattern": "<condition>: Checking if the variable 'tf' is not None and the item is an instance of either 'tf.Tensor' or 'tf.Variable'.\n\n<pattern>: Removing the 'elif' condition that checks if the item is an instance of 'tf.Tensor' or 'tf.Variable'.\n\n<code_one>: 'elif tf and isinstance(item, (tf.Tensor, tf.Variable))'.\n\n<code_two>: 'elif tf and isinstance(item, (tf.Tensor, tf.Variable)) and hasattr(item, \"numpy\")'.\n\nFix_pattern: In the condition of checking if the variable 'tf' is not None and the item is of type 'tf.Tensor' or 'tf.Variable', remove the code 'elif tf and isinstance(item, (tf.Tensor, tf.Variable))' and add the code 'elif tf and isinstance(item, (tf.Tensor, tf.Variable)) and hasattr(item, \"numpy\")' to fix the API misuse."}
{"number": 1163, "change": "class OpenAIGPTDoubleHeadsModel(OpenAIGPTPreTrainedModel):\ntokenizer = OpenAIGPTTokenizer.from_pretrained('openai-gpt')\nmodel = OpenAIGPTDoubleHeadsModel.from_pretrained('openai-gpt')\ntokenizer.add_special_tokens({'cls_token': '[CLS]'})  # Add a [CLS] to the vocabulary (we should train it also!)\n+        model.resize_token_embeddings(len(tokenizer))\n+\nchoices = [\"Hello, my dog is cute [CLS]\", \"Hello, my cat is cute [CLS]\"]\ninput_ids = torch.tensor([tokenizer.encode(s) for s in choices]).unsqueeze(0)  # Batch size 1, 2 choices\n-        mc_token_ids = torch.tensor([input_ids.size(-1), input_ids.size(-1)]).unsqueeze(0)  # Batch size 1\n+        mc_token_ids = torch.tensor([input_ids.size(-1)-1, input_ids.size(-1)-1]).unsqueeze(0)  # Batch size 1\n+\noutputs = model(input_ids, mc_token_ids=mc_token_ids)\nlm_prediction_scores, mc_prediction_scores = outputs[:2]\n", "fix_pattern": "Condition: The code is using the OpenAIGPTDoubleHeadsModel and OpenAIGPTTokenizer classes from the OpenAI-GPT library.\nPattern: The code is adding a special token [CLS] to the vocabulary, but it is not resizing the token embeddings.\nCode One (Removed): \"mc_token_ids = torch.tensor([input_ids.size(-1), input_ids.size(-1)]).unsqueeze(0)\"\nCode Two (Added): \"model.resize_token_embeddings(len(tokenizer))\" and \"mc_token_ids = torch.tensor([input_ids.size(-1)-1, input_ids.size(-1)-1]).unsqueeze(0)\"\nFix Pattern: In the condition of using OpenAIGPTDoubleHeadsModel and OpenAIGPTTokenizer, if the code is not resizing the token embeddings, then the fix is to add the line \"model.resize_token_embeddings(len(tokenizer))\" and change the line \"mc_token_ids = torch.tensor([input_ids.size(-1), input_ids.size(-1)]).unsqueeze(0)\" to \"mc_token_ids = torch.tensor([input_ids.size(-1)-1, input_ids.size(-1)-1]).unsqueeze(0)\"."}
{"number": 1164, "change": "class Model(ModelDesc):\nadd_moving_summary(tf.reduce_mean(wrong, name='train_error'))\n\n# weight decay on all W of fc layers\n-        wd_cost = tf.mul(0.0004,\n-                         regularize_cost('fc.*/W', tf.nn.l2_loss),\n-                         name='regularize_loss')\n+        wd_cost = regularize_cost('fc.*/W', l2_regularizer(4e-4), name='regularize_loss')\nadd_moving_summary(cost, wd_cost)\n\nadd_param_summary(('.*/W', ['histogram']))   # monitor W\n", "fix_pattern": "<condition>: The fix applies when there is a need to regularize the weights of fc layers.\n<pattern>: The pattern that needed to be detected is the use of tf.mul(0.0004, regularize_cost('fc.*/W', tf.nn.l2_loss), name='regularize_loss') for weight decay.\n<code_one>: The code that needed to be removed is tf.mul(0.0004, regularize_cost('fc.*/W', tf.nn.l2_loss), name='regularize_loss').\n<code_two>: The code that needed to be added is regularize_cost('fc.*/W', l2_regularizer(4e-4), name='regularize_loss').\nFix_pattern: In the condition of needing to regularize the weights of fc layers, if the pattern of using tf.mul(0.0004, regularize_cost('fc.*/W', tf.nn.l2_loss), name='regularize_loss') is detected, then remove the code and add regularize_cost('fc.*/W', l2_regularizer(4e-4), name='regularize_loss') to fix the API misuse."}
{"number": 1168, "change": "def reshape(\nshape: Union[ivy.NativeShape, Sequence[int]],\n*,\ncopy: Optional[bool] = None,\n-    out: Optional[tf.Tensor] = None,\n+    out: Optional[Union[tf.Tensor, tf.Variable]] = None,\n) -> Union[tf.Tensor, tf.Variable]:\nif copy:\nnewarr = tf.experimental.numpy.copy(x)\n", "fix_pattern": "<condition>: The condition is checking if the \"copy\" parameter is True.\n<pattern>: The pattern is the presence of the \"out\" parameter in the function definition.\n<code_one>: The code that was removed is the \"out: Optional[tf.Tensor] = None\" parameter.\n<code_two>: The code that was added is the \"out: Optional[Union[tf.Tensor, tf.Variable]] = None\" parameter.\nFix_pattern: In the condition of \"if copy\", if the pattern of \"out\" parameter is detected, then the code \"out: Optional[tf.Tensor] = None\" should be removed and replaced with \"out: Optional[Union[tf.Tensor, tf.Variable]] = None\" to fix the API misuse."}
{"number": 1191, "change": "class TestInvertAffineTransform:\nassert_allclose(matrix_inv, expected)\n\ndef test_gradcheck(self, device):\n-        matrix = torch.eye(2, 3).to(device)\n+        matrix = torch.eye(2, 3).to(device)[None]\nmatrix = utils.tensor_to_gradcheck_var(matrix)  # to var\nassert gradcheck(kornia.invert_affine_transform, (matrix,),\nraise_exception=True)\n", "fix_pattern": "<condition>: During the gradient check of the invert_affine_transform function.\n<pattern>: The matrix initialization statement was modified.\n<code_one>: matrix = torch.eye(2, 3).to(device).\n<code_two>: matrix = torch.eye(2, 3).to(device)[None].\nFix_pattern: In the condition of the gradient check of the invert_affine_transform function, if the matrix initialization statement is detected, then change the code to fix the API misuse by adding \"[None]\" to the matrix assignment."}
{"number": 1193, "change": "class RNN(torch.nn.Module):\ndef __init__(self, idim, elayers, cdim, hdim, dropout, typ=\"blstm\"):\nsuper(RNN, self).__init__()\nbidir = typ[0] == \"b\"\n-        self.nblstm = torch.nn.LSTM(idim, cdim, elayers, batch_first=True,\n-                                    dropout=dropout, bidirectional=bidir) if \"lstm\" in typ \\\n+        self.nbrnn = torch.nn.LSTM(idim, cdim, elayers, batch_first=True,\n+                                   dropout=dropout, bidirectional=bidir) if \"lstm\" in typ \\\nelse torch.nn.GRU(idim, cdim, elayers, batch_first=True, dropout=dropout,\nbidirectional=bidir)\nif bidir:\n", "fix_pattern": "Condition: The condition is not clear or specified in the given code snippet.\n\nPattern: The pattern is \"lstm\" in the typ variable.\n\nCode One: The code that was removed is \"self.nblstm = torch.nn.LSTM(idim, cdim, elayers, batch_first=True, dropout=dropout, bidirectional=bidir) if \"lstm\" in typ\".\n\nCode Two: The code that was added is \"self.nbrnn = torch.nn.LSTM(idim, cdim, elayers, batch_first=True, dropout=dropout, bidirectional=bidir) if \"lstm\" in typ\".\n\nFix Pattern: In the condition of unmentioned condition, if \"lstm\" is detected, then change the code_one \"self.nblstm\" to code_two \"self.nbrnn\" to fix the API misuse."}
{"number": 1197, "change": "class TFData2VecVisionForSemanticSegmentation(TFData2VecVisionPreTrainedModel):\n# FPNs\nself.fpn1 = [\ntf.keras.layers.Conv2DTranspose(config.hidden_size, kernel_size=2, strides=2, name=\"fpn1.0\"),\n-            tf.keras.layers.BatchNormalization(name=\"fpn1.1\"),\n+            tf.keras.layers.BatchNormalization(name=\"fpn1.1\", momentum=0.9, epsilon=1e-5),\ntf.keras.layers.Activation(\"gelu\"),\ntf.keras.layers.Conv2DTranspose(config.hidden_size, kernel_size=2, strides=2, name=\"fpn1.3\"),\n]\n", "fix_pattern": "Condition: There is a missing batch normalization layer in the FPN1 section of the code.\nPattern: The batch normalization layer is missing the momentum and epsilon parameters.\nCode_one: tf.keras.layers.BatchNormalization(name=\"fpn1.1\"),\nCode_two: tf.keras.layers.BatchNormalization(name=\"fpn1.1\", momentum=0.9, epsilon=1e-5),\nFix_pattern: In the condition of missing batch normalization layer, if the layer is detected, then add the momentum and epsilon parameters to fix the API misuse."}
{"number": 1199, "change": "def main():\nlogger.info(f\"Number of class images to sample: {num_new_images}.\")\n\nsample_dataset = PromptDataset(args.class_prompt, num_new_images)\n-            sample_dataloader = torch.utils.data.DataLoader(sample_dataset, batch_size=args.sample_batch_size)\n+            total_sample_batch_size = args.sample_batch_size * jax.local_device_count()\n+            sample_dataloader = torch.utils.data.DataLoader(sample_dataset, batch_size=total_sample_batch_size)\n\nfor example in tqdm(\nsample_dataloader, desc=\"Generating class images\", disable=not jax.process_index() == 0\n", "fix_pattern": "<condition>: The code is generating class images in a loop.\n<pattern>: The batch size of the dataloader used for generating class images needs to be updated.\n<code_one>: sample_dataloader = torch.utils.data.DataLoader(sample_dataset, batch_size=args.sample_batch_size)\n<code_two>: sample_dataloader = torch.utils.data.DataLoader(sample_dataset, batch_size=total_sample_batch_size)\nFix_pattern: In the condition of generating class images in a loop, if the batch size of the dataloader is set to the default value, then change the value to accommodate the total sample batch size."}
{"number": 1200, "change": "class TokenCharactersIndexer(TokenIndexer[List[int]]):\n# Removes the \"dummy token\".\npadded_tokens.pop()\n# Truncates all the tokens to the desired length, and return the result.\n-        return {key: [list(token[:desired_token_length]) for token in padded_tokens]}\n+        return {key: torch.LongTensor([list(token[:desired_token_length])\n+                                       for token in padded_tokens])}\n", "fix_pattern": "Condition: There is a need to convert a list of tokens into a dictionary format.\nPattern: List comprehension is used to create the dictionary format.\nCode one: [list(token[:desired_token_length]) for token in padded_tokens]\nCode two: torch.LongTensor([list(token[:desired_token_length]) for token in padded_tokens])\nFix pattern: In the condition of converting tokens to a dictionary format, if a list comprehension pattern is detected, then change the code from [list(token[:desired_token_length]) for token in padded_tokens] to torch.LongTensor([list(token[:desired_token_length]) for token in padded_tokens]) to fix the API misuse."}
{"number": 1257, "change": "class Module(tf.Module):\nelif initializer == 'ones':\ninitializer = tf_util.ones(shape=spec.shape, dtype=spec.type)\nelif initializer == 'constant':\n-            initializer = tf_util.fill(dims=spec.shape, value=self.initialization_scale)\n+            initializer = tf.fill(\n+                dims=spec.shape, value=tf_util.constant(value=initialization_scale, dtype=spec.type)\n+            )\n\n# Variable\nvariable = tf.Variable(\n", "fix_pattern": "<condition>: The condition is when the initializer is set to 'constant' in the if statement.\n<pattern>: The pattern is the use of tf_util.fill() for initializing the variable.\n<code_one>: The code being removed is `initializer = tf_util.fill(dims=spec.shape, value=self.initialization_scale)`.\n<code_two>: The code being added is `initializer = tf.fill(dims=spec.shape, value=tf_util.constant(value=initialization_scale, dtype=spec.type))`.\nFix_pattern: In the condition of initializer being 'constant', if tf_util.fill() is detected, then the code `initializer = tf_util.fill(dims=spec.shape, value=self.initialization_scale)` is changed to `initializer = tf.fill(dims=spec.shape, value=tf_util.constant(value=initialization_scale, dtype=spec.type))` to fix the API misuse."}
{"number": 1265, "change": "def test_transformer_conv():\n\nt = '(PairTensor, SparseTensor, NoneType) -> Tensor'\njit = torch.jit.script(conv.jittable(t))\n-    assert torch.allclose(conv((x1, x2), adj.t()), out, atol=1e-6)\n+    assert torch.allclose(jit((x1, x2), adj.t()), out, atol=1e-6)\n", "fix_pattern": "Condition: There is a function called `conv` that takes two arguments `(x1, x2)` and `adj.t()` and returns `out`.\nPattern: An assertion is used to check if the output of `conv` is close to the expected output `out` with a tolerance of `atol=1e-6`.\nCode one: `assert torch.allclose(conv((x1, x2), adj.t()), out, atol=1e-6)`\nCode two: `assert torch.allclose(jit((x1, x2), adj.t()), out, atol=1e-6)`\nFix_pattern: In the condition of using the `conv` function, if the pattern of asserting the output is detected, then change the code from using `conv` to using `jit`."}
{"number": 1266, "change": "PT_SEQUENCE_CLASSIFICATION_SAMPLE = r\"\"\"\n...     \"{checkpoint}\", num_labels=num_labels, problem_type=\"multi_label_classification\"\n... )\n\n-    >>> labels = torch.nn.functional.one_hot(torch.tensor(predicted_class_ids), num_classes=num_labels).to(torch.float)\n+    >>> labels = torch.sum(\n+    ...     torch.nn.functional.one_hot(predicted_class_ids[None, :].clone(), num_classes=num_labels), dim=1\n+    ... ).to(torch.float)\n>>> loss = model(**inputs, labels=labels).loss\n```\n\"\"\"\n", "fix_pattern": "Condition: There is a need to change the way labels are calculated in a sequence classification problem. \nPattern: The original code was generating one-hot encoded labels using a tensor and the predicted class IDs. \nCode One: The code was removed.\nCode Two: The updated code adds an additional step to clone the tensor of predicted class IDs before generating the one-hot encoded labels. \nFix_pattern: In the condition of sequence classification problem, if tensor-based one-hot encoding of labels is detected, then the code is changed to clone the tensor before generating the one-hot encoded labels to fix the API misuse."}
{"number": 1273, "change": "def test_gat_conv():\n\nt = '(OptPairTensor, SparseTensor, Size, NoneType) -> Tensor'\njit = torch.jit.script(conv.jittable(t))\n-    assert torch.allclose(conv((x1, x2), adj.t()), out1, atol=1e-6)\n-    assert torch.allclose(conv((x1, None), adj.t()), out2, atol=1e-6)\n+    assert torch.allclose(jit((x1, x2), adj.t()), out1, atol=1e-6)\n+    assert torch.allclose(jit((x1, None), adj.t()), out2, atol=1e-6)\n", "fix_pattern": "Condition: The condition is not clearly stated in the given context.\n\nPattern: The pattern is the change from calling the original `conv` function to calling the `jit` function.\n\nCode One: The code one is the calls to `conv` in the removed code.\n\nCode Two: The code two is the calls to `jit` in the added code.\n\nFix Pattern: In the condition of <condition>, if <pattern> is detected, then change the <code_one> to <code_two> to fix the API misuse."}
{"number": 1282, "change": "def multi_perspective_match_pairwise(\nnorm_value = vector1_norm * vector2_norm.transpose(2, 3)\n\n# (batch, seq_len1, seq_len2, num_perspectives)\n-    return (mul_result / norm_value.clamp(min=eps)).permute(0, 2, 3, 1)\n+    return (mul_result / norm_value.clamp(min=tiny_value_of_dtype(norm_value.dtype))).permute(\n+        0, 2, 3, 1\n+    )\n\n\nclass BiMpmMatching(nn.Module, FromParams):\n", "fix_pattern": "<condition>: The condition is the calculation of the norm value, which is the multiplication of vector1_norm and the transpose of vector2_norm. \n<pattern>: The pattern is the division of mul_result by norm_value.clamp(min=eps) in the return statement. \n<code_one>: The code_one is (mul_result / norm_value.clamp(min=eps)).permute(0, 2, 3, 1). \n<code_two>: The code_two is (mul_result / norm_value.clamp(min=tiny_value_of_dtype(norm_value.dtype))).permute(0, 2, 3, 1). \nFix_pattern: In the condition of calculating the norm value, if the division pattern is detected, then change code_one to code_two to fix the API misuse."}
{"number": 1290, "change": "def train(hyp, opt, device, tb_writer=None):\nif rank != -1:\nindices = torch.zeros([dataset.n], dtype=torch.int)\nif rank == 0:\n-                    indices[:] = torch.from_tensor(dataset.indices, dtype=torch.int)\n+                    indices[:] = torch.tensor(dataset.indices, dtype=torch.int)\ndist.broadcast(indices, 0)\nif rank != 0:\ndataset.indices = indices.cpu().numpy()\n", "fix_pattern": "Condition: The condition is \"rank != 0\".\n\nPattern: The pattern is the assignment of the variable \"indices\" using torch.from_tensor().\n\nCode One: The code one is \"indices[:] = torch.from_tensor(dataset.indices, dtype=torch.int)\".\n\nCode Two: The code two is \"indices[:] = torch.tensor(dataset.indices, dtype=torch.int)\".\n\nFix Pattern: In the condition of \"rank != 0\", if the pattern of assigning \"indices\" using torch.from_tensor() is detected, then change the \"code_one\" to \"code_two\" to fix the API misuse."}
{"number": 1294, "change": "class SCSEModule(nn.Module):\nnn.Conv2d(in_channels // reduction, in_channels, 1),\nnn.Sigmoid(),\n)\n-        self.sSE = nn.Sequential(nn.Conv2d(in_channels, in_channels, 1), nn.Sigmoid())\n+        self.sSE = nn.Sequential(nn.Conv2d(in_channels, 1, 1), nn.Sigmoid())\n\ndef forward(self, x):\nreturn x * self.cSE(x) + x * self.sSE(x)\n", "fix_pattern": "Condition: The condition for the fix pattern is when the \"sSE\" operation is performed in the forward function of the \"SCSEModule\" class.\n\nPattern: The pattern is the misuse of the \"sSE\" operation, specifically the incorrect number of output channels in the convolution operation.\n\nCode one: The code that was removed is the incorrect operation that creates the \"sSE\" operation with the same number of input and output channels.\n\nCode two: The code that was added is the correct operation that creates the \"sSE\" operation with one output channel.\n\nFix pattern: In the condition of performing the \"sSE\" operation in the \"SCSEModule\" class, if the misuse of the operation with the incorrect number of output channels is detected, then change the code that creates the operation to have one output channel to fix the API misuse."}
{"number": 1296, "change": "def interpolate(\nsize = [x.shape[0], *size, x.shape[1]]\n\nif align_corners or mode == \"area\":\n-        return ivy.interpolate(\n+        return ivy.functional.experimental.interpolate(\nx, size, mode=mode, align_corners=align_corners, antialias=antialias\n)\nx = jnp.transpose(x, (0, *range(2, dims + 2), 1))\n", "fix_pattern": "<condition>: If align_corners or mode == \"area\" in the code.\n<pattern>: The code is calling the interpolate function from the ivy module.\n<code_one>: The code is using ivy.interpolate.\n<code_two>: The code should be using ivy.functional.experimental.interpolate.\nFix_pattern: In the condition of align_corners or mode == \"area\", if the code is calling ivy.interpolate, then change it to ivy.functional.experimental.interpolate to fix the API misuse."}
{"number": 1308, "change": "def count_nonzero(\ndef _dtype_count_nonzero(a, axis, dtype):\nif dtype is None:\nreturn torch.count_nonzero(a, dim=axis)\n-        return torch.tensor(torch.count_nonzero(a, dim=axis), dtype=dtype)\n+        return torch.tensor(torch.count_nonzero(a, dim=axis),\n+                            dtype=ivy.as_native_dtype(dtype))\n\nx = _dtype_count_nonzero(a, axis, dtype)\nif not keepdims:\n", "fix_pattern": "Condition: The condition is if dtype is None.\nPattern: The pattern detected is the mismatch in dtype argument.\nCode_one: The code being removed is \"return torch.tensor(torch.count_nonzero(a, dim=axis), dtype=dtype)\".\nCode_two: The code being added is \"return torch.tensor(torch.count_nonzero(a, dim=axis), dtype=ivy.as_native_dtype(dtype))\".\nFix_pattern: In the condition of dtype being None, if there is a mismatch in dtype argument, then remove the code for dtype argument and add the corrected code for dtype using the function ivy.as_native_dtype."}
{"number": 1312, "change": "class TorchTensor(AbstractTensor):\n\n\"\"\"\n\n-        assert isinstance(self.child, PointerTensor)\n+        if not isinstance(self.child, PointerTensor):\n+            raise TypeError(\"child should be a PointerTensor\")\n\nps = list(pointers)\nps.append(self)\n", "fix_pattern": "Condition: The code is checking the type of the \"child\" attribute of an object. \nPattern: The pattern is checking whether the \"child\" attribute is an instance of the \"PointerTensor\" class. \nCode one: The code is asserting that the \"child\" attribute is of type \"PointerTensor\". \nCode two: The code is raising a TypeError with the message \"child should be a PointerTensor\" if the \"child\" attribute is not an instance of the \"PointerTensor\" class. \nFix pattern: In the condition of checking the type of the \"child\" attribute, if it is not an instance of \"PointerTensor\", then raise a TypeError to fix the API misuse."}
{"number": 1313, "change": "def unravel_index(\nfor dim in reversed(shape):\noutput.append(temp % dim)\ntemp = temp // dim\n-    return tuple(reversed(output))\n+    return torch.tensor(reversed(output))\n\n\nunravel_index.support_native_out = False\n", "fix_pattern": "Condition: There is no clear condition identified in the context.\nPattern: A return statement using the \"reversed\" function is being removed.\nCode One: \"return tuple(reversed(output))\"\nCode Two: \"return torch.tensor(reversed(output))\"\nFix Pattern: In the condition of no specific condition, if a return statement using the \"reversed\" function is detected, then change the code one from \"return tuple(reversed(output))\" to \"return torch.tensor(reversed(output))\" to fix the API misuse."}
{"number": 1315, "change": "class CanineSelfAttention(nn.Module):\n# Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n# masked positions, this operation will create a tensor which is 0.0 for\n# positions we want to attend and -10000.0 for masked positions.\n-                attention_mask = (1.0 - attention_mask.float()) * -10000.0\n+                attention_mask = (1.0 - attention_mask.float()) * torch.finfo(attention_scores.dtype).min\n# Apply the attention mask (precomputed for all layers in CanineModel forward() function)\nattention_scores = attention_scores + attention_mask\n", "fix_pattern": "<condition>: The condition is the calculation of the attention scores and attention mask in the CanineSelfAttention class.\n<pattern>: The pattern is the calculation of the attention mask using a float tensor and multiplying it by -10000.0.\n<code_one>: The code that is removed is \"attention_mask = (1.0 - attention_mask.float()) * -10000.0\".\n<code_two>: The code that is added is \"attention_mask = (1.0 - attention_mask.float()) * torch.finfo(attention_scores.dtype).min\".\nFix_pattern: In the condition of calculating the attention scores and attention mask, if the pattern of multiplying the attention mask by -10000.0 is detected, then change the code to multiply the attention mask by torch.finfo(attention_scores.dtype).min to fix the API misuse."}
{"number": 1323, "change": "class TFModel(NNModel, metaclass=TfModelMeta):\nopt_scope = tf.variable_scope(optimizer_scope_name)\nwith opt_scope:\nif learnable_scopes is None:\n-                variables_to_train = tf.trainable_variables()\n+                variables_to_train = tf.global_variables()\nelse:\nvariables_to_train = []\nfor scope_name in learnable_scopes:\n-                    for var in tf.trainable_variables():\n+                    for var in tf.global_variables():\nif scope_name in var.name:\nvariables_to_train.append(var)\n", "fix_pattern": "Condition: The condition is that the variable \"learnable_scopes\" is not None.\nPattern: The pattern is that the code mistakenly uses \"tf.trainable_variables()\" instead of \"tf.global_variables()\".\nCode_one: In the code removed section, the line \"variables_to_train = tf.trainable_variables()\" is removed.\nCode_two: In the code added section, the line \"variables_to_train = tf.global_variables()\" is added.\nFix Pattern: In the condition of \"learnable_scopes is not None\", if the pattern of using \"tf.trainable_variables()\" is detected, then change the code from \"variables_to_train = tf.trainable_variables()\" to \"variables_to_train = tf.global_variables()\" to fix the API misuse."}
{"number": 1324, "change": "def abs(x):\n\n\ndef sqrt(x):\n-    x = tf.clip_by_value(x, _EPSILON, np.inf)\n+    x = tf.clip_by_value(x, 0., np.inf)\nreturn tf.sqrt(x)\n", "fix_pattern": "<condition>: The condition is not clearly stated in the provided code.\n<pattern>: The pattern identified is a change in the value passed to `tf.clip_by_value()` function.\n<code_one>: The original code used `_EPSILON` as the lower bound for clipping.\n<code_two>: The fixed code changed `_EPSILON` to `0.` as the lower bound for clipping.\nFix_pattern: In the condition of no specific condition, if the value `_EPSILON` is detected as the lower bound for clipping using `tf.clip_by_value()`, then changing `_EPSILON` to `0.` will fix the API misuse."}
{"number": 1346, "change": "def reset_deterministic_algorithm():\nyield\nif _TORCH_GREATER_EQUAL_1_8:\ntorch.use_deterministic_algorithms(False)\n-    elif _TORCH_GREATER_EQUAL_1_7:\n+    else:\ntorch.set_deterministic(False)\n-    else:  # the minimum version Lightning supports is PyTorch 1.6\n-        torch._set_deterministic(False)\n\n\n@pytest.fixture\n", "fix_pattern": "Condition: The condition in this context is checking if the attribute `_TORCH_GREATER_EQUAL_1_7` is True.\nPattern: The pattern that is detected is an `elif` condition followed by an `else` condition.\nCode One: The code that is removed is `else:  # the minimum version Lightning supports is PyTorch 1.6 torch._set_deterministic(False)`.\nCode Two: The code that is added is `else:`.\nFix Pattern: In the condition of `_TORCH_GREATER_EQUAL_1_7`, if the pattern of an `elif` and `else` condition is detected, then remove the code `else:  # the minimum version Lightning supports is PyTorch 1.6 torch._set_deterministic(False)` and add the code `else:` to fix the API misuse."}
{"number": 1357, "change": "class MultiHeadAttention(nn.Module):\n# perform attention, result size = (n_head * mb_size) x len_q x d_v\noutputs, attns = self.attention(q_s, k_s, v_s, attn_mask=attn_mask.repeat(n_head, 1, 1))\n\n-        # back to original mb_size batch\n-        outputs = outputs.view(mb_size, len_q, -1)            # mb_size x len_q x (n_head*d_v)\n+        # back to original mb_size batch, result size = mb_size x len_q x (n_head*d_v)\n+        outputs = torch.cat(torch.split(outputs, mb_size, dim=0), dim=-1)\n\n# project back to residual size\noutputs = self.proj(outputs)\n", "fix_pattern": "<condition>: In the MultiHeadAttention class, when performing attention with the attention method, the outputs are of size (n_head * mb_size) x len_q x d_v.\n<pattern>: The pattern is to reshape the outputs to original mb_size batch size by using the view method.\n<code_one>: The code that was removed is \"outputs = outputs.view(mb_size, len_q, -1)\".\n<code_two>: The code that was added is \"outputs = torch.cat(torch.split(outputs, mb_size, dim=0), dim=-1)\".\nFix_pattern: In the condition of performing attention in the MultiHeadAttention class, if the outputs need to be reshaped back to the original mb_size batch size, replace the code \"outputs = outputs.view(mb_size, len_q, -1)\" with \"outputs = torch.cat(torch.split(outputs, mb_size, dim=0), dim=-1)\" to fix the API misuse."}
{"number": 1363, "change": "def get_timestep_embedding(\nassert len(timesteps.shape) == 1, \"Timesteps should be a 1d-array\"\n\nhalf_dim = embedding_dim // 2\n-    exponent = -math.log(max_period) * torch.arange(start=0, end=half_dim, dtype=torch.float32)\n+    exponent = -math.log(max_period) * torch.arange(\n+        start=0, end=half_dim, dtype=torch.float32, device=timesteps.device\n+    )\nexponent = exponent / (half_dim - downscale_freq_shift)\n\n-    emb = torch.exp(exponent).to(device=timesteps.device)\n+    emb = torch.exp(exponent)\nemb = timesteps[:, None].float() * emb[None, :]\n\n# scale embeddings\n", "fix_pattern": "Condition: The length of the array \"timesteps\" should be one-dimensional.\nPattern: The exponent variable is computed using the math.log function and torch.arange, and the \"emb\" variable is set using the torch.exp function.\nCode one: exponent = -math.log(max_period) * torch.arange(start=0, end=half_dim, dtype=torch.float32) emb = torch.exp(exponent).to(device=timesteps.device)\nCode two: exponent = -math.log(max_period) * torch.arange(start=0, end=half_dim, dtype=torch.float32, device=timesteps.device) emb = torch.exp(exponent)\nFix pattern: In the condition of checking if the array \"timesteps\" is one-dimensional, the fix is to change the code_one to code_two by adding the \"device=timesteps.device\" argument to the torch.arange function."}
{"number": 1365, "change": "class TestNormalize:\nf = kornia.enhance.Normalize(mean=mean, std=std)\ndata = torch.ones(2, 3, 256, 313)\nif isinstance(mean, float):\n-            expected = (data - torch.tensor(mean)) / torch.tensor(std)\n+            expected = (data - torch.as_tensor(mean)) / torch.as_tensor(std)\nelse:\n-            expected = (data - torch.tensor(mean[0])) / torch.tensor(std[0])\n+            expected = (data - torch.as_tensor(mean[0])) / torch.as_tensor(std[0])\nassert_close(f(data), expected)\n\n@staticmethod\n", "fix_pattern": "<condition>: The condition is checking if the variable \"mean\" is an instance of the float class.\n<pattern>: The pattern is removing the usage of the torch.tensor() function and replacing it with the torch.as_tensor() function.\n<code_one>: The code being removed is \"(data - torch.tensor(mean)) / torch.tensor(std)\" and \"(data - torch.tensor(mean[0])) / torch.tensor(std[0])\".\n<code_two>: The code being added is \"(data - torch.as_tensor(mean)) / torch.as_tensor(std)\" and \"(data - torch.as_tensor(mean[0])) / torch.as_tensor(std[0])\".\nFix_pattern: In the condition of checking if \"mean\" is an instance of the float class, the fix is to remove the usage of torch.tensor() and replace it with torch.as_tensor() to fix the API misuse."}
{"number": 1373, "change": "def train_func(config):\ntrain_dataset = Subset(train_dataset, list(range(64)))\nvalidation_dataset = Subset(validation_dataset, list(range(64)))\n\n-    train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"])\n-    validation_loader = DataLoader(validation_dataset, batch_size=config[\"batch_size\"])\n+    worker_batch_size = config[\"batch_size\"] // train.world_size()\n+\n+    train_loader = DataLoader(train_dataset, batch_size=worker_batch_size)\n+    validation_loader = DataLoader(validation_dataset, batch_size=worker_batch_size)\n\ntrain_loader = train.torch.prepare_data_loader(train_loader)\nvalidation_loader = train.torch.prepare_data_loader(validation_loader)\n", "fix_pattern": "<condition>: No clear condition is needed.\n<pattern>: Using an incorrect batch size for the data loaders.\n<code_one>: train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"])\n<code_two>: train_loader = DataLoader(train_dataset, batch_size=worker_batch_size)\nFix_pattern: In the condition of no clear condition, if an incorrect batch size is detected, then change the train_loader and validation_loader batch size from config[\"batch-size\"] to worker_batch_size to fix the API misuse."}
{"number": 1378, "change": "class FNetEmbeddings(nn.Module):\nif version.parse(torch.__version__) > version.parse(\"1.6.0\"):\nself.register_buffer(\n\"token_type_ids\",\n-                torch.zeros(self.position_ids.size(), dtype=torch.long, device=self.position_ids.device),\n+                torch.zeros(self.position_ids.size(), dtype=torch.long),\npersistent=False,\n)\n", "fix_pattern": "<condition>: The condition is that the installed version of the torch library should be greater than \"1.6.0\".\n<pattern>: The pattern is the instantiation of a torch tensor with zeros using the \"self.position_ids\" attribute.\n<code_one>: The code that was removed is \"torch.zeros(self.position_ids.size(), dtype=torch.long, device=self.position_ids.device)\".\n<code_two>: The code that was added is \"torch.zeros(self.position_ids.size(), dtype=torch.long)\".\nFix_pattern: In the condition of torch version being greater than \"1.6.0\", if the pattern of instantiating a tensor with zeros using \"self.position_ids\" is detected, then remove the code that specifies the device in \"torch.zeros\" to fix the API misuse."}
{"number": 1383, "change": "class TestInvertAffineTransform:\n\ndef test_rot90_batch(self, device):\nangle = torch.tensor([90.]).to(device)\n-        scale = torch.tensor([1.]).to(device)\n+        scale = torch.tensor([[1., 1.]]).to(device)\ncenter = torch.tensor([[0., 0.]]).to(device)\nexpected = torch.tensor([[\n[0., -1., 0.],\n", "fix_pattern": "<condition>: In the test_rot90_batch method of the TestInvertAffineTransform class.\n<pattern>: The scale tensor is changed from a single value tensor to a 2D tensor.\n<code_one>: scale = torch.tensor([1.]).to(device)\n<code_two>: scale = torch.tensor([[1., 1.]]).to(device)\nFix_pattern: In the condition of the test_rot90_batch method, if the scale tensor is a single value tensor, then change it to a 2D tensor by replacing \"scale = torch.tensor([1.]).to(device)\" with \"scale = torch.tensor([[1., 1.]]).to(device)\" to fix the API misuse."}
{"number": 1384, "change": "class LDMSuperResolutionPipelineIntegrationTests(unittest.TestCase):\nldm.to(torch_device)\nldm.set_progress_bar_config(disable=None)\n\n-        generator = torch.Generator(device=torch_device).manual_seed(0)\n+        generator = torch.manual_seed(0)\nimage = ldm(image=init_image, generator=generator, num_inference_steps=20, output_type=\"numpy\").images\n\nimage_slice = image[0, -3:, -3:, -1]\n\nassert image.shape == (1, 256, 256, 3)\n-        expected_slice = np.array([0.7418, 0.7472, 0.7424, 0.7422, 0.7463, 0.726, 0.7382, 0.7248, 0.6828])\n+        expected_slice = np.array([0.7644, 0.7679, 0.7642, 0.7633, 0.7666, 0.7560, 0.7425, 0.7257, 0.6907])\n+\nassert np.abs(image_slice.flatten() - expected_slice).max() < 1e-2\n", "fix_pattern": "Condition: The condition is not explicitly mentioned in the given context.\n\nPattern: The pattern is the detection of a specific code snippet that sets the seed for a generator and defines an expected slice.\n\nCode_one: The code that was removed sets the generator seed and defines the expected slice as an array.\n\nCode_two: The code that was added changes how the generator seed is set and modifies the expected slice array.\n\nFix_pattern: In the condition of unspecified context, if the specific code snippet for setting the generator seed and defining the expected slice is detected, then the code for setting the generator seed and defining the expected slice should be removed/altered to fix the API misuse."}
{"number": 1397, "change": "def binary_config():\ndef test_binary_input_feature(binary_config: Dict, encoder: str) -> None:\nbinary_config.update({\"encoder\": encoder})\nbinary_input_feature = BinaryInputFeature(binary_config)\n-    binary_tensor = torch.randn([SEQ_SIZE, BINARY_W_SIZE], dtype=torch.float32)\n+    binary_tensor = torch.randn([SEQ_SIZE, BINARY_W_SIZE], dtype=torch.float32).to(DEVICE)\nencoder_output = binary_input_feature(binary_tensor)\nassert encoder_output[\"encoder_output\"].shape[1:] == binary_input_feature.output_shape\n", "fix_pattern": "Condition: The code is updating a dictionary with a key-value pair.\nPattern: The code is missing a method to perform a specific operation.\nCode one: The line of code initializing the binary_tensor without the .to(DEVICE) method.\nCode two: The line of code initializing the binary_tensor with the .to(DEVICE) method to ensure it is on the correct device.\nFix pattern: In the condition of updating the binary_config dictionary, if the initialization of the binary_tensor is detected without the .to(DEVICE) method, then add the .to(DEVICE) method to fix the API misuse."}
{"number": 1409, "change": "class UnCLIPPipelineIntegrationTests(unittest.TestCase):\npipeline = pipeline.to(torch_device)\npipeline.set_progress_bar_config(disable=None)\n\n-        generator = torch.Generator(device=torch_device).manual_seed(0)\n+        generator = torch.Generator(device=\"cpu\").manual_seed(0)\noutput = pipeline(\n\"horse\",\nnum_images_per_prompt=1,\n", "fix_pattern": "Condition: There is no specific condition mentioned in the context.\nPattern: The pattern is the initialization of the `generator` variable with a specific device using `torch.Generator(device=torch_device).manual_seed(0)`.\nCode One: `generator = torch.Generator(device=torch_device).manual_seed(0)`.\nCode Two: `generator = torch.Generator(device=\"cpu\").manual_seed(0)`.\nFix Pattern: In the condition of no specific condition, if the pattern of initializing `generator` with a specific device is detected, then change the code to initialize `generator` with the device as \"cpu\" to fix the API misuse."}
{"number": 1415, "change": "class SwapBufferManager(object):\nself.count = count\nself.dtype = dtype\nself.all_buffers = [\n-            torch.zeros(num_elems,\n-                        device='cpu',\n-                        dtype=dtype).pin_memory() for _ in range(count)\n+            get_accelerator().pin_memory(\n+                torch.zeros(num_elems,\n+                            device='cpu',\n+                            dtype=dtype)) for _ in range(count)\n]\nself.free_buffer_index = [i for i in range(count)]\nself.used_buffer_index = {}\n", "fix_pattern": "Condition: There is a need to pin memory on tensors.\nPattern: A loop is used to create a list of tensors with pinned memory.\nCode one: torch.zeros(num_elems, device='cpu', dtype=dtype).pin_memory() for _ in range(count)\nCode two: get_accelerator().pin_memory(torch.zeros(num_elems, device='cpu', dtype=dtype)) for _ in range(count)\nFix pattern: In the condition of needing to pin memory, if a loop creating tensors with pinned memory is detected, then change the code to use the `get_accelerator().pin_memory()` function with the loop to fix the API misuse."}
{"number": 1422, "change": "def _setup_ddp(rank, worldsize):\ndef _ddp_test_fn(rank, worldsize):\n_setup_ddp(rank, worldsize)\ntensor = torch.tensor([1.0])\n-    actual = LightningModule._LightningModule__sync(tensor, sync_dist=True, sync_dist_op=torch.distributed.ReduceOp.SUM)\n+    sync = _Sync(sync_ddp_if_available, should=True, op=torch.distributed.ReduceOp.SUM)\n+    actual = sync(tensor)\nassert actual.item() == dist.get_world_size(), \"Result-Log does not work properly with DDP and Tensors\"\n", "fix_pattern": "<condition>: The code is using DDP (Distributed Data Parallel) to distribute computations across multiple GPUs.\n<pattern>: The code is using the private method \"_LightningModule__sync\" to synchronize tensors across different processes.\n<code_one>: LightningModule._LightningModule__sync(tensor, sync_dist=True, sync_dist_op=torch.distributed.ReduceOp.SUM)\n<code_two>: _Sync(sync_ddp_if_available, should=True, op=torch.distributed.ReduceOp.SUM); actual = sync(tensor)\nFix_pattern: In the condition of using DDP, if the code is using the private method \"_LightningModule__sync\" to synchronize tensors, then it should be replaced with the \"_Sync\" method to fix the API misuse."}
{"number": 1427, "change": "def test_gcn2_conv():\n\nt = '(Tensor, Tensor, SparseTensor, OptTensor) -> Tensor'\njit = torch.jit.script(conv.jittable(t))\n-    assert torch.allclose(conv(x, x_0, adj1.t()), out1, atol=1e-6)\n-    assert torch.allclose(conv(x, x_0, adj2.t()), out2, atol=1e-6)\n+    assert torch.allclose(jit(x, x_0, adj1.t()), out1, atol=1e-6)\n+    assert torch.allclose(jit(x, x_0, adj2.t()), out2, atol=1e-6)\n\nconv.cached = True\nconv(x, x_0, edge_index)\n", "fix_pattern": "<condition>: The condition is not clearly specified in the context section.\n\n<pattern>: The pattern is detecting the API misuse of 'conv' and replacing it with 'jit' to utilize the torch.jit.script functionality.\n\n<code_one>: The code to be removed is 'conv(x, x_0, adj1.t())' and 'conv(x, x_0, adj2.t())'.\n\n<code_two>: The code to be added is 'jit(x, x_0, adj1.t())' and 'jit(x, x_0, adj2.t())'.\n\nFix_pattern: In the condition of an undefined condition, if the API misuse pattern of calling 'conv' is detected, then replace the code with 'jit' to fix the API misuse."}
{"number": 1438, "change": "class TrainingOperator:\n\nlogger.debug(\"Registering optimizers.\")\nself._optimizers = optimizers\n-        if not isinstance(self._optimizers, Iterable):\n+        if isinstance(self._optimizers, torch.optim.Optimizer):\nself._optimizers = [self._optimizers]\n\nif schedulers:\n", "fix_pattern": "<condition>: there is a check for the type of \"self._optimizers\" variable.\n<pattern>: the condition checks if \"self._optimizers\" is not an instance of Iterable.\n<code_one>: \"if not isinstance(self._optimizers, Iterable):\"\n<code_two>: \"if isinstance(self._optimizers, torch.optim.Optimizer):\"\nFix_pattern: \nIn the condition of checking the type of \"self._optimizers\", if it is not an instance of Iterable, then the code \"if not isinstance(self._optimizers, Iterable):\" is removed and \"if isinstance(self._optimizers, torch.optim.Optimizer):\" is added to fix the API misuse."}
{"number": 1463, "change": "def main():\n\n# Save the result as an audio summary.\ndatestring = str(datetime.now()).replace(' ', 'T')\n-    writer = tf.train.SummaryWriter(logdir)\n-    tf.audio_summary('generated', decode, wavenet_params['sample_rate'])\n-    summaries = tf.merge_all_summaries()\n+    writer = tf.summary.FileWriter(logdir)\n+    tf.summary.audio('generated', decode, wavenet_params['sample_rate'])\n+    summaries = tf.summary.merge_all()\nsummary_out = sess.run(summaries,\nfeed_dict={samples: np.reshape(waveform, [-1, 1])})\nwriter.add_summary(summary_out)\n", "fix_pattern": "Condition: The code was using the deprecated function \"tf.audio_summary\".\nPattern: The code was creating an instance of tf.train.SummaryWriter, using tf.audio_summary to generate audio summaries and merging all summaries.\nCode one: \"writer = tf.train.SummaryWriter(logdir)\"\nCode two: \"writer = tf.summary.FileWriter(logdir)\"\nFix Pattern: In the condition of using the deprecated function \"tf.audio_summary\", the fix is to remove the usage of tf.audio_summary and replace the instance of tf.train.SummaryWriter with tf.summary.FileWriter to fix the API misuse."}
