[
    {
        "number": 0,
        "comments": "remove API version fix skip test",
        "commit_message": "Previously, many unit test files started with `enable_v2_behavior`, which would have caused them to run in V2 mode when executing with a V1 test flag. The correct behavior would in fact be to skip such tests when executing with a V1 test flag.\n\nThis fix significantly reduces the total V1 + V2 test load by eliminating redundancy.\n\nPiperOrigin-RevId: 424734850\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class IndexLookupDistributionTest(",
            "",
            "",
            "if __name__ == \"__main__\":",
            "-  tf.compat.v1.enable_v2_behavior()",
            "tf.__internal__.distribute.multi_process_runner.test_main()"
        ]
    },
    {
        "number": 3,
        "comments": "typo fix",
        "commit_message": "Add docformatter in pre-commit (#1242)\n\n* Add docformatter in pre-commit\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Apply suggestions from code review\n\n* Apply suggestions from code review\n\nCo-authored-by: Christian Clauss <cclauss@me.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Christian Clauss <cclauss@me.com>\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def load_homography(file_name):",
            "",
            "",
            "def load_image(file_name):",
            "-    \"\"\"Loads the image with OpenCV and converts to torch.Tensor\"\"\"",
            "+    \"\"\"Load the image with OpenCV and converts to torch.Tensor.\"\"\"",
            "if not os.path.isfile(file_name):",
            "raise AssertionError(f\"Invalid file {file_name}\")"
        ]
    },
    {
        "number": 4,
        "comments": "change class",
        "commit_message": "Fix CI with change of name of nlp (#7054)\n\n* nlp -> datasets\n\n* More nlp -> datasets\n\n* Woopsie\n\n* More nlp -> datasets\n\n* One last\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class TrainerIntegrationTest(unittest.TestCase):",
            "",
            "# Adding one column not used by the model should have no impact",
            "z = np.random.normal(size=(64,)).astype(np.float32)",
            "-        train_dataset = nlp.Dataset.from_dict({\"input_x\": x, \"label\": y, \"extra\": z})",
            "+        train_dataset = datasets.Dataset.from_dict({\"input_x\": x, \"label\": y, \"extra\": z})",
            "model = RegressionModel()",
            "trainer = Trainer(model, args, train_dataset=train_dataset)",
            "trainer.train()"
        ]
    },
    {
        "number": 5,
        "comments": "add condition check for version fix",
        "commit_message": "Fix torch version comparisons (#18460)\n\nComparisons like\nversion.parse(torch.__version__) > version.parse(\"1.6\")\nare True for torch==1.6.0+cu101 or torch==1.6.0+cpu\n\nversion.parse(version.parse(torch.__version__).base_version) are preferred (and available in pytorch_utils.py\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class AlbertEmbeddings(nn.Module):",
            "# position_ids (1, len position emb) is contiguous in memory and exported when serialized",
            "self.register_buffer(\"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)))",
            "self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")",
            "-        if version.parse(torch.__version__) > version.parse(\"1.6.0\"):",
            "+        if is_torch_greater_than_1_6:",
            "self.register_buffer(",
            "\"token_type_ids\",",
            "torch.zeros(self.position_ids.size(), dtype=torch.long),"
        ]
    },
    {
        "number": 6,
        "comments": "no API used",
        "commit_message": "Encoder abstractions (#518)\n\n* Allow stacked RNNs in PytorchSeq2SeqWrapper\n\n* Modify the zero sequence length logic in PytorchSeq2SeqWrapper to work with stateful RNNs\n\n* WIP: stateful RNNs\n\n* Stateful RNNs\n\n* pylint\n\n* more pylint\n\n* fix the docs\n\n* Address Joel's comments\n\n*  stateful=True that works with GRU\n\n* Deal with sorting in stateful RNNs\n\n* pylint\n\n* mypy\n\n* pylint\n\n* Remove max_batch_size\n\n* Add tests for correctness\n\n* pylint\n\n* initial pass at adding Seq2StackEncoder and adding an EncoderBase\n\n* remove print statements\n\n* docs work\n\n* make sort function return the indices it sorted by\n\n* more work on getting statefulness working correctly\n\n* tweaks to comments, make it clear that wrapping the state in a list is only for statefulness\n\n* finish state update logic and improve docs\n\n* add TODO for review\n\n* fix current tests\n\n* remove abstract Seq2Stack encoder\n\n* use Callable type for base\n\n* add tests for update states\n\n* lint, fix mypy using elipsis in tuple\n\n* fix up ELMo with docs and test\n\n* don't require docs for encoder_base\n\n* add comment for ignored docs\n\n* remove out of date docstring\n\n* remove num_valid from encoder_base, superficial fixes\n\n* use random state in tests, use correct num_valid size\n\n* rename a few things, don't zero out actual tensor in test\n\n* don't split elmo output, fix docs\n\n* actually fix docs\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def sort_batch_by_length(tensor: torch.autograd.Variable, sequence_lengths: torc",
            "index_range = Variable(index_range.long())",
            "_, reverse_mapping = permutation_index.sort(0, descending=False)",
            "restoration_indices = index_range.index_select(0, reverse_mapping)",
            "-    return sorted_tensor, sorted_sequence_lengths, restoration_indices",
            "+    return sorted_tensor, sorted_sequence_lengths, restoration_indices, permutation_index",
            "",
            "",
            "def get_dropout_mask(dropout_probability: float, tensor_for_masking: torch.autograd.Variable):"
        ]
    },
    {
        "number": 7,
        "comments": "change API call for math fix",
        "commit_message": "Fix unit test failures with torch==1.1.0 (#1840)\n\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def test_quantile():",
            "",
            "",
            "def test_pi():",
            "-    x = torch.empty(1000).log_normal_(0, 1)",
            "+    x = torch.randn(1000).exp()",
            "assert_equal(pi(x, prob=0.8), quantile(x, probs=[0.1, 0.9]))"
        ]
    },
    {
        "number": 8,
        "comments": "add condition check for state fix",
        "commit_message": "[bugfix] TPU + all_gather + SingleTPU shouldn't call xm.all_gather (#6296)\n\n* resolve an issue with TPU\n\n* update\n\n* add changelog\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class TPUAccelerator(Accelerator):",
            "Return:",
            "A tensor of shape (world_size, batch, ...)",
            "\"\"\"",
            "-        return xm.all_gather(tensor, group=group, sync_grads=sync_grads)",
            "+        # todo: Add support for backward with all_gather",
            "+        if torch.distributed.is_initialized():",
            "+            return xm.all_gather(tensor, group=group, sync_grads=sync_grads)",
            "+        return tensor"
        ]
    },
    {
        "number": 9,
        "comments": "change class name",
        "commit_message": "Add Swin2SR (#19784)\n\n* First draft\n\n* Add more improvements\n\n* Improve forward pass\n\n* Fix layernorm\n\n* Add upscaler\n\n* More improvements\n\n* More improvements\n\n* More improvements\n\n* Improve conversion script\n\n* Add preprocessing\n\n* Make output match original implementation\n\n* Add additional attributes\n\n* Add support for more models\n\n* Support more models\n\n* Add support for real world sr\n\n* Add initial Swin2SRFeatureExtractor\n\n* Add ImageSuperResolutionOutput\n\n* Make more tests pass\n\n* Use BaseModelOutput\n\n* Fix one more test\n\n* Fix more tests\n\n* Fix another test\n\n* Fix all tests\n\n* Rename to Swin2SRImageProcessor\n\n* Fix toctree\n\n* Fix toctree\n\n* Fix rebase\n\n* Improve Swin2SRImageProcessor\n\n* Remove feature extractor file\n\n* Improve model\n\n* Improve conversion script\n\n* Fix integration test\n\n* Fix init\n\n* Fix conversion script\n\n* Address comments\n\n* Improve upsampler\n\n* Add NearestConvUpsampler\n\n* Improve pixel shuffle upsampler\n\n* Improve auxiliary upsampler\n\n* Improve conversion script\n\n* Rename conv_last to final_convolution\n\n* Fix rebase\n\n* Improve upsample module\n\n* Add padding to image processor\n\n* Fix bug\n\n* Update padding\n\n* Remove print statement and fix integration test\n\n* Improve docs\n\n* Add image processor tests\n\n* Convert all checkpoints, fix tests\u00e9\n\n* Remove print statements\n\n* Fix import\n\nCo-authored-by: Niels Rogge <nielsrogge@Nielss-MacBook-Pro.local>\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class Swinv2SelfAttention(nn.Module):",
            "query_layer = self.transpose_for_scores(mixed_query_layer)",
            "",
            "# cosine attention",
            "-        attention_scores = F.normalize(query_layer, dim=-1) @ F.normalize(key_layer, dim=-1).transpose(-2, -1)",
            "+        attention_scores = nn.functional.normalize(query_layer, dim=-1) @ nn.functional.normalize(",
            "+            key_layer, dim=-1",
            "+        ).transpose(-2, -1)",
            "logit_scale = torch.clamp(self.logit_scale, max=math.log(1.0 / 0.01)).exp()",
            "attention_scores = attention_scores * logit_scale",
            "relative_position_bias_table = self.continuous_position_bias_mlp(self.relative_coords_table).view("
        ]
    },
    {
        "number": 10,
        "comments": "add condition check for resource fix",
        "commit_message": "Validate --task speed CPU fix (#10244)\n\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def main(opt):",
            "",
            "else:",
            "weights = opt.weights if isinstance(opt.weights, list) else [opt.weights]",
            "-        opt.half = True  # FP16 for fastest results",
            "+        opt.half = torch.cuda.is_available() and opt.device != 'cpu'  # FP16 for fastest results",
            "if opt.task == 'speed':  # speed benchmarks",
            "# python val.py --task speed --data coco.yaml --batch 1 --weights yolov5n.pt yolov5s.pt...",
            "opt.conf_thres, opt.iou_thres, opt.save_json = 0.25, 0.45, False"
        ]
    },
    {
        "number": 11,
        "comments": "revert the fix",
        "commit_message": "Revert fix on module name for hooked functionc\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class TorchHook:",
            "if type(native_func) in [types.FunctionType, types.BuiltinFunctionType]:",
            "# 3. Build the hooked function",
            "new_func = self.get_hooked_func(native_func)",
            "-                # 4. Move the native function to its original module",
            "-                # /!\\ Can be different from the torch_module!",
            "-                # Ex: in torch.py `torch.argmax = torch.functional.argmax`",
            "-                # ... So torch.argmax.__module__ is 'torch.functional' != 'torch'",
            "-                setattr(eval(native_func.__module__), f\"native_{func}\", native_func)",
            "+                # 4. Move the native function",
            "+                setattr(torch_module, f\"native_{func}\", native_func)",
            "# 5. Put instead the hooked one",
            "setattr(torch_module, func, new_func)"
        ]
    },
    {
        "number": 12,
        "comments": "add param for type fix",
        "commit_message": "[Enhance] Added dtype flag to geometry module (#820)\n\n* Added docs and tests\n\n* Fixed bug for batch prob generator\n\n* Fixed typo\n\n* Fixed mypy\n\n* Fixed mypy\n\n* Added warning\n\n* Updated tests for geometry\n\n* Added dtype for conversions\n\n* Completed tests\n\n* Fixed homography\n\n* Fixed pinhole camera\n\n* Fixed tests\n\n* Fix geometry testing bugs\n\n* Fixed augmentation tests\n\n* code updated\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def get_rotation_matrix2d(",
            "",
            "# create output tensor",
            "batch_size: int = center.shape[0]",
            "-    one = torch.tensor(1.).to(center.device)",
            "+    one = torch.tensor(1., device=center.device, dtype=center.dtype)",
            "M: torch.Tensor = torch.zeros(",
            "batch_size, 2, 3, device=center.device, dtype=center.dtype)",
            "M[..., 0:2, 0:2] = scaled_rotation"
        ]
    },
    {
        "number": 14,
        "comments": "add param for argument fix",
        "commit_message": "Option to provide seed to random generators to ensure reproducibility (#1572)\n\n* Option to provide seed to random generators to ensure reproducibility\n\nI added small function in utilities which imports torch, numpy, python\nrandom and sets seed for all of the libraries to ensure reproducibility\nof results.\n\n* Apply recommendations from core contributors on seeding\n\n1. Moved the seeding code to another file\n2. Make deterministic as a parameter for trainer class\n3. Add assertions for seeding numpy\n4. Added warnings\n5. torch.manual_seed should be enough for seeding torch\n\n* Revert \"Apply recommendations from core contributors on seeding\"\n\nThis reverts commit a213c8e6882eec8a9e7408b9418926d2db7c5461.\n\n* Revert \"Revert \"Apply recommendations from core contributors on seeding\"\"\n\nThis reverts commit 59b2da53c62878de7aab0aa3feb3115e105eea06.\n\n* Change in test, for correct seeding\n\n* Allow seed equal to 0\n\n* Allow seed to be uint32.max\n\n* Added deterministic to benchmarks\n\n* Cuda manual seed as in benchmark seeding\n\n* Seeding should be done before model initialization\n\n* cuda manual_seed is not necessary\n\n* Fixing seed test_cpu_lbfgs\n\nOn some seeds seems like lbfgs doesn't converge.\nSo I fixed the seed during testing.\n\n* rebasing issue with old reproducibility.py\n\n* Improved documentation and ability to seed before initializing Train\nclass\n\n* Change in docs\n\n* Removed seed from trainer, update for documentation\n\n* Typo in the docs\n\n* Added seed_everything to _all_\n\n* Fixing old changes\n\n* Model initialization should be earlier then Trainer\n\n* Update pytorch_lightning/trainer/__init__.py\n\nFrom Example to testcode\n\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\n\n* Fixing according to the contributors suggestions\n\n* Moving horovod deterministic to Trainer class\n\n* deterministic flag affects horovod docs update\n\n* Improved static typing\n\n* Added deterministic to test runners of horovod\n\nIt is failing on some versions, not very predictable\n\n* static seeds for horovod tests\n\n* Change for reset_seed function in tests\n\n* Seeding horovod using reset_seed from tutils\n\n* Update pytorch_lightning/trainer/__init__.py\n\n* chlog\n\n* Update trainer.py\n\n* change \"testcode\" to \"Example\" in trainer init documentation\n\n* Update pytorch_lightning/trainer/seed.py, first line in comment\n\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\n\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\nCo-authored-by: Jirka <jirka.borovec@seznam.cz>\nCo-authored-by: William Falcon <waf2107@columbia.edu>\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def lightning_loop(MODEL, num_runs=10, num_epochs=10):",
            "early_stop_callback=False,",
            "checkpoint_callback=False,",
            "distributed_backend='dp',",
            "+            deterministic=True,",
            ")",
            "trainer.fit(model)"
        ]
    },
    {
        "number": 15,
        "comments": "add condition check for state fix",
        "commit_message": "save/load model only for rank==0 (#212)\n\nSummary:\nPull Request resolved: https://github.com/facebookresearch/pytext/pull/212\n\nSave/load can break for distributed training with multiple processes writing to the same file.  Fix by only doing it for rank 0.\n\nReviewed By: hikushalhere, seayoung1112\n\nDifferential Revision: D13704156\n\nfbshipit-source-id: cb468f76ccda93e29735ab7badb130fedf946df9\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class Trainer(TrainerBase):",
            "break",
            "sys.stdout.flush()",
            "",
            "-        model.load_state_dict(torch.load(best_model_path))",
            "+        if rank == 0:",
            "+            model.load_state_dict(torch.load(best_model_path))",
            "return model, best_metric",
            "",
            "def _run_epoch("
        ]
    },
    {
        "number": 16,
        "comments": "change class name",
        "commit_message": "Refactor topological part of `engine` module (#10023)\n\n* Refactor topological part of Keras engine.\n\n* Fix imports\n\n* Fix merge mixup.\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def test_preprocess_weights_for_loading_gru_incompatible():",
            "",
            "def assert_not_compatible(src, dest, message):",
            "with pytest.raises(ValueError) as ex:",
            "-            keras.engine.topology.preprocess_weights_for_loading(",
            "+            keras.engine.saving.preprocess_weights_for_loading(",
            "dest, initialize_weights(src).get_weights())",
            "assert message in ex.value.message"
        ]
    },
    {
        "number": 17,
        "comments": "change class name",
        "commit_message": "Quick fix :) (#606)\n\n* Changing the name\n\n* style + quality\n\n* update doc and logo\n\n* clean up\n\n* circle-CI on the branche for now\n\n* fix daily dialog dataset\n\n* fix urls\n\nCo-authored-by: Quentin Lhoest <lhoest.q@gmail.com>\n",
        "label": "no",
        "answer": "no",
        "change": [
            "Returns:",
            "\"\"\"",
            "",
            "",
            "-class BERTScore(nlp.Metric):",
            "+class BERTScore(datasets.Metric):",
            "def _info(self):",
            "-        return nlp.MetricInfo(",
            "+        return datasets.MetricInfo(",
            "description=_DESCRIPTION,",
            "citation=_CITATION,",
            "homepage=\"https://github.com/Tiiiger/bert_score\",",
            "inputs_description=_KWARGS_DESCRIPTION,",
            "-            features=nlp.Features(",
            "+            features=datasets.Features(",
            "{",
            "-                    \"predictions\": nlp.Value(\"string\", id=\"sequence\"),",
            "-                    \"references\": nlp.Sequence(nlp.Value(\"string\", id=\"sequence\"), id=\"references\"),",
            "+                    \"predictions\": datasets.Value(\"string\", id=\"sequence\"),",
            "+                    \"references\": datasets.Sequence(datasets.Value(\"string\", id=\"sequence\"), id=\"references\"),",
            "}",
            "),",
            "codebase_urls=[\"https://github.com/Tiiiger/bert_score\"],"
        ]
    },
    {
        "number": 18,
        "comments": "change class",
        "commit_message": "[Fix]: fix mask rcnn training stuck problem when there is no positive rois (#3713)\n\n* Fix mask rcnn stuck problem when there is no positive rois\n\n* support non pos inference in cascade methods, link CU-49tawu\n\n* print mmcv version in CI\n\n* use mmcv repo to check wrappers\n\n* change cpu build\n\n* upgrade mmcv requirements and change ci back\n\n* use pre-built whl in CI\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class CoarseMaskHead(FCNMaskHead):",
            "for i in range(num_fcs):",
            "fc_in_channels = (",
            "last_layer_dim if i == 0 else self.fc_out_channels)",
            "-            self.fcs.append(nn.Linear(fc_in_channels, self.fc_out_channels))",
            "+            self.fcs.append(Linear(fc_in_channels, self.fc_out_channels))",
            "last_layer_dim = self.fc_out_channels",
            "output_channels = self.num_classes * self.output_area",
            "-        self.fc_logits = nn.Linear(last_layer_dim, output_channels)",
            "+        self.fc_logits = Linear(last_layer_dim, output_channels)",
            "",
            "def init_weights(self):",
            "for m in self.fcs.modules():"
        ]
    },
    {
        "number": 19,
        "comments": "style fix",
        "commit_message": "Rename config and environment variable for in memory max size (#2454)\n\n* Rename config and env variable IN_MEMORY_MAX_SIZE\n\n* Rename also in documentation\n\n* Fix style\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def test_load_from_disk_with_default_in_memory(",
            "current_dataset_size = 512  # arrow file size = 512, in-memory dataset size = 148",
            "if max_in_memory_dataset_size == \"default\":",
            "# default = 250 * 2 ** 20",
            "-        max_in_memory_dataset_size = datasets.config.HF_MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES",
            "+        max_in_memory_dataset_size = datasets.config.IN_MEMORY_MAX_SIZE",
            "else:",
            "-        monkeypatch.setattr(datasets.config, \"HF_MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES\", max_in_memory_dataset_size)",
            "+        monkeypatch.setattr(datasets.config, \"IN_MEMORY_MAX_SIZE\", max_in_memory_dataset_size)",
            "if max_in_memory_dataset_size:",
            "expected_in_memory = current_dataset_size < max_in_memory_dataset_size",
            "else:"
        ]
    },
    {
        "number": 21,
        "comments": "remove constraint",
        "commit_message": "fixes sequence tagger class\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class SequenceTagger(flair.nn.DefaultClassifier):",
            "for sentence in batch:",
            "sentence.remove_labels(label_name)",
            "",
            "-            loss = self._calculate_loss(features, gold_labels)",
            "-",
            "if return_loss:",
            "+                loss = self._calculate_loss(features, gold_labels)",
            "overall_loss += loss[0]",
            "label_count += loss[1]"
        ]
    },
    {
        "number": 23,
        "comments": "add param for version fix",
        "commit_message": "fix unittests for the latest updates\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class TacotronTrainTest(unittest.TestCase):",
            "optimizer = optim.Adam(model.parameters(), lr=c.lr)",
            "for i in range(5):",
            "mel_out, linear_out, align, stop_tokens = model.forward(",
            "-                input, input_lengths, mel_spec)",
            "+                input, input_lengths, mel_spec, speaker_ids)",
            "optimizer.zero_grad()",
            "loss = criterion(mel_out, mel_spec, mel_lengths)",
            "stop_loss = criterion_st(stop_tokens, stop_targets)"
        ]
    },
    {
        "number": 24,
        "comments": "add condition check for type fix",
        "commit_message": "Fix bug in examples: double wrap into DataParallel during eval\n\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def evaluate(args, model, tokenizer, prefix=\"\", test=False):",
            "eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)",
            "",
            "# multi-gpu evaluate",
            "-        if args.n_gpu > 1:",
            "+        if args.n_gpu > 1 and not isinstance(model, torch.nn.DataParallel):",
            "model = torch.nn.DataParallel(model)",
            "",
            "# Eval!"
        ]
    },
    {
        "number": 25,
        "comments": "add API call for resource fix",
        "commit_message": "fix cuda test issues\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class TestMotionBlur:",
            ") -> torch.Tensor:",
            "return kornia.filters.motion_blur(input, ksize, angle, direction)",
            "",
            "-        img = torch.rand(2, 3, 4, 5)",
            "+        img = torch.rand(2, 3, 4, 5).to(device)",
            "ksize = 5",
            "angle = 65.",
            "direction = .1"
        ]
    },
    {
        "number": 26,
        "comments": "update number",
        "commit_message": "Reproducibility 3/3 (#1924)\n\n* make tests deterministic\n\n* run slow tests\n\n* prepare for testing\n\n* finish\n\n* refactor\n\n* add print statements\n\n* finish more\n\n* correct some test failures\n\n* more fixes\n\n* set up to correct tests\n\n* more corrections\n\n* up\n\n* fix more\n\n* more prints\n\n* add\n\n* up\n\n* up\n\n* up\n\n* uP\n\n* uP\n\n* more fixes\n\n* uP\n\n* up\n\n* up\n\n* up\n\n* up\n\n* fix more\n\n* up\n\n* up\n\n* clean tests\n\n* up\n\n* up\n\n* up\n\n* more fixes\n\n* Apply suggestions from code review\n\nCo-authored-by: Suraj Patil <surajp815@gmail.com>\n\n* make\n\n* correct\n\n* finish\n\n* finish\n\nCo-authored-by: Suraj Patil <surajp815@gmail.com>\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class DiTPipelineIntegrationTests(unittest.TestCase):",
            "\"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main\"",
            "f\"/dit/{word}_fp16.npy\"",
            ")",
            "-            assert np.abs((expected_image - image).max()) < 1e-2",
            "+",
            "+            assert np.abs((expected_image - image).max()) < 7.5e-1"
        ]
    },
    {
        "number": 30,
        "comments": "add condition check for math fix",
        "commit_message": "Misc fixes (#88)\n\n* make log_dir if it doesn't exist\n\n* use tqdm for all dataset readers\n\n* save vocab if log_dir exists\n\n* fix logging error\n\n* get around passing batch_first to custom lstms\n\n* fix srl default params\n\n* tentative fix for tensor creation\n\n* use tdqm at correct abstraction in srl reader\n\n* don't do boolean checks on tensors\n\n* use an actual tensor in get_dropout_mask to preserve type\n\n* ensure tensors are on cpu for logging in training loop\n\n* raise if Datasets from readers are empty\n\n* use os.path.join fr logging\n\n* update config\n\n* use incorrect american spelling of labeller\n\n* use more sensible order for bulding things in train\n\n* try different lstm in model\n\n* unpack validation tensors into forward\n\n* fix byteTensor overflow bug in masking\n\n* pylint\n\n* fix merge\n\n* fixes for Matt\n\n* fix pylint\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class _Seq2VecWrapper:",
            "def from_params(self, params: Params) -> PytorchSeq2VecWrapper:",
            "if not params.pop('batch_first', True):",
            "raise ConfigurationError(\"Our encoder semantics assumes batch is always first!\")",
            "-        params['batch_first'] = True",
            "+        if self._module_class in self.PYTORCH_MODELS:",
            "+            params['batch_first'] = True",
            "module = self._module_class(**params.as_dict())",
            "return PytorchSeq2VecWrapper(module)"
        ]
    },
    {
        "number": 31,
        "comments": "add param for argument fix",
        "commit_message": "Fixed non-square highres fix generation\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class StableDiffusionProcessingTxt2Img(StableDiffusionProcessing):",
            "return samples",
            "",
            "x = create_random_tensors([opt_C, self.firstphase_height // opt_f, self.firstphase_width // opt_f], seeds=seeds, subseeds=subseeds, subseed_strength=self.subseed_strength, seed_resize_from_h=self.seed_resize_from_h, seed_resize_from_w=self.seed_resize_from_w, p=self)",
            "-        samples = self.sampler.sample(self, x, conditioning, unconditional_conditioning, image_conditioning=self.create_dummy_mask(x))",
            "+        samples = self.sampler.sample(self, x, conditioning, unconditional_conditioning, image_conditioning=self.create_dummy_mask(x, first_phase=True))",
            "",
            "samples = samples[:, :, self.truncate_y//2:samples.shape[2]-self.truncate_y//2, self.truncate_x//2:samples.shape[3]-self.truncate_x//2]"
        ]
    },
    {
        "number": 32,
        "comments": "change API",
        "commit_message": "Add exclude to initializer applicator, clean up types (#96)\n\n* Add exclude to initializer applicator, clean up types\n\n* Fix registrable test, and warning in nn.util\n\n* Add back in type ignore statement; I thought it passed...\n\n* Remove brackets\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def sort_batch_by_length(tensor: torch.autograd.Variable, sequence_lengths: torc",
            "sorted_tensor = tensor.index_select(0, permutation_index)",
            "# This is the equivalent of zipping with index, sorting by the original",
            "# sequence lengths and returning the now sorted indices.",
            "-    index_range = Variable(torch.range(0, len(sequence_lengths) - 1).long())",
            "+    index_range = Variable(torch.arange(0, len(sequence_lengths)).long())",
            "_, reverse_mapping = permutation_index.sort(0, descending=False)",
            "restoration_indices = index_range.index_select(0, reverse_mapping)",
            "return sorted_tensor, sorted_sequence_lengths, restoration_indices"
        ]
    },
    {
        "number": 33,
        "comments": "doc fix",
        "commit_message": "fix linter and add docs\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class LabelSmoothing(nn.Module):",
            "self.normalize_length = normalize_length",
            "",
            "def forward(self, x, target):",
            "+        \"\"\"Compute loss between x and target",
            "+",
            "+        :param torch.Tensor x: prediction (batch, seqlen, class)",
            "+        :param torch.Tensor target: target signal masked with self.padding_id (batch, seqlen)",
            "+        :return: scalar float value",
            "+        :rtype torch.Tensor",
            "+        \"\"\"",
            "assert x.size(2) == self.size",
            "batch_size = x.size(0)",
            "x = x.view(-1, self.size)"
        ]
    },
    {
        "number": 34,
        "comments": "no fix found",
        "commit_message": "Gpu fix (#66)\n\n* fixed an ambiguity in sourcing the USE_CUDA and added .cuda to decoder and encoder rnns\n\n* added cuda method call on batches in all pytorch forward prop calls if the env variable is True\n\n* small fix in sourcing USE_CUDA\n\n* added .cuda to some nets that were missing it and to all the correct inputs, enabled cudnn auto-tunner\n\n* only enabling torch.backends.cudnn.benchmark if USE_CUDA is set to true\n\n* added some documentation and removed a line that was only used for debugging\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class BaseModel(nn.Module):",
            "\"\"\"",
            "logging.error('You must define a forward method for this model')",
            "pass",
            "-",
            "-",
            "-",
            "-",
            "-"
        ]
    },
    {
        "number": 35,
        "comments": "no API",
        "commit_message": "chamfer for empty pointclouds #1174\n\nSummary: Fix divide by zero for empty pointcloud in chamfer. Also for empty batches. In process, needed to regularize num_points_per_cloud for empty batches.\n\nReviewed By: kjchalup\n\nDifferential Revision: D36311330\n\nfbshipit-source-id: 3378ab738bee77ecc286f2110a5c8dc445960340\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class Pointclouds:",
            "self._compute_packed()",
            "return self._cloud_to_packed_first_idx",
            "",
            "-    def num_points_per_cloud(self):",
            "+    def num_points_per_cloud(self) -> torch.Tensor:",
            "\"\"\"",
            "Return a 1D tensor x with length equal to the number of clouds giving",
            "the number of points in each cloud."
        ]
    },
    {
        "number": 37,
        "comments": "typo fix",
        "commit_message": "[MRG]Dcgan (#175)\n\n* Add Website Badge in README.md, apply timeout in search function in search.py\n\n* Add timeout in maximize_acq function in search.py\n\n* Update unit test to allow timeout to raise TimeoutError\n\n* Add unit test for timeout resume\n\n* Remove TimeoutError from expectation\n\n* Check Timeout exception in search() in search.py\n\n* finish workable version of gan\n\n* add unit test and small refactoring\n\n* add unsupervised super class\n\n* Fix test_dcgan ran too long issue, put default param in unsupervised::generate(input_sample=None)\n\n* remove examples/gan.py from repo\n\n* add missing import\n\n* correct model_trainer signature\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "from keras.datasets import mnist",
            "from autokeras import ImageClassifier",
            "+import tensorflow",
            "",
            "if __name__ == '__main__':",
            "+    print(tensorflow.__version__)",
            "(x_train, y_train), (x_test, y_test) = mnist.load_data()",
            "-    x_train = x_train.reshape(x_train.shape + (1,))",
            "-    x_test = x_test.reshape(x_test.shape + (1,))",
            "-",
            "+    x_train = x_train.reshape(x_train.shape+(1,))",
            "+    x_test = x_test.reshape(x_test.shape+(1,))",
            "clf = ImageClassifier(verbose=True, augment=False)",
            "clf.fit(x_train, y_train, time_limit=12 * 60 * 60)",
            "clf.final_fit(x_train, y_train, x_test, y_test, retrain=True)"
        ]
    },
    {
        "number": 38,
        "comments": "typo fix",
        "commit_message": "fix bug with misnamed variable in diffusion prior network\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class DiffusionPriorNetwork(nn.Module):",
            "",
            "null_text_embeds = self.null_text_embeds.to(text_embed.dtype)",
            "",
            "-        text_embeds = torch.where(",
            "+        text_embed = torch.where(",
            "text_keep_mask,",
            "text_embed,",
            "null_text_embeds"
        ]
    },
    {
        "number": 39,
        "comments": "format",
        "commit_message": "Fix TF tests for 2.10 (#4956)\n\n* Only create the model once in test_tensorflow\n\n* Unpin TF version\n\n* make style\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class TempSeedTest(TestCase):",
            "import tensorflow as tf",
            "from tensorflow.keras import layers",
            "",
            "+        model = layers.Dense(2)",
            "+",
            "def gen_random_output():",
            "-            model = layers.Dense(2)",
            "x = tf.random.uniform((1, 3))",
            "return model(x).numpy()"
        ]
    },
    {
        "number": 40,
        "comments": "update param for refactor fix",
        "commit_message": "fix isinstance() validation in pytorch\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def makenp(x, modality=None):",
            "",
            "def pytorch_np(x, modality):",
            "import torch",
            "-    if isinstance(x, torch.autograd.variable.Variable):",
            "+    if isinstance(x, torch.autograd.Variable):",
            "x = x.data",
            "x = x.cpu().numpy()",
            "if modality == 'IMG':"
        ]
    },
    {
        "number": 41,
        "comments": "add log",
        "commit_message": "Fix a typo relative_postion_if_large -> relative_position_if_large (#17366)\n\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class T5Attention(nn.Module):",
            "is_small = relative_position < max_exact",
            "",
            "# The other half of the buckets are for logarithmically bigger bins in positions up to max_distance",
            "-        relative_postion_if_large = max_exact + (",
            "+        relative_position_if_large = max_exact + (",
            "torch.log(relative_position.float() / max_exact)",
            "/ math.log(max_distance / max_exact)",
            "* (num_buckets - max_exact)",
            ").to(torch.long)",
            "-        relative_postion_if_large = torch.min(",
            "-            relative_postion_if_large, torch.full_like(relative_postion_if_large, num_buckets - 1)",
            "+        relative_position_if_large = torch.min(",
            "+            relative_position_if_large, torch.full_like(relative_position_if_large, num_buckets - 1)",
            ")",
            "",
            "-        relative_buckets += torch.where(is_small, relative_position, relative_postion_if_large)",
            "+        relative_buckets += torch.where(is_small, relative_position, relative_position_if_large)",
            "return relative_buckets",
            "",
            "def compute_bias(self, query_length, key_length):"
        ]
    },
    {
        "number": 43,
        "comments": "change param for resource fix",
        "commit_message": "Removed unnecessary `_move_optimizer_state` method overrides (#10849)\n\n* Update tpu tp share same logic with ttp\n\n* run test\n\n* Update tpu_spawn.py\n\n* debug\n\n* Add changelog\n\n* Apply suggestions from code review\n\nCo-authored-by: Adrian W\u00e4lchli <aedu.waelchli@gmail.com>\n\n* Update training_type_plugin.py\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Update training_type_plugin.py\n\nCo-authored-by: Adrian W\u00e4lchli <aedu.waelchli@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class TrainingTypePlugin(ABC):",
            "self.lr_schedulers = schedulers",
            "",
            "def _move_optimizer_state(self, device: Optional[torch.device] = None) -> None:",
            "-        \"\"\"Moves the state of the optimizers to the GPU if needed.\"\"\"",
            "-        device = device or self.root_device",
            "+        \"\"\"Moves the state of the optimizers to the appropriate device if needed.\"\"\"",
            "for opt in self.optimizers:",
            "for p, v in opt.state.items():",
            "-                opt.state[p] = apply_to_collection(v, torch.Tensor, move_data_to_device, device)",
            "+                # `self.root_device` would raise error if called outside the spawn process",
            "+                # while training on 8 and more cores.",
            "+                opt.state[p] = apply_to_collection(v, torch.Tensor, move_data_to_device, device or self.root_device)",
            "",
            "def optimizer_state(self, optimizer: Optimizer) -> Dict[str, Tensor]:",
            "\"\"\"Returns state of an optimizer."
        ]
    },
    {
        "number": 45,
        "comments": "rename",
        "commit_message": "Moving pipeline tests from `Narsil` to `hf-internal-testing`. (#14463)\n\n* Moving everything to `hf-internal-testing`.\n\n* Fixing test values.\n\n* Moving to other repo.\n\n* Last touch?\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class ImageSegmentationPipelineTests(unittest.TestCase, metaclass=PipelineTestCa",
            "",
            "import datasets",
            "",
            "-        dataset = datasets.load_dataset(\"Narsil/image_dummy\", \"image\", split=\"test\")",
            "+        dataset = datasets.load_dataset(\"hf-internal-testing/fixtures_image_utils\", \"image\", split=\"test\")",
            "",
            "batch = [",
            "Image.open(\"./tests/fixtures/tests_samples/COCO/000000039769.png\"),"
        ]
    },
    {
        "number": 47,
        "comments": "format",
        "commit_message": "fix: hot fix of two graphs' conflict\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class NerNetwork:",
            "return predictions_batch_no_pad",
            "",
            "def shutdown(self):",
            "-        self._sess.close()",
            "\\ No newline at end of file",
            "+        self._sess.close()"
        ]
    },
    {
        "number": 49,
        "comments": "remove comments",
        "commit_message": "pylint -> flake8 (#3288)\n\n* pylint\n\n* update pylint\n\n* undo a lot of the raise / else\n\n* add bound on typed-ast\n\n* first mypy fixes\n\n* new flag\n\n* fix mypy errors\n\n* requirements.txt\n\n* pylint -> flake8\n\n* mypy 0.720 -> mypy 0.730\n\n* add back erroneously removed initial newline\n\n* remove .pylintrc\n\n* remove pylintrc from Dockerfile\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class LayerNorm(torch.nn.Module):",
            "self.beta = torch.nn.Parameter(torch.zeros(dimension))",
            "self.eps = eps",
            "",
            "-    def forward(self, tensor: torch.Tensor):  # pylint: disable=arguments-differ",
            "+    def forward(self, tensor: torch.Tensor):",
            "mean = tensor.mean(-1, keepdim=True)",
            "std = tensor.std(-1, unbiased=False, keepdim=True)",
            "return self.gamma * (tensor - mean) / (std + self.eps) + self.beta"
        ]
    },
    {
        "number": 50,
        "comments": "add condition check for shape fix",
        "commit_message": "Fix: `to_hetero` with `GCN` on single node types (#4279)\n\n* fix\n\n* typo\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class GraphConv(MessagePassing):",
            "self.lin.reset_parameters()",
            "",
            "def forward(self, x, edge_index):",
            "+        if isinstance(x, Tensor):",
            "+            x = (x, x)",
            "return self.propagate(edge_index, x=(self.lin(x[0]), x[1]))"
        ]
    },
    {
        "number": 51,
        "comments": "version fix to fix",
        "commit_message": "Fix some more scope issues\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def resnet_argscope():",
            "with argscope([Conv2D, MaxPooling, BatchNorm], data_format='NCHW'), \\",
            "argscope(Conv2D, use_bias=False), \\",
            "argscope(BatchNorm, use_local_stat=False), \\",
            "-            tf.variable_scope(tf.get_variable_scope(),",
            "-                              custom_getter=maybe_freeze_affine):",
            "+            custom_getter_scope(maybe_freeze_affine):",
            "yield"
        ]
    },
    {
        "number": 53,
        "comments": "update API call for version fix",
        "commit_message": "update smddp api to v1.4.0 (#16371)\n\n* update smddp api to v1.4.0\n\n* Update src/transformers/trainer.py\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n\n* Update src/transformers/trainer.py\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n\n* address comments\n\n* fix style\n\n* remove unused import\n\n* fix indent\n\n* disable style check for import\n\n* fix space\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class Trainer:",
            ").to(self.args.device)",
            "",
            "elif is_sagemaker_dp_enabled():",
            "-            model = DDP(model, device_ids=[dist.get_local_rank()], broadcast_buffers=False)",
            "+            model = nn.parallel.DistributedDataParallel(",
            "+                model, device_ids=[int(os.getenv(\"SMDATAPARALLEL_LOCAL_RANK\"))]",
            "+            )",
            "elif self.args.local_rank != -1:",
            "kwargs = {}",
            "if self.args.ddp_find_unused_parameters is not None:"
        ]
    },
    {
        "number": 54,
        "comments": "rename",
        "commit_message": "New model in test cases. Fixed test cases.\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class Network(object):",
            "\"\"\"",
            "@layer",
            "def softmax(self, target, axis, name=None):",
            "-        max_axis = tf.reduce_max(target, axis, keepdims=True)",
            "+        max_axis = tf.reduce_max(target, axis, keep_dims=True)",
            "target_exp = tf.exp(target-max_axis)",
            "-        normalize = tf.reduce_sum(target_exp, axis, keepdims=True)",
            "+        normalize = tf.reduce_sum(target_exp, axis, keep_dims=True)",
            "softmax = tf.div(target_exp, normalize, name)",
            "return softmax"
        ]
    },
    {
        "number": 55,
        "comments": "add condition check for state fix",
        "commit_message": "fix error for rnn encoders flatten_parameters\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class RNN(torch.nn.Module):",
            "if not isinstance(ilens, torch.Tensor):",
            "ilens = torch.tensor(ilens)",
            "xs_pack = pack_padded_sequence(xs_pad, ilens.cpu(), batch_first=True)",
            "-        self.nbrnn.flatten_parameters()",
            "+        if self.training:",
            "+            self.nbrnn.flatten_parameters()",
            "if prev_state is not None and self.nbrnn.bidirectional:",
            "# We assume that when previous state is passed,",
            "# it means that we're streaming the input"
        ]
    },
    {
        "number": 56,
        "comments": "add API call for type fix",
        "commit_message": "XLA train step fixes (#17973)\n\n* Copy inputs to train and test step before modifying them, as this breaks things\n\n* Add XLA tests, fix our loss functions to be XLA-compatible\n\n* make fixup\n\n* Update loss computation test to expect vector of per-sample losses\n\n* Patch loss for TFLED\n\n* Patch loss for TFAlbert\n\n* Add a tf_legacy_loss config flag that enables old loss functions\n\n* Stop using config.get() because it's not a dict\n\n* Skip loss computation test for RAG because its loss is very strange and I'm afraid to rewrite it\n\n* make fixup\n\n* Add XLA-compatible RAG loss\n\n* Fix dtype of loss mask for TFAlbert\n\n* Fix test for XLNet too because it overrides the default one\n\n* make fixup\n\n* Fix config test\n\n* No more depending on GPU NaN behaviour\n\n* Add test, avoid potential zero division\n\n* Fix test item assignment\n\n* Fix loss computation masking test\n\n* make fixup\n\n* Fix dtype bugs\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class TFXLNetModelTest(TFModelTesterMixin, unittest.TestCase):",
            "# Send to model",
            "loss = model(tuple_input[:-1])[0]",
            "",
            "-                self.assertEqual(loss.shape, [loss_size])",
            "+                self.assertEqual(loss.shape.as_list(), expected_loss_size)",
            "",
            "",
            "@require_tf"
        ]
    },
    {
        "number": 58,
        "comments": "change param for shape fix",
        "commit_message": "Add OED estimators and accompanying tests (#1925)\n\n* Update EIG estimators and the accompanying tests\n\n* add back some line spacing\n\n* no need to add pre-release methods\n\n* delete old examples\n\n* remove deleted examples from test list\n\n* stray newline\n\n* remove other irrelevant code\n\n* flake8\n\n* do not use CensoredSigmoidNormal\n\n* remove unnecessary expands\n\n* another flake8\n\n* improve docstrings in glmm\n\n* use reshape in place of contiguous().view\n\n* add docstrings to oed/util.py\n\n* add reason= and deprecation warning\n\n* add a lot of docstrings\n\n* double escape\n\n* try writing warning on one line\n\n* rewrite broadcast_cat to use broadcast_tensors\n\n* rename loss functions\n\n* make use of nn.ParameterDict\n\n* always return an EIG by default\n\n* do not update eig_estimation_benchmarking on this PR\n\n* fix issue with glmm\n\n* change from martin\n\n* add citations\n\n* remove N_seq\n\n* rename mean_field_guide_entropy as mean_field_entropy, allow user to avoid mean-field assumption\n\n* delete eig_estimation_benchmarking\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def sigmoid_example(design):",
            "torch.tensor([[-1.5, 0.5], [1.5, 0.]])",
            "),",
            "(",
            "-        known_covariance_linear_model(torch.tensor([1., -1.]), torch.tensor(10.), torch.tensor(1.)),",
            "+        known_covariance_linear_model(torch.tensor([1., -1.]), torch.tensor([10., 10.]), torch.tensor(1.)),",
            "nz_lm_2p_10_10_1,",
            "torch.tensor([[-1., 0.5], [2.5, -2.]])",
            "),"
        ]
    },
    {
        "number": 59,
        "comments": "add API call for math fix",
        "commit_message": "prepare for \"__floordiv__ is deprecated  and its behavior will change in a future version of pytorch\" (#20211)\n\n* rounding_mode = \"floor\"  instead of // to prevent behavioral change\n\n* add other TODO\n\n* use `torch_int_div` from pytrch_utils\n\n* same for tests\n\n* fix copies\n\n* style\n\n* use relative imports when needed\n\n* Co-authored-by: sgugger <sylvain.gugger@gmail.com>\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class DetaModel(DetaPreTrainedModel):",
            "scale = 2 * math.pi",
            "",
            "dim_t = torch.arange(num_pos_feats, dtype=torch.float32, device=proposals.device)",
            "-        dim_t = temperature ** (2 * (dim_t // 2) / num_pos_feats)",
            "+        dim_t = temperature ** (2 * torch.div(dim_t, 2) / num_pos_feats)",
            "# batch_size, num_queries, 4",
            "proposals = proposals.sigmoid() * scale",
            "# batch_size, num_queries, 4, 128"
        ]
    },
    {
        "number": 61,
        "comments": "update API call for refactor fix",
        "commit_message": "use functional interface for softmax in attention (#14198)\n\n* use functional interface instead of instantiating module and immediately calling it\n\n* fix torch.nn.functional to nn.functional. Thank you Stas!\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class LxmertAttention(nn.Module):",
            "attention_scores = attention_scores + attention_mask",
            "",
            "# Normalize the attention scores to probabilities.",
            "-        attention_probs = nn.Softmax(dim=-1)(attention_scores)",
            "+        attention_probs = nn.functional.softmax(attention_scores, dim=-1)",
            "",
            "# This is actually dropping out entire tokens to attend to, which might",
            "# seem a bit unusual, but is taken from the original Transformer paper."
        ]
    },
    {
        "number": 63,
        "comments": "format",
        "commit_message": "lint fixes (#5332)\n\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def trace(",
            "axis2: int = 1,",
            "out: Optional[Union[tf.Tensor, tf.Variable]] = None,",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "-    ret = tf.experimental.numpy.trace(",
            "-        x, offset=offset, axis1=axis1, axis2=axis2",
            "-    )",
            "+    ret = tf.experimental.numpy.trace(x, offset=offset, axis1=axis1, axis2=axis2)",
            "return ret"
        ]
    },
    {
        "number": 65,
        "comments": "fix doc",
        "commit_message": "Doc fixes in preparation for the docstyle PR (#8061)\n\n* Fixes in preparation for doc styling\n\n* More fixes\n\n* Better syntax\n\n* Fixes\n\n* Style\n\n* More fixes\n\n* More fixes\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class TFOpenAIGPTDoubleHeadsModel(TFOpenAIGPTPreTrainedModel):",
            "training=False,",
            "):",
            "r\"\"\"",
            "-        mc_token_ids (:obj:`tf.Tensor` or :obj:`Numpy array` of shape :obj:`(batch_size, num_choices)`, `optional`, default to index of the last token of the input)",
            "+        mc_token_ids (:obj:`tf.Tensor` or :obj:`Numpy array` of shape :obj:`(batch_size, num_choices)`, `optional`, default to index of the last token of the input):",
            "Index of the classification token in each input sequence.",
            "Selected in the range ``[0, input_ids.size(-1) - 1]``."
        ]
    },
    {
        "number": 66,
        "comments": "format",
        "commit_message": "[MOD] fix typo, add tests for ldconv\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class DynamicConvolution2D(nn.Module):",
            "weight = self.linear_weight(x)  # B x T x kH",
            "weight = F.dropout(weight, self.dropout_rate, training=self.training)",
            "weight = weight.view(B, T, H, k).transpose(1, 2).contiguous()  # B x H x T x k",
            "-        weight_new = torch.zeros(B * H * T * (T + k - 1)).view(B, H, T, T + k - 1).fill_(float('-inf'))",
            "+        weight_new = torch.zeros(B * H * T * (T + k - 1), dtype=weight.dtype)",
            "+        weight_new = weight_new.view(B, H, T, T + k - 1).fill_(float('-inf'))",
            "weight_new = weight_new.to(x.device)  # B x H x T x T+k-1",
            "weight_new.as_strided((B, H, T, k), ((T + k - 1) * T * H, (T + k - 1) * T, T + k, 1)).copy_(weight)",
            "weight_new = weight_new.narrow(-1, int((k - 1) / 2), T)  # B x H x T x T(k)"
        ]
    },
    {
        "number": 67,
        "comments": "refactor fix",
        "commit_message": "Fixes to import\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def test_ddp_sharded_plugin_correctness_multi_gpu():",
            "run_sharded_correctness(gpus=2, accelerator='ddp_spawn')",
            "",
            "",
            "-@pytest.mark.skipif(",
            "-    LooseVersion(torch.__version__) < LooseVersion(\"1.6.0\"),",
            "-    reason=\"Minimal PT version is set to 1.6\")",
            "+@pytest.mark.skipif(not NATIVE_AMP_AVALAIBLE, reason=\"Requires native AMP\")",
            "@pytest.mark.skipif(platform.system() == \"Windows\",",
            "reason=\"Distributed training is not supported on Windows\")",
            "@pytest.mark.skipif(torch.cuda.device_count() < 2, reason=\"test requires multi-GPU machine\")"
        ]
    },
    {
        "number": 68,
        "comments": "add param for argument fix",
        "commit_message": "fixed trpo\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class Gru(TransformationBase):",
            "",
            "def tf_apply(self, x, sequence_length=None):",
            "x, state = tf.nn.dynamic_rnn(",
            "-            cell=self.cell, inputs=x, sequence_length=sequence_length, dtype=tf.float32,",
            "+            cell=self.cell, inputs=x, sequence_length=sequence_length, initial_state=None,",
            "+            dtype=util.tf_dtype(dtype='float'),",
            "# Weird TensorFlow behavior? (https://github.com/tensorflow/tensorflow/issues/15874)",
            "parallel_iterations=(self.input_spec['shape'][0] + 1)",
            ")"
        ]
    },
    {
        "number": 69,
        "comments": "change API call place",
        "commit_message": "Fix grad clip norm for FP16 use cases (#1514)\n\nSummary:\nPull Request resolved: https://github.com/facebookresearch/pytext/pull/1514\n\nThe Fairseq FP16 optimizers rely on passing in a `max_grad_norm` of 0 and don\u2019t expect the model gradient to update (https://www.fburl.com/diffusion/buqtrdb5)  . However in PyText optimizers we rely on the torch utility for clipping gradient norm which does not do this and ends up setting the gradients of the model to 0, thus stopping learning (https://www.fburl.com/diffusion/eg6v07qa) .\n\nTo fix this, Fairseq has its own variant of clip_grad_norm (https://www.fburl.com/diffusion/2q2uwjrf)  and this diff adjusts PyText to rely on that instead of the nn utility.\n\nReviewed By: mwu1993\n\nDifferential Revision: D24427025\n\nfbshipit-source-id: 12849c9ad5c08af355b06357d664b293fcc8a080\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class Optimizer(Component):",
            "For those we treat model as max_norm.",
            "eg. optimizer.clip_grad_norm(max_norm)",
            "\"\"\"",
            "-            return torch.nn.utils.clip_grad_norm_(self.params, max_norm)",
            "+            return clip_grad_norm_(self.params, max_norm)",
            "else:",
            "-            return torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)",
            "+            return clip_grad_norm_(model.parameters(), max_norm)",
            "",
            "def pre_export(self, model):",
            "pass"
        ]
    },
    {
        "number": 70,
        "comments": "update logging",
        "commit_message": "Release 1.8.6rc0 (#660)\n\n* Release 1.8.6rc0\n\n* Update CHANGELOG.md\n\n* Typo Fix\n\n* Changelog Updated\n\n* tl_logging more readable\n\n* Typo Fixed\n\n* https enforced for badges\n\n* RTD badges pointing to Latest Commit\n\n* Decorators API Refactored\n\n* extra_requires `all`, `all_cpu` and `all_gpu` added\n\n* Error fix\n\n* YAPF Formating Correction\n\n* Test for private method decorator added\n\n* Test Logging Verbosity Fixed to Debug when runned individually\n\n* YAPF corrections applied\n\n* Changelog Added\n\n* Changelog updated\n\n* PR number changed\n\n* Changelog updated\n\n* Update .travis.yml\n\n* Update CONTRIBUTING.md\n\n* Update CONTRIBUTING.md\n\n* Update CHANGELOG.md\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class Util_Predict_Test(CustomTestCase):",
            "",
            "if __name__ == '__main__':",
            "",
            "-    # tl.logging.set_verbosity(tl.logging.INFO)",
            "+    tf.logging.set_verbosity(tf.logging.DEBUG)",
            "tl.logging.set_verbosity(tl.logging.DEBUG)",
            "",
            "unittest.main()"
        ]
    },
    {
        "number": 71,
        "comments": "remove condition check for resource fix",
        "commit_message": "Fix quantized-inference & Add generic support of checkpoint loading (#2547)\n\n* fix checkpoint loading when it is a dictionary\n\n* fix some issues with saving ckpt & int8 inference\n\n* fix quantized-inference & add generic support of checkpoint loading\n\n* remove int8 hard-coded flag\n\n* fix mlp return tensors\n\n* fix several issue to load checkpoints of GPT-J, GPT-NEOX, and OPT with different TP-size\n\n* add more comments & description for checkpoint-loading module\n\nCo-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class DeepSpeedSelfAttention(nn.Module):",
            "data_type_fp = torch.half if config.fp16 else torch.float",
            "self.config.layer_id = DeepSpeedSelfAttention.num_layers",
            "DeepSpeedSelfAttention.num_layers = DeepSpeedSelfAttention.num_layers + 1",
            "-        device = torch.cuda.current_device() if config.bigscience_bloom else 'cpu'",
            "+        device = torch.cuda.current_device()  #if config.bigscience_bloom else 'cpu'",
            "qkv_size_per_partition = (self.config.hidden_size // self.config.mp_size) * 3",
            "self.attn_qkvw = nn.Parameter(torch.empty(self.config.hidden_size,",
            "qkv_size_per_partition,"
        ]
    },
    {
        "number": 73,
        "comments": "format",
        "commit_message": "remove numpy import (#1116)\n\n* remove numpy import\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* undo and add setup develop in gh actions\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* remove create_checkerboard\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class TestClosing:",
            "None, None, :, :",
            "]",
            "assert_allclose(",
            "-            closing(tensor, torch.ones_like(structural_element), structuring_element=structural_element), expected,",
            "-            atol=1e-4, rtol=1e-4",
            "+            closing(tensor, torch.ones_like(structural_element), structuring_element=structural_element),",
            "+            expected,",
            "+            atol=1e-4,",
            "+            rtol=1e-4,",
            ")",
            "",
            "def test_exception(self, device, dtype):"
        ]
    },
    {
        "number": 74,
        "comments": "version fix",
        "commit_message": "[RLlib] `rllib train` crashes when using torch PPO/PG/A2C. (#7508)\n\n* Fix.\n\n* Rollback.\n\n* TEST.\n\n* TEST.\n\n* TEST.\n\n* TEST.\n\n* TEST.\n\n* TEST.\n\n* TEST.\n\n* TEST.\n\n* TEST.\n\n* TEST.\n\n* TEST.\n\n* TEST.\n\n* TEST.\n\n* TEST.\n\n* TEST.\n\n* TEST.\n\n* TEST.\n\n* TEST.\n\n* TEST.\n\n* TEST.\n\n* TEST.\n\n* TEST.\n\n* TEST.\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class TorchCategorical(TorchDistributionWrapper):",
            "@override(ActionDistribution)",
            "def __init__(self, inputs, model=None, temperature=1.0):",
            "assert temperature > 0.0, \"Categorical `temperature` must be > 0.0!\"",
            "-        super().__init__(inputs / temperature, model)",
            "+        inputs /= temperature",
            "+        super().__init__(inputs, model)",
            "self.dist = torch.distributions.categorical.Categorical(",
            "logits=self.inputs)"
        ]
    },
    {
        "number": 75,
        "comments": "change funtional",
        "commit_message": "* feat: add SQuAD model for Russian lang (#192)\n\n* feat: add lr decay to squad model\n\n* refactor: cudnn layers refactored\n\n* fix: double transpose output fixed. seq_len init added to bi-gru\n\n* feat: add variational dropout usage\n\n* feat: sequence lengths are added to cudnn rnn layers\n\n* doc: docs for variational dropout\n\n* feat: add cudnn_bi_gru usage\n\n* fix: validation_patience in squad config\n\n* feat: add RU SQuAD dataset and config\n\n* fix: upd squad_ru config\n\n* feat: add lr logging for squad\n\n* feat: add weight decay\n\n* fix: upd squad_ru config and weight_decay parameter\n\n* fix: upd squad configs\n\n* docs: upd squad readme\n\n* test: add squad_ru to tests\n\n* fix: tf_layers compatibility for ner\n\n* fix: ontonotes cudnn fix\n\n* fix: initialization fixed\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def attention(inputs, state, att_size, mask, scope=\"attention\"):",
            "\"\"\"Computes weighted sum of inputs conditioned on state\"\"\"",
            "with tf.variable_scope(scope):",
            "u = tf.concat([tf.tile(tf.expand_dims(state, axis=1), [1, tf.shape(inputs)[1], 1]), inputs], axis=2)",
            "-        logits = tf.layers.dense(tf.layers.dense(u, att_size, activation=tf.nn.sigmoid), 1, use_bias=False)",
            "+        logits = tf.layers.dense(tf.layers.dense(u, att_size, activation=tf.nn.tanh), 1, use_bias=False)",
            "logits = softmax_mask(tf.squeeze(logits, [2]), mask)",
            "att_weights = tf.expand_dims(tf.nn.softmax(logits), axis=2)",
            "res = tf.reduce_sum(att_weights * inputs, axis=1)"
        ]
    },
    {
        "number": 76,
        "comments": "remove API call for version fix",
        "commit_message": "Fix examples and tutorials to use the updated tensor API (#886)\n\n* Fix examples and tutorials to use the updated tensor API\n\n* small fixes\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def main(args):",
            "bob_decision = Marginal(Search(bob))",
            "",
            "# Here Alice and Bob slightly prefer one location over the other a priori",
            "-    shared_preference = Variable(torch.Tensor([args.preference]))",
            "+    shared_preference = torch.tensor([args.preference])",
            "",
            "bob_depth = args.depth",
            "num_samples = args.num_samples"
        ]
    },
    {
        "number": 77,
        "comments": "format",
        "commit_message": "Fixing missing var\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "if torch.backends.cudnn.version() >= 7603:",
            "#",
            "# Channels Last support not limited by existing models, as any model can be converted to Channels Last and propagate format through the graph as soon as input formatted correctly.",
            "#",
            "-input = input.to(memory_format=torch.channels_last)",
            "-model = model.to(memory_format=torch.channels_last)",
            "+",
            "+# Need to be done once, after model initialization (or load)",
            "+model = model.to(memory_format=torch.channels_last) # Replace with your model",
            "+",
            "+# Need to be done for every input",
            "+input = input.to(memory_format=torch.channels_last) # Replace with your input",
            "output = model(input)",
            "",
            "#######################################################################"
        ]
    },
    {
        "number": 79,
        "comments": "format",
        "commit_message": "fix an error\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def time_distributed(incoming, fn, args=None, scope=None):",
            "else:",
            "x = [fn(x[i], *args) for i in range(timestep)]",
            "x = map(lambda t: tf.reshape(t, [-1, 1]+utils.get_incoming_shape(t)[1:]), x)",
            "-    return tf.concat(1, x)",
            "\\ No newline at end of file",
            "+    return tf.concat(1, x)"
        ]
    },
    {
        "number": 81,
        "comments": "add API call for type fix",
        "commit_message": "fixed mnist superpixels\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class Planetoid(Dataset):",
            "# Create unweighted sparse adjacency matrix.",
            "weight = torch.ones(index.size(1))",
            "n = input.size(0)",
            "-        adj = torch.sparse.FloatTensor(index, weight, torch.Size([n, n]))",
            "+        adj = SparseTensor(index, weight, torch.Size([n, n]))",
            "",
            "# Bundle graph to data object.",
            "-        self.data = Data(input, adj, position=None, target=target)",
            "+        self.data = Data(input, adj, position=None, target=target.long())",
            "",
            "def __getitem__(self, index):",
            "data = self.data"
        ]
    },
    {
        "number": 84,
        "comments": "add API call for state fix",
        "commit_message": "fixed multiple super\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class Tacotron2(TTSInterface, torch.nn.Module):",
            "",
            "def __init__(self, idim, odim, args):",
            "super(Tacotron2, self).__init__()",
            "+        torch.nn.Module.__init__(self)",
            "# store hyperparameters",
            "self.idim = idim",
            "self.odim = odim"
        ]
    },
    {
        "number": 85,
        "comments": "add condition check for resource fix",
        "commit_message": "Fix universal sentence encoder colab\n\nPiperOrigin-RevId: 387544699\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "\"import sys\\n\",",
            "\"sys.path.append(f'{os.getcwd()}/SentEval')\\n\",",
            "\"\\n\",",
            "-        \"import tensorflow_hub as hub\\n\",",
            "\"import tensorflow as tf\\n\",",
            "+        \"\\n\",",
            "+        \"# Prevent TF from claiming all GPU memory so there is some left for pytorch.\\n\",",
            "+        \"gpus = tf.config.list_physical_devices('GPU')\\n\",",
            "+        \"if gpus:\\n\",",
            "+        \"  # Memory growth needs to be the same across GPUs.\\n\",",
            "+        \"  for gpu in gpus:\\n\",",
            "+        \"    tf.config.experimental.set_memory_growth(gpu, True)\\n\",",
            "+        \"\\n\",",
            "+        \"import tensorflow_hub as hub\\n\",",
            "\"import tensorflow_text\\n\",",
            "\"import senteval\\n\",",
            "\"import time\\n\","
        ]
    },
    {
        "number": 86,
        "comments": "add param for argument fix",
        "commit_message": "Fix conflict\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class Encoder(torch.nn.Module):",
            "self.embed = Conv2dSubsampling(idim, attention_dim, dropout_rate)",
            "elif input_layer == \"embed\":",
            "self.embed = torch.nn.Sequential(",
            "-                torch.nn.Embedding(idim, attention_dim),",
            "+                torch.nn.Embedding(idim, attention_dim, padding_idx=padding_idx),",
            "pos_enc_class(attention_dim, positional_dropout_rate)",
            ")",
            "elif isinstance(input_layer, torch.nn.Module):"
        ]
    },
    {
        "number": 88,
        "comments": "add param for type fix",
        "commit_message": "[Enhance] Added dtype flag to geometry module (#820)\n\n* Added docs and tests\n\n* Fixed bug for batch prob generator\n\n* Fixed typo\n\n* Fixed mypy\n\n* Fixed mypy\n\n* Added warning\n\n* Updated tests for geometry\n\n* Added dtype for conversions\n\n* Completed tests\n\n* Fixed homography\n\n* Fixed pinhole camera\n\n* Fixed tests\n\n* Fix geometry testing bugs\n\n* Fixed augmentation tests\n\n* code updated\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def create_checkerboard(h, w, nw):",
            "",
            "",
            "# TODO: Isn't this function duplicated with eye_like?",
            "-def create_eye_batch(batch_size, eye_size):",
            "+def create_eye_batch(batch_size, eye_size, device=None, dtype=None):",
            "\"\"\"Creates a batch of identity matrices of shape Bx3x3",
            "\"\"\"",
            "-    return torch.eye(eye_size).view(",
            "+    return torch.eye(eye_size, device=device, dtype=dtype).view(",
            "1, eye_size, eye_size).expand(batch_size, -1, -1)"
        ]
    },
    {
        "number": 91,
        "comments": "change condition check for version fix",
        "commit_message": "Move complex tensor related functions to espnet2/enh/layers/complex_utils.py; Refactor complex support code; fix unit tests\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class TransformerSeparator(AbsSeparator):",
            "",
            "# if complex spectrum,",
            "if isinstance(input, ComplexTensor) or (",
            "-            is_torch_1_8_plus and torch.is_complex(input)",
            "+            is_torch_1_9_plus and torch.is_complex(input)",
            "):",
            "feature = abs(input)",
            "else:"
        ]
    },
    {
        "number": 92,
        "comments": "math param update",
        "commit_message": "final fix\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class PipelineTesterMixin(unittest.TestCase):",
            "image_slice = image[0, -1, -3:, -3:].cpu()",
            "",
            "assert image.shape == (1, 3, 32, 32)",
            "-        expected_slice = torch.tensor([-0.5712, -0.6215, -0.5953, -0.5438, -0.4775, -0.4539, -0.5172, -0.4872, -0.5105])",
            "+        expected_slice = torch.tensor(",
            "+            [-0.5712, -0.6215, -0.5953, -0.5438, -0.4775, -0.4539, -0.5172, -0.4872, -0.5105]",
            "+        )",
            "assert (image_slice.flatten() - expected_slice).abs().max() < 1e-2",
            "",
            "@slow"
        ]
    },
    {
        "number": 93,
        "comments": "not clear",
        "commit_message": "fix dense graph conv test\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class DNAConv(MessagePassing):",
            "num_edges = edge_index.size(1)",
            "",
            "edge_index, edge_weight = gcn_norm(edge_index, x.size(self.node_dim),",
            "-                                           edge_weight, self.improved, x.dtype)",
            "+                                           edge_weight, dtype=x.dtype)",
            "",
            "if self.cached:",
            "self._cache = (num_edges, edge_index, edge_weight)"
        ]
    },
    {
        "number": 94,
        "comments": "API update",
        "commit_message": "[style] consistent nn. and nn.functional (#12124)\n\n* consistent nn. and nn.functional\n\n* fix glitch\n\n* fix glitch #2\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class T5Attention(nn.Module):",
            "position_bias = position_bias + mask  # (batch_size, n_heads, seq_length, key_length)",
            "",
            "scores += position_bias",
            "-        attn_weights = F.softmax(scores.float(), dim=-1).type_as(",
            "+        attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(",
            "scores",
            ")  # (batch_size, n_heads, seq_length, key_length)",
            "-        attn_weights = F.dropout(",
            "+        attn_weights = nn.functional.dropout(",
            "attn_weights, p=self.dropout, training=self.training",
            ")  # (batch_size, n_heads, seq_length, key_length)"
        ]
    },
    {
        "number": 95,
        "comments": "update doc",
        "commit_message": "[App] Fix multi-node pytorch example CI (#15753)\n\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class PyTorchDistributed(L.LightningWork):",
            ")",
            "",
            "",
            "-# 32 GPUs: (8 nodes x 4 v 100)",
            "+# 8 GPUs: (2 nodes x 4 v 100)",
            "compute = L.CloudCompute(\"gpu-fast-multi\")  # 4xV100",
            "component = MultiNode(PyTorchDistributed, num_nodes=2, cloud_compute=compute)",
            "app = L.LightningApp(component)"
        ]
    },
    {
        "number": 97,
        "comments": "add API call for type fix",
        "commit_message": "Fix bug with multiple models running in parallels (#43)\n\n* add possibility to select the maximum number of threads per model to be used\n\n* change version\n\nCo-authored-by: morgoth95 <diego.fiori@epfl.ch>\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def _get_ort_session_options() -> ort.SessionOptions:",
            "if not torch.cuda.is_available():",
            "sess_options.execution_mode = ort.ExecutionMode.ORT_PARALLEL",
            "sess_options.inter_op_num_threads = 1",
            "-        sess_options.intra_op_num_threads = max(torch.get_num_threads(), 1)",
            "+        sess_options.intra_op_num_threads = max(",
            "+            int(",
            "+                os.environ.get(\"NEBULLVM_THREADS_PER_MODEL\")",
            "+                or torch.get_num_threads()",
            "+            ),",
            "+            1,",
            "+        )",
            "return sess_options"
        ]
    },
    {
        "number": 98,
        "comments": "typo fix",
        "commit_message": "bug fix\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def save_best_model(model, optimizer, model_loss, best_loss, out_path,",
            "def check_update(model, grad_clip, grad_top):",
            "r'''Check model gradient against unexpected jumps and failures'''",
            "skip_flag = False",
            "-    grad_norm = torch.nn.utils.clip_grad_norm(model.parameters(), grad_clip)",
            "+    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)",
            "if np.isinf(grad_norm):",
            "print(\" | > Gradient is INF !!\")",
            "skip_flag = True"
        ]
    },
    {
        "number": 99,
        "comments": "add condition check for type fix",
        "commit_message": "fix quaternion_exp_to_log test errors\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def quaternion_exp_to_log(quaternion: torch.Tensor,",
            ">>> kornia.quaternion_exp_to_log(quaternion)",
            "tensor([0., 0., 0.])",
            "\"\"\"",
            "-    if not torch.is_tensor(quaternion):",
            "+    if not isinstance(quaternion, torch.Tensor):",
            "raise TypeError(\"Input type is not a torch.Tensor. Got {}\".format(",
            "type(quaternion)))"
        ]
    },
    {
        "number": 100,
        "comments": "add API call for type fix",
        "commit_message": "Replace strided slice with tf.expand_dims (#10078)\n\n* Replace tf.newaxis -> tf.expand_dims\n\n* Fix tests\n\n* Fix tests\n\n* Use reshape when a tensors needs a double expand\n\n* Fix GPT2\n\n* Fix GPT2\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class TFFlaubertMainLayer(tf.keras.layers.Layer):",
            "tensor_normalized = self.layer_norm2[i](tensor)",
            "tensor = tensor + self.ffns[i](tensor_normalized)",
            "",
            "-            tensor = tensor * mask[..., tf.newaxis]",
            "+            tensor = tensor * tf.expand_dims(mask, axis=-1)",
            "",
            "# Add last hidden state",
            "if inputs[\"output_hidden_states\"]:"
        ]
    },
    {
        "number": 101,
        "comments": "change API call for type fix",
        "commit_message": "fix bug in 552c2b3b51c83fc27bdf6df691f1d707e40865f9\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def _replace_global_by_local(kwargs):",
            "if 'collections' in kwargs:",
            "collections = kwargs['collections']",
            "if not collections:",
            "-        collections = set(tf.GraphKeys.GLOBAL_VARIABLES)",
            "+        collections = {tf.GraphKeys.GLOBAL_VARIABLES}",
            "else:",
            "collections = set(collections.copy())",
            "collections.remove(tf.GraphKeys.GLOBAL_VARIABLES)"
        ]
    },
    {
        "number": 103,
        "comments": "change condition check for null fix",
        "commit_message": "fix for pytorch < 1.6 (#6300)\n\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class ReformerLayer(nn.Module):",
            "\"\"\"",
            "# randomize seeds",
            "# use cuda generator if available",
            "-        if len(torch.cuda.default_generators) > 0:",
            "+        if hasattr(torch.cuda, \"default_generators\") and len(torch.cuda.default_generators) > 0:",
            "# GPU",
            "device_idx = torch.cuda.current_device()",
            "self.feed_forward_seed = torch.cuda.default_generators[device_idx].seed()"
        ]
    },
    {
        "number": 104,
        "comments": "add API call for shape fix",
        "commit_message": "Math fixes for 3d affine transformation & doc updates\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def get_projective_transform(center: torch.Tensor, angles: torch.Tensor) -> torc",
            "",
            "# create rotation matrix",
            "angle_axis_rad: torch.Tensor = K.deg2rad(angles)",
            "-    rmat: torch.Tensor = K.angle_axis_to_rotation_matrix(angle_axis_rad)  # Bx3x3",
            "+    rmat: torch.Tensor = K.angle_axis_to_rotation_matrix(angle_axis_rad) * scales.view(-1, 1, 1)  # Bx3x3",
            "",
            "# define matrix to move forth and back to origin",
            "from_origin_mat = torch.eye(4)[None].repeat(rmat.shape[0], 1, 1).type_as(center)  # Bx4x4"
        ]
    },
    {
        "number": 105,
        "comments": "add param for math fix",
        "commit_message": "[RLlib]: Off-Policy Evaluation fixes. (#25899)\n\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class FQETorchModel:",
            "q_values, _ = self.q_model({\"obs\": obs}, [], None)",
            "if actions is not None:",
            "actions = torch.tensor(actions, device=self.device, dtype=int)",
            "-            q_values = torch.gather(q_values, -1, actions.unsqueeze(-1)).squeeze()",
            "+            q_values = torch.gather(q_values, -1, actions.unsqueeze(-1)).squeeze(-1)",
            "return q_values.detach()",
            "",
            "def estimate_v("
        ]
    },
    {
        "number": 106,
        "comments": "refactor fix",
        "commit_message": "pip install espnet[train] (#3755)\n\n* Add pytorch=1.10.0 to CI configuration\n\n* fix:   test/espnet2/bin/test_k2_asr_inference.py\n\n* fix:   espnet2/main_funcs/pack_funcs.py\n\n* pip install espnet[train]\n\n* modified:   ci/install.sh\n\n* fix\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def decode(args):",
            "",
            "# define function for plot prob and att_ws",
            "def _plot_and_save(array, figname, figsize=(6, 4), dpi=150):",
            "+        import matplotlib",
            "+",
            "+        matplotlib.use(\"Agg\")",
            "import matplotlib.pyplot as plt",
            "",
            "shape = array.shape"
        ]
    },
    {
        "number": 107,
        "comments": "add condition check for refactor fix",
        "commit_message": "[RLlib] Fixed import tensorflow when module not available (#16171)\n\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class ModelCatalog:",
            "model_name (str): Name to register the model under.",
            "model_class (type): Python class of the model.",
            "\"\"\"",
            "-        if issubclass(model_class, tf.keras.Model):",
            "-            deprecation_warning(old=\"register_custom_model\", error=False)",
            "+        if tf is not None:",
            "+            if issubclass(model_class, tf.keras.Model):",
            "+                deprecation_warning(old=\"register_custom_model\", error=False)",
            "_global_registry.register(RLLIB_MODEL, model_name, model_class)",
            "",
            "@staticmethod"
        ]
    },
    {
        "number": 109,
        "comments": "remove condition check for shape fix",
        "commit_message": "Refactoring, cleanup, improved test coverage.\n* Add eca_nfnet_l2 weights, 84.7 @ 384x384\n* All 'non-std' (ie transformer / mlp) models have classifier / default_cfg test added\n* Fix #694 reset_classifer / num_features / forward_features / num_classes=0 consistency for transformer / mlp models\n* Add direct loading of npz to vision transformer (pure transformer so far, hybrid to come)\n* Rename vit_deit* to deit_*\n* Remove some deprecated vit hybrid model defs\n* Clean up classifier flatten for conv classifiers and unusual cases (mobilenetv3/ghostnet)\n* Remove explicit model fns for levit conv, just pass in arg\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class DLA(nn.Module):",
            "if self.drop_rate > 0.:",
            "x = F.dropout(x, p=self.drop_rate, training=self.training)",
            "x = self.fc(x)",
            "-        if not self.global_pool.is_identity():",
            "-            x = x.flatten(1)  # conv classifier, flatten if pooling isn't pass-through (disabled)",
            "+        x = self.flatten(x)",
            "return x"
        ]
    },
    {
        "number": 110,
        "comments": "add API call for type fix",
        "commit_message": "[RLlib] Bandit tf2 fix (+ add tf2 to test cases). (#24908)\n\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class OnlineLinearRegression(tf.Module if tf else object):",
            "x = tf.squeeze(x, axis=0)",
            "y = y[0]",
            "self.time += 1",
            "-        self.delta_f += y * x",
            "+        self.delta_f += tf.cast(y, tf.float32) * x",
            "self.delta_b += tf.tensordot(x, x, axes=0)",
            "# Can follow an update schedule if not doing sherman morison updates",
            "if self.time % self.update_schedule == 0:"
        ]
    },
    {
        "number": 112,
        "comments": "add API call for math fix",
        "commit_message": "Fix disabled grads after call to predict (#6657)\n\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class Trainer(",
            "",
            "results = self.predict_loop.on_predict_epoch_end()",
            "self.predict_loop.on_predict_end()",
            "+",
            "+        # re-enable grads",
            "+        torch.set_grad_enabled(True)",
            "+",
            "return results",
            "",
            "def run_sanity_check(self, ref_model):"
        ]
    },
    {
        "number": 113,
        "comments": "add API call for type fix",
        "commit_message": "Augmentation Base Refactor (#2117)\n\n* refactor\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Updated augmentation base.\n\n* Removed label & start apply/inverse\n\n* Finished augmentaion base refactor\n\n* container refactoring\n\n* Added missing files\n\n* Added ops\n\n* Update sequential ops\n\n* Almost there\n\n* Fixed computation matrix computation\n\n* Fixed randomcrop\n\n* Fixed erasing\n\n* almost almost\n\n* finished\n\n* Added missing file\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Bug and typing fix\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Added list typing\n\n* fixed test base\n\n* Fixed typing\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* bug fix\n\n* Update kornia/augmentation/_2d/geometric/crop.py\n\nCo-authored-by: Jo\u00e3o Gustavo A. Amorim <joaogustavoamorim@gmail.com>\n\n* Fixed build-docs\n\n* Fixed bfloat16 issue on torch1.13.1\n\n* Revert the last commit\n\n* Fixed typing\n\n* Fixed typos\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Typo fix\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Jo\u00e3o Gustavo A. Amorim <joaogustavoamorim@gmail.com>\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def filter2d(",
            "input = input.view(-1, tmp_kernel.size(0), input.size(-2), input.size(-1))",
            "",
            "# convolve the tensor with the kernel.",
            "-    output = F.conv2d(input, tmp_kernel, groups=tmp_kernel.size(0), padding=0, stride=1)",
            "+    # NOTE: type(...) to fix getting `torch.bfloat16` type.",
            "+    # TODO: @johnnv1, fix it through the Augmentation Base.",
            "+    output = F.conv2d(input, tmp_kernel, groups=tmp_kernel.size(0), padding=0, stride=1).type(input.dtype)",
            "",
            "if padding == 'same':",
            "out = output.view(b, c, h, w)"
        ]
    },
    {
        "number": 114,
        "comments": "add API call for type fix",
        "commit_message": "small fixes to remainder dtype handling when modulus=False.\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def remainder(",
            "res_floored = torch.where(res >= 0, torch.floor(res), torch.ceil(res))",
            "diff = res - res_floored",
            "diff, x2 = ivy.promote_types_of_inputs(diff, x2)",
            "-        return torch.mul(diff, x2, out=out)",
            "+        return torch.mul(diff, x2, out=out).to(x1.dtype)",
            "return torch.remainder(x1, x2, out=out)"
        ]
    },
    {
        "number": 115,
        "comments": "change param for math fix",
        "commit_message": "fix: action mask parameter\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class GoalOrientedBotNetwork(TFModel):",
            "name='features')",
            "self._action = tf.placeholder(tf.int32, [1, None],",
            "name='ground_truth_action')",
            "-        self._action_mask = tf.placeholder(tf.float32, [1, None, self.n_actions],",
            "+        self._action_mask = tf.placeholder(tf.float32, [None, None, self.n_actions],",
            "name='action_mask')",
            "",
            "def _build_body(self):"
        ]
    },
    {
        "number": 116,
        "comments": "add API call for resource fix",
        "commit_message": "[RLlib] SAC Torch (incl. Atari learning) (#7984)\n\n* Policy-classes cleanup and torch/tf unification.\n- Make Policy abstract.\n- Add `action_dist` to call to `extra_action_out_fn` (necessary for PPO torch).\n- Move some methods and vars to base Policy\n  (from TFPolicy): num_state_tensors, ACTION_PROB, ACTION_LOGP and some more.\n\n* Fix `clip_action` import from Policy (should probably be moved into utils altogether).\n\n* - Move `is_recurrent()` and `num_state_tensors()` into TFPolicy (from DynamicTFPolicy).\n- Add config to all Policy c'tor calls (as 3rd arg after obs and action spaces).\n\n* Add `config` to c'tor call to TFPolicy.\n\n* Add missing `config` to c'tor call to TFPolicy in marvil_policy.py.\n\n* Fix test_rollout_worker.py::MockPolicy and BadPolicy classes (Policy base class is now abstract).\n\n* Fix LINT errors in Policy classes.\n\n* Implement StatefulPolicy abstract methods in test cases: test_multi_agent_env.py.\n\n* policy.py LINT errors.\n\n* Create a simple TestPolicy to sub-class from when testing Policies (reduces code in some test cases).\n\n* policy.py\n- Remove abstractmethod from `apply_gradients` and `compute_gradients` (these are not required iff `learn_on_batch` implemented).\n- Fix docstring of `num_state_tensors`.\n\n* Make QMIX torch Policy a child of TorchPolicy (instead of Policy).\n\n* QMixPolicy add empty implementations of abstract Policy methods.\n\n* Store Policy's config in self.config in base Policy c'tor.\n\n* - Make only compute_actions in base Policy's an abstractmethod and provide pass\nimplementation to all other methods if not defined.\n- Fix state_batches=None (most Policies don't have internal states).\n\n* Cartpole tf learning.\n\n* Cartpole tf AND torch learning (in ~ same ts).\n\n* Cartpole tf AND torch learning (in ~ same ts). 2\n\n* Cartpole tf (torch syntax-broken) learning (in ~ same ts). 3\n\n* Cartpole tf AND torch learning (in ~ same ts). 4\n\n* Cartpole tf AND torch learning (in ~ same ts). 5\n\n* Cartpole tf AND torch learning (in ~ same ts). 6\n\n* Cartpole tf AND torch learning (in ~ same ts). Pendulum tf learning.\n\n* WIP.\n\n* WIP.\n\n* SAC torch learning Pendulum.\n\n* WIP.\n\n* SAC torch and tf learning Pendulum and Cartpole after cleanup.\n\n* WIP.\n\n* LINT.\n\n* LINT.\n\n* SAC: Move policy.target_model to policy.device as well.\n\n* Fixes and cleanup.\n\n* Fix data-format of tf keras Conv2d layers (broken for some tf-versions which have data_format=\"channels_first\" as default).\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* Fix and LINT.\n\n* WIP.\n\n* Test fixes and LINT.\n\n* Fixes and LINT.\n\nCo-authored-by: Sven Mika <sven@Svens-MacBook-Pro.local>\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class EpsilonGreedy(Exploration):",
            "torch.multinomial(random_valid_action_logits, 1), axis=1)",
            "# Pick either random or greedy.",
            "action = torch.where(",
            "-                torch.empty((batch_size, )).uniform_() < epsilon,",
            "+                torch.empty(",
            "+                    (batch_size, )).uniform_().to(self.device) < epsilon,",
            "random_actions, exploit_action)",
            "",
            "return action, action_logp"
        ]
    },
    {
        "number": 117,
        "comments": "no API",
        "commit_message": "[sgd] Extend distributed pytorch functionality (#5675)\n\n* raysgd\n\n* apply fn\n\n* double quotes\n\n* removed duplicate TimerStat\n\n* removed duplicate find_free_port\n\n* imports in pytorch_trainer\n\n* init doc\n\n* ray.experimental\n\n* remove resize example\n\n* resnet example\n\n* cifar\n\n* Fix up after kwargs\n\n* data_dir and dataloader_workers args\n\n* formatting\n\n* loss\n\n* init\n\n* update code\n\n* lint\n\n* smoketest\n\n* better_configs\n\n* fix\n\n* fix\n\n* fix\n\n* train_loader\n\n* fixdocs\n\n* ok\n\n* ok\n\n* fix\n\n* fix_update\n\n* fix\n\n* fix\n\n* done\n\n* fix\n\n* fix\n\n* fix\n\n* small\n\n* lint\n\n* fix\n\n* fix\n\n* fix_test\n\n* fix\n\n* validate\n\n* fix\n\n* fi\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def test_save_and_restore(ray_start_2_cpus, num_replicas):  # noqa: F811",
            "model_creator,",
            "data_creator,",
            "optimizer_creator,",
            "+        loss_creator=lambda config: nn.MSELoss(),",
            "num_replicas=num_replicas)",
            "trainer2.restore(filename)"
        ]
    },
    {
        "number": 118,
        "comments": "add API call for type fix",
        "commit_message": "Fix bug in dist.Delta.expand() type (#1236)\n\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class Delta(TorchDistribution):",
            "",
            "def expand(self, batch_shape):",
            "validate_args = self.__dict__.get('_validate_args')",
            "+        batch_shape = torch.Size(batch_shape)",
            "v = self.v.expand(batch_shape + self.event_shape)",
            "log_density = self.log_density.expand(batch_shape)",
            "return Delta(v, log_density, self.event_dim, validate_args=validate_args)"
        ]
    },
    {
        "number": 119,
        "comments": "add API call for resource fix",
        "commit_message": "fix buffer transfer bug (#2045)\n\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def main():",
            "",
            "pruner = AGP_Pruner(model, configure_list)",
            "model = pruner.compress()",
            "-",
            "+    model = model.to(device)",
            "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)",
            "for epoch in range(10):",
            "pruner.update_epoch(epoch)",
            "print('# Epoch {} #'.format(epoch))",
            "train(model, device, train_loader, optimizer)",
            "test(model, device, test_loader)",
            "-    pruner.export_model('model.pth', 'mask.pth', 'model.onnx', [1, 1, 28, 28])",
            "+    pruner.export_model('model.pth', 'mask.pth', 'model.onnx', [1, 1, 28, 28], device)",
            "",
            "",
            "if __name__ == '__main__':"
        ]
    },
    {
        "number": 120,
        "comments": "change API call for type fix",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class TestBidirectionalLanguageModelTransformer(AllenNlpTestCase):",
            "input_dim=32, hidden_dim=64, num_layers=2",
            ")",
            "",
            "-        mask = torch.ones(3, 6).int()",
            "-        mask[0, 3:] = 0",
            "-        mask[1, 5:] = 0",
            "+        mask = torch.ones(3, 6).bool()",
            "+        mask[0, 3:] = False",
            "+        mask[1, 5:] = False",
            "",
            "forward_mask, backward_mask = transformer_encoder.get_attention_masks(mask)"
        ]
    },
    {
        "number": 121,
        "comments": "add API call for state fix",
        "commit_message": "[release][train] Add `FileLock` to `tensorflow_mnist_example`. (#32712)\n\n* [release][train] Add `FileLock` to `tensorflow_mnist_example`.\n\nSigned-off-by: xwjiang2010 <xwjiang2010@gmail.com>\n\n* fix\n\nSigned-off-by: xwjiang2010 <xwjiang2010@gmail.com>\n\n---------\n\nSigned-off-by: xwjiang2010 <xwjiang2010@gmail.com>\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "from ray.air.config import ScalingConfig",
            "",
            "",
            "def mnist_dataset(batch_size: int) -> tf.data.Dataset:",
            "-    (x_train, y_train), _ = tf.keras.datasets.mnist.load_data()",
            "+    with FileLock(os.path.expanduser(\"~/.mnist_lock\")):",
            "+        (x_train, y_train), _ = tf.keras.datasets.mnist.load_data()",
            "# The `x` arrays are in uint8 and have values in the [0, 255] range.",
            "# You need to convert them to float32 with values in the [0, 1] range.",
            "x_train = x_train / np.float32(255)"
        ]
    },
    {
        "number": 122,
        "comments": "change param for math fix",
        "commit_message": "Fix examples given new tensorflow rnn interface. Update bidirectional rnn to return final state.\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def rnn_model(X, y):",
            "# Given encoding of RNN, take encoding of last step (e.g hidden size of the",
            "# neural network of last step) and pass it as features for logistic",
            "# regression over output classes.",
            "-    return skflow.models.logistic_regression(encoding[-1], y)",
            "+    return skflow.models.logistic_regression(encoding, y)",
            "",
            "classifier = skflow.TensorFlowEstimator(model_fn=rnn_model, n_classes=15,",
            "steps=1000, optimizer='Adam', learning_rate=0.01, continue_training=True)"
        ]
    },
    {
        "number": 125,
        "comments": "doc update",
        "commit_message": "d-vector handling (#1945)\n\n* Update BaseDatasetConfig\n\n- Add dataset_name\n- Chane name to formatter_name\n\n* Update compute_embedding\n\n- Allow entering dataset by args\n- Use released model by default\n- Use the new key format\n\n* Update loading\n\n* Update recipes\n\n* Update other dep code\n\n* Update tests\n\n* Fixup\n\n* Load multiple embedding files\n\n* Fix argument names in dep code\n\n* Update docs\n\n* Fix argument name\n\n* Fix linter\n",
        "label": "no",
        "answer": "no",
        "change": [
            "config.save_json(config_path)",
            "command_train = (",
            "f\"CUDA_VISIBLE_DEVICES='{get_device_id()}'  python TTS/bin/train_tts.py --config_path {config_path}  \"",
            "f\"--coqpit.output_path {output_path} \"",
            "-    \"--coqpit.datasets.0.name ljspeech_test \"",
            "+    \"--coqpit.datasets.0.formatter ljspeech_test \"",
            "\"--coqpit.datasets.0.meta_file_train metadata.csv \"",
            "\"--coqpit.datasets.0.meta_file_val metadata.csv \"",
            "\"--coqpit.datasets.0.path tests/data/ljspeech \""
        ]
    },
    {
        "number": 126,
        "comments": "update API call for version fix",
        "commit_message": "fix deprecation warnings; remove pydoop dependency; update README\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def main_fun(argv, ctx):",
            "grads = average_gradients(tower_grads)",
            "",
            "# Add a summary to track the learning rate.",
            "-      summaries.append(tf.scalar_summary('learning_rate', lr))",
            "+      summaries.append(tf.summary.scalar('learning_rate', lr))",
            "",
            "# Add histograms for gradients.",
            "for grad, var in grads:",
            "if grad is not None:",
            "summaries.append(",
            "-              tf.histogram_summary(var.op.name + '/gradients', grad))",
            "+              tf.summary.histogram(var.op.name + '/gradients', grad))",
            "",
            "# Apply the gradients to adjust the shared variables.",
            "apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)",
            "",
            "# Add histograms for trainable variables.",
            "for var in tf.trainable_variables():",
            "-        summaries.append(tf.histogram_summary(var.op.name, var))",
            "+        summaries.append(tf.summary.histogram(var.op.name, var))",
            "",
            "# Track the moving averages of all trainable variables.",
            "variable_averages = tf.train.ExponentialMovingAverage("
        ]
    },
    {
        "number": 127,
        "comments": "add condition check for resource fix",
        "commit_message": "Fix PyText GPU Test (#378)\n\nSummary:\nPull Request resolved: https://github.com/facebookresearch/pytext/pull/378\n\nA patch to Fix GPU Testing.\n\nReviewed By: hikushalhere\n\nDifferential Revision: D14400231\n\nfbshipit-source-id: 0cf058bf887fc11d2a88e17dc0809157f3f9eb68\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class Trainer(TrainerBase):",
            "",
            "@timing.time(\"Trainer.test\")",
            "def test(self, test_iter, model, metric_reporter: MetricReporter):",
            "+        if cuda.CUDA_ENABLED:",
            "+            model = model.cuda()",
            "+",
            "model.eval()",
            "with torch.no_grad():",
            "test_metric = self._run_epoch("
        ]
    },
    {
        "number": 128,
        "comments": "add condition check for resource fix",
        "commit_message": "fix _setup_devices in case where there is no torch.distributed package in build (#16821)\n\n* fix _setup_devices in case where there is not torch.distributed\n\n* in training_args_sm.py as well\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class TrainingArguments:",
            "@torch_required",
            "def _setup_devices(self) -> \"torch.device\":",
            "logger.info(\"PyTorch: setting up devices\")",
            "-        if torch.distributed.is_initialized() and self.local_rank == -1:",
            "+        if torch.distributed.is_available() and torch.distributed.is_initialized() and self.local_rank == -1:",
            "logger.warning(",
            "\"torch.distributed process group is initialized, but local_rank == -1. \"",
            "\"In order to use Torch DDP, launch your script with `python -m torch.distributed.launch\""
        ]
    },
    {
        "number": 131,
        "comments": "update API call for version fix",
        "commit_message": "Fix deprecated functions warnings\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "with tf.Graph().as_default():",
            "checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")",
            "if not os.path.exists(checkpoint_dir):",
            "os.makedirs(checkpoint_dir)",
            "-        saver = tf.train.Saver(tf.all_variables())",
            "+        saver = tf.train.Saver(tf.global_variables())",
            "",
            "# Write vocabulary",
            "vocab_processor.save(os.path.join(out_dir, \"vocab\"))",
            "",
            "# Initialize all variables",
            "-        sess.run(tf.initialize_all_variables())",
            "+        sess.run(tf.global_variables_initializer())",
            "",
            "def train_step(x_batch, y_batch):",
            "\"\"\""
        ]
    },
    {
        "number": 132,
        "comments": "add param for type fix",
        "commit_message": "Not use -1e4 as attn mask (#17306)\n\n* Use torch.finfo(self.dtype).min\n\n* for GPTNeoX\n\n* for Albert\n\n* For Splinter\n\n* Update src/transformers/models/data2vec/modeling_data2vec_audio.py\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* fix -inf used in Bart-like models\n\n* Fix a few remaining -inf\n\n* more fix\n\n* clean up\n\n* For CLIP\n\n* For FSMT\n\n* clean up\n\n* fix test\n\n* Add dtype argument and use it for LayoutLMv3\n\n* update FlaxLongT5Attention\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class CLIPTextTransformer(nn.Module):",
            "attentions=encoder_outputs.attentions,",
            ")",
            "",
            "-    def _build_causal_attention_mask(self, bsz, seq_len):",
            "+    def _build_causal_attention_mask(self, bsz, seq_len, dtype):",
            "# lazily create causal attention mask, with full attention between the vision tokens",
            "# pytorch uses additive attention mask; fill with -inf",
            "-        mask = torch.empty(bsz, seq_len, seq_len)",
            "-        mask.fill_(torch.tensor(float(\"-inf\")))",
            "+        mask = torch.empty(bsz, seq_len, seq_len, dtype=dtype)",
            "+        mask.fill_(torch.tensor(torch.finfo(dtype).min))",
            "mask.triu_(1)  # zero out the lower diagonal",
            "mask = mask.unsqueeze(1)  # expand mask",
            "return mask"
        ]
    },
    {
        "number": 134,
        "comments": "format",
        "commit_message": "enable black in the precommit (#1777)\n\n* enable black in the precommit\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add some fixes\n\n* update libface detection url\n\n* added url from kornia checkpoint\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class RandomPerspective(GeometricAugmentationBase2D):",
            "size: Optional[Tuple[int, int]] = None,",
            ") -> Tensor:",
            "return self.apply_transform(",
            "-            input, params=self._params, transform=torch.as_tensor(transform, device=input.device, dtype=input.dtype),",
            "-            flags=flags",
            "+            input,",
            "+            params=self._params,",
            "+            transform=torch.as_tensor(transform, device=input.device, dtype=input.dtype),",
            "+            flags=flags,",
            ")"
        ]
    },
    {
        "number": 135,
        "comments": "update API call for version fix",
        "commit_message": "Fix examples and tutorials to use the updated tensor API (#886)\n\n* Fix examples and tutorials to use the updated tensor API\n\n* small fixes\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def get_mini_batch(mini_batch_indices, sequences, seq_lengths, cuda=False):",
            "# get mask for mini-batch",
            "mini_batch_mask = get_mini_batch_mask(mini_batch, sorted_seq_lengths)",
            "",
            "-    # wrap in PyTorch Variables",
            "-    mini_batch = Variable(torch.Tensor(mini_batch))",
            "-    mini_batch_reversed = Variable(torch.Tensor(mini_batch_reversed))",
            "-    mini_batch_mask = Variable(torch.Tensor(mini_batch_mask))",
            "+    # wrap in PyTorch Tensors",
            "+    mini_batch = torch.tensor(mini_batch)",
            "+    mini_batch_reversed = torch.tensor(mini_batch_reversed)",
            "+    mini_batch_mask = torch.tensor(mini_batch_mask)",
            "",
            "# cuda() here because need to cuda() before packing",
            "if cuda:"
        ]
    },
    {
        "number": 136,
        "comments": "doc update",
        "commit_message": "CDN urls (#4030)\n\n* [file_utils] use_cdn + documentation\n\n* Move to cdn. urls for weights\n\n* [urls] Hotfix for bert-base-japanese\n",
        "label": "no",
        "answer": "no",
        "change": [
            "logger = logging.getLogger(__name__)",
            "# for the pretrained weights provided with the models",
            "####################################################",
            "XXX_PRETRAINED_MODEL_ARCHIVE_MAP = {",
            "-    \"xxx-base-uncased\": \"https://s3.amazonaws.com/models.huggingface.co/bert/xxx-base-uncased-pytorch_model.bin\",",
            "-    \"xxx-large-uncased\": \"https://s3.amazonaws.com/models.huggingface.co/bert/xxx-large-uncased-pytorch_model.bin\",",
            "+    \"xxx-base-uncased\": \"https://cdn.huggingface.co/xxx-base-uncased-pytorch_model.bin\",",
            "+    \"xxx-large-uncased\": \"https://cdn.huggingface.co/xxx-large-uncased-pytorch_model.bin\",",
            "}"
        ]
    },
    {
        "number": 137,
        "comments": "change API call for version fix",
        "commit_message": "Abstract accelerator (step 3) (#2677)\n\n* Integrate accelerator abstraction interface into deepspeed/\n\n* Fix error message in fp16/fused_optimizer\n\n* fix error message in fp16/unfused_optimizer.py\n\n* assign get_accelerator().pin_memory() result to input Tensor name\n\n* no need to check cuda and whether nvtx supported\n\n* move try-except into inner most block\n\n* call Event() and Stream() in get_accelerator() for data type\n\n* Make Stream and Event as properties of abstract interface so they can be used as data type in deepspeed\n\n* Apply op_builder backend api change from #2705 from @jeffra\n\n* fix tests where Builder NAME is used\n\n* keep original ...Builder.NAME interface instead of ...Builder().NAME interface\n\n* fix builder closure for installation\n\n* fix randomltd builder\n\n* add comments to clarify create_op_builder and get_op_builder\n\n* fix compatibility with pip install -e\n\nCo-authored-by: Cheng Li <pistasable@gmail.com>\nCo-authored-by: Olatunji Ruwase <olruwase@microsoft.com>\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class FlopsProfiler(object):",
            "start_time_hook)",
            "",
            "def end_time_hook(module, input, output):",
            "-                torch.cuda.synchronize()",
            "+                get_accelerator().synchronize()",
            "module.__duration__ += time.time() - module.__start_time__",
            "",
            "if not hasattr(module, \"__end_time_hook_handle__\"):"
        ]
    },
    {
        "number": 138,
        "comments": "format",
        "commit_message": "BatchNorm2D -> BatchNorm2d (#558)\n\n* BatchNorm2D -> BatchNorm2d\n\n* Fix typo\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class TestOpt(unittest.TestCase):",
            "assert len(GlobalCounters.cache) == 2, \"optimizer didn't fold conv/relu\"",
            "",
            "if __name__ == '__main__':",
            "-  unittest.main()",
            "\\ No newline at end of file",
            "+  unittest.main()"
        ]
    },
    {
        "number": 139,
        "comments": "format",
        "commit_message": "Update GP use the new Parameterized class (#1621)\n\n* add ..nparameterized\n\n* noise constraint positive\n\n* move parameterized to gp\n\n* fix docs\n\n* add helper train and use it\n\n* make scrub tutorials\n\n* make docs\n\n* use full path for SVI TraceELBO\n\n* change test_benckmark too\n\n* resolve merge conflict\n\n* fix bug at svdkl and other docs\n\n* address comments\n\n* raise NotImplementError\n\n* remove name and get_param\n\n* filling missing stuffs\n\n* fix remaining tests\n\n* fix all nits\n\n* update tutorial\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class Brownian(Kernel):",
            "",
            "Zt = Z.t()",
            "return torch.where(X.sign() == Zt.sign(),",
            "-                           variance * torch.min(X.abs(), Zt.abs()),",
            "+                           self.variance * torch.min(X.abs(), Zt.abs()),",
            "X.data.new_zeros(X.size(0), Z.size(0)))"
        ]
    },
    {
        "number": 140,
        "comments": "change API call for math fix",
        "commit_message": "fix seed generator\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class PaintByExample(DiffusionInpaintModel):",
            "mask: [H, W, 1] 255 means area to repaint",
            "return: BGR IMAGE",
            "\"\"\"",
            "-        set_seed(config.paint_by_example_seed)",
            "-",
            "output = self.model(",
            "image=PIL.Image.fromarray(image),",
            "mask_image=PIL.Image.fromarray(mask[:, :, -1], mode=\"L\"),",
            "example_image=config.paint_by_example_example_image,",
            "num_inference_steps=config.paint_by_example_steps,",
            "output_type='np.array',",
            "+            generator=torch.manual_seed(config.paint_by_example_seed)",
            ").images[0]",
            "",
            "output = (output * 255).round().astype(\"uint8\")"
        ]
    },
    {
        "number": 141,
        "comments": "format",
        "commit_message": "prepare for \"__floordiv__ is deprecated  and its behavior will change in a future version of pytorch\" (#20211)\n\n* rounding_mode = \"floor\"  instead of // to prevent behavioral change\n\n* add other TODO\n\n* use `torch_int_div` from pytrch_utils\n\n* same for tests\n\n* fix copies\n\n* style\n\n* use relative imports when needed\n\n* Co-authored-by: sgugger <sylvain.gugger@gmail.com>\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "class BigBirdPegasusBlockSparseAttention(nn.Module):",
            "num_indices_to_gather = indices.shape[-2] * indices.shape[-1]",
            "num_indices_to_pick_from = params.shape[2]",
            "",
            "-        indices_shift = (",
            "-            torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)",
            "-            // num_indices_to_gather",
            "-            * num_indices_to_pick_from",
            "-        )",
            "+        shift = torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)",
            "+        indices_shift = torch_int_div(shift, num_indices_to_gather) * num_indices_to_pick_from",
            "",
            "flattened_indices = indices.view(-1) + indices_shift",
            "flattened_params = params.reshape(-1, params.shape[-2], params.shape[-1])"
        ]
    },
    {
        "number": 142,
        "comments": "use custom method for function",
        "commit_message": "fix dataset purge method\nmove publish_obj.exists assert statement before .get\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "def test_benchmark_datasets() -> None:",
            "assert benchmark_report[key_size][\"publish_secs\"] <= timeout",
            "",
            "print(\"purge datasets...\")",
            "-    domain.datasets.purge(skip_checks=True)",
            "+    clean_datasets_on_domain(DOMAIN1_PORT)"
        ]
    },
    {
        "number": 143,
        "comments": "add print",
        "commit_message": "Removed mannually adapting the preprocessing layers (#1016)\n\n* remove adapt\n\n* fix\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def test_feature_encoder_layer():",
            "",
            "model2 = tf.keras.Model(input_node, hidden_node)",
            "result = model2.predict(data)",
            "+    print(result)",
            "assert result[0][0] == result[2][0]",
            "assert result[0][0] != result[1][0]",
            "assert result[0][1] != result[1][1]"
        ]
    },
    {
        "number": 144,
        "comments": "class name change",
        "commit_message": "upgrade to pytorch 0.4.0 (#1126)\n\n* bump pytorch to 0.4 + fix sanitize\n\n* remove check for Variable in block_orthogonal\n\n* rename parameter split_size=\n\n* remove checks for Variable\n\n* fix more tests\n\n* fixes\n\n* get tests to pass\n\n* fix warnings\n\n* get rid of some of the Variables\n\n* more tests passing\n\n* more elimination of variables\n\n* finish removing all Variables\n\n* pylint and such\n\n* a few fixes\n\n* move torch.no_grad into model.forward_on_instances\n\n* more pytorch 0.4 changes\n\n* detach() -> data\n\n* pylint\n\n* fix bad tensor creation\n\n* fix types\n\n* remove print statement\n\n* more 0.4 goodness\n\n* factor out is_tensor\n\n* add no_grad to elmo command\n\n* cleanup\n\n* remove TODO\n\n* address PR feedback\n\n* replace all() with item()\n\n* more cleanup\n\n* further cleanup\n\n* remove Variable\n\n* really fix merge conflict\n\n* fix pylint\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class F1Measure(Metric):",
            "raise ConfigurationError(\"A gold label passed to F1Measure contains an id >= {}, \"",
            "\"the number of classes.\".format(num_classes))",
            "if mask is None:",
            "-            mask = ones_like(gold_labels)",
            "+            mask = torch.ones_like(gold_labels)",
            "mask = mask.float()",
            "gold_labels = gold_labels.float()",
            "positive_label_mask = gold_labels.eq(self._positive_label).float()"
        ]
    },
    {
        "number": 148,
        "comments": "more return",
        "commit_message": "edge index bugfix in tu read\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def compute_slices(dataset, batch):",
            "y_slice = node_slice if dataset.y.size(0) == num_nodes else graph_slice",
            "slices['y'] = y_slice",
            "",
            "-    return slices",
            "+    return dataset, slices"
        ]
    },
    {
        "number": 149,
        "comments": "change class name",
        "commit_message": "Fix symbol which no longer exists in tf2 root.\n\nThis was leading to a weird error if the filesystem threw an OpError\nwhen finding out if a path exists.\n\nPiperOrigin-RevId: 303131046\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def load_module_spec(path):",
            "",
            "Raises:",
            "ValueError: on unexpected values in the module spec.",
            "-    tf.OpError: on file handling exceptions.",
            "+    tf.errors.OpError: on file handling exceptions.",
            "\"\"\"",
            "path = registry.resolver(path)",
            "return registry.loader(path)"
        ]
    },
    {
        "number": 151,
        "comments": "add API call for type fix",
        "commit_message": "Remove duplicated functional of gp (#1607)\n\n* clean gp\n\n* rearange sgpr\n\n* kernel add -> sum in test_benchmark\n\n* nit\n\n* fix error\n\n* fix error during clean\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def main(args):",
            "# CNN will transform a high dimension image into a low dimension 2D tensors for RBF kernel.",
            "# This kernel accepts inputs are inputs of CNN and gives outputs are covariance matrix of RBF on",
            "# outputs of CNN.",
            "-    kernel = gp.kernels.RBF(input_dim=10, lengthscale=torch.ones(10)).warp(iwarping_fn=cnn_fn)",
            "+    kernel = gp.kernels.Warp(gp.kernels.RBF(input_dim=10, lengthscale=torch.ones(10)),",
            "+                             iwarping_fn=cnn_fn)",
            "",
            "# init inducing points (taken randomly from dataset)",
            "Xu = next(iter(train_loader))[0][:args.num_inducing]"
        ]
    },
    {
        "number": 153,
        "comments": "add param for type fix",
        "commit_message": "fixing torch backend matrix_rank helper to pass array-api test\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def linspace_helper(start, stop, num, axis=None, *, device):",
            "else:",
            "res = [linspace_method(start, stp, num, device=device) for stp in stop]",
            "else:",
            "-        return linspace_method(start, stop, num, device=device)",
            "+        return linspace_method(start, stop, num, dtype=torch.float64, device=device)",
            "res = torch.cat(res, -1).reshape(sos_shape + [num])",
            "if axis is not None:",
            "res = torch.transpose(res, axis, -1)"
        ]
    },
    {
        "number": 154,
        "comments": "add custom function",
        "commit_message": "Allow saving / loading checkpoints from cloud paths (#683)\n\n* Allow saving / loading checkpoints from cloud paths\n\nAllows saving and loading checkpoints directly from cloud paths like\nAmazon S3 (s3://) and Google Cloud Storage (gs://) by using fsspec.\n\nNote: The user will have to install the relevant dependency for each\nprotocol. Otherwise fsspec will fail and specify which dependency is\nmissing.\n\n* Append suffix _fsspec to save/load function names\n\n* Add a lower bound to the fsspec dependency\n\nSkips the 0 major version.\n\n* Add missing changes from refactor\n\n* Use fsspec for remaining artifacts\n\n* Add test case with path requiring fsspec\n\n* Avoid writing logs to file unless output_path is local\n\n* Document the possibility of using paths supported by fsspec\n\n* Fix style and lint\n\n* Add missing lint fixes\n\n* Add type annotations to new functions\n\n* Use Coqpit method for converting config to dict\n\n* Fix type annotation in semi-new function\n\n* Add return type for load_fsspec\n\n* Fix bug where fs not always created\n\n* Restore the experiment removal functionality\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def save_best_model(model, optimizer, criterion, model_loss, best_loss, out_path",
            "bestmodel_path = \"best_model.pth.tar\"",
            "bestmodel_path = os.path.join(out_path, bestmodel_path)",
            "print(\"\\n > BEST MODEL ({0:.5f}) : {1:}\".format(model_loss, bestmodel_path))",
            "-        torch.save(state, bestmodel_path)",
            "+        save_fsspec(state, bestmodel_path)",
            "return best_loss"
        ]
    },
    {
        "number": 156,
        "comments": "add API call for state fix",
        "commit_message": "bugfix for unittests\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class UnittestBase(object):",
            "datetime.now().strftime('%H:%M:%S'), self.__class__.__name__[4:], name",
            "))",
            "sys.stdout.flush()",
            "+        tf.compat.v1.reset_default_graph()",
            "",
            "def finished_test(self, assertion=None):",
            "\"\"\""
        ]
    },
    {
        "number": 158,
        "comments": "change param for math fix",
        "commit_message": "Not use -1e4 as attn mask (#17306)\n\n* Use torch.finfo(self.dtype).min\n\n* for GPTNeoX\n\n* for Albert\n\n* For Splinter\n\n* Update src/transformers/models/data2vec/modeling_data2vec_audio.py\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* fix -inf used in Bart-like models\n\n* Fix a few remaining -inf\n\n* more fix\n\n* clean up\n\n* For CLIP\n\n* For FSMT\n\n* clean up\n\n* fix test\n\n* Add dtype argument and use it for LayoutLMv3\n\n* update FlaxLongT5Attention\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class GPTNeoXModel(GPTNeoXPreTrainedModel):",
            "# Since we are adding it to the raw scores before the softmax, this is",
            "# effectively the same as removing these entirely.",
            "attention_mask = attention_mask.to(dtype=self.dtype)  # fp16 compatibility",
            "-            attention_mask = (1.0 - attention_mask) * -10000.0",
            "+            attention_mask = (1.0 - attention_mask) * torch.finfo(self.dtype).min",
            "",
            "# Prepare head mask if needed",
            "# 1.0 in head_mask indicate we keep the head"
        ]
    },
    {
        "number": 159,
        "comments": "no API used",
        "commit_message": "batch and test fixed\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "class PatchAffineShapeEstimator(nn.Module):",
            "\"input shape should be must be [Bx1x{}x{}]. \"",
            "\"Got {}\".format(self.patch_size, self.patch_size, patch.size()))",
            "self.weighting = self.weighting.to(patch.dtype).to(patch.device)",
            "-        grads: torch.Tensor = self.gradient(patch)",
            "+        grads: torch.Tensor = self.gradient(patch) * self.weighting",
            "# unpack the edges",
            "gx: torch.Tensor = grads[:, :, 0]",
            "gy: torch.Tensor = grads[:, :, 1]"
        ]
    },
    {
        "number": 160,
        "comments": "add param for argument fix",
        "commit_message": "fix minor bug\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def compute_tf_latency(",
            "with tf.device(device):",
            "for _ in range(steps):",
            "starting_time = time.time()",
            "-            _ = model(x)",
            "+            _ = model(*xs)",
            "latencies.append(time.time() - starting_time)",
            "latency = sum(latencies) / steps",
            "return latency, latencies"
        ]
    },
    {
        "number": 162,
        "comments": "format",
        "commit_message": "Use f-strings in the dataset scripts (#3291)\n\n* Finishes #3257\n\nUsed f-strings to format the .py files in the dataset folder\n\n* Fix style\n\n* Fix hkcancor dataset\n\nCo-authored-by: Mario \u0160a\u0161ko <mario@huggingface.co>\nCo-authored-by: mariosasko <mariosasko777@gmail.com>\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class ArxivDataset(datasets.GeneratorBasedBuilder):",
            "path_to_manual_file = os.path.join(os.path.abspath(os.path.expanduser(dl_manager.manual_dir)), _FILENAME)",
            "if not os.path.exists(path_to_manual_file):",
            "raise FileNotFoundError(",
            "-                \"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('arxiv_dataset', data_dir=...)` that includes a file name {}. Manual download instructions: {})\".format(",
            "-                    path_to_manual_file, _FILENAME, self.manual_download_instructions",
            "-                )",
            "+                \"{path_to_manual_file} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('arxiv_dataset', data_dir=...)` that includes a file name {_FILENAME}. Manual download instructions: {self.manual_download_instructions})\"",
            ")",
            "return [datasets.SplitGenerator(name=datasets.Split.TRAIN, gen_kwargs={\"path\": path_to_manual_file})]"
        ]
    },
    {
        "number": 163,
        "comments": "add param for resource fix",
        "commit_message": "Hub device mismatch bug fix (#1619)\n\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def non_max_suppression(prediction, conf_thres=0.1, iou_thres=0.6, classes=None,",
            "merge = False  # use merge-NMS",
            "",
            "t = time.time()",
            "-    output = [torch.zeros(0, 6)] * prediction.shape[0]",
            "+    output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]",
            "for xi, x in enumerate(prediction):  # image index, image inference",
            "# Apply constraints",
            "# x[((x[..., 2:4] < min_wh) | (x[..., 2:4] > max_wh)).any(1), 4] = 0  # width-height"
        ]
    },
    {
        "number": 164,
        "comments": "doc update",
        "commit_message": "Doc styler v2 (#14950)\n\n* New doc styler\n\n* Fix issue with args at the start\n\n* Code sample fixes\n\n* Style code examples in MDX\n\n* Fix more patterns\n\n* Typo\n\n* Typo\n\n* More patterns\n\n* Do without black for now\n\n* Get more info in error\n\n* Docstring style\n\n* Re-enable check\n\n* Quality\n\n* Fix add_end_docstring decorator\n\n* Fix docstring\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def glue_convert_examples_to_features(",
            "output_mode: String indicating the output mode. Either `regression` or `classification`",
            "",
            "Returns:",
            "-        If the `examples` input is a `tf.data.Dataset`, will return a `tf.data.Dataset` containing the",
            "-        task-specific features. If the input is a list of `InputExamples`, will return a list of task-specific",
            "-        `InputFeatures` which can be fed to the model.",
            "+        If the `examples` input is a `tf.data.Dataset`, will return a `tf.data.Dataset` containing the task-specific",
            "+        features. If the input is a list of `InputExamples`, will return a list of task-specific `InputFeatures` which",
            "+        can be fed to the model.",
            "",
            "\"\"\"",
            "warnings.warn(DEPRECATION_WARNING.format(\"function\"), FutureWarning)"
        ]
    },
    {
        "number": 165,
        "comments": "add layer note clear",
        "commit_message": "small fix\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class ESPnetUASRModel(AbsESPnetModel):",
            "#  e.g. STFT and Feature extract",
            "#       data_loader may send time-domain signal in this case",
            "# speech (Batch, NSamples) -> feats: (Batch, NFrames, Dim)",
            "+            speech = F.layer_norm(speech, speech.shape)",
            "feats, feats_lengths = self.frontend(speech, speech_lengths)",
            "else:",
            "# No frontend and no feature extract (usually with pre-extracted feat)"
        ]
    },
    {
        "number": 166,
        "comments": "add param for argument fix",
        "commit_message": "Fix DARTS 2nd order (#4385)\n\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class DartsTrainer(BaseOneShotTrainer):",
            "p += e * d",
            "",
            "_, loss = self._logits_and_loss(trn_X, trn_y)",
            "-            dalphas.append(torch.autograd.grad(loss, [c.alpha for c in self.nas_modules]))",
            "+            dalphas.append(torch.autograd.grad(loss, [c.alpha for _, c in self.nas_modules]))",
            "",
            "dalpha_pos, dalpha_neg = dalphas  # dalpha { L_trn(w+) }, # dalpha { L_trn(w-) }",
            "hessian = [(p - n) / (2. * eps) for p, n in zip(dalpha_pos, dalpha_neg)]"
        ]
    },
    {
        "number": 167,
        "comments": "update API call for refactor fix",
        "commit_message": "Fixed subract fn (#4068)\n\nCo-authored-by: abdrahmandiab <abdrahmandiab99@gmail.com>\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def subtract(",
            "out: Optional[Union[tf.Tensor, tf.Variable]] = None,",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "x1, x2 = ivy.promote_types_of_inputs(x1, x2)",
            "-    return tf.subtract(x1, x2)",
            "+    return tf.experimental.numpy.subtract(x1, x2)",
            "",
            "",
            "def tan("
        ]
    },
    {
        "number": 168,
        "comments": "no API",
        "commit_message": "Refactor model summary + generalize example input array (#1773)\n\n* squash\n\nvariant a\n\n\nvariant b\n\n\nadd test\n\n\nrevert rename\n\n\nadd changelog\n\n\ndocs\n\n\nmove changelog entry to top\n\n\nuse hooks\n\n\nwip\n\n\nwipp\n\n\nlayer summary\n\n\nclean up, refactor\n\n\ntype hints\n\n\nrename\n\n\nremove obsolete code\n\n\nrename\n\n\nunused imports\n\n\nsimplify formatting of table and increase readability\n\n\ndoctest\n\n\nsuperclass object\n\n\nupdate examples\n\n\nprint unknown sizes\n\n\nmore docs and doctest\n\n\ntesting\n\n\nunknown layers\n\n\nadd rnn test\n\n\nremove main\n\n\nrestore train mode\n\n\ntest device wip\n\n\ndevice\n\n\nconstant\n\n\nsimplify model forward transfer\n\n\nreturn summary object in method\n\n\nextend tests\n\n\nfix summary for empty module\n\n\nextend tests\n\n\nrefactor and added hook\n\n\nvariant a\n\n\nvariant b\n\n\nadd test\n\n\nrevert rename\n\n\nadd changelog\n\n\ndocs\n\n\nmove changelog entry to top\n\n\nremove hardcoded string\n\n\nsimplify\n\n\ntest unknown shapes and all others\n\n\ncomments for tests\n\n\nfix hparams attribute\n\n* update default\n\n* unused import\n\n* clean up\n\n* replace hardcoded strings\n\n* fix doctest\n\n* fix top/full\n\n* black\n\n* fix rnn test\n\n* fix rnn\n\n* update debugging docs\n\n\nupdate docs\n\n\ntypo\n\n\nupdate docs\n\n\nupdate docs\n\n* add changelog\n\n* extract constant\n\n* setter and getter\n\n* move parity models to test folder\n\n* parameterize mode\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class LightningTemplateModel(LightningModule):",
            "self.c_d2 = nn.Linear(in_features=self.hidden_dim,",
            "out_features=self.out_features)",
            "",
            "+        self.example_input_array = torch.zeros(2, 1, 28, 28)",
            "+",
            "def forward(self, x):",
            "\"\"\"",
            "No special modification required for Lightning, define it as you normally would"
        ]
    },
    {
        "number": 169,
        "comments": "update API call for version fix",
        "commit_message": "Fix compatibility with 1.12 (#17925)\n\n* Fix compatibility with 1.12\n\n* Remove pin from examples requirements\n\n* Update torch scatter version\n\n* Fix compatibility with 1.12\n\n* Remove pin from examples requirements\n\n* Update torch scatter version\n\n* fix torch.onnx.symbolic_opset12 import\n\n* Reject bad version\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class XDropout(torch.autograd.Function):",
            "# Once https://github.com/pytorch/pytorch/issues/78391 is fixed, do something like:",
            "# if opset_version < 12:",
            "#   return torch.onnx.symbolic_opset9.dropout(g, input, dropout_p, train)",
            "-        return torch.onnx.symbolic_opset12.dropout(g, input, dropout_p, train)",
            "+        return symbolic_opset12.dropout(g, input, dropout_p, train)",
            "",
            "",
            "# Copied from transformers.models.deberta.modeling_deberta.StableDropout"
        ]
    },
    {
        "number": 170,
        "comments": "change condition check for version fix",
        "commit_message": "Use packaging to handle versions (#2777)\n\n* Get Python version from platform module\n\n* Set PY_VERSION as version class\n\n* Set PYARROW_VERSION as version class\n\n* Set TORCH_VERSION as version class\n\n* Set TF_VERSION as version class\n\n* Set JAX_VERSION as version class\n\n* Set BEAM_VERSION as version class\n\n* Set RARFILE_VERSION as version class\n\n* Use version class to validate PyArrow version at import\n\n* Use version class in SCRIPTS_VERSION at import\n\n* Use config.PYARROW_VERSION for parquet submodules\n\n* Fix style\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class Parquet(datasets.ArrowBasedBuilder):",
            "BUILDER_CONFIG_CLASS = ParquetConfig",
            "",
            "def _info(self):",
            "-        if version.parse(pa.__version__) < version.parse(\"3.0.0\"):",
            "+        if datasets.config.PYARROW_VERSION.major < 3:",
            "raise ImportError(",
            "\"PyArrow >= 3.0.0 is required to used the Parquet dataset builder: pip install --upgrade pyarrow\"",
            ")"
        ]
    },
    {
        "number": 171,
        "comments": "add API call for state fix",
        "commit_message": "[bug-fix] Trainer.test points to latest best_model_path (#5161)\n\n* resolve bug\n\n* update code\n\n* add set -e\n\n* Update pytorch_lightning/callbacks/model_checkpoint.py\n\nCo-authored-by: Adrian W\u00e4lchli <aedu.waelchli@gmail.com>\n\n* update test\n\n* Update tests/checkpointing/test_trainer_checkpoint.py\n\nCo-authored-by: Sean Naren <sean.narenthiran@gmail.com>\n\n* Update tests/checkpointing/test_trainer_checkpoint.py\n\nCo-authored-by: Carlos Mochol\u00ed <carlossmocholi@gmail.com>\n\n* update on comments\n\n* resolve test\n\n* convert to set\n\n* update\n\n* add error triggering\n\n* update\n\n* update on comments\n\n* update\n\n* resolve import\n\n* update\n\n* update\n\n* Update pytorch_lightning/plugins/rpc_plugin.py\n\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\n\n* update\n\nCo-authored-by: Adrian W\u00e4lchli <aedu.waelchli@gmail.com>\nCo-authored-by: Sean Naren <sean.narenthiran@gmail.com>\nCo-authored-by: Carlos Mochol\u00ed <carlossmocholi@gmail.com>\nCo-authored-by: Ubuntu <ubuntu@ip-172-31-62-109.ec2.internal>\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\n\n(cherry picked from commit d5b367871fa3924090ec74bf903bd172bd3e2343)\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class RPCPlugin(DDPPlugin):",
            "world_size: int) -> None:",
            "os.environ['MASTER_PORT'] = os.getenv('RPC_MASTER_PORT', '15000')",
            "rpc.init_rpc(f\"worker{global_rank}\", rank=global_rank, world_size=world_size)",
            "+        rpc._set_rpc_timeout(self.rpc_timeout_sec)",
            "self.rpc_initialized = True",
            "",
            "def rpc_save_model(self,"
        ]
    },
    {
        "number": 172,
        "comments": "add param for type fix",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class SimpleSeq2SeqTest(ModelTestCase):",
            "state = self.model._init_decoder_state(state)",
            "batch_size = state[\"source_mask\"].size()[0]",
            "start_predictions = state[\"source_mask\"].new_full(",
            "-            (batch_size,), fill_value=self.model._start_index",
            "+            (batch_size,), fill_value=self.model._start_index, dtype=torch.long",
            ")",
            "all_top_k_predictions, _ = beam_search.search(",
            "start_predictions, state, self.model.take_step"
        ]
    },
    {
        "number": 173,
        "comments": "format",
        "commit_message": "ci: Redo `format.sh --all` script & backfill lint fixes (#9956)\n\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def pg_tf_loss(policy, model, dist_class, train_batch):",
            "logits, _ = model.from_batch(train_batch)",
            "action_dist = dist_class(logits, model)",
            "return -tf.reduce_mean(",
            "-        action_dist.logp(train_batch[SampleBatch.ACTIONS]) *",
            "-        tf.cast(train_batch[Postprocessing.ADVANTAGES], dtype=tf.float32))",
            "+        action_dist.logp(train_batch[SampleBatch.ACTIONS]) * tf.cast(",
            "+            train_batch[Postprocessing.ADVANTAGES], dtype=tf.float32))",
            "",
            "",
            "PGTFPolicy = build_tf_policy("
        ]
    },
    {
        "number": 174,
        "comments": "remove condition check for resource fix",
        "commit_message": "`enable_model_cpu_offload` (#2285)\n\n* enable_model_offload PoC\n\nIt's surprisingly more involved than expected, see comments in the PR.\n\n* Rename final_offload_hook\n\n* Invoke the vae forward hook manually.\n\n* Completely remove decoder.\n\n* Style\n\n* apply_forward_hook decorator\n\n* Rename method.\n\n* Style\n\n* Copy enable_model_cpu_offload\n\n* Fix copies.\n\n* Remove comment.\n\n* Fix copies\n\n* Missing import\n\n* Fix doc-builder style.\n\n* Merge main and fix again.\n\n* Add docs\n\n* Fix docs.\n\n* Add a couple of tests.\n\n* style\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class StableDiffusionDepth2ImgPipeline(DiffusionPipeline):",
            "`pipeline.enable_sequential_cpu_offload()` the execution device can only be inferred from Accelerate's module",
            "hooks.",
            "\"\"\"",
            "-        if self.device != torch.device(\"meta\") or not hasattr(self.unet, \"_hf_hook\"):",
            "+        if not hasattr(self.unet, \"_hf_hook\"):",
            "return self.device",
            "for module in self.unet.modules():",
            "if ("
        ]
    },
    {
        "number": 175,
        "comments": "format",
        "commit_message": "Fix CUDA tests on dev (#1610)\n\n* Fix CUDA tests on dev\n\n* fix distn tests; skip  nef test on cuda\n\n* skip jit tests on cuda\n\n* fix test_enum\n\n* skip jit tests on cuda\n\n* fix test exponential gamma\n\n* fix tests in ops/stats\n\n* fix thresholds\n\n* use default seed\n\n* revert num samples\n\n* fix test_eig\n\n* make specific to non-reparam\n\n* fix lint\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def test_quantile():",
            "z = torch.randn(2000)",
            "",
            "assert_equal(quantile(x, probs=[0., 0.4, 0.5, 1.]), torch.tensor([0., 0.8, 1., 2.]))",
            "-    assert_equal(quantile(y, probs=0.2), torch.tensor(0.2), prec=0.01)",
            "-    assert_equal(quantile(z, probs=0.8413), torch.tensor(1.), prec=0.001)",
            "+    assert_equal(quantile(y, probs=0.2), torch.tensor(0.2), prec=0.02)",
            "+    assert_equal(quantile(z, probs=0.8413), torch.tensor(1.), prec=0.02)",
            "",
            "",
            "def test_pi():"
        ]
    },
    {
        "number": 176,
        "comments": "change name",
        "commit_message": "fixing rebase issues\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class TacotronGSTTrainTest(unittest.TestCase):",
            "input_lengths = torch.randint(100, 129, (8, )).long().to(device)",
            "input_lengths[-1] = 128",
            "mel_spec = torch.rand(8, 120, c.audio['num_mels']).to(device)",
            "-        linear_spec = torch.rand(8, 120, c.audio['num_freq']).to(device)",
            "+        linear_spec = torch.rand(8, 120, c.audio['fft_size']).to(device)",
            "mel_lengths = torch.randint(20, 120, (8, )).long().to(device)",
            "mel_lengths[-1] = 120",
            "stop_targets = torch.zeros(8, 120, 1).float().to(device)"
        ]
    },
    {
        "number": 177,
        "comments": "update param for refactor fix",
        "commit_message": "small fix to Haiku converter method.\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class IvyModule(ivy.Module):",
            "if ivy.array_mode():",
            "a, kw = ivy.args_to_native(*a, **kw)",
            "# noinspection PyUnresolvedReferences",
            "-        params_hk = self._native_module.init(ivy.functional.core.random.RNG, *a, **kw)",
            "+        params_hk = self._native_module.init(ivy.random.RNG, *a, **kw)",
            "params_dict = _hk_flat_map_to_dict(params_hk)",
            "self._hk_params = ivy.Container(params_dict)",
            "param_iterator = self._hk_params.to_iterator()"
        ]
    },
    {
        "number": 178,
        "comments": "parameterize the variable",
        "commit_message": "Fix: Failing test in data_modules(dp) (#5924)\n\n* Update test_datamodules.py\n\n* fix code format issue\n\n* fix test restore\n\n* fix code format issue\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class ClassificationModel(LightningModule):",
            "return logits",
            "",
            "def configure_optimizers(self):",
            "-        optimizer = torch.optim.Adam(self.parameters(), lr=0.01)",
            "+        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)",
            "return [optimizer], []",
            "",
            "def training_step(self, batch, batch_idx):"
        ]
    },
    {
        "number": 179,
        "comments": "remove API call for resource fix",
        "commit_message": "use `f`-strings (#3557)\n\n* fix f-strings\n\n* update\n\n* update\n\n* update\n\n* fix test\n\n* update\n\n* fix tests\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class DataParallel(torch.nn.DataParallel):",
            "Batch.from_data_list(data_list[split[i]:split[i + 1]],",
            "follow_batch=self.follow_batch,",
            "exclude_keys=self.exclude_keys).to(",
            "-                                     torch.device('cuda:{}'.format(",
            "-                                         device_ids[i])))",
            "+                                     torch.device(f'cuda:{device_ids[i]}'))",
            "for i in range(len(split) - 1)",
            "]"
        ]
    },
    {
        "number": 180,
        "comments": "change API call for type fix",
        "commit_message": "fix #827\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class ProjectedAdaptiveLogSoftmax(nn.Module):",
            "d_emb_i = d_embed // (div_val ** i)",
            "",
            "self.out_projs.append(",
            "-                    nn.Parameter(torch.Tensor(d_proj, d_emb_i))",
            "+                    nn.Parameter(torch.FloatTensor(d_proj, d_emb_i))",
            ")",
            "",
            "self.out_layers.append(nn.Linear(d_emb_i, r_idx-l_idx))"
        ]
    },
    {
        "number": 181,
        "comments": "update API call for refactor fix",
        "commit_message": "fix dropout management in TensorFlowEstimator._predict - get correct list of dropout variables, pass keep_prob = 1.0\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class TensorFlowEstimator(BaseEstimator):",
            "raise NotFittedError()",
            "predict_data_feeder = setup_predict_data_feeder(X)",
            "preds = []",
            "-        dropouts = tf.get_collection(DROPOUTS)",
            "-        feed_dict = {prob: 0.0 for prob in dropouts}",
            "+        dropouts = self._graph.get_collection(DROPOUTS)",
            "+        feed_dict = {prob: 1.0 for prob in dropouts}",
            "for data in predict_data_feeder:",
            "feed_dict[self._inp] = data",
            "preds.append(self._session.run("
        ]
    },
    {
        "number": 182,
        "comments": "value update",
        "commit_message": "Update keras RNG logic to use tf.random.Generator if possible.\n\nThis change also update the RNG behavior for initializer. The seeded initializer will no longer produce same random value across multiple calls. Instead, it will produce different value, and multiple initializer created with same seed will produce same sequences. This change will the make the seeded initializer behavior align between v1 and v2.\n\nKeras was using stateful RNG op in various place when seed is not provided. The recommended approach in v2 is using tf.random.Generator which can be treat as a variable (seed) with stateless RNG op. This change make sure we use this new approach when seed is provided in v2, and also leave a flag to enforce the new approach, which has not been turn on yet. The new approach will be turned on when all the internal tests are fixed. V1 graph mode, the behavior is not change.\n\nPiperOrigin-RevId: 392092094\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class GradientsTest(tf.test.TestCase):",
            "self.assertAllClose(eager_result, function_result)",
            "backprop_result, numeric_result = tf.test.compute_gradient(",
            "m, [inp], delta=1e-3)",
            "-    self.assertAllClose(numeric_result, backprop_result, rtol=1e-2)",
            "+    self.assertAllClose(numeric_result, backprop_result, atol=1e-3)",
            "self.assertAllClose(tf.reshape(numeric_result, [-1]),",
            "-                        tf.reshape(eager_result, [-1]), rtol=1e-2)",
            "+                        tf.reshape(eager_result, [-1]), atol=1e-3)",
            "",
            "def testEmbeddingLookupGradientsHaveKnownShape(self):"
        ]
    },
    {
        "number": 184,
        "comments": "state fixialize log but API fix",
        "commit_message": "Fixup `trainer.py` \ud83d\udee0\ufe0f\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class Trainer:",
            "self.tb_logger.tb_eval_figures(self.total_steps_done, figures)",
            "if audios is not None:",
            "self.tb_logger.tb_eval_audios(self.total_steps_done, audios, self.ap.sample_rate)",
            "+            self.tb_logger.tb_eval_stats(self.total_steps_done, self.keep_avg_eval.avg_values)",
            "",
            "def test_run(self) -> None:",
            "\"\"\"Run test and log the results. Test run must be defined by the model."
        ]
    },
    {
        "number": 185,
        "comments": "remove check for not clear reason",
        "commit_message": "Fix torch type promotion (#5095)\n\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def bitwise_left_shift(",
            "out: Optional[torch.Tensor] = None,",
            ") -> torch.Tensor:",
            "x1, x2 = ivy.promote_types_of_inputs(x1, x2, array_api_promotion=True)",
            "-    ivy.assertions.check_all(x2 >= 0, message=\"shifts must be non-negative\")",
            "return torch.bitwise_left_shift(x1, x2, out=out)"
        ]
    },
    {
        "number": 186,
        "comments": "global variable update",
        "commit_message": "Refactor and add tests in `get_perspective_transform` (#1767)\n\n* use torch.linalg.lstsq\n\n* refactor tests\n\n* add improvements\n\n* add missing import\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* improve formula docs\n\n* codespell\n\n* linter\n\n* few fixes\n\n* fix mypy versioning\n\n* disable standalone mymy check\n\n* remove type ignore\n\n* fix mypy version checker\n\n* skip tests for < 1.11.0\n\n* switch back to torch.solve\n\n* doctest fixes\n\n* remove lstsq\n\n* remove import lstsq\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "no",
        "answer": "no",
        "change": [
            "TEST_DEVICES: Dict[str, torch.device] = get_test_devices()",
            "TEST_DTYPES: Dict[str, torch.dtype] = get_test_dtypes()",
            "",
            "# Combinations of device and dtype to be excluded from testing.",
            "-DEVICE_DTYPE_BLACKLIST = {('cpu', 'float16')}",
            "+# DEVICE_DTYPE_BLACKLIST = {('cpu', 'float16')}",
            "+DEVICE_DTYPE_BLACKLIST = {}",
            "",
            "",
            "@pytest.fixture()"
        ]
    },
    {
        "number": 187,
        "comments": "add API call for type fix",
        "commit_message": "fix small issues\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class Pix2PixModel(BaseModel):",
            "def backward_D(self):",
            "# Fake",
            "# stop backprop to the generator by detaching fake_B",
            "-        fake_AB = self.fake_AB_pool.query(torch.cat((self.real_A, self.fake_B), 1))",
            "+        fake_AB = self.fake_AB_pool.query(torch.cat((self.real_A, self.fake_B), 1).data)",
            "pred_fake = self.netD.forward(fake_AB.detach())",
            "self.loss_D_fake = self.criterionGAN(pred_fake, False)",
            "",
            "# Real",
            "real_AB = torch.cat((self.real_A, self.real_B), 1)",
            "pred_real = self.netD.forward(real_AB)",
            "-        self.loss_D_real = self.criterionGAN(self.pred_real, True)",
            "+        self.loss_D_real = self.criterionGAN(pred_real, True)",
            "",
            "# Combined loss",
            "self.loss_D = (self.loss_D_fake + self.loss_D_real) * 0.5"
        ]
    },
    {
        "number": 188,
        "comments": "fix print update",
        "commit_message": "Fix typos & supplement descriptions\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "args = parser.parse_args()",
            "torch.manual_seed(args.seed)",
            "if torch.cuda.is_available():",
            "if not args.cuda:",
            "-        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")",
            "+        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda.\")",
            "",
            "device = torch.device(\"cuda\" if args.cuda else \"cpu\")",
            "",
            "if args.temperature < 1e-3:",
            "-    parser.error(\"--temperature has to be greater or equal 1e-3\")",
            "+    parser.error(\"--temperature has to be greater or equal 1e-3.\")",
            "",
            "with open(args.checkpoint, 'rb') as f:",
            "model = torch.load(f).to(device)"
        ]
    },
    {
        "number": 189,
        "comments": "add condition check for null fix",
        "commit_message": "fix FSDP ShardedGradScaler (#18358)\n\nrenaming it\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class Trainer:",
            "transformer_cls_to_wrap = get_module_class_from_name(",
            "model, self.args.fsdp_transformer_layer_cls_to_wrap",
            ")",
            "+                    if transformer_cls_to_wrap is None:",
            "+                        raise Exception(\"Could not find the transformer layer class to wrap in the model.\")",
            "auto_wrap_policy = functools.partial(",
            "transformer_auto_wrap_policy,",
            "# Transformer layer class to wrap"
        ]
    },
    {
        "number": 190,
        "comments": "add custom method to get var",
        "commit_message": "agents and models base classes moved, various fixes\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class NAFModel(Model):",
            "# Naf directly outputs V(s)",
            "target_value[action] = target_value_output",
            "",
            "-            target_output_vars = get_variables('target_outputs')",
            "+            target_output_vars = tf.contrib.framework.get_variables('target_outputs')",
            "",
            "with tf.name_scope(\"update\"):",
            "for action in self.action:"
        ]
    },
    {
        "number": 191,
        "comments": "add docs",
        "commit_message": "Scope TokenIndexer output by indexer name (#3597)\n\n* Indexer tests are now passing, at least\n\n* Fixed some masking issues, and padding keys for one config file\n\n* TextFieldEmbedder tests pass\n\n* Fixing fixtures, some more tests passing\n\n* Fixed weird ordering bug\n\n* All TokenEmbedder tests passing?\n\n* Update fixtures\n\n* Fix more hard-coded references\n\n* fix field tests\n\n* fix dataset reader tests\n\n* Fix iterator tests\n\n* More tests passing\n\n* fix hotflip and some other tests\n\n* more tests\n\n* more test fixes\n\n* more tests\n\n* most tests passing; I think the remaining ones are spacy model changes\n\n* hard-code POS tag test\n\n* last test, I think\n\n* black\n\n* updated black\n\n* flake8\n\n* mypy\n\n* black again\n\n* fix training configs\n\n* remove reference to embedder_to_indexer_map\n\n* Other fixes from PR comments\n\n* fix breakage from incorrect merge during rebase\n\n* flake, some docstring formatting\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class SpanConstituencyParserTest(ModelTestCase):",
            "# A very annoying edge case: the PTB has several single word sentences.",
            "# when running with a batch size 1, we have to be very careful",
            "# about how we .squeeze/.unsqueeze things to make sure it still runs.",
            "-        text = {\"tokens\": torch.LongTensor([[1]])}",
            "+        text = {\"tokens\": {\"tokens\": torch.LongTensor([[1]])}}",
            "pos_tags = torch.LongTensor([[1]])",
            "spans = torch.LongTensor([[[0, 0]]])",
            "label = torch.LongTensor([[1]])"
        ]
    },
    {
        "number": 192,
        "comments": "not ML API",
        "commit_message": "Fix tests on single-GPU machine (#16911)\n\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class MixedPrecisionBoringFabric(BoringFabric):",
            "[",
            "(\"cpu\", \"16-mixed\", torch.bfloat16),",
            "(\"cpu\", \"bf16-mixed\", torch.bfloat16),",
            "-        pytest.param(\"cuda\", \"16-mixed\", torch.float16, marks=RunIf(min_cuda_gpus=1)),",
            "-        pytest.param(\"cuda\", \"bf16-mixed\", torch.bfloat16, marks=RunIf(min_cuda_gpus=1, bf16_cuda=True)),",
            "+        pytest.param(\"cuda\", \"16-mixed\", torch.float16, marks=RunIf(min_cuda_gpus=2)),",
            "+        pytest.param(\"cuda\", \"bf16-mixed\", torch.bfloat16, marks=RunIf(min_cuda_gpus=2, bf16_cuda=True)),",
            "],",
            ")",
            "def test_amp(accelerator, precision, expected_dtype):"
        ]
    },
    {
        "number": 193,
        "comments": "add API call for shape fix",
        "commit_message": "fix tensorflow pre-trained model last layer outputshape problem.\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class tensorflow_extractor(base_extractor):",
            "writer.close()",
            "sess.run(init)",
            "saver = tf.train.Saver()",
            "+            tf.train.export_meta_graph(\"kit.meta\", as_text=True)",
            "saver.restore(sess, path + cls.architecture_map[architecture]['filename'])",
            "save_path = saver.save(sess, path + \"imagenet_{}.ckpt\".format(architecture))",
            "print(\"Model saved in file: %s\" % save_path)"
        ]
    },
    {
        "number": 194,
        "comments": "change API call for math fix",
        "commit_message": "Stabilize autoguide scale parameters via SoftplusTransform (#2767)\n\n* Update to PyTorch nightly\n\n* Update README.md\n\n* Commit to PyTorch nightly on dev branch\n\n* Fix constraint bugs\n\n* Relax torchvision version\n\n* Fix CorrCholesky constraint and test data\n\n* Fix funsor bugs\n\n* Try harder to generate positive data\n\n* xfail; switch to torch_test channel\n\n* Fix versions\n\n* Pin to fixed nightly version\n\n* lint\n\n* xfail some funsor tests\n\n* Remove accidental file\n\n* Use softplus transforms for autoguide scales\n\n* Add transform tests\n\n* Rename stable_positive -> softplus_positive\n\n* Make autoguide constraints configurable\n\n* Address review comments\n\n* lint\n\n* Tweak parameters in inference test\n\n* Regster transforms in docs\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def test_auto_diagonal_gaussians(auto_class, Elbo):",
            "guide = auto_class(model, rank=1)",
            "else:",
            "guide = auto_class(model)",
            "-    adam = optim.Adam({\"lr\": .001, \"betas\": (0.95, 0.999)})",
            "+    adam = optim.ClippedAdam({\"lr\": .01, \"betas\": (0.95, 0.999),",
            "+                              \"lrd\": 0.1 ** (1 / n_steps)})",
            "svi = SVI(model, guide, adam, loss=Elbo())",
            "",
            "for k in range(n_steps):"
        ]
    },
    {
        "number": 195,
        "comments": "add param for math fix",
        "commit_message": "fix random seed for stable test\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def test_frontend_backward_multi_channel(train, use_wpe, use_beamformer):",
            "frontend.train()",
            "else:",
            "frontend.eval()",
            "+    torch.random.manual_seed(14)",
            "x = torch.randn(2, 1000, 2, requires_grad=True)",
            "x_lengths = torch.LongTensor([1000, 980])",
            "y, y_lengths = frontend(x, x_lengths)"
        ]
    },
    {
        "number": 196,
        "comments": "remove as",
        "commit_message": "Fix issues suggested by codacy. (#344)\n\n* remove two dangerous default values\n\n* fix mnist tutorial based on codacy\n\n* address hao's comments.\n\n* remove unused y_op\n\n* hao conv.py\n\n* hao prepro.py\n\n* hao files.py\n\n* remove str statement\n\n* hao example mnist\n\n* yapf\n\n* hao cifar10\n\n* hao inceptionv3\n\n* hao ptb tfrecord image processing\n\n* hao tutorials\n\n* str comment\n\n* str docs\n\n* Update README.md\n\n* remove unused code\n\n* minor fix\n\n* small fix\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class Conv1dLayer(Layer):",
            "act = tf.identity",
            "logging.info(\"Conv1dLayer %s: shape:%s stride:%s pad:%s act:%s\" % (self.name, str(shape), str(stride), padding, act.__name__))",
            "",
            "-        with tf.variable_scope(name) as vs:",
            "+        with tf.variable_scope(name):  # as vs:",
            "W = tf.get_variable(name='W_conv1d', shape=shape, initializer=W_init, dtype=D_TYPE, **W_init_args)",
            "self.outputs = tf.nn.convolution(",
            "self.inputs, W, strides=(stride, ), padding=padding, dilation_rate=(dilation_rate, ), data_format=data_format)  # 1.2"
        ]
    },
    {
        "number": 197,
        "comments": "test fix",
        "commit_message": "Fix hub (#2687)\n\nSummary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/2687\n\nReviewed By: alexeib\n\nDifferential Revision: D24095130\n\nPulled By: myleott\n\nfbshipit-source-id: 7d371bccb550ec68b2b9b39dfa4c0718356508d6\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "class TestGradientScaling(unittest.TestCase):",
            "optimizer = FP16Optimizer.build_optimizer(self.namespace_dls, params)",
            "",
            "self.run_iter(model, params, optimizer)",
            "-        self.assertTrue(torch.all(optimizer.fp32_params.eq(torch.tensor([3.1000, 5.1000], device='cuda:0', requires_grad=True))))",
            "+        self.assertTrue(all(",
            "+            torch.all(fp32_params.eq(torch.tensor([3.1000, 5.1000], device='cuda:0', requires_grad=True)))",
            "+            for fp32_params in optimizer.fp32_params.values()",
            "+        ))",
            "",
            "def test_memory_efficient(self):",
            "model = copy.deepcopy(self.model)"
        ]
    },
    {
        "number": 198,
        "comments": "test api",
        "commit_message": "[Feat] Add tpu-testing in circleci (#787)\n\n* add needed files for tpu-testing in circleci\n\n* adapt script for pytorch nghtlies\n\n* fix shape test\n\n* fix color tests\n\n* add pytorch version to the dockerfile\n\n* fix xla precision and disable torch-xla[1.6,1.7]\n\n* adjust rgb2luv precision\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class TestLuvToRgb(BaseTester):",
            "[0.06325728, 0.78878325, 0.74280596, 0.99514300, 0.47176042]]",
            "]], device=device, dtype=dtype)",
            "",
            "-        assert_allclose(kornia.color.luv_to_rgb(data), expected)",
            "+        assert_allclose(kornia.color.luv_to_rgb(data), expected, rtol=1e-4, atol=1e-4)",
            "",
            "def test_forth_and_back(self, device, dtype):",
            "data = torch.rand(3, 4, 5, device=device, dtype=dtype)"
        ]
    },
    {
        "number": 199,
        "comments": "add param for type fix",
        "commit_message": "Fix HALTON sequence utilization in Euler Sampling. Disable Sobol random_type in Euler sampling when time_step is non-constant till TensorFlow 2.2 is released.\n\nPiperOrigin-RevId: 291143718\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class EulerSamplingTest(tf.test.TestCase, parameterized.TestCase):",
            "times=times,",
            "num_samples=num_samples,",
            "initial_state=x0,",
            "-            random_type=tff.math.random.RandomType.SOBOL,",
            "+            random_type=tff.math.random.RandomType.HALTON,",
            "time_step=0.01,",
            "-            seed=12134))",
            "+            seed=12134,",
            "+            skip=100,",
            "+            dtype=tf.float32))",
            "",
            "-    self.assertAllClose(paths.shape, (num_samples, 5, 2), atol=0)",
            "+    self.assertAllClose(paths.shape, (num_samples, 3, 2), atol=0)",
            "means = np.mean(paths, axis=0)",
            "times = np.reshape(times, [-1, 1])",
            "expected_means = x0 + (2.0 / 3.0) * mu * np.power(times, 1.5)"
        ]
    },
    {
        "number": 201,
        "comments": "debug log",
        "commit_message": "Logging fix (#661)\n\n* `tf.logging` replaced by: `tl.logging`\n\n* Changelog updated\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class Layer_Shape_Test(unittest.TestCase):",
            "",
            "if __name__ == '__main__':",
            "",
            "-    # tf.logging.set_verbosity(tf.logging.INFO)",
            "-    tf.logging.set_verbosity(tf.logging.DEBUG)",
            "+    # tl.logging.set_verbosity(tl.logging.INFO)",
            "+    tl.logging.set_verbosity(tl.logging.DEBUG)",
            "",
            "unittest.main()"
        ]
    },
    {
        "number": 202,
        "comments": "use custom method to fix the bug",
        "commit_message": "Generalize TorchHook to FrameworkHook (#2561)\n\n* create FrameworkHook and move generic hooking functionality over\n\n* PEP-ify import statements all around\n\n* PR comment - fix docstring\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class TorchTensor(AbstractTensor):",
            ")",
            "# This handles case 3: it redirects the command to the appropriate class depending",
            "# of the syft type of the arguments and returns",
            "-            if args_type not in (torch.Tensor, torch.nn.Parameter):",
            "+            if args_type not in FrameworkTensor:",
            "return args_type.handle_func_command(command)",
            "",
            "# build the new command"
        ]
    },
    {
        "number": 203,
        "comments": "add API call for state fix",
        "commit_message": "Fix BatchNorm TransformModule (#2459)\n\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class BatchNorm(TransformModule):",
            "if self.training:",
            "mean, var = y.mean(0), y.var(0)",
            "",
            "-            # NOTE: The momentum variable agrees with the definition in e.g. `torch.nn.BatchNorm1d`",
            "-            self.moving_mean.mul_(1 - self.momentum).add_(mean * self.momentum)",
            "-            self.moving_variance.mul_(1 - self.momentum).add_(var * self.momentum)",
            "+            with torch.no_grad():",
            "+                # NOTE: The momentum variable agrees with the definition in e.g. `torch.nn.BatchNorm1d`",
            "+                self.moving_mean.mul_(1 - self.momentum).add_(mean * self.momentum)",
            "+                self.moving_variance.mul_(1 - self.momentum).add_(var * self.momentum)",
            "",
            "# During test time, use smoothed averages rather than the sample ones",
            "else:"
        ]
    },
    {
        "number": 205,
        "comments": "remove API call for math fix",
        "commit_message": "fixed a bug about dropout\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class RNNLM(nn.Module):",
            "",
            "def forward(self, state, x):",
            "h0 = self.embed(x)",
            "-        h1, c1 = self.l1(F.dropout(h0), (state['h1'], state['c1']))",
            "-        h2, c2 = self.l2(F.dropout(h1), (state['h2'], state['c2']))",
            "-        y = self.lo(F.dropout(h2))",
            "+        h1, c1 = self.l1(self.d0(h0), (state['h1'], state['c1']))",
            "+        h2, c2 = self.l2(self.d1(h1), (state['h2'], state['c2']))",
            "+        y = self.lo(self.d2(h2))",
            "state = {'c1': c1, 'h1': h1, 'c2': c2, 'h2': h2}",
            "return state, y"
        ]
    },
    {
        "number": 207,
        "comments": "test fix",
        "commit_message": "fix test\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def test_link_neighbor_loader_edge_label():",
            "",
            "for batch in loader:",
            "assert batch.edge_label.dtype == torch.long",
            "-        assert torch.all(batch.edge_label[:10] == 2)",
            "+        assert torch.all(batch.edge_label[:10] == 1)",
            "assert torch.all(batch.edge_label[10:] == 0)"
        ]
    },
    {
        "number": 208,
        "comments": "add condition check for resource fix",
        "commit_message": "translate bug fix\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def reportScore(name, scoreTotal, wordsTotal):",
            "def main():",
            "opt = parser.parse_args()",
            "opt.cuda = opt.gpu > -1",
            "-    torch.cuda.set_device(opt.gpu)",
            "+    if opt.cuda:",
            "+        torch.cuda.set_device(opt.gpu)",
            "",
            "translator = onmt.Translator(opt)"
        ]
    },
    {
        "number": 209,
        "comments": "change param for math fix",
        "commit_message": "Fix static padding calculation\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class Conv2dStaticSamePadding(nn.Conv2d):",
            "pad_h = max((oh - 1) * self.stride[0] + (kh - 1) * self.dilation[0] + 1 - ih, 0)",
            "pad_w = max((ow - 1) * self.stride[1] + (kw - 1) * self.dilation[1] + 1 - iw, 0)",
            "if pad_h > 0 or pad_w > 0:",
            "-            self.static_padding = nn.ZeroPad2d((pad_w - pad_w // 2, pad_w - pad_w // 2,",
            "-                                                pad_h - pad_h // 2, pad_h - pad_h // 2))",
            "+            self.static_padding = nn.ZeroPad2d((pad_w // 2, pad_w - pad_w // 2,",
            "+                                                pad_h // 2, pad_h - pad_h // 2))",
            "else:",
            "self.static_padding = nn.Identity()"
        ]
    },
    {
        "number": 210,
        "comments": "format",
        "commit_message": "lazy datasets (#675)\n\n* lazy datasets\n\n* add end-to-end lazy dataset test + fix bugs\n\n* fix notebook tests\n\n* remove accidentally committed ipynb checkpoints\n\n* remove duplicate code in tests\n\n* Iterator -> Iterable\n\n* address PR comments\n\n* fix api docs\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def train_model(params: Params, serialization_dir: str) -> Model:",
            "",
            "logger.info(\"Creating a vocabulary using %s data.\", \", \".join(datasets_for_vocab_creation))",
            "vocab = Vocabulary.from_params(params.pop(\"vocabulary\", {}),",
            "-                                   Dataset([instance for key, dataset in all_datasets.items()",
            "-                                            for instance in dataset.instances",
            "-                                            if key in datasets_for_vocab_creation]))",
            "+                                   (instance for key, dataset in all_datasets.items()",
            "+                                    for instance in dataset",
            "+                                    if key in datasets_for_vocab_creation))",
            "vocab.save_to_files(os.path.join(serialization_dir, \"vocabulary\"))",
            "",
            "model = Model.from_params(vocab, params.pop('model'))"
        ]
    },
    {
        "number": 213,
        "comments": "add API call for state fix",
        "commit_message": "Fix multi-speaker init of Tacotron models & tests\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class SpeedySpeech(BaseTTS):",
            "outputs = {\"model_outputs\": o_de.transpose(1, 2), \"durations_log\": o_dr_log.squeeze(1), \"alignments\": attn}",
            "return outputs",
            "",
            "+    @torch.no_grad()",
            "def inference(self, x, aux_input={\"d_vectors\": None, \"speaker_ids\": None}):  # pylint: disable=unused-argument",
            "\"\"\"",
            "Shapes:"
        ]
    },
    {
        "number": 214,
        "comments": "add API call for math fix",
        "commit_message": "Fix einsum for keras implementation\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class KerasBackend(AbstractBackend):",
            "return keras",
            "",
            "def einsum(self, pattern, *x):",
            "-        return self.tf.einsum(pattern, *x)",
            "+        return self.tf.vectorized_map(",
            "+            functools.partial(self.tf.einsum, pattern),",
            "+            *x",
            "+        )",
            "",
            "",
            "class OneFlowBackend(AbstractBackend):"
        ]
    },
    {
        "number": 215,
        "comments": "add API call for math fix",
        "commit_message": "additional fix for difference model merging\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def run_modelmerger(primary_model_name, secondary_model_name, teritary_model_nam",
            "t2 = theta_2.get(key, torch.zeros_like(theta_1[key]))",
            "theta_1[key] = theta_func1(theta_1[key], t2)",
            "else:",
            "-                    theta_1[key] = 0",
            "+                    theta_1[key] = torch.zeros_like(theta_1[key])",
            "del theta_2, teritary_model",
            "",
            "for key in tqdm.tqdm(theta_0.keys()):"
        ]
    },
    {
        "number": 216,
        "comments": "add param for resource fix",
        "commit_message": "Fix loss computation for empty tensors\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class DefaultClassifier(Classifier):",
            "",
            "def _calculate_loss(self, scores, labels):",
            "",
            "-        if len(labels) == 0: return torch.tensor(0., requires_grad=True), 1",
            "+        if len(labels) == 0: return torch.tensor(0., requires_grad=True, device=flair.device), 1",
            "",
            "if self.multi_label:",
            "labels = torch.tensor([[1 if l in all_labels_for_point else 0 for l in self.label_dictionary.get_items()]"
        ]
    },
    {
        "number": 217,
        "comments": "change API call for resource fix",
        "commit_message": "Abstract accelerator (step 3) (#2677)\n\n* Integrate accelerator abstraction interface into deepspeed/\n\n* Fix error message in fp16/fused_optimizer\n\n* fix error message in fp16/unfused_optimizer.py\n\n* assign get_accelerator().pin_memory() result to input Tensor name\n\n* no need to check cuda and whether nvtx supported\n\n* move try-except into inner most block\n\n* call Event() and Stream() in get_accelerator() for data type\n\n* Make Stream and Event as properties of abstract interface so they can be used as data type in deepspeed\n\n* Apply op_builder backend api change from #2705 from @jeffra\n\n* fix tests where Builder NAME is used\n\n* keep original ...Builder.NAME interface instead of ...Builder().NAME interface\n\n* fix builder closure for installation\n\n* fix randomltd builder\n\n* add comments to clarify create_op_builder and get_op_builder\n\n* fix compatibility with pip install -e\n\nCo-authored-by: Cheng Li <pistasable@gmail.com>\nCo-authored-by: Olatunji Ruwase <olruwase@microsoft.com>\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class EmbeddingLayer(nn.Module):",
            "torch.empty(weight_shape[0],",
            "weight_shape[1],",
            "dtype=dtype,",
            "-                        device=torch.cuda.current_device()))",
            "+                        device=get_accelerator().current_device_name()))",
            "",
            "def forward(self, input):",
            "return F.embedding(input, self.weight)"
        ]
    },
    {
        "number": 218,
        "comments": "change API call for version fix",
        "commit_message": "[rllib] tuple space shouldn't assume elements are all the same size (#2637)\n\n* fix\n\n* lint\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class MultiActionDistribution(ActionDistribution):",
            "",
            "def logp(self, x):",
            "\"\"\"The log-likelihood of the action distribution.\"\"\"",
            "-        split_list = self.reshaper.split_tensor(x)",
            "+        split_list = tf.split(x, len(self.input_lens), axis=1)",
            "for i, distribution in enumerate(self.child_distributions):",
            "# Remove extra categorical dimension",
            "if isinstance(distribution, Categorical):"
        ]
    },
    {
        "number": 219,
        "comments": "add API call for math fix",
        "commit_message": "fixed stochastic policy shapes, naf under construction\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class CategoricalOneHotPolicy(StochasticPolicy):",
            "def __init__(self, network, session, state, random, action_count=1, scope='policy'):",
            "with tf.variable_scope(scope):",
            "action_layer = linear(layer_input=network.output, config={'num_outputs': action_count}, scope='outputs')",
            "+            action_layer = tf.reshape(action_layer, [-1, action_count])",
            "+",
            "distribution = tf.nn.softmax(action_layer)",
            "sample = tf.multinomial(distribution, 1)"
        ]
    },
    {
        "number": 221,
        "comments": "version fix",
        "commit_message": "Support FP16 training (#520)\n\n* add fp16 support\n\n* fpn donot need bn normalize\n\n* refactor wrapped bn\n\n* fix bug of retinanet\n\n* add fp16 ssd300 voc, cascade r50, cascade mask r50\n\n* fix bug in cascade rcnn testing\n\n* add support to fix bn training\n\n* add fix bn cfg\n\n* delete fixbn cfg, mv fixbn fp16 to a new branch\n\n* fix cascade mask fp16 bug in test\n\n* fix bug in cascade mask rcnn fp16 test\n\n* add more fp16 cfgs\n\n* add fp16 fast-r50 and faster-dconv-r50\n\n* add fp16 test, minor fix\n\n* clean code\n\n* fix config work_dir name\n\n* add patch func, refactor code\n\n* fix format\n\n* clean code\n\n* move convert rois to single_level_extractor\n\n* fix bug for cascade mask, the seg mask is ndarray\n\n* refactor code, add two decorator force_fp32 and auto_fp16\n\n* add fp16_enable attribute\n\n* add more comment, fix format and test assertion\n\n* fix pep8 format error\n\n* format commont and api\n\n* rename distribute to distributed, fix dict copy\n\n* rename function name\n\n* move function, add comment\n\n* remove unused parameter\n\n* mv decorators into decorators.py, hook related functions to hook\n\n* add auto_fp16 to forward of semantic head\n\n* add auto_fp16 to all heads and fpn\n\n* add docstrings and minor bug fix\n\n* simple refactoring\n\n* bug fix for patching forward method\n\n* roi extractor in fp32 mode\n\n* fix flake8 error\n\n* fix ci error\n\n* add fp16 support to ga head\n\n* remove parallel test assert\n\n* minor fix\n\n* add comment in build_optimizer\n\n* fix typo in comment\n\n* fix typo enable --> enabling\n\n* udpate README\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class SingleRoIExtractor(nn.Module):",
            "out_size = self.roi_layers[0].out_size",
            "num_levels = len(feats)",
            "target_lvls = self.map_roi_levels(rois, num_levels)",
            "-        roi_feats = torch.cuda.FloatTensor(rois.size()[0], self.out_channels,",
            "-                                           out_size, out_size).fill_(0)",
            "+        roi_feats = feats[0].new_zeros(rois.size()[0], self.out_channels,",
            "+                                       out_size, out_size)",
            "for i in range(num_levels):",
            "inds = target_lvls == i",
            "if inds.any():"
        ]
    },
    {
        "number": 222,
        "comments": "add condition check for resource fix",
        "commit_message": "train with multi-gpu half test bug fix #99\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def test(data,",
            "else:  # called by train.py",
            "training = True",
            "device = next(model.parameters()).device  # get model device",
            "-        half = device.type != 'cpu'  # half precision only supported on CUDA",
            "+        half = device.type != 'cpu' and torch.cuda.device_count() == 1  # half precision only supported on single-GPU",
            "if half:",
            "model.half()  # to FP16"
        ]
    },
    {
        "number": 223,
        "comments": "update API call for version fix",
        "commit_message": "Fix issue with torchvision 0.11.0\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class MobileNetV3LargeEncoder(MobileNetV3):",
            ")",
            "",
            "if pretrained:",
            "-            self.load_state_dict(load_state_dict_from_url(",
            "+            self.load_state_dict(torch.hub.load_state_dict_from_url(",
            "'https://download.pytorch.org/models/mobilenet_v3_large-8738ca79.pth'))",
            "",
            "del self.avgpool"
        ]
    },
    {
        "number": 224,
        "comments": "add param for type fix",
        "commit_message": "fixed zeros matrix definition\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def make_non_pad_mask(lengths):",
            "\"\"\"",
            "bs = int(len(lengths))",
            "maxlen = int(max(lengths))",
            "-    mask = torch.zeros(bs, maxlen).byte()",
            "+    mask = torch.zeros(bs, maxlen, dtype=torch.uint8)",
            "for i, l in enumerate(lengths):",
            "mask[i, :l] = 1"
        ]
    },
    {
        "number": 225,
        "comments": "no API",
        "commit_message": "Fix BeitForMaskedImageModeling (#13275)\n\n* First pass\n\n* Fix docs of bool_masked_pos\n\n* Add integration script\n\n* Fix docstring\n\n* Add integration test for BeitForMaskedImageModeling\n\n* Remove file\n\n* Fix docs\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class BeitForMaskedImageModeling(BeitPreTrainedModel):",
            "",
            "outputs = self.beit(",
            "pixel_values,",
            "+            bool_masked_pos=bool_masked_pos,",
            "head_mask=head_mask,",
            "output_attentions=output_attentions,",
            "output_hidden_states=output_hidden_states,"
        ]
    },
    {
        "number": 226,
        "comments": "remove constraint",
        "commit_message": "fix mt task collect_stats at stage 9\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class Embedding(AbsFrontend):",
            "assert check_argument_types()",
            "super().__init__()",
            "self.embed_dim = embed_dim",
            "-        self.padding = padding",
            "self.embed_scale = 1.0 if no_embed_scale else math.sqrt(embed_dim)",
            "-        self.embed = torch.nn.Embedding(input_size, embed_dim, padding_idx=padding)",
            "+        self.embed = torch.nn.Embedding(input_size, embed_dim)",
            "",
            "def forward(",
            "self, input: torch.Tensor, input_lengths: torch.Tensor"
        ]
    },
    {
        "number": 227,
        "comments": "format",
        "commit_message": "Fixed failing tests for astype\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class Finfo:",
            "# -------------------#",
            "",
            "",
            "-def astype(x: torch.Tensor, dtype: torch.dtype, *, copy: bool = True) -> torch.Tensor:",
            "+def astype(",
            "+    x: torch.Tensor, dtype: torch.dtype, /, *, copy: bool = True",
            "+) -> torch.Tensor:",
            "dtype = ivy.as_native_dtype(dtype)",
            "if isinstance(dtype, str):",
            "dtype = ivy.as_native_dtype(dtype)"
        ]
    },
    {
        "number": 229,
        "comments": "change param for type fix",
        "commit_message": "Update serving signatures and make sure we actually use them (#19034)\n\n* Override save() to use the serving signature as the default\n\n* Replace int32 with int64 in all our serving signatures\n\n* Remember one very important line so as not to break every test at once\n\n* Dtype fix for TFLED\n\n* dtype fix for shift_tokens_right in general\n\n* Dtype fixes in mBART and RAG\n\n* Fix dtypes for test_unpack_inputs\n\n* More dtype fixes\n\n* Yet more mBART + RAG dtype fixes\n\n* Yet more mBART + RAG dtype fixes\n\n* Add a check that the model actually has a serving method\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class TFXGLMPreTrainedModel(TFPreTrainedModel):",
            "@tf.function(",
            "input_signature=[",
            "{",
            "-                \"input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"input_ids\"),",
            "-                \"attention_mask\": tf.TensorSpec((None, None), tf.int32, name=\"attention_mask\"),",
            "+                \"input_ids\": tf.TensorSpec((None, None), tf.int64, name=\"input_ids\"),",
            "+                \"attention_mask\": tf.TensorSpec((None, None), tf.int64, name=\"attention_mask\"),",
            "}",
            "]",
            ")"
        ]
    },
    {
        "number": 231,
        "comments": "format",
        "commit_message": "rename log_softmax, support dim, fix onnx Softmax\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class SpeedyResNet:",
            "nn.Linear(512, num_classes, bias=False)",
            "]",
            "",
            "-  # note, pytorch just uses https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html instead of logsoftmax",
            "-  def __call__(self, x): return x.sequential(self.net).logsoftmax()",
            "+  # note, pytorch just uses https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html instead of log_softmax",
            "+  def __call__(self, x): return x.sequential(self.net).log_softmax()",
            "",
            "from extra.jit import TinyJit",
            "@TinyJit"
        ]
    },
    {
        "number": 233,
        "comments": "add param for argument fix",
        "commit_message": "bug fix for tacotron and tests update\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class TacotronTrainTest(unittest.TestCase):",
            "optimizer = optim.Adam(model.parameters(), lr=c.lr)",
            "for i in range(5):",
            "mel_out, linear_out, align, stop_tokens = model.forward(",
            "-                input, mel_spec)",
            "+                input, input_lengths, mel_spec)",
            "assert stop_tokens.data.max() <= 1.0",
            "assert stop_tokens.data.min() >= 0.0",
            "optimizer.zero_grad()"
        ]
    },
    {
        "number": 234,
        "comments": "remove condition check for resource fix",
        "commit_message": "Fix slow tests (#1210)\n\n* fix tests\n\n* Fix more\n\n* more\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class EulerAncestralDiscreteScheduler(SchedulerMixin, ConfigMixin):",
            "",
            "prev_sample = sample + derivative * dt",
            "",
            "-        device = model_output.device if torch.is_tensor(model_output) else torch.device(\"cpu\")",
            "+        device = model_output.device",
            "if device.type == \"mps\":",
            "# randn does not work reproducibly on mps",
            "noise = torch.randn(model_output.shape, dtype=model_output.dtype, device=\"cpu\", generator=generator).to("
        ]
    },
    {
        "number": 236,
        "comments": "update API call for version fix",
        "commit_message": "Disable use of timm nn.Linear wrapper since AMP autocast + torchscript use appears fixed\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def _create_fc(num_features, num_classes, use_conv=False):",
            "elif use_conv:",
            "fc = nn.Conv2d(num_features, num_classes, 1, bias=True)",
            "else:",
            "-        # NOTE: using my Linear wrapper that fixes AMP + torchscript casting issue",
            "-        fc = Linear(num_features, num_classes, bias=True)",
            "+        fc = nn.Linear(num_features, num_classes, bias=True)",
            "return fc"
        ]
    },
    {
        "number": 237,
        "comments": "no API",
        "commit_message": "[RLlib] 2 bug fixes: Bandit registration not working if torch not installed. Env checker for MA envs. (#22821)\n\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class OnlineLinearRegression(nn.Module):",
            "batch_dots = batch_dots.reshape([B, C])",
            "return batch_dots",
            "",
            "-    def forward(self, x, sample_theta=False):",
            "+    def forward(self, x: TensorType, sample_theta: bool = False):",
            "\"\"\"Predict scores on input batch using the underlying linear model.",
            "",
            "Args:",
            "-            x (torch.Tensor): Input feature tensor of shape",
            "-                (batch_size, feature_dim)",
            "-            sample_theta (bool): Whether to sample the weights from its",
            "+            x: Input feature tensor of shape (batch_size, feature_dim)",
            "+            sample_theta: Whether to sample the weights from its",
            "posterior distribution to perform Thompson Sampling as per",
            "http://proceedings.mlr.press/v28/agrawal13.pdf .",
            "\"\"\""
        ]
    },
    {
        "number": 240,
        "comments": "doc update",
        "commit_message": "Improves API docs and docstring consistency (#4244)\n\n* refactor py2md\n\n* improve py2md, warn if backticks missing\n\n* ensure backticks consistent\n\n* remove docstring help test\n\n* fixes and handle more edge cases\n\n* add failing test for pydoc-markdown bug\n\n* update pydoc-markdown\n\n* fix some links\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class SequenceAccuracy(Metric):",
            "A tensor of predictions of shape (batch_size, k, sequence_length).",
            "gold_labels : `torch.Tensor`, required.",
            "A tensor of integer class label of shape (batch_size, sequence_length).",
            "-        mask : `torch.BoolTensor`, optional (default = None).",
            "+        mask : `torch.BoolTensor`, optional (default = `None`).",
            "A masking tensor the same size as `gold_labels`.",
            "\"\"\"",
            "predictions, gold_labels, mask = self.detach_tensors(predictions, gold_labels, mask)"
        ]
    },
    {
        "number": 241,
        "comments": "doc update",
        "commit_message": "added cbs to notebooks, made copy-paste error fix in generation_utils (#16246)\n\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class GenerationMixin:",
            "continue  # don't waste resources running the code we don't need",
            "",
            "next_token_logits = outputs.logits[:, -1, :]",
            "-",
            "-            # hack: adjust tokens for Marian. For Marian we have to make sure that the `pad_token_id`",
            "-            # cannot be generated both before and after the `nn.functional.log_softmax` operation.",
            "-            next_token_logits = outputs.logits[:, -1, :]",
            "# hack: adjust tokens for Marian. For Marian we have to make sure that the `pad_token_id`",
            "# cannot be generated both before and after the `nn.functional.log_softmax` operation.",
            "next_token_logits = self.adjust_logits_during_generation(next_token_logits, cur_len=cur_len)"
        ]
    },
    {
        "number": 242,
        "comments": "remove debug",
        "commit_message": "Fixup `utils` for the trainer\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def get_commit_hash():",
            "return commit",
            "",
            "",
            "-def create_experiment_folder(root_path, model_name, debug):",
            "+def create_experiment_folder(root_path, model_name):",
            "\"\"\"Create a folder with the current date and time\"\"\"",
            "date_str = datetime.datetime.now().strftime(\"%B-%d-%Y_%I+%M%p\")",
            "-    if debug:",
            "-        commit_hash = \"debug\"",
            "-    else:",
            "-        commit_hash = get_commit_hash()",
            "+    commit_hash = get_commit_hash()",
            "output_folder = os.path.join(root_path, model_name + \"-\" + date_str + \"-\" + commit_hash)",
            "os.makedirs(output_folder, exist_ok=True)",
            "print(\" > Experiment folder: {}\".format(output_folder))"
        ]
    },
    {
        "number": 243,
        "comments": "test fix",
        "commit_message": "test fix\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def k_hop_subgraph(node_idx, num_hops, edge_index, relabel_nodes=False,",
            "node_mask[subsets[-1]] = True",
            "torch.index_select(node_mask, 0, row, out=edge_mask)",
            "subsets.append(col[edge_mask])",
            "-    subset = torch.cat(subsets).unique(sorted=False)",
            "+    subset = torch.cat(subsets).unique()",
            "# Add `node_idx` to the beginning of `subset`.",
            "subset = subset[subset != node_idx]",
            "subset = torch.cat([torch.tensor([node_idx], device=row.device), subset])"
        ]
    },
    {
        "number": 244,
        "comments": "update API call for version fix",
        "commit_message": "fix slow sd test\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def test_runway_sd_1_5_negative_prompt(sd_device, strategy, sampler):",
            "def test_cv2(strategy, cv2_flag, cv2_radius):",
            "model = ModelManager(",
            "name=\"cv2\",",
            "-        device=device,",
            "+        device=torch.device(device),",
            ")",
            "cfg = get_config(strategy, cv2_flag=cv2_flag, cv2_radius=cv2_radius)",
            "assert_equal("
        ]
    },
    {
        "number": 245,
        "comments": "parameterize the parameter",
        "commit_message": "Upgrade shufflenet; fix paramsetter for restore\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "class Model(ModelDesc):",
            "summary.add_moving_summary(self.cost)",
            "",
            "def _get_optimizer(self):",
            "-        lr = tf.get_variable('learning_rate', initializer=1e-3, trainable=False)",
            "+        lr = tf.get_variable('learning_rate', initializer=self.learning_rate, trainable=False)",
            "opt = tf.train.AdamOptimizer(lr, epsilon=1e-3)",
            "return optimizer.apply_grad_processors(",
            "opt, [gradproc.GlobalNormClip(10), gradproc.SummaryGradient()])"
        ]
    },
    {
        "number": 246,
        "comments": "change name",
        "commit_message": "Fixed naming of the fully connected layer\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class Network(object):",
            "weights = self.make_var('weights', shape=[dim, num_out])",
            "biases = self.make_var('biases', [num_out])",
            "op = tf.nn.relu_layer if relu else tf.nn.xw_plus_b",
            "-            fc = op(feed_in, weights, biases, name=scope.name)",
            "+            #fc = op(feed_in, weights, biases, name=scope.name)",
            "+            fc = op(feed_in, weights, biases, name=name)",
            "return fc"
        ]
    },
    {
        "number": 250,
        "comments": "update API call for version fix",
        "commit_message": "Updated code to mesh with get_weights returning a dict and new tf code (#187)\n\n* Updated code to mesh with get_weights returning a dict and new tf code\n\n* Added tf.global_variables_initalizer to hyperopt example as well\n\n* Small fix.\n\n* Small name change.\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def train_cnn_and_compute_accuracy(params, steps, train_images, train_labels, va",
            "# Do the training and evaluation.",
            "with tf.Session() as sess:",
            "# Initialize the network weights.",
            "-    sess.run(tf.initialize_all_variables())",
            "+    sess.run(tf.global_variables_initializer())",
            "for i in range(1, steps + 1):",
            "# Fetch the next batch of data.",
            "image_batch = get_batch(train_images, i, batch_size)"
        ]
    },
    {
        "number": 251,
        "comments": "change API call for math fix",
        "commit_message": "[RLlib] Fix PyTorch A3C / A2C loss function using mixed reduced sum / mean (#11449)\n\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def actor_critic_loss(policy, model, dist_class, train_batch):",
            "values = model.value_function()",
            "dist = dist_class(logits, model)",
            "log_probs = dist.logp(train_batch[SampleBatch.ACTIONS])",
            "-    policy.entropy = dist.entropy().mean()",
            "+    policy.entropy = dist.entropy().sum()",
            "policy.pi_err = -train_batch[Postprocessing.ADVANTAGES].dot(",
            "log_probs.reshape(-1))",
            "-    policy.value_err = nn.functional.mse_loss(",
            "-        values.reshape(-1), train_batch[Postprocessing.VALUE_TARGETS])",
            "+    policy.value_err = torch.sum(",
            "+        torch.pow(",
            "+            values.reshape(-1) - train_batch[Postprocessing.VALUE_TARGETS],",
            "+            2.0))",
            "overall_err = sum([",
            "policy.pi_err,",
            "policy.config[\"vf_loss_coeff\"] * policy.value_err,"
        ]
    },
    {
        "number": 252,
        "comments": "update API call for version fix",
        "commit_message": "Abstract accelerator (step 3) (#2677)\n\n* Integrate accelerator abstraction interface into deepspeed/\n\n* Fix error message in fp16/fused_optimizer\n\n* fix error message in fp16/unfused_optimizer.py\n\n* assign get_accelerator().pin_memory() result to input Tensor name\n\n* no need to check cuda and whether nvtx supported\n\n* move try-except into inner most block\n\n* call Event() and Stream() in get_accelerator() for data type\n\n* Make Stream and Event as properties of abstract interface so they can be used as data type in deepspeed\n\n* Apply op_builder backend api change from #2705 from @jeffra\n\n* fix tests where Builder NAME is used\n\n* keep original ...Builder.NAME interface instead of ...Builder().NAME interface\n\n* fix builder closure for installation\n\n* fix randomltd builder\n\n* add comments to clarify create_op_builder and get_op_builder\n\n* fix compatibility with pip install -e\n\nCo-authored-by: Cheng Li <pistasable@gmail.com>\nCo-authored-by: Olatunji Ruwase <olruwase@microsoft.com>\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class DeepSpeedDataLoader(object):",
            "else:",
            "if data_sampler is None:",
            "data_sampler = RandomSampler(dataset)",
            "-                device_count = torch.cuda.device_count()",
            "+                device_count = get_accelerator().device_count()",
            "batch_size *= device_count",
            "",
            "if num_local_io_workers is None:"
        ]
    },
    {
        "number": 253,
        "comments": "typo fix",
        "commit_message": "Fixed typo in test\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class CategoricalToNumerical(preprocessor.Preprocessor):",
            "\"column_names\": config[\"column_names\"],",
            "}",
            "obj = cls(**init_config)",
            "-        obj.layer = keras_layers.MultiCategoryEncoding(config[\"encoding\"])",
            "-        obj.layer.build(None)",
            "+        obj.layer = preprocessors.deserialize(config[\"layer\"])",
            "for encoding_layer, vocab in zip(",
            "obj.layer.encoding_layers, config[\"encoding_vocab\"]",
            "):"
        ]
    },
    {
        "number": 254,
        "comments": "use customized API",
        "commit_message": "Refactor library namespaces [pre-release][0.6-rc1] (#1412)\n\n* flake fixes\n\n* initial flake8 fixeS\n\n* remove top level from kornia.color\n\n* kornia filters\n\n* kornia losses\n\n* kornia features\n\n* geomtry and all ok\n\n* removed jit module\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* apply formatting and few fixes\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add keep block for isort\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* skip init\n\n* fix the docs\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* remove old code\n\n* simplify ci workflow\n\n* fix circular dependency\n\n* few format fixes\n\n* fix code format test\n\n* remove kornia.jit imports\n\n* final fixes\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* ci fixes\n\n* add versioneer\n\n* fix pnp import\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* exclude version files from pre-commit\n\n* exclude files precommit\n\n* update to pytorch 1.10 and add fixes\n\n* Update tests_cpu.yml\n\n* Update setup_dev_env.sh\n\n* Update tests_cpu.yml\n\n* undo versioneer\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix no_grad\n\n* Apply suggestions from code review\n\n* fix skip windows tests\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Update test_integrated.py\n\n* Apply suggestions from code review\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def transform_bbox(trans_mat: torch.Tensor, boxes: torch.Tensor, mode: str = \"xy",
            "boxes[..., -2] = boxes[..., 0] + boxes[..., -2]  # x + w",
            "boxes[..., -1] = boxes[..., 1] + boxes[..., -1]  # y + h",
            "",
            "-    transformed_boxes: torch.Tensor = kornia.transform_points(trans_mat, boxes.view(boxes.shape[0], -1, 2))",
            "+    transformed_boxes: torch.Tensor = transform_points(trans_mat, boxes.view(boxes.shape[0], -1, 2))",
            "transformed_boxes = transformed_boxes.view_as(boxes)",
            "",
            "if mode == 'xywh':"
        ]
    },
    {
        "number": 255,
        "comments": "print fix",
        "commit_message": "Use preprocessing layers for categorical encoding (#1090)\n\n* removed sigmoid layer\n\n* added lookup\n\n* bug fix\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def test_feature_encoder_layer():",
            "",
            "model2 = tf.keras.Model(input_node, hidden_node)",
            "result = model2.predict(data)",
            "-    print(result)",
            "+    model2.predict(data2)",
            "assert result[0][0] == result[2][0]",
            "assert result[0][0] != result[1][0]",
            "assert result[0][1] != result[1][1]"
        ]
    },
    {
        "number": 257,
        "comments": "format",
        "commit_message": "fix tensorflow asinh failure\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def asin(x: Union[tf.Tensor, tf.Variable]) -> Union[tf.Tensor, tf.Variable]:",
            "return tf.asin(x)",
            "",
            "",
            "-def asinh(",
            "-        x: Union[tf.Tensor, tf.Variable]",
            "-) -> Union[tf.Tensor, tf.Variable]:",
            "-    x = tf.cast(x, tf.float32)",
            "+def asinh(x: Union[tf.Tensor, tf.Variable]) -> Union[tf.Tensor, tf.Variable]:",
            "return tf.asinh(x)"
        ]
    },
    {
        "number": 258,
        "comments": "add API call for type fix",
        "commit_message": "Fix typing issue in meters when resuming FP16 training (#1132)\n\nSummary:\nWhen we save checkpoints, we move all CUDA tensors to CPU. This includes meter values (e.g., grad norm). Upon reloading the checkpoint, these meter values remain on the CPU, but subsequent meter values are likely to be on GPU, thus raising an exception (PyTorch doesn't support operations between CPU and CUDA tensors). In the case of FP16 training, you get a slightly different exception due to trying to add float16 tensors on CPU, but it's the same underlying cause.\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1132\n\nReviewed By: ngoyal2707\n\nDifferential Revision: D20850925\n\nPulled By: myleott\n\nfbshipit-source-id: df12b051f2eae3566a1f4cd1b621ed1c8fdf0050\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class StopwatchMeter(Meter):",
            "if self.start_time is not None:",
            "delta = time.perf_counter() - self.start_time",
            "self.sum = self.sum + delta",
            "-            self.n = self.n + n",
            "+            self.n = type_as(self.n, n) + n",
            "",
            "def reset(self):",
            "self.sum = 0  # cumulative time during which stopwatch was active"
        ]
    },
    {
        "number": 259,
        "comments": "update API call for version fix",
        "commit_message": "[style] consistent nn. and nn.functional (#12124)\n\n* consistent nn. and nn.functional\n\n* fix glitch\n\n* fix glitch #2\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class RagTokenForGeneration(RagPreTrainedModel):",
            "n_docs = n_docs if n_docs is not None else self.config.n_docs",
            "",
            "# RAG-token marginalization",
            "-        seq_logprobs = torch.nn.functional.log_softmax(seq_logits, dim=-1).view(",
            "+        seq_logprobs = nn.functional.log_softmax(seq_logits, dim=-1).view(",
            "seq_logits.shape[0] // n_docs, n_docs, -1, seq_logits.size(-1)",
            ")",
            "doc_logprobs = torch.log_softmax(doc_scores, dim=1)"
        ]
    },
    {
        "number": 261,
        "comments": "def",
        "commit_message": "followups to D37592429\n\nSummary: Fixing comments on D37592429 (https://github.com/facebookresearch/pytorch3d/commit/0dce883241ae638b9fa824f34fca9590d5f0782c)\n\nReviewed By: shapovalov\n\nDifferential Revision: D37752367\n\nfbshipit-source-id: 40aa7ee4dc0c5b8b7a84a09d13a3933a9e3afedd\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class HarmonicTimeEncoder(GlobalEncoderBase, torch.nn.Module):",
            "time = frame_timestamp / self.time_divisor",
            "return self._harmonic_embedding(time)  # pyre-ignore: 29",
            "",
            "-    def calc_squared_encoding_norm(self):",
            "-        return 0.0",
            "+    def calculate_squared_encoding_norm(self) -> Optional[torch.Tensor]:",
            "+        return None"
        ]
    },
    {
        "number": 263,
        "comments": "doc update",
        "commit_message": "a few more doc fixes (#4078)\n\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class ElmoLstm(_EncoderBase):",
            "",
            "# Returns",
            "",
            "-        A `torch.Tensor` of shape (num_layers, batch_size, sequence_length, hidden_size),",
            "-        where the num_layers dimension represents the LSTM output from that layer.",
            "+        `torch.Tensor`",
            "+            A `torch.Tensor` of shape (num_layers, batch_size, sequence_length, hidden_size),",
            "+            where the num_layers dimension represents the LSTM output from that layer.",
            "\"\"\"",
            "batch_size, total_sequence_length = mask.size()",
            "stacked_sequence_output, final_states, restoration_indices = self.sort_and_run_forward("
        ]
    },
    {
        "number": 265,
        "comments": "add param for resource fix",
        "commit_message": "[train][docs] update docstrings/quickstarts to work when `use_gpu=True` (#31692)\n\nFixes Trainer docstrings and quickstarts to work when use_gpu=True.\n\nSigned-off-by: Matthew Deng <matt@anyscale.com>\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class HorovodTrainer(DataParallelTrainer):",
            "),",
            ")",
            "train_dataset = ray.data.from_items([{\"x\": x, \"y\": x + 1} for x in range(32)])",
            "-        scaling_config = ScalingConfig(num_workers=3)",
            "-        # If using GPUs, use the below scaling config instead.",
            "-        # scaling_config = ScalingConfig(num_workers=3, use_gpu=True)",
            "+        scaling_config = ScalingConfig(num_workers=3, use_gpu=use_gpu)",
            "trainer = HorovodTrainer(",
            "train_loop_per_worker=train_loop_per_worker,",
            "scaling_config=scaling_config,"
        ]
    },
    {
        "number": 266,
        "comments": "remove API call for version fix",
        "commit_message": "upgrade to pytorch 0.4.0 (#1126)\n\n* bump pytorch to 0.4 + fix sanitize\n\n* remove check for Variable in block_orthogonal\n\n* rename parameter split_size=\n\n* remove checks for Variable\n\n* fix more tests\n\n* fixes\n\n* get tests to pass\n\n* fix warnings\n\n* get rid of some of the Variables\n\n* more tests passing\n\n* more elimination of variables\n\n* finish removing all Variables\n\n* pylint and such\n\n* a few fixes\n\n* move torch.no_grad into model.forward_on_instances\n\n* more pytorch 0.4 changes\n\n* detach() -> data\n\n* pylint\n\n* fix bad tensor creation\n\n* fix types\n\n* remove print statement\n\n* more 0.4 goodness\n\n* factor out is_tensor\n\n* add no_grad to elmo command\n\n* cleanup\n\n* remove TODO\n\n* address PR feedback\n\n* replace all() with item()\n\n* more cleanup\n\n* further cleanup\n\n* remove Variable\n\n* really fix merge conflict\n\n* fix pylint\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class TestStackedSelfAttention(AllenNlpTestCase):",
            "feedforward_hidden_dim=5,",
            "num_layers=3,",
            "num_attention_heads=3)",
            "-        inputs = Variable(torch.randn([3, 5, 9]))",
            "+        inputs = torch.randn([3, 5, 9])",
            "encoder_output = encoder(inputs, None)",
            "assert list(encoder_output.size()) == [3, 5, 12]"
        ]
    },
    {
        "number": 267,
        "comments": "change condition check for version fix",
        "commit_message": "PyTorch 1.4 compat\n\nSummary: Restore compatibility with PyTorch 1.4 and 1.5, and a few lint fixes.\n\nReviewed By: patricklabatut\n\nDifferential Revision: D30048115\n\nfbshipit-source-id: ee05efa7c625f6079fb06a3cc23be93e48df9433\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def qr(A: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:  # pragma: no cove",
            "\"\"\"",
            "Like torch.linalg.qr.",
            "\"\"\"",
            "-    if hasattr(torch.linalg, \"qr\"):",
            "+    if hasattr(torch, \"linalg\") and hasattr(torch.linalg, \"qr\"):",
            "# PyTorch version >= 1.9",
            "return torch.linalg.qr(A)",
            "return torch.qr(A)"
        ]
    },
    {
        "number": 268,
        "comments": "add param for resource fix",
        "commit_message": "fix test (#9669)\n\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def prepare_bart_inputs_dict(",
            "if decoder_attention_mask is None:",
            "decoder_attention_mask = decoder_input_ids.ne(config.pad_token_id)",
            "if head_mask is None:",
            "-        head_mask = torch.ones(config.encoder_layers, config.encoder_attention_heads)",
            "+        head_mask = torch.ones(config.encoder_layers, config.encoder_attention_heads, device=torch_device)",
            "if decoder_head_mask is None:",
            "-        decoder_head_mask = torch.ones(config.decoder_layers, config.decoder_attention_heads)",
            "+        decoder_head_mask = torch.ones(config.decoder_layers, config.decoder_attention_heads, device=torch_device)",
            "return {",
            "\"input_ids\": input_ids,",
            "\"decoder_input_ids\": decoder_input_ids,"
        ]
    },
    {
        "number": 269,
        "comments": "assertion test",
        "commit_message": "[Pytorch] pytorch only timesteps (#724)\n\n* pytorch timesteps\n\n* style\n\n* get rid of if-else\n\n* fix test\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class PNDMSchedulerTest(SchedulerCommonTest):",
            "scheduler_config = self.get_scheduler_config(steps_offset=1)",
            "scheduler = scheduler_class(**scheduler_config)",
            "scheduler.set_timesteps(10)",
            "-        assert np.equal(",
            "+        assert torch.equal(",
            "scheduler.timesteps,",
            "-            np.array([901, 851, 851, 801, 801, 751, 751, 701, 701, 651, 651, 601, 601, 501, 401, 301, 201, 101, 1]),",
            "-        ).all()",
            "+            torch.LongTensor(",
            "+                [901, 851, 851, 801, 801, 751, 751, 701, 701, 651, 651, 601, 601, 501, 401, 301, 201, 101, 1]",
            "+            ),",
            "+        )",
            "",
            "def test_betas(self):",
            "for beta_start, beta_end in zip([0.0001, 0.001], [0.002, 0.02]):"
        ]
    },
    {
        "number": 270,
        "comments": "add param for state fix",
        "commit_message": "[RLlib] Dreamer fixes and reinstate Dreamer test. (#17821)\n\nCo-authored-by: sven1977 <svenmika1977@gmail.com>\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class DreamerModel(TorchModelV2, nn.Module):",
            "and policy to obtain action.",
            "\"\"\"",
            "if state is None:",
            "-            self.initial_state()",
            "+            self.state = self.get_initial_state(batch_size=obs.shape[0])",
            "else:",
            "self.state = state",
            "post = self.state[:4]"
        ]
    },
    {
        "number": 271,
        "comments": "add condition check for null fix",
        "commit_message": "multiple bug fixes\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class Plan(Serializable):",
            "# prevent circular dependency",
            "# syft relative",
            "from ...core.node.vm.vm import VirtualMachine  # noqa: F401",
            "+        if self.local_executor is not None:",
            "+            # this is necessary for syfts nn.module, because the plan contains state from the module",
            "+            # in order to use this state, we first need to send the model, and then execute te plan",
            "+            return self.local_executor(**kwargs)",
            "",
            "alice = VirtualMachine(name=\"plan_executor\")",
            "alice_client: client.Client = alice.get_client()"
        ]
    },
    {
        "number": 272,
        "comments": "change API call for math fix",
        "commit_message": "more fixes\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class GradTTS(DiffusionPipeline):",
            "mu_y = mu_y.transpose(1, 2)",
            "",
            "# Sample latent representation from terminal distribution N(mu_y, I)",
            "-        z = mu_y + torch.randn_like(mu_y, device=mu_y.device) / temperature",
            "+        z = mu_y + torch.randn(mu_y.shape, device=mu_y.device, generator=generator) / temperature",
            "",
            "xt = z * y_mask",
            "h = 1.0 / num_inference_steps"
        ]
    },
    {
        "number": 273,
        "comments": "add API call for type fix",
        "commit_message": "Data2vec prelim (#2929)\n\nSummary:\nPreliminaries for data2vec release, include some minor improvements and bug fixes\n\nMost important change is that we now default to raising an exception when fields in config do not have a corresponding field in the model dataclass\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2929\n\nReviewed By: wnhsu\n\nDifferential Revision: D33649708\n\nPulled By: alexeib\n\nfbshipit-source-id: 629bdb4c361550740b451c570c2005bb956c6fcb\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class NanDetector:",
            "gradients = {}",
            "for name, param in self.named_parameters:",
            "if param.grad is not None:",
            "-                grad_norm = torch.norm(param.grad.data, p=2, dtype=torch.float32)",
            "+                grad_norm = torch.norm(param.grad.data.float(), p=2)",
            "norm[name] = grad_norm.item()",
            "if torch.isnan(grad_norm).any() or torch.isinf(grad_norm).any():",
            "gradients[name] = param.grad.data"
        ]
    },
    {
        "number": 274,
        "comments": "add condition check for null fix",
        "commit_message": "Mixup and prefetcher improvements\n* Do mixup in custom collate fn if prefetcher enabled, reduces performance impact\n* Move mixup code to own file\n* Add arg to disable prefetcher\n* Fix no cuda transfer when prefetcher off\n* Random erasing when prefetcher off wasn't changed to match new args, fixed\n* Default random erasing to off (prob = 0.) for train\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def create_loader(",
            "# of samples per-process, will slightly alter validation results",
            "sampler = OrderedDistributedSampler(dataset)",
            "",
            "+    if collate_fn is None:",
            "+        collate_fn = fast_collate if use_prefetcher else torch.utils.data.dataloader.default_collate",
            "+",
            "loader = torch.utils.data.DataLoader(",
            "dataset,",
            "batch_size=batch_size,",
            "shuffle=sampler is None and is_training,",
            "num_workers=num_workers,",
            "sampler=sampler,",
            "-        collate_fn=fast_collate if use_prefetcher else torch.utils.data.dataloader.default_collate,",
            "+        collate_fn=collate_fn,",
            "drop_last=is_training,",
            ")",
            "if use_prefetcher:"
        ]
    },
    {
        "number": 276,
        "comments": "format",
        "commit_message": "Fix build test. (#1057)\n\n* 2.2.1 release\n\n* Fix build test.\n\n* apply yapf\n\n* ping yapf to 0.28.0\n\n* fix build\n\n* use yapf 0.29\n\n* fix yapf\n\n* include tests in make format.\n\n* ping autoflake and isort version.\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def set_gpu_fraction(gpu_fraction=0.3):",
            "",
            "",
            "def train_epoch(",
            "-        network, X, y, cost, train_op=tf.optimizers.Adam(learning_rate=0.0001), acc=None, batch_size=100, shuffle=True",
            "+    network, X, y, cost, train_op=tf.optimizers.Adam(learning_rate=0.0001), acc=None, batch_size=100, shuffle=True",
            "):",
            "\"\"\"Training a given non time-series network by the given cost function, training data, batch_size etc.",
            "for one epoch."
        ]
    },
    {
        "number": 277,
        "comments": "log update",
        "commit_message": "fix deprecations about casting & initializers in tf1.13\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def fpn_map_rois_to_levels(boxes):",
            "Be careful that the returned tensor could be empty.",
            "\"\"\"",
            "sqrtarea = tf.sqrt(tf_area(boxes))",
            "-    level = tf.to_int32(tf.floor(",
            "-        4 + tf.log(sqrtarea * (1. / 224) + 1e-6) * (1.0 / np.log(2))))",
            "+    level = tf.cast(tf.floor(",
            "+        4 + tf.log(sqrtarea * (1. / 224) + 1e-6) * (1.0 / np.log(2))), tf.int32)",
            "",
            "# RoI levels range from 2~5 (not 6)",
            "level_ids = ["
        ]
    },
    {
        "number": 278,
        "comments": "change param for math fix",
        "commit_message": "bugfix in cotcurv laplacian loss. closes #551 (#553)\n\nSummary: Pull Request resolved: https://github.com/facebookresearch/pytorch3d/pull/553\n\nReviewed By: theschnitz\n\nDifferential Revision: D26257591\n\nPulled By: gkioxari\n\nfbshipit-source-id: 899a3f733a77361e8572b0900a34b55764ff08f2\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def mesh_laplacian_smoothing(meshes, method: str = \"uniform\"):",
            "elif method == \"cot\":",
            "loss = L.mm(verts_packed) * norm_w - verts_packed",
            "elif method == \"cotcurv\":",
            "-        loss = (L.mm(verts_packed) - verts_packed) * norm_w",
            "+        loss = (L.mm(verts_packed) - L_sum * verts_packed) * norm_w",
            "loss = loss.norm(dim=1)",
            "",
            "loss = loss * weights"
        ]
    },
    {
        "number": 279,
        "comments": "not clear",
        "commit_message": "Update DARTS trainer and fix docstring issues (#1772)\n\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def apply_fixed_architecture(model, fixed_arc_path, device=None):",
            "architecture = FixedArchitecture(model, fixed_arc)",
            "architecture.to(device)",
            "architecture.reset()",
            "+    return architecture"
        ]
    },
    {
        "number": 280,
        "comments": "add condition check for resource fix",
        "commit_message": "bug fix\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class AsyncMultiGPUTrainer(MultiGPUTrainer,",
            "",
            "self._setup_predictor_factory(predict_tower)",
            "self._average_gradient = average_gradient",
            "+        assert tf.test.is_gpu_available()",
            "",
            "def _setup(self):",
            "super(AsyncMultiGPUTrainer, self)._setup()"
        ]
    },
    {
        "number": 281,
        "comments": "add value to list",
        "commit_message": "Fix Autoformer to compatible with RandomOneShot strategy (#4987)\n\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def test_hub_oneshot(space_type, strategy_type):",
            "NDS_SPACES = ['amoeba', 'darts', 'pnas', 'enas', 'nasnet']",
            "if strategy_type == 'proxyless':",
            "if 'width' in space_type or 'depth' in space_type or \\",
            "-                any(space_type.startswith(prefix) for prefix in NDS_SPACES + ['proxylessnas', 'mobilenetv3']):",
            "+                any(space_type.startswith(prefix) for prefix in NDS_SPACES + ['proxylessnas', 'mobilenetv3', 'autoformer']):",
            "pytest.skip('The space has used unsupported APIs.')",
            "if strategy_type in ['darts', 'gumbel'] and space_type == 'mobilenetv3':",
            "pytest.skip('Skip as it consumes too much memory.')"
        ]
    },
    {
        "number": 282,
        "comments": "add param for argument fix",
        "commit_message": "fixed self-loop bug in gcn\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class GCNConv(MessagePassing):",
            "x = torch.matmul(x, self.weight)",
            "",
            "if not self.cached or self.cached_result is None:",
            "-            edge_index, norm = GCNConv.norm(edge_index,",
            "-                                            x.size(0), edge_weight,",
            "+            edge_index, norm = GCNConv.norm(edge_index, x.size(0), edge_weight,",
            "self.improved, x.dtype)",
            "self.cached_result = edge_index, norm"
        ]
    },
    {
        "number": 283,
        "comments": "add API call for shape fix",
        "commit_message": "fix for asr_mix\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class CTC(torch.nn.Module):",
            "if self.ctc_type == \"builtin\":",
            "olens = to_device(ys_hat, torch.LongTensor([len(s) for s in ys]))",
            "hlens = hlens.long()",
            "+            ys_pad = torch.cat(ys)  # without this the code breaks for asr_mix",
            "self.loss = self.loss_fn(ys_hat, ys_pad, hlens, olens)",
            "else:",
            "self.loss = None"
        ]
    },
    {
        "number": 284,
        "comments": "def",
        "commit_message": "Fixes calibration and adds example scripts (#2431)\n\n* Adds calibration to binary and category output feature schema.\n\n* Adds type annotations for create_calibration_module.\n\n* Fixes initialization of calibration module for category features.\n\n* First pass at forest cover and mushroom edibility.\n\n* Fixed brier plot.\n\n* Adds forest cover visualizations.\n\n* Reduce epochs to 1 and default transformer params.\n\n* Adds calibration as an output feature key which should not be nested inside decoder.\n\n* Moved output_features below input_features.\n\nCo-authored-by: Daniel Treiman <daniel@predibase.com>\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class CategoryOutputFeature(CategoryFeatureMixin, OutputFeature):",
            "# hidden: shape [batch_size, size of final fully connected layer]",
            "return {LOGITS: self.decoder_obj(hidden), PROJECTION_INPUT: hidden}",
            "",
            "-    def create_calibration_module(self, feature) -> torch.nn.Module:",
            "+    def create_calibration_module(self, feature: CategoryOutputFeatureConfig) -> torch.nn.Module:",
            "\"\"\"Creates the appropriate calibration module based on the feature config.",
            "",
            "Today, only one type of calibration (\"temperature_scaling\") is available, but more options may be supported in",
            "the future.",
            "\"\"\"",
            "-        if feature.get(\"calibration\"):",
            "+        if feature.calibration:",
            "calibration_cls = calibration.get_calibration_cls(CATEGORY, \"temperature_scaling\")",
            "return calibration_cls(num_classes=self.num_classes)",
            "return None"
        ]
    },
    {
        "number": 285,
        "comments": "change value",
        "commit_message": "fixed lstm, added gru, other fixes\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class PiecewiseConstant(Parameter):",
            "self.values = values",
            "",
            "def get_parameter_value(self):",
            "-        if self.unit == 'timestep':",
            "+        if self.unit == 'timesteps':",
            "step = Module.retrieve_tensor(name='timestep')",
            "-        elif self.unit == 'episode':",
            "+        elif self.unit == 'episodes':",
            "step = Module.retrieve_tensor(name='episode')",
            "",
            "+        # step = tf.Print(step, (step,))",
            "+",
            "parameter = tf.train.piecewise_constant(",
            "x=step, boundaries=self.boundaries, values=self.values",
            ")"
        ]
    },
    {
        "number": 286,
        "comments": "test fix",
        "commit_message": "fix tests on ubuntu\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "def test_gcn_conv():",
            "assert out2.size() == (4, 32)",
            "assert torch.allclose(conv(x, adj2.t()), out2, atol=1e-6)",
            "",
            "-    torch.jit.script(conv.jittable())",
            "-",
            "t = '(Tensor, Tensor, OptTensor) -> Tensor'",
            "jit = torch.jit.script(conv.jittable(t))",
            "assert jit(x, edge_index).tolist() == out1.tolist()"
        ]
    },
    {
        "number": 287,
        "comments": "no API",
        "commit_message": "[Flax SD finetune] Fix dtype (#1038)\n\nfix jnp dtype\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "def main():",
            "train_dataset, shuffle=True, collate_fn=collate_fn, batch_size=total_train_batch_size, drop_last=True",
            ")",
            "",
            "-    weight_dtype = torch.float32",
            "+    weight_dtype = jnp.float32",
            "if args.mixed_precision == \"fp16\":",
            "-        weight_dtype = torch.float16",
            "+        weight_dtype = jnp.float16",
            "elif args.mixed_precision == \"bf16\":",
            "-        weight_dtype = torch.bfloat16",
            "+        weight_dtype = jnp.bfloat16",
            "",
            "# Load models and create wrapper for stable diffusion",
            "tokenizer = CLIPTokenizer.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"tokenizer\")"
        ]
    },
    {
        "number": 290,
        "comments": "format",
        "commit_message": "Added reversed operations to RST (#4508)\n\n* initialized replicated sharing tensor\n\n* removed crypto provider for now\n\n* added reconstruction implementation and sharing from native tensor\n\n* cleaning\n\n* cleaning\n\n* cleaning\n\n* added more tests\n\n* added private sharing\n\n* added public addetion and general addetion\n\n* added subtraction\n\n* added negative results and refactors\n\n* fix mod_to_real and switch priavte public\n\n* cleaning\n\n* communication rounds optimization\n\n* more communication rounds optimization\n\n* cleaning\n\n* added edge cases to tests\n\n* cleaning\n\n* more cleaning\n\n* even more cleaning\n\n* even more cleaning\n\n* arrange workers fix\n\n* add and sub with operator\n\n* added public multiplication\n\n* cleaning\n\n* more cleaning\n\n* more cleaning\n\n* added private multiplication without correlated randomness\n\n* formatting\n\n* added matrix multiplication\n\n* added public matrix multiplication\n\n* cleaning\n\n* cleaning\n\n* added shape, apply to shares\n\n* added correlated randomness\n\n* przs edits\n\n* cleaning\n\n* support for bigger ring size\n\n* convolution support wootwoot!!\ud83c\udf89\ud83c\udf89\n\n* cleaning\n\n* more cleaning\n\n* black box principle\n\n* negative tests\n\n* cleaning\n\n* added support for 1-xor 2-consecutive arithmetic 3- arbitrary ring_size\n\n* cleaning\n\n* cleaning\n\n* more cleaning, information hiding\n\n* falcon_tensor.py, falcon.py, falcon_helper\n\n* moved xor to falcon_tensor.py\n\n* clean\n\n* removed non-tensor operations from RST\n\n* merge master\n\n* cleaning\n\n* fixed black version\n\n* fixing black\n\n* added reversed operations to RST\n\nCo-authored-by: Muhammed Abogazia <abogaziah@users.noreply.github.com>\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class ReplicatedSharingTensor(AbstractTensor):",
            "return self.__private_multiplication_operation(secret, mul)",
            "",
            "__mul__ = mul",
            "+    __rmul__ = mul",
            "",
            "def matmul(self, value):",
            "return self.__switch_public_private(value, self.__public_matmul, self.__private_matmul)"
        ]
    },
    {
        "number": 293,
        "comments": "remove version fixcheck",
        "commit_message": "Various fixes (#2127)\n\nSummary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/2127\n\nReviewed By: ngoyal2707\n\nDifferential Revision: D21550962\n\nPulled By: myleott\n\nfbshipit-source-id: ddbe3f287f170862378e0702fc378a4fe400793a\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class TestJitLSTMModel(unittest.TestCase):",
            "scripted_model = torch.jit.script(model)",
            "self._test_save_and_load(scripted_model)",
            "",
            "-    @unittest.skipIf(",
            "-        torch.__version__ < \"1.6.0\", \"Targeting OSS scriptability for the 1.6 release\"",
            "-    )",
            "def test_assert_jit_vs_nonjit_(self):",
            "task, parser = get_dummy_task_and_parser()",
            "LSTMModel.add_args(parser)"
        ]
    },
    {
        "number": 294,
        "comments": "format",
        "commit_message": "Quantisation (#5706)\n\n* empty\n\n* sq\n\n* obs\n\n\n* int\n\n* ts\n\n* helpers\n\n* chlog\n\n* yapf\n\n* avg\n\n* dupl\n\n* Apply suggestions from code review\n\nCo-authored-by: Nicki Skafte <skaftenicki@gmail.com>\n\n* Apply suggestions from code review\n\nCo-authored-by: Carlos Mochol\u00ed <carlossmocholi@gmail.com>\n\n* fixes\n\n* Apply suggestions from code review\n\nCo-authored-by: Carlos Mochol\u00ed <carlossmocholi@gmail.com>\n\n* fixes\n\n* note\n\n* warn\n\n* 45\n\n* link\n\nCo-authored-by: Carlos Mochol\u00ed <carlossmocholi@gmail.com>\n\n* Apply suggestions from code review\n\nCo-authored-by: Carlos Mochol\u00ed <carlossmocholi@gmail.com>\n\n* yapf\n\n* flake8\n\n* Apply suggestions from code review\n\n* Apply suggestions from code review\n\nCo-authored-by: Nicki Skafte <skaftenicki@gmail.com>\nCo-authored-by: Carlos Mochol\u00ed <carlossmocholi@gmail.com>\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def test_result_reduce_ddp(result_cls):",
            "pytest.param(5, False, 0, id='nested_list_predictions'),",
            "pytest.param(6, False, 0, id='dict_list_predictions'),",
            "pytest.param(7, True, 0, id='write_dict_predictions'),",
            "-        pytest.param(",
            "-            0,",
            "-            True,",
            "-            1,",
            "-            id='full_loop_single_gpu',",
            "-            marks=pytest.mark.skipif(torch.cuda.device_count() < 1, reason=\"test requires single-GPU machine\")",
            "-        )",
            "+        pytest.param(0, True, 1, id='full_loop_single_gpu', marks=pytest.mark.skipif(**_SKIPIF_ARGS_NO_GPU))",
            "]",
            ")",
            "def test_result_obj_predictions(tmpdir, test_option, do_train, gpus):"
        ]
    },
    {
        "number": 295,
        "comments": "format",
        "commit_message": "flops counter formatting fix (#3837)\n\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def count_flops_params(model, x, custom_ops=None, verbose=True, mode='default'):",
            "print(f'FLOPs total: {profiler.sum_flops()}')",
            "print(f'#Params total: {profiler.sum_params()}')",
            "",
            "-    return profiler.sum_flops(), profiler.sum_params(), profiler.results",
            "\\ No newline at end of file",
            "+    return profiler.sum_flops(), profiler.sum_params(), profiler.results"
        ]
    },
    {
        "number": 296,
        "comments": "add API call for resource fix",
        "commit_message": "Fix torch device issues (#20304)\n\n* fix device issue\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class DeformableDetrModelIntegrationTests(unittest.TestCase):",
            "results = feature_extractor.post_process_object_detection(",
            "outputs, threshold=0.3, target_sizes=[image.size[::-1]]",
            ")[0]",
            "-        expected_scores = torch.tensor([0.7999, 0.7894, 0.6331, 0.4720, 0.4382])",
            "+        expected_scores = torch.tensor([0.7999, 0.7894, 0.6331, 0.4720, 0.4382]).to(torch_device)",
            "expected_labels = [17, 17, 75, 75, 63]",
            "-        expected_slice_boxes = torch.tensor([16.5028, 52.8390, 318.2544, 470.7841])",
            "+        expected_slice_boxes = torch.tensor([16.5028, 52.8390, 318.2544, 470.7841]).to(torch_device)",
            "",
            "self.assertEqual(len(results[\"scores\"]), 5)",
            "self.assertTrue(torch.allclose(results[\"scores\"], expected_scores, atol=1e-4))"
        ]
    },
    {
        "number": 297,
        "comments": "change param for math fix",
        "commit_message": "tl.layers API Refactoring and various modifications (#667)\n\n* Decorators API Refactored\n\n* extra_requires `all`, `all_cpu` and `all_gpu` added\n\n* Error fix\n\n* YAPF Formating Correction\n\n* Test for private method decorator added\n\n* Test Logging Verbosity Fixed to Debug when runned individually\n\n* YAPF corrections applied\n\n* Changelog Added\n\n* Changelog updated\n\n* PR number changed\n\n* First Refactoring Pass done\n\n* cleaning second pass\n\n* Refactoring 3rd pass\n\n* Refactoring 4th Pass\n\n* Code Error fix\n\n* YAPF Formating Fix\n\n* Arguments now using self\n\n* YAPF error correction\n\n* Bug Fix in Decorator\n\n* act name bug fix\n\n* Error Correction\n\n* YAPF formating fix\n\n* Useless tf.identity removed\n\n* Error Fix\n\n* Changelog Updated\n\n* Error fix in tl.activation\n\n* Documentation error fix\n\n* Lazy Import added\n\n* Import Refactoring with LazyImport when necessary\n\n* Changelog Updated\n\n* Gitter Removed\n\n* Fixed proposed by @zsdonghao\n\n* Documentation updated\n\n* Missing requirements added\n\n* Update to TensorLayer 1.8.6rc1\n\n* Requirements error fix\n\n* Docker Files updated\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class Critic(object):",
            "n = InputLayer(self.s, name='in')",
            "n = DenseLayer(n, n_units=30, act=tf.nn.relu6, W_init=tf.random_uniform_initializer(0, 0.01), name='hidden')",
            "# n = DenseLayer(n, n_units=5, act=tf.nn.relu, W_init=tf.random_uniform_initializer(0, 0.01), name='hidden2')",
            "-            n = DenseLayer(n, n_units=1, act=tf.identity, name='V')",
            "+            n = DenseLayer(n, n_units=1, act=None, name='V')",
            "self.v = n.outputs",
            "",
            "with tf.variable_scope('squared_TD_error'):"
        ]
    },
    {
        "number": 298,
        "comments": "update API call for refactor fix",
        "commit_message": "Fix wrong replacement\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class TensorflowBackendInferenceLearner(TensorflowBaseInferenceLearner):",
            "metadata = LearnerMetadata.read(path)",
            "network_parameters = ModelParams(**metadata.network_parameters)",
            "input_tfms = metadata.input_tfms",
            "-        model = nebullvm.operations.inference_learners.utils.load_model(",
            "+        model = tf.keras.models.load_model(",
            "path / TENSORFLOW_BACKEND_FILENAMES[\"tf_model\"]",
            ")",
            "device = Device(metadata.device)"
        ]
    },
    {
        "number": 300,
        "comments": "add API call for state fix",
        "commit_message": "[RLlib] Curiosity minor fixes, do-overs, and testing. (#10143)\n\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "FileType = Any",
            "# Represents the result dict returned by Trainer.train().",
            "ResultDict = dict",
            "",
            "+# A tf or torch local optimizer object.",
            "+LocalOptimizer = Union[\"tf.keras.optimizers.Optimizer\",",
            "+                       \"torch.optim.Optimizer\"]",
            "+",
            "# Dict of tensors returned by compute gradients on the policy, e.g.,",
            "# {\"td_error\": [...], \"learner_stats\": {\"vf_loss\": ..., ...}}, for multi-agent,",
            "# {\"policy1\": {\"learner_stats\": ..., }, \"policy2\": ...}."
        ]
    },
    {
        "number": 301,
        "comments": "method define",
        "commit_message": "Refactor triton buffer to use CLBuffer of cuda runtime (#524)\n\n* Refactor triton buffer to use CLBuffer of runtime\n\n* Fix opencl GT0\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class CLImage:",
            "",
            "class CLBuffer:",
            "def __init__(self, size): self.cl = cuda.mem_alloc(size)",
            "-  def copyin(self, b:np.ndarray): cuda.memcpy_htod_async(self.cl, b)",
            "+  def copyin(self, b:np.ndarray, stream:Optional[cuda.Stream]=None): cuda.memcpy_htod_async(self.cl, b, stream)",
            "def copyout(self, a:np.ndarray): cuda.memcpy_dtoh(a, self.cl)",
            "",
            "class CLProgram:"
        ]
    },
    {
        "number": 302,
        "comments": "value change",
        "commit_message": "Fix RMSLE metric (#3188)\n\n* fix rmsle\n\n* Updated test to match rmsle fix\n\n* Updated RMSLE example result to match functional\n\n* chlog\n\n* add randomized test\n\n* fix pep8\n\nCo-authored-by: Jirka Borovec <jirka@pytorchlightning.ai>\nCo-authored-by: Nicki Skafte <skaftenicki@gmail.com>\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "def rmsle(",
            ">>> x = torch.tensor([0., 1, 2, 3])",
            ">>> y = torch.tensor([0., 1, 2, 2])",
            ">>> rmsle(x, y)",
            "-        tensor(0.0207)",
            "+        tensor(0.1438)",
            "",
            "\"\"\"",
            "-    rmsle = mse(torch.log(pred + 1), torch.log(target + 1), reduction=reduction)",
            "+    rmsle = rmse(torch.log(pred + 1), torch.log(target + 1), reduction=reduction)",
            "return rmsle"
        ]
    },
    {
        "number": 303,
        "comments": "test fix",
        "commit_message": "More unit test fixes\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "def add_dataset_args(parser, train=False, gen=False):",
            "",
            "def add_distributed_training_args(parser):",
            "group = parser.add_argument_group('Distributed training')",
            "-    group.add_argument('--distributed-world-size', default=1, type=int, metavar='N',",
            "-                       help='total number of GPUs across all nodes, default: 1 GPU')",
            "+    group.add_argument('--distributed-world-size', type=int, metavar='N',",
            "+                       default=torch.cuda.device_count(),",
            "+                       help='total number of GPUs across all nodes (default: all visible GPUs)')",
            "group.add_argument('--distributed-rank', default=0, type=int,",
            "help='rank of the current worker')",
            "group.add_argument('--distributed-backend', default='nccl', type=str,"
        ]
    },
    {
        "number": 304,
        "comments": "format",
        "commit_message": "Fixed format in some files\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class DCCRNSeparator(AbsSeparator):",
            "self.flatten_parameters()",
            "",
            "def forward(",
            "-        self,",
            "-        input: Union[torch.Tensor, ComplexTensor],",
            "+        self,",
            "+        input: Union[torch.Tensor, ComplexTensor],",
            "ilens: torch.Tensor,",
            "additional: Optional[Dict] = None,",
            ") -> Tuple[List[Union[torch.Tensor, ComplexTensor]], torch.Tensor, OrderedDict]:"
        ]
    },
    {
        "number": 305,
        "comments": "add param for type fix",
        "commit_message": "Added dtype parameter to zeros_like and ones_like (#5062)\n\n* Fixed checking input masks in Layer.compute_mask\n\n* Added dtype parameter to zeros_like and ones_like\n\n* Fix existing docstring for ones_like and zeros_like\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def ones_like(x, name=None):",
            "[ 1.,  1.,  1.]], dtype=float32)",
            "```",
            "\"\"\"",
            "-    return tf.ones_like(x, name=name)",
            "+    return tf.ones_like(x, dtype=dtype, name=name)",
            "",
            "",
            "def random_uniform_variable(shape, low, high, dtype=None,"
        ]
    },
    {
        "number": 306,
        "comments": "rename param for method",
        "commit_message": "YOLOv5 Apple Metal Performance Shader (MPS) support (#7878)\n\n* Apple Metal Performance Shader (MPS) device support\n\nFollowing https://pytorch.org/blog/introducing-accelerated-pytorch-training-on-mac/\n\nShould work with Apple M1 devices with PyTorch nightly installed with command `--device mps`. Usage examples:\n```bash\npython train.py --device mps\npython detect.py --device mps\npython val.py --device mps\n```\n\n* Update device strategy to fix MPS issue\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "class Ensemble(nn.ModuleList):",
            "return y, None  # inference, train output",
            "",
            "",
            "-def attempt_load(weights, map_location=None, inplace=True, fuse=True):",
            "+def attempt_load(weights, device=None, inplace=True, fuse=True):",
            "from models.yolo import Detect, Model",
            "",
            "# Loads an ensemble of models weights=[a,b,c] or a single model weights=[a] or weights=a",
            "model = Ensemble()",
            "for w in weights if isinstance(weights, list) else [weights]:",
            "-        ckpt = torch.load(attempt_download(w), map_location=map_location)  # load",
            "-        ckpt = (ckpt.get('ema') or ckpt['model']).float()  # FP32 model",
            "+        ckpt = torch.load(attempt_download(w))",
            "+        ckpt = (ckpt.get('ema') or ckpt['model']).to(device).float()  # FP32 model",
            "model.append(ckpt.fuse().eval() if fuse else ckpt.eval())  # fused or un-fused model in eval mode",
            "",
            "# Compatibility updates"
        ]
    },
    {
        "number": 307,
        "comments": "remove API call for version fix",
        "commit_message": "upgrade to pytorch 0.4.0 (#1126)\n\n* bump pytorch to 0.4 + fix sanitize\n\n* remove check for Variable in block_orthogonal\n\n* rename parameter split_size=\n\n* remove checks for Variable\n\n* fix more tests\n\n* fixes\n\n* get tests to pass\n\n* fix warnings\n\n* get rid of some of the Variables\n\n* more tests passing\n\n* more elimination of variables\n\n* finish removing all Variables\n\n* pylint and such\n\n* a few fixes\n\n* move torch.no_grad into model.forward_on_instances\n\n* more pytorch 0.4 changes\n\n* detach() -> data\n\n* pylint\n\n* fix bad tensor creation\n\n* fix types\n\n* remove print statement\n\n* more 0.4 goodness\n\n* factor out is_tensor\n\n* add no_grad to elmo command\n\n* cleanup\n\n* remove TODO\n\n* address PR feedback\n\n* replace all() with item()\n\n* more cleanup\n\n* further cleanup\n\n* remove Variable\n\n* really fix merge conflict\n\n* fix pylint\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "from allennlp.common.params import Params",
            "",
            "class TestStackedBidirectionalLstm(AllenNlpTestCase):",
            "def test_stacked_bidirectional_lstm_completes_forward_pass(self):",
            "-        input_tensor = torch.autograd.Variable(torch.rand(4, 5, 3))",
            "+        input_tensor = torch.rand(4, 5, 3)",
            "input_tensor[1, 4:, :] = 0.",
            "input_tensor[2, 2:, :] = 0.",
            "input_tensor[3, 1:, :] = 0."
        ]
    },
    {
        "number": 309,
        "comments": "add param for argument fix",
        "commit_message": "fixing CTRL tests and OpenAI GPT tests\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class TFCTRLMainLayer(tf.keras.layers.Layer):",
            "token_type_embeds = 0",
            "position_ids = tf.reshape(position_ids, [-1, shape_list(position_ids)[-1]])",
            "",
            "-        inputs_embeds = self.w(input_ids)",
            "+        inputs_embeds = self.w(input_ids, mode='embedding')",
            "# x = embedded.unsqueeze(0) if len(input_ids.shape)<2 else embedded",
            "seq_len = input_shape[-1]",
            "mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)"
        ]
    },
    {
        "number": 310,
        "comments": "format",
        "commit_message": "Fix issue with non-canonical TF version name format.\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def _preprocess_conv3d_input(x, data_format):",
            "A tensor.",
            "\"\"\"",
            "# tensorflow doesn't support float64 for conv layer before 1.8.0",
            "-    if (dtype(x) == 'float64'",
            "-            and StrictVersion(tf.__version__) < StrictVersion('1.8.0')):",
            "+    if (dtype(x) == 'float64' and",
            "+            StrictVersion(tf.__version__.split('-')[0]) < StrictVersion('1.8.0')):",
            "x = tf.cast(x, 'float32')",
            "tf_data_format = 'NDHWC'",
            "if data_format == 'channels_first':"
        ]
    },
    {
        "number": 311,
        "comments": "add value",
        "commit_message": "Fix xtreme s metrics (#3957)\n\n* make wer more robust\n\n* [XTREME-S Metrics] Add babel again\n\n* fix\n\n* make style\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class XtremeS(datasets.Metric):",
            "tokenize=tokenize,",
            "use_effective_order=use_effective_order,",
            ")",
            "-        elif self.config_name in [\"fleurs-asr\", \"mls\", \"voxpopuli\"]:",
            "+        elif self.config_name in [\"fleurs-asr\", \"mls\", \"voxpopuli\", \"babel\"]:",
            "concatenate_texts = wer_kwargs.pop(\"concatenate_texts\", False)",
            "return wer_and_cer(predictions, references, concatenate_texts, self.config_name)",
            "else:"
        ]
    },
    {
        "number": 312,
        "comments": "remove constraint for version fix",
        "commit_message": "add fix for torch 1.0 on RTD (#1591)\n\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "from __future__ import absolute_import, division, print_function",
            "",
            "+import os",
            "+",
            "import torch",
            "",
            "-assert torch.__version__.startswith('1.')",
            "+if 'READTHEDOCS' not in os.environ:",
            "+    # RTD is running 0.4.1 due to a memory issue with pytorch 1.0",
            "+    assert torch.__version__.startswith('1.')",
            "",
            "",
            "def patch_dependency(target, root_module=torch):"
        ]
    },
    {
        "number": 313,
        "comments": "no API",
        "commit_message": "Fix DataCollatorForWholeWordMask again (#8397)\n\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class DataCollatorForWholeWordMask(DataCollatorForLanguageModeling):",
            "mask_labels = []",
            "for e in examples:",
            "ref_tokens = []",
            "-            for id in e[\"input_ids\"].tolist():",
            "+            for id in tolist(e[\"input_ids\"]):",
            "token = self.tokenizer._convert_id_to_token(id)",
            "ref_tokens.append(token)",
            "",
            "# For Chinese tokens, we need extra inf to mark sub-word, e.g [\u559c,\u6b22]-> [\u559c\uff0c##\u6b22]",
            "if \"chinese_ref\" in e:",
            "-                ref_pos = e[\"chinese_ref\"].tolist()",
            "+                ref_pos = tolist(e[\"chinese_ref\"])",
            "len_seq = e[\"input_ids\"].size(0)",
            "for i in range(len_seq):",
            "if i in ref_pos:"
        ]
    },
    {
        "number": 314,
        "comments": "functional change",
        "commit_message": "Fix bug in get_or_create_layer migration utility that produced regularization losses of the wrong rank, causing failures on model fit.\n\nPiperOrigin-RevId: 414066868\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class _EagerVariableStore(tf.Module):",
            "layer = create_layer_method()",
            "self._layers[name] = layer",
            "if isinstance(layer, base_layer.Layer):",
            "-        self._regularizers[name] = lambda: layer.losses",
            "+        self._regularizers[name] = lambda: tf.math.reduce_sum(layer.losses)",
            "return self._layers[name]",
            "",
            "def add_regularizer(self, var, regularizer):"
        ]
    },
    {
        "number": 315,
        "comments": "add param for argument fix",
        "commit_message": "Fix Padded Batch Error 12282 (#12487)\n\nThis fixes the padded batch [issue](https://github.com/huggingface/transformers/issues/12282). The error was generated due to the maximum sequence length of the attention mask not matching the padded sequence length of the hidden_states. `np.allclose` now passes with a 1e-2 absolute tolerance.\n\nThis change fixes\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class TFWav2Vec2MainLayer(tf.keras.layers.Layer):",
            "if inputs[\"attention_mask\"] is not None:",
            "# compute real output lengths according to convolution formula",
            "output_lengths = self._get_feat_extract_output_lengths(tf.reduce_sum(inputs[\"attention_mask\"], -1))",
            "-            attention_mask = tf.sequence_mask(output_lengths, dtype=hidden_states.dtype)",
            "+",
            "+            attention_mask = tf.sequence_mask(",
            "+                output_lengths, maxlen=shape_list(hidden_states)[1], dtype=hidden_states.dtype",
            "+            )",
            "",
            "hidden_states = self.feature_projection(hidden_states, training=inputs[\"training\"])"
        ]
    },
    {
        "number": 317,
        "comments": "functional change",
        "commit_message": "Fix batch log pdf masked view (#322)\n\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class DiagNormal(Distribution):",
            "# when the data is a ragged tensor. also useful for KL annealing. this entire logic",
            "# will likely be done in a better/cleaner way in the future",
            "if log_pdf_mask is not None:",
            "-            # TODO fix this to broadcasting as below, e.g. by instead:",
            "-            # log_pxs *= log_pdf_mask  # Then continue with broadcasting logic below.",
            "-            return torch.sum(log_pdf_mask * log_pxs, -1)",
            "+            log_pxs = log_pxs * log_pdf_mask",
            "batch_log_pdf = torch.sum(log_pxs, -1)",
            "batch_log_pdf_shape = x.size()[:-1] + (1,)",
            "return batch_log_pdf.contiguous().view(batch_log_pdf_shape)"
        ]
    },
    {
        "number": 320,
        "comments": "remove constraint",
        "commit_message": "Refactoring, cleanup, improved test coverage.\n* Add eca_nfnet_l2 weights, 84.7 @ 384x384\n* All 'non-std' (ie transformer / mlp) models have classifier / default_cfg test added\n* Fix #694 reset_classifer / num_features / forward_features / num_classes=0 consistency for transformer / mlp models\n* Add direct loading of npz to vision transformer (pure transformer so far, hybrid to come)\n* Rename vit_deit* to deit_*\n* Remove some deprecated vit hybrid model defs\n* Clean up classifier flatten for conv classifiers and unusual cases (mobilenetv3/ghostnet)\n* Remove explicit model fns for levit conv, just pass in arg\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class MobileNetV3(nn.Module):",
            "",
            "def forward(self, x):",
            "x = self.forward_features(x)",
            "-        if not self.global_pool.is_identity():",
            "-            x = x.flatten(1)",
            "+        x = self.flatten(x)",
            "if self.drop_rate > 0.:",
            "x = F.dropout(x, p=self.drop_rate, training=self.training)",
            "return self.classifier(x)"
        ]
    },
    {
        "number": 321,
        "comments": "format",
        "commit_message": "linter fixes\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class ParallelWaveganGenerator(torch.nn.Module):",
            "",
            "def apply_weight_norm(self):",
            "def _apply_weight_norm(m):",
            "-            if isinstance(m, torch.nn.Conv1d) or isinstance(",
            "-                    m, torch.nn.Conv2d):",
            "+            if isinstance(m, (torch.nn.Conv1d, torch.nn.Conv2d)):",
            "torch.nn.utils.weight_norm(m)",
            "# print(f\"Weight norm is applied to {m}.\")"
        ]
    },
    {
        "number": 322,
        "comments": "value change",
        "commit_message": "Fix docstrings for TF BLIP (#22618)\n\n* Fix docstrings for TFBLIP\n\n* Fix missing line in TF port!\n\n* Use values from torch tests now other bugs fixed\n\n* Use values from torch tests now other bugs fixed\n\n* Fix doctest string\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class TFBlipModelIntegrationTest(unittest.TestCase):",
            "out_itm = model(**inputs)",
            "out = model(**inputs, use_itm_head=False, training=False)",
            "",
            "-        expected_scores = tf.convert_to_tensor([[0.9798, 0.0202]])",
            "+        expected_scores = tf.convert_to_tensor([[0.0029, 0.9971]])",
            "self.assertTrue(np.allclose(tf.nn.softmax(out_itm[0]).numpy(), expected_scores, rtol=1e-3, atol=1e-3))",
            "-        self.assertTrue(np.allclose(out[0], tf.convert_to_tensor([[0.5053]]), rtol=1e-3, atol=1e-3))",
            "+        self.assertTrue(np.allclose(out[0], tf.convert_to_tensor([[0.5162]]), rtol=1e-3, atol=1e-3))"
        ]
    },
    {
        "number": 323,
        "comments": "update API call for version fix",
        "commit_message": "fix small issues in the code refactoring\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class BaseModel():",
            "save_filename = '%s_net_%s.pth' % (which_epoch, name)",
            "save_path = os.path.join(self.save_dir, save_filename)",
            "net = getattr(self, 'net' + name)",
            "-                net.load_state_dict(torch.load(save_path))",
            "+                net.module.load_state_dict(torch.load(save_path))",
            "",
            "# print network information",
            "def print_networks(self, verbose):"
        ]
    },
    {
        "number": 324,
        "comments": "change param for type fix",
        "commit_message": "Fix dynamic info extraction for stable diffusion\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class SpeedsterRootOp(Operation):",
            ") -> List[BaseInferenceLearner]:",
            "if self.orig_latency_measure_op.get_result() is not None:",
            "model_outputs = self.orig_latency_measure_op.get_result()[0]",
            "-            if isinstance(model, Module):",
            "+            if isinstance(model, torch.nn.Module):",
            "optimization_op = self.torch_optimization_op",
            "elif isinstance(model, tf.Module) and model is not None:",
            "optimization_op = self.tensorflow_optimization_op"
        ]
    },
    {
        "number": 325,
        "comments": "rename",
        "commit_message": "YOLOv5 Apple Metal Performance Shader (MPS) support (#7878)\n\n* Apple Metal Performance Shader (MPS) device support\n\nFollowing https://pytorch.org/blog/introducing-accelerated-pytorch-training-on-mac/\n\nShould work with Apple M1 devices with PyTorch nightly installed with command `--device mps`. Usage examples:\n```bash\npython train.py --device mps\npython detect.py --device mps\npython val.py --device mps\n```\n\n* Update device strategy to fix MPS issue\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def run(",
            "):",
            "# PyTorch model",
            "im = torch.zeros((batch_size, 3, *imgsz))  # BCHW image",
            "-    model = attempt_load(weights, map_location=torch.device('cpu'), inplace=True, fuse=False)",
            "+    model = attempt_load(weights, device=torch.device('cpu'), inplace=True, fuse=False)",
            "_ = model(im)  # inference",
            "model.info()"
        ]
    },
    {
        "number": 327,
        "comments": "rename",
        "commit_message": "Fix AnchorHead in_channels (#1506)\n\n* test that all configs can be loaded\n\n* Use in_channels correctly in anchor_head and guided_anchor_head\n\n* Fix lint errors. Only tests a subset of configs\n\n* remove local config\n\n* fix yapf\n\n* Remove slower tests\n\n* Remove debug code\n\n* trigger travis\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "class GuidedAnchorHead(AnchorHead):",
            "",
            "def _init_layers(self):",
            "self.relu = nn.ReLU(inplace=True)",
            "-        self.conv_loc = nn.Conv2d(self.feat_channels, 1, 1)",
            "-        self.conv_shape = nn.Conv2d(self.feat_channels, self.num_anchors * 2,",
            "-                                    1)",
            "+        self.conv_loc = nn.Conv2d(self.in_channels, 1, 1)",
            "+        self.conv_shape = nn.Conv2d(self.in_channels, self.num_anchors * 2, 1)",
            "self.feature_adaption = FeatureAdaption(",
            "-            self.feat_channels,",
            "+            self.in_channels,",
            "self.feat_channels,",
            "kernel_size=3,",
            "deformable_groups=self.deformable_groups)"
        ]
    },
    {
        "number": 328,
        "comments": "update API call for version fix",
        "commit_message": "Inference support for `mps` device (#355)\n\n* Initial support for mps in Stable Diffusion pipeline.\n\n* Initial \"warmup\" implementation when using mps.\n\n* Make some deterministic tests pass with mps.\n\n* Disable training tests when using mps.\n\n* SD: generate latents in CPU then move to device.\n\nThis is especially important when using the mps device, because\ngenerators are not supported there. See for example\nhttps://github.com/pytorch/pytorch/issues/84288.\n\nIn addition, the other pipelines seem to use the same approach: generate\nthe random samples then move to the appropriate device.\n\nAfter this change, generating an image in MPS produces the same result\nas when using the CPU, if the same seed is used.\n\n* Remove prints.\n\n* Pass AutoencoderKL test_output_pretrained with mps.\n\nSampling from `posterior` must be done in CPU.\n\n* Style\n\n* Do not use torch.long for log op in mps device.\n\n* Perform incompatible padding ops in CPU.\n\nUNet tests now pass.\nSee https://github.com/pytorch/pytorch/issues/84535\n\n* Style: fix import order.\n\n* Remove unused symbols.\n\n* Remove MPSWarmupMixin, do not apply automatically.\n\nWe do apply warmup in the tests, but not during normal use.\nThis adopts some PR suggestions by @patrickvonplaten.\n\n* Add comment for mps fallback to CPU step.\n\n* Add README_mps.md for mps installation and use.\n\n* Apply `black` to modified files.\n\n* Restrict README_mps to SD, show measures in table.\n\n* Make PNDM indexing compatible with mps.\n\nAddresses #239.\n\n* Do not use float64 when using LDMScheduler.\n\nFixes #358.\n\n* Fix typo identified by @patil-suraj\n\nCo-authored-by: Suraj Patil <surajp815@gmail.com>\n\n* Adapt example to new output style.\n\n* Restore 1:1 results reproducibility with CompVis.\n\nHowever, mps latents need to be generated in CPU because generators\ndon't work in the mps device.\n\n* Move PyTorch nightly to requirements.\n\n* Adapt `test_scheduler_outputs_equivalence` ton MPS.\n\n* mps: skip training tests instead of ignoring silently.\n\n* Make VQModel tests pass on mps.\n\n* mps ddim tests: warmup, increase tolerance.\n\n* ScoreSdeVeScheduler indexing made mps compatible.\n\n* Make ldm pipeline tests pass using warmup.\n\n* Style\n\n* Simplify casting as suggested in PR.\n\n* Add Known Issues to readme.\n\n* `isort` import order.\n\n* Remove _mps_warmup helpers from ModelMixin.\n\nAnd just make changes to the tests.\n\n* Skip tests using unittest decorator for consistency.\n\n* Remove temporary var.\n\n* Remove spurious blank space.\n\n* Remove unused symbol.\n\n* Remove README_mps.\n\nCo-authored-by: Suraj Patil <surajp815@gmail.com>\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com> \n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class UNet2DConditionModel(ModelMixin, ConfigMixin):",
            "if not torch.is_tensor(timesteps):",
            "timesteps = torch.tensor([timesteps], dtype=torch.long, device=sample.device)",
            "elif torch.is_tensor(timesteps) and len(timesteps.shape) == 0:",
            "-            timesteps = timesteps[None].to(sample.device)",
            "+            timesteps = timesteps.to(dtype=torch.float32)",
            "+            timesteps = timesteps[None].to(device=sample.device)",
            "",
            "# broadcast to batch dimension in a way that's compatible with ONNX/Core ML",
            "timesteps = timesteps.expand(sample.shape[0])"
        ]
    },
    {
        "number": 330,
        "comments": "value change",
        "commit_message": "Fix donut image processor (#20625)\n\n* fix donut image processor\n\n* Update test values\n\n* Apply lower bound on resizing size\n\n* Add in missing size param\n\n* Resolve resize channel_dimension bug\n\n* Update src/transformers/image_transforms.py\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class DonutModelIntegrationTest(unittest.TestCase):",
            "self.assertEqual(len(outputs.scores), 11)",
            "self.assertTrue(",
            "torch.allclose(",
            "-                outputs.scores[0][0, :3], torch.tensor([5.3153, -3.5276, 13.4781], device=torch_device), atol=1e-4",
            "+                outputs.scores[0][0, :3], torch.tensor([5.6019, -3.5070, 13.7123], device=torch_device), atol=1e-4",
            ")",
            ")"
        ]
    },
    {
        "number": 332,
        "comments": "value change",
        "commit_message": "keep the '-summary' suffix\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def add_moving_summary(*args, **kwargs):",
            "ema_ops.append(ema_op)",
            "with tf.name_scope(None):",
            "# cannot add it into colocate group -- will force everything to cpus",
            "-            tf.summary.scalar(name, ema_op)    # write the EMA value as a summary",
            "+            tf.summary.scalar(name + '-summary', ema_op)    # write the EMA value as a summary",
            "if coll is not None:",
            "for op in ema_ops:",
            "# TODO a new collection to summary every step?"
        ]
    },
    {
        "number": 333,
        "comments": "update API call for version fix",
        "commit_message": "upgrade to pytorch 0.4.0 (#1126)\n\n* bump pytorch to 0.4 + fix sanitize\n\n* remove check for Variable in block_orthogonal\n\n* rename parameter split_size=\n\n* remove checks for Variable\n\n* fix more tests\n\n* fixes\n\n* get tests to pass\n\n* fix warnings\n\n* get rid of some of the Variables\n\n* more tests passing\n\n* more elimination of variables\n\n* finish removing all Variables\n\n* pylint and such\n\n* a few fixes\n\n* move torch.no_grad into model.forward_on_instances\n\n* more pytorch 0.4 changes\n\n* detach() -> data\n\n* pylint\n\n* fix bad tensor creation\n\n* fix types\n\n* remove print statement\n\n* more 0.4 goodness\n\n* factor out is_tensor\n\n* add no_grad to elmo command\n\n* cleanup\n\n* remove TODO\n\n* address PR feedback\n\n* replace all() with item()\n\n* more cleanup\n\n* further cleanup\n\n* remove Variable\n\n* really fix merge conflict\n\n* fix pylint\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class BilinearSimilarity(SimilarityFunction):",
            "self.reset_parameters()",
            "",
            "def reset_parameters(self):",
            "-        torch.nn.init.xavier_uniform(self._weight_matrix)",
            "+        torch.nn.init.xavier_uniform_(self._weight_matrix)",
            "self._bias.data.fill_(0)",
            "",
            "@overrides"
        ]
    },
    {
        "number": 334,
        "comments": "doc update",
        "commit_message": "enable `disallow_incomplete_defs` on mypy (#2094)\n\n* enable `disallow_incomplete_defs` on mypy\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix `blur_pool2d` doc\n\n* finish v1: works on torch 1.13.1\n\n- Remove JIT support for Boxes3D\n\n* rip off the np typing\n\n* replace `Size` with `Tuple[int, ...]` on augs\n\n* add `Dtype` to kornia.filters.kernels\n\n* minor fix after rebase\n\n* Remove old torch from typing CI\n\n---------\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class UniformRaySampler(RaySampler):",
            "self._calc_ray_params(cameras, points_2d_camera)",
            "",
            "",
            "-def sample_lengths(num_rays: int, num_ray_points: int, device: Device, dtype: torch.dtype, irregular=False) -> Tensor:",
            "+def sample_lengths(",
            "+    num_rays: int, num_ray_points: int, device: Device, dtype: torch.dtype, irregular: bool = False",
            "+) -> Tensor:",
            "if num_ray_points <= 1:",
            "raise ValueError('Number of ray points must be greater than 1')",
            "if not irregular:"
        ]
    },
    {
        "number": 335,
        "comments": "update API call for refactor fix",
        "commit_message": "[style] consistent nn. and nn.functional (#12124)\n\n* consistent nn. and nn.functional\n\n* fix glitch\n\n* fix glitch #2\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class AdaptiveEmbedding(nn.Module):",
            "",
            "inp_i = inp_flat.index_select(0, indices_i) - l_idx",
            "emb_i = self.emb_layers[i](inp_i)",
            "-                emb_i = F.linear(emb_i, self.emb_projs[i])",
            "+                emb_i = nn.functional.linear(emb_i, self.emb_projs[i])",
            "",
            "emb_flat.index_copy_(0, indices_i, emb_i)"
        ]
    },
    {
        "number": 337,
        "comments": "change param for math fix",
        "commit_message": "fix and use corr2d without tf cast\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class Residual(tf.keras.Model):  #@save",
            "if self.conv3 is not None:",
            "X = self.conv3(X)",
            "Y += X",
            "-        return tf.keras.activations.relu(Y + X)",
            "+        return tf.keras.activations.relu(Y)"
        ]
    },
    {
        "number": 338,
        "comments": "value change",
        "commit_message": "fix empty partitions in tests/integration_tests/test_preprocessing.py::test_dask_known_divisions (#2310)\n\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "def test_with_split(backend, csv_filename, tmpdir):",
            "def test_dask_known_divisions(feature_fn, csv_filename, tmpdir):",
            "import dask.dataframe as dd",
            "",
            "-    num_examples = NUM_EXAMPLES",
            "-",
            "input_features = [feature_fn(os.path.join(tmpdir, \"generated_output\"))]",
            "output_features = [category_feature(vocab_size=5, reduce_input=\"sum\")]",
            "-    data_csv = generate_data(",
            "-        input_features, output_features, os.path.join(tmpdir, csv_filename), num_examples=num_examples",
            "-    )",
            "-    data_df = dd.from_pandas(pd.read_csv(data_csv), npartitions=10)",
            "+",
            "+    # num_examples=100 and npartitions=2 to ensure the test is not flaky, by having non-empty post-split datasets.",
            "+    data_csv = generate_data(input_features, output_features, os.path.join(tmpdir, csv_filename), num_examples=100)",
            "+    data_df = dd.from_pandas(pd.read_csv(data_csv), npartitions=2)",
            "assert data_df.known_divisions",
            "",
            "config = {"
        ]
    },
    {
        "number": 339,
        "comments": "format",
        "commit_message": "fix doc build. make MergeAllSummaries a Triggerable\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "sys.path.insert(0, os.path.abspath('../'))",
            "os.environ['TENSORPACK_DOC_BUILDING'] = '1'",
            "",
            "",
            "-MOCK_MODULES = ['scipy',",
            "-                #'tensorflow', 'tensorflow.contrib',",
            "-                #'tensorflow.python.ops',",
            "-                #'tensorflow.contrib.framework',",
            "-                #'tensorflow.python',",
            "-                #'tensorflow.python.training',",
            "+MOCK_MODULES = ['scipy', 'tabulate',",
            "'sklearn.datasets', 'sklearn',",
            "'scipy.misc', 'h5py', 'nltk',",
            "'cv2', 'scipy.io', 'dill', 'zmq', 'subprocess32', 'lmdb',"
        ]
    },
    {
        "number": 341,
        "comments": "add API call for state fix",
        "commit_message": "Fix Mobilenet V3 model name for sotabench. Minor res2net cleanup.\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "for m in model_list:",
            "data_root=os.environ.get('IMAGENET_DIR', './imagenet')",
            ")",
            "",
            "+    torch.cuda.empty_cache()",
            "+"
        ]
    },
    {
        "number": 342,
        "comments": "add API call for type fix",
        "commit_message": "Refactoring the TF activations functions (#7150)\n\n* Refactoring the activations functions into a common file\n\n* Apply style\n\n* remove unused import\n\n* fix tests\n\n* Fix tests.\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class TFMLP(tf.keras.layers.Layer):",
            "nx = config.n_embd",
            "self.c_fc = TFConv1D(n_state, nx, initializer_range=config.initializer_range, name=\"c_fc\")",
            "self.c_proj = TFConv1D(nx, n_state, initializer_range=config.initializer_range, name=\"c_proj\")",
            "-        self.act = gelu",
            "+        self.act = get_tf_activation(\"gelu\")",
            "self.dropout = tf.keras.layers.Dropout(config.resid_pdrop)",
            "",
            "def call(self, x, training=False):"
        ]
    },
    {
        "number": 346,
        "comments": "add custom API",
        "commit_message": "Reland BT enablement on fairseq - fairseq change (#4513)\n\nSummary:\nPull Request resolved: https://github.com/facebookresearch/fairseq/pull/4513\nWith some fixes to torchscript using dual copies.\nReland this diff.\n\nReviewed By: erichan1\n\nDifferential Revision: D37371293\n\nfbshipit-source-id: 4fcfc4083955b6f5fc4ef8600f1b517b6ba69aae\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class TestExportModels(unittest.TestCase):",
            "_test_save_and_load(scripted)",
            "",
            "@unittest.skipIf(",
            "-        torch.__version__ < \"1.6.0\", \"Targeting OSS scriptability for the 1.6 release\"",
            "+        version_check(),",
            "+        \"Targeting OSS scriptability for the 1.13.0.dev20220613 release\",",
            ")",
            "def test_export_transformer_no_token_pos_emb(self):",
            "task, parser = get_dummy_task_and_parser()"
        ]
    },
    {
        "number": 347,
        "comments": "remove param for math fix",
        "commit_message": "fix mozilla/TTS#685\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class TacotronAbstract(ABC, nn.Module):",
            "def _backward_pass(self, mel_specs, encoder_outputs, mask):",
            "\"\"\" Run backwards decoder \"\"\"",
            "decoder_outputs_b, alignments_b, _ = self.decoder_backward(",
            "-            encoder_outputs, torch.flip(mel_specs, dims=(1,)), mask,",
            "-            self.speaker_embeddings_projected)",
            "+            encoder_outputs, torch.flip(mel_specs, dims=(1,)), mask)",
            "decoder_outputs_b = decoder_outputs_b.transpose(1, 2).contiguous()",
            "return decoder_outputs_b, alignments_b"
        ]
    },
    {
        "number": 348,
        "comments": "add param for resource fix",
        "commit_message": "fix test ddp initialize (#2100)\n\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def main():",
            "model = MMDataParallel(model, device_ids=[0])",
            "outputs = single_gpu_test(model, data_loader, args.show)",
            "else:",
            "-        model = MMDistributedDataParallel(model.cuda())",
            "+        model = MMDistributedDataParallel(",
            "+            model.cuda(),",
            "+            device_ids=[torch.cuda.current_device()],",
            "+            broadcast_buffers=False)",
            "outputs = multi_gpu_test(model, data_loader, args.tmpdir,",
            "args.gpu_collect)"
        ]
    },
    {
        "number": 349,
        "comments": "change API",
        "commit_message": "Fixes warnings and add compatibility stub in torch solve (#1235)\n\n* add a compatibiliy stub for torch solve\n\n* change missing _torch_solve_cast calls\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* bump pytorch version to 1.7.1\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def get_tps_transform(points_src: torch.Tensor, points_dst: torch.Tensor) -> Tup",
            "l_matrix: torch.Tensor = torch.cat((k_matrix, p_matrix), -1)",
            "l_matrix = torch.cat((l_matrix, p_matrix_t), 1)",
            "",
            "-    weights, _ = torch.solve(dest_with_zeros, l_matrix)",
            "+    weights, _ = _torch_solve_cast(dest_with_zeros, l_matrix)",
            "kernel_weights: torch.Tensor = weights[:, :-3]",
            "affine_weights: torch.Tensor = weights[:, -3:]"
        ]
    },
    {
        "number": 350,
        "comments": "rename",
        "commit_message": "Allow directly passing text embeddings to Stable Diffusion Pipeline for prompt weighting (#2071)\n\n* add text embeds to sd\n\n* add text embeds to sd\n\n* finish tests\n\n* finish\n\n* finish\n\n* make style\n\n* fix tests\n\n* make style\n\n* make style\n\n* up\n\n* better docs\n\n* fix\n\n* fix\n\n* new try\n\n* up\n\n* up\n\n* finish\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "class StableDiffusionImageVariationPipeline(DiffusionPipeline):",
            "image_embeddings = image_embeddings.view(bs_embed * num_images_per_prompt, seq_len, -1)",
            "",
            "if do_classifier_free_guidance:",
            "-            uncond_embeddings = torch.zeros_like(image_embeddings)",
            "+            negative_prompt_embeds = torch.zeros_like(image_embeddings)",
            "",
            "# For classifier free guidance, we need to do two forward passes.",
            "# Here we concatenate the unconditional and text embeddings into a single batch",
            "# to avoid doing two forward passes",
            "-            image_embeddings = torch.cat([uncond_embeddings, image_embeddings])",
            "+            image_embeddings = torch.cat([negative_prompt_embeds, image_embeddings])",
            "",
            "return image_embeddings"
        ]
    },
    {
        "number": 351,
        "comments": "rename",
        "commit_message": "[RLlib] Tf2x preparation; part 2 (upgrading `try_import_tf()`). (#9136)\n\n* WIP.\n\n* Fixes.\n\n* LINT.\n\n* WIP.\n\n* WIP.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* WIP.\n\n* Fixes.\n\n* Test\n\n* Fix.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* LINT.\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class AsyncSamplesOptimizerTest(unittest.TestCase):",
            "",
            "def _make_envs(self):",
            "def make_sess():",
            "-            return tf.Session(config=tf.ConfigProto(device_count={\"CPU\": 2}))",
            "+            return tf1.Session(config=tf1.ConfigProto(device_count={\"CPU\": 2}))",
            "",
            "local = RolloutWorker(",
            "env_creator=lambda _: gym.make(\"CartPole-v0\"),"
        ]
    },
    {
        "number": 352,
        "comments": "add anotation",
        "commit_message": "TF ALBERT + TF Utilities + Fix warnings\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class AlbertModel(AlbertPreTrainedModel):",
            "inner_group_idx = int(layer - group_idx * self.config.inner_group_num)",
            "self.encoder.albert_layer_groups[group_idx].albert_layers[inner_group_idx].attention.prune_heads(heads)",
            "",
            "+    @add_start_docstrings_to_callable(ALBERT_INPUTS_DOCSTRING)",
            "def forward(",
            "self,",
            "input_ids=None,"
        ]
    },
    {
        "number": 353,
        "comments": "format",
        "commit_message": "for discussion: incorporate black code formatter (#3308)\n\n* setup files\n\n* run black\n\n* undo\n\n* update CONTRIBUTING.md\n\n* fix quotes in test_other_modules\n\n* make flake8 happy\n\n* set black to 100 characters per line\n\n* move type: ignore to where mypy wants them\n\n* more flake8\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class TestSelfAttentiveSpanExtractor:",
            "extractor._global_attention._module.weight.data.fill_(0.0)",
            "extractor._global_attention._module.bias.data.fill_(0.0)",
            "",
            "-        indices = torch.LongTensor([[[1, 3],",
            "-                                     [2, 4]],",
            "-                                    [[0, 2],",
            "-                                     [3, 4]]]) # smaller span tests masking.",
            "+        indices = torch.LongTensor(",
            "+            [[[1, 3], [2, 4]], [[0, 2], [3, 4]]]",
            "+        )  # smaller span tests masking.",
            "span_representations = extractor(sequence_tensor, indices)",
            "assert list(span_representations.size()) == [2, 2, input_dim]"
        ]
    },
    {
        "number": 357,
        "comments": "remove param for shape fix",
        "commit_message": "fix input shape in get_rotation_matrix2d\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class Tester(unittest.TestCase):",
            "# generate input data",
            "batch_size = 1",
            "center = torch.zeros(batch_size, 2)",
            "-        angle = torch.ones(batch_size, 1)",
            "-        scale = torch.ones(batch_size, 1)",
            "+        angle = torch.ones(batch_size)",
            "+        scale = torch.ones(batch_size)",
            "",
            "center = utils.tensor_to_gradcheck_var(center)  # to var",
            "angle = utils.tensor_to_gradcheck_var(angle)  # to var"
        ]
    },
    {
        "number": 359,
        "comments": "rename",
        "commit_message": "Fixed issues with trace in Torch frontend\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def test_torch_trace(",
            "dtype_and_values,",
            "as_variable,",
            "num_positional_args,",
            "+    with_out,",
            "native_array,",
            "fw,",
            "):",
            "input_dtype, value = dtype_and_values",
            "-    # if \"float16\" in input_dtype:",
            "-    #    input_dtype = ivy.FloatDtype(\"float32\")  # Float16 is unsupported for trace.",
            "helpers.test_frontend_function(",
            "input_dtypes=input_dtype,",
            "as_variable_flags=as_variable,",
            "-        with_out=False,",
            "+        with_out=with_out,",
            "num_positional_args=num_positional_args,",
            "native_array_flags=native_array,",
            "fw=fw,"
        ]
    },
    {
        "number": 360,
        "comments": "change condition check for state fix",
        "commit_message": "[deepspeed] check whether model is NLP one instead of counting on input type (#21800)\n\n* trying to figure out whether model is NLP\n\n* drop my changes and apply easier fix\n\n* trying to handle all int input types\n\n* fix logic\n\n---------\n\nCo-authored-by: Stas Bekman <stas@stason.org>\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class Trainer:",
            "return type(data)(self._prepare_input(v) for v in data)",
            "elif isinstance(data, torch.Tensor):",
            "kwargs = {\"device\": self.args.device}",
            "-            if self.deepspeed and data.dtype != torch.int64:",
            "-                # NLP models inputs are int64 and those get adjusted to the right dtype of the",
            "+            if self.deepspeed and (torch.is_floating_point(data) or torch.is_complex(data)):",
            "+                # NLP models inputs are int/uint and those get adjusted to the right dtype of the",
            "# embedding. Other models such as wav2vec2's inputs are already float and thus",
            "# may need special handling to match the dtypes of the model",
            "kwargs.update({\"dtype\": self.args.hf_deepspeed_config.dtype()})"
        ]
    },
    {
        "number": 361,
        "comments": "customized method ",
        "commit_message": "fix: fix import of intents\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class HybridCodeNetworkModel(TFModel):",
            "self.obs_size = params['obs_size']",
            "",
            "def _build_graph(self):",
            "-        tf.reset_default_graph()",
            "-",
            "self._add_placeholders()",
            "",
            "# build body"
        ]
    },
    {
        "number": 362,
        "comments": "add API call for resource fix",
        "commit_message": "Fix torch device issues (#20584)\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class DeformableDetrImageProcessor(BaseImageProcessor):",
            "img_w = torch.Tensor([i[1] for i in target_sizes])",
            "else:",
            "img_h, img_w = target_sizes.unbind(1)",
            "-        scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1)",
            "+        scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1).to(boxes.device)",
            "boxes = boxes * scale_fct[:, None, :]",
            "",
            "results = []"
        ]
    },
    {
        "number": 363,
        "comments": "change API call for type fix",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class SpanBasedF1Test(AllenNlpTestCase):",
            "gold_indices = [self.vocab.get_token_index(x, \"tags\") for x in bio_tags]",
            "gold_tensor = torch.tensor([gold_indices], device=device)",
            "prediction_tensor = torch.rand([1, 6, self.vocab.get_vocab_size(\"tags\")], device=device)",
            "-        mask = torch.tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]], device=device)",
            "+        mask = torch.BoolTensor(",
            "+            [[True, True, True, True, True, True, True, True, True]], device=device",
            "+        )",
            "",
            "# Make prediction so that it is exactly correct.",
            "for i, tag_index in enumerate(gold_indices):"
        ]
    },
    {
        "number": 364,
        "comments": "add condition check for null fix",
        "commit_message": "Remove prefix for first tower in replicated mode. Support inference now.\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class TowerContext(object):",
            "self._ctxs = []",
            "if len(self._name):",
            "if self.has_own_variables:",
            "-                # open new variable scopes",
            "-                self._ctxs.append(tf.variable_scope(self._name))",
            "+                if self.vs_name:",
            "+                    self._ctxs.append(tf.variable_scope(self.vs_name))",
            "else:",
            "# use existing variable scope",
            "reuse = self.index > 0 or (not self.is_training)"
        ]
    },
    {
        "number": 366,
        "comments": "rename method",
        "commit_message": "Bug fix in update norm calculation\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class DistributedFusedLAMB(torch.optim.Optimizer):",
            "l2_norm = torch.zeros(size=[self._model_params_num], dtype=torch.float32, device='cuda')",
            "local_contrib_l2_norm = multi_tensor_applier(self.multi_tensor_l2norm, self._overflow_buf, [self._contrib_update_frag_for_norm], True)[1] ** 2",
            "l2_norm.masked_scatter_(self._model_param_is_contrib, local_contrib_l2_norm)",
            "-        torch.distributed.allreduce(l2_norm, group=self._ag_pg[0])",
            "+        torch.distributed.all_reduce(l2_norm, group=self._ag_pg[0])",
            "return l2_norm.masked_select(self._model_param_is_contrib)",
            "",
            "def _pipeline_step(self):"
        ]
    },
    {
        "number": 367,
        "comments": "add param for shape fix",
        "commit_message": "Fixes shape issue in `_BinaryPostprocessing` (#2094)\n\n* Fixes shape issue in\n\n* adds regression test\n\nCo-authored-by: Geoffrey Angus <geoffrey@predibase.com>\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class _BinaryPostprocessing(torch.nn.Module):",
            "predictions = [self.bool2str.get(pred, self.bool2str[0]) for pred in predictions]",
            "",
            "probs = preds[self.probabilities_key]",
            "-        probs = torch.dstack(1 - probs, probs)",
            "+        probs = torch.stack([1 - probs, probs], dim=-1)",
            "",
            "return {",
            "self.predictions_key: predictions,"
        ]
    },
    {
        "number": 368,
        "comments": "change API call for resource fix",
        "commit_message": "revert \"chore(core): update launch backend to subprocess (#158)\" (#176)\n\nThis reverts commit 9ac889 to fix training bug\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "if __name__ == \"__main__\":",
            "exp = get_exp(args.exp_file, args.name)",
            "exp.merge(args.opts)",
            "",
            "-    num_gpu = get_num_devices() if args.devices is None else args.devices",
            "-    assert num_gpu <= get_num_devices()",
            "+    num_gpu = torch.cuda.device_count() if args.devices is None else args.devices",
            "+    assert num_gpu <= torch.cuda.device_count()",
            "",
            "dist_url = \"auto\" if args.dist_url is None else args.dist_url",
            "launch("
        ]
    },
    {
        "number": 370,
        "comments": "change API call for type fix",
        "commit_message": "Fix tests failing on CUDA (#1834)\n\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def test_log_prob_eta1(d):",
            "assert (lps_less_ladj - lps_less_ladj.min()).abs().sum() < 1e-4",
            "",
            "",
            "-@pytest.mark.parametrize(\"eta\", [.1, .5, 1, 2, 5])",
            "+@pytest.mark.parametrize(\"eta\", [.1, .5, 1., 2., 5.])",
            "def test_log_prob_d2(eta):",
            "-    dist = LKJCorrCholesky(2, torch.DoubleTensor([eta]))",
            "+    dist = LKJCorrCholesky(2, torch.tensor([eta]))",
            "test_dist = TransformedDistribution(Beta(eta, eta), AffineTransform(loc=-1., scale=2.0))",
            "",
            "samples = dist.sample(torch.Size([100]))"
        ]
    },
    {
        "number": 372,
        "comments": "rename the variable",
        "commit_message": "fix a bug in DDPG.py.\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class Critic(object):",
            "self.train_op = tf.train.AdamOptimizer(self.lr).minimize(self.loss)",
            "",
            "with tf.variable_scope('a_grad'):",
            "-            self.a_grads = tf.gradients(self.q, a)[0]   # tensor of gradients of each sample (None, a_dim)",
            "+            self.a_grads = tf.gradients(self.q, self.a)[0]   # tensor of gradients of each sample (None, a_dim)",
            "",
            "if self.replacement['name'] == 'hard':",
            "self.t_replace_counter = 0"
        ]
    },
    {
        "number": 375,
        "comments": "add API call for state fix",
        "commit_message": "[PyTorch] Set net.eval and net.train explicitly (#1110)\n\n* [PyTorch] Set net.eval and net.train explicitly\n\n* [PyTorch] Set net.eval if isinstance nn.Module\n\n* [PyTorch] Set net.train if isinstance nn.Module\n\n* [Fix] Custom Dropout, Closes #1093\n\n* 2 spaces before inline comments\n\n* 2 spaces before inline comments\n\n* 2 spaces before inline comments\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def corr2d(X, K):  #@save",
            "",
            "# Defined in file: ./chapter_convolutional-neural-networks/lenet.md",
            "def evaluate_accuracy_gpu(net, data_iter, device=None): #@save",
            "+    net.eval()  # Set the model to evaluation mode",
            "if not device:",
            "device = next(iter(net.parameters())).device",
            "metric = d2l.Accumulator(2)  # num_corrected_examples, num_examples"
        ]
    },
    {
        "number": 376,
        "comments": "add param for argument fix",
        "commit_message": "[feat] updated patch augmentation (#1095)\n\n* updated patch augmentation\n\n* Fixed lint\n\n* Renamed forward_batchwise and forward_patchwise\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class VideoSequential(ImageSequential):",
            "# Size of T",
            "frame_num = input.size(self._temporal_channel)",
            "# Got param generation shape to (B, C, H, W). Ignoring T.",
            "-        batch_shape = self.__infer_channel_exclusive_batch_shape__(input)",
            "+        batch_shape = self.__infer_channel_exclusive_batch_shape__(input, self._temporal_channel)",
            "input = self._input_shape_convert_in(input)",
            "input = input.reshape(-1, *batch_shape[1:])",
            "if not self.same_on_frame:"
        ]
    },
    {
        "number": 377,
        "comments": "add param for resource fix",
        "commit_message": "Fix2 `select_device()` for Multi-GPU (#6461)\n\n* Fix2 select_device() for Multi-GPU\n\n* Cleanup\n\n* Cleanup\n\n* Simplify error message\n\n* Improve assert\n\n* Update torch_utils.py\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def create_dataloader(path, imgsz, batch_size, stride, single_cls=False, hyp=Non",
            "prefix=prefix)",
            "",
            "batch_size = min(batch_size, len(dataset))",
            "-    nw = min([os.cpu_count() // DEVICE_COUNT, batch_size if batch_size > 1 else 0, workers])  # number of workers",
            "+    nd = torch.cuda.device_count()  # number of CUDA devices",
            "+    nw = min([os.cpu_count() // max(nd, 1), batch_size if batch_size > 1 else 0, workers])  # number of workers",
            "sampler = None if rank == -1 else distributed.DistributedSampler(dataset, shuffle=shuffle)",
            "loader = DataLoader if image_weights else InfiniteDataLoader  # only DataLoader allows for attribute updates",
            "return loader(dataset,"
        ]
    },
    {
        "number": 378,
        "comments": "rename",
        "commit_message": "Docs 4/n (#15628)\n\n* remove source-lit\n\n* docs\n\n* docs\n\n* docs\n\n* docs\n\n* ic\n\n* deploy\n\n* deploy\n\n* deploy\n\n* deploy\n\n* deploy\n\n* deploy\n\n* Apply suggestions from code review\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* make build run\n\nCo-authored-by: Jirka Borovec <6035284+Borda@users.noreply.github.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Rick Izzo <rick@grid.ai>\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class LitStreamlit(L.app.components.ServeStreamlit):",
            "audio.seek(0)",
            "st.audio(audio)",
            "",
            "-app = L.LightningApp(LitStreamlit())",
            "+app = L.LightningApp(StreamlitApp())"
        ]
    },
    {
        "number": 379,
        "comments": "change API",
        "commit_message": "fixed init\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class MultiHeadedAttention(BaseMultiHeadedAttention):",
            "",
            "def __init__(self, q_dim, k_dim, v_dim, n_head, n_feat, dropout_rate=0.0):",
            "\"\"\"Initialize multi head attention module.\"\"\"",
            "-        super(MultiHeadedAttention, self).__init__()",
            "+        torch.nn.Module.__init__(self)",
            "assert n_feat % n_head == 0",
            "# We assume d_v always equals d_k",
            "self.d_k = n_feat // n_head"
        ]
    },
    {
        "number": 381,
        "comments": "add module",
        "commit_message": "POC crypten.nn.module in plans (#3531)\n\n* parent db61f12afe51b3a216c53db80813b8d8ac378055\nauthor George Muraru <murarugeorgec@gmail.com> 1589458543 +0300\ncommitter George Muraru <murarugeorgec@gmail.com> 1592429175 +0300\n\nInitial POC for models in plans\n\n* Format linter\n\n* Flake\n\n* Add tests for OnnxModel\n\n* Add decrypt as method to overload\n\n* Add tests compared to jail\n\n* Review suggestions\n\n* Remove protobuf for Onnx and linter check\n\n* Remove TODO indent\n\n* Put wrap back\n\n* Remove OnnxModel from tests\n\n* Remove duplicated test\n\n* Put comment back\n\n* Flake8 fix\n\n* Fix test serde\n\n* Add tests to increase cov\n\n* Changes from crypten - prevent enc of shapes\n\n* Add no coverage\n\n* Remove Onnx Model\n\n* Fix syft-proto\n\n* Fix review\n\n* Fix review some more\n",
        "label": "no",
        "answer": "no",
        "change": [
            "if dependency_check.crypten_available:",
            "",
            "framework_packages[\"crypten\"] = crypten",
            "framework_tensors.append(crypten.mpc.MPCTensor)",
            "+    framework_tensors.append(crypten.nn.Module)",
            "+",
            "",
            "framework_tensors = tuple(framework_tensors)",
            "FrameworkTensorType = Union[framework_tensors]"
        ]
    },
    {
        "number": 384,
        "comments": "update API call for type fix",
        "commit_message": "fixing some imports etc.\n\nsome leftover from commit 44af78a09c306d764aec5985de4cf0e054316f28\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def array(object_in, dtype: Optional[str] = None, dev: Optional[str] = None):",
            "dev = default_device(dev)",
            "dtype = dtype_from_str(default_dtype(dtype, object_in))",
            "if isinstance(object_in, np.ndarray):",
            "-        return _torch.Tensor(object_in).to(dev_from_str(dev))",
            "+        return torch.Tensor(object_in).to(dev_from_str(dev))",
            "if dtype is not None:",
            "-        return _torch.tensor(object_in, dtype=dtype, device=dev_from_str(dev))",
            "-    elif isinstance(object_in, _torch.Tensor):",
            "+        return torch.tensor(object_in, dtype=dtype, device=dev_from_str(dev))",
            "+    elif isinstance(object_in, torch.Tensor):",
            "return object_in.to(dev_from_str(dev))",
            "else:",
            "-        return _torch.tensor(object_in, device=dev_from_str(dev))",
            "+        return torch.tensor(object_in, device=dev_from_str(dev))",
            "",
            "asarray = array"
        ]
    },
    {
        "number": 385,
        "comments": "test fix",
        "commit_message": "[Fix] 3d augmentations return wrong transformation matrix (#665)\n\n* Transformation matrix and tests fixed\n\n* doctest fixed\n\n* Autoformat\n\nCo-authored-by: Feher Gergo (CC-AD/EPE-Bp) <fhe2bp@bosch.com>\nCo-authored-by: Feher Gergo (CC-AD/EPE-Bp) <bosch.feher@extaccount.com>\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def compute_dflip_transformation3d(input: torch.Tensor, params: Dict[str, torch.",
            "d: int = input.shape[-3]",
            "flip_mat: torch.Tensor = torch.tensor([[1, 0, 0, 0],",
            "[0, 1, 0, 0],",
            "-                                           [0, 0, -1, d],",
            "+                                           [0, 0, -1, d - 1],",
            "[0, 0, 0, 1]])",
            "",
            "trans_mat[to_flip] = flip_mat.type_as(input)"
        ]
    },
    {
        "number": 387,
        "comments": "add API call for resource fix",
        "commit_message": "Shubhamagarwal92 master (#1349)\n\n* SA: for #958: set torch cuda device when finding root\n\n* SA: for #958: removing root gpu hack in trainer/evaluation_loop\n\n* SA: setting torch cuda device\n\n* comment line too long\n\n* check if root gpu exists or available\n\n* Incorporating suggestions on #1094\n\n* since root gpu returns none instead of -1 for cpu\n\n* undo changes\n\n* fixed dp memory thing\n\nCo-authored-by: Shubham Agarwal <shubhamagarwal92@gmail.com>\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class Trainer(",
            "self.gpus = gpus",
            "self.data_parallel_device_ids = parse_gpu_ids(self.gpus)",
            "self.root_gpu = determine_root_gpu_device(self.data_parallel_device_ids)",
            "+        self.root_device = torch.device(\"cpu\")",
            "",
            "# tpu state flags",
            "self.use_tpu = False"
        ]
    },
    {
        "number": 388,
        "comments": "doc update",
        "commit_message": "Merging vision into main (#4800)\n\n* An initial VilBERT model for NLVR2 (#4423)\n\n* Some initial work; lots left to do\n\n* Initial test mostly passing, though things are still a bit of a mess\n\n* tests are passing with small fixtures\n\n* remove prints\n\n* Test more stuff\n\n* PathLike\n\n* Make vilbert pass tests\n\n* PR comments\n\n* call float before log\n\n* add CI\n\nCo-authored-by: Dirk Groeneveld <dirkg@allenai.org>\n\n* Initializing a VilBERT model from a pre-trained transformer (#4495)\n\n* saving state\n\n* Code is running, though it is returning zero gradients (but not None)\n\n* initial test passing, still working on albert\n\n* albert works, but bert-base-uncased still gives zero gradients\n\n* Loading of weights should now work\n\n* black, flake, mypy\n\n* remove drop and mask functionality from reader\n\n* make comment better\n\n* fix tests\n\n* flake\n\nCo-authored-by: Dirk Groeneveld <dirkg@allenai.org>\n\n* new data loading (#4497)\n\n* first implementation\n\n* update docstrings\n\n* fixes\n\n* fix sharding logic\n\n* clean up DatasetReader\n\n* fix samplers\n\n* fixes\n\n* fixes\n\n* patch models for now\n\n* more fixes\n\n* fix linting error\n\n* fix model test case\n\n* some fixes\n\n* fix linting err\n\n* updates\n\n* rename dataloader -> data_loader\n\n* fixes\n\n* more JoinableQueue\n\n* set daemon=True\n\n* fixes\n\n* fix\n\n* fixes\n\n* fix\n\n* update shuffle logic\n\n* load instances right away when not lazy\n\n* add tqdm when num_workers <= 0\n\n* apply_token_indexers\n\n* fix bug causing high mem usage\n\n* address some of @dirkgr's comments\n\n* fix lazy\n\n* use sensible default for max_batches_in_mem\n\n* ensure workers terminated on err\n\n* fix\n\n* start adding some tests\n\n* more tests\n\n* add some more tests\n\n* address most of Matt's comments\n\n* update PyTorchDataLoader test\n\n* get rid of lazy option\n\n* fix linting\n\n* update docs, change max_batches_per_epoch to max_instances_per_epcoh\n\n* update CHANGELOG\n\n* fix drop_last validation\n\n* fix py2md test fixture\n\n* handle drop_last\n\n* update docs\n\n* implement sharding for most readers\n\n* fix worker init fn\n\n* limit tqdm output\n\n* fixes\n\n* ensure vision CI runs on each commit (#4582)\n\n* ensure vision CI runs on each commit\n\n* fix\n\n* try fix CHANGELOG check\n\n* ensure models check runs on right branch\n\n* Formatting updates for new version of black (#4607)\n\n* reformat for new version of black (#4605)\n\n* reformat for new version of black\n\n* pin black\n\n* reformat for black\n\n* fix\n\n* rename 'node_rank' to 'global_rank' in dataset reader 'DistributedInfo' (#4608)\n\n* rename 'node_rank' to 'global_rank'\n\n* Clarify doc comments\n\n* fix line length\n\n* remove duplicate padding calculations in collate fn (#4617)\n\n* fix len calculation for new data loader (#4618)\n\n* fix len calculation for new data loader\n\n* add test\n\nCo-authored-by: Dirk Groeneveld <dirkg@allenai.org>\n\n* make existing readers work with multi-process loading (#4597)\n\n* make existing readers work with multi-process loading\n\n* add 'overrides' decorator\n\n* call apply_token_indexers in predictor\n\n* clean up\n\n* fix tests\n\n* Add MultiTaskModel (#4601)\n\n* Initial design of the multi-task model\n\n* PR comments, more implementation\n\n* changelog and docs fix\n\n* More tests, and fixes for those tests\n\n* mypy and make test less flaky\n\n* Update allennlp/models/multitask.py\n\n* Update allennlp/models/multitask.py\n\nCo-authored-by: Dirk Groeneveld <groeneveld@gmail.com>\n\n* Update allennlp/models/multitask.py\n\nCo-authored-by: James Barry <james.barry26@mail.dcu.ie>\n\n* respect active heads in get_metrics\n\n* Clean up changelog\n\n* black (apparently github UI doesn't add newlines?)\n\nCo-authored-by: Dirk Groeneveld <dirkg@allenai.org>\nCo-authored-by: Dirk Groeneveld <groeneveld@gmail.com>\nCo-authored-by: James Barry <james.barry26@mail.dcu.ie>\n\n* Detectron NLVR2 (#4481)\n\n* Passes a batch of detectron images to the model in the correct format\n\n* Loads a model and runs inference on it\n\n* Some initial work; lots left to do\n\n* Initial test mostly passing, though things are still a bit of a mess\n\n* tests are passing with small fixtures\n\n* remove prints\n\n* More configurable reader\n\n* add image_root and feature extraction to detectron model\n\n* Use general detectron cfg functions\n\n* Adds TensorField\n\n* Fix detectron dependency\n\n* Adds a detectron processor that we can use in dataset readers\n\n* Test more stuff\n\n* PathLike\n\n* Make vilbert pass tests\n\n* PR comments\n\n* call float before log\n\n* add CI\n\n* PathLike\n\n* Adds another NLVR2 reader\n\n* add region feature and grid feature configuration json and attrtibute to cfg file\n\n* change detectron_utils based on https://github.com/vedanuj/grid-feats-vqa/blob/master/extract_feature.py\n\n* add bottom up and top down roi head into detectron2 based on allennlp/models/detectron.py\n\n* Fix padding in TensorField\n\n* Fix field construction\n\n* Adds ability to read an arbitrary file\n\n* More type annotations\n\n* Remove old reader, add test for new one\n\n* Use the right kind of field\n\n* Run Jiasen's configs as tests\n\n* We don't need this field\n\n* Removes detectron reader\n\n* Remove detectron reader and field\n\n* Unify ArrayField and TensorField\n\n* Making sure that no merge will go cleanly from now on\n\n* Clean up the new output from the detectron processor a bit\n\n* Fix Detectron2 version as v0.2\n\n* saving state\n\n* Code is running, though it is returning zero gradients (but not None)\n\n* initial test passing, still working on albert\n\n* albert works, but bert-base-uncased still gives zero gradients\n\n* Note\n\n* Formatting\n\n* Adds Registrable base classes for image operations\n\n* Adds a real example of a image2image module\n\n* Run the new code (without implementation) in the nlvr2 reader\n\n* Solve some issue involving circular imports\n\n* add new modules for vilbert\n\n* add parameters for detectron image loader.\n\n* push current code on implementing proposal generator.\n\n* push current progress on proposal generator\n\n* Update FasterRCNNProposalGenerator & Merge Detectron2 config\n\n* Loading of weights should now work\n\n* black, flake, mypy\n\n* Run detectron pipeline pieces one at a time\n\nThis is unfinished and will not run this way.\n\n* Fix the data format for the backbone\n\n* Handle image sizes separately\n\n* remove drop and mask functionality from reader\n\n* make comment better\n\n* remove proposal_embedder, and finish proposal generator\n\n* working on grid embedder\n\n* added simple test for resnet backbone, which passes\n\n* Got proposal generator test passing\n\n* Change default number of detections per image: 100 => 36\n\n* Fix detectron config hierarchy: test_detectron_per_image\n\n* Make number of detections configurable & Add test\n\n* rename ProposalGenerator to RegionDetector\n\n* try to fix makefile\n\n* another attempt at makefile\n\n* quotes in the pip command...\n\n* added a simple test for the dataset reader, made it pass\n\n* add feature caching to the dataset reader\n\n* another try with the makefile\n\n* a better temporary fix for installing detectron\n\n* writing files before committing is good...\n\n* fix tests\n\n* fix (at least part of) the vilbert tests\n\n* ok, this makefile change should actually work\n\n* add torchvision, try to remove eager import of detectron code\n\n* flake\n\n* cleanup\n\n* more cleanup\n\n* mypy, flake\n\n* add back code I shouldn't have removed\n\n* black\n\n* test and flake fixes\n\n* fix region_detector for multiple images and add feature and coords padding\n\n* fix imports\n\n* restore null grid embedder\n\n* add back (todo) null region detector\n\n* Bring back import changes, to fix circular imports caused by NLVR2\nreader\n\n* region detector test passing\n\n* model test finally passing\n\n* update torchvision version\n\n* add vqav2 dataset\n\n* add gpu support for detectron feature extraction\n\n* add lmdbCache to cache feature into lmdb database\n\n* fix typo\n\n* update vqa jsonnet\n\n* fix url adding by cat\n\n* Fixes type annotation\n\n* Fixes borked error message\n\n* New feature cache\n\n* Formatting\n\n* Fix the tensor cache\n\n* Be explicit about our dependencies\n\n* Use the new tensor cache\n\n* Adds a test using the tensor cache\n\n* Run NLVR dataprep on GPU\n\n* Tqdm when finding images\n\n* Fixes padding in array field\n\n* Adjust max_length when truncating in PretrainedTransformerTokenizer\n\n* Fewer print statements\n\n* remove VQA from this branch and copy default vilbert parameters.\n\n* Sanjay's vision features cache script (#4633)\n\n* Use LMDB cache in NLVR2 dataset reader; fix a few typos\n\n* Standalone script for caching image features\n\n* Removing reference to LMDB cache in NLVR2 dataset reader\n\n* Adding back asterisk in nlvr2 dataset reader\n\n* Fixing one variable name mistake\n\n* Decreasing batch size and making a few cuda-related changes\n\n* Loading images in batches to avoid GPU OOM error\n\n* Pedantic changes for consistency\n\n* Run the pre-processing with the models and not the data loading\n\n* Filter out paths of images already cached\n\n* Add image extensions other than png\n\n* Fixes import error\n\n* Makes the vision features script work alongside other scripts or training runs\n\nCo-authored-by: sanjays <sanjays@ip-10-0-0-157.us-west-2.compute.internal>\nCo-authored-by: sanjays <sanjays@ip-10-1-10-157.us-west-2.compute.internal>\nCo-authored-by: Sanjay Subramanian <sanjays@allennlp-server1.corp.ai2>\nCo-authored-by: Sanjay Subramanian <sanjays_ssubramanian@hotmail.com>\n\n* Adds missing imports\n\n* Makes TensorCache into a real MutableMapping\n\n* Formatting\n\n* Changelog\n\n* Fix typecheck\n\n* Makes the NLVR2 reader work with Pete's new code\n\n* Fix type annotation\n\n* Formatting\n\n* Backwards compatibility\n\n* Fix tests\n\n* Fix broken config\n\n* Update grid embedder test\n\n* Fix vilbert_from_huggingface configuration\n\n* Don't run the vilbert_from_huggingface test anymore\n\n* Remove unused test fixtures\n\n* Fix the region detector test\n\n* Fix vilbert-from-huggingface and bring it back\n\n* Fuck the linter\n\n* Run the region detector test on GPU\n\n* Run more stuff on GPU\n\nThe CPU test runner doesn't have enough memory.\n\n* Depend on newer version of Detectron\n\n* Reinstall Detectron before running tests\n\n* Just force CUDA to be on, instead of reinstalling Detecton2\n\n* Detectron needs CUDA_HOME to be set during install\n\nAt least this thing fails quickly.\n\n* Try a different way of wrangling the detectron installer\n\n* Bring back amp\n\n* Trying to make tests faster, and passing\n\n* use two regions, to make tests pass\n\n* black\n\n* Documentation for TensorCache\n\n* Documentation for the NLVR2 dataset reader\n\n* Rename ArrayField to TensorField\n\nCo-authored-by: Matt Gardner <mattg@allenai.org>\nCo-authored-by: jiasenlu <jiasenlu@gatech.edu>\nCo-authored-by: Jaemin Cho <heythisischo@gmail.com>\nCo-authored-by: jiasenlu <echosenm@gmail.com>\nCo-authored-by: sanjays <sanjays@ip-10-0-0-157.us-west-2.compute.internal>\nCo-authored-by: sanjays <sanjays@ip-10-1-10-157.us-west-2.compute.internal>\nCo-authored-by: Sanjay Subramanian <sanjays@allennlp-server1.corp.ai2>\nCo-authored-by: Sanjay Subramanian <sanjays_ssubramanian@hotmail.com>\n\n* This should have been part of the previously merged PR\n\n* Transformer toolkit (#4577)\n\n* transformer toolkit: BertEmbeddings\n\n* transformer toolkit: BertSelfAttention\n\n* transformer toolkit: BertSelfOutput\n\n* transformer toolkit: BertAttention\n\n* transformer toolkit: BertIntermediate\n\n* transformer toolkit: BertOutput\n\n* transformer toolkit: BertLayer\n\n* transformer toolkit: BertBiAttention\n\n* transformer toolkit: BertEmbeddings\n\n* transformer toolkit: BertSelfAttention\n\n* transformer toolkit: BertSelfOutput\n\n* transformer toolkit: BertAttention\n\n* transformer toolkit: BertIntermediate\n\n* transformer toolkit: BertOutput\n\n* transformer toolkit: BertLayer\n\n* transformer toolkit: BertBiAttention\n\n* Attention scoring functions\n\n* merging output and self output\n\n* utility to replicate layers, further cleanup\n\n* adding sinusoidal positional encoding\n\n* adding activation layer\n\n* adding base class for generic loading of pretrained weights\n\n* further generalizing, adding tests\n\n* updates\n\n* adding bimodal encoder, kwargs in from_pretrained_module\n\n* vilbert using transformer toolkit\n\n* fixing test function\n\n* changing to torch.allclose\n\n* fixing attention score api\n\n* bug fix in bimodal output\n\n* changing to older attention modules\n\n* _construct_default_mapping returns mapping\n\n* adding kwargs to _get_input_arguments, adding examples\n\n* using cached_transformers\n\n* making transformer_encoder more general\n\n* added get_relevant_module, loading by name\n\n* fixing constructor name\n\n* undoing failure after merge\n\n* misc minor changes\n\nCo-authored-by: Dirk Groeneveld <dirkg@allenai.org>\n\n* Transformer toolkit: BiModalEncoder now has separate `num_attention_heads` for both modalities (#4728)\n\n* separate num_attention_heads for both modalities, default arguments\n\n* adding tests for toolkit examples\n\n* debug statements for failing test\n\n* removing debug statements, reordering\n\n* Let's be more tolerant\n\n* removing commented code\n\nCo-authored-by: Dirk Groeneveld <dirkg@allenai.org>\n\n* separating TransformerPooler as a new module (#4730)\n\n* separating TransformerPooler as a new module\n\n* adding size check\n\n* fix failing tests\n\n* Generalizing self attention (#4756)\n\n* generalizing SelfAttention\n\n* typecheck changes\n\n* adding shape information to docstring\n\nCo-authored-by: Dirk Groeneveld <dirkg@allenai.org>\n\n* Multitask data loading and scheduling (#4625)\n\n* Some initial work, still a bunch left to do\n\n* Adds a utility function that can shuffle iterables\n\n* remove shuffle\n\n* Getting close; saving state before fixing lint and adding tests\n\n* mypy and flake\n\n* put in some initial schedulers and samplers; just need to write tests\n\n* added some tests\n\n* changelog\n\n* add more-itertools to setup.py\n\n* finish docstring\n\n* some PR comments addressed\n\n* mypy\n\n* use homogeneous scheduler by default, not the non-homogeneous one\n\n* add option to not shuffle\n\n* normalize dataset proportions\n\n* Update allennlp/data/data_loaders/multitask_data_loader.py\n\nCo-authored-by: Dirk Groeneveld <dirkg@allenai.org>\n\n* improve independence of vision components (#4793)\n\n* improve independence of vision components\n\n* fix install\n\n* fix failing test\n\n* haha, actually fix\n\n* include torchvision exception too\n\n* fix torchvision install\n\n* remove vision push trigger\n\n* VQAv2 (#4639)\n\n* albert works, but bert-base-uncased still gives zero gradients\n\n* Note\n\n* Formatting\n\n* Adds Registrable base classes for image operations\n\n* Adds a real example of a image2image module\n\n* Run the new code (without implementation) in the nlvr2 reader\n\n* Solve some issue involving circular imports\n\n* add new modules for vilbert\n\n* add parameters for detectron image loader.\n\n* push current code on implementing proposal generator.\n\n* push current progress on proposal generator\n\n* Update FasterRCNNProposalGenerator & Merge Detectron2 config\n\n* Loading of weights should now work\n\n* black, flake, mypy\n\n* Run detectron pipeline pieces one at a time\n\nThis is unfinished and will not run this way.\n\n* Fix the data format for the backbone\n\n* Handle image sizes separately\n\n* remove drop and mask functionality from reader\n\n* make comment better\n\n* remove proposal_embedder, and finish proposal generator\n\n* working on grid embedder\n\n* added simple test for resnet backbone, which passes\n\n* Got proposal generator test passing\n\n* Change default number of detections per image: 100 => 36\n\n* Fix detectron config hierarchy: test_detectron_per_image\n\n* Make number of detections configurable & Add test\n\n* rename ProposalGenerator to RegionDetector\n\n* try to fix makefile\n\n* another attempt at makefile\n\n* quotes in the pip command...\n\n* added a simple test for the dataset reader, made it pass\n\n* add feature caching to the dataset reader\n\n* another try with the makefile\n\n* a better temporary fix for installing detectron\n\n* writing files before committing is good...\n\n* fix tests\n\n* fix (at least part of) the vilbert tests\n\n* ok, this makefile change should actually work\n\n* add torchvision, try to remove eager import of detectron code\n\n* flake\n\n* cleanup\n\n* more cleanup\n\n* mypy, flake\n\n* add back code I shouldn't have removed\n\n* black\n\n* test and flake fixes\n\n* fix region_detector for multiple images and add feature and coords padding\n\n* fix imports\n\n* restore null grid embedder\n\n* add back (todo) null region detector\n\n* Bring back import changes, to fix circular imports caused by NLVR2\nreader\n\n* region detector test passing\n\n* model test finally passing\n\n* update torchvision version\n\n* add vqav2 dataset\n\n* add gpu support for detectron feature extraction\n\n* add lmdbCache to cache feature into lmdb database\n\n* fix typo\n\n* update vqa jsonnet\n\n* fix url adding by cat\n\n* Fixes type annotation\n\n* Fixes borked error message\n\n* New feature cache\n\n* Formatting\n\n* Fix the tensor cache\n\n* Be explicit about our dependencies\n\n* Use the new tensor cache\n\n* Adds a test using the tensor cache\n\n* Run NLVR dataprep on GPU\n\n* Tqdm when finding images\n\n* Fixes padding in array field\n\n* Adjust max_length when truncating in PretrainedTransformerTokenizer\n\n* Fewer print statements\n\n* remove VQA from this branch and copy default vilbert parameters.\n\n* add VQAv2 dataset\n\n* Added dataset reader and model tests, which are now passing\n\n* Sanjay's vision features cache script (#4633)\n\n* Use LMDB cache in NLVR2 dataset reader; fix a few typos\n\n* Standalone script for caching image features\n\n* Removing reference to LMDB cache in NLVR2 dataset reader\n\n* Adding back asterisk in nlvr2 dataset reader\n\n* Fixing one variable name mistake\n\n* Decreasing batch size and making a few cuda-related changes\n\n* Loading images in batches to avoid GPU OOM error\n\n* Pedantic changes for consistency\n\n* Run the pre-processing with the models and not the data loading\n\n* Filter out paths of images already cached\n\n* Add image extensions other than png\n\n* Fixes import error\n\n* Makes the vision features script work alongside other scripts or training runs\n\nCo-authored-by: sanjays <sanjays@ip-10-0-0-157.us-west-2.compute.internal>\nCo-authored-by: sanjays <sanjays@ip-10-1-10-157.us-west-2.compute.internal>\nCo-authored-by: Sanjay Subramanian <sanjays@allennlp-server1.corp.ai2>\nCo-authored-by: Sanjay Subramanian <sanjays_ssubramanian@hotmail.com>\n\n* Adds missing imports\n\n* Makes TensorCache into a real MutableMapping\n\n* Formatting\n\n* Changelog\n\n* Fix typecheck\n\n* Makes the NLVR2 reader work with Pete's new code\n\n* Fix type annotation\n\n* Formatting\n\n* Backwards compatibility\n\n* Restore NLVR to former glory\n\n* Types and multi-process reading for VQAv2\n\n* Formatting\n\n* Fix tests\n\n* Fix broken config\n\n* Update grid embedder test\n\n* Fix vilbert_from_huggingface configuration\n\n* Don't run the vilbert_from_huggingface test anymore\n\n* Remove unused test fixtures\n\n* Fix the region detector test\n\n* Fix vilbert-from-huggingface and bring it back\n\n* Fuck the linter\n\n* Fix for VQA test\n\n* Why was this metric disabled?\n\n* Black and flake\n\n* Re-add VQA reader\n\n* Image featurizers now need to be called with sizes\n\n* Run the region detector test on GPU\n\n* Run more stuff on GPU\n\nThe CPU test runner doesn't have enough memory.\n\n* Depend on newer version of Detectron\n\n* Reinstall Detectron before running tests\n\n* Just force CUDA to be on, instead of reinstalling Detecton2\n\n* Fixes VQA2 DatasetReader\n\n* Fix documentation\n\n* Detectron needs CUDA_HOME to be set during install\n\nAt least this thing fails quickly.\n\n* Try a different way of wrangling the detectron installer\n\n* Try a different way of wrangling the detectron installer\n\n* Bring back amp\n\n* Refactored VQA reader\n\n* More training paths\n\n* Remove debug code\n\n* Don't check in debug code\n\n* Auto-detect GPU to use\n\n* Apply indexers later\n\n* Fix typo\n\n* Register the model\n\n* Fields live on CPU. Only batches get GPUs.\n\n* black\n\n* black, flake\n\n* mypy\n\n* more flake\n\n* More realistic training config\n\n* Adds a basic Predictor for VQAv2\n\n* Make vilbert output human-readable\n\n* Forgot to enumerate\n\n* Use the right namspace\n\n* Trying to make tests faster, and passing\n\n* add image prefix when loading coco image\n\n* fix vqav2 dataset reader and config file\n\n* use two regions, to make tests pass\n\n* black\n\n* Output probabilities in addition to logits\n\n* Make it possible to turn off the cache\n\n* Turn off the cache in the predictor\n\n* Fix the VQA predictor\n\n* change the experiment to the defualt vilbert hyperparams.\n\n* add default experiment_from_huggingface.json\n\n* fix typos in vqa reader\n\n* Proper probabilities\n\n* Formatting\n\n* Remove unused variable\n\n* Make mypy happy\n\n* Fixed loss function, metric, and got tests to pass\n\n* Updates the big training config\n\n* Put real settings into the vilbert_vqa config\n\n* Strings are lists in Python\n\n* Make mypy happy\n\n* Formatting\n\n* Unsatisfying mypy\n\n* Config changes to make this run\n\n* Fix dimensionality of embeddings\n\n* clean the code and add the image_num_heads and combine_num_heads\n\n* fix answer vocab and add save and load from pre-extracted vocab\n\n* fix loss and update save_answer_vocab script\n\n* Typo\n\n* Fixed fusion method\n\n* Tweaking the VQA config some more\n\n* Moved the from_huggingface config\n\n* 20 epochs\n\n* Set up the learning rate properly\n\n* Simplify\n\n* Hardcoded answer vocab\n\n* Don't be lazy\n\n* Steps per epoch cannot be None\n\n* Let's chase the right score\n\n* Fixing some parameter names\n\n* Fields are stored on CPUs\n\n* Bigger batch size, easier distributed training\n\n* Don't run the debug code by default\n\n* VQA with the Transformer Toolkit (#4729)\n\n* transformer toolkit: BertEmbeddings\n\n* transformer toolkit: BertSelfAttention\n\n* transformer toolkit: BertSelfOutput\n\n* transformer toolkit: BertAttention\n\n* transformer toolkit: BertIntermediate\n\n* transformer toolkit: BertOutput\n\n* transformer toolkit: BertLayer\n\n* transformer toolkit: BertBiAttention\n\n* transformer toolkit: BertEmbeddings\n\n* transformer toolkit: BertSelfAttention\n\n* transformer toolkit: BertSelfOutput\n\n* transformer toolkit: BertAttention\n\n* transformer toolkit: BertIntermediate\n\n* transformer toolkit: BertOutput\n\n* transformer toolkit: BertLayer\n\n* transformer toolkit: BertBiAttention\n\n* Attention scoring functions\n\n* merging output and self output\n\n* utility to replicate layers, further cleanup\n\n* adding sinusoidal positional encoding\n\n* adding activation layer\n\n* adding base class for generic loading of pretrained weights\n\n* further generalizing, adding tests\n\n* updates\n\n* adding bimodal encoder, kwargs in from_pretrained_module\n\n* vilbert using transformer toolkit\n\n* fixing test function\n\n* changing to torch.allclose\n\n* fixing attention score api\n\n* bug fix in bimodal output\n\n* changing to older attention modules\n\n* _construct_default_mapping returns mapping\n\n* adding kwargs to _get_input_arguments, adding examples\n\n* using cached_transformers\n\n* making transformer_encoder more general\n\n* added get_relevant_module, loading by name\n\n* fixing constructor name\n\n* undoing failure after merge\n\n* misc minor changes\n\n* Transformer toolkit (#4577)\n\n* transformer toolkit: BertEmbeddings\n\n* transformer toolkit: BertSelfAttention\n\n* transformer toolkit: BertSelfOutput\n\n* transformer toolkit: BertAttention\n\n* transformer toolkit: BertIntermediate\n\n* transformer toolkit: BertOutput\n\n* transformer toolkit: BertLayer\n\n* transformer toolkit: BertBiAttention\n\n* transformer toolkit: BertEmbeddings\n\n* transformer toolkit: BertSelfAttention\n\n* transformer toolkit: BertSelfOutput\n\n* transformer toolkit: BertAttention\n\n* transformer toolkit: BertIntermediate\n\n* transformer toolkit: BertOutput\n\n* transformer toolkit: BertLayer\n\n* transformer toolkit: BertBiAttention\n\n* Attention scoring functions\n\n* merging output and self output\n\n* utility to replicate layers, further cleanup\n\n* adding sinusoidal positional encoding\n\n* adding activation layer\n\n* adding base class for generic loading of pretrained weights\n\n* further generalizing, adding tests\n\n* updates\n\n* adding bimodal encoder, kwargs in from_pretrained_module\n\n* vilbert using transformer toolkit\n\n* fixing test function\n\n* changing to torch.allclose\n\n* fixing attention score api\n\n* bug fix in bimodal output\n\n* changing to older attention modules\n\n* _construct_default_mapping returns mapping\n\n* adding kwargs to _get_input_arguments, adding examples\n\n* using cached_transformers\n\n* making transformer_encoder more general\n\n* added get_relevant_module, loading by name\n\n* fixing constructor name\n\n* undoing failure after merge\n\n* misc minor changes\n\nCo-authored-by: Dirk Groeneveld <dirkg@allenai.org>\n\n* separate num_attention_heads for both modalities, default arguments\n\n* adding tests for toolkit examples\n\n* debug statements for failing test\n\n* removing debug statements, reordering\n\n* Typo\n\n* Some compatibility with the transformer toolkit\n\n* Reorganize the image inputs\n\n* More transformer toolkit compatibility\n\n* Debug settings\n\n* Let's be more tolerant\n\n* Fix how VilBERT runs\n\nCo-authored-by: Akshita Bhagia <akshita23bhagia@gmail.com>\n\n* Make the region detector and region embedder lazy\n\n* Fix references to the model\n\n* Make various automated tests pass\n\n* Formatting\n\n* More logging\n\n* One more logging statement\n\n* Read answer vocab from vocab file instead of determining it automatically\n\n* Don't keep the files open so long\n\n* Use most of the validation set for training as well\n\n* Get ready to be lazy\n\n* Upgrade paths\n\n* Be lazy\n\n* Keep unanswerable questions only during test time\n\n* Fix the from_huggingface config\n\n* Fixes the VQA score\n\n* VQA specific metric\n\n* Fixes some tests\n\n* Tests pass!\n\n* Formatting\n\n* Use the correct directory\n\n* Use the region detector that's meant for testing\n\n* Read the test split properly\n\n* Be a little more verbose while discovering images\n\n* Modernize Vilbert VQA\n\n* Update NLVR, but it still doesn't run\n\n* Formatting\n\n* Remove NLVR\n\n* Fix the last test\n\n* Formatting\n\n* Conditionally export the VilbertVqaPredictor\n\n* ModuleNotFoundError is a type of ImportError\n\n* Fix test-install\n\n* Try the broken test with a fixed seed\n\n* Try a bunch of seeds\n\n* Smaller model to get bigger magnitudes\n\n* Now that the test works, we don't need to specify the seeds anymore\n\nCo-authored-by: Matt Gardner <mattg@allenai.org>\nCo-authored-by: jiasenlu <jiasenlu@gatech.edu>\nCo-authored-by: Jaemin Cho <heythisischo@gmail.com>\nCo-authored-by: jiasenlu <echosenm@gmail.com>\nCo-authored-by: sanjays <sanjays@ip-10-0-0-157.us-west-2.compute.internal>\nCo-authored-by: sanjays <sanjays@ip-10-1-10-157.us-west-2.compute.internal>\nCo-authored-by: Sanjay Subramanian <sanjays@allennlp-server1.corp.ai2>\nCo-authored-by: Sanjay Subramanian <sanjays_ssubramanian@hotmail.com>\nCo-authored-by: Akshita Bhagia <akshita23bhagia@gmail.com>\nCo-authored-by: Evan Pete Walsh <epwalsh10@gmail.com>\n\n* SNLI_VE dataset reader (#4799)\n\n* adding VE reader\n\n* removing jsonlines\n\n* blackify\n\n* intial VE model\n\n* adding VisionReader for common vision components\n\n* fix test file\n\n* fix doc\n\n* temporarily removing VE model\n\n* bug fix\n\n* cleanup\n\n* removing unnecessary check\n\n* simplify\n\n* Visual entailment model code (#4822)\n\n* VE model code\n\n* adding VE model\n\n* misc minor updates\n\n* update changelog\n\n* Added GQA reader (#4832)\n\n* Adds reader for GQA dataset. Will download questions from https://cs.stanford.edu/people/dorarad/gqa/download.html.\n\n* Cleaned up GQA reader tests\n\n* Other VQA datasets (#4834)\n\n* Make the VQA reader work for the other datasets\n\n* Also find pngs\n\n* Really support pngs\n\n* Remove debug code\n\n* More logging\n\n* Unexpected formatting\n\n* Respect the device\n\n* This is how your replace things in named tuples.\n\n* Remove unused import\n\n* This is how you override a method properly.\n\n* This is how you set parameters in detectron.\n\n* Also set the device for the region detector\n\n* Training configs for all three datasets contained in VQA\n\n* Bigger batches\n\n* Bigger batches for image processing\n\n* Fix vilbert-from-huggingface config\n\n* Make the config switch modes for constructing vocab\n\n* More vocab, more docs, better way of deriving vocab\n\n* Modernize the from_huggingface config\n\n* More updates to the from_huggingface config\n\n* Better hyperparameters stolen from another project\n\n* Fix for inverted parameter\n\n* Formatting\n\n* Throw a meaningful error message when we don't have images\n\n* Add a warning that includes instructions for how to fix things\n\n* Remove unused script\n\n* Merge issue\n\n* adding multilabel option (#4843)\n\n* Generalizing transformer layers (#4776)\n\n* adding HF tests, docstrings for AttentionLayer, TransformerLayer, TransformerBlock\n\n* temp change to check if tests pass\n\n* undoing temp change\n\n* ci update\n\n* more ci updates\n\n* changing test run\n\n* update makefile\n\n* temp change\n\n* isolating failing case\n\n* further debugging\n\n* fail check\n\n* reverting to older CI\n\n* test with reduced batch size\n\n* cleanup\n\n* more cleanup\n\n* oops, fix\n\n* gqa reader fixes during vilbert training (#4851)\n\n* Refactored shared code\n\n* typecheck fix\n\n* rebase\n\n* Refactored shared code\n\n* typecheck fix\n\n* rebase\n\n* Cleaned up GQA reader tests\n\n* Modify instance format for vilbert-vqa model\n\n* update for vision branch bump\n\nCo-authored-by: Jackson Stokes <jacksons@Jacksons-MacBook-Pro.local>\nCo-authored-by: Dirk Groeneveld <dirkg@allenai.org>\n\n* Toolkit: Adding documentation and small changes for `BiModalAttention` (#4859)\n\n* adding documentation for bimodal attn, small fixes\n\n* changing the way mask is applied\n\n* using large value rather than inf\n\n* Update comment\n\nCo-authored-by: Dirk Groeneveld <groeneveld@gmail.com>\n\n* moving apply_mask to util\n\nCo-authored-by: Dirk Groeneveld <groeneveld@gmail.com>\n\n* Make tests work again (#4865)\n\n* New import paths\n\n* Duplicate entries\n\n* Dataset readers can't be lazy anymore\n\n* Switch to torchvision for vision components \ud83d\udc40, simplify and improve MultiProcessDataLoader (#4821)\n\n* implement TorchImageLoader\n\n* implement ResnetBackbone\n\n* add resize + normalize to image loader\n\n* finalize FasterRcnnRegionDetector\n\n* pin torchvision\n\n* fix VQAv2Reader\n\n* add box mask field\n\n* dataset reader fixes\n\n* fix model tests\n\n* doc fixes\n\n* add threshold parameters to FasterRcnnRegionDetector\n\n* address @dirkgr comments\n\n* mask fixes\n\n* shape comments\n\n* add some more comments\n\n* cache answers_by_question_id\n\n* implement LocalCacheResource\n\n* fix\n\n* add read-only option to cache\n\n* fix\n\n* simplify data loader\n\n* make featurizer and detector optional in readers\n\n* Cache in memory\n\n* back pressure is important I guess\n\n* merge\n\n* Updated configs\n\n* Fixes the way we apply masks\n\n* Use more of Jiasen's real settings\n\n* Upgrade the from_huggingface config\n\n* Switch back to the images on corpnet\n\n* Fix random seeds\n\n* Bigger model needs smaller batch size\n\n* Adds ability to selectively ignore one input\n\n* address some comments\n\n* format + lint\n\n* fixes\n\n* Bring back bert-base configs\n\n* fix error handling\n\n* fix test\n\n* fix typo\n\n* use lock when possible\n\nCo-authored-by: Dirk Groeneveld <dirkg@allenai.org>\n\n* doc fixes\n\n* Only cache, no featurizing (#4870)\n\n* implement TorchImageLoader\n\n* implement ResnetBackbone\n\n* add resize + normalize to image loader\n\n* finalize FasterRcnnRegionDetector\n\n* pin torchvision\n\n* fix VQAv2Reader\n\n* add box mask field\n\n* dataset reader fixes\n\n* fix model tests\n\n* doc fixes\n\n* add threshold parameters to FasterRcnnRegionDetector\n\n* address @dirkgr comments\n\n* mask fixes\n\n* shape comments\n\n* add some more comments\n\n* cache answers_by_question_id\n\n* implement LocalCacheResource\n\n* fix\n\n* add read-only option to cache\n\n* fix\n\n* simplify data loader\n\n* make featurizer and detector optional in readers\n\n* Cache in memory\n\n* back pressure is important I guess\n\n* merge\n\n* Updated configs\n\n* Fixes the way we apply masks\n\n* Use more of Jiasen's real settings\n\n* Upgrade the from_huggingface config\n\n* Switch back to the images on corpnet\n\n* Fix random seeds\n\n* Bigger model needs smaller batch size\n\n* Adds ability to selectively ignore one input\n\n* address some comments\n\n* format + lint\n\n* fixes\n\n* Bring back bert-base configs\n\n* fix error handling\n\n* fix test\n\n* Adds the ability to read from a feature cache, but not run any featurization\n\n* Update tests\n\n* Let's stick with \"feature_cache\"\n\nAs long as we're consistent ...\n\n* More epochs, more random\n\n* Use the new parameters\n\n* Fix initialization\n\n* Make tests work, add some documentation\n\n* Remove the read_from_cache parameter\n\n* Cleanup of training configs\n\n* Typecheck\n\n* Building docs right\n\n* Better settings for VQA\n\n* Leave the image_feature_dim at 1024\n\nCo-authored-by: epwalsh <epwalsh10@gmail.com>\n\n* Make images easier to find for Visual Entailment (#4878)\n\n* implement TorchImageLoader\n\n* implement ResnetBackbone\n\n* add resize + normalize to image loader\n\n* finalize FasterRcnnRegionDetector\n\n* pin torchvision\n\n* fix VQAv2Reader\n\n* add box mask field\n\n* dataset reader fixes\n\n* fix model tests\n\n* doc fixes\n\n* add threshold parameters to FasterRcnnRegionDetector\n\n* address @dirkgr comments\n\n* mask fixes\n\n* shape comments\n\n* add some more comments\n\n* cache answers_by_question_id\n\n* implement LocalCacheResource\n\n* fix\n\n* add read-only option to cache\n\n* fix\n\n* simplify data loader\n\n* make featurizer and detector optional in readers\n\n* Cache in memory\n\n* back pressure is important I guess\n\n* merge\n\n* Updated configs\n\n* Fixes the way we apply masks\n\n* Use more of Jiasen's real settings\n\n* Upgrade the from_huggingface config\n\n* Switch back to the images on corpnet\n\n* Fix random seeds\n\n* Bigger model needs smaller batch size\n\n* Adds ability to selectively ignore one input\n\n* address some comments\n\n* format + lint\n\n* fixes\n\n* Bring back bert-base configs\n\n* fix error handling\n\n* fix test\n\n* Adds the ability to read from a feature cache, but not run any featurization\n\n* Update tests\n\n* Let's stick with \"feature_cache\"\n\nAs long as we're consistent ...\n\n* More epochs, more random\n\n* Use the new parameters\n\n* Fix initialization\n\n* Make tests work, add some documentation\n\n* Remove the read_from_cache parameter\n\n* Cleanup of training configs\n\n* Typecheck\n\n* Building docs right\n\n* Better settings for VQA\n\n* Open cached paths when reading json lines\n\n* By default, autodetect GPUs when training\n\n* Switch to torchvision\n\n* Download training data from the web\n\n* This needs to stay at 1024 until we get the new featurization model\n\n* Have a more descriptive error message when images are missing\n\n* Update vilbert_ve_from_huggingface.jsonnet\n\nCo-authored-by: epwalsh <epwalsh10@gmail.com>\nCo-authored-by: Akshita Bhagia <akshita23bhagia@gmail.com>\n\n* Adding f1 score (#4890)\n\n* adding f1 score\n\n* updated config\n\n* import MultiTaskDataLoader to data_loaders/__init__.py (#4885)\n\n* Make GQA work (#4884)\n\n* Refactored shared code\n\n* typecheck fix\n\n* rebase\n\n* Refactored shared code\n\n* typecheck fix\n\n* rebase\n\n* Cleaned up GQA reader tests\n\n* Modify instance format for vilbert-vqa model\n\n* update for vision branch bump\n\n* Adding training config for GQA\n\n* Unnamed variable\n\n* Various GQA fixes\n\n* Temporary extra configs needed to make vocab\n\n* Remove unused file\n\n* Optimize VQA score instead of F-Score\n\n* Use our newly created vocab\n\n* Remove temporary configs\n\n* Don't fail when we don't need to create a directory\n\n* Make a config that works on the servers as well\n\n* Update comment\n\n* A new command to count instances\n\n* Temporary config to count instances\n\n* Undo temporary changes\n\n* Put in the correct number of steps per epoch\n\n* Remove this number from the config because it's almost certainly wrong\n\n* Don't put Fields in Tuples\n\n* Formatting\n\n* More informative error message when batches are heterogeneous\n\n* Formatting\n\n* Not my type\n\n* Generate the fields properly when answers are missing\n\n* Properly discard instances with missing answers\n\n* Changelog\n\n* Update number of steps per epoch\n\n* Adds a config for balanced GQA\n\n* fix file_utils extract with directory\n\n* fix Batch._check_types\n\n* Fill in URL\n\nCo-authored-by: Jackson Stokes <jacksons@Jacksons-MacBook-Pro.local>\nCo-authored-by: Akshita Bhagia <akshita23bhagia@gmail.com>\nCo-authored-by: Evan Pete Walsh <epwalsh10@gmail.com>\n\n* Toolkit: Cleaning up TransformerEmbeddings (#4900)\n\n* fixing issue of non-deterministic dropout\n\n* updating TransformerEmbeddings\n\n* ImageFeatureEmbeddings is now a subclass of Embeddings\n\n* allowing for no token type embeddings\n\n* fixing kwargs for loading pretrained module\n\n* Data loading cuda device (#4879)\n\n* add test with tensor fields\n\n* improve nn.util.move_to_device\n\n* ensure start_method is 'spawn' when using lazy and mem pin\n\n* add 'non_blocking' arg to 'move_to_device'\n\n* fix fake test tensor\n\n* fix sampler test\n\n* lint\n\n* fix 'move_to_device'\n\n* fix condition check\n\n* add device to data loader\n\n* clean up doc string\n\n* rename 'device' arg to 'cuda_device'\n\n* pinning is very slow, revert\n\n* DataLoaders load to CUDA device\n\n* fix evaluate test\n\n* rename 'multi_process_*' -> 'multiprocess' for consistency (#4906)\n\n* MultiProcessDataLoader takes PathLike data_path (#4908)\n\n* remove PyTorchDataLoader, add SimpleDataLoader for testing (#4907)\n\n* remove PyTorchDataLoader, add SimpleDataLoader for testing\n\n* fix test\n\n* comments\n\n* improve data loading docs (#4909)\n\n* improve data loading docs\n\n* document best practices, add 'get_batch_size' method to samplers\n\n* try fix annoying unrelated test\n\n* revert that\n\n* clarify handling of 'max_instances_in_memory'\n\n* fix imports in file_utils\n\n* rename 'master' -> 'primary' for distributed training (#4910)\n\n* improve worker error handling in MultiProcessDataLoader (#4912)\n\n* improve worker error handling\n\n* rename test file\n\n* Toolkit decoder (#4914)\n\n* adding cross_attention, renaming block -> stack\n\n* stack can be initialized with layer too\n\nCo-authored-by: Dirk Groeneveld <dirkg@allenai.org>\n\n* resolve _read type (#4916)\n\n* resolve _read type\n\n* fix sharded reader\n\n* fix data loader arg\n\n* Multitask example (#4898)\n\n* Make the VQA reader work for the other datasets\n\n* Also find pngs\n\n* Really support pngs\n\n* Remove debug code\n\n* More logging\n\n* Unexpected formatting\n\n* Respect the device\n\n* This is how your replace things in named tuples.\n\n* Remove unused import\n\n* This is how you override a method properly.\n\n* This is how you set parameters in detectron.\n\n* Also set the device for the region detector\n\n* Training configs for all three datasets contained in VQA\n\n* Bigger batches\n\n* Bigger batches for image processing\n\n* Fix vilbert-from-huggingface config\n\n* Make the config switch modes for constructing vocab\n\n* More vocab, more docs, better way of deriving vocab\n\n* Modernize the from_huggingface config\n\n* More updates to the from_huggingface config\n\n* Better hyperparameters stolen from another project\n\n* Fix for inverted parameter\n\n* Formatting\n\n* Throw a meaningful error message when we don't have images\n\n* Add a warning that includes instructions for how to fix things\n\n* Remove unused script\n\n* Merge issue\n\n* Adds named splits to the SNLI-VE reader\n\n* Make the multitask data loader discoverable\n\n* Formatting\n\n* More flexible inputs to the dataset readers\n\n* Prototype config for the multitask training job\n\n* json_lines_from_file() already calls cached_path()\n\n* Visual entailment should track accuracy\n\n* Switching to torch\n\n* Fixing VE image paths\n\n* Formatting\n\n* Experimentally use threaded_generator to read instances from readers simultaneously\n\n* Vilbert backbone\n\n* Fixed paths\n\n* Formatting\n\n* Adds heads\n\n* Revert \"Experimentally use threaded_generator to read instances from readers simultaneously\"\n\nThis reverts commit a633e67134cf82f103071eba8ee560825f258c7b.\n\n* Multitask trains now!\n\n* Remove useless parameter from GQA reader\n\n* Updated multitask config\n\n* Schedulers produce batches, not instances\n\n* Track multiple metrics\n\n* Make mypy happy\n\n* Formatting\n\n* Keep better track of which heads have been called\n\n* Fix the merge\n\n* We have more than strings for input\n\n* Remove unused imports\n\n* -1 is CPU\n\n* Go back to tracking instances per epoch so that the samplers can work\n\n* Better error message\n\n* A useful sampler to have\n\n* We haven't indexed until we've indexed\n\n* Makes tests pass\n\n* Formatting\n\n* Fine-tuning the metric tracker\n\n* Update model configs for my changes\n\n* Fixing model configs for Akshita's changes\n\n* Implement VisionTextModel in terms of VilbertBackbone\n\n* Formatting\n\n* Fix stale comment\n\n* Use the server paths by default, not Dirk's desktop\n\n* Fix tests\n\n* Formatting again\n\n* Removed data loader parameters that don't exist anymore\n\n* Clarified comment\n\nCo-authored-by: Evan Pete Walsh <epwalsh10@gmail.com>\n\n* Moves vision models to allennlp-models (#4918)\n\n* Moves vision models to allennlp-models\n\n* Also move test fixtures\n\n* Don't return so many instances if we're cutting them out later anyways\n\n* We actually need this image\n\n* Formatting\n\n* Fixing more paths\n\n* Prepare for release v2.0.0rc1\n\n* Make releasing work with the renamed master branch, and with the vision branch\n\n* Debugging the release process in the slowest way possible\n\n* Another attempt at fixing the release process\n\n* Generic Callbacks (#4917)\n\n* Better Callbacks\n\n* Reformatting\n\n* Fixes\n\n* Tests for updated TrainerCallback\n\n* Formatting and Type-Checking fixes\n\n* Consistent metric tracker (#4928)\n\n* Makes the metric tracker more consistent\n\n* Turns out we need best_epoch_metrics after all.\n\n* Backwards compatibility\n\n* Formatting\n\n* Remove old script\n\n* Changes CI since we won't have a `vision` branch anymore\n\n* fix up CHANGELOG\n\nCo-authored-by: Matt Gardner <mattg@allenai.org>\nCo-authored-by: epwalsh <epwalsh10@gmail.com>\nCo-authored-by: James Barry <james.barry26@mail.dcu.ie>\nCo-authored-by: jiasenlu <jiasenlu@gatech.edu>\nCo-authored-by: Jaemin Cho <heythisischo@gmail.com>\nCo-authored-by: jiasenlu <echosenm@gmail.com>\nCo-authored-by: sanjays <sanjays@ip-10-0-0-157.us-west-2.compute.internal>\nCo-authored-by: sanjays <sanjays@ip-10-1-10-157.us-west-2.compute.internal>\nCo-authored-by: Sanjay Subramanian <sanjays@allennlp-server1.corp.ai2>\nCo-authored-by: Sanjay Subramanian <sanjays_ssubramanian@hotmail.com>\nCo-authored-by: Akshita Bhagia <akshita23bhagia@gmail.com>\nCo-authored-by: jvstokes <40584422+jvstokes@users.noreply.github.com>\nCo-authored-by: Jackson Stokes <jacksons@Jacksons-MacBook-Pro.local>\nCo-authored-by: Karen Hambardzumyan <mahnerak@gmail.com>\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class TestNnUtil(AllenNlpTestCase):",
            "assert parameters_inspection_dict == util.inspect_parameters(model)",
            "",
            "def test_move_to_device(self):",
            "-        # We're faking the tensor here so that we can test the calls to .cuda() without actually",
            "+        # We're faking the tensor here so that we can test the calls to .to() without actually",
            "# needing a GPU.",
            "class FakeTensor(torch.Tensor):",
            "def __init__(self):",
            "self._device = None",
            "",
            "-            def cuda(self, device):",
            "+            def to(self, device, **kwargs):",
            "self._device = device",
            "return self"
        ]
    },
    {
        "number": 389,
        "comments": "def",
        "commit_message": "Fixing out arg (#1770)\n\n* Updated out arg in functional ivy activations\n\n* Updated out arg docstring in activations\n\n* Updated out arg in functional ivy creation\n\n* Updated out arg in functional ivy dtype\n\n* Updated out arg in functional ivy elementwise\n\n* Updated out arg in functional ivy general\n\n* Updated out arg in functional ivy gradients\n\n* Updated out arg in functional ivy layers\n\n* Updated out arg in functional ivy linalg\n\n* Updated out arg in functional ivy manipulation\n\n* Updated out arg in functional ivy random\n\n* Updated out arg in functional ivy searching\n\n* Updated out arg in functional ivy set\n\n* Updated out arg in functional ivy statistical\n\n* Updated out arg in functional ivy norms\n\n* Updated @handle-out-arg decorator in compositional functions\n\n* Updated out arg in functional ivy general\n\n* Updated out arg in ivy functional layers, searching and set\n\n* lint fixes\n\n* lint fixes in dtype and general\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def solve(x1: torch.Tensor, x2: torch.Tensor) -> torch.Tensor:",
            "",
            "",
            "def svd(",
            "-    x: torch.Tensor, full_matrices: bool = True, out: Optional[torch.Tensor] = None",
            "+    x: torch.Tensor, full_matrices: bool = True",
            ") -> Union[torch.Tensor, Tuple[torch.Tensor, ...]]:",
            "results = namedtuple(\"svd\", \"U S Vh\")",
            "",
            "-    U, D, VT = torch.linalg.svd(x, full_matrices=full_matrices, out=out)",
            "+    U, D, VT = torch.linalg.svd(x, full_matrices=full_matrices)",
            "ret = results(U, D, VT)",
            "return ret"
        ]
    },
    {
        "number": 390,
        "comments": "update API call for refactor fix",
        "commit_message": "[style] consistent nn. and nn.functional (#12124)\n\n* consistent nn. and nn.functional\n\n* fix glitch\n\n* fix glitch #2\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def _calculate_expected_result(",
            "aggregation_op_only_probs = gumbel_dist.sample()",
            "else:",
            "# <float32>[batch_size, num_aggregation_labels - 1]",
            "-        aggregation_op_only_probs = torch.nn.functional.softmax(",
            "+        aggregation_op_only_probs = nn.functional.softmax(",
            "logits_aggregation[:, 1:] / config.aggregation_temperature, dim=-1",
            ")"
        ]
    },
    {
        "number": 392,
        "comments": "format",
        "commit_message": "Fix critical bug for DQN variants and DPG agent\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "class TensorforceModel(Model):",
            "discounts = tf.math.pow(x=discount, y=exponent)",
            "if not self.predict_terminal_values:",
            "discounts = tf.where(",
            "-                    condition=tf.math.greater(x=_terminal, y=one),",
            "-                    x=discounts, y=tf.zeros_like(input=discounts)",
            "+                    condition=tf.math.equal(x=_terminal, y=one),",
            "+                    x=tf.zeros_like(input=discounts), y=discounts",
            ")",
            "",
            "-            reward += discounts * horizon_values",
            "+            reward = reward + discounts * horizon_values",
            "",
            "dependencies = [reward]",
            "if self.summaries == 'all' or 'reward' in self.summaries:"
        ]
    },
    {
        "number": 393,
        "comments": "text fix",
        "commit_message": "Fixed test discovery errors (#7136)\n\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def test_tensorflow_deserialize(",
            "",
            "",
            "@handle_frontend_test(",
            "+    fn_tree=\"tensorflow.keras.activations.get\",",
            "fn_name=st.sampled_from(get_callable_functions(\"keras.activations\")).filter(",
            "lambda x: not x[0].isupper()",
            "and x"
        ]
    },
    {
        "number": 394,
        "comments": "update API call for resource fix",
        "commit_message": "Fix CI issues related to cupy install (#2483)\n\n* remove any cupy install when setting up environments\n\n* revert previous changes to run on cu111 runners\n\n* fix for when no cupy is installed\n\n* remove cupy uninstall for workflows not using latest torch version\n\n* update to cu116 for inference tests\n\n* fix pip uninstall line\n\n* move python environment list to after DS install\n\n* remove cupy uninstall\n\n* re-add --forked\n\n* fix how we get cupy version (should be based on nvcc version)\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "if torch_available and torch.cuda.is_available():",
            "if rocm_major <= 4:",
            "cupy = f\"cupy-rocm-{rocm_major}-{rocm_minor}\"",
            "else:",
            "-        cupy = f\"cupy-cuda{torch.version.cuda.replace('.','')[:3]}\"",
            "+        cupy = f\"cupy-cuda{''.join(map(str,installed_cuda_version()))}\"",
            "if cupy:",
            "extras_require['1bit'].append(cupy)",
            "extras_require['1bit_mpi'].append(cupy)"
        ]
    },
    {
        "number": 395,
        "comments": "rename",
        "commit_message": "Fixes test\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def test_cgcnn_conv():",
            "edge_index = torch.tensor([[0, 0, 0, 1, 2, 3], [1, 2, 3, 0, 0, 0]])",
            "num_nodes = edge_index.max().item() + 1",
            "x = torch.randn((num_nodes, node_dim))",
            "-    pseudo = torch.rand((edge_index.size(1), 3))",
            "+    pseudo = torch.rand((edge_index.size(1), edge_dim))",
            "",
            "conv = CGCNNConv(node_dim, edge_dim)",
            "assert conv.__repr__() == 'CGCNNConv(16, 16)'"
        ]
    },
    {
        "number": 396,
        "comments": "doc update",
        "commit_message": "Refactor/framework context to context (#2115)\n\n* refactor(model): rename framework_context to context\n\n* style: reformat\n\n* refactor: context format changes\n\n* test: fix tensorflow_hub test\n\n* fix: tensorflow_hub related tests\n\n* Apply suggestions from code review\n\n* fix: move detectron test sample image to local\n\n* fix: fix onnx rebase\n\n* fix: fix style\n\nCo-authored-by: Chaoyu <paranoyang@gmail.com>\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "def save(",
            "",
            "Examples::",
            "\"\"\"  # noqa",
            "-    context: t.Dict[str, t.Any] = {\"statsmodels\": statsmodels.__version__}",
            "+    context: t.Dict[str, t.Any] = {",
            "+        \"framework_name\": \"statsmodels\",",
            "+        \"pip_dependencies\": [f\"statsmodels=={_statsmodels_version}\"],",
            "+    }",
            "_model = Model.create(",
            "name,",
            "module=__name__,",
            "metadata=metadata,",
            "-        framework_context=context,",
            "+        context=context,",
            ")",
            "",
            "model.save(_model.path_of(f\"{SAVE_NAMESPACE}{PKL_EXT}\"))"
        ]
    },
    {
        "number": 397,
        "comments": "update API call for refactor fix",
        "commit_message": "Fixed bug after contrib merging\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def spatial_soft_argmax2d(",
            ">>> coords = kornia.spatial_soft_argmax2d(input, False)",
            "tensor([[[1.0000, 1.0000]]])",
            "\"\"\"",
            "-    input_soft: torch.Tensor = dsnt.spatial_softmax_2d(input, temperature)",
            "-    output: torch.Tensor = dsnt.spatial_softargmax_2d(input_soft,",
            "-                                                      normalized_coordinates)",
            "+    input_soft: torch.Tensor = spatial_softmax_2d(input, temperature)",
            "+    output: torch.Tensor = spatial_softargmax_2d(input_soft,",
            "+                                                 normalized_coordinates)",
            "return output"
        ]
    },
    {
        "number": 398,
        "comments": "test",
        "commit_message": "metal timing, fix speed test\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class TestSpeed(unittest.TestCase):",
            "",
            "def test_sum(self):",
            "def f(a, b): return a.sum()",
            "+    helper_test_generic_square('sum', 2048, f, f, onearg=True)",
            "helper_test_generic_square('sum', 4096, f, f, onearg=True)",
            "",
            "def test_partial_sum(self):"
        ]
    },
    {
        "number": 400,
        "comments": "test fix",
        "commit_message": "small fixes to test_frontend_function.\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "def test_frontend_function(",
            "ivy.set_backend(frontend)",
            "",
            "# check for unsupported dtypes in frontend framework",
            "-    function = getattr(ivy, fn_name)",
            "+    function = getattr(ivy.functional.frontends.__dict__[frontend], fn_name)",
            "for d in input_dtypes:",
            "if d in ivy.function_unsupported_dtypes(function, None):",
            "return"
        ]
    },
    {
        "number": 401,
        "comments": "add param for resource fix",
        "commit_message": "Fixed torch.Tensor.to to support device input as a positional argument\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def test_torch_instance_to(",
            "frontend,",
            "):",
            "input_dtype, x, method_num_positional_args, method_all_as_kwargs_np = args_kwargs",
            "+    method_flags.num_positional_args = method_num_positional_args",
            "helpers.test_frontend_method(",
            "init_input_dtypes=input_dtype,",
            "init_all_as_kwargs_np={"
        ]
    },
    {
        "number": 403,
        "comments": "doc update",
        "commit_message": "Fix some typos (#759)\n\n* Fix some typos\n\n* add change log for PR #759\n\n* Update CHANGELOG.md\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def absolute_difference_error(output, target, is_mean=False, name=\"mean_squared_",
            "An optional name to attach to this function.",
            "",
            "\"\"\"",
            "-    # with tf.name_scope(\"mean_squared_error_loss\"):",
            "+    # with tf.name_scope(\"absolute_difference_error_loss\"):",
            "if output.get_shape().ndims == 2:  # [batch_size, n_feature]",
            "if is_mean:",
            "loss = tf.reduce_mean(tf.reduce_mean(tf.abs(output - target), 1), name=name)"
        ]
    },
    {
        "number": 407,
        "comments": "update param for type fix",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class UnigramRecall(Metric):",
            "A tensor of predictions of shape (batch_size, k, sequence_length).",
            "gold_labels : `torch.Tensor`, required.",
            "A tensor of integer class label of shape (batch_size, sequence_length).",
            "-        mask : `torch.Tensor`, optional (default = None).",
            "+        mask : `torch.BoolTensor`, optional (default = None).",
            "A masking tensor the same size as `gold_labels`.",
            "\"\"\"",
            "predictions, gold_labels, mask = self.detach_tensors(predictions, gold_labels, mask)"
        ]
    },
    {
        "number": 408,
        "comments": "update API call for refactor fix",
        "commit_message": "Fix examples and tutorials to use the updated tensor API (#886)\n\n* Fix examples and tutorials to use the updated tensor API\n\n* small fixes\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "\"        # compute the gating function and one minus the gating function\\n\",",
            "\"        gate_intermediate = self.relu(self.lin_gate_z_to_hidden(z_t_1))\\n\",",
            "\"        gate = self.sigmoid(self.lin_gate_hidden_to_z(gate_intermediate))\\n\",",
            "-    \"        one_minus_gate = ng_ones(gate.size()).type_as(gate) - gate\\n\",",
            "+    \"        one_minus_gate = torch.ones(gate.size()).type_as(gate) - gate\\n\",",
            "\"        # compute the 'proposed mean'\\n\",",
            "\"        proposed_mean_intermediate = self.relu(self.lin_proposed_mean_z_to_hidden(z_t_1))\\n\",",
            "\"        proposed_mean = self.lin_proposed_mean_hidden_to_z(proposed_mean_intermediate)\\n\","
        ]
    },
    {
        "number": 410,
        "comments": "change param for math fix",
        "commit_message": "fix formulation issue in rotation_matrix_to_quaternion\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def rotation_matrix_to_quaternion(",
            "return torch.cat([qx, qy, qz, qw], dim=-1)",
            "",
            "def cond_3():",
            "-        sq = torch.sqrt(trace + 1.0 + m00 - m11 - m22) * 2.  # sq = 4 * qw.",
            "+        sq = torch.sqrt(1.0 + m00 - m11 - m22 + eps) * 2.  # sq = 4 * qw.",
            "qw = safe_zero_division(m10 - m01, sq)",
            "qx = safe_zero_division(m02 - m20, sq)",
            "qy = safe_zero_division(m12 - m21, sq)"
        ]
    },
    {
        "number": 411,
        "comments": "test",
        "commit_message": "Do not ignore undefined names in Python code (#1210)\n\n* Undefined name: import io for line 86\n\nDiscovered via #1209\n\n* Undefined name: import Tuple for lines 103, 145, 187\n\n* Undefined name: import Dict for line 35\n\n* Undefined name: import Dict for line 158\n\n* Undefined name: import RandomCrop for lines 830 and 841\n\n* Undefined name: create_pinhole() -> dgm.utils.create_pinhole()\n\n* Undefined name: import rtvec_to_pose for line 505\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Raising NameErrors inside with pytest.raises(Exception):\n\n* Undefined name: op_script = torch.jit.script(op)\n\n* Do not ignore undefined names in Python code\n\n* Do not ignore undefined names in Python code\n\n* Do not ignore undefined names in Python code\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* black --diff to help contributors\n\n* requirements-dev.txt: Add torchgeometry\n\n* Ignore missing rtvec_to_pose()\n\n* Revert changes to requirements-dev.txt\n\n* # noqa: F821 type: ignore\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class TestPosterize(BaseTester):",
            "@pytest.mark.jit",
            "def test_jit(self, device, dtype):",
            "op = torch.jit.script(kornia.enhance.adjust.posterize)",
            "+        op_script = torch.jit.script(op)",
            "inputs = torch.rand(2, 1, 3, 3).to(device=device, dtype=dtype)",
            "expected = op(input, 8)",
            "actual = op_script(input, 8)"
        ]
    },
    {
        "number": 412,
        "comments": "change API",
        "commit_message": "Speed-up warp_affine and fix bugs in RandomAffine [WIP: to add tests] (#474)\n\n* speed-up warp_affine, rotate, random_crop\n\n* added basic speed test for warp_affine\n\n* fixed centerization for random shear and bug (radians instead of degrees)\n\n* add test versus torchvision\n\n* added convert_affinematrix_to_homography function\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def crop_by_boxes(tensor, src_box, dst_box,",
            "dst_trans_src = dst_trans_src.expand(tensor.shape[0], -1, -1)",
            "",
            "bbox = _infer_bounding_box(dst_box)",
            "-    patches: torch.Tensor = warp_perspective(",
            "-        tensor, dst_trans_src, (int(bbox[0].int().data.item()), int(bbox[1].int().data.item())))",
            "+    patches: torch.Tensor = warp_affine(",
            "+        tensor, dst_trans_src[:, :2, :], (int(bbox[0].int().data.item()), int(bbox[1].int().data.item())))",
            "",
            "# return in the original shape",
            "if is_unbatched:"
        ]
    },
    {
        "number": 413,
        "comments": "typo fix",
        "commit_message": "Fix typo\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def KitModel(weight_file = None):",
            "if not dilations:",
            "dilations = [1] * len(IR_node.get_attr('kernel_shape'))",
            "",
            "-        self.add_body(1, \"{:<15} = layers.Conv2DTranpose(name='{}', {}, kernel_size=({}), strides=({}), dilation_rate=({}), padding='{}', use_bias={})({})\".format(",
            "+        self.add_body(1, \"{:<15} = layers.Conv2DTranspose(name='{}', {}, kernel_size=({}), strides=({}), dilation_rate=({}), padding='{}', use_bias={})({})\".format(",
            "IR_node.variable_name,",
            "IR_node.name,",
            "filters_str,"
        ]
    },
    {
        "number": 414,
        "comments": "not clear",
        "commit_message": "Fix bipartite message passing with `target_to_source` flow (#3907)\n\n* fix bipartite message passing with reverse flow\n\n* typo\n\n* update\n\n* fix test\n\n* update\n\n* update\n\n* update\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def test_dynamic_edge_conv_conv():",
            "jit = torch.jit.script(conv.jittable(t))",
            "assert jit((x1, x2)).tolist() == out21.tolist()",
            "assert jit((x1, x2), (batch1, batch2)).tolist() == out22.tolist()",
            "+",
            "+    torch.jit.script(conv.jittable())  # Test without explicit typing."
        ]
    },
    {
        "number": 415,
        "comments": "format",
        "commit_message": "lintfixbot: Auto-commit fixed lint errors in codebase\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def gradient(",
            "",
            "",
            "def xlogy(",
            "-    x: torch.tensor,",
            "-    y: torch.tensor,",
            "-    /,",
            "-    *,",
            "-    out: Optional[torch.tensor] = None",
            "+    x: torch.tensor, y: torch.tensor, /, *, out: Optional[torch.tensor] = None",
            ") -> torch.tensor:",
            "return torch.xlogy(x, y, out=out)"
        ]
    },
    {
        "number": 416,
        "comments": "change API doc",
        "commit_message": "[RLlib] Tf2x preparation; part 2 (upgrading `try_import_tf()`). (#9136)\n\n* WIP.\n\n* Fixes.\n\n* LINT.\n\n* WIP.\n\n* WIP.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* WIP.\n\n* Fixes.\n\n* Test\n\n* Fix.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* LINT.\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def ddpg_actor_critic_loss(policy, model, _, train_batch):",
            "twin_q_t = model.get_twin_q_values(model_out_t,",
            "train_batch[SampleBatch.ACTIONS])",
            "# q_batchnorm_update_ops = list(",
            "-    #     set(tf.get_collection(tf.GraphKeys.UPDATE_OPS)) - prev_update_ops)",
            "+    #     set(tf1.get_collection(tf.GraphKeys.UPDATE_OPS)) - prev_update_ops)",
            "",
            "# Target q-net(s) evaluation.",
            "q_tp1 = policy.target_model.get_q_values(target_model_out_tp1,"
        ]
    },
    {
        "number": 417,
        "comments": "update API call for version fix",
        "commit_message": "fix use of deprecated TF functions.\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class Model(ModelDesc):",
            "if get_current_tower_context().is_training:",
            "wd_w = tf.train.exponential_decay(2e-4, get_global_step_var(),",
            "80000, 0.7, True)",
            "-            wd_cost = tf.mul(wd_w, regularize_cost('.*/W', tf.nn.l2_loss), name='wd_cost')",
            "+            wd_cost = tf.multiply(wd_w, regularize_cost('.*/W', tf.nn.l2_loss), name='wd_cost')",
            "costs.append(wd_cost)",
            "",
            "add_param_summary(('.*/W', ['histogram']))   # monitor W"
        ]
    },
    {
        "number": 418,
        "comments": "remove API call for resource fix",
        "commit_message": "More speed improvements, doc for the model, fixed improper cycling in random_partial_utterances, new color scale for UMAP\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "if __name__ == '__main__':",
            "loss_values.clear()",
            "accuracies.clear()",
            "if step % 100 == 0:",
            "-            vis.draw_projections(embeds.detach().cpu().numpy(), utterances_per_speaker, step)",
            "+            vis.draw_projections(embeds.detach().numpy(), utterances_per_speaker, step)"
        ]
    },
    {
        "number": 419,
        "comments": "change API call for type fix",
        "commit_message": "Improve reproduceability 2/3 (#1906)\n\n* [Repro] Correct reproducability\n\n* up\n\n* up\n\n* uP\n\n* up\n\n* need better image\n\n* allow conversion from no state dict checkpoints\n\n* up\n\n* up\n\n* up\n\n* up\n\n* check tensors\n\n* check tensors\n\n* check tensors\n\n* check tensors\n\n* next try\n\n* up\n\n* up\n\n* better name\n\n* up\n\n* up\n\n* Apply suggestions from code review\n\n* correct more\n\n* up\n\n* replace all torch randn\n\n* fix\n\n* correct\n\n* correct\n\n* finish\n\n* fix more\n\n* up\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class ScoreSdeVpScheduler(SchedulerMixin, ConfigMixin):",
            "x_mean = x + drift * dt",
            "",
            "# add noise",
            "-        noise = torch.randn(x.shape, layout=x.layout, generator=generator).to(x.device)",
            "+        noise = randn_tensor(x.shape, layout=x.layout, generator=generator, device=x.device, dtype=x.dtype)",
            "x = x_mean + diffusion * math.sqrt(-dt) * noise",
            "",
            "return x, x_mean"
        ]
    },
    {
        "number": 422,
        "comments": "update param for math fix",
        "commit_message": "fix #1182\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def get_keras_model():",
            "M.add(KL.Conv2D(32, 3, padding='same', activation='relu'))",
            "M.add(KL.Flatten())",
            "M.add(KL.Dense(512, activation='relu', kernel_regularizer=keras.regularizers.l2(1e-5)))",
            "-        M.add(KL.Dropout(0.5))",
            "+        M.add(KL.Dropout(rate=0.5))",
            "M.add(KL.Dense(10, activation=None, kernel_regularizer=keras.regularizers.l2(1e-5)))",
            "return M"
        ]
    },
    {
        "number": 423,
        "comments": "no API",
        "commit_message": "FIX: trainig fails if targets list is empty (#198)\n\n* FIX: trainig fails if targets list is empty\n\n* Update utils.py\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def build_targets(model, targets):",
            "",
            "# Class",
            "tcls.append(c)",
            "-        if c.shape[0]:",
            "+        if nt:",
            "assert c.max() <= layer.nC, 'Target classes exceed model classes'",
            "",
            "return txy, twh, tcls, indices"
        ]
    },
    {
        "number": 424,
        "comments": "change API call for resource fix",
        "commit_message": "adding eval lm changes for model parallel (#1113)\n\nSummary:\n# Before submitting\n\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\n- [ ] Did you make sure to update the docs?\n- [ ] Did you write any new necessary tests?\n\n## What does this PR do?\nFixes # (issue).\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding \ufffd\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1113\n\nReviewed By: myleott\n\nDifferential Revision: D20670665\n\nfbshipit-source-id: 8e2846637195b7200f1f60a8421d2fe5ffab789b\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def main(parsed_args):",
            "",
            "def cli_main():",
            "parser = options.get_eval_lm_parser()",
            "+    add_distributed_training_args(parser)",
            "args = options.parse_args_and_arch(parser)",
            "-    main(args)",
            "+    distributed_utils.call_main(args, main)",
            "",
            "",
            "if __name__ == '__main__':"
        ]
    },
    {
        "number": 425,
        "comments": "add param for resource fix",
        "commit_message": "degree device bugfix\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def degree(index, num_nodes=None, dtype=None, device=None):",
            "tensor([3., 1., 1.])",
            "\"\"\"",
            "num_nodes = maybe_num_nodes(index, num_nodes)",
            "-    out = torch.zeros((num_nodes), dtype=dtype, device=device)",
            "+    out = torch.zeros((num_nodes), dtype=dtype, device=index.device)",
            "return out.scatter_add_(0, index, out.new_ones((index.size(0))))"
        ]
    },
    {
        "number": 426,
        "comments": "change condition check for version fix",
        "commit_message": "fix ctc init condition\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class TransducerTasks(torch.nn.Module):",
            "if ctc_loss:",
            "self.ctc_lin = torch.nn.Linear(encoder_dim, output_dim)",
            "",
            "-            if LooseVersion(torch.__version__) < LooseVersion(\"1.7.0\"):",
            "+            if LooseVersion(torch.__version__) > LooseVersion(\"1.0.1\"):",
            "self.ctc_loss = torch.nn.CTCLoss(",
            "blank=blank_id,",
            "reduction=\"sum\","
        ]
    },
    {
        "number": 429,
        "comments": "update API call for refactor fix",
        "commit_message": "[Feat] Initiate AutoAugment modules (#2181)\n\n* init\n\n* Added autoaugment\n\n* Added RandAugment\n\n* Added trivial augment\n\n* Added missing files\n\n* Updated docs\n\n* refactoring\n\n* Added missing files\n\n* Added shear and translate functions\n\n* update\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* update\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fixed translate bug\n\n* Added color\n\n* Added autocontrast\n\n* Fix\n\n* Fix doctest\n\n* Fixed typing\n\n* Removed legacy generators\n\n* fix\n\n* fixed cutmix float64 bug\n\n* include more test\n\n* Sequential support first commit\n\n* Added missing files\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* first refactor\n\n* Make it compatible with AugmentationSequential\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Added more tests\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Make ops module lazy loading\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Make policy module lazy loading\n\n* Lazy loading auto module\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Make augmentation module lazy loading\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Bug fix\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix\n\n* fix\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix\n\n* final fix\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Apply suggestions from code review\n\nCo-authored-by: Jo\u00e3o Gustavo A. Amorim <joaogustavoamorim@gmail.com>\n\n* Revert review commnets\n\n* Update\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\nCo-authored-by: Jo\u00e3o Gustavo A. Amorim <joaogustavoamorim@gmail.com>\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class RandomThinPlateSpline(AugmentationBase2D):",
            "",
            "def generate_parameters(self, shape: torch.Size) -> Dict[str, Tensor]:",
            "B, _, _, _ = shape",
            "-        src = torch.tensor([[[-1.0, -1.0], [-1.0, 1.0], [1.0, -1.0], [1.0, 1.0], [0.0, 0.0]]]).expand(B, 5, 2)  # Bx5x2",
            "+        src = tensor([[[-1.0, -1.0], [-1.0, 1.0], [1.0, -1.0], [1.0, 1.0], [0.0, 0.0]]]).expand(B, 5, 2)  # Bx5x2",
            "dst = src + self.dist.rsample(src.shape)",
            "return dict(src=src, dst=dst)"
        ]
    },
    {
        "number": 431,
        "comments": "typo fix",
        "commit_message": "[Bug Fixed] use torch.cuda.is_available() (#2661)\n\nCo-authored-by: Olatunji Ruwase <olruwase@microsoft.com>\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class FP16_Optimizer(DeepSpeedOptimizer):",
            "self.deepspeed = deepspeed",
            "self.has_moe_layers = has_moe_layers",
            "self.using_pipeline = self.deepspeed.pipeline_parallelism",
            "-        if not torch.cuda.is_available:",
            "+        if not torch.cuda.is_available():",
            "raise SystemError(\"Cannot use fp16 without CUDA.\")",
            "self.optimizer = init_optimizer"
        ]
    },
    {
        "number": 432,
        "comments": "version fix",
        "commit_message": "[Pytorch] pytorch only timesteps (#724)\n\n* pytorch timesteps\n\n* style\n\n* get rid of if-else\n\n* fix test\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class PNDMScheduler(SchedulerMixin, ConfigMixin):",
            "::-1",
            "].copy()  # we copy to avoid having negative strides which are not supported by torch.from_numpy",
            "",
            "-        self.timesteps = np.concatenate([self.prk_timesteps, self.plms_timesteps]).astype(np.int64)",
            "+        timesteps = np.concatenate([self.prk_timesteps, self.plms_timesteps]).astype(np.int64)",
            "+        self.timesteps = torch.from_numpy(timesteps).to(device)",
            "",
            "self.ets = []",
            "self.counter = 0"
        ]
    },
    {
        "number": 433,
        "comments": "update API call for refactor fix",
        "commit_message": "fix homography regression example (#349)\n\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def HomographyRegressionApp():",
            "[-1, 1],  # top-right",
            "]]).to(dst_homo_src.device)",
            "# transform points",
            "-            pts_dst = dgm.transform_points(dgm.inverse(dst_homo_src), pts_src)",
            "+            pts_dst = dgm.transform_points(torch.inverse(dst_homo_src), pts_src)",
            "",
            "def compute_factor(size):",
            "return 1.0 * size / 2"
        ]
    },
    {
        "number": 435,
        "comments": "value change",
        "commit_message": "fix examples broken by external changes\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "if __name__ == \"__main__\":",
            "print(\"args:\", args)",
            "",
            "cluster = TFCluster.run(sc, main_fun, args, args.cluster_size, num_ps=0, tensorboard=args.tensorboard, input_mode=TFCluster.InputMode.TENSORFLOW, log_dir=args.model_dir, master_node='chief', eval_node=True)",
            "-  cluster.shutdown(grace_secs=120)",
            "+  cluster.shutdown(grace_secs=60)"
        ]
    },
    {
        "number": 436,
        "comments": "update API call for refactor fix",
        "commit_message": "upgrade to pytorch 0.4.1 + make work with python 3.7 (but still 3.6 also) (#1543)\n\n* changes for pytorch 0.4.1\n\n* increase tolerance for srl test\n\n* update versions in setup.py\n\n* add script to check requirements.txt vs setup.py + fix setup.py\n\n* loosen bounds on pytorch version\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class Highway(torch.nn.Module):",
            "# above, too.",
            "nonlinear_part, gate = projected_input.chunk(2, dim=-1)",
            "nonlinear_part = self._activation(nonlinear_part)",
            "-            gate = torch.nn.functional.sigmoid(gate)",
            "+            gate = torch.sigmoid(gate)",
            "current_input = gate * linear_part + (1 - gate) * nonlinear_part",
            "return current_input"
        ]
    },
    {
        "number": 437,
        "comments": "change param for type fix",
        "commit_message": "fix is_tensor\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class Model(object):",
            "\"It should be either Tensor or a list of Tensor.\"",
            ")",
            "for idx in range(len(check_argu)):",
            "-                        if not isinstance(check_argu[idx], tf_ops._TensorLike) or not tf_ops.is_dense_tensor_like(",
            "+                        if not isinstance(check_argu[idx], [tf.Tensor, tf.SparseTensor, tf.Variable]) or not tf_ops.is_dense_tensor_like(",
            "check_argu[idx]):",
            "raise TypeError(",
            "\"The argument `%s` should be either Tensor or a list of Tensor \" % (check_order[co]) +"
        ]
    },
    {
        "number": 438,
        "comments": "doc update",
        "commit_message": "Update vision docstring bool masked pos (#22237)\n\n* Add bool_masked_pos to forward docstrings\n\n* Add note about mask ratio - videomae\n\n* Fix up\n\n* Fix indenting\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class TFData2VecVisionModel(TFData2VecVisionPreTrainedModel):",
            "return_dict: Optional[bool] = None,",
            "training: bool = False,",
            ") -> Union[tuple, TFData2VecVisionModelOutputWithPooling]:",
            "+        r\"\"\"",
            "+        bool_masked_pos (`tf.Tensor` of shape `(batch_size, num_patches)`, *optional*):",
            "+            Boolean masked positions. Indicates which patches are masked (1) and which aren't (0).",
            "+        \"\"\"",
            "outputs = self.data2vec_vision(",
            "pixel_values=pixel_values,",
            "bool_masked_pos=bool_masked_pos,"
        ]
    },
    {
        "number": 439,
        "comments": "format",
        "commit_message": "upgrade to pytorch 0.4.0 (#1126)\n\n* bump pytorch to 0.4 + fix sanitize\n\n* remove check for Variable in block_orthogonal\n\n* rename parameter split_size=\n\n* remove checks for Variable\n\n* fix more tests\n\n* fixes\n\n* get tests to pass\n\n* fix warnings\n\n* get rid of some of the Variables\n\n* more tests passing\n\n* more elimination of variables\n\n* finish removing all Variables\n\n* pylint and such\n\n* a few fixes\n\n* move torch.no_grad into model.forward_on_instances\n\n* more pytorch 0.4 changes\n\n* detach() -> data\n\n* pylint\n\n* fix bad tensor creation\n\n* fix types\n\n* remove print statement\n\n* more 0.4 goodness\n\n* factor out is_tensor\n\n* add no_grad to elmo command\n\n* cleanup\n\n* remove TODO\n\n* address PR feedback\n\n* replace all() with item()\n\n* more cleanup\n\n* further cleanup\n\n* remove Variable\n\n* really fix merge conflict\n\n* fix pylint\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class TestBasicTextFieldEmbedder(AllenNlpTestCase):",
            "})",
            "token_embedder = BasicTextFieldEmbedder.from_params(self.vocab, params)",
            "inputs = {",
            "-                'words': Variable(torch.rand(3, 4, 5, 6) * 20).long(),",
            "-                'characters': Variable(torch.rand(3, 4, 5, 6, 7) * 15).long(),",
            "+                'words': (torch.rand(3, 4, 5, 6) * 20).long(),",
            "+                'characters': (torch.rand(3, 4, 5, 6, 7) * 15).long(),",
            "}",
            "assert token_embedder(inputs, num_wrapping_dims=2).size() == (3, 4, 5, 6, 12)"
        ]
    },
    {
        "number": 440,
        "comments": "format",
        "commit_message": "upgrade to pytorch 0.4.0 (#1126)\n\n* bump pytorch to 0.4 + fix sanitize\n\n* remove check for Variable in block_orthogonal\n\n* rename parameter split_size=\n\n* remove checks for Variable\n\n* fix more tests\n\n* fixes\n\n* get tests to pass\n\n* fix warnings\n\n* get rid of some of the Variables\n\n* more tests passing\n\n* more elimination of variables\n\n* finish removing all Variables\n\n* pylint and such\n\n* a few fixes\n\n* move torch.no_grad into model.forward_on_instances\n\n* more pytorch 0.4 changes\n\n* detach() -> data\n\n* pylint\n\n* fix bad tensor creation\n\n* fix types\n\n* remove print statement\n\n* more 0.4 goodness\n\n* factor out is_tensor\n\n* add no_grad to elmo command\n\n* cleanup\n\n* remove TODO\n\n* address PR feedback\n\n* replace all() with item()\n\n* more cleanup\n\n* further cleanup\n\n* remove Variable\n\n* really fix merge conflict\n\n* fix pylint\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "class Metric(Registrable):",
            "return cls.by_name(metric_type)(**params.as_dict())  # type: ignore",
            "",
            "@staticmethod",
            "-    def unwrap_to_tensors(*tensors):",
            "+    def unwrap_to_tensors(*tensors: torch.Tensor):",
            "\"\"\"",
            "-        If you actually passed in Variables to a Metric instead of Tensors, there will be",
            "+        If you actually passed gradient-tracking Tensors to a Metric, there will be",
            "a huge memory leak, because it will prevent garbage collection for the computation",
            "graph. This method ensures that you're using tensors directly and that they are on",
            "the CPU.",
            "\"\"\"",
            "-        return (x.data.cpu() if isinstance(x, torch.autograd.Variable) else x for x in tensors)",
            "+        return (x.detach().cpu() if isinstance(x, torch.Tensor) else x for x in tensors)"
        ]
    },
    {
        "number": 442,
        "comments": "no API",
        "commit_message": "1. Implementation of local volatility model.\n2. Bug fix in implied volatility calculation with non-trivial discount factors.\n\nPiperOrigin-RevId: 343091512\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "def _make_black_objective_and_vega_func(prices, forwards, strikes, expiries,",
            "implied_prices = tf.where(",
            "tf.broadcast_to(is_call_options, tf.shape(put_prices)),",
            "implied_prices, put_prices)",
            "-    vega = x * phi.prob(d1) * sqrt_t",
            "+    vega = x * phi.prob(d1) * sqrt_t / discount_factors",
            "return implied_prices - normalized_prices, vega",
            "",
            "return _black_objective_and_vega"
        ]
    },
    {
        "number": 443,
        "comments": "format",
        "commit_message": "Adds a dataset that can be read and written lazily (#5344)\n\n* Adds a dataset that can be read and written lazily\n\nThis does not work yet. I'm still working on supporting classes.\n\n* This approach might work better.\n\n* Make ShuffledSequence take indices\n\n* Formatting\n\n* Adds failing test\n\n* Fix sparse sequence tests\n\n* Fixes the Sqlite format\n\n* Quality-of-life hack\n\n* Makes an internal string less alarming\n\n* Save the files to the right place\n\n* Formatting\n\n* Fix for SqliteDatasetFormat\n\n* Performance improvement for SqliteSparseSequence\n\n* Changelog\n\n* Global imports\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class IndexField(Field[torch.Tensor]):",
            "",
            "@overrides",
            "def get_padding_lengths(self) -> Dict[str, int]:",
            "-",
            "return {}",
            "",
            "@overrides",
            "def as_tensor(self, padding_lengths: Dict[str, int]) -> torch.Tensor:",
            "-",
            "-        tensor = torch.LongTensor([self.sequence_index])",
            "-        return tensor",
            "+        return torch.LongTensor([self.sequence_index])",
            "",
            "@overrides",
            "def empty_field(self):"
        ]
    },
    {
        "number": 444,
        "comments": "format",
        "commit_message": "bugfix\n\nSummary: It seemed that even though the chamfer diff was rebased on top of the knn autograd diff, some of the final updates did not get applied. I'm really surprised that the sandcastle tests did not fail and prevent the diff from landing.\n\nReviewed By: gkioxari\n\nDifferential Revision: D21066156\n\nfbshipit-source-id: 5216efe95180c1b6082d0bac404fa1920cfb7b02\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "def chamfer_distance(",
            "",
            "if return_normals:",
            "# Gather the normals using the indices and keep only value for k=0",
            "-        x_normals_near = knn_gather(y_normals, x_idx, y_lengths)[..., 0, :]",
            "-        y_normals_near = knn_gather(x_normals, y_idx, x_lengths)[..., 0, :]",
            "+        x_normals_near = knn_gather(y_normals, x_nn.idx, y_lengths)[..., 0, :]",
            "+        y_normals_near = knn_gather(x_normals, y_nn.idx, x_lengths)[..., 0, :]",
            "",
            "cham_norm_x = 1 - torch.abs(",
            "F.cosine_similarity(x_normals, x_normals_near, dim=2, eps=1e-6)"
        ]
    },
    {
        "number": 445,
        "comments": "format",
        "commit_message": "define and apply YAPF formatting  (#1039)\n\n* define YAPF\n\n* yapf kornia -rip\n\n* yapf examples -rip\n\n* yapf test -rip\n\n* Update CONTRIBUTING.rst\n\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\n\n* update config\n\n* fix 4lint\n\n* mypy\n\n* mypy\n\n* mypy\n\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class SOSNet(nn.Module):",
            "nn.Conv2d(128, 128, kernel_size=8, bias=False),",
            "nn.BatchNorm2d(128, affine=False),",
            ")",
            "-        self.desc_norm = nn.Sequential(",
            "-            nn.LocalResponseNorm(256, alpha=256.0, beta=0.5, k=0.0)",
            "-        )",
            "+        self.desc_norm = nn.Sequential(nn.LocalResponseNorm(256, alpha=256.0, beta=0.5, k=0.0))",
            "# load pretrained model",
            "if pretrained:",
            "-            pretrained_dict = torch.hub.load_state_dict_from_url(",
            "-                urls['lib'], map_location=lambda storage, loc: storage",
            "-            )",
            "+            pretrained_dict = torch.hub.load_state_dict_from_url(urls['lib'], map_location=lambda storage, loc: storage)",
            "self.load_state_dict(pretrained_dict, strict=True)",
            "",
            "return"
        ]
    },
    {
        "number": 446,
        "comments": "add API call for type fix",
        "commit_message": "Unpack `dl_manager.iter_files` to allow parallization (#4625)\n\n* Unpack `dl_manager.iter_files` to allow parallization\n\n* Fix _generate_tables\n\n* Fix remaining tests\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class Csv(datasets.ArrowBasedBuilder):",
            "if schema is not None",
            "else None",
            ")",
            "-        for file_idx, file in enumerate(files):",
            "+        for file_idx, file in enumerate(itertools.chain.from_iterable(files)):",
            "csv_file_reader = pd.read_csv(file, iterator=True, dtype=dtype, **self.config.read_csv_kwargs)",
            "try:",
            "for batch_idx, df in enumerate(csv_file_reader):"
        ]
    },
    {
        "number": 447,
        "comments": "format",
        "commit_message": "Doc styler examples (#14953)\n\n* Fix bad examples\n\n* Add black formatting to style_doc\n\n* Use first nonempty line\n\n* Put it at the right place\n\n* Don't add spaces to empty lines\n\n* Better templates\n\n* Deal with triple quotes in docstrings\n\n* Result of style_doc\n\n* Enable mdx treatment and fix code examples in MDXs\n\n* Result of doc styler on doc source files\n\n* Last fixes\n\n* Break copy from\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class ElectraForPreTraining(ElectraPreTrainedModel):",
            ">>> from transformers import ElectraTokenizer, ElectraForPreTraining",
            ">>> import torch",
            "",
            "-        >>> tokenizer = ElectraTokenizer.from_pretrained('google/electra-small-discriminator')",
            "-        >>> model = ElectraForPreTraining.from_pretrained('google/electra-small-discriminator')",
            "+        >>> tokenizer = ElectraTokenizer.from_pretrained(\"google/electra-small-discriminator\")",
            "+        >>> model = ElectraForPreTraining.from_pretrained(\"google/electra-small-discriminator\")",
            "",
            "-        >>> input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1",
            "+        >>> input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(",
            "+        ...     0",
            "+        >>> )  # Batch size 1",
            ">>> logits = model(input_ids).logits",
            "```\"\"\"",
            "return_dict = return_dict if return_dict is not None else self.config.use_return_dict"
        ]
    },
    {
        "number": 448,
        "comments": "add condition check for null fix",
        "commit_message": "fix arma conv\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class ARMAConv(MessagePassing):",
            "if self.bias is not None:",
            "out += self.bias[0 if self.shared_weights else t]",
            "",
            "-            if t < self.num_layers - 1:",
            "+            if self.act is not None and t < self.num_layers - 1:",
            "out = self.act(out)",
            "",
            "return out.mean(dim=-3)"
        ]
    },
    {
        "number": 449,
        "comments": "add API call for type fix",
        "commit_message": "fix typing in dependency parser model\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class DependencyParser(flair.nn.Model):",
            "sentence_tensor = self.word_dropout(sentence_tensor)",
            "",
            "if self.use_rnn:",
            "-            sentence_tensor = pack_padded_sequence(sentence_tensor, lengths, True, False)",
            "+            sentence_sequence = pack_padded_sequence(sentence_tensor, torch.IntTensor(lengths), True, False)",
            "",
            "-            sentence_tensor, _ = self.lstm(sentence_tensor)",
            "-            sentence_tensor, _ = pad_packed_sequence(sentence_tensor, True, total_length=seq_len)",
            "+            sentence_sequence, _ = self.lstm(sentence_sequence)",
            "+            sentence_tensor, _ = pad_packed_sequence(sentence_sequence, True, total_length=seq_len)",
            "",
            "# apply MLPs for arc and relations to the BiLSTM output states",
            "arc_h = self.mlp_arc_h(sentence_tensor)"
        ]
    },
    {
        "number": 450,
        "comments": "rename version fix",
        "commit_message": "fix pickle\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "if __name__ == '__main__':",
            "saver = tf.train.Saver()",
            "",
            "try:",
            "-        ckpt = tf.train.get_checkpoint_state(\"checkpoints/\")",
            "+        ckpt = tf.train.get_checkpoint_state(cfg.TEST.checkpoints_path)",
            "+        #ckpt=tf.train.get_checkpoint_state(\"output/ctpn_end2end/voc_2007_trainval/\")",
            "print('Restoring from {}...'.format(ckpt.model_checkpoint_path), end=' ')",
            "saver.restore(sess, ckpt.model_checkpoint_path)",
            "print('done')"
        ]
    },
    {
        "number": 451,
        "comments": "add API call for type fix",
        "commit_message": "Fix linspace error\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def testtanh():",
            "",
            "Ptensor = PolynomialTensor()",
            "",
            "-    x = torch.linspace(-3, 3, steps=10)",
            "+    x = torch.tensor(np.linspace(-3, 3, 10))",
            "expected = torch.tensor(",
            "[",
            "-3.3883e02,"
        ]
    },
    {
        "number": 452,
        "comments": "add param for resource fix",
        "commit_message": "[ci] fix 3 remaining slow GPU failures (#4584)\n\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class BartTranslationTests(unittest.TestCase):",
            "with torch.no_grad():",
            "logits, *other_stuff = model(**self.net_input)",
            "",
            "-        expected_slice = torch.tensor([9.0078, 10.1113, 14.4787])",
            "+        expected_slice = torch.tensor([9.0078, 10.1113, 14.4787], device=torch_device)",
            "result_slice = logits[0][0][:3]",
            "self.assertTrue(torch.allclose(expected_slice, result_slice, atol=TOLERANCE))"
        ]
    },
    {
        "number": 453,
        "comments": "update API call for version fix",
        "commit_message": "Fix unit test errors in test_dc_crn_separator.py\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def test_dc_crn_separator_invalid_type():",
            "def test_dc_crn_separator_output():",
            "real = torch.rand(2, 10, 17)",
            "imag = torch.rand(2, 10, 17)",
            "-    x = ComplexTensor(real, imag) if is_torch_1_9_plus else torch.complex(real, imag)",
            "+    x = torch.complex(real, imag) if is_torch_1_9_plus else ComplexTensor(real, imag)",
            "x_lens = torch.tensor([10, 8], dtype=torch.long)",
            "",
            "for num_spk in range(1, 3):"
        ]
    },
    {
        "number": 454,
        "comments": "rename",
        "commit_message": "Fix Pylint issues\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class ModelSavingTests(unittest.TestCase):",
            "model = T.nn.DataParallel(layer)",
            "",
            "# save the model",
            "-        best_loss = save_best_model(model, None, 0, 100, OUT_PATH, 10, 1)",
            "+        save_best_model(model, None, 0, 100, OUT_PATH, 10, 1)",
            "",
            "# load the model to CPU",
            "-        model_dict = torch.load(",
            "+        model_dict = T.load(",
            "MODEL_PATH, map_location=lambda storage, loc: storage)",
            "model.load_state_dict(model_dict['model'])"
        ]
    },
    {
        "number": 456,
        "comments": "doc update",
        "commit_message": "Fix mypy issues\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class DatasetRequestAPI(RequestAPI):",
            "super().create(**kwargs)",
            "",
            "def create_grid_ui(self, path: str, **kwargs) -> Dict[str, str]:  # type: ignore",
            "-        response = self.node.conn.send_files(",
            "+        response = self.node.conn.send_files( # type: ignore",
            "\"/datasets\", path, form_name=\"metadata\", form_values=kwargs",
            ")  # type: ignore",
            "logging.info(response[RequestAPIFields.MESSAGE])"
        ]
    },
    {
        "number": 457,
        "comments": "update API call for refactor fix",
        "commit_message": "upgrade detection / some v2 compat fix\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def multilevel_roi_align(features, rcnn_boxes, resolution):",
            "all_rois = tf.concat(all_rois, axis=0)  # NCHW",
            "# Unshuffle to the original order, to match the original samples",
            "level_id_perm = tf.concat(level_ids, axis=0)  # A permutation of 1~N",
            "-    level_id_invert_perm = tf.invert_permutation(level_id_perm)",
            "+    level_id_invert_perm = tf.math.invert_permutation(level_id_perm)",
            "all_rois = tf.gather(all_rois, level_id_invert_perm, name=\"output\")",
            "return all_rois"
        ]
    },
    {
        "number": 458,
        "comments": "change param for math fix",
        "commit_message": "fix bug in temperature & async\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def SoftMax(x, use_temperature=False, temperature_init=1.0):",
            ":param x: a 2D tensor",
            "\"\"\"",
            "if use_temperature:",
            "-        t = tf.get_variable('temp', [1],",
            "+        t = tf.get_variable('invtemp', [],",
            "initializer=tf.constant_initializer(1.0 / float(temperature_init)))",
            "x = x * t",
            "return tf.nn.softmax(x, name='output')"
        ]
    },
    {
        "number": 459,
        "comments": "add API call for type fix",
        "commit_message": "Potential deconv model saving fix? (#4999)\n\nAdding this cast to a tuple seems to fix this issue.\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def _preprocess_deconv_output_shape(x, shape, dim_ordering):",
            "shape = (shape[0], shape[2], shape[3], shape[1])",
            "",
            "if shape[0] is None:",
            "-        shape = (tf.shape(x)[0], ) + shape[1:]",
            "+        shape = (tf.shape(x)[0], ) + tuple(shape[1:])",
            "return shape"
        ]
    },
    {
        "number": 460,
        "comments": "test fix",
        "commit_message": "Fix doctests for `DeiT` and `TFGroupViT` (#19466)\n\n* Fix some doctests\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "class TFGroupViTModel(TFGroupViTPreTrainedModel):",
            "",
            ">>> outputs = model(**inputs)",
            ">>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score",
            "-        >>> probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities",
            "+        >>> probs = tf.math.softmax(logits_per_image, axis=1)  # we can take the softmax to get the label probabilities",
            "```\"\"\"",
            "",
            "outputs = self.groupvit("
        ]
    },
    {
        "number": 461,
        "comments": "custom API",
        "commit_message": "Option to provide seed to random generators to ensure reproducibility (#1572)\n\n* Option to provide seed to random generators to ensure reproducibility\n\nI added small function in utilities which imports torch, numpy, python\nrandom and sets seed for all of the libraries to ensure reproducibility\nof results.\n\n* Apply recommendations from core contributors on seeding\n\n1. Moved the seeding code to another file\n2. Make deterministic as a parameter for trainer class\n3. Add assertions for seeding numpy\n4. Added warnings\n5. torch.manual_seed should be enough for seeding torch\n\n* Revert \"Apply recommendations from core contributors on seeding\"\n\nThis reverts commit a213c8e6882eec8a9e7408b9418926d2db7c5461.\n\n* Revert \"Revert \"Apply recommendations from core contributors on seeding\"\"\n\nThis reverts commit 59b2da53c62878de7aab0aa3feb3115e105eea06.\n\n* Change in test, for correct seeding\n\n* Allow seed equal to 0\n\n* Allow seed to be uint32.max\n\n* Added deterministic to benchmarks\n\n* Cuda manual seed as in benchmark seeding\n\n* Seeding should be done before model initialization\n\n* cuda manual_seed is not necessary\n\n* Fixing seed test_cpu_lbfgs\n\nOn some seeds seems like lbfgs doesn't converge.\nSo I fixed the seed during testing.\n\n* rebasing issue with old reproducibility.py\n\n* Improved documentation and ability to seed before initializing Train\nclass\n\n* Change in docs\n\n* Removed seed from trainer, update for documentation\n\n* Typo in the docs\n\n* Added seed_everything to _all_\n\n* Fixing old changes\n\n* Model initialization should be earlier then Trainer\n\n* Update pytorch_lightning/trainer/__init__.py\n\nFrom Example to testcode\n\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\n\n* Fixing according to the contributors suggestions\n\n* Moving horovod deterministic to Trainer class\n\n* deterministic flag affects horovod docs update\n\n* Improved static typing\n\n* Added deterministic to test runners of horovod\n\nIt is failing on some versions, not very predictable\n\n* static seeds for horovod tests\n\n* Change for reset_seed function in tests\n\n* Seeding horovod using reset_seed from tutils\n\n* Update pytorch_lightning/trainer/__init__.py\n\n* chlog\n\n* Update trainer.py\n\n* change \"testcode\" to \"Example\" in trainer init documentation\n\n* Update pytorch_lightning/trainer/seed.py, first line in comment\n\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\n\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\nCo-authored-by: Jirka <jirka.borovec@seznam.cz>\nCo-authored-by: William Falcon <waf2107@columbia.edu>\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "def lightning_loop(MODEL, num_runs=10, num_epochs=10):",
            "",
            "# set seed",
            "seed = i",
            "-        _set_seed(seed)",
            "+        seed_everything(seed)",
            "",
            "-        # init model parts",
            "model = MODEL()",
            "+        # init model parts",
            "trainer = Trainer(",
            "max_epochs=num_epochs,",
            "progress_bar_refresh_rate=0,",
            "weights_summary=None,",
            "gpus=1,",
            "early_stop_callback=False,",
            "-            checkpoint_callback=False",
            "+            checkpoint_callback=False,",
            "+            deterministic=True,",
            ")",
            "trainer.fit(model)"
        ]
    },
    {
        "number": 462,
        "comments": "change condition check for null fix",
        "commit_message": "rgcn bugfix\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class RGCNConv(MessagePassing):",
            "return out if edge_norm is None else out * edge_norm.view(-1, 1)",
            "",
            "def update(self, aggr_out, x):",
            "-        if x.dtype == torch.long:",
            "+        if x is None:",
            "out = aggr_out + self.root",
            "else:",
            "out = aggr_out + torch.matmul(x, self.root)"
        ]
    },
    {
        "number": 467,
        "comments": "format",
        "commit_message": "Saver/summaries/distributed handling updated to using MonitoredSession, hooks, etc; custom save problems fixed; added saver/summary/distributed_spec config entries; added batched_observe config entry to address performance problem; modified episode/timestep counter handling in runner; various other and related fixes\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class Baseline(object):",
            "self.all_variables[name] = variable",
            "if kwargs.get('trainable', True) and not name.startswith('optimization'):",
            "self.variables[name] = variable",
            "-                    if 'variables' in self.summary_labels:",
            "-                        summary = tf.summary.histogram(name=name, values=variable)",
            "-                        self.summaries.append(summary)",
            "+                        if 'variables' in self.summary_labels:",
            "+                            summary = tf.summary.histogram(name=name, values=variable)",
            "+                            self.summaries.append(summary)",
            "return variable",
            "",
            "self.predict = tf.make_template("
        ]
    },
    {
        "number": 469,
        "comments": "format",
        "commit_message": "Convert docstrings to active tense (#1275)\n\n* Convert docstrings to active tense\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def load_depth(file_name):",
            "",
            "",
            "def load_camera_data(file_name):",
            "-    \"\"\"Loads the camera data using the sintel SDK and converts to torch.Tensor.\"\"\"",
            "+    \"\"\"Load the camera data using the sintel SDK and converts to torch.Tensor.\"\"\"",
            "if not os.path.isfile(file_name):",
            "raise AssertionError(f\"Invalid file {file_name}\")",
            "import sintel_io"
        ]
    },
    {
        "number": 470,
        "comments": "change API call for math fix",
        "commit_message": "Merge PyG master (#48)\n\n* avoid the 'inf'\n\n* Create GATv2Conv\n\n* Update gatv2_conv.py\n\n* Update gatv2_conv.py\n\n* Update gatv2_conv.py\n\n* More doc\n\n* Update gatv2_conv.py\n\n* Update gatv2_conv.py\n\n* Update README.md\n\n* Update README.md\n\n* Create test_gatv2_conv.py\n\n* Update test_gatv2_conv.py\n\n* fixed gatv2 test\n\n* Fixed types\n\n* Update gatv2_conv.py\n\n* fix types\n\n* remove script folder\n\n* update test CI\n\n* fixed comments\n\n* lint + type\n\n* lint\n\n* Update test_gatv2_conv.py\n\n* Update test_gatv2_conv.py\n\n* fixed gatv2 test+ types\n\n* Update gatv2_conv.py\n\n* Update gatv2_conv.py\n\n* pytorch 1.9.0 support\n\n* typo\n\n* The dataset was introduced in the MUSAE paper\n\nDear Matthias,\n\nThese datasets were introduced in the Multi-scale Attributed Node Embedding paper.\n\nhttps://arxiv.org/abs/1909.13021\n\nBenedek\n\n* Github Dataset\n\n* Github Dataset\n\n* Github Dataset\n\n* Revert \"Merge branch 'master' into master\"\n\nThis reverts commit ef38f142465f736692c7c251a315ada287d7f104, reversing\nchanges made to d86de00a98173653a6158fc40238d34d0fb57cc1.\n\n* clean up\n\n* fix doc\n\n* fix gnn explainer\n\n* remove OGB-LSC\n\nCo-authored-by: Ethanzjp <13810907+Ethanzjp@users.noreply.github.com>\nCo-authored-by: shakedbr <shakedbr@campus.technion.ac.il>\nCo-authored-by: Uri Alon <urialon1@gmail.com>\nCo-authored-by: Shaked Brody <shakedbr@gmail.com>\nCo-authored-by: Benedek Rozemberczki <benedek.rozemberczki@gmail.com>\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class DenseGCNConv(torch.nn.Module):",
            "idx = torch.arange(N, dtype=torch.long, device=adj.device)",
            "adj[:, idx, idx] = 1 if not self.improved else 2",
            "",
            "-        out = self.lin(x)",
            "+        out = torch.matmul(x, self.weight)",
            "deg_inv_sqrt = adj.sum(dim=-1).clamp(min=1).pow(-0.5)",
            "",
            "adj = deg_inv_sqrt.unsqueeze(-1) * adj * deg_inv_sqrt.unsqueeze(-2)"
        ]
    },
    {
        "number": 471,
        "comments": "format",
        "commit_message": "Black preview (#17217)\n\n* Black preview\n\n* Fixup too!\n\n* Fix check copies\n\n* Use the same version as the CI\n\n* Bump black\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class TFConvNextModelTest(TFModelTesterMixin, unittest.TestCase):",
            "else:",
            "self.assertTrue(",
            "all(tf.equal(tuple_object, dict_object)),",
            "-                        msg=f\"Tuple and dict output are not equal. Difference: {tf.math.reduce_max(tf.abs(tuple_object - dict_object))}\",",
            "+                        msg=(",
            "+                            \"Tuple and dict output are not equal. Difference:\"",
            "+                            f\" {tf.math.reduce_max(tf.abs(tuple_object - dict_object))}\"",
            "+                        ),",
            ")",
            "",
            "recursive_check(tuple_output, dict_output)"
        ]
    },
    {
        "number": 472,
        "comments": "add condition check for state fix",
        "commit_message": "Fix half of the tests that are failing in v1.\n\nMost of them are failing since the actual code are expected to run only in v2 (eg need eager/resource variable, or certain fix we added is only applied to the v2 code path).\n\nPiperOrigin-RevId: 394765626\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class GradientsTest(tf.test.TestCase):",
            "",
            "",
            "if __name__ == \"__main__\":",
            "-  tf.test.main()",
            "+  if tf.__internal__.tf2.enabled():",
            "+    tf.test.main()"
        ]
    },
    {
        "number": 473,
        "comments": "update API call for version fix",
        "commit_message": "fix deprecations about casting & initializers in tf1.13\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class Model(ModelDesc):",
            "cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)",
            "cost = tf.reduce_mean(cost, name='cross_entropy_loss')",
            "",
            "-        acc = tf.to_float(tf.nn.in_top_k(logits, label, 1))",
            "+        acc = tf.cast(tf.nn.in_top_k(logits, label, 1), tf.float32)",
            "",
            "acc = tf.reduce_mean(acc, name='accuracy')",
            "summary.add_moving_summary(acc)"
        ]
    },
    {
        "number": 475,
        "comments": "format",
        "commit_message": "fix moe override defaults\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "def block(params, scope, layer_num, bias, sequence_dim, memory_length_dim, varia",
            "if use_moe:",
            "moe_params = mtf.transformer.moe.HParams()",
            "mtf.transformer.moe.set_default_moe_hparams(moe_params)",
            "+",
            "+                # override defaults",
            "for k, v in params[\"moe_params\"].items():",
            "moe_params.add_hparam(k, v)",
            "-                mtf.transformer.moe.set_default_moe_hparams(moe_params)",
            "+",
            "moe_train = params[\"mode\"] == \"train\"",
            "",
            "m, aux_loss = mtf.transformer.moe.transformer_moe_layer_v1(res_x, x.shape[-1], moe_params,"
        ]
    },
    {
        "number": 476,
        "comments": "update API call for version fix",
        "commit_message": "Fix tests of mixed precision now that experimental is deprecated (#17300)\n\n* Fix tests of mixed precision now that experimental is deprecated\n\n* Fix mixed precision in training_args_tf.py too\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class TFCoreModelTesterMixin:",
            "",
            "self.assertIsNotNone(outputs)",
            "",
            "-        tf.keras.mixed_precision.experimental.set_policy(\"float32\")",
            "+        tf.keras.mixed_precision.set_global_policy(\"float32\")",
            "",
            "@slow",
            "def test_train_pipeline_custom_model(self):"
        ]
    },
    {
        "number": 477,
        "comments": "rename",
        "commit_message": "Fix various failing tests in v1. Most of them are failing because of the slight different behavior between v1 and v2. Some of them are only targeting to work with v2 behavior.\n\nPiperOrigin-RevId: 395026999\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class KerasCallbacksTest(keras_parameterized.TestCase):",
            "1, activation='sigmoid'),))",
            "model.compile(",
            "optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])",
            "-    expected_log = r'(.*- loss:.*- accuracy:.*epoch)+'",
            "+    expected_log = r'(.*- loss:.*- acc.*:.*epoch)+'",
            "with self.captureWritesToStream(sys.stdout) as printed:",
            "model.fit(data, labels, verbose=2, epochs=20)",
            "self.assertRegex(printed.contents(), expected_log)"
        ]
    },
    {
        "number": 478,
        "comments": "test",
        "commit_message": "Fix tests\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class Wavernn(BaseVocoder):",
            "f\"test_{idx}/prediction\": plot_spectrogram(x_hat.T),",
            "}",
            ")",
            "-            audios.update({f\"test_{idx}/audio\", y_hat})",
            "+            audios.update({f\"test_{idx}/audio\": y_hat})",
            "return figures, audios",
            "",
            "@staticmethod"
        ]
    },
    {
        "number": 479,
        "comments": "add API call for type fix",
        "commit_message": "Add test to ensure models can take int64 inputs (#17210)\n\n* Add test to ensure models can take int64 inputs\n\n* is_integer is an attribute, not a method\n\n* Fix test when some inputs aren't tensors\n\n* Add casts to blenderbot and blenderbot-small\n\n* Add casts to the other failing models\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class TFMBartForConditionalGeneration(TFMBartPreTrainedModel, TFCausalLanguageMo",
            "if labels is not None:",
            "labels = tf.where(",
            "labels == self.config.pad_token_id,",
            "-                tf.fill(shape_list(labels), -100),",
            "+                tf.cast(tf.fill(shape_list(labels), -100), labels.dtype),",
            "labels,",
            ")",
            "use_cache = False"
        ]
    },
    {
        "number": 480,
        "comments": "no API",
        "commit_message": "[WIP] Test TF Flaubert + Add {XLM, Flaubert}{TokenClassification, MultipleC\u2026 (#5614)\n\n* Test TF Flaubert + Add {XLM, Flaubert}{TokenClassification, MultipleChoice} models and tests\n\n* AutoModels\n\n\nTiny tweaks\n\n* Style\n\n* Final changes before merge\n\n* Re-order for simpler review\n\n* Final fixes\n\n* Addressing @sgugger's comments\n\n* Test MultipleChoice\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class TFModelTesterMixin:",
            "def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):",
            "if model_class in TF_MODEL_FOR_MULTIPLE_CHOICE_MAPPING.values():",
            "inputs_dict = {",
            "-                k: tf.tile(tf.expand_dims(v, 1), (1, self.model_tester.num_choices, 1))",
            "-                if isinstance(v, tf.Tensor) and v.ndim != 0",
            "+                k: tf.tile(tf.expand_dims(v, 1), (1, self.model_tester.num_choices) + (1,) * (v.ndim - 1))",
            "+                if isinstance(v, tf.Tensor) and v.ndim > 0",
            "else v",
            "for k, v in inputs_dict.items()",
            "}"
        ]
    },
    {
        "number": 481,
        "comments": "add condition check for resource fix",
        "commit_message": "fix n_gpu count when no_cuda flag is activated (#3077)\n\n* fix n_gpu count when no_cuda flag is activated\n\n* someone was left behind\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def main():",
            "# Setup CUDA, GPU & distributed training",
            "if args.local_rank == -1 or args.no_cuda:",
            "device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")",
            "-        args.n_gpu = torch.cuda.device_count()",
            "+        args.n_gpu = 0 if args.no_cuda else torch.cuda.device_count()",
            "else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs",
            "torch.cuda.set_device(args.local_rank)",
            "device = torch.device(\"cuda\", args.local_rank)"
        ]
    },
    {
        "number": 482,
        "comments": "add API call for type fix",
        "commit_message": "Unpack `dl_manager.iter_files` to allow parallization (#4625)\n\n* Unpack `dl_manager.iter_files` to allow parallization\n\n* Fix _generate_tables\n\n* Fix remaining tests\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class Pandas(datasets.ArrowBasedBuilder):",
            "return pa_table",
            "",
            "def _generate_tables(self, files):",
            "-        for i, file in enumerate(files):",
            "+        for i, file in enumerate(itertools.chain.from_iterable(files)):",
            "with open(file, \"rb\") as f:",
            "pa_table = pa.Table.from_pandas(pd.read_pickle(f))",
            "yield i, self._cast_table(pa_table)"
        ]
    },
    {
        "number": 483,
        "comments": "add API call for type fix",
        "commit_message": "fix mypy errors\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class DiceLoss(nn.Module):",
            "cardinality = torch.sum(input_soft + target_one_hot, dims)",
            "",
            "dice_score = 2. * intersection / (cardinality + self.eps)",
            "-        return torch.mean(1. - dice_score)",
            "+        return torch.mean(torch.tensor(1.) - dice_score)",
            "",
            "",
            "######################"
        ]
    },
    {
        "number": 486,
        "comments": "format",
        "commit_message": "for discussion: incorporate black code formatter (#3308)\n\n* setup files\n\n* run black\n\n* undo\n\n* update CONTRIBUTING.md\n\n* fix quotes in test_other_modules\n\n* make flake8 happy\n\n* set black to 100 characters per line\n\n* move type: ignore to where mypy wants them\n\n* more flake8\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "from allennlp.training.metrics import ConllCorefScores",
            "class ConllCorefScoresTest(AllenNlpTestCase):",
            "def test_get_predicted_clusters(self):",
            "top_spans = torch.Tensor([[0, 1], [4, 6], [8, 9]]).long()",
            "-        antecedent_indices = torch.Tensor([[-1, -1, -1],",
            "-                                           [0, -1, -1],",
            "-                                           [0, 1, -1]]).long()",
            "+        antecedent_indices = torch.Tensor([[-1, -1, -1], [0, -1, -1], [0, 1, -1]]).long()",
            "predicted_antecedents = torch.Tensor([-1, -1, 1]).long()",
            "-        clusters, mention_to_cluster = ConllCorefScores.get_predicted_clusters(top_spans,",
            "-                                                                               antecedent_indices,",
            "-                                                                               predicted_antecedents)",
            "+        clusters, mention_to_cluster = ConllCorefScores.get_predicted_clusters(",
            "+            top_spans, antecedent_indices, predicted_antecedents",
            "+        )",
            "assert len(clusters) == 1",
            "assert set(clusters[0]) == {(4, 6), (8, 9)}",
            "assert mention_to_cluster == {(4, 6): clusters[0], (8, 9): clusters[0]}"
        ]
    },
    {
        "number": 487,
        "comments": "customize API",
        "commit_message": "Fix TF Hub compat.as_str PY3 compatibility issues.\n\nPiperOrigin-RevId: 209449340\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def find_state_op_colocation_error(graph, reported_tags=None):",
            "for op in state_op_map.values():",
            "for colocation_group in op.colocation_groups():",
            "if not (colocation_group.startswith(tf.compat.as_bytes(\"loc:@\")) and",
            "-              tf.compat.as_str(colocation_group[5:]) in state_op_map):",
            "+              tf.compat.as_str_any(colocation_group[5:]) in state_op_map):",
            "tags_prefix = (\"\" if reported_tags is None else",
            "\"in the graph for tags %s, \" % reported_tags)",
            "return ("
        ]
    },
    {
        "number": 488,
        "comments": "add param for state fix",
        "commit_message": "Adds timeout argument to training_args to avoid socket timeouts in DDP (#18562)\n\n* chore(training_args): Adds support for timeout argument.\n\n* fix(training_args): Passes make style through changes.\n\n* fix(training_args): Removes wrong docstring sentence.\n\n* fix(training_args): Fixes timeout not being JSON serializable.\n\n* fix(training_args_sm): Also updates timeout to timeout_delta.\n\n* fix(training_args): Fixes PR according to suggestions.\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class SageMakerTrainingArguments(TrainingArguments):",
            "# Here, we'll use torch.distributed.",
            "# Initializes the distributed backend which will take care of synchronizing nodes/GPUs",
            "if not torch.distributed.is_initialized():",
            "-                torch.distributed.init_process_group(backend=\"nccl\")",
            "+                torch.distributed.init_process_group(backend=\"nccl\", timeout=self.ddp_timeout_delta)",
            "device = torch.device(\"cuda\", self.local_rank)",
            "self._n_gpu = 1"
        ]
    },
    {
        "number": 489,
        "comments": "change param for null fix",
        "commit_message": "tl.layers API Refactoring and various modifications (#667)\n\n* Decorators API Refactored\n\n* extra_requires `all`, `all_cpu` and `all_gpu` added\n\n* Error fix\n\n* YAPF Formating Correction\n\n* Test for private method decorator added\n\n* Test Logging Verbosity Fixed to Debug when runned individually\n\n* YAPF corrections applied\n\n* Changelog Added\n\n* Changelog updated\n\n* PR number changed\n\n* First Refactoring Pass done\n\n* cleaning second pass\n\n* Refactoring 3rd pass\n\n* Refactoring 4th Pass\n\n* Code Error fix\n\n* YAPF Formating Fix\n\n* Arguments now using self\n\n* YAPF error correction\n\n* Bug Fix in Decorator\n\n* act name bug fix\n\n* Error Correction\n\n* YAPF formating fix\n\n* Useless tf.identity removed\n\n* Error Fix\n\n* Changelog Updated\n\n* Error fix in tl.activation\n\n* Documentation error fix\n\n* Lazy Import added\n\n* Import Refactoring with LazyImport when necessary\n\n* Changelog Updated\n\n* Gitter Removed\n\n* Fixed proposed by @zsdonghao\n\n* Documentation updated\n\n* Missing requirements added\n\n* Update to TensorLayer 1.8.6rc1\n\n* Requirements error fix\n\n* Docker Files updated\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def main(_):",
            "# net = tl.layers.ReshapeLayer(net,",
            "#       shape=[-1, int(net.outputs._shape[-1])], name='reshape')",
            "net = tl.layers.DropoutLayer(net, keep=keep_prob, is_fix=True, is_train=is_training, name='drop3')",
            "-            net = tl.layers.DenseLayer(net, vocab_size, W_init=init, b_init=init, act=tf.identity, name='output')",
            "+            net = tl.layers.DenseLayer(net, vocab_size, W_init=init, b_init=init, act=None, name='output')",
            "return net, lstm1, lstm2",
            "",
            "# Inference for Training"
        ]
    },
    {
        "number": 490,
        "comments": "change API raise error",
        "commit_message": "Layer API Refactoring (#692)\n\n* Layer API Refactoring\n\n* Conv Layers refactored\n\n* dense layers refactored\n\n* Error fix in ElementwiseLayer\n\n* Various cleaning\n\n* docstring cleaning\n\n* Documentation print clean\n\n* Changelog updated\n\n* @zsdonghao corrections added\n\n* Import Bug Fix\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "class SlimNetsLayer(Layer):",
            "slim_variables = tf.get_collection(TF_GRAPHKEYS_VARIABLES, scope=self.name)",
            "",
            "if slim_variables == []:",
            "-            logging.error(",
            "-                \"No variables found under %s : the name of SlimNetsLayer should be matched with the begining of the ckpt file, see tutorial_inceptionV3_tfslim.py for more details\"",
            "-                % self.name",
            "+            raise RuntimeError(",
            "+                \"No variables found under %s : the name of SlimNetsLayer should be matched with the begining of the ckpt file.\\n\"",
            "+                \"see tutorial_inceptionV3_tfslim.py for more details\" % self.name",
            ")",
            "",
            "slim_layers = []",
            "",
            "for v in end_points.values():",
            "-            # tf.contrib.layers.summaries.summarize_activation(v)",
            "slim_layers.append(v)",
            "",
            "self._add_layers(slim_layers)"
        ]
    },
    {
        "number": 494,
        "comments": "update param for type fix",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class DecoderLayer(nn.Module):",
            "self.sublayer = nn_util.clone(SublayerConnection(size, dropout), 3)",
            "",
            "def forward(",
            "-        self, x: torch.Tensor, memory: torch.Tensor, src_mask: torch.Tensor, tgt_mask: torch.Tensor",
            "+        self,",
            "+        x: torch.Tensor,",
            "+        memory: torch.Tensor,",
            "+        src_mask: torch.BoolTensor,",
            "+        tgt_mask: torch.BoolTensor,",
            ") -> torch.Tensor:",
            "# Follow Figure 1 (right) for connections.",
            "x = self.sublayer[0](x, lambda y: self.self_attn(y, y, y, tgt_mask))"
        ]
    },
    {
        "number": 495,
        "comments": "no fix",
        "commit_message": "Tree decoding fix (#1606)\n\n* initial fix\n\n* correct approach\n\n* fix and test\n\n* fix predictor test\n\n* fix pylint\n\n* use unique edge weights\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class BiaffineDependencyParser(Model):",
            "head_tags.append(head_tag)",
            "return torch.from_numpy(numpy.stack(heads)), torch.from_numpy(numpy.stack(head_tags))",
            "",
            "-",
            "def _get_head_tags(self,",
            "head_tag_representation: torch.Tensor,",
            "child_tag_representation: torch.Tensor,"
        ]
    },
    {
        "number": 497,
        "comments": "add condition check for resource fix",
        "commit_message": "fix Average metric (#4624)\n\n* fix Average metric\n\n* try spawn\n\n* try again\n\n* oops\n\n* clean up, fix evalb too\n\n* use different start method for GPU vs CPU tests\n\n* add comment\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class EvalbBracketingScorer(Metric):",
            "shutil.rmtree(tempdir)",
            "",
            "if is_distributed():",
            "-            # Setting the device to CPU since this metric is not expected to run on GPUs.",
            "-            device = torch.device(\"cpu\")",
            "+            device = torch.device(\"cuda\" if dist.get_backend() == \"nccl\" else \"cpu\")",
            "correct_predicted_brackets = torch.tensor(_correct_predicted_brackets).to(device)",
            "predicted_brackets = torch.tensor(_predicted_brackets).to(device)",
            "gold_brackets = torch.tensor(_gold_brackets).to(device)"
        ]
    },
    {
        "number": 498,
        "comments": "version fix",
        "commit_message": "formatting fixes for Array API submodule in Torch backend.\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def ones(shape: Union[int, Tuple[int, ...]],",
            "dtype: Optional[torch.dtype] = None,",
            "device: Optional[Union[torch.device, str]] = None) \\",
            "-> torch.Tensor:",
            "-    dtype_val: torch.dtype = ivy.dtype_from_str(dtype)",
            "-    dev = ivy.default_device(device)",
            "-    return torch.ones(shape, dtype=dtype_val, device=ivy.dev_from_str(dev))",
            "+    dtype_val: torch.dtype = dtype_from_str(dtype)",
            "+    dev = default_device(device)",
            "+    return torch.ones(shape, dtype=dtype_val, device=dev_from_str(dev))"
        ]
    },
    {
        "number": 499,
        "comments": "rename",
        "commit_message": "Fix typo of variable names for key and query projection layer (#17155)\n\nself.pos_proj and self.pos_q_proj should be changed to self.pos_key_proj and self.pos_query_proj as same as PyTorch implements.\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class TFDebertaV2DisentangledSelfAttention(tf.keras.layers.Layer):",
            "",
            "if not self.share_att_key:",
            "if \"c2p\" in self.pos_att_type:",
            "-                    self.pos_proj = tf.keras.layers.Dense(",
            "+                    self.pos_key_proj = tf.keras.layers.Dense(",
            "self.all_head_size,",
            "kernel_initializer=get_initializer(config.initializer_range),",
            "name=\"pos_proj\",",
            "use_bias=True,",
            ")",
            "if \"p2c\" in self.pos_att_type:",
            "-                    self.pos_q_proj = tf.keras.layers.Dense(",
            "+                    self.pos_query_proj = tf.keras.layers.Dense(",
            "self.all_head_size,",
            "kernel_initializer=get_initializer(config.initializer_range),",
            "name=\"pos_q_proj\","
        ]
    },
    {
        "number": 500,
        "comments": "format",
        "commit_message": "Add linting with black (#2678)\n\nSummary:\n# Before submitting\n\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\n- [ ] Did you make sure to update the docs?\n- [ ] Did you write any new necessary tests?\n\n## What does this PR do?\nFixes # (issue).\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding \ufffd\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2678\n\nReviewed By: Mortimerp9\n\nDifferential Revision: D32653381\n\nPulled By: dianaml0\n\nfbshipit-source-id: 2810d14867cd7d64f4d340740e2b590b82de47fe\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def quantize_model_(",
            "print(num_assignments)",
            "print(num_extra)",
            "assignments_bins = torch.arange(counts)",
            "-            assignments_rand = torch.randint(0, counts-1, (num_extra, ))",
            "+            assignments_rand = torch.randint(0, counts - 1, (num_extra,))",
            "assignments = torch.cat((assignments_bins, assignments_rand), 0)",
            "# assignments = assignments.type(torch.IntTensor)",
            "assignments.cuda()"
        ]
    },
    {
        "number": 501,
        "comments": "add condition check for null fix",
        "commit_message": "Bug fix for ctc mode:   espnet/nets/pytorch_backend/e2e_asr.py\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class E2E(torch.nn.Module):",
            "# Neither CPUTensor nor float/int value can be used",
            "# because NCCL communicates between GPU devices.",
            "device = next(self.parameters()).device",
            "-        acc = torch.tensor([acc], device=device)",
            "+",
            "+        acc = torch.tensor([acc], device=device) if acc is not None else None",
            "cer = torch.tensor([cer], device=device)",
            "wer = torch.tensor([wer], device=device)",
            "return self.loss, loss_ctc, loss_att, acc, cer, wer"
        ]
    },
    {
        "number": 502,
        "comments": "remove API call for type fix",
        "commit_message": "bug fixes, under construction\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class DeepQNetwork(ValueFunction):",
            "\"\"\"",
            "",
            "# Compute estimated future value",
            "-        float_terminals = tf.to_float(batch['terminals'])",
            "+        float_terminals = batch['terminals'].astype(float)",
            "q_targets = batch['rewards'] + (1. - float_terminals) \\",
            "* self.gamma * self.get_target_values(batch['next_states'])"
        ]
    },
    {
        "number": 503,
        "comments": "format",
        "commit_message": "define and apply YAPF formatting  (#1039)\n\n* define YAPF\n\n* yapf kornia -rip\n\n* yapf examples -rip\n\n* yapf test -rip\n\n* Update CONTRIBUTING.rst\n\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\n\n* update config\n\n* fix 4lint\n\n* mypy\n\n* mypy\n\n* mypy\n\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class BoxBlur(nn.Module):",
            "torch.Size([2, 4, 5, 7])",
            "\"\"\"",
            "",
            "-    def __init__(self, kernel_size: Tuple[int, int],",
            "-                 border_type: str = 'reflect',",
            "-                 normalized: bool = True) -> None:",
            "+    def __init__(self, kernel_size: Tuple[int, int], border_type: str = 'reflect', normalized: bool = True) -> None:",
            "super(BoxBlur, self).__init__()",
            "self.kernel_size: Tuple[int, int] = kernel_size",
            "self.border_type: str = border_type"
        ]
    },
    {
        "number": 505,
        "comments": "add version fix",
        "commit_message": "Update dataset_infos for udhr dataset (#4362)\n\n* Update dataset_infos for UDHN/udhr dataset\n\n* Update languages list, Add versioning\n\n* Reset version; remove stage1+2 languages; add licensing and repo information\n\n* bump dummy version\n\n* Fix language tags\n\n* Add citation to dataset card\n\nCo-authored-by: Albert Villanova del Moral <8515462+albertvillanova@users.noreply.github.com>\n",
        "label": "no",
        "answer": "no",
        "change": [
            "_TXT_DOWNLOAD_URL = \"https://unicode.org/udhr/assemblies/udhr_txt.zip\"",
            "class UDHN(datasets.GeneratorBasedBuilder):",
            "\"\"\"Universal Declaration of Human Rights\"\"\"",
            "",
            "+    VERSION = datasets.Version(\"1.0.0\")",
            "+",
            "def _info(self):",
            "return datasets.DatasetInfo(",
            "description=_DESCRIPTION,"
        ]
    },
    {
        "number": 508,
        "comments": "remove condition check for resource fix",
        "commit_message": "Bug fix\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class DistributedFusedAdam(torch.optim.Optimizer):",
            "grp = torch.distributed.new_group(ranks=ranks)",
            "if torch.distributed.get_rank() in ranks:",
            "self._rs_pg.append(grp)",
            "-            if self._compute_L2_grad_norm and torch.distributed.get_rank() in ranks:",
            "-                self._l2_grad_norm_pg = torch.distributed.new_group(ranks=ranks)",
            "-                torch.distributed.all_reduce(self._overflow_buf,group=self._l2_grad_norm_pg)",
            "+            if self._compute_L2_grad_norm:",
            "+                l2_grad_norm_pg = torch.distributed.new_group(ranks=ranks)",
            "+                if torch.distributed.get_rank() in ranks:",
            "+                    self._l2_grad_norm_pg = l2_grad_norm_pg",
            "+                    torch.distributed.all_reduce(self._overflow_buf,group=self._l2_grad_norm_pg)",
            "self._rs_st = [torch.cuda.Stream() for _ in range(self._num_rs_pg)]",
            "for rs_pg in self._rs_pg:",
            "torch.distributed.all_reduce(self._overflow_buf,group=rs_pg)"
        ]
    },
    {
        "number": 509,
        "comments": "remove old code",
        "commit_message": "[MRG] Simple version Image Pipeline Implemented (#650)\n\n* legacy\n\n* new files\n\n* Update demo.py\n\n* test tunner bug fixed\n\n* classes and funtions in the demo created\n\n* network implemented not tested\n\n* travis to ignore legacy tests\n\n* test connectedHyperparameter\n\n* test fixed\n\n* some basic blocks implemented\n\n* still debuging\n\n* basic test for hypergraph passed\n\n* merge layer implemented\n\n* hyper_graph fully tested\n\n* more args added to auto model\n\n* test fixed\n\n* local changes\n\n* test auto_model\n\n* auto_model fit signature changed for validation data\n\n* super classes extending object\n\n* change n_ to num_\n\n* refactored automodel to extend hypermodel and removed tuner from signature\n\n* rename HyperNode to Node. TextInput added\n\n* rename HyperGraph to GraphAutoModel extending AutoModel\n\n* refactor hyperhead, removed tensor heads\n\n* removed unnecessary blocks, rename build_output to build\n\n* changed some functions and attributes to private\n\n* remove trails from AutoModel public API\n\n* test cases changed accordingly\n\n* import modules instead of objects\n\n* tuner deleted from AutoModel contructor\n\n* change trails to num_trials\n\n* use the same quote sign\n\n* revised auto pipeline docs\n\n* removed compile from AutoModel\n\n* loss and metrics moved to hyper heads\n\n* do not flatten by default in hyperheads\n\n* inputs and outputs down to GraphAutoModel\n\n* changed AutoModel\n\n* name_scope changed to tf 2.0 and moved to hyperparameters and hypermodel\n\n* remove HierarchicalHyperParameters\n\n* renaming the tests\n\n* auto_pipeline\n\n* image module tested\n\n* image regressor tested\n\n* removed some requirements\n\n* update tf to 2.0 beta\n\n* Refactor (#646)\n\n* super classes extending object\n\n* change n_ to num_\n\n* refactored automodel to extend hypermodel and removed tuner from signature\n\n* rename HyperNode to Node. TextInput added\n\n* rename HyperGraph to GraphAutoModel extending AutoModel\n\n* refactor hyperhead, removed tensor heads\n\n* removed unnecessary blocks, rename build_output to build\n\n* changed some functions and attributes to private\n\n* remove trails from AutoModel public API\n\n* test cases changed accordingly\n\n* import modules instead of objects\n\n* tuner deleted from AutoModel contructor\n\n* change trails to num_trials\n\n* use the same quote sign\n\n* revised auto pipeline docs\n\n* removed compile from AutoModel\n\n* loss and metrics moved to hyper heads\n\n* do not flatten by default in hyperheads\n\n* inputs and outputs down to GraphAutoModel\n\n* changed AutoModel\n\n* name_scope changed to tf 2.0 and moved to hyperparameters and hypermodel\n\n* remove HierarchicalHyperParameters\n\n* renaming some variables and make private some members\n\n* demo update\n\n* remove legacy\n\n* test fixed\n\n* changed setup.py\n\n* Update hyper_block.py\n\nImprove the DenseBlock: Add some other layers'category to it.\n\n* Update hyper_block.py\n\n* pep8 style fix\n\n* changed docstrings to use markdown\n\n* renaming some of the variables and classes\n\n* extracted check new and old search space to tuner base class\n\n* fixing some pylint issues\n\n* change the normalization to use mean and stddev only\n\n* dependency\n\n* dependency changed to keras-tuner\n\n* pep8 formatting\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "-import tensorflow as tf",
            "-from autokeras.hyperparameters import HyperParameters",
            "-",
            "-",
            "-def test_hierarchical_hyperparameters():",
            "-    hp = HyperParameters()",
            "-    with tf.name_scope('abc'):",
            "-        hp.Choice('num_layers', [1, 2, 3], default=1)",
            "-    assert 'abc/num_layers' in hp.values"
        ]
    },
    {
        "number": 510,
        "comments": "add condition check for type fix",
        "commit_message": "Remove assert statement from non-test files (#1206)\n\n* Remove assert statement from non-test files\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Autofix issues in 4 files\n\nResolved issues in the following files via DeepSource Autofix:\n1. kornia/augmentation/container/image.py\n2. kornia/augmentation/utils/helpers.py\n3. kornia/augmentation/utils/param_validation.py\n4. kornia/enhance/adjust.py\n\n* fixl linting issues\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Autofix issues in 1 file\n\nResolved issues in kornia/augmentation/container/augment.py via DeepSource Autofix\n\nCo-authored-by: deepsource-autofix[bot] <62050782+deepsource-autofix[bot]@users.noreply.github.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def cartesian_product_of_parameters(**possible_parameters):",
            "",
            "",
            "def default_with_one_parameter_changed(*, default={}, **possible_parameters):",
            "-    assert isinstance(default, dict), f\"default should be a dict not a {type(default)}\"",
            "+    if not isinstance(default, dict):",
            "+        raise AssertionError(f\"default should be a dict not a {type(default)}\")",
            "",
            "for parameter_name, possible_values in possible_parameters.items():",
            "for v in possible_values:"
        ]
    },
    {
        "number": 511,
        "comments": "rename var",
        "commit_message": "center transform, py 2.7 fixes\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "def read_ply_data(path):",
            "pos = ([torch.tensor(data['vertex'][axis]) for axis in ['x', 'y', 'z']])",
            "pos = torch.stack(pos, dim=-1)",
            "",
            "-    face = data['face']['vertex_indices']",
            "-    face = [torch.tensor(f, dtype=torch.long) for f in face]",
            "-    face = torch.stack(face, dim=-1)",
            "+    faces = data['face']['vertex_indices']",
            "+    faces = [torch.tensor(face, dtype=torch.long) for face in faces]",
            "+    face = torch.stack(faces, dim=-1)",
            "",
            "edge_index = face_to_edge_index(face, num_nodes=pos.size(0))"
        ]
    },
    {
        "number": 514,
        "comments": "format",
        "commit_message": "fix `utility` submodule (#1709)\n\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def any(",
            "axis = tuple(range(num_dims))",
            "elif isinstance(axis, list):",
            "axis = tuple(axis)",
            "-    ret = tf.reduce_any(tf.cast(x, tf.bool), axis=axis, keepdims=keepdims)",
            "-    return ret",
            "+    return tf.reduce_any(tf.cast(x, tf.bool), axis=axis, keepdims=keepdims)"
        ]
    },
    {
        "number": 516,
        "comments": "add API call for type fix",
        "commit_message": "Fix `get_embedding` dtype at init. time (#19473)\n\n* cast positions dtype in XGLMModel\n\n* Get the correct dtype at init time\n\n* Get the correct dtype at init time\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class Speech2Text2SinusoidalPositionalEmbedding(nn.Module):",
            "emb = torch.cat([emb, torch.zeros(num_embeddings, 1)], dim=1)",
            "if padding_idx is not None:",
            "emb[padding_idx, :] = 0",
            "-        return emb",
            "+        return emb.to(torch.get_default_dtype())",
            "",
            "@torch.no_grad()",
            "def forward(self, input_ids: torch.Tensor, past_key_values_length: int = 0):"
        ]
    },
    {
        "number": 517,
        "comments": "format",
        "commit_message": "Reinforcement learning fix opt (#999)\n\n* change readme\n\n* Add files via upload\n\n* fix opt and make format\n\n* readme\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class DDPG(object):",
            "with tf.GradientTape() as tape:",
            "a = self.actor(bs)",
            "q = self.critic([bs, a])",
            "-            a_loss = -tf.reduce_mean(q)  # maximize the q",
            "+            a_loss = - tf.reduce_mean(q)  # maximize the q",
            "a_grads = tape.gradient(a_loss, self.actor.trainable_weights)",
            "self.actor_opt.apply_gradients(zip(a_grads, self.actor.trainable_weights))"
        ]
    },
    {
        "number": 518,
        "comments": "add API call for resource fix",
        "commit_message": "MPS schedulers: don't use float64 (#1169)\n\n* Schedulers: don't use float64 on mps\n\n* Test set_timesteps() on device (float schedulers).\n\n* SD pipeline: use device in set_timesteps.\n\n* SD in-painting pipeline: use device in set_timesteps.\n\n* Tests: fix mps crashes.\n\n* Skip test_load_pipeline_from_git on mps.\n\nNot compatible with float16.\n\n* Use device.type instead of str in Euler schedulers.\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class EulerAncestralDiscreteScheduler(SchedulerMixin, ConfigMixin):",
            "",
            "prev_sample = sample + derivative * dt",
            "",
            "-        device = model_output.device if torch.is_tensor(model_output) else \"cpu\"",
            "-        if str(device) == \"mps\":",
            "+        device = model_output.device if torch.is_tensor(model_output) else torch.device(\"cpu\")",
            "+        if device.type == \"mps\":",
            "# randn does not work reproducibly on mps",
            "noise = torch.randn(model_output.shape, dtype=model_output.dtype, device=\"cpu\", generator=generator).to(",
            "device"
        ]
    },
    {
        "number": 519,
        "comments": "add API call for resource fix",
        "commit_message": "Fix AutoRegNN with default CUDA tensor (#1308)\n\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class AutoRegressiveNN(nn.Module):",
            "",
            "if permutation is None:",
            "# By default set a random permutation of variables, which is important for performance with multiple steps",
            "-            self.permutation = torch.randperm(input_dim)",
            "+            self.permutation = torch.randperm(input_dim, device='cpu').to(torch.Tensor().device)",
            "else:",
            "# The permutation is chosen by the user",
            "self.permutation = permutation.type(dtype=torch.int64)"
        ]
    },
    {
        "number": 521,
        "comments": "not clear",
        "commit_message": "[MRG] Simple version Image Pipeline Implemented (#650)\n\n* legacy\n\n* new files\n\n* Update demo.py\n\n* test tunner bug fixed\n\n* classes and funtions in the demo created\n\n* network implemented not tested\n\n* travis to ignore legacy tests\n\n* test connectedHyperparameter\n\n* test fixed\n\n* some basic blocks implemented\n\n* still debuging\n\n* basic test for hypergraph passed\n\n* merge layer implemented\n\n* hyper_graph fully tested\n\n* more args added to auto model\n\n* test fixed\n\n* local changes\n\n* test auto_model\n\n* auto_model fit signature changed for validation data\n\n* super classes extending object\n\n* change n_ to num_\n\n* refactored automodel to extend hypermodel and removed tuner from signature\n\n* rename HyperNode to Node. TextInput added\n\n* rename HyperGraph to GraphAutoModel extending AutoModel\n\n* refactor hyperhead, removed tensor heads\n\n* removed unnecessary blocks, rename build_output to build\n\n* changed some functions and attributes to private\n\n* remove trails from AutoModel public API\n\n* test cases changed accordingly\n\n* import modules instead of objects\n\n* tuner deleted from AutoModel contructor\n\n* change trails to num_trials\n\n* use the same quote sign\n\n* revised auto pipeline docs\n\n* removed compile from AutoModel\n\n* loss and metrics moved to hyper heads\n\n* do not flatten by default in hyperheads\n\n* inputs and outputs down to GraphAutoModel\n\n* changed AutoModel\n\n* name_scope changed to tf 2.0 and moved to hyperparameters and hypermodel\n\n* remove HierarchicalHyperParameters\n\n* renaming the tests\n\n* auto_pipeline\n\n* image module tested\n\n* image regressor tested\n\n* removed some requirements\n\n* update tf to 2.0 beta\n\n* Refactor (#646)\n\n* super classes extending object\n\n* change n_ to num_\n\n* refactored automodel to extend hypermodel and removed tuner from signature\n\n* rename HyperNode to Node. TextInput added\n\n* rename HyperGraph to GraphAutoModel extending AutoModel\n\n* refactor hyperhead, removed tensor heads\n\n* removed unnecessary blocks, rename build_output to build\n\n* changed some functions and attributes to private\n\n* remove trails from AutoModel public API\n\n* test cases changed accordingly\n\n* import modules instead of objects\n\n* tuner deleted from AutoModel contructor\n\n* change trails to num_trials\n\n* use the same quote sign\n\n* revised auto pipeline docs\n\n* removed compile from AutoModel\n\n* loss and metrics moved to hyper heads\n\n* do not flatten by default in hyperheads\n\n* inputs and outputs down to GraphAutoModel\n\n* changed AutoModel\n\n* name_scope changed to tf 2.0 and moved to hyperparameters and hypermodel\n\n* remove HierarchicalHyperParameters\n\n* renaming some variables and make private some members\n\n* demo update\n\n* remove legacy\n\n* test fixed\n\n* changed setup.py\n\n* Update hyper_block.py\n\nImprove the DenseBlock: Add some other layers'category to it.\n\n* Update hyper_block.py\n\n* pep8 style fix\n\n* changed docstrings to use markdown\n\n* renaming some of the variables and classes\n\n* extracted check new and old search space to tuner base class\n\n* fixing some pylint issues\n\n* change the normalization to use mean and stddev only\n\n* dependency\n\n* dependency changed to keras-tuner\n\n* pep8 formatting\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class Input(Node):",
            "",
            "",
            "class ImageInput(Node):",
            "+",
            "def __init__(self, **kwargs):",
            "super().__init__(**kwargs)",
            "",
            "def build(self, hp):",
            "-        pass",
            "+        return tf.keras.Input(shape=self.shape)",
            "",
            "",
            "class TextInput(Node):",
            "+",
            "def __init__(self, **kwargs):",
            "super().__init__(**kwargs)"
        ]
    },
    {
        "number": 522,
        "comments": "change param for resource fix",
        "commit_message": "make fix copies\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class StableDiffusionInpaintPipeline(DiffusionPipeline):",
            "else:",
            "raise ImportError(\"Please install accelerate via `pip install accelerate`\")",
            "",
            "-        device = torch.device(\"cuda\")",
            "+        device = torch.device(f\"cuda:{gpu_id}\")",
            "",
            "for cpu_offloaded_model in [self.unet, self.text_encoder, self.vae, self.safety_checker]:",
            "if cpu_offloaded_model is not None:"
        ]
    },
    {
        "number": 524,
        "comments": "change API call for resource fix",
        "commit_message": "Add onnx convert script (#2291)\n\n* add onnx convert script\n\n* upload pytorch2onnx.py\n\n* restore config\n\n* set use_torchvion on-the-fly, update doc\n\n* update doc\n\n* support pass input\n\n* fixed passes\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class TwoStageDetector(BaseDetector, RPNTestMixin, BBoxTestMixin,",
            "if self.with_rpn:",
            "rpn_outs = self.rpn_head(x)",
            "outs = outs + (rpn_outs, )",
            "-        proposals = torch.randn(1000, 4).cuda()",
            "+        proposals = torch.randn(1000, 4).to(device=img.device)",
            "# bbox head",
            "rois = bbox2roi([proposals])",
            "if self.with_bbox:"
        ]
    },
    {
        "number": 526,
        "comments": "add API call for shape fix",
        "commit_message": "fixed shape errors, now numerical instability in categorical trpo update\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class PGModel(Model):",
            "actions = np.concatenate([path['actions'] for path in batch])",
            "batch_advantage = np.concatenate([path[\"advantage\"] for path in batch])",
            "batch_advantage = zero_mean_unit_variance(batch_advantage)",
            "+        batch_advantage = np.expand_dims(batch_advantage, axis=1)",
            "states = np.concatenate([path['states'] for path in batch])",
            "",
            "return action_log_stds, action_means, actions, batch_advantage, states"
        ]
    },
    {
        "number": 529,
        "comments": "change API call for math fix",
        "commit_message": "Add Cuda tests for pyro.distributions; fix errors (#297)\n\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class Categorical(Distribution):",
            "elif one_hot:",
            "boolean_mask = x",
            "else:",
            "-                boolean_mask = torch.zeros(ps.size()).scatter_(-1, x.data.long(), 1)",
            "+                boolean_mask = torch_zeros_like(ps.data).scatter_(-1, x.data.long(), 1)",
            "# apply log function to masked probability tensor",
            "return torch.log(ps.masked_select(boolean_mask.byte()).contiguous().view(*batch_pdf_size))"
        ]
    },
    {
        "number": 530,
        "comments": "add API call for resource fixfor resource fix",
        "commit_message": "Fix CI: test_inference_for_pretraining in ViTMAEModelTest (#16591)\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class ViTMAEModelIntegrationTest(unittest.TestCase):",
            "",
            "# forward pass",
            "with torch.no_grad():",
            "-            outputs = model(**inputs, noise=torch.from_numpy(noise))",
            "+            outputs = model(**inputs, noise=torch.from_numpy(noise).to(device=torch_device))",
            "",
            "# verify the logits",
            "expected_shape = torch.Size((1, 196, 768))"
        ]
    },
    {
        "number": 531,
        "comments": "doc update",
        "commit_message": "fix comment\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "\"source\": [",
            "\"## Computation\\n\",",
            "\"\\n\",",
            "-    \"**Note copmut\"",
            "+    \"**Note computation, tfe.serving.QueueServer etc. will move into model.share()**\"",
            "]",
            "},",
            "{"
        ]
    },
    {
        "number": 532,
        "comments": "add API call for type fix",
        "commit_message": "Update nlp.py\n\nFix some bugs like https://github.com/tensorflow/tensorflow/issues/5118.\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def initialize_vocabulary(vocabulary_path):",
            "rev_vocab = []",
            "with gfile.GFile(vocabulary_path, mode=\"rb\") as f:",
            "rev_vocab.extend(f.readlines())",
            "-    rev_vocab = [line.strip() for line in rev_vocab]",
            "+    rev_vocab = [tf.compat.as_bytes(line.strip()) for line in rev_vocab]",
            "vocab = dict([(x, y) for (y, x) in enumerate(rev_vocab)])",
            "return vocab, rev_vocab",
            "else:"
        ]
    },
    {
        "number": 533,
        "comments": "def",
        "commit_message": "enable `disallow_incomplete_defs` on mypy (#2094)\n\n* enable `disallow_incomplete_defs` on mypy\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix `blur_pool2d` doc\n\n* finish v1: works on torch 1.13.1\n\n- Remove JIT support for Boxes3D\n\n* rip off the np typing\n\n* replace `Size` with `Tuple[int, ...]` on augs\n\n* add `Dtype` to kornia.filters.kernels\n\n* minor fix after rebase\n\n* Remove old torch from typing CI\n\n---------\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class CenterCrop(GeometricAugmentationBase2D):",
            "padding_mode=\"zeros\",",
            ")",
            "",
            "-    def generate_parameters(self, batch_shape: torch.Size) -> Dict[str, Tensor]:",
            "+    def generate_parameters(self, batch_shape: Tuple[int, ...]) -> Dict[str, Tensor]:",
            "return rg.center_crop_generator(batch_shape[0], batch_shape[-2], batch_shape[-1], self.size, self.device)",
            "",
            "def compute_transformation(self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any]) -> Tensor:"
        ]
    },
    {
        "number": 534,
        "comments": "add condition check for version fix",
        "commit_message": "Fix complex support\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class ConformerSeparator(AbsSeparator):",
            "\"\"\"",
            "",
            "# if complex spectrum,",
            "-        if isinstance(input, ComplexTensor):",
            "+        if isinstance(input, ComplexTensor) or (",
            "+            is_torch_1_8_plus and torch.is_complex(input)",
            "+        ):",
            "feature = abs(input)",
            "else:",
            "feature = input"
        ]
    },
    {
        "number": 535,
        "comments": "remove condition",
        "commit_message": "[RLlib] Add testing framework_iterator. (#7852)\n\n* Add testing framework_iterator.\n\n* LINT.\n\n* WIP.\n\n* Fix and LINT.\n\n* LINT fix.\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def model_import_test(algo, config, env):",
            "agent.import_model(import_file=import_file)",
            "check(current_weight(agent), weight_after_import)",
            "",
            "-        if eager_mode_ctx:",
            "-            eager_mode_ctx.__exit__(None, None, None)",
            "-",
            "",
            "class TestModelImport(unittest.TestCase):",
            "def setUp(self):"
        ]
    },
    {
        "number": 537,
        "comments": "add API call for type fix",
        "commit_message": "bug fix : cnn tensorflow backend error (#3558)\n\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def batch_flatten(x):",
            "'''Turn a n-D tensor into a 2D tensor where",
            "the first dimension is conserved.",
            "'''",
            "-    x = tf.reshape(x, [-1, prod(shape(x)[1:])])",
            "+    x = tf.reshape(x, tf.pack([-1, prod(shape(x)[1:])]))",
            "return x"
        ]
    },
    {
        "number": 538,
        "comments": "add condition check for resource fix",
        "commit_message": "[App] Fix multi-node pytorch example CI (#15753)\n\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def distributed_train(local_rank: int, main_address: str, main_port: int, num_no",
            "# 2. PREPARE DISTRIBUTED MODEL",
            "model = torch.nn.Linear(32, 2)",
            "device = torch.device(f\"cuda:{local_rank}\") if torch.cuda.is_available() else torch.device(\"cpu\")",
            "-    model = DistributedDataParallel(model, device_ids=[local_rank]).to(device)",
            "+    model = DistributedDataParallel(model, device_ids=[local_rank] if torch.cuda.is_available() else None).to(device)",
            "",
            "# 3. SETUP LOSS AND OPTIMIZER",
            "criterion = torch.nn.MSELoss()"
        ]
    },
    {
        "number": 543,
        "comments": "test fix",
        "commit_message": "Refac module factory + avoid etag requests for hub datasets (#2986)\n\n* refac module factory + avoid etag requests for hub datasets\n\n* fix tests\n\n* typing\n\n* fixes\n\n* prepare timeout\n\n* fix offline simulator with hugginggace_hub\n\n* add module factory tests (1/N)\n\n* add module factory test (2/N)\n\n* add data files tests (1/N)\n\n* add data fiels tests (2/N)\n\n* add data files tests (3/N)\n\n* style\n\n* docstrings\n\n* don't update counts when running tests\n\n* nump huggingface_hub\n\n* add timeouts for offline mode\n\n* minor\n\n* minor bis\n\n* install ruamel-yaml properly in the CI for windows\n\n* fix windows test\n\n* style\n\n* fix comet intensive calls patcher\n\n* warning message when loading from the master branch\n\n* style\n\n* albert's comments\n\n* remove unnecessary check\n\n* don't use master if HF_SCRIPTS_VERSION is specified\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "class COMET(datasets.Metric):",
            "",
            "def _download_and_prepare(self, dl_manager):",
            "if self.config_name == \"default\":",
            "-            self.scorer = download_model(\"wmt-large-da-estimator-1719\")",
            "+            self.scorer = comet.models.download_model(\"wmt-large-da-estimator-1719\")",
            "else:",
            "-            self.scorer = download_model(self.config_name)",
            "+            self.scorer = comet.models.download_model(self.config_name)",
            "",
            "def _compute(self, sources, predictions, references, cuda=True, show_progress=False):",
            "data = {\"src\": sources, \"mt\": predictions, \"ref\": references}"
        ]
    },
    {
        "number": 544,
        "comments": "rename",
        "commit_message": "fixed a problem of quantile of torch backend that it rejects integer input\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def quantile(",
            "temp, q, dim=-1, keepdim=keepdims, interpolation=interpolation, out=out",
            ")",
            "return torch.quantile(",
            "-        a, q, dim=axis, keepdim=keepdims, interpolation=interpolation, out=out",
            "+        temp, q, dim=axis, keepdim=keepdims, interpolation=interpolation, out=out",
            ")"
        ]
    },
    {
        "number": 545,
        "comments": "change param for shape fix",
        "commit_message": "fix a size mismatch bug in faster_rcnn\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "if __name__ == '__main__':",
            "",
            "# dataset = roiLoader(roidb, imdb.num_classes)",
            "dataset = roibatchLoader(roidb, imdb.num_classes)",
            "-  dataloader = torch.utils.data.DataLoader(dataset, batch_size=cfg.TRAIN.IMS_PER_BATCH,",
            "+  dataloader = torch.utils.data.DataLoader(dataset, batch_size=8,",
            "shuffle=False, num_workers=5)",
            "",
            "# initilize the tensor holder here."
        ]
    },
    {
        "number": 547,
        "comments": "name change",
        "commit_message": "test validity fix\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def test_confusion_matrix(",
            "@handle_cmd_line_args",
            "@given(",
            "dtype_and_x=helpers.dtype_and_values(",
            "-        available_dtypes=tuple(ivy_tf.valid_numeric_dtypes)",
            "+        available_dtypes=tuple(ivy_tf.valid_float_dtypes)",
            "),",
            "x=helpers.array_values(shape=(3,), dtype=ivy.int32),",
            "as_variable=st.booleans(),"
        ]
    },
    {
        "number": 548,
        "comments": "change condition check for null fix",
        "commit_message": "Update benchmark script to add precision arg. Fix some downstream (DeiT) compat issues with latest changes. Bump version to 0.4.7\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class VisionTransformer(nn.Module):",
            "",
            "def forward(self, x):",
            "x = self.forward_features(x)",
            "-        if isinstance(x, tuple):",
            "-            x, x_dist = self.head(x[0]), self.head_dist(x[1])",
            "+        if self.head_dist is not None:",
            "+            x, x_dist = self.head(x[0]), self.head_dist(x[1])  # x must be a tuple",
            "if self.training and not torch.jit.is_scripting():",
            "# during inference, return the average of both classifier predictions",
            "return x, x_dist"
        ]
    },
    {
        "number": 549,
        "comments": "add condition check for type fix",
        "commit_message": "Wrap dataset generation function to disable autograph to fix issues with invalid tensor shapes (#2069)\n\nSigned-off-by: Travis Addair <taddair@uber.com>\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class TFKerasUtil(object):",
            "",
            "dataset = dataset.batch(batch_size).map(prep_data_tf_keras)",
            "return dataset",
            "-        return fn",
            "+        return tf.autograph.experimental.do_not_convert(fn) if _HAS_AUTOGRAPH else fn",
            "",
            "@staticmethod",
            "def get_horovod():"
        ]
    },
    {
        "number": 554,
        "comments": "change param for null fix",
        "commit_message": "tl.layers API Refactoring and various modifications (#667)\n\n* Decorators API Refactored\n\n* extra_requires `all`, `all_cpu` and `all_gpu` added\n\n* Error fix\n\n* YAPF Formating Correction\n\n* Test for private method decorator added\n\n* Test Logging Verbosity Fixed to Debug when runned individually\n\n* YAPF corrections applied\n\n* Changelog Added\n\n* Changelog updated\n\n* PR number changed\n\n* First Refactoring Pass done\n\n* cleaning second pass\n\n* Refactoring 3rd pass\n\n* Refactoring 4th Pass\n\n* Code Error fix\n\n* YAPF Formating Fix\n\n* Arguments now using self\n\n* YAPF error correction\n\n* Bug Fix in Decorator\n\n* act name bug fix\n\n* Error Correction\n\n* YAPF formating fix\n\n* Useless tf.identity removed\n\n* Error Fix\n\n* Changelog Updated\n\n* Error fix in tl.activation\n\n* Documentation error fix\n\n* Lazy Import added\n\n* Import Refactoring with LazyImport when necessary\n\n* Changelog Updated\n\n* Gitter Removed\n\n* Fixed proposed by @zsdonghao\n\n* Documentation updated\n\n* Missing requirements added\n\n* Update to TensorLayer 1.8.6rc1\n\n* Requirements error fix\n\n* Docker Files updated\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "with tf.device('/cpu:0'):",
            "net = FlattenLayer(net, name='flatten')",
            "net = DenseLayer(net, 384, act=tf.nn.relu, W_init=W_init2, b_init=b_init2, name='d1relu')",
            "net = DenseLayer(net, 192, act=tf.nn.relu, W_init=W_init2, b_init=b_init2, name='d2relu')",
            "-            net = DenseLayer(net, n_units=10, act=tf.identity, W_init=W_init2, name='output')",
            "+            net = DenseLayer(net, n_units=10, act=None, W_init=W_init2, name='output')",
            "y = net.outputs",
            "",
            "ce = tl.cost.cross_entropy(y, y_, name='cost')"
        ]
    },
    {
        "number": 555,
        "comments": "format",
        "commit_message": "[RLlib] Tf2x preparation; part 2 (upgrading `try_import_tf()`). (#9136)\n\n* WIP.\n\n* Fixes.\n\n* LINT.\n\n* WIP.\n\n* WIP.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* WIP.\n\n* Fixes.\n\n* Test\n\n* Fix.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* LINT.\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class GaussianNoise(Exploration):",
            "true_fn=lambda: stochastic_actions,",
            "false_fn=lambda: deterministic_actions)",
            "# Logp=always zero.",
            "-        logp = tf.zeros(shape=(batch_size, ), dtype=tf.float32)",
            "+        logp = tf.zeros(shape=(batch_size,), dtype=tf.float32)",
            "",
            "# Increment `last_timestep` by 1 (or set to `timestep`).",
            "-        assign_op = \\",
            "-            tf.assign_add(self.last_timestep, 1) if timestep is None else \\",
            "-            tf.assign(self.last_timestep, timestep)",
            "-        with tf.control_dependencies([assign_op]):",
            "+        assign_op = (",
            "+            tf1.assign_add(self.last_timestep, 1) if timestep is None else",
            "+            tf1.assign(self.last_timestep, timestep))",
            "+        with tf1.control_dependencies([assign_op]):",
            "return action, logp",
            "",
            "def _get_torch_exploration_action(self, action_dist, explore, timestep):"
        ]
    },
    {
        "number": 557,
        "comments": "change param for math fix",
        "commit_message": "Fixed bad use of ConvTranspose2D\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class up(nn.Module):",
            "if bilinear:",
            "self.up = nn.UpsamplingBilinear2d(scale_factor=2)",
            "else:",
            "-            self.up = nn.ConvTranspose2d(in_ch, out_ch, 2, stride=2)",
            "+            self.up = nn.ConvTranspose2d(in_ch//2, in_ch//2, 2, stride=2)",
            "",
            "self.conv = double_conv(in_ch, out_ch)"
        ]
    },
    {
        "number": 559,
        "comments": "add API call for state fix",
        "commit_message": "[Metrics] Detach bugfix (#4313)\n\n* detach on buffer\n\n* doc update\n\n* remove file\n\n* changelog\n\n* suggestions\n\n* Update docs/source/metrics.rst\n\nCo-authored-by: Teddy Koker <teddy.koker@gmail.com>\n\n* fix for 4266\n\n* Update docs/source/metrics.rst\n\nCo-authored-by: Adrian W\u00e4lchli <aedu.waelchli@gmail.com>\n\n* Update CHANGELOG.md\n\nCo-authored-by: Teddy Koker <teddy.koker@gmail.com>\nCo-authored-by: chaton <thomas@grid.ai>\nCo-authored-by: Adrian W\u00e4lchli <aedu.waelchli@gmail.com>\nCo-authored-by: Ananya Harsh Jha <ananya@pytorchlightning.ai>\nCo-authored-by: Roger Shieh <sh.rog@protonmail.ch>\nCo-authored-by: Sean Naren <sean.narenthiran@gmail.com>\nCo-authored-by: Rohit Gupta <rohitgr1998@gmail.com>\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class Metric(nn.Module, ABC):",
            "Automatically calls ``update()``. Returns the metric value over inputs if ``compute_on_step`` is True.",
            "\"\"\"",
            "# add current step",
            "-        self.update(*args, **kwargs)",
            "+        with torch.no_grad():",
            "+            self.update(*args, **kwargs)",
            "self._forward_cache = None",
            "",
            "if self.compute_on_step:"
        ]
    },
    {
        "number": 560,
        "comments": "change API call for resource fix",
        "commit_message": "fixing inference to use volatile variables\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "temperature = max(args.temperature, 1e-3)",
            "with open(args.outf, 'w') as outf:",
            "for i in range(args.nwords):",
            "",
            "-        output, hidden = model(Variable(input, requires_grad=False), hidden)",
            "-        gen = torch.multinomial(output[0].data.cpu().div(temperature).exp(), 1)[0][0] # FIXME: no multinomial on GPU?",
            "+        output, hidden = model(Variable(input, volatile=True), hidden)",
            "+        gen = torch.multinomial(output[0].data.div(temperature).exp().cpu(), 1)[0][0] # FIXME: multinomial is only for CPU",
            "input.fill_(gen)",
            "word = corpus.dic.idx2word[gen]",
            "outf.write(word)"
        ]
    },
    {
        "number": 561,
        "comments": "change param for type fix",
        "commit_message": "More TF int dtype fixes (#20384)\n\n* Add a test to ensure int dummy inputs are int64\n\n* Move the test into the existing int64 test and update a lot of existing dummies\n\n* Fix remaining dummies\n\n* Fix remaining dummies\n\n* Test for int64 serving sigs as well\n\n* Update core tests to use tf.int64\n\n* Add better messages to the assertions\n\n* Update all serving sigs to int64\n\n* More sneaky hiding tf.int32s\n\n* Add an optional int32 signature in save_pretrained\n\n* make fixup\n\n* Add Amy's suggestions\n\n* Switch all serving sigs back to tf.int32\n\n* Switch all dummies to tf.int32\n\n* Adjust tests to check for tf.int32 instead of tf.int64\n\n* Fix base dummy_inputs dtype\n\n* Start casting to tf.int32 in input_processing\n\n* Change dtype for unpack_inputs test\n\n* Add proper tf.int32 test\n\n* Make the alternate serving signature int64\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class TFFunnelForMultipleChoice(TFFunnelPreTrainedModel, TFMultipleChoiceLoss):",
            "@tf.function(",
            "input_signature=[",
            "{",
            "-                \"input_ids\": tf.TensorSpec((None, None), tf.int64, name=\"input_ids\"),",
            "+                \"input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"input_ids\"),",
            "\"attention_mask\": tf.TensorSpec((None, None), tf.float32, name=\"attention_mask\"),",
            "-                \"token_type_ids\": tf.TensorSpec((None, None), tf.int64, name=\"token_type_ids\"),",
            "+                \"token_type_ids\": tf.TensorSpec((None, None), tf.int32, name=\"token_type_ids\"),",
            "}",
            "]",
            ")"
        ]
    },
    {
        "number": 563,
        "comments": "not clear",
        "commit_message": "Prosody-aware Generative Spoken Language Modelling (#3063)\n\nSummary:\n# Before submitting\n\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\n- [ ] Did you make sure to update the docs?\n- [ ] Did you write any new necessary tests?\n\n## What does this PR do?\nFixes # (issue).\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding \ufffd\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/3063\n\nReviewed By: eugene-kharitonov\n\nDifferential Revision: D34323605\n\nPulled By: wnhsu\n\nfbshipit-source-id: 9dc779a6c399cda710863596e0880b9277ff2919\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class CodeGenerator(Generator):",
            "x = torch.cat([x, spkr], dim=1)",
            "",
            "for k, feat in kwargs.items():",
            "-            if k in [\"spkr\", \"code\", \"dur_prediction\"]:",
            "+            if k in [\"spkr\", \"code\", \"f0\", \"dur_prediction\"]:",
            "continue",
            "",
            "feat = self._upsample(feat, x.shape[-1])"
        ]
    },
    {
        "number": 564,
        "comments": "doc update",
        "commit_message": "fixed todo\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class TorchHook:",
            "setattr(torch_module, func, new_func)",
            "",
            "torch_modules = syft.torch.torch_modules",
            "-        # torch_modules = {\"torch.nn.functional\": self.torch.nn.functional,",
            "-                         # \"torch\": self.torch}",
            "-        # TODO Replace with syft.torch.torch_modules when hooking 'torch' will not break msgpack",
            "",
            "for module_name, torch_module in torch_modules.items():",
            "for func in dir(torch_module):"
        ]
    },
    {
        "number": 565,
        "comments": "doc update",
        "commit_message": "fixed trpo\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class PiecewiseConstant(Parameter):",
            "elif self.unit == 'episodes':",
            "step = Module.retrieve_tensor(name='episode')",
            "",
            "-        # step = tf.Print(step, (step,))",
            "-",
            "parameter = tf.train.piecewise_constant(",
            "x=step, boundaries=self.boundaries, values=self.values",
            ")"
        ]
    },
    {
        "number": 566,
        "comments": "change condition check for null fix",
        "commit_message": "Fix horovod training; import pyarrow without torch\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class ModelSaver(Callback):",
            "self.var_collections = var_collections",
            "if checkpoint_dir is None:",
            "checkpoint_dir = logger.get_logger_dir()",
            "-        assert checkpoint_dir is not None",
            "-        if not tf.gfile.IsDirectory(checkpoint_dir):",
            "-            tf.gfile.MakeDirs(checkpoint_dir)",
            "+        if checkpoint_dir is not None:",
            "+            if not tf.gfile.IsDirectory(checkpoint_dir):",
            "+                tf.gfile.MakeDirs(checkpoint_dir)",
            "self.checkpoint_dir = checkpoint_dir",
            "",
            "def _setup_graph(self):",
            "+        assert self.checkpoint_dir is not None, \\",
            "+            \"ModelSaver() doesn't have a valid checkpoint directory.\"",
            "vars = []",
            "for key in self.var_collections:",
            "vars.extend(tf.get_collection(key))"
        ]
    },
    {
        "number": 569,
        "comments": "add condition check for resource fix",
        "commit_message": "PyTorch 1.7 Stable support (#3821)\n\n* prepare for 1.7 support [ci skip]\n\n* tpu [ci skip]\n\n* test run 1.7\n\n* all 1.7, needs to fix tests\n\n* couple with torchvision\n\n* windows try\n\n* remove windows\n\n* 1.7 is here\n\n* on purpose fail [ci skip]\n\n* return [ci skip]\n\n* 1.7 docker\n\n* back to normal [ci skip]\n\n* change to some_val [ci skip]\n\n* add seed [ci skip]\n\n* 4 places [ci skip]\n\n* fail on purpose [ci skip]\n\n* verbose=True [ci skip]\n\n* use filename to track\n\n* use filename to track\n\n* monitor epoch + changelog\n\n* Update tests/checkpointing/test_model_checkpoint.py\n\nCo-authored-by: Rohit Gupta <rohitgr1998@gmail.com>\n\nCo-authored-by: Sean Naren <sean.narenthiran@gmail.com>\nCo-authored-by: Rohit Gupta <rohitgr1998@gmail.com>\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def setup_ddp(rank, world_size):",
            "os.environ[\"MASTER_ADDR\"] = 'localhost'",
            "os.environ['MASTER_PORT'] = '8088'",
            "",
            "-    if torch.distributed.is_available():",
            "+    if torch.distributed.is_available() and sys.platform not in ['win32', 'cygwin']:",
            "torch.distributed.init_process_group(\"gloo\", rank=rank, world_size=world_size)"
        ]
    },
    {
        "number": 570,
        "comments": "format",
        "commit_message": "fix few jit and cuda errors in color (#767)\n\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def luv_to_rgb(image: torch.Tensor, eps: float = 1e-12) -> torch.Tensor:",
            "g: torch.Tensor = torch.where(gs > 0.0031308, 1.055 * torch.pow(gs, 1 / 2.4) - 0.055, 12.92 * gs)",
            "b: torch.Tensor = torch.where(bs > 0.0031308, 1.055 * torch.pow(bs, 1 / 2.4) - 0.055, 12.92 * bs)",
            "",
            "-    rgb_im: torch.Tensor = torch.stack((r, g, b), dim=-3)",
            "+    rgb_im: torch.Tensor = torch.stack([r, g, b], dim=-3)",
            "",
            "return rgb_im"
        ]
    },
    {
        "number": 571,
        "comments": "change condition check for type fix",
        "commit_message": "[Feat] Cleanup ModelCheckpoint / EarlyStopping by moving logic to LoggerConnector (#5218)\n\n* [bug-fix] Metric reduction with Logging (#5150)\n\n* add test\n\n* resolve bug\n\n* udpate test\n\n* wrongly copy / paste\n\n* update test\n\n* resolve a second bug\n\nCo-authored-by: Ubuntu <ubuntu@ip-172-31-62-109.ec2.internal>\n\n* iupdate\n\n* resolve bugs\n\n* add test back\n\n* correct flake8\n\n* resolve flake8\n\n* update on comments\n\n* update tests\n\n* add a test\n\n* add test\n\n* update to Callable\n\nCo-authored-by: Ubuntu <ubuntu@ip-172-31-62-109.ec2.internal>\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class ModelCheckpoint(Callback):",
            "self.best_k_models.pop(del_filepath)",
            "",
            "# do not save nan, replace with +/- inf",
            "-        if torch.isnan(current):",
            "+        if isinstance(current, torch.Tensor) and torch.isnan(current):",
            "current = torch.tensor(float('inf' if self.mode == \"min\" else '-inf'))",
            "",
            "filepath = self._get_metric_interpolated_filepath_name(ckpt_name_metrics, epoch, step, del_filepath)"
        ]
    },
    {
        "number": 573,
        "comments": "rename",
        "commit_message": "Add t5 to pipeline(task='summarization') (#3413)\n\n* solve conflicts\n\n* move warnings below\n\n* incorporate changes\n\n* add pad_to_max_length to pipelines\n\n* add bug fix for T5 beam search\n\n* add prefix patterns\n\n* make style\n\n* fix conflicts\n\n* adapt pipelines for task specific parameters\n\n* improve docstring\n\n* remove unused patterns\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "class TFPreTrainedModel(tf.keras.Model, TFModelUtilsMixin):",
            "# set eos token prob to zero if min_length is not reached",
            "if eos_token_id is not None and cur_len < min_length:",
            "# create eos_token_id boolean mask",
            "+                num_batch_hypotheses = batch_size * num_beams",
            "+",
            "is_token_logit_eos_token = tf.convert_to_tensor(",
            "[True if token is eos_token_id else False for token in range(vocab_size)], dtype=tf.bool",
            ")",
            "-                eos_token_indices_mask = tf.broadcast_to(is_token_logit_eos_token, [batch_size, vocab_size])",
            "+                eos_token_indices_mask = tf.broadcast_to(is_token_logit_eos_token, [num_batch_hypotheses, vocab_size])",
            "",
            "scores = set_tensor_by_indices_to_value(scores, eos_token_indices_mask, -float(\"inf\"))"
        ]
    },
    {
        "number": 574,
        "comments": "remove API call for resource fix",
        "commit_message": "bug fix for distributed strategy (#1285)\n\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class Graph(kerastuner.HyperModel, serializable.Serializable):",
            "",
            "def build(self, hp):",
            "\"\"\"Build the HyperModel into a Keras Model.\"\"\"",
            "-        tf.keras.backend.clear_session()",
            "self._register_hps(hp)",
            "self.compile()",
            "real_nodes = {}"
        ]
    },
    {
        "number": 575,
        "comments": "change param for shape fix",
        "commit_message": "Add support for HuggingFace's TensorFlow models (#127)\n\n* added support for for HuggingFace's TensorFlow models\n\n* added notebook for HuggingFace's tensorflow bert model\n\n* change nebullvm name in logs\n\n* Add optimized model details + warning if static shape is used for HF models (#1)\n\n* add optimized model type info\n\n* fix tvm issue\n\n* edit dockerfile and add image auto building\n\n* add docker installation on azure pipeline\n\n* fix bug in neural compressor output shape\n\n* add support for openvino with python 3.10\n\n* add build docker image to azure pipelines\n\n* revert docker build from az pipelines and edit format of the optimization results\n\n* revert docker build from az pipelines\n\n* added tabulate to setup.py and general fixes\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class TensorflowONNXTensorRTInferenceLearner(",
            "else None",
            ")",
            "out_arrays = self._predict_array(cuda_input_arrays, input_shapes)",
            "-        return tuple(tf.convert_to_tensor(array) for array in out_arrays)",
            "+        return tuple(tf.convert_to_tensor(array[0]) for array in out_arrays)",
            "",
            "",
            "class NumpyONNXTensorRTInferenceLearner("
        ]
    },
    {
        "number": 576,
        "comments": "doc update",
        "commit_message": "Fixed typos in AdditiveAttention description.\n\nPiperOrigin-RevId: 381049091\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "class AdditiveAttention(BaseDenseAttention):",
            "shape `[batch_size, Tv, dim]` and `key` tensor of shape",
            "`[batch_size, Tv, dim]`. The calculation follows the steps:",
            "",
            "-  1. Reshape `query` and `value` into shapes `[batch_size, Tq, 1, dim]`",
            "+  1. Reshape `query` and `key` into shapes `[batch_size, Tq, 1, dim]`",
            "and `[batch_size, 1, Tv, dim]` respectively.",
            "2. Calculate scores with shape `[batch_size, Tq, Tv]` as a non-linear",
            "-     sum: `scores = tf.reduce_sum(tf.tanh(query + value), axis=-1)`",
            "+     sum: `scores = tf.reduce_sum(tf.tanh(query + key), axis=-1)`",
            "3. Use scores to calculate a distribution with shape",
            "`[batch_size, Tq, Tv]`: `distribution = tf.nn.softmax(scores)`.",
            "4. Use `distribution` to create a linear combination of `value` with"
        ]
    },
    {
        "number": 577,
        "comments": "not clear",
        "commit_message": "Some small fixes\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def main(args):",
            "update_gradient_vars.append(var)",
            "else:",
            "restore_vars = tf.all_variables()",
            "+            update_gradient_vars = tf.all_variables()",
            "",
            "# Build a Graph that trains the model with one batch of examples and updates the model parameters",
            "train_op = facenet.train(total_loss, global_step, args.optimizer,"
        ]
    },
    {
        "number": 578,
        "comments": "add param for math fix",
        "commit_message": "Cleanup the usage of `layer_norm_eps` in some models (#21336)\n\n* fix\n\n* fix\n\n* make style\n\n* For CLIP\n\n* For OwlViT\n\n* For XCLIP\n\n* For CLIPSeg\n\n* For GroupViT\n\n* fix docstrings\n\n* fix docstrings\n\n* For AltCLIP\n\n* For ChineseCLIP\n\n* For Blip\n\n* For GiT\n\n* make style\n\n* update\n\n* update\n\n* update\n\n* fix\n\n* fix\n\n* fix\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class GroupViTVisionTransformer(nn.Module):",
            "",
            "self.embeddings = GroupViTVisionEmbeddings(config)",
            "self.encoder = GroupViTVisionEncoder(config)",
            "-        self.layernorm = nn.LayerNorm(embed_dim)",
            "+        self.layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)",
            "",
            "@add_start_docstrings_to_model_forward(GROUPVIT_VISION_INPUTS_DOCSTRING)",
            "@replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=GroupViTVisionConfig)"
        ]
    },
    {
        "number": 579,
        "comments": "format",
        "commit_message": "Refactor library namespaces [pre-release][0.6-rc1] (#1412)\n\n* flake fixes\n\n* initial flake8 fixeS\n\n* remove top level from kornia.color\n\n* kornia filters\n\n* kornia losses\n\n* kornia features\n\n* geomtry and all ok\n\n* removed jit module\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* apply formatting and few fixes\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add keep block for isort\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* skip init\n\n* fix the docs\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* remove old code\n\n* simplify ci workflow\n\n* fix circular dependency\n\n* few format fixes\n\n* fix code format test\n\n* remove kornia.jit imports\n\n* final fixes\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* ci fixes\n\n* add versioneer\n\n* fix pnp import\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* exclude version files from pre-commit\n\n* exclude files precommit\n\n* update to pytorch 1.10 and add fixes\n\n* Update tests_cpu.yml\n\n* Update setup_dev_env.sh\n\n* Update tests_cpu.yml\n\n* undo versioneer\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix no_grad\n\n* Apply suggestions from code review\n\n* fix skip windows tests\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Update test_integrated.py\n\n* Apply suggestions from code review\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def configuration():",
            "",
            "",
            "class TestImageClassifierTrainer:",
            "-",
            "def test_fit(self, model, dataloader, criterion, optimizer, scheduler, configuration):",
            "-        trainer = ImageClassifierTrainer(",
            "-            model, dataloader, dataloader, criterion, optimizer, scheduler, configuration,",
            "-        )",
            "+        trainer = ImageClassifierTrainer(model, dataloader, dataloader, criterion, optimizer, scheduler, configuration)",
            "trainer.fit()",
            "",
            "def test_exception(self, model, dataloader, criterion, optimizer, scheduler, configuration):",
            "with pytest.raises(ValueError):",
            "ImageClassifierTrainer(",
            "-                model, dataloader, dataloader, criterion, optimizer, scheduler, configuration,",
            "-                callbacks={'frodo': None},",
            "+                model, dataloader, dataloader, criterion, optimizer, scheduler, configuration, callbacks={'frodo': None}",
            ")"
        ]
    },
    {
        "number": 580,
        "comments": "rename",
        "commit_message": "small fixes\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def train_hypernetwork(hypernetwork_name, learn_rate, batch_size, gradient_step,",
            "shared.state.textinfo = f\"\"\"",
            "<p>",
            "Loss: {loss_step:.7f}<br/>",
            "-Step: {hypernetwork.step}<br/>",
            "+Step: {steps_done}<br/>",
            "Last prompt: {html.escape(batch.cond_text[0])}<br/>",
            "Last saved hypernetwork: {html.escape(last_saved_file)}<br/>",
            "Last saved image: {html.escape(last_saved_image)}<br/>"
        ]
    },
    {
        "number": 585,
        "comments": "change param for type fix",
        "commit_message": "Fix hooks test\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class TestTrainSampleHook(tf.test.TestCase):",
            "pred_dict = {}",
            "pred_dict[\"predicted_tokens\"] = tf.constant([[\"Hello\", \"World\", \"\u7b11w\"]])",
            "pred_dict[\"labels.target_tokens\"] = tf.constant([[\"Hello\", \"World\", \"\u7b11w\"]])",
            "-    pred_dict[\"labels.target_len\"] = tf.constant([2]),",
            "+    pred_dict[\"labels.target_len\"] = tf.constant(2),",
            "graph_utils.add_dict_to_collection(pred_dict, \"predictions\")",
            "",
            "def tearDown(self):"
        ]
    },
    {
        "number": 586,
        "comments": "format",
        "commit_message": "fixing float_power promotion issue in backends and numpy frontend (#10332)\n\nFixed the promotion of the function but as the torch backend only is directed to return float64 and complex128 will add dtype option to the ivy function to govern the type of return we want from the function this will permit to produce proper results in the frontends\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def xlogy(",
            "return torch.xlogy(x, y, out=out)",
            "",
            "",
            "-def real(",
            "-    x: Union[torch.Tensor], /, *, out: Optional[torch.Tensor] = None",
            "-) -> torch.Tensor:",
            "+def real(x: torch.Tensor, /, *, out: Optional[torch.Tensor] = None) -> torch.Tensor:",
            "return torch.real(x)"
        ]
    },
    {
        "number": 587,
        "comments": "change API call for type fix",
        "commit_message": "Fixes to enable dev branch to move to pytorch-1.2 (#2001)\n\n* Fixes to enable dev branch to move to pytorch-1.2\n\n* change name for published cpu wheels\n\n* fix failing unit tests\n\n* change docs conf\n\n* change tracking example\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class VonMises(TorchDistribution):",
            "\"\"\"",
            "shape = self._extended_shape(sample_shape)",
            "x = torch.empty(shape, dtype=self.loc.dtype, device=self.loc.device)",
            "-        done = torch.zeros(shape, dtype=self.loc.dtype, device=self.loc.device).byte()",
            "+        done = torch.zeros(shape, dtype=self.loc.dtype, device=self.loc.device).bool()",
            "while not done.all():",
            "u = torch.rand((3,) + shape, dtype=self.loc.dtype, device=self.loc.device)",
            "u1, u2, u3 = u.unbind()"
        ]
    },
    {
        "number": 588,
        "comments": "change API",
        "commit_message": "bugfix\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class GridTest(TestCase):",
            "assert_equal(adj.to_dense().numpy(), expected_adj)",
            "",
            "def test_grid_with_connectivity_8(self):",
            "-        adj = grid(torch.Size([3, 2]), connectivity=8)",
            "+        adj = grid_3x3(torch.Size([3, 2]), connectivity=8)",
            "",
            "expected_adj = [",
            "[0, 1, 1, 2, 0, 0],"
        ]
    },
    {
        "number": 591,
        "comments": "update API call for version fix",
        "commit_message": "fix TF deprecation of concat/split/pack/unpack\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class Model(ModelDesc):",
            "self.last_state = tf.identity(last_state, 'last_state')",
            "",
            "# seqlen x (Bxrnnsize)",
            "-        output = tf.reshape(tf.concat(1, outputs), [-1, param.rnn_size])  # (Bxseqlen) x rnnsize",
            "+        output = tf.reshape(tf.concat_v2(outputs, 1), [-1, param.rnn_size])  # (Bxseqlen) x rnnsize",
            "logits = FullyConnected('fc', output, param.vocab_size, nl=tf.identity)",
            "self.prob = tf.nn.softmax(logits / param.softmax_temprature)"
        ]
    },
    {
        "number": 594,
        "comments": "format",
        "commit_message": "Logging fix (#661)\n\n* `tf.logging` replaced by: `tl.logging`\n\n* Changelog updated\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def prefetch_input_data(",
            "for pattern in file_pattern.split(\",\"):",
            "data_files.extend(tf.gfile.Glob(pattern))",
            "if not data_files:",
            "-        tf.logging.fatal(\"Found no input files matching %s\", file_pattern)",
            "+        tl.logging.fatal(\"Found no input files matching %s\", file_pattern)",
            "else:",
            "-        tf.logging.info(\"Prefetching values from %d files matching %s\", len(data_files), file_pattern)",
            "+        tl.logging.info(\"Prefetching values from %d files matching %s\", len(data_files), file_pattern)",
            "",
            "if is_training:",
            "print(\"   is_training == True : RandomShuffleQueue\")"
        ]
    },
    {
        "number": 595,
        "comments": "change API call for math fix",
        "commit_message": "RANSAC improvements (#1435)\n\n- Added LU solver for RANSAC-homography, it speeds-up the minimal-solver part x10\n- Now find_fundamental weights is optional, as it was for homography\n- Fix test precision and made one xFail test to be compulsory\n- Fixed tests for fundamental matrix, as one have to have >= 8 points (also added to docstring)\n- updates deprecated torch.svd to torch.linalg.svd\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def fit_line(points: Tensor, weights: Optional[Tensor] = None) -> ParametrizedLi",
            "A = A.transpose(-2, -1) @ A",
            "",
            "# NOTE: not optimal for 2d points, but for now works for other dimensions",
            "-    _, _, V = torch.linalg.svd(A)",
            "+    _, _, V = _torch_svd_cast(A)",
            "+    V = V.transpose(-2, -1)",
            "",
            "# the first left eigenvector is the direction on the fited line",
            "direction = V[..., 0, :]  # BxD"
        ]
    },
    {
        "number": 596,
        "comments": "change API call for shape fix",
        "commit_message": "Bug fix when target is a SparseTensor. (#4200)\n\n* Bug fix when target is a SparseTensor.\nCheck for sparsity when creating target placeholder.\nRemove shape argument when creating sparse placeholder.\n\n* Fixed ndim behavior for sparse tensor\n\n* Fix sparse variable instantiation.\n\n* Bug fix\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def ndim(x):",
            "'''Returns the number of axes in a tensor, as an integer.",
            "'''",
            "if is_sparse(x):",
            "-        return int(x.shape.get_shape()[0])",
            "+        return x._dims",
            "",
            "dims = x.get_shape()._dims",
            "if dims is not None:"
        ]
    },
    {
        "number": 599,
        "comments": "add condition check for null fix",
        "commit_message": "[Ray Dataset] fix the type infer of `pd.dataframe` (when dtype is `object`.)  (#25563)\n\nthis is a temp fix of #25556. When the dtype from the pandas dataframe gives object, we set the dtype to be None and make use of the auto-inferring of the type in the conversion.\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def convert_pandas_to_tf_tensor(",
            "# them. If the columns contain different types (for example, `float32`s",
            "# and `int32`s), then `tf.concat` raises an error.",
            "dtype: np.dtype = np.find_common_type(df.dtypes, [])",
            "+",
            "+            # if the columns are `ray.data.extensions.tensor_extension.TensorArray`,",
            "+            # the dtype will be `object`. In this case, we need to set the dtype to",
            "+            # none, and use the automatic type casting of `tf.convert_to_tensor`.",
            "+            if isinstance(dtype, object):",
            "+                dtype = None",
            "+",
            "except TypeError:",
            "# `find_common_type` fails if a series has `TensorDtype`. In this case,",
            "# don't cast any of the series and continue."
        ]
    },
    {
        "number": 600,
        "comments": "add param for argument fix",
        "commit_message": "fix: add missing symbolink of data_prep and reset GroupNorm initialization\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def initialize(model: torch.nn.Module, init: str):",
            "",
            "# reset some modules with default init",
            "for m in model.modules():",
            "-            if isinstance(m, (torch.nn.Embedding, torch.nn.LayerNorm)):",
            "+            if isinstance(m, (torch.nn.Embedding, torch.nn.LayerNorm, torch.nn.GroupNorm)):",
            "m.reset_parameters()",
            "if hasattr(m, \"espnet_initialization_fn\"):",
            "m.espnet_initialization_fn()"
        ]
    },
    {
        "number": 601,
        "comments": "update API call for refactor fix",
        "commit_message": "Change rnn-cell to fix #103 (#104)\n\n* Change rnn-cell to fix #103\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class Model(ModelDesc):",
            "input_list = tf.unstack(input_feature, axis=1)  # seqlen x (Bxrnnsize)",
            "",
            "# seqlen is 1 in inference. don't need loop_function",
            "-        outputs, last_state = tf.nn.rnn(cell, input_list, initial, scope='rnnlm')",
            "+        outputs, last_state = tf.contrib.rnn.static_rnn(cell, input_list, initial, scope='rnnlm')",
            "self.last_state = tf.identity(last_state, 'last_state')",
            "",
            "# seqlen x (Bxrnnsize)"
        ]
    },
    {
        "number": 605,
        "comments": "doc update",
        "commit_message": "for discussion: incorporate black code formatter (#3308)\n\n* setup files\n\n* run black\n\n* undo\n\n* update CONTRIBUTING.md\n\n* fix quotes in test_other_modules\n\n* make flake8 happy\n\n* set black to 100 characters per line\n\n* move type: ignore to where mypy wants them\n\n* more flake8\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class NoamLR(LearningRateScheduler):",
            "else:",
            "self.last_epoch = batch_num_total",
            "for param_group, learning_rate in zip(self.optimizer.param_groups, self.get_values()):",
            "-            param_group['lr'] = learning_rate",
            "+            param_group[\"lr\"] = learning_rate",
            "",
            "def get_values(self):",
            "step = max(self.last_epoch, 1)",
            "-        scale = self.factor * (self.model_size ** (-0.5) *",
            "-                               min(step ** (-0.5), step * self.warmup_steps ** (-1.5)))",
            "+        scale = self.factor * (",
            "+            self.model_size ** (-0.5) * min(step ** (-0.5), step * self.warmup_steps ** (-1.5))",
            "+        )",
            "",
            "return [scale for _ in range(len(self.base_values))]"
        ]
    },
    {
        "number": 606,
        "comments": "add param for argument fix",
        "commit_message": "bugfix VirtualWorker\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def tests_worker_convenience_methods():",
            "\"\"\"",
            "",
            "me = sy.torch.hook.local_worker",
            "-    bob = VirtualWorker()",
            "-    alice = VirtualWorker()",
            "+    bob = VirtualWorker(sy.torch.hook)",
            "+    alice = VirtualWorker(sy.torch.hook)",
            "obj = torch.Tensor([100, 100])",
            "",
            "# Send data to alice"
        ]
    },
    {
        "number": 609,
        "comments": "refactor",
        "commit_message": "fix multi-singer duration predictor\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class DurationPredictor(torch.nn.Module):",
            "self.norm_2 = LayerNorm(filter_channels, dim=1)",
            "self.proj = torch.nn.Conv1d(filter_channels, 1, 1)",
            "",
            "-        if gin_channels != 0:",
            "-            self.cond = torch.nn.Conv1d(gin_channels, channels, 1)",
            "+        if global_channels > 0:",
            "+            self.cond = torch.nn.Conv1d(global_channels, channels, 1)",
            "",
            "def forward(self, x, x_mask, beat_lab, g=None):",
            "x = torch.detach(x)"
        ]
    },
    {
        "number": 610,
        "comments": "add API call for type fix",
        "commit_message": "fix dtype, device in `sum`, `prod`, `to_dev` (#1358)\n\n* fix dtype, device in `sum`, `prod`, `to_dev`\n\n* make `device` have `None` as default\n\n* add `dtype = ivy.as_native_dtype(dtype)` and make `copy` positional\n\n* `to_dev` conform to array API\n\n* `astype` fixes to signature\n\n* black\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def prod(",
            "elif x.dtype == torch.bfloat16:",
            "dtype = torch.float16",
            "",
            "+    dtype = ivy.as_native_dtype(dtype)",
            "+",
            "if axis is None:",
            "axis = x.dim() - 1",
            "elif type(axis) == tuple:"
        ]
    },
    {
        "number": 611,
        "comments": "remove API call for math fix",
        "commit_message": "change the bitwise for masking and small fixes\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class Attention(nn.Module):",
            "query, processed_inputs)",
            "# apply masking",
            "if mask is not None:",
            "-            attention.data.masked_fill_(torch.bitwise_not(mask), self._mask_value)",
            "+            attention.data.masked_fill_(~mask, self._mask_value)",
            "# apply windowing - only in eval mode",
            "if not self.training and self.windowing:",
            "attention = self.apply_windowing(attention, inputs)"
        ]
    },
    {
        "number": 613,
        "comments": "add API call for resource fix",
        "commit_message": "Fix(Early Stopping): move best score to device (#7959)\n\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class EarlyStopping(Callback):",
            "f\" {self.monitor} = {current} {self.order_dict[self.mode]} {self.divergence_threshold}.\"",
            "\" Signaling Trainer to stop.\"",
            ")",
            "-        elif self.monitor_op(current - self.min_delta, self.best_score):",
            "+        elif self.monitor_op(current - self.min_delta, self.best_score.to(trainer.lightning_module.device)):",
            "should_stop = False",
            "reason = self._improvement_message(current)",
            "self.best_score = current"
        ]
    },
    {
        "number": 614,
        "comments": "update API call for refactor fix",
        "commit_message": "Replaced discontinued rnn_cell.BasicLSTMCell with rnn_cell.LSTMCell (#4703)\n\n* Fixed bug in Dirichlet (#4440)\n\n* Replaced deprecated rnn_cell.BasicLSTMCell with rnn_cell.LSTMCell\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class LSTM(Model):",
            "last_layer = add_time_dimension(features, self.seq_lens)",
            "",
            "# Setup the LSTM cell",
            "-        lstm = rnn.BasicLSTMCell(cell_size, state_is_tuple=True)",
            "+        lstm = tf.nn.rnn_cell.LSTMCell(cell_size, state_is_tuple=True)",
            "self.state_init = [",
            "np.zeros(lstm.state_size.c, np.float32),",
            "np.zeros(lstm.state_size.h, np.float32)"
        ]
    },
    {
        "number": 615,
        "comments": "add API call for type fix",
        "commit_message": "Fx support for multiple model architectures (#17393)\n\n* Support for Bart and LayoutLM, and partial support for XLNet\n\n* Support for mbart\n\n* A lot of new models supported\n\n* Support for other models\n\n* LayoutLM fix\n\n* Use strings instead of classes\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class XGLMModel(XGLMPreTrainedModel):",
            "",
            "hidden_states = inputs_embeds + positions",
            "",
            "-        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)",
            "+        hidden_states = nn.functional.dropout(hidden_states, p=float(self.dropout), training=self.training)",
            "",
            "# decoder layers",
            "all_hidden_states = () if output_hidden_states else None"
        ]
    },
    {
        "number": 616,
        "comments": "print",
        "commit_message": "small fix\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class VersatileDiffusionImageVariationPipelineIntegrationTests(unittest.TestCase",
            "image_slice = image[0, 253:256, 253:256, -1]",
            "",
            "assert image.shape == (1, 512, 512, 3)",
            "+        print(torch.from_numpy(image_slice.flatten()))",
            "expected_slice = np.array([0.0113, 0.2241, 0.4024, 0.0839, 0.0871, 0.2725, 0.2581, 0.0, 0.1096])",
            "assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-2"
        ]
    },
    {
        "number": 618,
        "comments": "test fix",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class CategoricalAccuracy(Metric):",
            "correct.unsqueeze_(-1)",
            "",
            "if mask is not None:",
            "-            correct *= mask.view(-1, 1).float()",
            "+            correct *= mask.view(-1, 1)",
            "self.total_count += mask.sum()",
            "else:",
            "self.total_count += gold_labels.numel()"
        ]
    },
    {
        "number": 619,
        "comments": "add condition check for math fix",
        "commit_message": "fix unconditional after simplification of text mask\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class Imagen(nn.Module):",
            "text_embeds, text_masks = t5_encode_text(texts, name = self.text_encoder_name, return_attn_mask = True)",
            "text_embeds, text_masks = map(lambda t: t.to(images.device), (text_embeds, text_masks))",
            "",
            "-        text_masks = default(text_masks, lambda: torch.any(text_embeds != 0., dim = -1))",
            "+        if not self.unconditional:",
            "+            text_masks = default(text_masks, lambda: torch.any(text_embeds != 0., dim = -1))",
            "",
            "assert not (self.condition_on_text and not exists(text_embeds)), 'text or text encodings must be passed into decoder if specified'",
            "assert not (not self.condition_on_text and exists(text_embeds)), 'decoder specified not to be conditioned on text, yet it is presented'"
        ]
    },
    {
        "number": 621,
        "comments": "change API call for refactor fix",
        "commit_message": "Update to PyTorch 1.11.0 (#3045)\n\n* Run black\n\n* Fix ProvenanceTensor\n\n* Replace torch.triangular_solve -> torch.linalg.solve_triangular\n\n* More fixes to torch.linalg.solve_triangular\n\n* Bump torch version\n\n* Bump Python version 3.6 -> 3.7\n\n* Fix some tests\n\n* Decrease tolerance on stable tests\n\n* Remove obsolete xfail\n\n* Resolve #3032\n\n* Fix solve_triangular in ops.gaussian, revert test weakening\n\n* Fix catching of singular matrices in hmc\n\n* xfail funsor tests\n\n* Work around pandas 1.3 bug\n\n* Allow mypy to install missing stubs\n\n* Clarify triangular_solve test\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def conditional(",
            "if f_scale_tril is not None:",
            "pack = torch.cat((pack, f_scale_tril_2D), dim=1)",
            "",
            "-        Lffinv_pack = pack.triangular_solve(Lff, upper=False)[0]",
            "+        Lffinv_pack = torch.linalg.solve_triangular(Lff, pack, upper=False)",
            "# unpack",
            "v_2D = Lffinv_pack[:, : f_loc_2D.size(1)]",
            "W = Lffinv_pack[:, f_loc_2D.size(1) : f_loc_2D.size(1) + M].t()"
        ]
    },
    {
        "number": 622,
        "comments": "remove try catch test",
        "commit_message": "Test fixtures (#4241)\n\n* fix test\n\n* typo\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* skip tests\n\n* run cron only in master repo\n\n* fix test\n\n* update\n\n* Add test\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def test_hgt_loader_on_cora():",
            "out2 = hetero_model(hetero_batch.x_dict, hetero_batch.edge_index_dict,",
            "hetero_batch.edge_weight_dict)['paper'][:batch_size]",
            "assert torch.allclose(out1, out2, atol=1e-6)",
            "-",
            "-    try:",
            "-        shutil.rmtree(root)",
            "-    except PermissionError:",
            "-        pass"
        ]
    },
    {
        "number": 623,
        "comments": "update API call for version fix",
        "commit_message": "fix use of deprecated TF functions.\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class Model(ModelDesc):",
            "wrong = prediction_incorrect(logits, label, 5, name='wrong-top5')",
            "add_moving_summary(tf.reduce_mean(wrong, name='train-error-top5'))",
            "",
            "-        wd_cost = tf.mul(1e-4, regularize_cost('.*/W', tf.nn.l2_loss), name='l2_regularize_loss')",
            "+        wd_cost = regularize_cost('.*/W', l2_regularizer(1e-4), name='l2_regularize_loss')",
            "add_moving_summary(loss, wd_cost)",
            "self.cost = tf.add_n([loss, wd_cost], name='cost')"
        ]
    },
    {
        "number": 625,
        "comments": "format",
        "commit_message": "CI fix3\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class StochasticDurationPredictor(torch.nn.Module):",
            "z, logdet = flow(z, x_mask, g=x, inverse=inverse)",
            "logdet_tot = logdet_tot + logdet",
            "nll = (",
            "-                torch.sum(0.5 * (math.log(2 * math.pi) + (z**2)) * x_mask, [1, 2])",
            "+                torch.sum(0.5 * (math.log(2 * math.pi) + (z ** 2)) * x_mask, [1, 2])",
            "- logdet_tot",
            ")",
            "return nll + logq  # (B,)"
        ]
    },
    {
        "number": 626,
        "comments": "annotation",
        "commit_message": "Add metrics usage examples and tests (#1820)\n\n* add metrics usage examples and tests\n\n* update template\n\n* remove instruction sentence\n\n* add metrics dependencies to the tests requirements\n\n* try fix pip install timeout\n\n* try again\n\n* try again\n\n* try again\n\n* try again by moving unbabel-comet\n\n* try again by ignoring fixed deps of comet\n\n* fix some comet deps\n\n* move deps\n\n* try again\n\n* download wordnet for meteor\n\n* style\n\n* don't test comet on windows\n\n* style\n\n* remove comet comment\n",
        "label": "no",
        "answer": "no",
        "change": [
            "CHECKPOINT_URLS = {",
            "}",
            "",
            "",
            "+@datasets.utils.file_utils.add_start_docstrings(_DESCRIPTION, _KWARGS_DESCRIPTION)",
            "class BLEURT(datasets.Metric):",
            "def _info(self):"
        ]
    },
    {
        "number": 627,
        "comments": "update API call for version fix",
        "commit_message": "Fix nccl package location on newer TF versions.\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class Optimizer:",
            "g = [dev_grads[dev][var_idx][0] for dev in devices]",
            "",
            "if np.prod(grad_shape):  # nccl does not support zero-sized tensors",
            "-                            g = tf.contrib.nccl.all_sum(g)",
            "+                            g = nccl_ops.all_sum(g)",
            "",
            "for dev, gg in zip(devices, g):",
            "dev_grads[dev][var_idx] = (gg, dev_grads[dev][var_idx][1])"
        ]
    },
    {
        "number": 631,
        "comments": "doc",
        "commit_message": "CDN urls (#4030)\n\n* [file_utils] use_cdn + documentation\n\n* Move to cdn. urls for weights\n\n* [urls] Hotfix for bert-base-japanese\n",
        "label": "no",
        "answer": "no",
        "change": [
            "from .modeling_utils import PoolerAnswerClass, PoolerEndLogits, PoolerStartLogit",
            "logger = logging.getLogger(__name__)",
            "",
            "XLNET_PRETRAINED_MODEL_ARCHIVE_MAP = {",
            "-    \"xlnet-base-cased\": \"https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-pytorch_model.bin\",",
            "-    \"xlnet-large-cased\": \"https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-large-cased-pytorch_model.bin\",",
            "+    \"xlnet-base-cased\": \"https://cdn.huggingface.co/xlnet-base-cased-pytorch_model.bin\",",
            "+    \"xlnet-large-cased\": \"https://cdn.huggingface.co/xlnet-large-cased-pytorch_model.bin\",",
            "}"
        ]
    },
    {
        "number": 633,
        "comments": "change param for type fix",
        "commit_message": "fix\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class Decoder(torch.nn.Module, ScorerInterface):",
            "",
            "if self.labeldist is not None:",
            "if self.vlabeldist is None:",
            "-                self.vlabeldist = to_device(hs_pad, torch.from_numpy(self.labeldist))",
            "+                self.vlabeldist = to_device(hs_pad[0], torch.from_numpy(self.labeldist))",
            "loss_reg = -torch.sum(",
            "(F.log_softmax(y_all, dim=1) * self.vlabeldist).view(-1), dim=0",
            ") / len(ys_in)"
        ]
    },
    {
        "number": 635,
        "comments": "change param for math fix",
        "commit_message": "Fix Transformer init_weights\n\nWeights of decoders are initialized twice. I think we need to call the zeros_() to initialize bias.\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class TransformerModel(nn.Module):",
            "def init_weights(self):",
            "initrange = 0.1",
            "nn.init.uniform_(self.encoder.weight, -initrange, initrange)",
            "-        nn.init.zeros_(self.decoder.weight)",
            "+        nn.init.zeros_(self.decoder.bias)",
            "nn.init.uniform_(self.decoder.weight, -initrange, initrange)",
            "",
            "def forward(self, src, has_mask=True):"
        ]
    },
    {
        "number": 636,
        "comments": "no API",
        "commit_message": "FIX errors in loading eval Dataset in `run_squad_pytorch`\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def main():",
            "",
            "model.eval()",
            "all_results = []",
            "-        for input_ids, input_mask, segment_ids, label_ids, example_index in eval_dataloader:",
            "+        #for input_ids, input_mask, segment_ids, label_ids, example_index in eval_dataloader:",
            "+        for input_ids, input_mask, segment_ids, example_index in eval_dataloader:",
            "if len(all_results) % 1000 == 0:",
            "logger.info(\"Processing example: %d\" % (len(all_results)))"
        ]
    },
    {
        "number": 637,
        "comments": "change param for type fix",
        "commit_message": "CrypTen Message Handler (#3676)\n\n* Fix plans framework\n\n* CrypTen Message Handler\n\n* Test fixup\n\n* Black fixup\n\n* Remove unused/undefined functions\n\n* Move message registration to worker level\n\n* Lint\n\n* Use global msgpack\n\n* factorize worker_id to rank translation\n\n* fix sequential model with last version of crypten\n\nCo-authored-by: youben11 <ayouben9@gmail.com>\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "import syft",
            "def model():",
            "l_in, l_h, l_out = 32, 16, 2",
            "model = crypten.nn.Sequential(",
            "-        [crypten.nn.Linear(l_in, l_h), crypten.nn.ReLU(), crypten.nn.Linear(l_h, l_out)]",
            "+        crypten.nn.Linear(l_in, l_h), crypten.nn.ReLU(), crypten.nn.Linear(l_h, l_out)",
            ")",
            "return model"
        ]
    },
    {
        "number": 639,
        "comments": "add API call for state fix",
        "commit_message": "Added functionality for retrieving variables from control dependencies (#220)\n\n* Added test for retriving variables from an optimizer\n\n* Added comments to test\n\n* Addressed comments\n\n* Fixed travis bug\n\n* Added fix to circular controls\n\n* Added set for explored operations and duplicate prefix stripping\n\n* Removed embeded ipython\n\n* Removed prefix, use seperate graph for each network\n\n* Removed redundant imports\n\n* Addressed comments and added separate graph to initializer\n\n* fix typos\n\n* get rid of prefix in documentation\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class LinearModel(object):",
            "return self.sess.run(self.cross_entropy_grads, feed_dict={self.x: xs, self.y_: ys})",
            "",
            "def net_initialization():",
            "-  return LinearModel([784,10])",
            "+  with tf.Graph().as_default():",
            "+    return LinearModel([784,10])",
            "",
            "# By default, when an environment variable is used by a remote function, the",
            "# initialization code will be rerun at the end of the remote task to ensure"
        ]
    },
    {
        "number": 640,
        "comments": "refacotr",
        "commit_message": "Fix for incorrect usage of detach(), cpu(), to() (#6216)\n\n* Fix for incorrect detach/cpu calls (#6214)\n\n* Fix incorrect use of detach(), to(), and cpu(), #6214\n\n* Fix incorrect use of detach() and cpu(), #6214\n\n* update pr\n\n* add typing\n\n* chlog\n\n* more...\n\n* revert on module\n\n* update on comments\n\n* revert changes on model\n\nCo-authored-by: tchaton <thomas@grid.ai>\nCo-authored-by: Jirka Borovec <jirka.borovec@seznam.cz>\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "class EpochResultStore:",
            "# attach capture batch_size",
            "Result.attach_batch_size(self._batch_size, hook_result)",
            "",
            "-            hook_result.detach()",
            "+            hook_result = hook_result.detach()",
            "if self.trainer.move_metrics_to_cpu:",
            "-                hook_result.cpu()",
            "+                hook_result = hook_result.cpu()",
            "elif self.trainer._distrib_type == DistributedType.DP:",
            "-                hook_result.to(torch.device(\"cuda\", self.trainer.root_gpu))",
            "+                hook_result = hook_result.to(torch.device(\"cuda\", self.trainer.root_gpu))",
            "",
            "self._internals[fx_name].append(hook_result, info)"
        ]
    },
    {
        "number": 641,
        "comments": "add param for argument fix",
        "commit_message": "Fix issue #2026\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class LinearRegression(d2l.Module):",
            "def __init__(self, lr):",
            "super().__init__()",
            "self.save_hyperparameters()",
            "-        self.net = tf.keras.layers.Dense(1)",
            "+        initializer = tf.initializers.RandomNormal(stddev=0.01)",
            "+        self.net = tf.keras.layers.Dense(1, kernel_initializer=initializer)",
            "",
            "def forward(self, X):",
            "\"\"\"The linear regression model."
        ]
    },
    {
        "number": 642,
        "comments": "add param for version fix",
        "commit_message": "[RLlib] SAC add discrete action support. (#7320)\n\n* Exploration API (+EpsilonGreedy sub-class).\n\n* Exploration API (+EpsilonGreedy sub-class).\n\n* Cleanup/LINT.\n\n* Add `deterministic` to generic Trainer config (NOTE: this is still ignored by most Agents).\n\n* Add `error` option to deprecation_warning().\n\n* WIP.\n\n* Bug fix: Get exploration-info for tf framework.\nBug fix: Properly deprecate some DQN config keys.\n\n* WIP.\n\n* LINT.\n\n* WIP.\n\n* Split PerWorkerEpsilonGreedy out of EpsilonGreedy.\nDocstrings.\n\n* Fix bug in sampler.py in case Policy has self.exploration = None\n\n* Update rllib/agents/dqn/dqn.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* WIP.\n\n* Update rllib/agents/trainer.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* WIP.\n\n* Change requests.\n\n* LINT\n\n* In tune/utils/util.py::deep_update() Only keep deep_updat'ing if both original and value are dicts. If value is not a dict, set\n\n* Completely obsolete syn_replay_optimizer.py's parameters schedule_max_timesteps AND beta_annealing_fraction (replaced with prioritized_replay_beta_annealing_timesteps).\n\n* Update rllib/evaluation/worker_set.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* Review fixes.\n\n* Fix default value for DQN's exploration spec.\n\n* LINT\n\n* Fix recursion bug (wrong parent c'tor).\n\n* Do not pass timestep to get_exploration_info.\n\n* Update tf_policy.py\n\n* Fix some remaining issues with test cases and remove more deprecated DQN/APEX exploration configs.\n\n* Bug fix tf-action-dist\n\n* DDPG incompatibility bug fix with new DQN exploration handling (which is imported by DDPG).\n\n* Switch off exploration when getting action probs from off-policy-estimator's policy.\n\n* LINT\n\n* Fix test_checkpoint_restore.py.\n\n* Deprecate all SAC exploration (unused) configs.\n\n* Properly use `model.last_output()` everywhere. Instead of `model._last_output`.\n\n* WIP.\n\n* Take out set_epsilon from multi-agent-env test (not needed, decays anyway).\n\n* WIP.\n\n* Trigger re-test (flaky checkpoint-restore test).\n\n* WIP.\n\n* WIP.\n\n* Add test case for deterministic action sampling in PPO.\n\n* bug fix.\n\n* Added deterministic test cases for different Agents.\n\n* Fix problem with TupleActions in dynamic-tf-policy.\n\n* Separate supported_spaces tests so they can be run separately for easier debugging.\n\n* LINT.\n\n* Fix autoregressive_action_dist.py test case.\n\n* Re-test.\n\n* Fix.\n\n* Remove duplicate py_test rule from bazel.\n\n* LINT.\n\n* WIP.\n\n* WIP.\n\n* SAC fix.\n\n* SAC fix.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* FIX 2 examples tests.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* Fix.\n\n* LINT.\n\n* Renamed test file.\n\n* WIP.\n\n* Add unittest.main.\n\n* Make action_dist_class mandatory.\n\n* fix\n\n* FIX.\n\n* WIP.\n\n* WIP.\n\n* Fix.\n\n* Fix.\n\n* Fix explorations test case (contextlib cannot find its own nullcontext??).\n\n* Force torch to be installed for QMIX.\n\n* LINT.\n\n* Fix determine_tests_to_run.py.\n\n* Fix determine_tests_to_run.py.\n\n* WIP\n\n* Add Random exploration component to tests (fixed issue with \"static-graph randomness\" via py_function).\n\n* Add Random exploration component to tests (fixed issue with \"static-graph randomness\" via py_function).\n\n* Rename some stuff.\n\n* Rename some stuff.\n\n* WIP.\n\n* update.\n\n* WIP.\n\n* Gumbel Softmax Dist.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP\n\n* WIP.\n\n* WIP.\n\n* Hypertune.\n\n* Hypertune.\n\n* Hypertune.\n\n* Lock-in.\n\n* Cleanup.\n\n* LINT.\n\n* Fix.\n\n* Update rllib/policy/eager_tf_policy.py\n\nCo-Authored-By: Kristian Hartikainen <kristian.hartikainen@gmail.com>\n\n* Update rllib/agents/sac/sac_policy.py\n\nCo-Authored-By: Kristian Hartikainen <kristian.hartikainen@gmail.com>\n\n* Update rllib/agents/sac/sac_policy.py\n\nCo-Authored-By: Kristian Hartikainen <kristian.hartikainen@gmail.com>\n\n* Update rllib/models/tf/tf_action_dist.py\n\nCo-Authored-By: Kristian Hartikainen <kristian.hartikainen@gmail.com>\n\n* Update rllib/models/tf/tf_action_dist.py\n\nCo-Authored-By: Kristian Hartikainen <kristian.hartikainen@gmail.com>\n\n* Fix items from review comments.\n\n* Add dm_tree to RLlib dependencies.\n\n* Add dm_tree to RLlib dependencies.\n\n* Fix DQN test cases ((Torch)Categorical).\n\n* Fix wrong pip install.\n\nCo-authored-by: Eric Liang <ekhliang@gmail.com>\nCo-authored-by: Kristian Hartikainen <kristian.hartikainen@gmail.com>\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class TorchCategorical(TorchDistributionWrapper):",
            "\"\"\"Wrapper class for PyTorch Categorical distribution.\"\"\"",
            "",
            "@override(ActionDistribution)",
            "-    def __init__(self, inputs, model):",
            "-        super().__init__(inputs, model)",
            "-        self.dist = torch.distributions.categorical.Categorical(logits=inputs)",
            "+    def __init__(self, inputs, model=None, temperature=1.0):",
            "+        assert temperature > 0.0, \"Categorical `temperature` must be > 0.0!\"",
            "+        super().__init__(inputs / temperature, model)",
            "+        self.dist = torch.distributions.categorical.Categorical(",
            "+            logits=self.inputs)",
            "",
            "@override(ActionDistribution)",
            "def deterministic_sample(self):"
        ]
    },
    {
        "number": 643,
        "comments": "add API call for type fix",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class TestPretrainedTransformerEmbedder(AllenNlpTestCase):",
            "def test_xlnet_token_type_ids(self):",
            "token_embedder = PretrainedTransformerEmbedder(\"xlnet-base-cased\")",
            "token_ids = torch.LongTensor([[1, 2, 3], [2, 3, 4]])",
            "-        mask = torch.ones_like(token_ids)",
            "+        mask = torch.ones_like(token_ids).bool()",
            "type_ids = torch.zeros_like(token_ids)",
            "type_ids[1, 1] = 1",
            "token_embedder(token_ids, mask, type_ids)"
        ]
    },
    {
        "number": 645,
        "comments": "change param for resource fix",
        "commit_message": "Fix bug in block layer\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class Block(Layer):",
            "layer_counter[layer_type] += 1",
            "",
            "# layer_name = self.name + '-' + layer_name",
            "-            self.layers[n] = self.submodule(",
            "+            layer = self.submodule(",
            "name=layer_name, module=layer_spec, modules=tensorforce.core.layer_modules,",
            "input_spec=self._input_spec",
            ")",
            "-            self._input_spec = self.layers[n].output_spec()",
            "-",
            "+            self.layers.append(layer)",
            "+            self._input_spec = layer.output_spec()",
            "",
            "return self.layers[0].input_spec.copy()"
        ]
    },
    {
        "number": 648,
        "comments": "add param for resource fix",
        "commit_message": "Fix Approx NN on devices other than CUDA\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def model():",
            "",
            "if sd_vae_approx_model is None:",
            "sd_vae_approx_model = VAEApprox()",
            "-        sd_vae_approx_model.load_state_dict(torch.load(os.path.join(paths.models_path, \"VAE-approx\", \"model.pt\")))",
            "+        sd_vae_approx_model.load_state_dict(torch.load(os.path.join(paths.models_path, \"VAE-approx\", \"model.pt\"), map_location='cpu' if devices.device.type != 'cuda' else None))",
            "sd_vae_approx_model.eval()",
            "sd_vae_approx_model.to(devices.device, devices.dtype)"
        ]
    },
    {
        "number": 650,
        "comments": "customized API",
        "commit_message": "[Metrics] Confusion matrix class interface (#4348)\n\n* docs + precision + recall + f_beta + refactor\n\nCo-authored-by: Teddy Koker <teddy.koker@gmail.com>\n\n* rebase\n\nCo-authored-by: Teddy Koker <teddy.koker@gmail.com>\n\n* fixes\n\nCo-authored-by: Teddy Koker <teddy.koker@gmail.com>\n\n* added missing file\n\n* docs\n\n* docs\n\n* extra import\n\n* add confusion matrix\n\n* add to docs\n\n* add test\n\n* pep8 + isort\n\n* update tests\n\n* move util function\n\n* unify functional and class\n\n* add to init\n\n* remove old implementation\n\n* update tests\n\n* pep8\n\n* add duplicate\n\n* fix doctest\n\n* Update pytorch_lightning/metrics/classification/confusion_matrix.py\n\nCo-authored-by: Justus Schock <12886177+justusschock@users.noreply.github.com>\n\n* changelog\n\n* bullet point args\n\n* bullet docs\n\n* bullet docs\n\nCo-authored-by: ananyahjha93 <ananya@pytorchlightning.ai>\nCo-authored-by: Teddy Koker <teddy.koker@gmail.com>\nCo-authored-by: Justus Schock <12886177+justusschock@users.noreply.github.com>\nCo-authored-by: chaton <thomas@grid.ai>\nCo-authored-by: Roger Shieh <55400948+s-rog@users.noreply.github.com>\nCo-authored-by: Rohit Gupta <rohitgr1998@gmail.com>\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "class Accuracy(Metric):",
            "preds: Predictions from model",
            "target: Ground truth values",
            "\"\"\"",
            "-        preds, target = self._input_format(preds, target)",
            "+        preds, target = _input_format_classification(preds, target, self.threshold)",
            "assert preds.shape == target.shape",
            "",
            "self.correct += torch.sum(preds == target)"
        ]
    },
    {
        "number": 651,
        "comments": "remove state fix check",
        "commit_message": "add pytest fixture as workaround for set_grad_enabled\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def test_tacotron2_trainable_and_decodable(model_dict, loss_dict):",
            "assert att_ws.shape[0] == bs",
            "assert att_ws.shape[1] == max(olens)",
            "assert att_ws.shape[2] == max(ilens)",
            "-    if not torch_is_old:",
            "-        torch.set_grad_enabled(True)"
        ]
    },
    {
        "number": 653,
        "comments": "doc update",
        "commit_message": "fixed typos in processing.py\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def create_random_tensors(shape, seeds, subseeds=None, subseed_strength=0.0, see",
            "",
            "# if we have multiple seeds, this means we are working with batch size>1; this then",
            "# enables the generation of additional tensors with noise that the sampler will use during its processing.",
            "-    # Using those pre-genrated tensors instead of siimple torch.randn allows a batch with seeds [100, 101] to",
            "+    # Using those pre-generated tensors instead of simple torch.randn allows a batch with seeds [100, 101] to",
            "# produce the same images as with two batches [100], [101].",
            "if p is not None and p.sampler is not None and len(seeds) > 1 and opts.enable_batch_seeds:",
            "sampler_noises = [[] for _ in range(p.sampler.number_of_needed_noises(p))]"
        ]
    },
    {
        "number": 654,
        "comments": "remove API call for resource fix",
        "commit_message": "Bug fix for norm calculation in absence of model parallel group (#551)\n\nIn the absence of a model parallel group, model_parallel_allreduce should not do any reduction. This commit fixes the bug which was doing a model parallel allreduce across world group when model parallel group is None\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class FP16_DeepSpeedZeroOptimizer(object):",
            "\"\"\" Perform all reduce within model parallel group, if any.",
            "\"\"\"",
            "if self.model_parallel_group is None:",
            "-            torch.distributed.all_reduce(tensor=tensor, op=op)",
            "+            pass",
            "else:",
            "torch.distributed.all_reduce(tensor=tensor,",
            "op=op,"
        ]
    },
    {
        "number": 655,
        "comments": "change condition check for state fix",
        "commit_message": "[T5] Fix speed degradation bug t5 (#10496)\n\n* fix speed degradation bug t5\n\n* fix for all models\n\n* fix code quality\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class {{cookiecutter.camelcase_modelname}}EncoderLayer(nn.Module):",
            "hidden_states = residual + hidden_states",
            "hidden_states = self.final_layer_norm(hidden_states)",
            "",
            "-        if torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any():",
            "+        if hidden_states.dtype == torch.float16 and (torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any()):",
            "clamp_value = torch.finfo(hidden_states.dtype).max - 1000",
            "hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)"
        ]
    },
    {
        "number": 656,
        "comments": "change API call for refactor fix",
        "commit_message": "[style] consistent nn. and nn.functional (#12124)\n\n* consistent nn. and nn.functional\n\n* fix glitch\n\n* fix glitch #2\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class LabelSmoother:",
            "",
            "def __call__(self, model_output, labels):",
            "logits = model_output[\"logits\"] if isinstance(model_output, dict) else model_output[0]",
            "-        log_probs = -torch.nn.functional.log_softmax(logits, dim=-1)",
            "+        log_probs = -nn.functional.log_softmax(logits, dim=-1)",
            "if labels.dim() == log_probs.dim() - 1:",
            "labels = labels.unsqueeze(-1)"
        ]
    },
    {
        "number": 657,
        "comments": "add API call for type fix",
        "commit_message": "fix mypy errors\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class NonMaximaSuppression2d(nn.Module):",
            "def forward(self, x: torch.Tensor) -> torch.Tensor:  # type: ignore",
            "assert len(x.shape) == 4, x.shape",
            "# find local maximum values",
            "-        x_max: torch.Tensor = self.max_pool2d(x)",
            "+        x_max: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]] = \\",
            "+            self.max_pool2d(x)",
            "",
            "# create mask for maximums in the original map",
            "x_mask: torch.Tensor = torch.where("
        ]
    },
    {
        "number": 658,
        "comments": "test fix",
        "commit_message": "Drop JIT support for `core.check`, `Boxes`, and others (#2219)\n\n* Drop JIT support for `core.check` API\n\n- Consequently for this, we drop support of JIT on the following items: (in of dynamo)\n  - enhance\n    - AdjustSigmoid\n    - AdjustLog\n    - AddWeighted\n  - geometry\n    - UndistortPoints\n    - bbox and Boxes - follow up on #2218\n    - EuclideanDistance\n    - TransformPoints\n    - HomographyWarper\n    - WarpPerspective\n    - UpscaleDouble\n  - losses\n\n* Update typing with pyupgrade\n* drop all jit related from bbox and boxes\n\nfrom #2218\n* fix/skip failing dynamo tests\n* fix loss hd\n* fix typing\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "class TestAdjustLog(BaseTester):",
            "f = kornia.enhance.AdjustLog()",
            "self.assert_close(f(data), expected)",
            "",
            "-    @pytest.mark.jit",
            "-    def test_jit(self, device, dtype):",
            "+    def test_dynamo(self, device, dtype, torch_optimizer):",
            "B, C, H, W = 2, 3, 4, 4",
            "img = torch.ones(B, C, H, W, device=device, dtype=dtype)",
            "op = kornia.enhance.adjust_log",
            "-        op_jit = torch.jit.script(op)",
            "-        self.assert_close(op(img), op_jit(img))",
            "+        op_optimized = torch_optimizer(op)",
            "+        self.assert_close(op(img), op_optimized(img))",
            "",
            "@pytest.mark.grad",
            "def test_gradcheck(self, device, dtype):"
        ]
    },
    {
        "number": 659,
        "comments": "add param for math fix",
        "commit_message": "Adding normalization bias verification (#4990)\n\n* adding batchnorm verification\n\n* Adding trainer callback\n\n* updating changelog\n\n* renaming class\n\n* detailed message for sanity check\n\n* run sanity checks by default\n\n* fix normalization bias issue in image embeddings\n\n* update docstring\n\n* fix test\n\nCo-authored-by: Dirk Groeneveld <dirkg@allenai.org>\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class ImageFeatureEmbeddings(Embeddings):",
            "",
            "def __init__(self, feature_size: int, embedding_size: int, dropout: float = 0.0):",
            "image_embeddings = torch.nn.Linear(feature_size, embedding_size)",
            "-        location_embeddings = torch.nn.Linear(4, embedding_size)",
            "+        location_embeddings = torch.nn.Linear(4, embedding_size, bias=False)",
            "embeddings = torch.nn.ModuleDict(",
            "{\"image_embeddings\": image_embeddings, \"location_embeddings\": location_embeddings}",
            ")"
        ]
    },
    {
        "number": 660,
        "comments": "format not clear deep",
        "commit_message": "fix linknet sum operation\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class DecoderBlock(nn.Module):",
            "x, skip = x",
            "x = self.block(x)",
            "if skip is not None:",
            "-            x += skip",
            "+            x = x + skip",
            "return x"
        ]
    },
    {
        "number": 661,
        "comments": "update API call for version fix",
        "commit_message": "fix deprecation about dropout; fix Keras compatibility in tf1.13\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class Model(ModelDesc):",
            ".apply(fg)",
            ".BatchNorm('bn5').apply(activate)",
            "# 5",
            "-                      .tf.nn.dropout(0.5 if is_training else 1.0)",
            "+                      .Dropout(rate=0.5 if is_training else 0.0)",
            ".Conv2D('conv6', 512, 5, padding='VALID')",
            ".apply(fg).BatchNorm('bn6')",
            ".apply(nonlin)"
        ]
    },
    {
        "number": 662,
        "comments": "format",
        "commit_message": "Add linting with black (#2678)\n\nSummary:\n# Before submitting\n\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\n- [ ] Did you make sure to update the docs?\n- [ ] Did you write any new necessary tests?\n\n## What does this PR do?\nFixes # (issue).\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding \ufffd\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2678\n\nReviewed By: Mortimerp9\n\nDifferential Revision: D32653381\n\nPulled By: dianaml0\n\nfbshipit-source-id: 2810d14867cd7d64f4d340740e2b590b82de47fe\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class LocationAttention(nn.Module):",
            "self.proj_enc = nn.Linear(encoder_dim, attn_dim)",
            "self.proj_dec = nn.Linear(decoder_dim, attn_dim, bias=False)",
            "self.proj_attn = nn.Linear(conv_dim, attn_dim, bias=False)",
            "-        self.conv = nn.Conv1d(attn_state_kernel_size, conv_dim,",
            "-                              2 * conv_kernel_size + 1,",
            "-                              padding=conv_kernel_size, bias=False)",
            "+        self.conv = nn.Conv1d(",
            "+            attn_state_kernel_size,",
            "+            conv_dim,",
            "+            2 * conv_kernel_size + 1,",
            "+            padding=conv_kernel_size,",
            "+            bias=False,",
            "+        )",
            "self.proj_out = nn.Sequential(nn.Tanh(), nn.Linear(attn_dim, 1))",
            "",
            "self.proj_enc_out = None  # cache"
        ]
    },
    {
        "number": 663,
        "comments": "no API fount",
        "commit_message": "Fixed module resolution for tf.keras optimizers and added unit tests (#1935)\n\nSigned-off-by: Travis Addair <taddair@uber.com>\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def load_model(filepath, custom_optimizers=None, custom_objects=None, compressio",
            "\"\"\"",
            "def wrap_optimizer(cls):",
            "return lambda **kwargs: DistributedOptimizer(cls(**kwargs), compression=compression)",
            "-    return _impl.load_model(keras, wrap_optimizer, filepath, custom_optimizers, custom_objects)",
            "+    return _impl.load_model(keras, wrap_optimizer, _OPTIMIZER_MODULES, filepath, custom_optimizers, custom_objects)"
        ]
    },
    {
        "number": 664,
        "comments": "custom method",
        "commit_message": "Fixing some linear algebra tests (#7418)\n\n* minor fix for cholesky test for jax\n\n* eigh:UPLO update inst. + test fix(to-do bug fix)\n\n* minor fix to matrix_power test\n\n* added container and static methods for eigh\n\n* updated positional args + out arg for qr\n\n* updated test_qr\n\n* fixed svd test\n\n* fixed tensordot pos args issue\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "def pinv(",
            "",
            "",
            "@with_unsupported_dtypes({\"2.9.1 and below\": (\"float16\", \"bfloat16\")}, backend_version)",
            "-def qr(x: Union[tf.Tensor, tf.Variable], /, *, mode: str = \"reduced\") -> NamedTuple:",
            "+def qr(",
            "+    x: Union[tf.Tensor, tf.Variable],",
            "+    /,",
            "+    *,",
            "+    mode: str = \"reduced\",",
            "+    out: Optional[tf.Tensor] = None,",
            "+) -> NamedTuple:",
            "res = namedtuple(\"qr\", [\"Q\", \"R\"])",
            "if mode == \"reduced\":",
            "q, r = tf.linalg.qr(x, full_matrices=False)"
        ]
    },
    {
        "number": 665,
        "comments": "change param for type fix",
        "commit_message": "upgrade to pytorch 1.2 (#3182)\n\n* first attempt at pytorch 1.2\n\n* explicit is better than implicit\n\n* more explicit\n\n* attempt to fix flaky tests\n\n* pylint\n\n* no disable dropout\n\n* disable dropout by default\n\n* restore dropout, don't deepcopy\n\n* change batch size for biaffine_dependency_parser_multilang_test, maybe that will make it pass? :(\n\n* try batch size 10\n\n* ignore bad gradient parameter\n\n* cleanup\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class BLEU(Metric):",
            "return math.exp(1.0 - self._reference_lengths / self._prediction_lengths)",
            "",
            "def _get_valid_tokens_mask(self, tensor: torch.LongTensor) -> torch.ByteTensor:",
            "-        valid_tokens_mask = torch.ones(tensor.size(), dtype=torch.uint8)",
            "+        valid_tokens_mask = torch.ones(tensor.size(), dtype=torch.bool)",
            "for index in self._exclude_indices:",
            "valid_tokens_mask = valid_tokens_mask & (tensor != index)",
            "return valid_tokens_mask"
        ]
    },
    {
        "number": 667,
        "comments": "doc update",
        "commit_message": "[raysgd] Fix More Docs (#7565)\n\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "MOCK_MODULES = [",
            "\"torch.nn\",",
            "\"torch.nn.parallel\",",
            "\"torch.utils.data\",",
            "+    \"torch.utils.data.distributed\"",
            "]",
            "for mod_name in MOCK_MODULES:",
            "sys.modules[mod_name] = mock.Mock()"
        ]
    },
    {
        "number": 668,
        "comments": "format",
        "commit_message": "ci: Redo `format.sh --all` script & backfill lint fixes (#9956)\n\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class ModelCatalogTest(unittest.TestCase):",
            "dist_cls, param_shape = ModelCatalog.get_action_dist(",
            "action_space, model_config)",
            "self.assertEqual(param_shape, (3, ))",
            "-        dist_input = tf1.placeholder(tf.float32, (None,) + param_shape)",
            "+        dist_input = tf1.placeholder(tf.float32, (None, ) + param_shape)",
            "model.model_config = model_config",
            "dist = dist_cls(dist_input, model=model)",
            "self.assertEqual(dist.sample().shape[1:], dist_input.shape[1:])"
        ]
    },
    {
        "number": 669,
        "comments": "change optimizor",
        "commit_message": "Hotfix optimizer\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "if __name__ == \"__main__\":",
            "collate_fn=dataset.collate_fn,",
            ")",
            "",
            "-    optimizer = torch.optim.SGD(model.parameters(), lr=0.0000)",
            "+    optimizer = torch.optim.Adam(model.parameters())",
            "",
            "metrics = [",
            "\"grid_size\","
        ]
    },
    {
        "number": 670,
        "comments": "add method",
        "commit_message": "Fix LARC with mixed precision (#793)\n\nThe LARC optimizer wraps an underlying optimizer and then needs to be passed\nto amp.initialize for mixed precision. There were 3 different crashes happening\nin this situation, fix all of them and add a unit test.\n\nI don't know if the 'LARC' in sys.modules check ever worked. In my setup, the\nentry in sys.modules is 'apex.parallel.LARC'. Checking if the variable is\ndefined seems more reliable though.\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class LARC(object):",
            "def __setstate__(self, state):",
            "self.optim.__setstate__(state)",
            "",
            "+    @property",
            "+    def state(self):",
            "+        return self.optim.state",
            "+",
            "def __repr__(self):",
            "return self.optim.__repr__()"
        ]
    },
    {
        "number": 671,
        "comments": "def",
        "commit_message": "Type promotion fixes (#2516)\n\n* casting fixes\n\n* lint fixes\n\n* changes\n\n* more changes\n\n* lint fixes\n\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def searchsorted(",
            "v: Union[tf.Tensor, tf.Variable],",
            "side=\"left\",",
            "sorter=None,",
            "-    out: Optional[Union[tf.Tensor, tf.Variable]] = None",
            "+    out: Optional[Union[tf.Tensor, tf.Variable]] = None,",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "return tf.searchsorted(x1, v, side=side)"
        ]
    },
    {
        "number": 672,
        "comments": "asset check doc update",
        "commit_message": "Add proper error messages in `__check_input__()` (#5042)\n\n* Add proper error messages in `__check_input__()`\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Format update to pass linting error\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class MessagePassing(torch.nn.Module):",
            "the_size: List[Optional[int]] = [None, None]",
            "",
            "if isinstance(edge_index, Tensor):",
            "-            assert edge_index.dtype == torch.long",
            "-            assert edge_index.dim() == 2",
            "-            assert edge_index.size(0) == 2",
            "+            assert edge_index.dtype == torch.long, \\",
            "+                \"edge_index.dtype is not of torch.long\"",
            "+            assert edge_index.dim() == 2, \\",
            "+                \"edge_index.dim() is not equal to 2\"",
            "+            assert edge_index.size(0) == 2, \\",
            "+                \"edge_index.size(0) is not equal to 2\"",
            "if size is not None:",
            "the_size[0] = size[0]",
            "the_size[1] = size[1]"
        ]
    },
    {
        "number": 673,
        "comments": "rename",
        "commit_message": "Fix computation of attention_probs when head_mask is provided. (#9853)\n\n* Fix computation of attention_probs when head_mask is provided.\n\nSigned-off-by: Morgan Funtowicz <funtowiczmo@gmail.com>\n\n* Apply changes to the template\n\nCo-authored-by: Lysandre <lysandre.debut@reseau.eseo.fr>\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class TFBertSelfAttention(tf.keras.layers.Layer):",
            "",
            "# Mask heads if we want to",
            "if head_mask is not None:",
            "-            attention_scores = tf.multiply(attention_scores, head_mask)",
            "+            attention_probs = tf.multiply(attention_probs, head_mask)",
            "",
            "attention_output = tf.einsum(\"acbe,aecd->abcd\", attention_probs, value_layer)",
            "outputs = (attention_output, attention_probs) if output_attentions else (attention_output,)"
        ]
    },
    {
        "number": 674,
        "comments": "add param for shape fix",
        "commit_message": "fixed\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class DecoderLayer(nn.Module):",
            "if self.normalize_before:",
            "x = self.norm2(x)",
            "if self.concate_after:",
            "-            x_concat = torch.cat(x, self.src_attn(x, memory, memory, memory_mask))",
            "+            x_concat = torch.cat((x, self.src_attn(x, memory, memory, memory_mask)), dim=-1)",
            "x = residual + self.concate_linear2(x_concat)",
            "else:",
            "x = residual + self.dropout(self.src_attn(x, memory, memory, memory_mask))"
        ]
    },
    {
        "number": 675,
        "comments": "format",
        "commit_message": "Fixed format in some files\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class FrequencyDomainDPCL(FrequencyDomainLoss):",
            ")",
            "",
            "V2 = torch.matmul(torch.transpose(inf, 2, 1), inf).pow(2).sum(dim=(1, 2))",
            "-        Y2 = torch.matmul(torch.transpose(re, 2, 1).float(), re.float()).pow(2).sum(dim=(1, 2))",
            "+        Y2 = (",
            "+            torch.matmul(torch.transpose(re, 2, 1).float(), re.float())",
            "+            .pow(2)",
            "+            .sum(dim=(1, 2))",
            "+        )",
            "VY = torch.matmul(torch.transpose(inf, 2, 1), re.float()).pow(2).sum(dim=(1, 2))",
            "",
            "return V2 + Y2 - 2 * VY"
        ]
    },
    {
        "number": 676,
        "comments": "change param for math fix",
        "commit_message": "Explicitly set max sequence length for the roberta encoder, fix output shape computation, and add unit test. (#2861)\n\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class RoBERTaEncoder(Encoder):",
            "@property",
            "def output_shape(self) -> torch.Size:",
            "if self.reduce_output is None:",
            "-            return torch.Size([self.max_sequence_length, self.transformer.module.config.hidden_size])",
            "+            return torch.Size([self.max_sequence_length - 2, self.transformer.module.config.hidden_size])",
            "return torch.Size([self.transformer.module.config.hidden_size])",
            "",
            "@property"
        ]
    },
    {
        "number": 677,
        "comments": "change API call for type fix",
        "commit_message": "fix type issue of `torch.less` in `clip` (#3852)\n\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def clip(",
            "*,",
            "out: Optional[torch.Tensor] = None,",
            ") -> torch.Tensor:",
            "-    assert torch.all(torch.less(x_min, x_max)), \"Min value must be less than max.\"",
            "+    assert torch.all(",
            "+        torch.less(torch.tensor(x_min), x_max)",
            "+    ), \"Min value must be less than max.\"",
            "if hasattr(x_min, \"dtype\"):",
            "promoted_type = torch.promote_types(x_min.dtype, x_max.dtype)",
            "promoted_type = torch.promote_types(promoted_type, x.dtype)"
        ]
    },
    {
        "number": 678,
        "comments": "remove API call for type fix",
        "commit_message": "Fix the TF Trainer gradient accumulation and the TF NER example (#6713)\n\n* Align TF NER example over the PT one\n\n* Fix Dataset call\n\n* Fix gradient accumulation training\n\n* Apply style\n\n* Address Sylvain's comments\n\n* Address Sylvain's comments\n\n* Apply style\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class TFTokenClassificationLoss:",
            ")",
            "# make sure only labels that are not equal to -100",
            "# are taken into account as loss",
            "-        if tf.math.reduce_any(labels == -1).numpy() is True:",
            "+        if tf.math.reduce_any(labels == -1):",
            "warnings.warn(\"Using `-1` to mask the loss for the token is deprecated. Please use `-100` instead.\")",
            "active_loss = tf.reshape(labels, (-1,)) != -1",
            "else:"
        ]
    },
    {
        "number": 680,
        "comments": "version fix",
        "commit_message": "Fix seed so that data is properly shuffled between epochs\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def get_perplexity(loss):",
            "def train(args, epoch, batch_offset, trainer, dataset, max_positions, num_gpus):",
            "\"\"\"Train the model for one epoch.\"\"\"",
            "",
            "-    torch.manual_seed(args.seed + epoch)",
            "-    trainer.set_seed(args.seed + epoch)",
            "+    seed = args.seed + epoch",
            "+    torch.manual_seed(seed)",
            "+    trainer.set_seed(seed)",
            "",
            "itr = dataset.dataloader(",
            "args.train_subset, num_workers=args.workers, max_tokens=args.max_tokens,",
            "-        seed=args.seed, epoch=epoch, max_positions=max_positions,",
            "+        seed=seed, epoch=epoch, max_positions=max_positions,",
            "sample_without_replacement=args.sample_without_replacement,",
            "skip_invalid_size_inputs_valid_test=args.skip_invalid_size_inputs_valid_test,",
            "sort_by_source_size=(epoch <= args.curriculum))"
        ]
    },
    {
        "number": 681,
        "comments": "add param for type fix",
        "commit_message": "Not use -1e4 as attn mask (#17306)\n\n* Use torch.finfo(self.dtype).min\n\n* for GPTNeoX\n\n* for Albert\n\n* For Splinter\n\n* Update src/transformers/models/data2vec/modeling_data2vec_audio.py\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* fix -inf used in Bart-like models\n\n* Fix a few remaining -inf\n\n* more fix\n\n* clean up\n\n* For CLIP\n\n* For FSMT\n\n* clean up\n\n* fix test\n\n* Add dtype argument and use it for LayoutLMv3\n\n* update FlaxLongT5Attention\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class LayoutLMv3Model(LayoutLMv3PreTrainedModel):",
            "position_ids = position_ids.expand_as(input_ids)",
            "final_position_ids = position_ids",
            "",
            "-        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, None, device)",
            "+        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(",
            "+            attention_mask, None, device, dtype=embedding_output.dtype",
            "+        )",
            "",
            "# Prepare head mask if needed",
            "# 1.0 in head_mask indicate we keep the head"
        ]
    },
    {
        "number": 682,
        "comments": "format",
        "commit_message": "lint fixes\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def cumsum(x: torch.Tensor, axis: int = 0, out: Optional[torch.Tensor] = None):",
            "",
            "",
            "def cumprod(",
            "-    x: torch.Tensor, axis: int = 0, exclusive: Optional[bool] = False, out: Optional[torch.Tensor] = None",
            "+    x: torch.Tensor,",
            "+    axis: int = 0,",
            "+    exclusive: Optional[bool] = False,",
            "+    out: Optional[torch.Tensor] = None,",
            ") -> torch.Tensor:",
            "if exclusive:",
            "x = torch.transpose(x, axis, -1)"
        ]
    },
    {
        "number": 683,
        "comments": "change condition check for shape fix",
        "commit_message": "Make sure all pipelines can run with batched input (#1669)\n\n* [SD] Make sure batched input works correctly\n\n* uP\n\n* uP\n\n* up\n\n* up\n\n* uP\n\n* up\n\n* fix mask stuff\n\n* up\n\n* uP\n\n* more up\n\n* up\n\n* uP\n\n* up\n\n* finish\n\n* Apply suggestions from code review\n\nCo-authored-by: Pedro Cuenca <pedro@huggingface.co>\n\nCo-authored-by: Pedro Cuenca <pedro@huggingface.co>\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class StableDiffusionKDiffusionPipeline(DiffusionPipeline):",
            "return_tensors=\"pt\",",
            ")",
            "text_input_ids = text_inputs.input_ids",
            "-        untruncated_ids = self.tokenizer(prompt, padding=\"max_length\", return_tensors=\"pt\").input_ids",
            "+        untruncated_ids = self.tokenizer(prompt, padding=\"longest\", return_tensors=\"pt\").input_ids",
            "",
            "-        if not torch.equal(text_input_ids, untruncated_ids):",
            "+        if untruncated_ids.shape[-1] >= text_input_ids.shape[-1] and not torch.equal(text_input_ids, untruncated_ids):",
            "removed_text = self.tokenizer.batch_decode(untruncated_ids[:, self.tokenizer.model_max_length - 1 : -1])",
            "logger.warning(",
            "\"The following part of your input was truncated because CLIP can only handle sequences up to\""
        ]
    },
    {
        "number": 684,
        "comments": "format fix",
        "commit_message": "Fix zero padding:   espnet/nets/pytorch_backend/frontends/mask_estimator.py\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class MaskEstimator(torch.nn.Module):",
            "# xs: (B, C, T, D) -> mask:(B, C, T, F)",
            "mask = linear(xs)",
            "",
            "+            mask = torch.sigmoid(mask)",
            "# Zero padding",
            "mask.masked_fill(make_pad_mask(ilens, mask, length_dim=2), 0)",
            "",
            "-            mask = torch.sigmoid(mask)",
            "-",
            "# (B, C, T, F) -> (B, F, C, T)",
            "mask = mask.permute(0, 3, 1, 2)"
        ]
    },
    {
        "number": 685,
        "comments": "change API call for refactor fix",
        "commit_message": "[tune] Fix TB Memory Leak (#5629)\n\n\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def to_tf_values(result, path):",
            "",
            "class TFLogger(Logger):",
            "def _init(self):",
            "-        logger.info(",
            "-            \"Initializing TFLogger instead of TF2Logger. We recommend \"",
            "-            \"migrating to TF2.0. This class will be removed in the future.\")",
            "-        self._file_writer = tf.summary.FileWriter(self.logdir)",
            "+        logger.info(\"Initializing TFLogger instead of TF2Logger.\")",
            "+        self._file_writer = tf.compat.v1.summary.FileWriter(self.logdir)",
            "",
            "def on_result(self, result):",
            "tmp = result.copy()"
        ]
    },
    {
        "number": 686,
        "comments": "format",
        "commit_message": "Add tensorboard support (pytorch, tf2+) (#124)\n\n* Add base of tensorboard support (pytorch, tf2+)\n\n* fix formatting\n\n* send protobuf message\n\n* add internal tbwatcher stub\n\n* Add tbdir watcher threads\n\n* Save files that are out of wandb files dir\n\n* add consumer, need logger\n\n* Connected up history, still debuggin tho\n\n* move tensorflow to framework dir\n\n* fix missing metrics with keras\n\n* consolidate summary in internal process\n\n* comment out keras summary test that is no longer valid\n\n* add directory watcher shutdown delay and flush\n\n* fixes from PR comments\n\n* fix indents\n\n* more PR feedback fixes and circleci timeout bump\n\n* fix histogram logging (on some versions of tf/tb)\n\n* chicken out and dont fail users who we cant support yet\n\n* disable console on windows for now\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "def test_hook():",
            "tf_summary.scalar(\"c1\", c1)",
            "summary_op = tf_summary.merge_all()",
            "",
            "-        hook = wandb_tensorflow.WandbHook(summary_op, history=history, steps_per_log=1)",
            "+        hook = wandb.tensorflow.WandbHook(summary_op, history=history, steps_per_log=1)",
            "with MonitoredTrainingSession(hooks=[hook]) as sess:",
            "summary, acc = sess.run([summary_op, c1])",
            "history.add({})  # Flush the previous row.",
            "",
            "-    assert wandb_tensorflow.tf_summary_to_dict(summary) == {\"c1\": 42.0}",
            "+    assert wandb.tensorboard.tf_summary_to_dict(summary) == {\"c1\": 42.0}",
            "assert summaries_logged[0][\"c1\"] == 42.0"
        ]
    },
    {
        "number": 688,
        "comments": "refactor fix",
        "commit_message": "[rllib] Remove dependency on TensorFlow (#4764)\n\n* remove hard tf dep\n\n* add test\n\n* comment fix\n\n* fix test\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class QNetwork(object):",
            "distributions and \\sigma are trainable variables which are expected to",
            "vanish along the training procedure",
            "\"\"\"",
            "+        import tensorflow.contrib.layers as layers",
            "+",
            "in_size = int(action_in.shape[1])",
            "",
            "epsilon_in = tf.random_normal(shape=[in_size])"
        ]
    },
    {
        "number": 690,
        "comments": "no API",
        "commit_message": "Replace Distribution+TorchDistribution with a thin Distribution mixin (#769)\n\n* Sketch Distribution class as mixin\n\n* Remove TorchDistribution class\n\n* Simplify TransformedDistribution\n\n* Update torch wrappers for most distributions\n\n* Fix docs\n\n* Use dist.Reshape() to set extra_event_dims\n\n* Fix bugs in Reshape distribution\n\n* Fix rejector tests\n\n* Update _Subsample distribution\n\n* Use .reshape() method for extra_event_dims and sample_shape\n\n* Refactor Distribution -> TorchDistribution class hierarchy\n\n* Update docs\n\n* Fix json error in air.ipynb\n\n* Fix bugs in air.ipynb and abstract_infer.py\n\n* Fix distributions docs\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class _Subsample(Distribution):",
            "self.subsample_size = subsample_size",
            "self.use_cuda = torch.Tensor.is_cuda if use_cuda is None else use_cuda",
            "",
            "-    def sample(self, sample_shape=None):",
            "+    def sample(self, sample_shape=torch.Size()):",
            "\"\"\"",
            ":returns: a random subsample of `range(size)`",
            ":rtype: torch.autograd.Variable of torch.LongTensor",
            "\"\"\"",
            "+        if sample_shape:",
            "+            raise NotImplementedError",
            "subsample_size = self.subsample_size",
            "if subsample_size is None or subsample_size > self.size:",
            "subsample_size = self.size"
        ]
    },
    {
        "number": 691,
        "comments": "format",
        "commit_message": "remove excess examples & fix docstrings\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def vander(",
            "increasing: Optional[bool] = False,",
            "out: Optional[torch.tensor] = None,",
            ") -> torch.tensor:",
            "-    return torch.vander(",
            "-        x, N=N, increasing=increasing",
            "-    )",
            "+    return torch.vander(x, N=N, increasing=increasing)",
            "",
            "",
            "vander.support_native_out = False"
        ]
    },
    {
        "number": 692,
        "comments": "format",
        "commit_message": "lintfixbot: Auto-commit fixed lint errors in codebase\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def imag(",
            "input: Union[tf.Tensor, tf.Variable],",
            "/,",
            "*,",
            "-    out: Optional[Union[tf.Tensor,tf.Variable]] = None,",
            "+    out: Optional[Union[tf.Tensor, tf.Variable]] = None,",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "return tf.math.imag(input, name=None)"
        ]
    },
    {
        "number": 693,
        "comments": "change param for resource fix",
        "commit_message": "experimental.py Apple MPS device fix (#8121)\n\n* experimental.py Apple MPS fix\n\nMay resolve https://github.com/ultralytics/yolov5/issues/8102\n\n* Update experimental.py\n\n* Update experimental.py\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class Ensemble(nn.ModuleList):",
            "",
            "",
            "def attempt_load(weights, device=None, inplace=True, fuse=True):",
            "+    # Loads an ensemble of models weights=[a,b,c] or a single model weights=[a] or weights=a",
            "from models.yolo import Detect, Model",
            "",
            "-    # Loads an ensemble of models weights=[a,b,c] or a single model weights=[a] or weights=a",
            "model = Ensemble()",
            "for w in weights if isinstance(weights, list) else [weights]:",
            "-        ckpt = torch.load(attempt_download(w), map_location=device)",
            "-        ckpt = (ckpt.get('ema') or ckpt['model']).float()  # FP32 model",
            "+        ckpt = torch.load(attempt_download(w), map_location='cpu')  # load",
            "+        ckpt = (ckpt.get('ema') or ckpt['model']).to(device).float()  # FP32 model",
            "model.append(ckpt.fuse().eval() if fuse else ckpt.eval())  # fused or un-fused model in eval mode",
            "",
            "# Compatibility updates"
        ]
    },
    {
        "number": 694,
        "comments": "change param for math fix",
        "commit_message": "fix the calcluation of the output dim after 6 subsampmling\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class Conv2dSubsampling6(torch.nn.Module):",
            "torch.nn.ReLU(),",
            ")",
            "self.out = torch.nn.Sequential(",
            "-            torch.nn.Linear(odim * (((idim - 1) // 2 - 1) // 3), odim),",
            "+            torch.nn.Linear(odim * (((idim - 1) // 2 - 2) // 3), odim),",
            "PositionalEncoding(odim, dropout_rate),",
            ")"
        ]
    },
    {
        "number": 698,
        "comments": "remove print",
        "commit_message": "addressed pr comments: added docs, fixed typo, added batch_dim test\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class NestedMapDataTest(TestCase):",
            "tr = poutine.trace(self.model)(self.means, self.stds)",
            "for name in tr.keys():",
            "if tr[name][\"type\"] == \"sample\":",
            "-                print(name, tr[name][\"scale\"])",
            "self.assertTrue(tr[name][\"scale\"] == 4.0 * 2.0)"
        ]
    },
    {
        "number": 699,
        "comments": "remove constraint not clear reason",
        "commit_message": "tl.layers API Refactoring and various modifications (#667)\n\n* Decorators API Refactored\n\n* extra_requires `all`, `all_cpu` and `all_gpu` added\n\n* Error fix\n\n* YAPF Formating Correction\n\n* Test for private method decorator added\n\n* Test Logging Verbosity Fixed to Debug when runned individually\n\n* YAPF corrections applied\n\n* Changelog Added\n\n* Changelog updated\n\n* PR number changed\n\n* First Refactoring Pass done\n\n* cleaning second pass\n\n* Refactoring 3rd pass\n\n* Refactoring 4th Pass\n\n* Code Error fix\n\n* YAPF Formating Fix\n\n* Arguments now using self\n\n* YAPF error correction\n\n* Bug Fix in Decorator\n\n* act name bug fix\n\n* Error Correction\n\n* YAPF formating fix\n\n* Useless tf.identity removed\n\n* Error Fix\n\n* Changelog Updated\n\n* Error fix in tl.activation\n\n* Documentation error fix\n\n* Lazy Import added\n\n* Import Refactoring with LazyImport when necessary\n\n* Changelog Updated\n\n* Gitter Removed\n\n* Fixed proposed by @zsdonghao\n\n* Documentation updated\n\n* Missing requirements added\n\n* Update to TensorLayer 1.8.6rc1\n\n* Requirements error fix\n\n* Docker Files updated\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def model(x, is_train, reuse):",
            ")",
            "n = tl.layers.FlattenLayer(n, name='flatten2')",
            "n = tl.layers.DenseLayer(n, n_units=1024, act=tf.nn.relu, name='out1')",
            "-        n = tl.layers.DenseLayer(n, n_units=10, act=tf.identity, name='out2')",
            "+        n = tl.layers.DenseLayer(n, n_units=10, name='out2')",
            "return n, s"
        ]
    },
    {
        "number": 700,
        "comments": "test fix",
        "commit_message": "fix tests\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def ids_tensor(shape, vocab_size, rng=None, name=None, dtype=tf.int32):",
            "for _ in range(total_dims):",
            "values.append(rng.randint(0, vocab_size - 1))",
            "",
            "-    return tf.constant(values, shape=shape, dtype=dtype)",
            "+    output = tf.constant(values,",
            "+                         shape=shape,",
            "+                         dtype=dtype if dtype is not None else tf.int32)",
            "+",
            "+    return output",
            "",
            "",
            "class TFModelUtilsTest(unittest.TestCase):"
        ]
    },
    {
        "number": 702,
        "comments": "add attribute parama",
        "commit_message": "[Audio datasets] Adapting all audio datasets (#3081)\n\n* up\n\n* up\n\n* up\n\n* up\n\n* up\n\n* Update src/datasets/utils/resources/readme_structure.yaml\n\nCo-authored-by: Albert Villanova del Moral <8515462+albertvillanova@users.noreply.github.com>\n\n* correct\n\n* correct 2\n\n* Update datasets/covost2/README.md\n\nCo-authored-by: Albert Villanova del Moral <8515462+albertvillanova@users.noreply.github.com>\n\n* Fix typo\n\nCo-authored-by: Albert Villanova del Moral <8515462+albertvillanova@users.noreply.github.com>\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class LibrispeechASR(datasets.GeneratorBasedBuilder):",
            "\"speaker_id\": speaker_id,",
            "\"chapter_id\": chapter_id,",
            "\"file\": os.path.join(transcript_dir_path, audio_file),",
            "+                        \"audio\": os.path.join(transcript_dir_path, audio_file),",
            "\"text\": transcript,",
            "}",
            "key += 1"
        ]
    },
    {
        "number": 705,
        "comments": "def",
        "commit_message": "Fixed positional and keyword arguments and added missing out argument to backend implementations (#9660)\n\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def broadcast_arrays(*arrays: torch.Tensor) -> List[torch.Tensor]:",
            "{\"1.11.0 and below\": (\"uint8\", \"uint16\", \"uint32\", \"uint64\")}, backend_version",
            ")",
            "def broadcast_to(",
            "-    x: torch.Tensor, shape: Union[ivy.NativeShape, Sequence[int]]",
            "+    x: torch.Tensor,",
            "+    /,",
            "+    shape: Union[ivy.NativeShape, Sequence[int]],",
            "+    *,",
            "+    out: Optional[torch.Tensor] = None,",
            ") -> torch.Tensor:",
            "if x.ndim > len(shape):",
            "return torch.broadcast_to(x.reshape(-1), shape)"
        ]
    },
    {
        "number": 707,
        "comments": "add API call for type fix",
        "commit_message": "Add support for HuggingFace's TensorFlow models (#127)\n\n* added support for for HuggingFace's TensorFlow models\n\n* added notebook for HuggingFace's tensorflow bert model\n\n* change nebullvm name in logs\n\n* Add optimized model details + warning if static shape is used for HF models (#1)\n\n* add optimized model type info\n\n* fix tvm issue\n\n* edit dockerfile and add image auto building\n\n* add docker installation on azure pipeline\n\n* fix bug in neural compressor output shape\n\n* add support for openvino with python 3.10\n\n* add build docker image to azure pipelines\n\n* revert docker build from az pipelines and edit format of the optimization results\n\n* revert docker build from az pipelines\n\n* added tabulate to setup.py and general fixes\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class TFLiteBackendInferenceLearner(TensorflowBaseInferenceLearner):",
            "self.interpreter.set_tensor(i, input_tensor)",
            "self.interpreter.invoke()",
            "return tuple(",
            "-            self.interpreter.get_tensor(output_detail[\"index\"])",
            "+            tf.convert_to_tensor(",
            "+                self.interpreter.get_tensor(output_detail[\"index\"])",
            "+            )",
            "for output_detail in output_details",
            ")"
        ]
    },
    {
        "number": 710,
        "comments": "update API call for refactor fix",
        "commit_message": "fix deprecation about dropout; fix Keras compatibility in tf1.13\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class Model(ModelDesc):",
            ".Conv2D('conv3.1', filters=128, padding='VALID') \\",
            ".Conv2D('conv3.2', filters=128, padding='VALID') \\",
            ".FullyConnected('fc0', 1024 + 512, activation=tf.nn.relu) \\",
            "-                .tf.nn.dropout(keep_prob) \\",
            "+                .Dropout(rate=drop_rate) \\",
            ".FullyConnected('fc1', 512, activation=tf.nn.relu) \\",
            ".FullyConnected('linear', out_dim=self.cifar_classnum)()"
        ]
    },
    {
        "number": 713,
        "comments": "change value",
        "commit_message": "force use identity activation in branchformer conv mod + fix main_params doc\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class ConvolutionalSpatialGatingUnit(torch.nn.Module):",
            ")",
            "",
            "self.norm = norm_class(channels, **norm_args)",
            "-        self.activation = activation",
            "+        self.activation = torch.nn.Identity()",
            "",
            "self.dropout = torch.nn.Dropout(dropout_rate)"
        ]
    },
    {
        "number": 716,
        "comments": "rename",
        "commit_message": "Private Tensors (#2709)\n\n* ADD Private Tensor into syft lib\n\n* Allow to serialize Private Tensor\n\n* Change native allow_to_get parameters\n\n* - Overload allow_to_get method\n - Add user parameters at get method (native.py, pointer_tensor.py, object_pointer.py)\n - Modify allow_to_get method at native.py\n - Implement allow_to_get method at PrivateTensor\n\n* Fix allowed_to_get method\n\n* Update virtual_worker test\n\n* Update fit method\n\n* ADD Private Tensor experimental notebook\n\n* Register Private Tensor at hook\n\n* - Update docstrings\n- Fix allow_to_get  method\n- Fix private_tensor method\n\n* ADD get_class_attributes method\n\n* Overload torch module methods\n\n* ADD Private Tensor unit tests\n\n* Fix flake8 issues\n\n* Fix code style\n\n* Fix unit tests coverage\n\n* Change proto reference\n\n* Update PrivateTensor simplify method\n\n* Overload Private Tensor methods\n- Create custom _get_hooked_method (_get_hooked_private_method).\n- Handle PrivateTensors with _get_hooked_private_method.\n- Add parents/command attributes at private tensor.\n\n* ADD new Exception -> SendNotPermittedError\n\n* - Verify permissions during send()/get() methods.\n   We need to verify user credentials during send()/get() tensor commands to keep it safe.\n- REFACTORY/RENAME allowed_to_get(user) -> allow(user)\n   Now, we're using this method to verify permissions in an generic context, not only for get() commands.\n\n* Fix CI tests\n\n* Fix coverage\n\n* Fix detail method to enable remote operations.\n  Change data structure used to store allowed users (list -> tuple).\n\n* Fix proto requirement reference\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def test_obj_not_found(workers):",
            "",
            "def test_get_not_permitted(workers):",
            "bob = workers[\"bob\"]",
            "-    with patch.object(torch.Tensor, \"allowed_to_get\") as mock_allowed_to_get:",
            "+    x = torch.tensor([1, 2, 3, 4, 5]).send(bob)",
            "+    with patch.object(torch.Tensor, \"allow\") as mock_allowed_to_get:",
            "mock_allowed_to_get.return_value = False",
            "-        x = torch.tensor([1, 2, 3, 4, 5]).send(bob)",
            "with pytest.raises(GetNotPermittedError):",
            "x.get()",
            "mock_allowed_to_get.assert_called_once()"
        ]
    },
    {
        "number": 720,
        "comments": "change param for type fix",
        "commit_message": "Fix for ST\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class CustomConverter(object):",
            "xs_pad = pad_list([torch.from_numpy(x).float() for x in xs], 0).to(device, dtype=self.dtype)",
            "",
            "ilens = torch.from_numpy(ilens).to(device)",
            "-        # NOTE: this is for multi-task learning (e.g., speech translation)",
            "-        ys_pad = pad_list([torch.from_numpy(np.array(y[0]) if isinstance(y, tuple) else y).long()",
            "+        # NOTE: this is for multi-output (e.g., speech translation)",
            "+        ys_pad = pad_list([torch.from_numpy(np.array(y[0][:]) if isinstance(y, tuple) else y).long()",
            "for y in ys], self.ignore_id).to(device)",
            "",
            "return xs_pad, ilens, ys_pad"
        ]
    },
    {
        "number": 721,
        "comments": "use custom API",
        "commit_message": "Scale Factor Fix (#2039)\n\n* fix (scale_factor): use new_tensor(scale_factor) in case it is numpy array\n\n* reformat (models): reformat with flake8, yapf, and isort to pass CI\n\n* fix (bbox_mapping): fix scale_factor bug in bbox_mapping\n\n* fix img_meta bug\n\nCo-authored-by: beansi <zhangwenwei@sensetime.com>\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class BBoxHead(nn.Module):",
            "if isinstance(scale_factor, float):",
            "bboxes /= scale_factor",
            "else:",
            "-                scale_factor = torch.from_numpy(scale_factor).to(bboxes.device)",
            "+                scale_factor = bboxes.new_tensor(scale_factor)",
            "bboxes = (bboxes.view(bboxes.size(0), -1, 4) /",
            "scale_factor).view(bboxes.size()[0], -1)"
        ]
    },
    {
        "number": 722,
        "comments": "test",
        "commit_message": "Fixed tests and docs (#654)\n\n* Fixed tests\n\n* Fixed doc error and warnings\n\n* Fixed lint\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class TestAffine2d:",
            "",
            "def test_affine_scale(self, device):",
            "torch.manual_seed(0)",
            "-        scale_factor = torch.rand(1, device=device) * 2.0",
            "+        _scale_factor = torch.rand(1, device=device) * 2.0",
            "+        scale_factor = torch.stack([_scale_factor, _scale_factor], dim=1)",
            "input = torch.rand(1, 2, 3, 4, device=device)",
            "",
            "transform = kornia.Affine(scale_factor=scale_factor).to(device)"
        ]
    },
    {
        "number": 725,
        "comments": "change API call for version fix",
        "commit_message": "Fix example about synthetic benchmark elastic (#2265)\n\nSigned-off-by: jiaqianjing <jiaqianjing@gmail.com>\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def run_benchmark(state):",
            "",
            "",
            "def on_state_reset():",
            "-    tf.keras.backend.set_value(model.optimizer.lr, lr * hvd.size())",
            "+    opt.lr.assign(lr * hvd.size())",
            "",
            "",
            "state = hvd.elastic.TensorFlowKerasState(model, opt, img_secs=[], iter=0, batch=0, warm=False)"
        ]
    },
    {
        "number": 726,
        "comments": "remove check",
        "commit_message": "[RLlib] Add testing framework_iterator. (#7852)\n\n* Add testing framework_iterator.\n\n* LINT.\n\n* WIP.\n\n* Fix and LINT.\n\n* LINT fix.\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def do_test_log_likelihood(run,",
            "prev_reward_batch=np.array([prev_r]))",
            "check(np.exp(logp), expected_prob, atol=0.2)",
            "",
            "-        if eager_ctx:",
            "-            eager_ctx.__exit__(None, None, None)",
            "-",
            "",
            "class TestComputeLogLikelihood(unittest.TestCase):",
            "def test_dqn(self):"
        ]
    },
    {
        "number": 727,
        "comments": "change param for math fix",
        "commit_message": "fix ndc/screen problem in blender/llff (#39)\n\nSummary:\nX-link: https://github.com/fairinternal/pytorch3d/pull/39\n\nBlender and LLFF cameras were sending screen space focal length and principal point to a camera init function expecting NDC\n\nReviewed By: shapovalov\n\nDifferential Revision: D37788686\n\nfbshipit-source-id: 2ddf7436248bc0d174eceb04c288b93858138582\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def _interpret_blender_cameras(",
            "",
            "Rpt3, Tpt3 = mtx[:, :3].split([3, 1], dim=0)",
            "",
            "-        focal_length_pt3 = torch.FloatTensor([[-focal, focal]])",
            "-        principal_point_pt3 = torch.FloatTensor([[W / 2, H / 2]])",
            "+        focal_length_pt3 = torch.FloatTensor([[focal, focal]])",
            "+        principal_point_pt3 = torch.FloatTensor([[0.0, 0.0]])",
            "",
            "cameras = PerspectiveCameras(",
            "focal_length=focal_length_pt3,"
        ]
    },
    {
        "number": 728,
        "comments": "add param for type fix",
        "commit_message": "Type promotion fixes (#2516)\n\n* casting fixes\n\n* lint fixes\n\n* changes\n\n* more changes\n\n* lint fixes\n\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def vector_to_skew_symmetric_matrix(",
            "a2s = vector_expanded[..., 1:2, :]",
            "a3s = vector_expanded[..., 2:3, :]",
            "# BS x 1 x 1",
            "-    zs = torch.zeros(batch_shape + [1, 1], device=vector.device)",
            "+    zs = torch.zeros(batch_shape + [1, 1], device=vector.device, dtype=vector.dtype)",
            "# BS x 1 x 3",
            "row1 = torch.cat((zs, -a3s, a2s), -1)",
            "row2 = torch.cat((a3s, zs, -a1s), -1)"
        ]
    },
    {
        "number": 730,
        "comments": "add param for resource fix",
        "commit_message": "Fix CUDA tests on master (#1542)\n\n* Fix CUDA tests on master\n\n* address comments\n\n* change to range(11)\n\n* remove unused import\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "from pyro.ops.einsum import contract",
            "def _finfo(tensor):",
            "# This can be replaced with torch.finfo once it is available",
            "# https://github.com/pytorch/pytorch/issues/10742",
            "-    return np.finfo(torch.empty(torch.Size(), dtype=tensor.dtype).numpy().dtype)",
            "+    return np.finfo(torch.empty(torch.Size(), dtype=tensor.dtype, device=\"cpu\").numpy().dtype)",
            "",
            "",
            "def _check_batch_dims_are_sensible(output_dims, nonoutput_ordinal):"
        ]
    },
    {
        "number": 731,
        "comments": "custom API",
        "commit_message": "[zero] fix missed subclasses partitioning bug (#1135)\n\n* fix missed subclassed partitioning bug\n\n* fix on exit\n\nCo-authored-by: Olatunji Ruwase <olruwase@microsoft.com>\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class InsertPostInitMethodToModuleSubClasses(object):",
            "cls.__init__ = cls._old_init",
            "",
            "# Replace .__init__() for all existing subclasses of torch.nn.Module",
            "-        for subclass in torch.nn.modules.module.Module.__subclasses__():",
            "+        for subclass in get_all_subclasses(torch.nn.modules.module.Module):",
            "_disable_class(subclass)",
            "",
            "# Replace .__init__() for future subclasses of torch.nn.Module"
        ]
    },
    {
        "number": 733,
        "comments": "add condition check for state fix",
        "commit_message": "Fix Tracing mode TS export for LayerNorm layer\n\nSummary: When under TorchScript Tracing (instead of only doing this for Scripting) we set `export=True` for `LayerNorm` as `FusedLayerNorm `doesn't work with JIT yet (see `torch.jit.unused decorator`).\n\nReviewed By: cndn\n\nDifferential Revision: D33103054\n\nfbshipit-source-id: f8c24a4a30a89dd4c70b19362fd60c51fcb9a1f0\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "try:",
            "with torch.cuda.device(x.device):",
            "return super().forward(x)",
            "",
            "+",
            "except ImportError:",
            "has_fused_layernorm = False",
            "",
            "",
            "def LayerNorm(normalized_shape, eps=1e-5, elementwise_affine=True, export=False):",
            "-    if torch.jit.is_scripting():",
            "+    if torch.jit.is_scripting() or torch.jit.is_tracing():",
            "export = True",
            "if not export and torch.cuda.is_available() and has_fused_layernorm:",
            "return FusedLayerNorm(normalized_shape, eps, elementwise_affine)"
        ]
    },
    {
        "number": 734,
        "comments": "doc update",
        "commit_message": "fix flake8 style in tensorpack/\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def class_balanced_sigmoid_cross_entropy(logits, label, name='cross_entropy_loss",
            "cost = tf.nn.weighted_cross_entropy_with_logits(logits, y, pos_weight)",
            "cost = tf.reduce_mean(cost * (1 - beta), name=name)",
            "",
            "-    #logstable = tf.log(1 + tf.exp(-tf.abs(z)))",
            "-    # loss_pos = -beta * tf.reduce_mean(-y *",
            "-    #(logstable - tf.minimum(0.0, z)))",
            "-    # loss_neg = (1. - beta) * tf.reduce_mean((y - 1.) *",
            "-    #(logstable + tf.maximum(z, 0.0)))",
            "-    #cost = tf.sub(loss_pos, loss_neg, name=name)",
            "+    # logstable = tf.log(1 + tf.exp(-tf.abs(z)))",
            "+    # loss_pos = -beta * tf.reduce_mean(-y * (logstable - tf.minimum(0.0, z)))",
            "+    # loss_neg = (1. - beta) * tf.reduce_mean((y - 1.) * (logstable + tf.maximum(z, 0.0)))",
            "+    # cost = tf.sub(loss_pos, loss_neg, name=name)",
            "return cost"
        ]
    },
    {
        "number": 735,
        "comments": "add API call for state fix",
        "commit_message": "fixed missing super update call in TRPO\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class TRPOModel(PolicyGradientModel):",
            ":param batch:",
            ":return:",
            "\"\"\"",
            "+        super(TRPOModel, self).update(batch)",
            "+",
            "self.feed_dict = {state: batch['states'][name] for name, state in self.state.items()}",
            "self.feed_dict.update({action: batch['actions'][name] for name, action in self.action.items()})",
            "self.feed_dict[self.reward] = batch['rewards']"
        ]
    },
    {
        "number": 740,
        "comments": "change param for math fix",
        "commit_message": "fixed weight indices for root node in 3d case\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def spline_gcn(",
            "row = row.view(-1, 1).expand(row.size(0), output.size(1))",
            "output = zero.scatter_add_(0, row, output)",
            "",
            "-    # Weighten root node features by multiplying with the meaned weights at the",
            "-    # origin.",
            "-    index = torch.arange(0, kernel_size[-1]).long()",
            "+    # Weighten root node features by multiplying with the meaned weights from",
            "+    # the origin.",
            "+    index = torch.arange(0, reduce(lambda x, y: x * y, kernel_size[1:])).long()",
            "root_weight = weight[index].mean(0)",
            "output += torch.mm(features, root_weight)"
        ]
    },
    {
        "number": 742,
        "comments": "update API call for refactor fix",
        "commit_message": "[RLlib] Minor cleanup in preparation to tf2.x support. (#9130)\n\n* WIP.\n\n* Fixes.\n\n* LINT.\n\n* Fixes.\n\n* Fixes and LINT.\n\n* WIP.\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def stats(policy, train_batch):",
            "\"cur_lr\": tf.cast(policy.cur_lr, tf.float64),",
            "\"policy_loss\": policy.loss.pi_loss,",
            "\"entropy\": policy.loss.entropy,",
            "-        \"var_gnorm\": tf.global_norm(policy.model.trainable_variables()),",
            "+        \"var_gnorm\": tf.linalg.global_norm(policy.model.trainable_variables()),",
            "\"vf_loss\": policy.loss.vf_loss,",
            "\"vf_explained_var\": explained_variance(",
            "tf.reshape(policy.loss.value_targets, [-1]),"
        ]
    },
    {
        "number": 743,
        "comments": "add API call for type fix",
        "commit_message": "Fix issues with tests\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def test_tensorrt_torch(",
            "res_orig = tuple(model(*inputs_example))",
            "assert all(",
            "[",
            "-                    torch.allclose(res_tensor, res_orig_tensor, rtol=1e-01)",
            "+                    torch.allclose(",
            "+                        res_tensor.float(), res_orig_tensor, rtol=1e-01",
            "+                    )",
            "for (res_tensor, res_orig_tensor) in zip(res, res_orig)",
            "]",
            ")"
        ]
    },
    {
        "number": 744,
        "comments": "update API call for refactor fix",
        "commit_message": "use functional interface for softmax in attention (#14198)\n\n* use functional interface instead of instantiating module and immediately calling it\n\n* fix torch.nn.functional to nn.functional. Thank you Stas!\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class Attention(nn.Module):",
            "# Apply the attention mask",
            "w = w + attention_mask",
            "",
            "-        w = nn.Softmax(dim=-1)(w)",
            "+        w = nn.functional.softmax(w, dim=-1)",
            "w = self.attn_dropout(w)",
            "",
            "# Mask heads if we want to"
        ]
    },
    {
        "number": 745,
        "comments": "refactor fix",
        "commit_message": "Fix typo: compat.v1\n\nPiperOrigin-RevId: 306641300\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "\"from skimage.transform import AffineTransform\\n\",",
            "\"from six import BytesIO\\n\",",
            "\"\\n\",",
            "-        \"import tensorflow.compat.v2 as tf\\n\",",
            "+        \"import tensorflow.compat.v1 as tf\\n\",",
            "\"tf.disable_v2_behavior()\\n\",",
            "\"\\n\",",
            "\"import tensorflow_hub as hub\\n\","
        ]
    },
    {
        "number": 750,
        "comments": "log update",
        "commit_message": "Fix tests and memory issues\n\nFix issue with openvino when using large models\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class ONNXTensorRTCompiler(TensorRTCompiler):",
            "assert os.path.isfile(onnx_model_path)",
            "except Exception:",
            "# Use original model",
            "+                self.logger.warning(",
            "+                    \"Unable to simplify model with ONNX Simplifier. \"",
            "+                    \"Original ONNX model will be used to build \"",
            "+                    \"TensorRT engine\"",
            "+                )",
            "onnx_model_path = str(model)",
            "self.simplify_model = False",
            "else:"
        ]
    },
    {
        "number": 752,
        "comments": "add API call for type fix",
        "commit_message": "Fix parrots compatibility issues (#4143)\n\n* fix parrots compatibility\n\n* add comments\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class DistributedGroupSampler(Sampler):",
            "if size > 0:",
            "indice = np.where(self.flag == i)[0]",
            "assert len(indice) == size",
            "-                indice = indice[list(torch.randperm(int(size),",
            "-                                                    generator=g))].tolist()",
            "+                # add .numpy() to avoid bug when selecting indice in parrots.",
            "+                # TODO: check whether torch.randperm() can be replaced by",
            "+                # numpy.random.permutation().",
            "+                indice = indice[list(",
            "+                    torch.randperm(int(size), generator=g).numpy())].tolist()",
            "extra = int(",
            "math.ceil(",
            "size * 1.0 / self.samples_per_gpu / self.num_replicas)"
        ]
    },
    {
        "number": 754,
        "comments": "doc update",
        "commit_message": "[keras/{backend,losses,distribute/keras_premade_models_test,mixed_precision/policy_test,premade/linear_test,premade/wide_deep_test}.py] Fix `int` given for `float` args\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def dot(x, y):",
            "",
            "If `x` is an N-D array and `y` is an M-D array (where M>=2), it is a sum",
            "product over the last axis of `x` and the second-to-last axis of `y`.",
            "-  >>> x = tf.keras.backend.random_uniform_variable(shape=(2, 3), low=0, high=1)",
            "+  >>> x = tf.keras.backend.random_uniform_variable(shape=(2, 3), low=.0, high=1.)",
            ">>> y = tf.keras.backend.ones((4, 3, 5))",
            ">>> xy = tf.keras.backend.dot(x, y)",
            ">>> tf.keras.backend.int_shape(xy)"
        ]
    },
    {
        "number": 757,
        "comments": "add method",
        "commit_message": "Fix merge_dot tests\n\n* Fix merge_dot tests\n\n* Make batch_dot unique\n\nbatch_dot is not tensordot! It only accepts one reduce dimension at a\ntime. Other reduce dimensions should be dome afterwards with K.sum\nThis means that K.batch_dot will have the same behavior in both\ntensorflow and theano. This also means that we have less parenthesis and\nless nested lists.\n\nNew usage:\n\nmerge_mode = 'dot', dot_axes=[axis1, axis2]\n\nBefore:\n\nmerge_mode = 'dot', dot_axes=[[axis1], [axis2]]\n\n* Backport sign by @the-moliver\n\n* Fix docstrings\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def round(x):",
            "return tf.round(x)",
            "",
            "",
            "+def sign(x):",
            "+    return tf.sign(x)",
            "+",
            "+",
            "def pow(x, a):",
            "return tf.pow(x, a)"
        ]
    },
    {
        "number": 758,
        "comments": "custom API",
        "commit_message": "Add bilinear attention (#1349)\n\n* Add bilinear attention, some code cleanup, make semantic parsers use new attention\n\n* Fix pylint and docs\n\n* Increase beam size for wikitables ERM parser, so test doesn't fail\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class Attention(torch.nn.Module, Registrable):",
            "vector: torch.Tensor,",
            "matrix: torch.Tensor,",
            "matrix_mask: torch.Tensor = None) -> torch.Tensor:",
            "-        similarities = self._forward_internal(vector, matrix, matrix_mask)",
            "+        similarities = self._forward_internal(vector, matrix)",
            "if self._normalize:",
            "return masked_softmax(similarities, matrix_mask)",
            "else:",
            "return similarities",
            "",
            "-    def _forward_internal(self, vector: torch.Tensor, matrix: torch.Tensor,",
            "-                          matrix_mask: torch.Tensor = None) -> torch.Tensor:",
            "+    def _forward_internal(self, vector: torch.Tensor, matrix: torch.Tensor) -> torch.Tensor:",
            "raise NotImplementedError",
            "",
            "@classmethod"
        ]
    },
    {
        "number": 759,
        "comments": "add param for math fix",
        "commit_message": "fix pairnorm test\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def test_pair_norm(scale_individually):",
            "assert out1.size() == (100, 16)",
            "",
            "out2 = norm(torch.cat([x, x], dim=0), torch.cat([batch, batch + 1], dim=0))",
            "-    assert torch.allclose(out1, out2[:100])",
            "-    assert torch.allclose(out1, out2[100:])",
            "+    assert torch.allclose(out1, out2[:100], atol=1e-6)",
            "+    assert torch.allclose(out1, out2[100:], atol=1e-6)"
        ]
    },
    {
        "number": 761,
        "comments": "format",
        "commit_message": "linter fix\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class Decoder(nn.Module):",
            "memories = torch.cat((memory, memories), dim=0)",
            "memories = self._update_memory(memories)",
            "if speaker_embeddings is not None:",
            "-                memories = torch.cat([memories, speaker_embeddings], dim=-1)",
            "+            memories = torch.cat([memories, speaker_embeddings], dim=-1)",
            "memories = self.prenet(memories)",
            "",
            "self._init_states(inputs, mask=mask)"
        ]
    },
    {
        "number": 763,
        "comments": "doc update",
        "commit_message": "Convert shape_list, fix softmax axis default\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def attn(x, scope, n_state, *, past, params, block_offset=0, train=False):",
            "",
            "def merge_heads(x):",
            "# TODO: convert to mtf code",
            "-        # Reverse of split_heads",
            "+        # Reverse of split_heads : result shape [batch, sequence, features]",
            "return merge_states(tf.transpose(x, [0, 2, 1, 3]))",
            "",
            "# the old mask_attn_weights applied directly to the QK; this returns a bias that the attention code from mtf adds to the attention matrix."
        ]
    },
    {
        "number": 764,
        "comments": "add condition check for type fix",
        "commit_message": "[T5] Fix speed degradation bug t5 (#10496)\n\n* fix speed degradation bug t5\n\n* fix for all models\n\n* fix code quality\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class BlenderbotSmallEncoderLayer(nn.Module):",
            "hidden_states = residual + hidden_states",
            "hidden_states = self.final_layer_norm(hidden_states)",
            "",
            "-        if torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any():",
            "+        if hidden_states.dtype == torch.float16 and (",
            "+            torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any()",
            "+        ):",
            "clamp_value = torch.finfo(hidden_states.dtype).max - 1000",
            "hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)"
        ]
    },
    {
        "number": 765,
        "comments": "remove logger",
        "commit_message": "fix logging on rank 0 only (#2425)\n\n* fix and test for ddp block logging rank > 0\n\n* rename\n\n* use the dummy logger\n\n* dummy logger test\n\n* set the logger in  model\n\n* decorator for rank zero experiment\n\n* simplify check\n\n* simplify\n\n* fix problem with None in checkpoint path\n\n* revert configure logger\n\n* unused import\n\n* offline\n\n* try rank 0 decorator in checkpoint\n\n* try fix test\n\n* imgs\n\n* add asserts to make sure log zero only saves checkpoints\n\n* add asserts to make sure log zero only saves checkpoints\n\n* add asserts to make sure log zero only saves checkpoints\n\n* add asserts to make sure log zero only saves checkpoints\n\n* add asserts to make sure log zero only saves checkpoints\n\n* fix tpu tests\n\n* fix tpu tests\n\nCo-authored-by: William Falcon <waf2107@columbia.edu>\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class Trainer(",
            "",
            "\"\"\"",
            "# bind logger and other properties",
            "-        model.logger = self.logger",
            "self.copy_trainer_model_properties(model)",
            "",
            "# clean hparams"
        ]
    },
    {
        "number": 770,
        "comments": "not clear no API",
        "commit_message": "Fix DDP unused params (#497)\n\nSummary:\nPull Request resolved: https://github.com/facebookresearch/pytext/pull/497\n\n1 DDP assumes all used params are in model.forward, however in CRF model, there's params used outside of forward, which will casue issue. This diff fixed this by setting find_unused_parameters to False\n\n2 DDP can detect unused params now, so removing the previous hack in PyText\n\nReviewed By: chenyangyu1988\n\nDifferential Revision: D15027698\n\nfbshipit-source-id: 9499cf082f7eed96ec22179076f345943de2177d\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class RNNGParser(Model, Component):",
            "",
            "\"\"\"",
            "",
            "-        nn.Module.__init__(self)",
            "+        super().__init__()",
            "",
            "self.embedding = embedding",
            "# self.embedding.config: FeatureConfig object cannot be pickled but,"
        ]
    },
    {
        "number": 773,
        "comments": "format",
        "commit_message": "flake8 fixes (#3064)\n\n* flake8 fixes\n\n* fix pep8\n\n* fix pep8\n\nCo-authored-by: William Falcon <waf2107@columbia.edu>\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class ModelSummary(object):",
            "input_ = model.transfer_batch_to_device(input_, model.device)",
            "",
            "if trainer is not None and trainer.amp_backend == AMPType.NATIVE and not trainer.use_tpu:",
            "-                model.forward = torch.cuda.amp.autocast()(model.forward)",
            "+            model.forward = torch.cuda.amp.autocast()(model.forward)",
            "",
            "mode = model.training",
            "model.eval()"
        ]
    },
    {
        "number": 774,
        "comments": "update API call for refactor fix",
        "commit_message": "[style] consistent nn. and nn.functional (#12124)\n\n* consistent nn. and nn.functional\n\n* fix glitch\n\n* fix glitch #2\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class QuantLinear(nn.Module):",
            "x_int = x / prev_act_scaling_factor",
            "",
            "return (",
            "-            F.linear(x_int, weight=self.weight_integer, bias=self.bias_integer) * bias_scaling_factor,",
            "+            nn.functional.linear(x_int, weight=self.weight_integer, bias=self.bias_integer) * bias_scaling_factor,",
            "bias_scaling_factor,",
            ")"
        ]
    },
    {
        "number": 776,
        "comments": "format",
        "commit_message": "TRPO working, other fixes and improvements\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class Gaussian(Distribution):",
            "if self.action_spec.max_value is not None:",
            "action = tf.minimum(x=self.action_spec.max_value, y=action)",
            "",
            "-        return action",
            "+            return action",
            "",
            "@tf_function(num_args=2)",
            "def log_probability(self, *, parameters, action):"
        ]
    },
    {
        "number": 777,
        "comments": "change value",
        "commit_message": "Fix visformer in_chans stem handling\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class Visformer(nn.Module):",
            "img_size //= 8",
            "else:",
            "self.stem = nn.Sequential(",
            "-                    nn.Conv2d(3, self.init_channels, 7, stride=2, padding=3, bias=False),",
            "+                    nn.Conv2d(in_chans, self.init_channels, 7, stride=2, padding=3, bias=False),",
            "nn.BatchNorm2d(self.init_channels),",
            "nn.ReLU(inplace=True)",
            ")"
        ]
    },
    {
        "number": 779,
        "comments": "update API call for refactor fix",
        "commit_message": "Fix various docs formatting (#1688)\n\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class Importance(TracePosterior):",
            "\"\"\"",
            "if self.log_weights:",
            "log_w_norm = self.get_normalized_weights(log_scale=True)",
            "-            ess = torch.exp(-logsumexp(2*log_w_norm, 0))",
            "+            ess = torch.exp(-torch.logsumexp(2*log_w_norm, 0))",
            "else:",
            "warnings.warn(\"The log_weights list is empty, effective sample size is zero.\")",
            "ess = 0"
        ]
    },
    {
        "number": 780,
        "comments": "update API call for version fix",
        "commit_message": "fix for API changes\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "for epoch in range(opt.niter):",
            "vutils.save_image(fake.data, 'fake_samples.png')",
            "",
            "# do checkpointing",
            "-    torch.save(netG.parameter_dict(), 'netG_epoch_%d.pth' % epoch)",
            "-    torch.save(netD.parameter_dict(), 'netD_epoch_%d.pth' % epoch)",
            "+    torch.save(netG.state_dict(), 'netG_epoch_%d.pth' % epoch)",
            "+    torch.save(netD.state_dict(), 'netD_epoch_%d.pth' % epoch)"
        ]
    },
    {
        "number": 781,
        "comments": "test",
        "commit_message": "Fix onnx unitest (#6369)\n\nFix all unitests\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def distance2bbox(points, distance, max_shape=None):",
            "bboxes = torch.stack([x1, y1, x2, y2], -1)",
            "",
            "if max_shape is not None:",
            "-        if points.dim() == 2 and not torch.onnx.is_in_onnx_export():",
            "+        if bboxes.dim() == 2 and not torch.onnx.is_in_onnx_export():",
            "# speed up",
            "bboxes[:, 0::2].clamp_(min=0, max=max_shape[1])",
            "bboxes[:, 1::2].clamp_(min=0, max=max_shape[0])"
        ]
    },
    {
        "number": 782,
        "comments": "update API call for version fix",
        "commit_message": "Fixed a broken test for v0.6 with importing linear function instead of module\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def dnn(tensor_in, hidden_units, activation=tf.nn.relu, keep_prob=None):",
            "with tf.variable_scope('dnn'):",
            "for i, n_units in enumerate(hidden_units):",
            "with tf.variable_scope('layer%d' % i):",
            "-                tensor_in = linear.linear(tensor_in, n_units, True)",
            "+                tensor_in = linear(tensor_in, n_units, True)",
            "tensor_in = activation(tensor_in)",
            "if keep_prob:",
            "tensor_in = tf.nn.dropout(tensor_in, keep_prob)"
        ]
    },
    {
        "number": 783,
        "comments": "update API call for refactor fix",
        "commit_message": "Abstract accelerator (step 3) (#2677)\n\n* Integrate accelerator abstraction interface into deepspeed/\n\n* Fix error message in fp16/fused_optimizer\n\n* fix error message in fp16/unfused_optimizer.py\n\n* assign get_accelerator().pin_memory() result to input Tensor name\n\n* no need to check cuda and whether nvtx supported\n\n* move try-except into inner most block\n\n* call Event() and Stream() in get_accelerator() for data type\n\n* Make Stream and Event as properties of abstract interface so they can be used as data type in deepspeed\n\n* Apply op_builder backend api change from #2705 from @jeffra\n\n* fix tests where Builder NAME is used\n\n* keep original ...Builder.NAME interface instead of ...Builder().NAME interface\n\n* fix builder closure for installation\n\n* fix randomltd builder\n\n* add comments to clarify create_op_builder and get_op_builder\n\n* fix compatibility with pip install -e\n\nCo-authored-by: Cheng Li <pistasable@gmail.com>\nCo-authored-by: Olatunji Ruwase <olruwase@microsoft.com>\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class DSClipEncoder(torch.nn.Module):",
            "seq_len,",
            "seq_len,",
            "dtype=dtype,",
            "-                           device=torch.cuda.current_device())",
            "+                           device=get_accelerator().current_device_name())",
            "mask.fill_(torch.tensor(torch.finfo(dtype).min))",
            "mask.triu_(1)",
            "mask = mask.unsqueeze(1)"
        ]
    },
    {
        "number": 784,
        "comments": "change value",
        "commit_message": "[ConvNeXT] Fix drop_path_rate (#17280)\n\n* Fix drop_path_rate\n\n* Fix TF's drop path rate\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class ConvNextEncoder(nn.Module):",
            "out_channels=out_chs,",
            "stride=2 if i > 0 else 1,",
            "depth=config.depths[i],",
            "-                drop_path_rates=drop_path_rates[cur],",
            "+                drop_path_rates=drop_path_rates[i],",
            ")",
            "self.stages.append(stage)",
            "-            cur += config.depths[i]",
            "prev_chs = out_chs",
            "",
            "def forward("
        ]
    },
    {
        "number": 785,
        "comments": "add condition check for resource fix",
        "commit_message": "fix some compatibility problems with PyTorch 1.3.0 in ESPnet\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class CTC(torch.nn.Module):",
            "if self.ctc_type == \"warpctc\":",
            "# warpctc only supports float32",
            "ys_hat = ys_hat.to(dtype=torch.float32)",
            "+        else:",
            "+            # use GPU when using the cuDNN implementation",
            "+            ys_true = to_device(self, ys_true)",
            "self.loss = to_device(self, self.loss_fn(ys_hat, ys_true, hlens, olens)).to(dtype=dtype)",
            "if self.reduce:",
            "# NOTE: sum() is needed to keep consistency since warpctc return as tensor w/ shape (1,)"
        ]
    },
    {
        "number": 786,
        "comments": "format",
        "commit_message": "Fix indentation to be self-consistent (#279)\n\n* Fix indentation to be self-consistent\n\nReplace 2-space with 4-space indentation\n\n* Fix indentation to be self-consistent\n\nReplace 2-space with 4-space indentation\n\n* Fix indentation\n\nReplace 3-space indentation with 4-space indentation\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "for epoch in range(1, args.epochs + 1):",
            "test(epoch)",
            "sample = Variable(torch.randn(64, 20))",
            "if args.cuda:",
            "-       sample = sample.cuda()",
            "+        sample = sample.cuda()",
            "sample = model.decode(sample).cpu()",
            "save_image(sample.data.view(64, 1, 28, 28),",
            "'results/sample_' + str(epoch) + '.png')"
        ]
    },
    {
        "number": 788,
        "comments": "test",
        "commit_message": "Fix test for uint8\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def test_uint8_representation_not_allowed_with_negative_values(workers):",
            "",
            "def test_uint_representation(workers):",
            "x = torch.tensor([[1.5, 2.0, 3.0], [4.5, 5.0, 6.0]])",
            "-    enlarged = x.fix_prec(internal_type=torch.int16, precision_fractional=256)",
            "+    enlarged = x.fix_prec(internal_type=torch.uint8, precision_fractional=256)",
            "restored = enlarged.float_precision()",
            "# And now x and restored must be the same",
            "assert torch.all(torch.eq(x, restored))"
        ]
    },
    {
        "number": 789,
        "comments": "change API call for version fix",
        "commit_message": "Data Input Refactoring\n\n- Data input pipeline can now deal properly with missing target data.\nFixes #101\n- Moved some of the featurizer functionality into the data reader and\nmade featurizer implicit part of the model.\n- Misc: Replace concat with concat_v2\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class AttentionDecoder(DecoderBase):",
            "])",
            "else:",
            "attention_context = output.attention_context",
            "-    return tf.concat(1, [next_input, attention_context])",
            "+    return tf.concat_v2([next_input, attention_context], 1)",
            "",
            "def _pad_att_scores(self, scores):",
            "\"\"\"Pads attention scores to fixed length. This is a hack because raw_rnn"
        ]
    },
    {
        "number": 791,
        "comments": "change param for math fix",
        "commit_message": "fix test\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def test_graph_saint():",
            "assert sample.node_norm.numel() == sample.num_nodes",
            "assert sample.edge_norm.numel() == sample.num_edges",
            "",
            "+    torch.manual_seed(12345)",
            "loader = GraphSAINTRandomWalkSampler(data, batch_size=2, walk_length=1,",
            "-                                         num_steps=4, log=False)",
            "+                                         num_steps=4, sample_coverage=10,",
            "+                                         log=False)",
            "",
            "for sample in loader:",
            "assert len(sample) == 4"
        ]
    },
    {
        "number": 792,
        "comments": "remove API call for type fix",
        "commit_message": "[torch] remove deprecated uint8 in favor of bool (#21384)\n\n* uint8 -> bool\n\n* fix copies\n\n* style\n\n* update test modeling commen when checking attention buffers\n\n* style\n\n* use logical not on random mask instead of subtraction with 1\n\n* remove torch uint8\n\n* quality\n\n* remove modified modeling utils\n\n* Update based on review\n\nCo-authored-by: sgugger <sylvain.gugger@gmail.com>\n\n---------\n\nCo-authored-by: sgugger <sylvain.gugger@gmail.com>\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class GPTJAttention(nn.Module):",
            "):",
            "# compute causal mask from causal mask buffer",
            "query_length, key_length = query.size(-2), key.size(-2)",
            "-        causal_mask = self.bias[:, :, key_length - query_length : key_length, :key_length].to(torch.bool)",
            "+        causal_mask = self.bias[:, :, key_length - query_length : key_length, :key_length]",
            "",
            "# Keep the attention weights computation in fp32 to avoid overflow issues",
            "query = query.to(torch.float32)"
        ]
    },
    {
        "number": 793,
        "comments": "format",
        "commit_message": "fix lint errors\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class Tester(unittest.TestCase):",
            "",
            "# create checkerboard",
            "board = utils.create_checkerboard(height, width, 4)",
            "-        patch_src = torch.from_numpy(board).view( \\",
            "+        patch_src = torch.from_numpy(board).view(",
            "1, 1, height, width).expand(batch_size, 1, height, width)",
            "patch_src = utils.tensor_to_gradcheck_var(patch_src)  # to var"
        ]
    },
    {
        "number": 794,
        "comments": "format",
        "commit_message": "upgrade to pytorch 0.4.0 (#1126)\n\n* bump pytorch to 0.4 + fix sanitize\n\n* remove check for Variable in block_orthogonal\n\n* rename parameter split_size=\n\n* remove checks for Variable\n\n* fix more tests\n\n* fixes\n\n* get tests to pass\n\n* fix warnings\n\n* get rid of some of the Variables\n\n* more tests passing\n\n* more elimination of variables\n\n* finish removing all Variables\n\n* pylint and such\n\n* a few fixes\n\n* move torch.no_grad into model.forward_on_instances\n\n* more pytorch 0.4 changes\n\n* detach() -> data\n\n* pylint\n\n* fix bad tensor creation\n\n* fix types\n\n* remove print statement\n\n* more 0.4 goodness\n\n* factor out is_tensor\n\n* add no_grad to elmo command\n\n* cleanup\n\n* remove TODO\n\n* address PR feedback\n\n* replace all() with item()\n\n* more cleanup\n\n* further cleanup\n\n* remove Variable\n\n* really fix merge conflict\n\n* fix pylint\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def main(serialization_directory, device):",
            "iterator.index_with(model.vocab)",
            "",
            "model_predictions = []",
            "-    batches = iterator(instances, num_epochs=1, shuffle=False, cuda_device=device, for_training=False)",
            "+    batches = iterator(instances, num_epochs=1, shuffle=False, cuda_device=device)",
            "for batch in Tqdm.tqdm(batches):",
            "result = model(**batch)",
            "predictions = model.decode(result)"
        ]
    },
    {
        "number": 795,
        "comments": "no API",
        "commit_message": "Bugfixes (#1159)\n\nSummary:\nSeveral bugfixes to get tests passing on OSS master\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1159\n\nReviewed By: ngoyal2707\n\nDifferential Revision: D21331993\n\nPulled By: myleott\n\nfbshipit-source-id: 327ae19f6797f92b8c6083a49d5f5edb0872223e\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class TestExportModels(unittest.TestCase):",
            "_test_save_and_load(scripted)",
            "",
            "@unittest.skipIf(",
            "-        torch.__version__ < \"1.5.0\", \"Targeting OSS scriptability for the 1.5 release\"",
            "+        torch.__version__ < \"1.6.0\", \"Targeting OSS scriptability for the 1.6 release\"",
            ")",
            "def test_export_transformer(self):",
            "task, parser = get_dummy_task_and_parser()"
        ]
    },
    {
        "number": 796,
        "comments": "no API check version fix",
        "commit_message": "Fix parsing Keras version identifiers like `2.2.4-tf` (#3794)\n\nSigned-off-by: Max H. Gerlach <git@maxgerlach.de>\n",
        "label": "no",
        "answer": "no",
        "change": [
            "from horovod.tensorflow.keras import callbacks, elastic",
            "try:",
            "# In later versions of TensorFlow, optimizers are spread across multiple modules. This set is used to distinguish",
            "# stock optimizers that come with tf.keras from custom optimizers that may need to be wrapped specially.",
            "-    if version.parse(keras.__version__) < version.parse(\"2.11\"):",
            "+    if version.parse(keras.__version__.replace(\"-tf\", \"+tf\")) < version.parse(\"2.11\"):",
            "optimizer_type = tf.keras.optimizers.Optimizer",
            "else:",
            "optimizer_type = keras.optimizers.legacy.Optimizer"
        ]
    },
    {
        "number": 798,
        "comments": "format",
        "commit_message": "Fix bug in reparameterize() (#419)\n\nThe reparameterize() function should not be deterministic for validation/test mode. It should be the same as in training mode, otherwise what is computed is not the ELBO (i.e. the correct loss) but something else. This change will slightly change the test set error.\n",
        "label": "no",
        "answer": "no",
        "change": [
            "if __name__ == \"__main__\":",
            "sample = torch.randn(64, 20).to(device)",
            "sample = model.decode(sample).cpu()",
            "save_image(sample.view(64, 1, 28, 28),",
            "-                       'results/sample_' + str(epoch) + '.png')",
            "\\ No newline at end of file",
            "+                       'results/sample_' + str(epoch) + '.png')"
        ]
    },
    {
        "number": 800,
        "comments": "update API call for refactor fix",
        "commit_message": "Update to PyTorch 1.9.0 (#2887)\n\n* Update to PyTorch 1.9.0\n\n* Update torch.cholesky, torch.symeig\n\n* Fix torch.tensordot, torch.qr, funsor dependency\n\n* Fix torch import, AutoGuideList usage\n\n* Ignore bug in PyTorch jit\n\n* Ignore tracer warnings\n\n* Replace torch.solve with torch.linalg.solve\n\n* Xfail vectorized markov funsor tests\n\n* Update funsor version\n\n* Switch MNIST mirrors\n\n* Pin pillow version\n\n* Bump torchvision version\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class GeneralizedChannelPermute(ConditionedGeneralizedChannelPermute, TransformM",
            "self.__delattr__('permutation')",
            "",
            "# Sample a random orthogonal matrix",
            "-        W, _ = torch.qr(torch.randn(channels, channels))",
            "+        W, _ = torch.linalg.qr(torch.randn(channels, channels))",
            "",
            "# Construct the partially pivoted LU-form and the pivots",
            "LU, pivots = W.lu()"
        ]
    },
    {
        "number": 802,
        "comments": "change param for shape fix",
        "commit_message": "Fixed nightly errors (#886)\n\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class TestConfusionMatrix:",
            "conf_mat = kornia.utils.metrics.confusion_matrix(",
            "predicted, actual, num_classes)",
            "conf_mat_real = torch.tensor(",
            "-            [[[3, 1],",
            "-              [0, 4]]], dtype=torch.float32)",
            "+            [",
            "+                [[3, 1], [0, 4]],",
            "+                [[3, 1], [0, 4]]",
            "+            ], dtype=torch.float32)",
            "assert_allclose(conf_mat, conf_mat_real)",
            "",
            "def test_three_classes(self):"
        ]
    },
    {
        "number": 803,
        "comments": "update param for type fix",
        "commit_message": "remove graph api (#818)\n\n* remove graph api\n\n* changelog\n\n* timeout test\n\n* codacy\n\n* remove graph in tl.models\n\n* increase timeout time\n\n* remove tl in tests\n\n* Additional Cleaning\n\n* Timeout time added\n\n* tests directory refactored\n\n* Tests Fix Update python, 2.6\n\n* YAPF Cleaning\n\n* get_env fix\n\n* Python 2 fix applied\n\n* Python 2 Error fixes\n\n* TL 1.10.1rc0 released\n\n* Doc and YAPF Fix\n\n* Test YAPF Fix\n\n* RTD Lazy Import Fix\n\n* Travis config restored\n\n* Revert \"Travis config restored\"\n\nThis reverts commit 03fe83eb568c91765461012a84bde309b8d15e69.\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def model(x, is_train, reuse):",
            "# nt = Conv2d(nin, 16, (3, 3), (2, 2), act=tf.nn.relu, padding='SAME', name='tc1')",
            "# nt = Conv2d(nt, 8, (3, 3), (2, 2), act=tf.nn.relu, padding='SAME', name='tc2')",
            "## 2. Spatial transformer module (sampler)",
            "-        n = tl.layers.SpatialTransformer2dAffineLayer(nin, theta_layer=nt, out_size=[40, 40], name='spatial')",
            "+        n = tl.layers.SpatialTransformer2dAffineLayer(nin, theta_layer=nt, out_size=(40, 40), name='spatial')",
            "s = n",
            "## 3. Classifier",
            "n = tl.layers.Conv2d("
        ]
    },
    {
        "number": 804,
        "comments": "change API call for shape fix",
        "commit_message": "\ud83d\udea8 \ud83d\udea8 \ud83d\udea8 Fix ViT parameter initialization (#19341)\n\nThis PR aims to rectify the discrepancy between the training performances of HF and Timm ViT implementations.\n\n- Initializes torch and flax ViT dense layer weights with trunc_normal instead of normal (consistent with the TF implementation.\n- Initializes cls_token and positional_embeddings with trunc_normal\n- Updates DeiT copy to reflect the changes\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class DeiTPreTrainedModel(PreTrainedModel):",
            "def _init_weights(self, module: Union[nn.Linear, nn.Conv2d, nn.LayerNorm]) -> None:",
            "\"\"\"Initialize the weights\"\"\"",
            "if isinstance(module, (nn.Linear, nn.Conv2d)):",
            "-            # Slightly different from the TF version which uses truncated_normal for initialization",
            "-            # cf https://github.com/pytorch/pytorch/pull/5617",
            "-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)",
            "+            module.weight.data = nn.init.trunc_normal_(module.weight.data, mean=0.0, std=self.config.initializer_range)",
            "if module.bias is not None:",
            "module.bias.data.zero_()",
            "elif isinstance(module, nn.LayerNorm):"
        ]
    },
    {
        "number": 806,
        "comments": "change condition check for type fix",
        "commit_message": "Refactoring, cleanup, improved test coverage.\n* Add eca_nfnet_l2 weights, 84.7 @ 384x384\n* All 'non-std' (ie transformer / mlp) models have classifier / default_cfg test added\n* Fix #694 reset_classifer / num_features / forward_features / num_classes=0 consistency for transformer / mlp models\n* Add direct loading of npz to vision transformer (pure transformer so far, hybrid to come)\n* Rename vit_deit* to deit_*\n* Remove some deprecated vit hybrid model defs\n* Clean up classifier flatten for conv classifiers and unusual cases (mobilenetv3/ghostnet)\n* Remove explicit model fns for levit conv, just pass in arg\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class SelectAdaptivePool2d(nn.Module):",
            "assert False, 'Invalid pool type: %s' % pool_type",
            "",
            "def is_identity(self):",
            "-        return self.pool_type == ''",
            "+        return not self.pool_type",
            "",
            "def forward(self, x):",
            "x = self.pool(x)",
            "-        if self.flatten:",
            "-            x = x.flatten(1)",
            "+        x = self.flatten(x)",
            "return x",
            "",
            "def feat_mult(self):"
        ]
    },
    {
        "number": 807,
        "comments": "log fix",
        "commit_message": "Fix tests. Disable verbose test logging.\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class AttentionDecoderTest(tf.test.TestCase, DecoderTests):",
            "\"\"\"",
            "def setUp(self):",
            "tf.test.TestCase.setUp(self)",
            "+    tf.logging.set_verbosity(tf.logging.INFO)",
            "DecoderTests.__init__(self)",
            "self.attention_dim = 64",
            "self.input_seq_len = 10"
        ]
    },
    {
        "number": 808,
        "comments": "add API call for type fix",
        "commit_message": "Fix matmul inputs dtype (#18585)\n\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class DisentangledSelfAttention(nn.Module):",
            "dim=-1,",
            "index=p2c_pos.squeeze(0).expand([query_layer.size(0), key_layer.size(-2), key_layer.size(-2)]),",
            ").transpose(-1, -2)",
            "-            score += p2c_att / scale",
            "+            score += p2c_att / torch.tensor(scale, dtype=p2c_att.dtype)",
            "",
            "return score"
        ]
    },
    {
        "number": 809,
        "comments": "config replace rename",
        "commit_message": "Fix Tutorial 9  (#734)\n\n* Add package download\n\n* Change dev to train file\n",
        "label": "no",
        "answer": "no",
        "change": [
            "\"\\n\",",
            "\"retriever.train(\\n\",",
            "\"    data_dir=doc_dir,\\n\",",
            "-    \"    train_filename=dev_filename,\\n\",",
            "+    \"    train_filename=train_filename,\\n\",",
            "\"    dev_filename=dev_filename,\\n\",",
            "\"    test_filename=dev_filename,\\n\",",
            "\"    n_epochs=1,\\n\","
        ]
    },
    {
        "number": 812,
        "comments": "add condition check for resource fix",
        "commit_message": "fix some code\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def evaluate(model, data_loader, device):",
            "image = list(img.to(device) for img in image)",
            "targets = [{k: v.to(device) for k, v in t.items()} for t in targets]",
            "",
            "-        torch.cuda.synchronize(device)",
            "+        # \u5f53\u4f7f\u7528CPU\u65f6\uff0c\u8df3\u8fc7GPU\u76f8\u5173\u6307\u4ee4",
            "+        if device != torch.device(\"cpu\"):",
            "+            torch.cuda.synchronize(device)",
            "+",
            "model_time = time.time()",
            "outputs = model(image)"
        ]
    },
    {
        "number": 813,
        "comments": "remove param for argument fix",
        "commit_message": "minor fixe lambda test\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class Layer_Lambda_Test(CustomTestCase):",
            "self.dense1 = tl.layers.Dense(in_channels=1, n_units=5)",
            "self.dense2 = tl.layers.Dense(in_channels=1, n_units=5)",
            "self.dense3 = tl.layers.Dense(in_channels=1, n_units=5)",
            "-                self.lambdalayer = tl.layers.ElementwiseLambda(customize_func, fn_weights=[], fn_args={'foo': 1024})",
            "+                self.lambdalayer = tl.layers.ElementwiseLambda(customize_func, fn_args={'foo': 1024})",
            "",
            "def forward(self, x, bar=None):",
            "noise = self.dense1(x)"
        ]
    },
    {
        "number": 814,
        "comments": "rename",
        "commit_message": "mass linter fix\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class TacotronTrainTest(unittest.TestCase):",
            "optimizer = optim.Adam(model.parameters(), lr=c.lr)",
            "for i in range(5):",
            "mel_out, mel_postnet_out, align, stop_tokens = model.forward(",
            "-                input, input_lengths, mel_spec, mel_lengths, speaker_ids)",
            "+                input_dummy, input_lengths, mel_spec, mel_lengths, speaker_ids)",
            "assert torch.sigmoid(stop_tokens).data.max() <= 1.0",
            "assert torch.sigmoid(stop_tokens).data.min() >= 0.0",
            "optimizer.zero_grad()"
        ]
    },
    {
        "number": 815,
        "comments": "format",
        "commit_message": "lint fix\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def scatter_nd(",
            "initial_val = torch.tensor(0).type(dtype)",
            "elif reduction == \"min\":",
            "if dtype.is_floating_point:",
            "-            initial_val = min(torch.finfo(dtype).max, 1e12)",
            "+            initial_val = min(torch.finfo(dtype).max, 1e12)",
            "else:",
            "initial_val = min(torch.iinfo(dtype).max, 1e12)",
            "elif reduction == \"max\":",
            "if dtype.is_floating_point:",
            "-            initial_val = max(torch.finfo(dtype).min, 1e-12)",
            "+            initial_val = max(torch.finfo(dtype).min, 1e-12)",
            "else:",
            "initial_val = max(torch.iinfo(dtype).min, 1e-12)",
            "else:"
        ]
    },
    {
        "number": 816,
        "comments": "format",
        "commit_message": "Fixes #79: Added custom dropout op, that adds probability tensor to the collection DROPOUTS and zeros it on prediction\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "def dnn(tensor_in, hidden_units, activation=tf.nn.relu, keep_prob=None):",
            "for i, n_units in enumerate(hidden_units):",
            "with tf.variable_scope('layer%d' % i):",
            "tensor_in = linear(tensor_in, n_units, True)",
            "-            tensor_in = activation(tensor_in)",
            "-            if keep_prob:",
            "-                tensor_in = tf.nn.dropout(tensor_in, keep_prob)",
            "+                tensor_in = activation(tensor_in)",
            "+                if keep_prob:",
            "+                    tensor_in = skflow.ops.dropout(tensor_in, keep_prob)",
            "return tensor_in"
        ]
    },
    {
        "number": 819,
        "comments": "doc update",
        "commit_message": "[Feature]: Evaluation with TensorRT backend (#5198)\n\n* evaluate trt models\n\n* update version of onnx\n\n* update maskrcnn results\n\n* add backend argument\n\n* update fcos results\n\n* update\n\n* fix bug\n\n* update  doc\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class SingleStageDetector(BaseDetector):",
            "# get shape as tensor",
            "img_shape = torch._shape_as_tensor(img)[2:]",
            "img_metas[0]['img_shape_for_onnx'] = img_shape",
            "+        # get pad input shape to support onnx dynamic shape for exporting",
            "+        # `CornerNet` and `CentripetalNet`, which 'pad_shape' is used",
            "+        # for inference",
            "+        img_metas[0]['pad_shape_for_onnx'] = img_shape",
            "# TODO:move all onnx related code in bbox_head to onnx_export function",
            "det_bboxes, det_labels = self.bbox_head.get_bboxes(*outs, img_metas)"
        ]
    },
    {
        "number": 820,
        "comments": "no API",
        "commit_message": "[style] consistent nn. and nn.functional (#12124)\n\n* consistent nn. and nn.functional\n\n* fix glitch\n\n* fix glitch #2\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def convert_bort_checkpoint_to_pytorch(bort_checkpoint_path: str, pytorch_dump_f",
            "# | `encoder.transformer_cells.*.proj.weight`                      | `bert.encoder.layer.*.output.dense.weight`",
            "",
            "# Helper function to convert MXNET Arrays to PyTorch",
            "-    def to_torch(mx_array) -> torch.nn.Parameter:",
            "-        return torch.nn.Parameter(torch.FloatTensor(mx_array.data().asnumpy()))",
            "+    def to_torch(mx_array) -> nn.Parameter:",
            "+        return nn.Parameter(torch.FloatTensor(mx_array.data().asnumpy()))",
            "",
            "# Check param shapes and map new HF param back",
            "def check_and_map_params(hf_param, gluon_param):"
        ]
    },
    {
        "number": 821,
        "comments": "add condition check for resource fix",
        "commit_message": "Misc fixes (#2342)\n\nSummary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/2342\n\nReviewed By: ngoyal2707\n\nDifferential Revision: D22601110\n\nPulled By: myleott\n\nfbshipit-source-id: 7a704c07d507692f274c31ec74b090134fa9dee3\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def clip_grad_norm_(params, max_norm, aggregate_norm_fn=None) -> torch.Tensor:",
            "if multi_tensor_l2norm_available:",
            "total_norm = multi_tensor_total_norm(grads)",
            "else:",
            "-            warnings.warn(",
            "-                \"amp_C fused kernels unavailable, disabling multi_tensor_l2norm; \"",
            "-                \"you may get better performance by installing NVIDIA's apex library\"",
            "-            )",
            "+            if torch.cuda.is_available():",
            "+                warnings.warn(",
            "+                    \"amp_C fused kernels unavailable, disabling multi_tensor_l2norm; \"",
            "+                    \"you may get better performance by installing NVIDIA's apex library\"",
            "+                )",
            "total_norm = torch.norm(",
            "torch.stack([torch.norm(g, p=2, dtype=torch.float32) for g in grads])",
            ")"
        ]
    },
    {
        "number": 823,
        "comments": "add condition check for type fix",
        "commit_message": "fix data device type bug (#3856)\n\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class ModelSpeedupTensorRT(BaseModelSpeedup):",
            "Model input tensor",
            "\"\"\"",
            "# convert pytorch tensor to numpy darray",
            "+        if test_data.device != torch.device(\"cpu\"):",
            "+            test_data = test_data.to(\"cpu\")",
            "test_data = test_data.numpy()",
            "# Numpy dtype should be float32",
            "assert test_data.dtype == np.float32"
        ]
    },
    {
        "number": 824,
        "comments": "no API",
        "commit_message": "fixed the case when r > 1\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "class Dio(AbsFeatsExtract):",
            "",
            "return f0",
            "",
            "-    @staticmethod",
            "-    def _average_by_duration(x: torch.Tensor, d: torch.Tensor) -> torch.Tensor:",
            "-        assert d.sum() == len(x)",
            "+    def _average_by_duration(self, x: torch.Tensor, d: torch.Tensor) -> torch.Tensor:",
            "+        assert len(x) - d.sum() < self.reduction_factor",
            "d_cumsum = F.pad(d.cumsum(dim=0), (1, 0))",
            "x_avg = [",
            "x[start:end].masked_select(x[start:end].gt(0.0)).mean(dim=0)"
        ]
    },
    {
        "number": 826,
        "comments": "rename",
        "commit_message": "refactor of the optimization interface (#212)\n\n* initial commit\n\n* optim.py\n\n* basic thing working\n\n* got some tests to pass\n\n* fixedguidemodel test happy\n\n* more comments, default implementation for loss_and_grads, ...\n\n* change import\n\n* got a lot more tests to pass; a tiny bit inference test refactoring\n\n* flake8\n\n* fix more tests\n\n* made conjugate model tests faster; made example test pass; all tests pass now maybe?\n\n* made conjugate model tests faster; made example test pass; all tests pass now maybe?\n\n* xfail test_examples\n\n* xfail test_examples flake8\n\n* made tests/poutine/test_mapdata.py happy\n\n* fix test lambda test\n\n* more refactoring...\n\n* more refactoring...\n\n* progress on further rerefactoring\n\n* always more refactoring\n\n* linting\n\n* done with initial re-refactor?\n\n* fix bug\n\n* fl8\n\n* address tests\n\n*  make examples happy\n\n* fix tests. fixed everything?\n\n* final clean-up?\n\n* assert fix\n\n* sets not hashable error\n\n* exclusively doc string changes\n\n* batch rearrange\n\n* fix fake conflict\n\n* fl8\n\n* doc rebuild workaround\n\n* fix random module test (tags)\n\n* wrap loss\n\n* rebalance tests\n\n* whoops\n\n* fix more merge conflicts; fix test imports\n\n* fl8\n\n*  delete old tracegraph_kl_qp.py\n\n* travis workaround\n\n* better rng seeds/inits?\n\n* fix travis yml\n\n* these seeds/inits should be good\n\n* fix data\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def main():",
            "parser.add_argument('-n', '--num-epochs', nargs='?', default=1000, type=int)",
            "args = parser.parse_args()",
            "for step in range(args.num_epochs):",
            "-        kl_optim.step(observed_data)  # loss",
            "+        svi.step(observed_data)  # loss",
            "if step % 100 == 0:",
            "if verbose:",
            "print(\"[epoch %d] mean_mu: %.3f\" % (step, pyro.param(\"mean_mu\").data[0]))"
        ]
    },
    {
        "number": 828,
        "comments": "add condition check for type fix",
        "commit_message": "Fix #1712 broken support for AMP w/ PyTorch < 1.10. Disable loss scaler for bfloat16\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def main():",
            "if utils.is_primary(args):",
            "_logger.info('Using NVIDIA APEX AMP. Training in mixed precision.')",
            "elif use_amp == 'native':",
            "-        amp_autocast = partial(torch.autocast, device_type=device.type, dtype=amp_dtype)",
            "-        if device.type == 'cuda':",
            "+        try:",
            "+            amp_autocast = partial(torch.autocast, device_type=device.type, dtype=amp_dtype)",
            "+        except (AttributeError, TypeError):",
            "+            # fallback to CUDA only AMP for PyTorch < 1.10",
            "+            assert device.type == 'cuda'",
            "+            amp_autocast = torch.cuda.amp.autocast",
            "+        if device.type == 'cuda' and amp_dtype == torch.float16:",
            "+            # loss scaler only used for float16 (half) dtype, bfloat16 does not need it",
            "loss_scaler = NativeScaler()",
            "if utils.is_primary(args):",
            "_logger.info('Using native Torch AMP. Training in mixed precision.')"
        ]
    },
    {
        "number": 829,
        "comments": "remove API call for version fix",
        "commit_message": "Previously, many unit test files started with `enable_v2_behavior`, which would have caused them to run in V2 mode when executing with a V1 test flag. The correct behavior would in fact be to skip such tests when executing with a V1 test flag.\n\nThis fix significantly reduces the total V1 + V2 test load by eliminating redundancy.\n\nPiperOrigin-RevId: 424734850\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class AutoOutsideCompilationWithKerasTest(tf.test.TestCase):",
            "",
            "",
            "if __name__ == '__main__':",
            "-  tf.compat.v1.enable_eager_execution()",
            "tf.test.main()"
        ]
    },
    {
        "number": 830,
        "comments": "format",
        "commit_message": "hyperpose compatible:\n(1)maxpool and batchnorm dataformat debuged,support \"channels_first\"\n(2)vgg forward fixed\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class BatchNorm(Layer):",
            "if self.axes is None:",
            "self.axes = [i for i in range(len(inputs.shape)) if i != self.channel_axis]",
            "",
            "+        mean, var = tf.nn.moments(inputs, self.axes, keepdims=False)",
            "if self.is_train:",
            "# update moving_mean and moving_var",
            "-            mean, var = tf.nn.moments(inputs, self.axes, keepdims=False)",
            "self.moving_mean = moving_averages.assign_moving_average(",
            "self.moving_mean, mean, self.decay, zero_debias=False",
            ")"
        ]
    },
    {
        "number": 831,
        "comments": "doc update",
        "commit_message": "fix bug cache dataset, cache should before shuffle and batch.\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class WeightNormalization(WeightNormalizationOriginal):",
            "",
            "def build(self, input_shape):",
            "\"\"\"Build `Layer`\"\"\"",
            "-        #input_shape = tf.TensorShape(input_shape)",
            "-        #self.input_spec = tf.keras.layers.InputSpec(shape=[None] + input_shape[1:])",
            "+        # input_shape = tf.TensorShape(input_shape)",
            "+        # self.input_spec = tf.keras.layers.InputSpec(shape=[None] + input_shape[1:])",
            "",
            "# remove 2 lines above to run weight-norm on tf.function with dynamic shape"
        ]
    },
    {
        "number": 834,
        "comments": "format",
        "commit_message": "lint fixes (#5332)\n\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def trace(",
            "offset: int = 0,",
            "axis1: int = 0,",
            "axis2: int = 1,",
            "-    out: Optional[torch.Tensor] = None",
            "+    out: Optional[torch.Tensor] = None,",
            ") -> torch.Tensor:",
            "ret = torch.diagonal(x, offset=offset, dim1=axis1, dim2=axis2)",
            "ret = torch.sum(ret)"
        ]
    },
    {
        "number": 836,
        "comments": "rename",
        "commit_message": "ugly fix of MODEL_KEY\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class ValidationCallback(PeriodicCallback):",
            "self.cost_var_name = cost_var_name",
            "",
            "def _before_train(self):",
            "-        self.input_vars = tf.get_collection(MODEL_KEY)[0].get_input_vars()",
            "+        self.input_vars = tf.get_collection(INPUT_VARS_KEY)",
            "self.cost_var = self.get_tensor(self.cost_var_name)",
            "self._find_output_vars()"
        ]
    },
    {
        "number": 837,
        "comments": "add API call for type fix",
        "commit_message": "fixed to keep compatibility\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class Encoder(torch.nn.Module):",
            "pos_enc_class(attention_dim, positional_dropout_rate),",
            ")",
            "elif input_layer is None:",
            "-            self.embed = pos_enc_class(attention_dim, positional_dropout_rate)",
            "+            self.embed = torch.nn.Sequential(",
            "+                pos_enc_class(attention_dim, positional_dropout_rate)",
            "+            )",
            "else:",
            "raise ValueError(\"unknown input_layer: \" + input_layer)",
            "self.normalize_before = normalize_before"
        ]
    },
    {
        "number": 838,
        "comments": "add param for math fix",
        "commit_message": "Cleanup the usage of `layer_norm_eps` in some models (#21336)\n\n* fix\n\n* fix\n\n* make style\n\n* For CLIP\n\n* For OwlViT\n\n* For XCLIP\n\n* For CLIPSeg\n\n* For GroupViT\n\n* fix docstrings\n\n* fix docstrings\n\n* For AltCLIP\n\n* For ChineseCLIP\n\n* For Blip\n\n* For GiT\n\n* make style\n\n* update\n\n* update\n\n* update\n\n* fix\n\n* fix\n\n* fix\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class ChineseCLIPVisionTransformer(nn.Module):",
            "embed_dim = config.hidden_size",
            "",
            "self.embeddings = ChineseCLIPVisionEmbeddings(config)",
            "-        self.pre_layrnorm = nn.LayerNorm(embed_dim)",
            "+        self.pre_layrnorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)",
            "self.encoder = ChineseCLIPVisionEncoder(config)",
            "-        self.post_layernorm = nn.LayerNorm(embed_dim)",
            "+        self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)",
            "",
            "@add_start_docstrings_to_model_forward(CHINESE_CLIP_VISION_INPUTS_DOCSTRING)",
            "@replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=ChineseCLIPVisionConfig)"
        ]
    },
    {
        "number": 839,
        "comments": "format",
        "commit_message": "fix issues with outer\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def svd(",
            "def outer(",
            "x1: torch.Tensor, x2: torch.Tensor, out: Optional[torch.Tensor] = None",
            ") -> torch.Tensor:",
            "-    ret = torch.outer(x1, x2, out=out)",
            "-    return ret",
            "+    return torch.outer(x1, x2, out=out)",
            "",
            "",
            "def diagonal("
        ]
    },
    {
        "number": 840,
        "comments": "add API call for math fix",
        "commit_message": "fix failing tests related to `solve_cast` on torch 1.9 (#2066)\n\n* update torch seed\n\n* rerun CI\n\n* update to use rescale\n\n* rerun ci\n\n* update tests cases to have hardcoded input\n\ndeleted:    test/utilities.py - not used anywhere\n\n* remove fail fast at ci\n\n* manual seed 0 TestImageStitcher::test_smoke\n\n* manual seed 0 TestHomographyTracker::test_real\n\n* manual seed 6 TestHomographyTracker::test_real\n\n* manual seed 245 TestImageStitcher::test_smoke and hardcoded case\n\n* manual seed 1 TestImageStitcher::test_smoke\n\n* manual seed 8 TestHomographyTracker::test_real\n\n- Comment the second frame test\n\n* rerun CI\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class TestSolveCast:",
            "",
            "class TestSolveWithMask:",
            "def test_smoke(self, device, dtype):",
            "+        torch.manual_seed(0)  # issue kornia#2027",
            "A = torch.randn(2, 3, 1, 4, 4, device=device, dtype=dtype)",
            "B = torch.randn(2, 3, 1, 4, 6, device=device, dtype=dtype)"
        ]
    },
    {
        "number": 842,
        "comments": "change param for type fix",
        "commit_message": "Update serving signatures and make sure we actually use them (#19034)\n\n* Override save() to use the serving signature as the default\n\n* Replace int32 with int64 in all our serving signatures\n\n* Remember one very important line so as not to break every test at once\n\n* Dtype fix for TFLED\n\n* dtype fix for shift_tokens_right in general\n\n* Dtype fixes in mBART and RAG\n\n* Fix dtypes for test_unpack_inputs\n\n* More dtype fixes\n\n* Yet more mBART + RAG dtype fixes\n\n* Yet more mBART + RAG dtype fixes\n\n* Add a check that the model actually has a serving method\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class TFHubertPreTrainedModel(TFPreTrainedModel):",
            "input_signature=[",
            "{",
            "\"input_values\": tf.TensorSpec((None, None), tf.float32, name=\"input_values\"),",
            "-                \"attention_mask\": tf.TensorSpec((None, None), tf.int32, name=\"attention_mask\"),",
            "-                \"token_type_ids\": tf.TensorSpec((None, None), tf.int32, name=\"token_type_ids\"),",
            "+                \"attention_mask\": tf.TensorSpec((None, None), tf.int64, name=\"attention_mask\"),",
            "+                \"token_type_ids\": tf.TensorSpec((None, None), tf.int64, name=\"token_type_ids\"),",
            "}",
            "]",
            ")"
        ]
    },
    {
        "number": 844,
        "comments": "return change",
        "commit_message": "Metrics in distributed setting (#4525)\n\n* initial commit to ensure that metrics work correctly in distributed setting\n\n* updating global_distributed_metric to take metric object\n\n* adding distributed f1 score\n\n* adding distributed attachment scores\n\n* bug fix\n\n* adding distributed boolean accuracy\n\n* adding distributed entropy\n\n* adding distributed evalb\n\n* adding distributed mean_absolute_error\n\n* adding distributed sequence accuracy\n\n* adding distributed unigram recall\n\n* making models compatible with distributed metrics\n\n* adding distributed auc\n\n* adding distributed bleu\n\n* adding missing argument\n\n* initial commit to ensure that metrics work correctly in distributed setting\n\n* updating global_distributed_metric to take metric object\n\n* adding distributed f1 score\n\n* adding distributed attachment scores\n\n* bug fix\n\n* adding distributed boolean accuracy\n\n* adding distributed entropy\n\n* adding distributed evalb\n\n* adding distributed mean_absolute_error\n\n* adding distributed sequence accuracy\n\n* adding distributed unigram recall\n\n* making models compatible with distributed metrics\n\n* adding distributed auc\n\n* adding distributed bleu\n\n* adding missing argument\n\n* changing start method\n\n* removing unnecessary argument\n\n* adding remaining metrics, removing extra argument\n\n* allowing float values\n\n* bug fix\n\n* more bug fixes\n\n* changing average to return float\n\n* adding timeout for distributed test\n\n* testing unequal batches\n\n* adding distributed auc\n\n* adding distributed spearman correlation\n\n* adding distributed covariance and pearson correlation\n\n* changing distributed test to function, misc changes\n\n* checking batch lengths explicitly to raise errors\n\nCo-authored-by: Dirk Groeneveld <dirkg@allenai.org>\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class Entropy(Metric):",
            "average_value = self._entropy / self._count if self._count > 0 else 0",
            "if reset:",
            "self.reset()",
            "-        return average_value",
            "+        return {\"entropy\": average_value}",
            "",
            "@overrides",
            "def reset(self):"
        ]
    },
    {
        "number": 845,
        "comments": "change param for math fix",
        "commit_message": "fix a bug in test\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def test_maskctc(encoder_arch, interctc_layer_idx, interctc_use_conditioning):",
            "inputs = dict(",
            "speech=torch.randn(2, 10, 20, requires_grad=True),",
            "speech_lengths=torch.tensor([10, 8], dtype=torch.long),",
            "-        text=torch.randint(0, vocab_size + 1, [2, 4], dtype=torch.long),",
            "+        text=torch.randint(2, 4, [2, 4], dtype=torch.long),",
            "text_lengths=torch.tensor([4, 3], dtype=torch.long),",
            ")",
            "loss, *_ = model(**inputs)"
        ]
    },
    {
        "number": 847,
        "comments": "update API call for version fix",
        "commit_message": "fix deprecation warnings; remove pydoop dependency; update README\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def _add_gradients_summaries(grads_and_vars):",
            "grad_values = grad.values",
            "else:",
            "grad_values = grad",
            "-      summaries.append(tf.histogram_summary(var.op.name + ':gradient',",
            "+      summaries.append(tf.summary.histogram(var.op.name + ':gradient',",
            "grad_values))",
            "-      summaries.append(tf.histogram_summary(var.op.name + ':gradient_norm',",
            "+      summaries.append(tf.summary.histogram(var.op.name + ':gradient_norm',",
            "tf.global_norm([grad_values])))",
            "else:",
            "tf.logging.info('Var %s has no gradient', var.op.name)"
        ]
    },
    {
        "number": 848,
        "comments": "value update",
        "commit_message": "chore: add release candidate backwards compatibility warnings (#2512)\n\n* docs updates\n\n* remove unused import\n\n* CI - fetch depth 1\n\n* NotImplementedError for unsupported framework modules\n\n* update supported frameworks doc\n\n* reformat\n\n* fix sklearn\n\n* add backwards compatibility warning for supported ML framework modules\n\n* deprecate use of SAVE_NAMESPACE constant\n\n* framework docs MVP\n\n* template typo and notes\n\n* use a simpler example for reusable runnable class\n\n* fix f-string placeholder warning\n\n* fix sklearn integration tests\n\n* fix pickable model integration test\n",
        "label": "no",
        "answer": "no",
        "change": [
            "except ImportError:  # pragma: no cover",
            ")",
            "",
            "MODULE_NAME = \"bentoml.sklearn\"",
            "-MODEL_FILENAME = f\"{SAVE_NAMESPACE}.{PKL_EXT}\"",
            "+MODEL_FILENAME = \"saved_model.pkl\"",
            "",
            "logger = logging.getLogger(__name__)"
        ]
    },
    {
        "number": 849,
        "comments": "add param for type fix",
        "commit_message": "fix warning\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class QM9(InMemoryDataset):",
            "edge_type += 2 * [self.bonds[bond.GetBondType()]]",
            "",
            "edge_index = torch.tensor([row, col], dtype=torch.long)",
            "-            edge_type = torch.tensor(edge_type)",
            "-            edge_attr = F.one_hot(torch.tensor(edge_type),",
            "+            edge_type = torch.tensor(edge_type, dtype=torch.long)",
            "+            edge_attr = F.one_hot(edge_type,",
            "num_classes=len(self.bonds)).to(torch.float)",
            "",
            "perm = (edge_index[0] * N + edge_index[1]).argsort()"
        ]
    },
    {
        "number": 850,
        "comments": "doc update",
        "commit_message": "fix mymy issues\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "def connect(",
            "self.client_type = client_type",
            "",
            "if credentials:",
            "-                metadata, _user_key = self.conn.login(credentials=credentials)",
            "+                metadata, _user_key = self.conn.login(credentials=credentials)  # type: ignore",
            "_user_key = SigningKey(_user_key.encode(\"utf-8\"), encoder=HexEncoder)",
            "else:",
            "-                metadata = self.conn._get_metadata()",
            "+                metadata = self.conn._get_metadata()  # type: ignore",
            "if not user_key:",
            "_user_key = SigningKey.generate()",
            "else:"
        ]
    },
    {
        "number": 851,
        "comments": "feature",
        "commit_message": "OPT-fix (#17229)\n\n* try fixes\n\n* Revert \"try fixes\"\n\nThis reverts commit a8ad75ef69d4fc03a402ef61bd034b018aa8555e.\n\n* add correct shape\n\n* add correct path\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class OPTEmbeddingsTest(unittest.TestCase):",
            "def test_logits(self):",
            "model = OPTForCausalLM.from_pretrained(self.path_model)",
            "model = model.eval()",
            "-        tokenizer = GPT2Tokenizer.from_pretrained(\"patrickvonplaten/opt_gpt2_tokenizer\")",
            "+        tokenizer = GPT2Tokenizer.from_pretrained(self.path_model)",
            "tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})",
            "",
            "prompts = ["
        ]
    },
    {
        "number": 853,
        "comments": "logger doc print update",
        "commit_message": "Merge pull request #7872 from PyTorchLightning/refactor/logger-poc-changes\n\nRandom fixes for logger connector PoC\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def test_model_checkpoint_options(tmpdir, save_top_k, save_last, expected_files)",
            "for i, loss in enumerate(losses):",
            "trainer.train_loop.current_epoch = i",
            "trainer.train_loop.global_step = i",
            "-        trainer.logger_connector.callback_metrics = {\"checkpoint_on\": torch.tensor(loss)}",
            "+        trainer.logger_connector.callback_metrics.update({\"checkpoint_on\": loss})",
            "checkpoint_callback.on_validation_end(trainer, trainer.lightning_module)",
            "",
            "file_lists = set(os.listdir(tmpdir))"
        ]
    },
    {
        "number": 854,
        "comments": "update API call for version fix",
        "commit_message": "Replace some fs operations by tf.gfile for other fs support. (fix #416)\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class MinSaver(Callback):",
            "newname = os.path.join(logger.LOG_DIR,",
            "self.filename or",
            "('max-' + self.monitor_stat if self.reverse else 'min-' + self.monitor_stat))",
            "-        files_to_copy = glob.glob(path + '*')",
            "+        files_to_copy = tf.gfile.Glob(path + '*')",
            "for file_to_copy in files_to_copy:",
            "-            shutil.copy(file_to_copy, file_to_copy.replace(path, newname))",
            "+            tf.gfile.Copy(file_to_copy, file_to_copy.replace(path, newname), overwrite=True)",
            "logger.info(\"Model with {} '{}' saved.\".format(",
            "'maximum' if self.reverse else 'minimum', self.monitor_stat))"
        ]
    },
    {
        "number": 855,
        "comments": "add API call for state fix",
        "commit_message": "fix prediction (apply pre-processing)\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class Evaluator(object):",
            "The mean average result per tensor over the entire dataset.",
            "",
            "\"\"\"",
            "+        tflearn.is_training(False, self.session)",
            "coord = tf.train.Coordinator()",
            "inputs = tf.get_collection(tf.GraphKeys.INPUTS)",
            "# Data Preprocessing"
        ]
    },
    {
        "number": 857,
        "comments": "add API call for type fix",
        "commit_message": "One more scalar -> tensor fix for lamb optimizer\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class Lamb(Optimizer):",
            "global_grad_norm.add_(grad.pow(2).sum())",
            "",
            "global_grad_norm = torch.sqrt(global_grad_norm)",
            "-        max_grad_norm = self.defaults['max_grad_norm']",
            "+        # FIXME it'd be nice to remove explicit tensor conversion of scalars when torch.where promotes",
            "+        # scalar types properly https://github.com/pytorch/pytorch/issues/9190",
            "+        max_grad_norm = torch.tensor(self.defaults['max_grad_norm'], device=device)",
            "clip_global_grad_norm = torch.where(",
            "global_grad_norm > max_grad_norm,",
            "global_grad_norm / max_grad_norm,"
        ]
    },
    {
        "number": 858,
        "comments": "format",
        "commit_message": "Fixes to tests and line lengths\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class TokenClassificationIntegrationTest(test_combinations.TestCase):",
            "layers, input_shape=(None,))",
            "model.compile(",
            "loss='sparse_categorical_crossentropy',",
            "-        optimizer='adam'",
            "+        optimizer='adam',",
            "metrics=['acc'],",
            "run_eagerly=test_utils.should_run_eagerly())",
            "history = model.fit(dataset, epochs=10,"
        ]
    },
    {
        "number": 859,
        "comments": "no API",
        "commit_message": "fix zits\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class ZITS(InpaintModel):",
            "masks: [H, W]",
            "return: BGR IMAGE",
            "\"\"\"",
            "+        mask = mask[:, :, 0]",
            "items = load_image(image, mask, device=self.device)",
            "",
            "self.wireframe_edge_and_line(items, config.zits_wireframe)"
        ]
    },
    {
        "number": 860,
        "comments": "add API call for math fix",
        "commit_message": "[rllib] General RNN support (#2299)\n\n* wip\n\n* cls\n\n* re\n\n* wip\n\n* wip\n\n* a3c working\n\n* torch support\n\n* pg works\n\n* lint\n\n* rm v2\n\n* consumer id\n\n* clean up pg\n\n* clean up more\n\n* fix python 2.7\n\n* tf session management\n\n* docs\n\n* dqn wip\n\n* fix compile\n\n* dqn\n\n* apex runs\n\n* up\n\n* impotrs\n\n* ddpg\n\n* quotes\n\n* fix tests\n\n* fix last r\n\n* fix tests\n\n* lint\n\n* pass checkpoint restore\n\n* kwar\n\n* nits\n\n* policy graph\n\n* fix yapf\n\n* com\n\n* class\n\n* pyt\n\n* vectorization\n\n* update\n\n* test cpe\n\n* unit test\n\n* fix ddpg2\n\n* changes\n\n* wip\n\n* args\n\n* faster test\n\n* common\n\n* fix\n\n* add alg option\n\n* batch mode and policy serving\n\n* multi serving test\n\n* todo\n\n* wip\n\n* serving test\n\n* doc async env\n\n* num envs\n\n* comments\n\n* thread\n\n* remove init hook\n\n* update\n\n* fix ppo\n\n* comments1\n\n* fix\n\n* updates\n\n* add jenkins tests\n\n* fix\n\n* fix pytorch\n\n* fix\n\n* fixes\n\n* fix a3c policy\n\n* fix squeeze\n\n* fix trunc on apex\n\n* fix squeezing for real\n\n* update\n\n* remove horizon test for now\n\n* multiagent wip\n\n* update\n\n* fix race condition\n\n* fix ma\n\n* t\n\n* doc\n\n* st\n\n* wip\n\n* example\n\n* wip\n\n* working\n\n* cartpole\n\n* wip\n\n* batch wip\n\n* fix bug\n\n* make other_batches None default\n\n* working\n\n* debug\n\n* nit\n\n* warn\n\n* comments\n\n* fix ppo\n\n* fix obs filter\n\n* update\n\n* wip\n\n* tf\n\n* update\n\n* fix\n\n* cleanup\n\n* cleanup\n\n* spacing\n\n* model\n\n* fix\n\n* dqn\n\n* fix ddpg\n\n* doc\n\n* keep names\n\n* update\n\n* fix\n\n* com\n\n* docs\n\n* clarify model outputs\n\n* Update torch_policy_graph.py\n\n* fix obs filter\n\n* pass thru worker index\n\n* fix\n\n* rename\n\n* vlad torch comments\n\n* fix log action\n\n* debug name\n\n* fix lstm\n\n* remove unused ddpg net\n\n* remove conv net\n\n* revert lstm\n\n* wip\n\n* wip\n\n* cast\n\n* wip\n\n* works\n\n* fix a3c\n\n* works\n\n* lstm util test\n\n* doc\n\n* clean up\n\n* update\n\n* fix lstm check\n\n* move to end\n\n* fix sphinx\n\n* fix cmd\n\n* remove bad doc\n\n* clarify\n\n* copy\n\n* async sa\n\n* fix\n\n* comments\n\n* fix a3c conf\n\n* tune lstm\n\n* fix reshape\n\n* fix\n\n* back to 16\n\n* tuned a3c update\n\n* update\n\n* tuned\n\n* optional\n\n* fix catalog\n\n* remove prep\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class ModelCatalogTest(unittest.TestCase):",
            "def testCustomModel(self):",
            "ray.init()",
            "ModelCatalog.register_custom_model(\"foo\", CustomModel)",
            "-        p1 = ModelCatalog.get_model(1, 5, {\"custom_model\": \"foo\"})",
            "+        p1 = ModelCatalog.get_model(",
            "+            tf.constant([1, 2, 3]), 5, {\"custom_model\": \"foo\"})",
            "self.assertEqual(str(type(p1)), str(CustomModel))"
        ]
    },
    {
        "number": 861,
        "comments": "format",
        "commit_message": "fix recent lint\n\nSummary: lint clean again\n\nReviewed By: patricklabatut\n\nDifferential Revision: D20868775\n\nfbshipit-source-id: ade4301c1012c5c6943186432465215701d635a9\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def corresponding_points_alignment(",
            "U, S, V = torch.svd(XYcov)",
            "",
            "# identity matrix used for fixing reflections",
            "-    E = torch.eye(dim, dtype=XYcov.dtype, device=XYcov.device)[None].repeat(",
            "-        b, 1, 1",
            "-    )",
            "+    E = torch.eye(dim, dtype=XYcov.dtype, device=XYcov.device)[None].repeat(b, 1, 1)",
            "",
            "if not allow_reflection:",
            "# reflection test:"
        ]
    },
    {
        "number": 862,
        "comments": "change condition check for resource fix",
        "commit_message": "fix a bug in DCGAN (#121)\n\nUsing single GPU in DCGAN caused an error because  'gpu_ids'  in forward function will be None .\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class _netD(nn.Module):",
            "",
            "def forward(self, input):",
            "gpu_ids = None",
            "-        if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > 1:",
            "+        if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > =1:",
            "gpu_ids = range(self.ngpu)",
            "output = nn.parallel.data_parallel(self.main, input, gpu_ids)",
            "return output.view(-1, 1)"
        ]
    },
    {
        "number": 864,
        "comments": "add condition check for type fix",
        "commit_message": "Add Dorckerfile and implement TFLite and Torchscript as model-backends (#64)\n\n* add tf and torch backeneds\n\n* add torch backend\n\n* add tqdm to requirements\n\n* avoid installation of compilers when NO_COMPILER_INSTALLATION is set\n\n* fix error with onnx installation\n\n* fix error with onnx installation\n\n* fix error with onnx installation\n\n* fix error with onnx installation\n\n* Added Dockerfile & bugfix (#63)\n\n* add dockerfile\n\n* fix tvm configs issue in the tvm installer\n\n* fix tvm issue\n\n* fix tvm\n\n* fix dockerfile & created build script for the docker images\n\n* removed redundant spaces\n\nCo-authored-by: Valerio Sofi <v.sofi@nebuly.ai>\n\n* add tflite to tf api\n\n* fix logging\n\n* fix error with half precision in torch\n\n* fix minor bugs\n\n* fix bugs\n\n* fix import\n\n* fix bug with tf\n\n* fix error with DeviceArrays in polygraphy\n\n* fix another bug\n\n* upgrade version\n\nCo-authored-by: morgoth95 <d.fiori@nebuly.ai>\nCo-authored-by: Valerio Sofi <v.sofi@nebuly.ai>\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "from nebullvm.transformations.base import BaseTransformation",
            "",
            "",
            "class VerifyContiguity(BaseTransformation):",
            "-    def _transform(self, _input: torch.Tensor, **kwargs) -> Any:",
            "+    def _transform(self, _input: Any, **kwargs) -> Any:",
            "+        if not isinstance(_input, torch.Tensor):",
            "+            return _input",
            "if not _input.is_contiguous():",
            "_input = _input.contiguous()",
            "return _input"
        ]
    },
    {
        "number": 865,
        "comments": "add param for resource fix",
        "commit_message": "distributed strategy bug fix\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "from tests import utils",
            "def test_image_classifier(tmp_path):",
            "train_x = utils.generate_data(num_instances=320, shape=(32, 32, 3))",
            "train_y = utils.generate_one_hot_labels(num_instances=320, num_classes=10)",
            "-    clf = ak.ImageClassifier(directory=tmp_path, max_trials=2, seed=utils.SEED)",
            "+    clf = ak.ImageClassifier(",
            "+        directory=tmp_path,",
            "+        max_trials=2,",
            "+        seed=utils.SEED,",
            "+        distribution_strategy=tf.distribute.MirroredStrategy(),",
            "+    )",
            "clf.fit(train_x, train_y, epochs=1, validation_split=0.2)",
            "keras_model = clf.export_model()",
            "clf.evaluate(train_x, train_y)"
        ]
    },
    {
        "number": 867,
        "comments": "remove API call for type fix",
        "commit_message": "Fix conv2d_transpose\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),",
            "output_shape[3],",
            "output_shape[1])",
            "if output_shape[0] is None:",
            "-        output_shape = (tf.shape(x)[0],) + tuple(output_shape[1:])",
            "-        output_shape = tf.stack(list(output_shape))",
            "+        output_shape = (shape(x)[0],) + tuple(output_shape[1:])",
            "+",
            "+    output_shape = tf.stack(list(output_shape))",
            "",
            "padding = _preprocess_padding(padding)",
            "if tf_data_format == 'NHWC':"
        ]
    },
    {
        "number": 868,
        "comments": "add API call for math fix",
        "commit_message": "lint fixes\n\nSummary:\nRan the linter.\nTODO: need to update the linter as per D21353065.\n\nReviewed By: bottler\n\nDifferential Revision: D21362270\n\nfbshipit-source-id: ad0e781de0a29f565ad25c43bc94a19b1828c020\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class TestRasterizeMeshes(TestCaseMixin, unittest.TestCase):",
            "], dtype=torch.int64, device=device)",
            "# fmt: on",
            "",
            "-        pix_to_face_padded = -torch.ones_like(pix_to_face_frontface)",
            "+        pix_to_face_padded = -(torch.ones_like(pix_to_face_frontface))",
            "# Run with and without culling",
            "# Without culling, for k=0, the front face (i.e. face 2) is",
            "# rasterized and for k=1, the back face (i.e. face 3) is"
        ]
    },
    {
        "number": 870,
        "comments": "customized API",
        "commit_message": "Fit `RGATConv` different device error (#5187)\n\n* fix rgatconv device\n\n* changelog\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class RGATConv(MessagePassing):",
            "alpha = torch.where(alpha > 0, alpha + 1, alpha)",
            "",
            "elif self.mod == \"f-scaled\":",
            "-            ones = torch.ones(index.size())",
            "+            ones = alpha.new_ones(index.size())",
            "degree = scatter_add(ones, index,",
            "dim_size=size_i)[index].unsqueeze(-1)",
            "alpha = alpha * degree"
        ]
    },
    {
        "number": 871,
        "comments": "add API call for state fix",
        "commit_message": "add compat.is_gpu_available(); fix test failure\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class PipelineTest(test.SparkTest):",
            "import tensorflow as tf",
            "from tensorflowonspark import TFNode",
            "",
            "+      tf.compat.v1.disable_eager_execution()",
            "tf.compat.v1.reset_default_graph()",
            "strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()"
        ]
    },
    {
        "number": 875,
        "comments": "change condition check for version fix",
        "commit_message": "Fix parsing Keras version identifiers like `2.2.4-tf` (#3794)\n\nSigned-off-by: Max H. Gerlach <git@maxgerlach.de>\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class SparkKerasTests(tf.test.TestCase):",
            "",
            "def test_fit_model_multiclass(self):",
            "model = create_mnist_model()",
            "-        if version.parse(tf.keras.__version__) < version.parse(\"2.11\"):",
            "+        if version.parse(tf.keras.__version__.replace(\"-tf\", \"+tf\")) < version.parse(\"2.11\"):",
            "optimizer = tf.keras.optimizers.Adadelta(1.0)",
            "else:",
            "optimizer = tf.keras.optimizers.legacy.Adadelta(1.0)"
        ]
    },
    {
        "number": 877,
        "comments": "doc update",
        "commit_message": "Fix docs links to PyTorch documentation (#856)\n\n\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def torch_sum(x):",
            "",
            "def torch_backward(x):",
            "\"\"\"",
            "-    Like ``x.backward()`` for a ``torch.autograd.Variable``, but also accepts",
            "+    Like ``x.backward()`` for a :class:`~torch.autograd.Variable`, but also accepts",
            "numbers (a no-op if given a number).",
            "\"\"\"",
            "if isinstance(x, torch.autograd.Variable):"
        ]
    },
    {
        "number": 878,
        "comments": "change param for resource fix",
        "commit_message": "fix amp+multi gpu in asr\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def train(args):",
            "dtype = torch.float32",
            "model = model_class(args.n_vocab, args).to(dtype=dtype)",
            "if args.ngpu > 0:",
            "-        model.to(\"cuda:0\")",
            "+        model.to(\"cuda\")",
            "gpu_id = list(range(args.ngpu))",
            "else:",
            "gpu_id = [-1]"
        ]
    },
    {
        "number": 880,
        "comments": "change param for shape fix",
        "commit_message": "fix create_meshgrid indexing and refactor tensor_to_image\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def create_meshgrid(height, width, normalized_coordinates=True):",
            "else:",
            "xs = torch.linspace(0, width - 1, width)",
            "ys = torch.linspace(0, height - 1, height)",
            "-    return torch.stack(torch.meshgrid([ys, xs])).view(1, 2, -1)",
            "+    return torch.stack(torch.meshgrid([ys, xs])).view(1, 2, -1)[:, (1, 0), :]",
            "",
            "",
            "class HomographyWarper(nn.Module):"
        ]
    },
    {
        "number": 881,
        "comments": "change control flow",
        "commit_message": "Fix tensorflow bugs\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "try:",
            "\"GPU\"",
            ")",
            "if len(physical_devices) > 0:",
            "-        tensorflow.config.experimental.set_memory_growth(",
            "-            physical_devices[0], True",
            "-        )",
            "+        for physical_device in physical_devices:",
            "+            tensorflow.config.experimental.set_memory_growth(",
            "+                physical_device, True",
            "+            )",
            "",
            "tensorflow.get_logger().setLevel(\"ERROR\")",
            "tensorflow.autograph.set_verbosity(0)"
        ]
    },
    {
        "number": 882,
        "comments": "add condition check for resource fix",
        "commit_message": "fix device\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def main():",
            "# download url: https://download.pytorch.org/models/resnet34-333f7ec4.pth",
            "model_weight_path = \"./resnet34-pre.pth\"",
            "assert os.path.exists(model_weight_path), \"file {} does not exist.\".format(model_weight_path)",
            "-    net.load_state_dict(torch.load(model_weight_path, map_location=device))",
            "+    net.load_state_dict(torch.load(model_weight_path, map_location='cpu'))",
            "# for param in net.parameters():",
            "#     param.requires_grad = False"
        ]
    },
    {
        "number": 883,
        "comments": "update API call for refactor fix",
        "commit_message": "Update to PyTorch 1.9.0 (#2887)\n\n* Update to PyTorch 1.9.0\n\n* Update torch.cholesky, torch.symeig\n\n* Fix torch.tensordot, torch.qr, funsor dependency\n\n* Fix torch import, AutoGuideList usage\n\n* Ignore bug in PyTorch jit\n\n* Ignore tracer warnings\n\n* Replace torch.solve with torch.linalg.solve\n\n* Xfail vectorized markov funsor tests\n\n* Update funsor version\n\n* Switch MNIST mirrors\n\n* Pin pillow version\n\n* Bump torchvision version\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def triangular_solve(x, y, upper=False, transpose=False):",
            "",
            "",
            "def precision_to_scale_tril(P):",
            "-    Lf = torch.cholesky(torch.flip(P, (-2, -1)))",
            "+    Lf = torch.linalg.cholesky(torch.flip(P, (-2, -1)))",
            "L_inv = torch.transpose(torch.flip(Lf, (-2, -1)), -2, -1)",
            "L = torch.triangular_solve(torch.eye(P.shape[-1], dtype=P.dtype, device=P.device),",
            "L_inv, upper=False)[0]"
        ]
    },
    {
        "number": 884,
        "comments": "remove API call for type fixfor distributed bug",
        "commit_message": "Summaries completely changed, distributed mode incompletely changed, various fixes\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class NaturalGradient(Optimizer):",
            "applied = self.apply_step(variables=variables, diffs=estimated_diffs)",
            "",
            "with tf.control_dependencies(control_inputs=(applied,)):",
            "-                return [tf.identity(input=estimated_diff) for estimated_diff in estimated_diffs]",
            "+                return [estimated_diff + 0.0 for estimated_diff in estimated_diffs]",
            "",
            "def false_fn():",
            "return [tf.zeros_like(tensor=diff) for diff in diffs]"
        ]
    },
    {
        "number": 885,
        "comments": "refactor fix error",
        "commit_message": "\ud83d\udc1b  fix path, entrypoint and logging\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def configure_logger(verbose: bool) -> None:",
            "verbose (bool):",
            "`True` to use verbose logger, `False` otherwise.",
            "\"\"\"",
            "-    tf_logger = tf.get_logger()",
            "+    from tensorflow import get_logger",
            "+    from tensorflow.compat.v1 import logging as tf_logging",
            "+    tf_logger = get_logger()",
            "tf_logger.handlers = [handler]",
            "if verbose:",
            "tf_logging.set_verbosity(tf_logging.INFO)"
        ]
    },
    {
        "number": 886,
        "comments": "add API call for type fix",
        "commit_message": "Fix argsort test for tensorflow (#2389)\n\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def argsort(",
            "ret = tf.argsort(",
            "tf.convert_to_tensor(x), axis=axis, direction=\"ASCENDING\", stable=stable",
            ")",
            "-    return ret",
            "+    return tf.cast(ret, dtype=tf.int64)",
            "",
            "",
            "def sort("
        ]
    },
    {
        "number": 887,
        "comments": "update API call for version fix",
        "commit_message": "remove redundant recursive calls in _to_native and fix _to_ivy_array\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def _to_ivy(x: Any) -> Any:",
            "",
            "",
            "def _to_ivy_array(x: Any) -> ivy.Array:",
            "-    if isinstance(x, (torch.Tensor, tf.Tensor, jnp.numpy.DeviceArray, numpy.ndarray)):",
            "+    if isinstance(x, (torch.Tensor, tf.Tensor, jnp.DeviceArray, numpy.ndarray)):",
            "return ivy.array(numpy.array(x))",
            "return x"
        ]
    },
    {
        "number": 888,
        "comments": "add condition check for type fix",
        "commit_message": "Another fix to vecdot (#4639)\n\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def vecdot(",
            "out: Optional[Union[tf.Tensor, tf.Variable]] = None,",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "dtype = ivy.as_native_dtype(ivy.promote_types(x1.dtype, x2.dtype))",
            "-    x1, x2 = tf.cast(x1, tf.float32), tf.cast(x2, tf.float32)",
            "+    if dtype != \"float64\":",
            "+        x1, x2 = tf.cast(x1, tf.float32), tf.cast(x2, tf.float32)",
            "ret = tf.cast(tf.tensordot(x1, x2, axes=(axis, axis)), dtype)",
            "return ret"
        ]
    },
    {
        "number": 890,
        "comments": "customize API",
        "commit_message": "Add save and load tests to fairseq export test (#1653)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/fairseq/pull/1653\n\nEarlier we had some issues at pickling. Type information gets lost. Fixed in https://github.com/pytorch/pytorch/pull/32569.\n\nThese save_and_load tests are added for protection in the future.\n\nReviewed By: myleott\n\nDifferential Revision: D19435988\n\nfbshipit-source-id: 560ea65ed3493bebcf394327818364b3fcd6fc92\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "class TestExportModels(unittest.TestCase):",
            "TransformerModel.add_args(parser)",
            "args = parser.parse_args([])",
            "model = TransformerModel.build_model(args, task)",
            "-        torch.jit.script(model)",
            "+        scripted = torch.jit.script(model)",
            "+        self._test_save_and_load(scripted)",
            "",
            "",
            "if __name__ == \"__main__\":"
        ]
    },
    {
        "number": 892,
        "comments": "print update",
        "commit_message": "Test documentation (#511)\n\n* Activation Cleaning Docstring Test\n\n* Requirements Pinned with range to insure tested versions are used. Range are used to prevent updating requirements all the time.\n\n* setup.cfg file added with PEP8 configuration\n\n* activation.py refactored\n\n* docstring fixed - ready for documentation unittest\n\n* Yapf correction for max_line_length: 120\n\n* test yapf refactored\n\n* test documentation added\n\n* Missing requirement added: sphinx\n\n* Allow test on documentation to pass on warning\n\n* Fix travis dependencies install\n\n* Travis install script fixed\n\n* Travis install command fixed\n\n* Requirements conflict solved\n\n* Yapf Style modified and merged in file \"setup.cfg\"\n\n* Yapf Confiuguration Updated\n\n* Code Refactored with new YAPF formating style\n\n* Code Refactored with new YAPF formating style\n\n* Code Refactored with new YAPF formating style\n\n* shorten codes\n\n* Various Cleaning\n\n* Trailing Slashes removed\n\n* Test Recurrent Fixed\n\n* Line Width Fix\n\n* docs requirements updated\n\n* fix example docs style\n\n* Codacy Issue Fixed\n\n* Merge Errors fixed\n\n* YAPF Style Applied\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "with tf.device('/cpu:0'):",
            "n_batch += 1",
            "",
            "if epoch + 1 == 1 or (epoch + 1) % print_freq == 0:",
            "-            print(\"Epoch %d : Step %d-%d of %d took %fs\" % \\",
            "-                (epoch, step, step + n_step_epoch, n_step, time.time() - start_time))",
            "+            print(",
            "+                \"Epoch %d : Step %d-%d of %d took %fs\" %",
            "+                (epoch, step, step + n_step_epoch, n_step, time.time() - start_time)",
            "+            )",
            "print(\"   train loss: %f\" % (train_loss / n_batch))",
            "print(\"   train acc: %f\" % (train_acc / n_batch))"
        ]
    },
    {
        "number": 895,
        "comments": "add API call for type fix",
        "commit_message": "Fix buffers in sinusoidal positional embeddings\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class SinusoidalPositionalEmbedding(nn.Module):",
            "self.embedding_dim,",
            "self.padding_idx,",
            ").type_as(self.weights)",
            "+        self.weights = self.weights.type_as(self._float_tensor)",
            "weights = Variable(self.weights)",
            "",
            "if incremental_state is not None:"
        ]
    },
    {
        "number": 897,
        "comments": "custom API",
        "commit_message": "fixed lstm, added gru, other fixes\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def shape(x, unknown=-1):",
            "",
            "",
            "def no_operation():",
            "-    return tf.constant(value=False, dtype=tf_dtype(dtype='bool'))",
            "+    return identity_operation(x=tf.constant(value=False, dtype=tf_dtype(dtype='bool')))",
            "",
            "",
            "def identity_operation(x, operation_name=None):"
        ]
    },
    {
        "number": 900,
        "comments": "customize API",
        "commit_message": "Allow saving / loading checkpoints from cloud paths (#683)\n\n* Allow saving / loading checkpoints from cloud paths\n\nAllows saving and loading checkpoints directly from cloud paths like\nAmazon S3 (s3://) and Google Cloud Storage (gs://) by using fsspec.\n\nNote: The user will have to install the relevant dependency for each\nprotocol. Otherwise fsspec will fail and specify which dependency is\nmissing.\n\n* Append suffix _fsspec to save/load function names\n\n* Add a lower bound to the fsspec dependency\n\nSkips the 0 major version.\n\n* Add missing changes from refactor\n\n* Use fsspec for remaining artifacts\n\n* Add test case with path requiring fsspec\n\n* Avoid writing logs to file unless output_path is local\n\n* Document the possibility of using paths supported by fsspec\n\n* Fix style and lint\n\n* Add missing lint fixes\n\n* Add type annotations to new functions\n\n* Use Coqpit method for converting config to dict\n\n* Fix type annotation in semi-new function\n\n* Add return type for load_fsspec\n\n* Fix bug where fs not always created\n\n* Restore the experiment removal functionality\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class HifiganGenerator(torch.nn.Module):",
            "def load_checkpoint(",
            "self, config, checkpoint_path, eval=False",
            "):  # pylint: disable=unused-argument, redefined-builtin",
            "-        state = torch.load(checkpoint_path, map_location=torch.device(\"cpu\"))",
            "+        state = load_fsspec(checkpoint_path, map_location=torch.device(\"cpu\"))",
            "self.load_state_dict(state[\"model\"])",
            "if eval:",
            "self.eval()"
        ]
    },
    {
        "number": 901,
        "comments": "add API call for type fix",
        "commit_message": "fix generate_share range and reduce field to 62\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class AdditiveSharingTensor(AbstractTensor):",
            "random_shares = [random_type(secret.shape) for _ in range(n_workers - 1)]",
            "",
            "for share in random_shares:",
            "-            share.random_(-field, field)",
            "+            share.random_(int(-field/2), int(field/2)-1)",
            "",
            "shares = []",
            "for i in range(n_workers):"
        ]
    },
    {
        "number": 902,
        "comments": "update API call for version fix",
        "commit_message": "Abstract accelerator (step 3) (#2677)\n\n* Integrate accelerator abstraction interface into deepspeed/\n\n* Fix error message in fp16/fused_optimizer\n\n* fix error message in fp16/unfused_optimizer.py\n\n* assign get_accelerator().pin_memory() result to input Tensor name\n\n* no need to check cuda and whether nvtx supported\n\n* move try-except into inner most block\n\n* call Event() and Stream() in get_accelerator() for data type\n\n* Make Stream and Event as properties of abstract interface so they can be used as data type in deepspeed\n\n* Apply op_builder backend api change from #2705 from @jeffra\n\n* fix tests where Builder NAME is used\n\n* keep original ...Builder.NAME interface instead of ...Builder().NAME interface\n\n* fix builder closure for installation\n\n* fix randomltd builder\n\n* add comments to clarify create_op_builder and get_op_builder\n\n* fix compatibility with pip install -e\n\nCo-authored-by: Cheng Li <pistasable@gmail.com>\nCo-authored-by: Olatunji Ruwase <olruwase@microsoft.com>\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class PipelineEngine(DeepSpeedEngine):",
            "mem_cached = new_cached",
            "mem_alloced = new_alloced",
            "",
            "-        max_alloced = torch.cuda.max_memory_allocated()",
            "-        max_cached = torch.cuda.max_memory_cached()",
            "+        max_alloced = get_accelerator().max_memory_allocated()",
            "+        max_cached = get_accelerator().max_memory_cached()",
            "",
            "# convert to GB for printing",
            "new_alloced /= 1024**3"
        ]
    },
    {
        "number": 903,
        "comments": "add param for resource fix",
        "commit_message": "Fix gpu placement bug in enum_discrete (#672)\n\n\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def torch_multinomial(input, num_samples, replacement=False):",
            "Does not support keyword argument `out`.",
            "\"\"\"",
            "if input.is_cuda:",
            "-        return torch_multinomial(input.cpu(), num_samples, replacement).cuda()",
            "+        return torch.multinomial(input.cpu(), num_samples, replacement).cuda(input.get_device())",
            "else:",
            "return torch.multinomial(input, num_samples, replacement)"
        ]
    },
    {
        "number": 904,
        "comments": "add param for argument fix",
        "commit_message": "add redis test marker to conftest.py\nadd redis database tests to tox and .github flows\nuse print instead of std.write\nfix datasets tests to work with the new skip_checks flag\nadd skip_checks flag on dataset purge\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def test_delete_entire_dataset(domain_owner, cleanup_storage):",
            "assert domain_owner.datasets[0].name == \"Dataset_1\"",
            "assert domain_owner.datasets[1].name == \"Dataset_2\"",
            "",
            "-    domain_owner.datasets.delete(dataset_id=domain_owner.datasets[0].id)",
            "+    domain_owner.datasets.delete(",
            "+        dataset_id=domain_owner.datasets[0].id, skip_checks=True",
            "+    )",
            "",
            "# Check if the number of available datasets has been decreased",
            "assert len(domain_owner.datasets) == 1"
        ]
    },
    {
        "number": 905,
        "comments": "update API call for refactor fix",
        "commit_message": "modified logspace to also include the dtype argument, fixing the failing unit test.\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def logspace(",
            "base=10.0,",
            "axis=None,",
            "*,",
            "+    dtype: torch.dtype,",
            "device: torch.device,",
            "out: Optional[torch.Tensor] = None,",
            "):",
            "-    power_seq = linspace(",
            "-        start, stop, num, axis, dtype=None, device=default_device(device)",
            "+    power_seq = ivy.linspace(",
            "+        start, stop, num, axis, dtype=dtype, device=ivy.default_device(device)",
            ")",
            "return base**power_seq"
        ]
    },
    {
        "number": 906,
        "comments": "value change",
        "commit_message": "fix crypto lr test (#2691)\n\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def test_crypto_lr(fit_intercept, hook, workers):",
            "",
            "K = 2  # number of features",
            "",
            "-    beta = torch.Tensor([1.0, 10.0]).view(-1, 1)  # \"real\" coefficients",
            "-    intercept = 3.0 if fit_intercept else 0  # \"real\" intercept",
            "+    beta = torch.Tensor([1.0, 2.0]).view(-1, 1)  # \"real\" coefficients",
            "+    intercept = 0.5 if fit_intercept else 0  # \"real\" intercept",
            "",
            "# Alice's data",
            "torch.manual_seed(0)  # Truncation might not always work so we set the random seed"
        ]
    },
    {
        "number": 908,
        "comments": "add API call for type fix",
        "commit_message": "Add pytorch=1.10.0 to CI configuration (#3749)\n\n* Add pytorch=1.10.0 to CI configuration\n\n* fix:   test/espnet2/bin/test_k2_asr_inference.py\n\n* fix:   espnet2/main_funcs/pack_funcs.py\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def pack(",
            "try:",
            "import torch",
            "",
            "-        meta_objs.update(torch=torch.__version__)",
            "+        meta_objs.update(torch=str(torch.__version__))",
            "except ImportError:",
            "pass",
            "try:"
        ]
    },
    {
        "number": 909,
        "comments": "update API call for refactor fix",
        "commit_message": "Should be fixed finally\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def main():",
            "# recog",
            "logging.info('backend = ' + args.backend)",
            "if args.backend == \"chainer\":",
            "-        from espnet.lmchainer.asr_chainer import recog",
            "+        from espnet.asr.chainer.asr_chainer import recog",
            "recog(args)",
            "elif args.backend == \"pytorch\":",
            "-        from espnet.lmpytorch.asr_pytorch import recog",
            "+        from espnet.asr.pytorch.asr_pytorch import recog",
            "recog(args)",
            "else:",
            "raise ValueError(\"Only chainer and pytorch are supported.\")"
        ]
    },
    {
        "number": 910,
        "comments": "change API call for version fix",
        "commit_message": "[RLlib] DDPG refactor and Exploration API action noise classes. (#7314)\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* Fix\n\n* WIP.\n\n* Add TD3 quick Pendulum regresison.\n\n* Cleanup.\n\n* Fix.\n\n* LINT.\n\n* Fix.\n\n* Sort quick_learning test cases, add TD3.\n\n* Sort quick_learning test cases, add TD3.\n\n* Revert test_checkpoint_restore.py (debugging) changes.\n\n* Fix old soft_q settings in documentation and test configs.\n\n* More doc fixes.\n\n* Fix test case.\n\n* Fix test case.\n\n* Lower test load.\n\n* WIP.\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class Schedule(metaclass=ABCMeta):",
            "raise NotImplementedError",
            "",
            "def value(self, t):",
            "-        if self.framework == \"tf\" and tf.executing_eagerly() is False:",
            "+        if self.framework == \"tf\":",
            "return tf.cast(",
            "-                tf.py_func(self._value, [t], tf.float64),",
            "+                tf.py_function(self._value, [t], tf.float64),",
            "tf.float32,",
            "-                name=\"schedule-value\")",
            "+                name=\"schedule_value\")",
            "return self._value(t)",
            "",
            "def __call__(self, t):"
        ]
    },
    {
        "number": 911,
        "comments": "conditional warning",
        "commit_message": "misc small changes and fix #688\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def Dropout(x, *args, **kwargs):",
            "if 'is_training' in kwargs:",
            "kwargs['training'] = kwargs.pop('is_training')",
            "if len(args) > 0:",
            "-        logger.warn(",
            "-            \"The first positional argument to tensorpack.Dropout is the probability to keep rather than to drop. \"",
            "-            \"This is different from the rate argument in tf.layers.Dropout due to historical reasons. \"",
            "-            \"To mimic tf.layers.Dropout, explicitly use keyword argument 'rate' instead\")",
            "+        if args[0] != 0.5:",
            "+            logger.warn(",
            "+                \"The first positional argument to tensorpack.Dropout is the probability to keep, rather than to drop. \"",
            "+                \"This is different from the rate argument in tf.layers.Dropout due to historical reasons. \"",
            "+                \"To mimic tf.layers.Dropout, explicitly use keyword argument 'rate' instead\")",
            "rate = 1 - args[0]",
            "elif 'keep_prob' in kwargs:",
            "assert 'rate' not in kwargs, \"Cannot set both keep_prob and rate!\""
        ]
    },
    {
        "number": 912,
        "comments": "update API call for version fix",
        "commit_message": "fixed a bug of overwriting the variable\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class FeedForwardTransformer(TTSInterface, torch.nn.Module):",
            "",
            "# concat speaker embedding",
            "if self.spk_embed_dim is not None:",
            "-            spembs = torch.nn.functional.normalize(spembs).unsqueeze(1).expand(-1, hs.size(1), -1)",
            "-            hs = self.projection(torch.cat([hs, spembs], dim=-1))",
            "+            spembs_ = F.normalize(spembs).unsqueeze(1).expand(-1, hs.size(1), -1)",
            "+            hs = self.projection(torch.cat([hs, spembs_], dim=-1))",
            "",
            "# forward duration predictor and length regulator",
            "d_masks = make_pad_mask(ilens).to(xs.device)"
        ]
    },
    {
        "number": 916,
        "comments": "value change",
        "commit_message": " Upgrade to modern Python syntax (#1213)\n\n* Upgrade to modern Python syntax\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Use f-strings\n\n* Placate DeepSource Python\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def symmetrical_epipolar_distance(",
            "",
            "\"\"\"",
            "if not isinstance(Fm, torch.Tensor):",
            "-        raise TypeError(\"Fm type is not a torch.Tensor. Got {}\".format(type(Fm)))",
            "+        raise TypeError(f\"Fm type is not a torch.Tensor. Got {type(Fm)}\")",
            "",
            "if (len(Fm.shape) != 3) or not Fm.shape[-2:] == (3, 3):",
            "-        raise ValueError(\"Fm must be a (*, 3, 3) tensor. Got {}\".format(Fm.shape))",
            "+        raise ValueError(f\"Fm must be a (*, 3, 3) tensor. Got {Fm.shape}\")",
            "",
            "if pts1.size(-1) == 2:",
            "pts1 = kornia.convert_points_to_homogeneous(pts1)"
        ]
    },
    {
        "number": 917,
        "comments": "version fix",
        "commit_message": "improved global tensor handling, various other fixes\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class Replay(Queue):",
            "",
            "zero = tf.constant(value=0, dtype=util.tf_dtype(dtype='long'))",
            "indices = tf.zeros(shape=(0,), dtype=util.tf_dtype(dtype='long'))",
            "-        indices, _ = tf.while_loop(",
            "+        indices, _ = self.while_loop(",
            "cond=cond, body=reduce_range_concat, loop_vars=(indices, zero),",
            "shape_invariants=(tf.TensorShape(dims=(None,)), zero.get_shape()), back_prop=False",
            ")"
        ]
    },
    {
        "number": 918,
        "comments": "no API",
        "commit_message": "Superset Behaviour - linalg submodule (#4768)\n\n* added superset behaviour to cross, eigh, eighvalsh\n\n* fixed tests to support superset behaviour\n",
        "label": "no",
        "answer": "no",
        "change": [
            "eigh.unsupported_dtypes = (",
            "eigh.support_native_out = True",
            "",
            "",
            "-def eigvalsh(x: torch.Tensor, /, *, out: Optional[torch.Tensor] = None) -> torch.Tensor:",
            "-    return torch.linalg.eigvalsh(x, out=out)",
            "+def eigvalsh(",
            "+    x: torch.Tensor, /, *, UPLO: Optional[str] = \"L\", out: Optional[torch.Tensor] = None",
            "+) -> torch.Tensor:",
            "+    return torch.linalg.eigvalsh(x, UPLO=UPLO, out=out)",
            "",
            "",
            "eigvalsh.unsupported_dtypes = ("
        ]
    },
    {
        "number": 919,
        "comments": "update param for version fix",
        "commit_message": "Fix unused variable found (#1250)\n\n* Fix Unused variable found\n\n* more fixes\n\n* Update kornia/enhance/histogram.py\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def image_histogram2d(",
            "hist = hist.squeeze()",
            "elif image.dim() == 3:",
            "hist = hist.squeeze(0)",
            "-    return hist, torch.zeros_like(hist, dtype=hist.dtype, device=device)",
            "+    return hist, torch.zeros_like(hist, dtype=hist.dtype, device=hist.device)"
        ]
    },
    {
        "number": 920,
        "comments": "remove API call for type fix",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class FeedForwardEncoder(Seq2SeqEncoder):",
            "return self._feedforward(inputs)",
            "else:",
            "outputs = self._feedforward(inputs)",
            "-            return outputs * mask.unsqueeze(dim=-1).float()",
            "+            return outputs * mask.unsqueeze(dim=-1)"
        ]
    },
    {
        "number": 921,
        "comments": "change API call for type fix",
        "commit_message": "Small refactor on `filters` module: Dropping JIT support (#2187)\n\n* add suport to Tensor for sigmas\n\n- Removed the functions `gaussian_blur2d_t`, `get_gaussian_kernel1d_t`, `get_gaussian_kernel2d_t`, and\n`get_gaussian_kernel3d_t` in favor of support sigmas as `Tensors`, floats, or tuple of floats on the functions without\nthe suffix `_t`\n\n* Remove crashing JIT tests related to filters\n\n* fix typing\n\n* add support to kernel size as tuple or integer\n\n* update laplacian test\n\n* update DTYPES atol and rtol\n\n- added bfloat16\n- use same values for BaseTester class and the function\n\n* add device and dtype for default gaussian\n\n* add device and dtype for discrete gaussian by erf\n\n* add device, dtype, batched for discrete gaussian\n\n- add support to compute batched kernerl for discrete gaussian by bessel functions\n- add support to pass device and dtype when sigma is a float\n\n* add device and dtype for laplacian kernels\n\n* add device and dtype for binary and box kernels\n\n* add new typping annotations\n\n- by adding `from __future__ import annotations` we can use the pyupgrade\nto update the typing annotations to be like on python 3.10\n\n* add device and dtype for static kernels (sobel)\n\n* add dtype and device for canny\n\n- fix gaussian_blur2d sigma shape\n\n* add KORNIA_CHECK API to filters\n\n* fix typing and docs\n\n* fix laplacian module kernel_size typing\n\n* remove unsqueeze in favor of pythonic slicing\n\n* update kernel size canny module\n\n* add depreciation for `*_t` functions\n\n* update blur tests\n\n* add noncontiguous tests to blur\n\n* update canny tests\n\n- add BaseTester\n- add Dynamo test\n\n* update gaussian tests\n\n- Add basetester\n- add dynamo tests\n\n* update hanning tests\n\n* update laplacian tests\n\n- Add BaseTester\n- Add dynamo test\n\n* update median tests\n\n- add BaseTester\n- add dynamo test\n\n* remove border parameter\n\n- this should be tested just on `filter2d`, `filter3d`, etc\n\n* update motion tests\n\n- Add BaseTester\n- Add dynamo\n- Add tests for 3D\n\n* update sobel tests\n\n- Add basetester\n- add dynamo tests\n\n* update unsharp mask test\n\n- add basetester\n- add dynamo test\n\n* update filter 2d and 3d tests\n\n- add basetester\n- add dynamo\n\n* fix typing\n\n* remove Pyr down and up jit tests\n\n* update atol for fp64 based on old _DTYPE_PRECISIONS values\n\n* add TODO note about dtype precision for fp64\n\n* skip filter3d with reflect border for < 1.9 torch\n\n* small fix\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def elastic_transform2d(",
            "sigma_t = sigma.to(device=device, dtype=dtype)",
            "",
            "# Get Gaussian kernel for 'y' and 'x' displacement",
            "-    kernel_x: torch.Tensor = get_gaussian_kernel2d_t(kernel_size, sigma_t[0].expand(2).unsqueeze(0))",
            "-    kernel_y: torch.Tensor = get_gaussian_kernel2d_t(kernel_size, sigma_t[1].expand(2).unsqueeze(0))",
            "+    kernel_x: torch.Tensor = get_gaussian_kernel2d(kernel_size, sigma_t[0].expand(2).unsqueeze(0))",
            "+    kernel_y: torch.Tensor = get_gaussian_kernel2d(kernel_size, sigma_t[1].expand(2).unsqueeze(0))",
            "",
            "# Convolve over a random displacement matrix and scale them with 'alpha'",
            "disp_x: torch.Tensor = noise[:, :1]"
        ]
    },
    {
        "number": 923,
        "comments": "format",
        "commit_message": "Fix typos\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class TpuStrategyTest(tf.test.TestCase):",
            "self.assertIn(prediction1, (\"yes\", \"no\"))",
            "",
            "prediction2 = loaded_serving_fn(",
            "-        tf.constant([\"ironman\", \"ironman\", \"unkonwn\"]))[\"output_0\"]",
            "+        tf.constant([\"ironman\", \"ironman\", \"unknown\"]))[\"output_0\"]",
            "self.assertIn(prediction2, (\"yes\", \"no\"))"
        ]
    },
    {
        "number": 925,
        "comments": "customized API",
        "commit_message": "Barber-Agakov, and OED refactoring (#1361)\n\n* Merge changes from oed-master excluding sequential sigmoid example\n\n* Some flake8 and Python 2 changes\n\n* Indentation and noqa for lambdas\n\n* Blank __init__ file\n\n* Better docstrings, fix some tests\n\n* Revert Makefile\n\n* Some cosmetic tune-ups\n\n* Correct ewma formula in doc\n\n* A better docstring\n\n* Add from __future__ import... to all files\n\n* Better treatment of intial values\n\n* Rename variables\n\n* Rename shuffled properly\n\n* Better docstring\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "def main(N, M):",
            "item_thetas = torch.tensor([[0., 0.], [0., .5], [0., 1.]])",
            "design_tensor = build_design_tensor(item_thetas, individual_assignment)",
            "print(\"Design tensor\", design_tensor)",
            "-    y = naive_rainforth(model, design_tensor, target_labels=[\"w_global\", \"w_local\"], N=N, M=M)",
            "+    y = naive_rainforth_eig(model, design_tensor, observation_labels=\"y\",",
            "+                            target_labels=[\"w\", \"u\", \"G_u\"], N=N, M=M)",
            "print(\"EIG\", y)"
        ]
    },
    {
        "number": 926,
        "comments": "format",
        "commit_message": "fixing typo in torch backend subtract() | numpy-array-api test should pass\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def subtract(",
            "return torch.subtract(x1, x2, out=out)",
            "return torch.subtract(",
            "x1 if isinstance(x1, torch.Tensor) else torch.tensor(x1, dtype=x2.dtype),",
            "-        x2 if isinstance(x2, torch.Tensor) else torch.tensor(x2, dtype=x2.dtype),",
            "+        x2 if isinstance(x2, torch.Tensor) else torch.tensor(x2, dtype=x1.dtype),",
            ")"
        ]
    },
    {
        "number": 928,
        "comments": "add graph",
        "commit_message": "Revert \"attempt to fix media types test in pytoch (#2339)\"\n\nThis reverts commit c4ede069c33adc380ce5c3223f1acca035a2756e.\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def main():",
            "'tensorflow-variable-single-summary': tensorflow_variable_single,",
            "'tensorflow-variable-multi-summary': tensorflow_variable_multi,",
            "",
            "-            #'graph-summary': graph,",
            "+            'graph-summary': graph,",
            "})",
            "",
            "#history.add({"
        ]
    },
    {
        "number": 929,
        "comments": "add API call for type fix",
        "commit_message": "[BugFix] fix compression bugs (#5140)\n\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def rand_like_with_shape(shape, ori_t):",
            "higher_bound = torch.max(ori_t)",
            "",
            "if dtype in [torch.uint8, torch.int16, torch.short, torch.int16, torch.long, torch.bool]:",
            "-        return torch.randint(lower_bound, higher_bound+1, shape, dtype=dtype, device=device)",
            "+        return torch.randint(lower_bound.long(), higher_bound.long() + 1, shape, dtype=dtype, device=device)",
            "else:",
            "return torch.rand(shape, dtype=dtype, device=device, requires_grad=require_grad)"
        ]
    },
    {
        "number": 930,
        "comments": "add condition check for resource fix",
        "commit_message": "More cuda fixes (#379)\n\nDefault use_cuda=torch.Tensor.is_cuda in irange and friends\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def map_data_vector_model(subsample_size):",
            "pyro.sample(\"x\", dist.normal, mu[batch], sigma[batch])",
            "return batch",
            "",
            "-    ind = Variable(torch.LongTensor(range(20)))",
            "+    LongTensor = torch.cuda.LongTensor if torch.Tensor.is_cuda else torch.LongTensor",
            "+    ind = Variable(LongTensor(range(20)))",
            "batch = pyro.map_data('mapdata', ind, local_model, batch_size=subsample_size)",
            "return list(batch.data)"
        ]
    },
    {
        "number": 932,
        "comments": "print update",
        "commit_message": "fixed small bug in style transfer solution\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def train(model, generated_image, initial_image):",
            "## 2. create writer to write your graph",
            "saver = tf.train.Saver()",
            "sess.run(tf.global_variables_initializer())",
            "+        writer = tf.summary.FileWriter(EXP + '/graphs', sess.graph)",
            "###############################",
            "sess.run(generated_image.assign(initial_image))",
            "ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/checkpoint'))"
        ]
    },
    {
        "number": 933,
        "comments": "change param for math fix",
        "commit_message": "fix dropout scaling from p to 1/(1-p) (#816)\n\nCo-authored-by: Sukru Eryilmaz <seryilmaz@computelab-dgx1v-32.nvidia.com>\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class SelfAttnFunc(torch.autograd.Function):",
            "values_grads   = torch.bmm(dropout_results.transpose(1,2), output_lin_grads, out=values_grads.transpose(0,1))",
            "",
            "# Mask and Scaling for Dropout (not a publically documented op)",
            "-        dropout_grads = torch._masked_scale(matmul2_dgrad1, dropout_mask, dropout_prob_t[0])",
            "+        dropout_grads = torch._masked_scale(matmul2_dgrad1, dropout_mask, 1.0/(1.0-dropout_prob_t[0]))",
            "",
            "# Softmax Grad (not a publically documented op)",
            "softmax_grads = torch._softmax_backward_data(dropout_grads, softmax_results, -1, softmax_results)"
        ]
    },
    {
        "number": 934,
        "comments": "add condition check for null fix",
        "commit_message": "refactor trainer checks (#1651)\n\n* refactor trainer checks\n\n* opt\n\n* none\n\n* Apply suggestions from code review\n\n* imports\n\n* fix tensors\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class ValidationEpochEndVariations(ABC):",
            "",
            "val_acc_mean += val_acc",
            "",
            "-        val_loss_mean /= len(outputs)",
            "-        val_acc_mean /= len(outputs)",
            "+        if outputs:  # skip zero divisions",
            "+            val_loss_mean /= len(outputs)",
            "+            val_acc_mean /= len(outputs)",
            "",
            "metrics_dict = {'val_loss': val_loss_mean.item(), 'val_acc': val_acc_mean.item()}",
            "results = {'progress_bar': metrics_dict, 'log': metrics_dict}"
        ]
    },
    {
        "number": 935,
        "comments": "add param for resource fix",
        "commit_message": "TensorRT PyTorch Hub inference fix (#7560)\n\nSolution proposed in https://github.com/ultralytics/yolov5/issues/7128 to TRT PyTorch Hub CUDA illegal memory errors.\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class AutoShape(nn.Module):",
            "#   multiple:        = [Image.open('image1.jpg'), Image.open('image2.jpg'), ...]  # list of images",
            "",
            "t = [time_sync()]",
            "-        p = next(self.model.parameters()) if self.pt else torch.zeros(1)  # for device and type",
            "+        p = next(self.model.parameters()) if self.pt else torch.zeros(1, device=self.model.device)  # for device, type",
            "autocast = self.amp and (p.device.type != 'cpu')  # Automatic Mixed Precision (AMP) inference",
            "if isinstance(imgs, torch.Tensor):  # torch",
            "with amp.autocast(autocast):"
        ]
    },
    {
        "number": 936,
        "comments": "add condition check for null fix",
        "commit_message": "Fix all stable diffusion (#1415)\n\n* up\n\n* uP\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class CycleDiffusionPipeline(DiffusionPipeline):",
            "",
            "device = torch.device(f\"cuda:{gpu_id}\")",
            "",
            "-        for cpu_offloaded_model in [self.unet, self.text_encoder, self.vae, self.safety_checker]:",
            "+        for cpu_offloaded_model in [self.unet, self.text_encoder, self.vae]:",
            "if cpu_offloaded_model is not None:",
            "cpu_offload(cpu_offloaded_model, device)",
            "",
            "+        if self.safety_checker is not None:",
            "+            # TODO(Patrick) - there is currently a bug with cpu offload of nn.Parameter in accelerate",
            "+            # fix by only offloading self.safety_checker for now",
            "+            cpu_offload(self.safety_checker.vision_model)",
            "+",
            "@property",
            "# Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline._execution_device",
            "def _execution_device(self):"
        ]
    },
    {
        "number": 937,
        "comments": "add param for type fix",
        "commit_message": "TF Seq2Seq int dtype fix (#13496)\n\nFixes problems with passing int64 input to TF Seq2Seq models.\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def shift_tokens_right(input_ids: tf.Tensor, pad_token_id: int, decoder_start_to",
            "",
            "if tf.executing_eagerly():",
            "# \"Verify that `labels` has only positive values and -100\"",
            "-        assert_gte0 = tf.debugging.assert_greater_equal(shifted_input_ids, tf.constant(0))",
            "+        assert_gte0 = tf.debugging.assert_greater_equal(shifted_input_ids, tf.constant(0, dtype=input_ids.dtype))",
            "",
            "# Make sure the assertion op is called by wrapping the result in an identity no-op",
            "with tf.control_dependencies([assert_gte0]):"
        ]
    },
    {
        "number": 939,
        "comments": "format",
        "commit_message": "fix for alphabet import\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class ConvEncoder(AbsEncoder):",
            "return self._output_dim",
            "",
            "def forward(self, input: torch.Tensor, ilens: torch.Tensor):",
            "-        \"\"\"",
            "+        \"\"\"Forward.",
            "+",
            "Args:",
            "-            input (torch.Tensor): mixed speech [Batch, sample]",
            "-            ilens (torch.Tensor): input lengths [Batch]",
            "+        input (torch.Tensor): mixed speech [Batch, sample]",
            "+        ilens (torch.Tensor): input lengths [Batch]",
            "\"\"\"",
            "assert input.dim() == 2, \"Currently only support single channle input\""
        ]
    },
    {
        "number": 940,
        "comments": "change API call for math fix",
        "commit_message": "fixes to arange\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def arange(start, stop=None, step=1, dtype=None, dev=None):",
            "if dtype in [torch.int8, torch.uint8, torch.int16]:",
            "return torch.arange(start, stop, step=step, dtype=torch.int64, device=dev).to(dtype)",
            "else:",
            "-            return torch.range(start, stop, step=step, dtype=dtype, device=dev)",
            "+            return torch.arange(start, stop, step=step, dtype=dtype, device=dev)"
        ]
    },
    {
        "number": 941,
        "comments": "add method",
        "commit_message": "Api (#679)\n\n* legacy\n\n* new files\n\n* Update demo.py\n\n* test tunner bug fixed\n\n* classes and funtions in the demo created\n\n* network implemented not tested\n\n* travis to ignore legacy tests\n\n* test connectedHyperparameter\n\n* test fixed\n\n* some basic blocks implemented\n\n* still debuging\n\n* basic test for hypergraph passed\n\n* merge layer implemented\n\n* hyper_graph fully tested\n\n* more args added to auto model\n\n* test fixed\n\n* local changes\n\n* test auto_model\n\n* auto_model fit signature changed for validation data\n\n* Refactor (#646)\n\n* super classes extending object\n\n* change n_ to num_\n\n* refactored automodel to extend hypermodel and removed tuner from signature\n\n* rename HyperNode to Node. TextInput added\n\n* rename HyperGraph to GraphAutoModel extending AutoModel\n\n* refactor hyperhead, removed tensor heads\n\n* removed unnecessary blocks, rename build_output to build\n\n* changed some functions and attributes to private\n\n* remove trails from AutoModel public API\n\n* test cases changed accordingly\n\n* import modules instead of objects\n\n* tuner deleted from AutoModel contructor\n\n* change trails to num_trials\n\n* use the same quote sign\n\n* revised auto pipeline docs\n\n* removed compile from AutoModel\n\n* loss and metrics moved to hyper heads\n\n* do not flatten by default in hyperheads\n\n* inputs and outputs down to GraphAutoModel\n\n* changed AutoModel\n\n* name_scope changed to tf 2.0 and moved to hyperparameters and hypermodel\n\n* remove HierarchicalHyperParameters\n\n* renaming some variables and make private some members\n\n* remove legacy\n\n* test fixed\n\n* changed setup.py\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "+import tensorflow as tf",
            "+from autokeras.hyperparameters import HyperParameters",
            "+",
            "+",
            "+def test_hierarchical_hyperparameters():",
            "+    hp = HyperParameters()",
            "+    with tf.name_scope('abc'):",
            "+        hp.Choice('num_layers', [1, 2, 3], default=1)",
            "+    assert 'abc/num_layers' in hp.values"
        ]
    },
    {
        "number": 942,
        "comments": "doc string print error log update",
        "commit_message": "Use f-strings in the dataset scripts (#3291)\n\n* Finishes #3257\n\nUsed f-strings to format the .py files in the dataset folder\n\n* Fix style\n\n* Fix hkcancor dataset\n\nCo-authored-by: Mario \u0160a\u0161ko <mario@huggingface.co>\nCo-authored-by: mariosasko <mariosasko777@gmail.com>\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class TimesOfIndiaNewsHeadlines(datasets.GeneratorBasedBuilder):",
            "path_to_manual_file = os.path.join(os.path.abspath(os.path.expanduser(dl_manager.manual_dir)), _FILENAME)",
            "if not os.path.exists(path_to_manual_file):",
            "raise FileNotFoundError(",
            "-                \"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('times_of_india_news_headlines', data_dir=...)` that includes a file name {}. Manual download instructions: {})\".format(",
            "-                    path_to_manual_file, _FILENAME, self.manual_download_instructions",
            "-                )",
            "+                f\"{path_to_manual_file} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('times_of_india_news_headlines', data_dir=...)` that includes a file name {_FILENAME}. Manual download instructions: {self.manual_download_instructions})\"",
            ")",
            "return [datasets.SplitGenerator(name=datasets.Split.TRAIN, gen_kwargs={\"path\": path_to_manual_file})]"
        ]
    },
    {
        "number": 943,
        "comments": "format",
        "commit_message": "Issue #642 - Add `AtrousDeConv2dLayer` (#662)\n\n* Update visualize.py\n\n* Update README.md\n\nAdd an example for Adversarial Learning\n\n* Update more.rst\n\nUpdate the URLs\n\n* Update more.rst\n\n* Update example.rst\n\nAdd the same link of BEGAN implementation.\n\n* Update example.rst\n\n* Update example.rst\n\n* Create tutorial_tfslim.py\n\nfixes #552\n\n* Update tutorial_tfslim.py\n\n* Update utils.py\n\nFix #565\n\n* Update utils.py\n\n* Create test_utils_predict.py\n\nrelated with #288, #565, #566\n\n* Create test_utils_predict.py\n\n* Update utils.py\n\n* Update test_utils_predict.py\n\n* Update CHANGELOG.md\n\nrelated to #566\n\n* Update test_utils_predict.py\n\n* Update CHANGELOG.md\n\n* Update CHANGELOG.md\n\n* Update CHANGELOG.md\n\n* Update test_utils_predict.py\n\n* Update test_utils_predict.py\n\n* Update test_utils_predict.py\n\n* Update test_utils_predict.py\n\n* Update test_utils_predict.py\n\n* Update test_utils_predict.py (fix Bad Coding Style)\n\n* Update test_utils_predict.py\n\n* Update CHANGELOG.md\n\n* Update CHANGELOG.md\n\n* Update CHANGELOG.md\n\n* Update CHANGELOG.md\n\n* Update convolution.py (Add AtrousConv2dTransLayer)\n\n* Add AtrousConv2dTransLayer\n\n* Fix some mistakes\n\n* Follow protocols\n\n* Fix coding style (yapf)\n\n* AtrousConv2dLayer fixed\n\n* AtrousConv2dTransposeLayer refactored\n\n* Fix coding style (yapf)\n\n* Fix error\n\n* Bias Add using premade tf func\n\n* Old TF Code Removed\n\n* Renamed to AtrousDeConv2dLayer\n\n* Update CHANGELOG.md\n\n* Release 1.8.6rc2\n\n* Documentation Fix\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def main(_):",
            "# n_examples = batch_size * num_steps",
            "# so",
            "# cost is the averaged cost of each mini-batch (concurrent process).",
            "-        loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example(  # loss = tf.nn.seq2seq.sequence_loss_by_example( # TF0.12",
            "-            [outputs], [tf.reshape(targets, [-1])], [tf.ones_like(tf.reshape(targets, [-1]), dtype=tf.float32)])",
            "+        loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example(",
            "+            [outputs], [tf.reshape(targets, [-1])], [tf.ones_like(tf.reshape(targets, [-1]), dtype=tf.float32)]",
            "+        )",
            "# [tf.ones([batch_size * num_steps])])",
            "cost = tf.reduce_sum(loss) / batch_size",
            "return cost"
        ]
    },
    {
        "number": 944,
        "comments": "change API call for shape fix",
        "commit_message": "Speed-up warp_affine and fix bugs in RandomAffine [WIP: to add tests] (#474)\n\n* speed-up warp_affine, rotate, random_crop\n\n* added basic speed test for warp_affine\n\n* fixed centerization for random shear and bug (radians instead of degrees)\n\n* add test versus torchvision\n\n* added convert_affinematrix_to_homography function\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def _apply_affine(input: torch.Tensor,",
            "",
            "height, width = x_data.shape[-2:]",
            "transform: torch.Tensor = params['transform'].to(device, dtype)",
            "-",
            "-    out_data: torch.Tensor = warp_perspective(x_data, transform, (height, width))",
            "+    out_data: torch.Tensor = warp_affine(x_data, transform[:, :2, :], (height, width))",
            "",
            "if return_transform:",
            "return out_data.view_as(input), transform"
        ]
    },
    {
        "number": 945,
        "comments": "format",
        "commit_message": "added docstrings examples for general:unstack (#2594)\n\n* added docstrings examples for general:unstack\n\n* fixed bugs in unstack docstrings\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def floormod(",
            "return ret",
            "",
            "",
            "-def unstack(x, axis: int, keepdims: bool = False) -> List[torch.Tensor]:",
            "+def unstack(",
            "+    x: torch.Tensor,",
            "+    axis: int,",
            "+    keepdims: bool = False",
            "+) -> List[torch.Tensor]:",
            "if x.shape == ():",
            "return [x]",
            "ret = list(torch.unbind(x, axis))"
        ]
    },
    {
        "number": 946,
        "comments": "update API call for refactor fix",
        "commit_message": "[style] consistent nn. and nn.functional (#12124)\n\n* consistent nn. and nn.functional\n\n* fix glitch\n\n* fix glitch #2\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class GPTNeoAttentionMixin:",
            "else:",
            "raise ValueError(f\"Input tensor rank should be one of [2, 3], but is: {len(tensor.shape)}\")",
            "",
            "-        padded_tensor = F.pad(tensor, padding_side, value=pad_value)",
            "+        padded_tensor = nn.functional.pad(tensor, padding_side, value=pad_value)",
            "padded_tensor = padded_tensor.unfold(dimension=1, size=window_size + block_length, step=block_length)",
            "",
            "if is_key_value:"
        ]
    },
    {
        "number": 947,
        "comments": "format",
        "commit_message": "lintfixbot: Auto-commit fixed lint errors in codebase\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "import ivy",
            "from typing import Optional, Union",
            "",
            "",
            "-def logit(x: Union[tf.Tensor, tf.Variable],",
            "-          /,",
            "-          *,",
            "-          eps: Optional[float] = None,",
            "-          out=None):",
            "+def logit(",
            "+    x: Union[tf.Tensor, tf.Variable], /, *, eps: Optional[float] = None, out=None",
            "+):",
            "x_dtype = x.dtype",
            "if eps is None:",
            "x = tf.where(tf.math.logical_or(x > 1, x < 0), ivy.nan, x)"
        ]
    },
    {
        "number": 949,
        "comments": "change param for type fix",
        "commit_message": "Update serving signatures and make sure we actually use them (#19034)\n\n* Override save() to use the serving signature as the default\n\n* Replace int32 with int64 in all our serving signatures\n\n* Remember one very important line so as not to break every test at once\n\n* Dtype fix for TFLED\n\n* dtype fix for shift_tokens_right in general\n\n* Dtype fixes in mBART and RAG\n\n* Fix dtypes for test_unpack_inputs\n\n* More dtype fixes\n\n* Yet more mBART + RAG dtype fixes\n\n* Yet more mBART + RAG dtype fixes\n\n* Add a check that the model actually has a serving method\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class TFTransfoXLPreTrainedModel(TFPreTrainedModel):",
            "@tf.function(",
            "input_signature=[",
            "{",
            "-                \"input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"input_ids\"),",
            "+                \"input_ids\": tf.TensorSpec((None, None), tf.int64, name=\"input_ids\"),",
            "}",
            "]",
            ")"
        ]
    },
    {
        "number": 951,
        "comments": "change param for argument fix",
        "commit_message": "Scope TokenIndexer output by indexer name (#3597)\n\n* Indexer tests are now passing, at least\n\n* Fixed some masking issues, and padding keys for one config file\n\n* TextFieldEmbedder tests pass\n\n* Fixing fixtures, some more tests passing\n\n* Fixed weird ordering bug\n\n* All TokenEmbedder tests passing?\n\n* Update fixtures\n\n* Fix more hard-coded references\n\n* fix field tests\n\n* fix dataset reader tests\n\n* Fix iterator tests\n\n* More tests passing\n\n* fix hotflip and some other tests\n\n* more tests\n\n* more test fixes\n\n* more tests\n\n* most tests passing; I think the remaining ones are spacy model changes\n\n* hard-code POS tag test\n\n* last test, I think\n\n* black\n\n* updated black\n\n* flake8\n\n* mypy\n\n* black again\n\n* fix training configs\n\n* remove reference to embedder_to_indexer_map\n\n* Other fixes from PR comments\n\n* fix breakage from incorrect merge during rebase\n\n* flake, some docstring formatting\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class BiattentiveClassificationNetwork(Model):",
            "# Create ELMo embeddings if applicable",
            "if self._elmo:",
            "if elmo_tokens is not None:",
            "-                elmo_representations = self._elmo(elmo_tokens)[\"elmo_representations\"]",
            "+                elmo_representations = self._elmo(elmo_tokens[\"tokens\"])[\"elmo_representations\"]",
            "# Pop from the end is more performant with list",
            "if self._use_integrator_output_elmo:",
            "integrator_output_elmo = elmo_representations.pop()"
        ]
    },
    {
        "number": 953,
        "comments": "test fix",
        "commit_message": "fix some unit tests\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def test_tagged_corpus_downsample():",
            "",
            "assert 10 == len(corpus.train)",
            "",
            "-    corpus.downsample(percentage=0.3, only_downsample_train=True)",
            "+    corpus.downsample(percentage=0.3, downsample_dev=False, downsample_test=False)",
            "",
            "assert 3 == len(corpus.train)"
        ]
    },
    {
        "number": 957,
        "comments": "change param for refactor fix",
        "commit_message": "Fix LiL sparse matrix on Tensorflow (#4173)\n\nLiL sparse matrices would not work correctly due to dtype being\ndifferent. Using the sparse_coo data fixes it.\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class Function(object):",
            "if is_sparse(tensor):",
            "sparse_coo = value.tocoo()",
            "indices = np.concatenate((np.expand_dims(sparse_coo.row, 1), np.expand_dims(sparse_coo.col, 1)), 1)",
            "-                value = (indices, value.data, value.shape)",
            "+                value = (indices, sparse_coo.data, sparse_coo.shape)",
            "feed_dict[tensor] = value",
            "session = get_session()",
            "updated = session.run(self.outputs + [self.updates_op], feed_dict=feed_dict)"
        ]
    },
    {
        "number": 958,
        "comments": "version fix",
        "commit_message": "Adds model configs to ludwig.datasets (#2540)\n\n* Adds README and stub for reading dataset configs.\n\n* Adds __init__.py for configs, moves circular import into function scope in ludwig/datasets/__init__.py\n\n* Print config files in datasets folder.\n\n* First pass at automatic archive extraction.\n\n* Implemented downloading and extract.\n\n* Refactor DatasetConfig into its own file.\n\n* Fixed bugs downloading kaggle dataset.\n\n* Makes registry store dataset instances, not classes. Also comments out import_submodules for testing.\n\n* Typo fix.\n\n* Only pass data files on to load_unprocessed_dataframe, symlink directories.\n\n* Downloading dataset files into existing directory if exists.\n\n* Refactor: make datasets fully config-first, lazy load dataset loaders.\n\n* Implemented agnews custom loader.\n\n* Implements train/validation/test split by files, and globbing support\n\n* Adds _glob_multiple\n\n* Adds adult_census_income, agnews, allstate_claims_severity.\n\n* Implements sha256 verification, adds more datasets up to creditcard_fraud.\n\n* Adds checksums, dbpedia, electricity\n\n* Fixes gzip file name returned as string not list, adds up to forest_cover dataset.\n\n* Adds datasets up to reuters_r8\n\n* Adds all datasets which don't require a custom class.\n\n* Restore dataset import behavior by implementing module __getattr__\n\n* Adds KDD datasets.\n\n* Adds ieee_fraud.\n\n* Adds imbalanced_insurance, insurance_lite.\n\n* Adds mnist.\n\n* Completes implementation of all of the built-in datasets.\n\n* Made cache_dir optional, read from environment variable if set.\n\n* Upgrades datasets tests.\n\n* Adds test for new dataset config API.  Also adds scripts for dataset link checking.\n\n* Fixes loading allstate claims severity dataset.\n\n* Use @lru_cache(1), @cache not supported in python < 3.9\n\n* Deletes dataset registry, updates automl test utils\n\n* Fix imports of datasets API.\n\n* Adds more detail to sha256: docstring and basic README\n\n* Copy-paste link oops.\n\n* Fixes handling of nested archive types like .tar.bz  Also adds a LUDWIG_CACHE and export to the README\n\n* Adds link for twitter bots.\n\n* Fix order of splits in README.md\n\n* typo\n\n* Adds verify as a phase in doc string.\n\n* Support .pqt, .pq extensions for parquet.\n\n* Handle nested archives with longer file extensions like .csv.zip\n\n* Handle nested .gz types properly too.  Check all extensions with .endswith\n\n* Handle all archive types with .endswith\n\n* Update ludwig/datasets/loaders/split_loaders.py\n\nCo-authored-by: Joppe Geluykens <joppe@rvrie.com>\n\n* Adds explanation for export, fixes preserve_paths (should be relative to processed_dataset_dir)\n\n* Resolve preserved paths relative to raw dataset dir before move.\n\n* Catch runtime exception from extracting sub-archives.\n\n* Started adding info to README about dataset model configs.\n\n* Adds method to get model configs for datasets.\n\n* Adds mnist, titanic examples as default configs.\n\n* Export dataset before training.\n\n* Adds multiprocessing version of train_all_model_configs, and adds a few configs mosttly from automl experiments.\n\n* Adds a few more configs, removes AG news, training is too slow.\n\n* Default to only 4 processes due to memory constraints.\n\n* Visualize learning curves.\n\n* Started documenting API functions in readme.\n\n* Adds test for model configs API, updates tests to mock protected _load_dataset_config\n\n* Clear dataset cache after testing with mock datasets.\n\n* Adds best configs, improved README\n\n* higgs_best consistent formatting.\n\n* Update ludwig/datasets/README.md\n\nCo-authored-by: abidwael <103003638+abidwael@users.noreply.github.com>\n\n* Adds model commit hash to results.\n\n* Adds MSE, MAE to metric list.\n\n* Don't printout ludwig commit.\n\n* Increase display width to show more columns in print output.\n\n* Fix error in get_commit_hash\n\nCo-authored-by: Daniel Treiman <daniel@predibase.com>\nCo-authored-by: Joppe Geluykens <joppe@rvrie.com>\nCo-authored-by: abidwael <103003638+abidwael@users.noreply.github.com>\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "def test_download_mnist_dataset(tmpdir):",
            ")",
            "",
            "ludwig.datasets._get_dataset_configs.cache_clear()",
            "-    with mock.patch(\"ludwig.datasets.load_dataset_config\", return_value=config):",
            "+    with mock.patch(\"ludwig.datasets._load_dataset_config\", return_value=config):",
            "dataset = ludwig.datasets.get_dataset(\"mnist\", cache_dir=tmpdir)",
            "assert not dataset.state == DatasetState.DOWNLOADED",
            "assert not dataset.state == DatasetState.TRANSFORMED",
            "dataset.download()",
            "",
            "assert dataset.state == DatasetState.DOWNLOADED",
            "+    ludwig.datasets._get_dataset_configs.cache_clear()"
        ]
    },
    {
        "number": 960,
        "comments": "add param for argument fix",
        "commit_message": "minor fix of li52\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def test_transformer_trainable_and_decodable(model_dict):",
            "attn_dict = model.calculate_all_attentions(",
            "x[0:1], ilens[0:1], y_tgt[0:1], y_src[0:1]",
            ")",
            "-    plot.plot_multi_head_attention(data, attn_dict, \"\", savefn=_savefn)",
            "+    plot.plot_multi_head_attention(data, uttid_list, attn_dict, \"\", savefn=_savefn)",
            "",
            "# test CTC plot",
            "ctc_probs = model.calculate_all_ctc_probs("
        ]
    },
    {
        "number": 961,
        "comments": "change param for type fix",
        "commit_message": "Update serving signatures and make sure we actually use them (#19034)\n\n* Override save() to use the serving signature as the default\n\n* Replace int32 with int64 in all our serving signatures\n\n* Remember one very important line so as not to break every test at once\n\n* Dtype fix for TFLED\n\n* dtype fix for shift_tokens_right in general\n\n* Dtype fixes in mBART and RAG\n\n* Fix dtypes for test_unpack_inputs\n\n* More dtype fixes\n\n* Yet more mBART + RAG dtype fixes\n\n* Yet more mBART + RAG dtype fixes\n\n* Add a check that the model actually has a serving method\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class TFLayoutLMv3PreTrainedModel(TFPreTrainedModel):",
            "@tf.function(",
            "input_signature=[",
            "{",
            "-                \"input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"input_ids\"),",
            "-                \"bbox\": tf.TensorSpec((None, None, 4), tf.int32, name=\"bbox\"),",
            "+                \"input_ids\": tf.TensorSpec((None, None), tf.int64, name=\"input_ids\"),",
            "+                \"bbox\": tf.TensorSpec((None, None, 4), tf.int64, name=\"bbox\"),",
            "\"pixel_values\": tf.TensorSpec((None, None, None, None), tf.float32, name=\"pixel_values\"),",
            "-                \"attention_mask\": tf.TensorSpec((None, None), tf.int32, name=\"attention_mask\"),",
            "+                \"attention_mask\": tf.TensorSpec((None, None), tf.int64, name=\"attention_mask\"),",
            "}",
            "]",
            ")"
        ]
    },
    {
        "number": 962,
        "comments": "add API call for resource fix",
        "commit_message": "Fix gpt2 fp16 training when tracing is enabled (#20656)\n\n* ONNX tracing fix\n\n* Remove conditional\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class DecisionTransformerGPT2Attention(nn.Module):",
            "# Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.",
            "# Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`",
            "mask_value = torch.full([], mask_value, dtype=attn_weights.dtype).to(attn_weights.device)",
            "-            attn_weights = torch.where(causal_mask, attn_weights, mask_value)",
            "+            attn_weights = torch.where(causal_mask, attn_weights.to(attn_weights.dtype), mask_value)",
            "",
            "if attention_mask is not None:",
            "# Apply the attention mask"
        ]
    },
    {
        "number": 963,
        "comments": "update API call for refactor fix",
        "commit_message": "Fix and enable some tfcoreml converter convent tests\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def load_tf_graph(graph_file):",
            "\"\"\"",
            "# We load the protobuf file from the disk and parse it to retrieve the",
            "# unserialized graph_def",
            "-    with tf.gfile.GFile(graph_file, \"rb\") as f:",
            "-        graph_def = tf.GraphDef()",
            "+    with tf.io.gfile.GFile(graph_file, \"rb\") as f:",
            "+        graph_def = tf.compat.v1.GraphDef()",
            "graph_def.ParseFromString(f.read())",
            "",
            "# Then, we import the graph_def into a new Graph and returns it"
        ]
    },
    {
        "number": 964,
        "comments": "add method",
        "commit_message": "stop gradients (#3221)\n\n* stop gradients\n\n* fix stop grad test\n\n* stop gradients\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def gradients(loss, variables):",
            "return tf.gradients(loss, variables)",
            "",
            "",
            "+def stop_gradient(variables):",
            "+    '''Returns `variables` but with zero gradient with respect to every other",
            "+    variables.",
            "+    '''",
            "+    return tf.stop_gradient(variables)",
            "+",
            "+",
            "# CONTROL FLOW",
            "",
            "def rnn(step_function, inputs, initial_states,"
        ]
    },
    {
        "number": 965,
        "comments": "doc update",
        "commit_message": "[BugFix] fix compression bugs (#5140)\n\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "},",
            "\"outputs\": [],",
            "\"source\": [",
            "-        \"import torch\\nimport torch.nn.functional as F\\nfrom torch.optim import SGD\\n\\nfrom scripts.compression_mnist_model import TorchModel, trainer, evaluator, device, test_trt\\n\\n# define the model\\nmodel = TorchModel().to(device)\\n\\n# define the optimizer and criterion for pre-training\\n\\noptimizer = SGD(model.parameters(), 1e-2)\\ncriterion = F.nll_loss\\n\\n# pre-train and evaluate the model on MNIST dataset\\nfor epoch in range(3):\\n    trainer(model, optimizer, criterion)\\n    evaluator(model)\"",
            "+        \"import torch\\nimport torch.nn.functional as F\\nfrom torch.optim import SGD\\n\\nfrom nni_assets.compression.mnist_model import TorchModel, trainer, evaluator, device, test_trt\\n\\n# define the model\\nmodel = TorchModel().to(device)\\n\\n# define the optimizer and criterion for pre-training\\n\\noptimizer = SGD(model.parameters(), 1e-2)\\ncriterion = F.nll_loss\\n\\n# pre-train and evaluate the model on MNIST dataset\\nfor epoch in range(3):\\n    trainer(model, optimizer, criterion)\\n    evaluator(model)\"",
            "]",
            "},",
            "{"
        ]
    },
    {
        "number": 966,
        "comments": "doc update",
        "commit_message": "Doc styler examples (#14953)\n\n* Fix bad examples\n\n* Add black formatting to style_doc\n\n* Use first nonempty line\n\n* Put it at the right place\n\n* Don't add spaces to empty lines\n\n* Better templates\n\n* Deal with triple quotes in docstrings\n\n* Result of style_doc\n\n* Enable mdx treatment and fix code examples in MDXs\n\n* Result of doc styler on doc source files\n\n* Last fixes\n\n* Break copy from\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class Wav2Vec2ForMaskedLM(Wav2Vec2PreTrainedModel):",
            ">>> processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")",
            ">>> model = Wav2Vec2ForMaskedLM.from_pretrained(\"facebook/wav2vec2-base-960h\")",
            "",
            "+",
            ">>> def map_to_array(batch):",
            "-        >>>     speech, _ = sf.read(batch[\"file\"])",
            "-        >>>     batch[\"speech\"] = speech",
            "-        >>>     return batch",
            "+        ...     speech, _ = sf.read(batch[\"file\"])",
            "+        ...     batch[\"speech\"] = speech",
            "+        ...     return batch",
            "+",
            "",
            ">>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")",
            ">>> ds = ds.map(map_to_array)"
        ]
    },
    {
        "number": 967,
        "comments": "add API call for type fix",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class TestScalarMix(AllenNlpTestCase):",
            "tensors = [torch.randn([3, 4, 5]) for _ in range(3)]",
            "numpy_mask = numpy.ones((3, 4), dtype=\"int32\")",
            "numpy_mask[1, 2:] = 0",
            "-        mask = torch.from_numpy(numpy_mask)",
            "+        mask = torch.from_numpy(numpy_mask).bool()",
            "",
            "weights = [0.1, 0.2, 0.3]",
            "for k in range(3):"
        ]
    },
    {
        "number": 968,
        "comments": "change param for type fix",
        "commit_message": "fix bn and rewrite saverrestore with var.load\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def get_global_step_var():",
            "with tf.variable_scope(scope, reuse=False), \\",
            "tf.name_scope(None):",
            "var = tf.get_variable(GLOBAL_STEP_OP_NAME,",
            "-                                  initializer=0,",
            "-                                  trainable=False, dtype=tf.int32)",
            "+                                  initializer=tf.constant(0, dtype=tf.int64),",
            "+                                  trainable=False, dtype=tf.int64)",
            "return var"
        ]
    },
    {
        "number": 969,
        "comments": "add API call for type fix",
        "commit_message": "Speech2TextTransformer (#10175)\n\n* s2t\n\n* fix config\n\n* conversion script\n\n* fix import\n\n* add tokenizer\n\n* fix tok init\n\n* fix tokenizer\n\n* first version working\n\n* fix embeds\n\n* fix lm head\n\n* remove extra heads\n\n* fix convert script\n\n* handle encoder attn mask\n\n* style\n\n* better enc attn mask\n\n* override _prepare_attention_mask_for_generation\n\n* handle attn_maks in encoder and decoder\n\n* input_ids => input_features\n\n* enable use_cache\n\n* remove old code\n\n* expand embeddings if needed\n\n* remove logits bias\n\n* masked_lm_loss => loss\n\n* hack tokenizer to support feature processing\n\n* fix model_input_names\n\n* style\n\n* fix error message\n\n* doc\n\n* remove inputs_embeds\n\n* remove input_embeds\n\n* remove unnecessary docstring\n\n* quality\n\n* SpeechToText => Speech2Text\n\n* style\n\n* remove shared_embeds\n\n* subsample => conv\n\n* remove Speech2TextTransformerDecoderWrapper\n\n* update output_lengths formula\n\n* fix table\n\n* remove max_position_embeddings\n\n* update conversion scripts\n\n* add possibility to do upper case for now\n\n* add FeatureExtractor and Processor\n\n* add tests for extractor\n\n* require_torch_audio => require_torchaudio\n\n* add processor test\n\n* update import\n\n* remove classification head\n\n* attention mask is now 1D\n\n* update docstrings\n\n* attention mask should be of type long\n\n* handle attention mask from generate\n\n* alwyas return attention_mask\n\n* fix test\n\n* style\n\n* doc\n\n* Speech2TextTransformer => Speech2Text\n\n* Speech2TextTransformerConfig => Speech2TextConfig\n\n* remove dummy_inputs\n\n* nit\n\n* style\n\n* multilinguial tok\n\n* fix tokenizer\n\n* add tgt_lang setter\n\n* save lang_codes\n\n* fix tokenizer\n\n* add forced_bos_token_id to tokenizer\n\n* apply review suggestions\n\n* add torchaudio to extra deps\n\n* add speech deps to CI\n\n* fix dep\n\n* add libsndfile to ci\n\n* libsndfile1\n\n* add speech to extras all\n\n* libsndfile1 -> libsndfile1\n\n* libsndfile\n\n* libsndfile1-dev\n\n* apt update\n\n* add sudo to install\n\n* update deps table\n\n* install libsndfile1-dev on CI\n\n* tuple to list\n\n* init conv layer\n\n* add model tests\n\n* quality\n\n* add integration tests\n\n* skip_special_tokens\n\n* add speech_to_text_transformer in toctree\n\n* fix tokenizer\n\n* fix fp16 tests\n\n* add tokenizer tests\n\n* fix copyright\n\n* input_values => input_features\n\n* doc\n\n* add model in readme\n\n* doc\n\n* change checkpoint names\n\n* fix copyright\n\n* fix code example\n\n* add max_model_input_sizes in tokenizer\n\n* fix integration tests\n\n* add do_lower_case to tokenizer\n\n* remove clamp trick\n\n* fix \"Add modeling imports here\"\n\n* fix copyrights\n\n* fix tests\n\n* SpeechToTextTransformer => SpeechToText\n\n* fix naming\n\n* fix table formatting\n\n* fix typo\n\n* style\n\n* fix typos\n\n* remove speech dep from extras[testing]\n\n* fix copies\n\n* rename doc file,\n\n* put imports under is_torch_available\n\n* run feat extract tests when torch is available\n\n* dummy objects for processor and extractor\n\n* fix imports in tests\n\n* fix import in modeling test\n\n* fxi imports\n\n* fix torch import\n\n* fix imports again\n\n* fix positional embeddings\n\n* fix typo in import\n\n* adapt new extractor refactor\n\n* style\n\n* fix torchscript test\n\n* doc\n\n* doc\n\n* Apply suggestions from code review\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* fix docs, copied from, style\n\n* fix docstring\n\n* handle imports\n\n* remove speech from all extra deps\n\n* remove s2t from seq2seq lm mapping\n\n* better names\n\n* skip training tests\n\n* add install instructions\n\n* List => Tuple\n\n* doc\n\n* fix conversion script\n\n* fix urls\n\n* add instruction for libsndfile\n\n* fix fp16 test\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class SequenceFeatureExtractionTestMixin(FeatureExtractionSavingTestMixin):",
            "input_np = feat_extract.pad(processed_features, padding=\"longest\", return_tensors=\"np\")[input_name]",
            "input_tf = feat_extract.pad(processed_features, padding=\"longest\", return_tensors=\"tf\")[input_name]",
            "",
            "-        self.assertTrue(abs(input_np.sum() - input_tf.numpy().sum()) < 1e-2)",
            "+        self.assertTrue(abs(input_np.astype(np.float32).sum() - input_tf.numpy().sum()) < 1e-2)",
            "",
            "def test_attention_mask(self):",
            "feat_dict = self.feat_extract_dict"
        ]
    },
    {
        "number": 971,
        "comments": "add param for resource fix",
        "commit_message": "[Pytorch] pytorch only timesteps (#724)\n\n* pytorch timesteps\n\n* style\n\n* get rid of if-else\n\n* fix test\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class ScoreSdeVeScheduler(SchedulerMixin, ConfigMixin):",
            "\"\"\"",
            "sampling_eps = sampling_eps if sampling_eps is not None else self.config.sampling_eps",
            "",
            "-        self.timesteps = torch.linspace(1, sampling_eps, num_inference_steps)",
            "+        self.timesteps = torch.linspace(1, sampling_eps, num_inference_steps, device=device)",
            "",
            "def set_sigmas(",
            "self, num_inference_steps: int, sigma_min: float = None, sigma_max: float = None, sampling_eps: float = None"
        ]
    },
    {
        "number": 972,
        "comments": "add API call for type fix",
        "commit_message": "fix mypy errors\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class SSIM(nn.Module):",
            "ssim_map = ((2 * mu1_mu2 + self.C1) * (2 * sigma12 + self.C2)) / \\",
            "((mu1_sq + mu2_sq + self.C1) * (sigma1_sq + sigma2_sq + self.C2))",
            "",
            "-        loss = torch.clamp(1. - ssim_map, min=0, max=1) / 2.",
            "+        loss = torch.clamp(torch.tensor(1.) - ssim_map, min=0, max=1) / 2.",
            "",
            "if self.reduction == 'mean':",
            "loss = torch.mean(loss)"
        ]
    },
    {
        "number": 974,
        "comments": "add API call for math fix",
        "commit_message": "bugfix\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def rpn_losses(anchor_labels, anchor_boxes, label_logits, box_logits):",
            "placeholder = 1.",
            "label_loss = tf.nn.sigmoid_cross_entropy_with_logits(",
            "labels=tf.to_float(valid_anchor_labels), logits=valid_label_logits)",
            "-    label_loss = label_loss * (1. / config.RPN_BATCH_PER_IM)",
            "+    label_loss = tf.reduce_sum(label_loss) * (1. / config.RPN_BATCH_PER_IM)",
            "label_loss = tf.where(tf.equal(nr_valid, 0), placeholder, label_loss, name='label_loss')",
            "",
            "pos_anchor_boxes = tf.boolean_mask(anchor_boxes, pos_mask)"
        ]
    },
    {
        "number": 975,
        "comments": "add API call for type fix",
        "commit_message": "bug fix about calculation precision\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def test_beamformer_net_wpe_output(ch, num_spk, use_dnn_mask_for_wpe):",
            "def test_beamformer_net_bf_output(num_spk):",
            "ch = 3",
            "inputs = torch.randn(2, 16, ch)",
            "+    inputs = inputs.float()",
            "ilens = torch.LongTensor([16, 12])",
            "model = BeamformerNet(",
            "n_fft=8,"
        ]
    },
    {
        "number": 976,
        "comments": "remove API call for version fix",
        "commit_message": "upgrade to pytorch 0.4.0 (#1126)\n\n* bump pytorch to 0.4 + fix sanitize\n\n* remove check for Variable in block_orthogonal\n\n* rename parameter split_size=\n\n* remove checks for Variable\n\n* fix more tests\n\n* fixes\n\n* get tests to pass\n\n* fix warnings\n\n* get rid of some of the Variables\n\n* more tests passing\n\n* more elimination of variables\n\n* finish removing all Variables\n\n* pylint and such\n\n* a few fixes\n\n* move torch.no_grad into model.forward_on_instances\n\n* more pytorch 0.4 changes\n\n* detach() -> data\n\n* pylint\n\n* fix bad tensor creation\n\n* fix types\n\n* remove print statement\n\n* more 0.4 goodness\n\n* factor out is_tensor\n\n* add no_grad to elmo command\n\n* cleanup\n\n* remove TODO\n\n* address PR feedback\n\n* replace all() with item()\n\n* more cleanup\n\n* further cleanup\n\n* remove Variable\n\n* really fix merge conflict\n\n* fix pylint\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "from allennlp.common.testing import AllenNlpTestCase",
            "",
            "class TestElmoLstmCell(AllenNlpTestCase):",
            "def test_elmo_lstm(self):",
            "-        input_tensor = Variable(torch.rand(4, 5, 3))",
            "+        input_tensor = torch.rand(4, 5, 3)",
            "input_tensor[1, 4:, :] = 0.",
            "input_tensor[2, 2:, :] = 0.",
            "input_tensor[3, 1:, :] = 0.",
            "-        mask = Variable(torch.ones([4, 5]))",
            "+        mask = torch.ones([4, 5])",
            "mask[1, 4:] = 0.",
            "mask[2, 2:] = 0.",
            "mask[3, 1:] = 0."
        ]
    },
    {
        "number": 977,
        "comments": "update API call for version fix",
        "commit_message": "Remove object metadata when saving SavedModel.\n\nThis change also fixes a few bugs when loading the metadata file, and fixes Keras tests so that they use model.save instead of tf.saved_model.save\n\nPiperOrigin-RevId: 378963258\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class TpuStrategyTest(tf.test.TestCase):",
            "serving_fn = create_serving_signature(model)",
            "",
            "saved_model_dir = tempfile.mkdtemp(dir=self.get_temp_dir())",
            "-      tf.saved_model.save(",
            "-          model, saved_model_dir, signatures={\"serving_default\": serving_fn})",
            "+      model.save(saved_model_dir, save_format=\"tf\",",
            "+                 signatures={\"serving_default\": serving_fn})",
            "",
            "# Test the saved_model.",
            "loaded_serving_fn = tf.keras.models.load_model("
        ]
    },
    {
        "number": 979,
        "comments": "doc update",
        "commit_message": "fix errors in test/espnet2/enh and wsj0_2mix_spatialized_data_prep\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class BeamformerNet(torch.nn.Module):",
            "def forward_rawwav(self, input: torch.Tensor, ilens: torch.Tensor):",
            "\"\"\"",
            "Args:",
            "-            input (torch.Tensor): mixed speech [Batch, sample]",
            "+            input (torch.Tensor): mixed speech [Batch, Nsample, Channel]",
            "ilens (torch.Tensor): input lengths [Batch]",
            "",
            "Returns:",
            "predcited speech wavs (single-channel):",
            "-                torch.Tensor(Batch, sample), or List[torch.Tensor(Batch, sample)]",
            "+                torch.Tensor(Batch, Nsamples), or List[torch.Tensor(Batch, Nsamples)]",
            "output lengths",
            "predcited masks: OrderedDict[",
            "'dereverb': torch.Tensor(Batch, Frames, Channel, Freq),"
        ]
    },
    {
        "number": 980,
        "comments": "change param for refactor fix",
        "commit_message": "remove unused import + fix LM zero_state\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class SequentialRNNLM(AbsLM):",
            "c = torch.zeros((self.nlayers, self.nhid), dtype=torch.float)",
            "state = h, c",
            "else:",
            "-            state = torch.zeros((nlayers, nhid), dtype=torch.float)",
            "+            state = torch.zeros((self.nlayers, self.nhid), dtype=torch.float)",
            "",
            "return state"
        ]
    },
    {
        "number": 981,
        "comments": "change API feature change",
        "commit_message": "Add TorchScript support for `Node2Vec` (#6726)\n\nThe original jit test of `Node2Vec` model is not really using\nTorchScript, just fix this.\n\n---------\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def test_node2vec():",
            "assert 0 <= acc and acc <= 1",
            "",
            "if is_full_test():",
            "-        jit = torch.jit.export(model)",
            "+        jit = torch.jit.script(model)",
            "",
            "assert jit(torch.arange(3)).size() == (3, 16)"
        ]
    },
    {
        "number": 984,
        "comments": "add condition check for version fix",
        "commit_message": "Convolutional layer supports float64 dtype after tensorflow 1.8.0 (#10977)\n\n* Support float64 dtype after tensorflow 1.8.0\n\n* Fix explanation message.\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def _preprocess_conv3d_input(x, data_format):",
            "# Returns",
            "A tensor.",
            "\"\"\"",
            "-    if dtype(x) == 'float64':",
            "+    # tensorflow doesn't support float64 for conv layer before 1.8.0",
            "+    if (dtype(x) == 'float64'",
            "+            and StrictVersion(tf.__version__) < StrictVersion('1.8.0')):",
            "x = tf.cast(x, 'float32')",
            "tf_data_format = 'NDHWC'",
            "if data_format == 'channels_first':"
        ]
    },
    {
        "number": 985,
        "comments": "format",
        "commit_message": "CI fix3\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class TextEncoder(torch.nn.Module):",
            "",
            "# define modules",
            "self.emb = torch.nn.Embedding(vocabs, attention_dim)",
            "-        torch.nn.init.normal_(self.emb.weight, 0.0, attention_dim**-0.5)",
            "+        torch.nn.init.normal_(self.emb.weight, 0.0, attention_dim ** -0.5)",
            "self.encoder = Encoder(",
            "idim=-1,",
            "input_layer=None,"
        ]
    },
    {
        "number": 986,
        "comments": "format",
        "commit_message": "black fix\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class TextEncoder(torch.nn.Module):",
            "",
            "# define modules",
            "self.emb = torch.nn.Embedding(vocabs, attention_dim)",
            "-        torch.nn.init.normal_(self.emb.weight, 0.0, attention_dim ** -0.5)",
            "+        torch.nn.init.normal_(self.emb.weight, 0.0, attention_dim**-0.5)",
            "self.encoder = Encoder(",
            "idim=-1,",
            "input_layer=None,"
        ]
    },
    {
        "number": 987,
        "comments": "add condition check for version fix",
        "commit_message": "add pat change (#3414)\n\n* add pat change\n\n* fix grid roi head\n\n* fix comments\n\n* clean\n\n* revert change\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class SingleRoIExtractor(BaseRoIExtractor):",
            "num_levels = len(feats)",
            "roi_feats = feats[0].new_zeros(",
            "rois.size(0), self.out_channels, *out_size)",
            "+        # TODO: remove this when parrots supports",
            "+        if torch.__version__ == 'parrots':",
            "+            roi_feats.requires_grad = True",
            "",
            "if num_levels == 1:",
            "if len(rois) == 0:"
        ]
    },
    {
        "number": 988,
        "comments": "add param for resource fix",
        "commit_message": "Dreambooth: reduce VRAM usage (#2039)\n\n* Dreambooth: use `optimizer.zero_grad(set_to_none=True)` to reduce VRAM usage\n\n* Allow the user to control `optimizer.zero_grad(set_to_none=True)` with --set_grads_to_none\n\n* Update Dreambooth readme\n\n* Fix link in readme\n\n* Fix header size in readme\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def main(args):",
            "accelerator.clip_grad_norm_(params_to_clip, args.max_grad_norm)",
            "optimizer.step()",
            "lr_scheduler.step()",
            "-                optimizer.zero_grad()",
            "+                optimizer.zero_grad(set_to_none=args.set_grads_to_none)",
            "",
            "# Checks if the accelerator has performed an optimization step behind the scenes",
            "if accelerator.sync_gradients:"
        ]
    },
    {
        "number": 989,
        "comments": "update API call for refactor fix",
        "commit_message": "Fix dropout by temporarily replacing with nn.Dropout\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class Tagger(nn.Module):",
            "# criterion",
            "self.crit = nn.CrossEntropyLoss(ignore_index=0) # ignore padding",
            "",
            "-        self.drop = Dropout(args['dropout'])",
            "+        self.drop = nn.Dropout(args['dropout'])",
            "self.worddrop = WordDropout(args['word_dropout'])",
            "",
            "def forward(self, word, word_mask, wordchars, wordchars_mask, upos, xpos, ufeats, pretrained, word_orig_idx, sentlens, wordlens):"
        ]
    },
    {
        "number": 990,
        "comments": "feature change",
        "commit_message": "fixed scatter_sum call\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)",
            "def train():",
            "model.train()",
            "optimizer.zero_grad()",
            "-    pos_z, neg_z, summary = model(data.x, data.edge_index, data.edge_attr)",
            "-    loss = model.loss(pos_z, neg_z, summary)",
            "+    y = model(data.x, data.edge_index, data.edge_attr)",
            "+    loss = torch.sum(y) #TODO: actual loss function",
            "loss.backward()",
            "optimizer.step()",
            "return loss.item()"
        ]
    },
    {
        "number": 991,
        "comments": "update API call for refactor fix",
        "commit_message": "Fix torch meshgrid warnings (#20475)\n\n* fix torch meshgrid warnings\n\n* support lower torch versions\n\n* don't edit examples\n\n* dont edit examples\n\n* fix ci\n\n* fix style\n\n* rebase cleanup\n\n* fix ci again\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def build_linear_positions(index_dims, output_range=(-1.0, 1.0)):",
            "return torch.linspace(start=output_range[0], end=output_range[1], steps=n_xels_per_dim, dtype=torch.float32)",
            "",
            "dim_ranges = [_linspace(n_xels_per_dim) for n_xels_per_dim in index_dims]",
            "-    array_index_grid = torch.meshgrid(*dim_ranges)",
            "+    array_index_grid = meshgrid(*dim_ranges, indexing=\"ij\")",
            "",
            "return torch.stack(array_index_grid, dim=-1)"
        ]
    },
    {
        "number": 994,
        "comments": "remove print",
        "commit_message": "fix pin_memory with different latent sampling method\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def train_embedding(embedding_name, learn_rate, batch_size, gradient_step, data_",
            "# go back until we reach gradient accumulation steps",
            "if (j + 1) % gradient_step != 0:",
            "continue",
            "-                #print(f\"grad:{embedding.vec.grad.detach().cpu().abs().mean().item():.7f}\")",
            "-                #scaler.unscale_(optimizer)",
            "-                #print(f\"grad:{embedding.vec.grad.detach().cpu().abs().mean().item():.7f}\")",
            "-                #torch.nn.utils.clip_grad_norm_(embedding.vec, max_norm=1.0)",
            "-                #print(f\"grad:{embedding.vec.grad.detach().cpu().abs().mean().item():.7f}\")",
            "scaler.step(optimizer)",
            "scaler.update()",
            "embedding.step += 1"
        ]
    },
    {
        "number": 995,
        "comments": "version fix",
        "commit_message": "implement regularization (#347)\n\n* make regularization happen\n\n* have model.from_params call initialize\n\n* better defaults\n\n* fix docs\n\n* add test for regularization\n\n* remove initializer from model subclass constructors\n\n* fix comment\n\n* add missing test fixture\n\n* address PR feedback\n\n* add regularization tests\n\n* use InitializerApplicator() instead of None\n\n* update docstrings\n\n* update doscstrings more\n\n* change variable return type to float\n\n* fix docstring that was breaking sphinx\n\n* remove regularizer from simple_tagger\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class Model(torch.nn.Module, Registrable):",
            "@classmethod",
            "def from_params(cls, vocab: Vocabulary, params: Params) -> 'Model':",
            "choice = params.pop_choice(\"type\", cls.list_available())",
            "-        return cls.by_name(choice).from_params(vocab, params)",
            "+        model = cls.by_name(choice).from_params(vocab, params)",
            "+        return model",
            "",
            "@classmethod",
            "def load(cls,"
        ]
    },
    {
        "number": 999,
        "comments": "format",
        "commit_message": "Refactor library namespaces [pre-release][0.6-rc1] (#1412)\n\n* flake fixes\n\n* initial flake8 fixeS\n\n* remove top level from kornia.color\n\n* kornia filters\n\n* kornia losses\n\n* kornia features\n\n* geomtry and all ok\n\n* removed jit module\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* apply formatting and few fixes\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add keep block for isort\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* skip init\n\n* fix the docs\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* remove old code\n\n* simplify ci workflow\n\n* fix circular dependency\n\n* few format fixes\n\n* fix code format test\n\n* remove kornia.jit imports\n\n* final fixes\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* ci fixes\n\n* add versioneer\n\n* fix pnp import\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* exclude version files from pre-commit\n\n* exclude files precommit\n\n* update to pytorch 1.10 and add fixes\n\n* Update tests_cpu.yml\n\n* Update setup_dev_env.sh\n\n* Update tests_cpu.yml\n\n* undo versioneer\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix no_grad\n\n* Apply suggestions from code review\n\n* fix skip windows tests\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Update test_integrated.py\n\n* Apply suggestions from code review\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class TestInverseWithMask:",
            "assert_close(y, y_expected)",
            "assert torch.equal(mask, torch.ones_like(mask))",
            "",
            "-    @pytest.mark.skipif((int(torch.__version__.split('.')[0]) == 1) and (int(torch.__version__.split('.')[1]) < 9),",
            "-                        reason='<1.9.0 not supporting')",
            "+    @pytest.mark.skipif(",
            "+        (int(torch.__version__.split('.')[0]) == 1) and (int(torch.__version__.split('.')[1]) < 9),",
            "+        reason='<1.9.0 not supporting',",
            "+    )",
            "def test_all_bad(self, device, dtype):",
            "A = torch.ones(10, 3, 3, device=device, dtype=dtype)",
            "X, mask = safe_inverse_with_mask(A)"
        ]
    },
    {
        "number": 1000,
        "comments": "rename",
        "commit_message": "Readability and documentation (#67)\n\n* Readability and documentation\n\n* Add unit tests and other fixes\n\n* Typo\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def batch_to_time(value, dilation, name=None):",
            "return tf.reshape(transposed, [tf.div(shape[0], dilation), -1, shape[2]])",
            "",
            "",
            "-def causal_conv(value, filter_, dilation, name=None):",
            "-    with tf.name_scope('causal_conv'):",
            "+def causal_conv(value, filter_, dilation, name='causal_conv'):",
            "+    with tf.name_scope(name):",
            "# Pad beforehand to preserve causality",
            "filter_width = tf.shape(filter_)[0]",
            "padded = tf.pad(value, [[0, 0], [(filter_width - 1) * dilation, 0], [0, 0]])"
        ]
    },
    {
        "number": 1001,
        "comments": "string fix",
        "commit_message": "Use f-strings in the dataset scripts (#3291)\n\n* Finishes #3257\n\nUsed f-strings to format the .py files in the dataset folder\n\n* Fix style\n\n* Fix hkcancor dataset\n\nCo-authored-by: Mario \u0160a\u0161ko <mario@huggingface.co>\nCo-authored-by: mariosasko <mariosasko777@gmail.com>\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class Matinf(datasets.GeneratorBasedBuilder):",
            "",
            "if not os.path.exists(data_dir):",
            "raise FileNotFoundError(",
            "-                \"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('matinf', data_dir=...)` that includes files unzipped from the MATINF zip. Manual download instructions: {}\".format(",
            "-                    data_dir, self.manual_download_instructions",
            "-                )",
            "+                f\"{data_dir} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('matinf', data_dir=...)` that includes files unzipped from the MATINF zip. Manual download instructions: {self.manual_download_instructions}\"",
            ")",
            "return [",
            "datasets.SplitGenerator("
        ]
    },
    {
        "number": 1003,
        "comments": "rename",
        "commit_message": "Quick fix\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def SubpixelConv2d(net, scale=2, n_out_channel=None, act=tf.identity, name='subp",
            "bsize = tf.shape(X)[0] # Handling Dimension(None) type for undefined batch dim",
            "Xs=tf.split(X,r,3) #b*h*w*r*r",
            "Xr=tf.concat(Xs,2) #b*h*(r*w)*r",
            "-            X=tf.reshape(Xr,(b,r*a,r*b,c)) # b*(r*h)*(r*w)*c",
            "+            X=tf.reshape(Xr,(bsize,r*a,r*b,c)) # b*(r*h)*(r*w)*c",
            "else:",
            "print(_err_log)",
            "return X"
        ]
    },
    {
        "number": 1006,
        "comments": "no API",
        "commit_message": "Implicit out (#2158)\n\n* add missing Nones\n\n* fix linting in backend tf set\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def unique_inverse(",
            "",
            "",
            "def unique_values(",
            "-    x: Union[tf.Tensor, tf.Variable], *, out: Optional[Union[tf.Tensor, tf.Variable]]",
            "+    x: Union[tf.Tensor, tf.Variable],",
            "+    *,",
            "+    out: Optional[Union[tf.Tensor, tf.Variable]] = None",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "ret = tf.unique(tf.reshape(x, [-1]))[0]",
            "return ret"
        ]
    },
    {
        "number": 1007,
        "comments": "update API call for refactor fix",
        "commit_message": "use functional interface for softmax in attention (#14198)\n\n* use functional interface instead of instantiating module and immediately calling it\n\n* fix torch.nn.functional to nn.functional. Thank you Stas!\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class GPT2Attention(nn.Module):",
            "# Apply the attention mask",
            "attn_weights = attn_weights + attention_mask",
            "",
            "-        attn_weights = nn.Softmax(dim=-1)(attn_weights)",
            "+        attn_weights = nn.functional.softmax(attn_weights, dim=-1)",
            "",
            "# Downcast (if necessary) back to V's dtype (if in mixed-precision) -- No-Op if otherwise",
            "if attn_weights.dtype != torch.float32:"
        ]
    },
    {
        "number": 1008,
        "comments": "add param for type fix",
        "commit_message": "upgrade to pytorch 0.4.0 (#1126)\n\n* bump pytorch to 0.4 + fix sanitize\n\n* remove check for Variable in block_orthogonal\n\n* rename parameter split_size=\n\n* remove checks for Variable\n\n* fix more tests\n\n* fixes\n\n* get tests to pass\n\n* fix warnings\n\n* get rid of some of the Variables\n\n* more tests passing\n\n* more elimination of variables\n\n* finish removing all Variables\n\n* pylint and such\n\n* a few fixes\n\n* move torch.no_grad into model.forward_on_instances\n\n* more pytorch 0.4 changes\n\n* detach() -> data\n\n* pylint\n\n* fix bad tensor creation\n\n* fix types\n\n* remove print statement\n\n* more 0.4 goodness\n\n* factor out is_tensor\n\n* add no_grad to elmo command\n\n* cleanup\n\n* remove TODO\n\n* address PR feedback\n\n* replace all() with item()\n\n* more cleanup\n\n* further cleanup\n\n* remove Variable\n\n* really fix merge conflict\n\n* fix pylint\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class BidirectionalEndpointSpanExtractor(SpanExtractor):",
            "sequence_lengths = util.get_lengths_from_binary_sequence_mask(sequence_mask)",
            "else:",
            "# shape (batch_size), filled with the sequence length size of the sequence_tensor.",
            "-            sequence_lengths = util.ones_like(sequence_tensor[:, 0, 0]).long() * sequence_tensor.size(1)",
            "+            sequence_lengths = (torch.ones_like(sequence_tensor[:, 0, 0], dtype=torch.long) *",
            "+                                sequence_tensor.size(1))",
            "",
            "# shape (batch_size, num_spans, 1)",
            "end_sentinel_mask = (exclusive_span_ends == sequence_lengths.unsqueeze(-1)).long().unsqueeze(-1)"
        ]
    },
    {
        "number": 1009,
        "comments": "value change",
        "commit_message": "Fixed issue with build_proto and isort\n\n- Added torchvision>=0.5 to requirements\n- Removed unused imports in .proto files\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "_SHARETENSOR = _descriptor.Descriptor(",
            "syntax=\"proto3\",",
            "extension_ranges=[],",
            "oneofs=[],",
            "-    serialized_start=154,",
            "-    serialized_end=257,",
            "+    serialized_start=115,",
            "+    serialized_end=218,",
            ")",
            "",
            "_SHARETENSOR.fields_by_name["
        ]
    },
    {
        "number": 1010,
        "comments": "doc",
        "commit_message": "Fix layer combination bug\n\nThe raw outputs of each layer need to be combined at the end, not the\nskipped inputs for the next layer.\nThanks to @keskival for pointing this out!\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class WaveNet(object):",
            "tf.histogram_summary('postprocess2_weights', w2)",
            "",
            "# We skip connections from the outputs of each layer, adding them all up here",
            "-            # We perform pairwise addition instead of using tf.add_n, so TensorFlow can free",
            "-            # the memory of previous layers",
            "total = outputs[0]",
            "for out in outputs[1:]:",
            "total += out"
        ]
    },
    {
        "number": 1013,
        "comments": "update API call for refactor fix",
        "commit_message": "[RLlib] Tf2x preparation; part 2 (upgrading `try_import_tf()`). (#9136)\n\n* WIP.\n\n* Fixes.\n\n* LINT.\n\n* WIP.\n\n* WIP.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* WIP.\n\n* Fixes.\n\n* Test\n\n* Fix.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* LINT.\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class ARSTFPolicy:",
            "self.num_params = sum(",
            "np.prod(variable.shape.as_list())",
            "for _, variable in self.variables.variables.items())",
            "-        self.sess.run(tf.global_variables_initializer())",
            "+        self.sess.run(tf1.global_variables_initializer())",
            "",
            "def compute_actions(self,",
            "observation,"
        ]
    },
    {
        "number": 1014,
        "comments": "format",
        "commit_message": "remove numpy import (#1116)\n\n* remove numpy import\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* undo and add setup develop in gh actions\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* remove create_checkerboard\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class TestOpening:",
            "None, None, :, :",
            "]",
            "assert_allclose(",
            "-            opening(tensor, torch.ones_like(structural_element), structuring_element=structural_element), expected,",
            "-            atol=1e-4, rtol=1e-4",
            "+            opening(tensor, torch.ones_like(structural_element), structuring_element=structural_element),",
            "+            expected,",
            "+            atol=1e-4,",
            "+            rtol=1e-4,",
            ")",
            "",
            "def test_exception(self, device, dtype):"
        ]
    },
    {
        "number": 1015,
        "comments": "no API",
        "commit_message": "Added type hints for Pytorch Marian calls (#16200)\n\n* Added type hinting for forward functions in pytorch marian\n\n* typo correction\n\n* Removed type hints on functions from BART per Suraj Patil request\n\n* fix import pb\n\n* fix typo\n\n* corrected tuple call\n\n* ran black\n\n* after fix-copies\nSome optional tags on primitives were removed, past_key_values in MarianForCausalLM changed from Tuple of Tuple to List\n\n* Fixing copies to roformer and pegasus\n\nCo-authored-by: Clementine Fourrier <cfourrie@inria.fr>\nCo-authored-by: matt <rocketknight1@gmail.com>\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class RoFormerSinusoidalPositionalEmbedding(nn.Embedding):",
            "return out",
            "",
            "@torch.no_grad()",
            "-    def forward(self, input_ids_shape: torch.Size, past_key_values_length: int = 0):",
            "+    def forward(self, input_ids_shape: torch.Size, past_key_values_length: int = 0) -> torch.Tensor:",
            "\"\"\"`input_ids_shape` is expected to be [bsz x seqlen].\"\"\"",
            "bsz, seq_len = input_ids_shape[:2]",
            "positions = torch.arange("
        ]
    },
    {
        "number": 1016,
        "comments": "change API call for resource fix",
        "commit_message": "GH-464: fix text generation on cuda:1\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class LanguageModel(nn.Module):",
            "",
            "for i in range(number_of_characters):",
            "",
            "-                if torch.cuda.is_available():",
            "-                    input = input.cuda()",
            "+                input = input.to(flair.device)",
            "",
            "# get predicted weights",
            "prediction, _, hidden = self.forward(input, hidden)"
        ]
    },
    {
        "number": 1017,
        "comments": "format",
        "commit_message": "Fix optional typehint (continued) (#11923)\n\nCo-authored-by: @AnnaTz\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def instance_norm(",
            "",
            "",
            "def lp_normalize(",
            "-    x: Union[tf.Tensor, tf.Variable], /, *, p: float = 2, axis: int = None, out=None",
            "+    x: Union[tf.Tensor, tf.Variable],",
            "+    /,",
            "+    *,",
            "+    p: float = 2,",
            "+    axis: Optional[int] = None,",
            "+    out: Optional[tf.Tensor] = None,",
            ") -> tf.Tensor:",
            "denorm = tf.norm(x, ord=p, axis=axis, keepdims=True)",
            "denorm = tf.math.maximum(denorm, 1e-12)"
        ]
    },
    {
        "number": 1020,
        "comments": "doc",
        "commit_message": "Add dilated conv support (#9347)\n\n* added dilate conv support\n\n* added dilate conv support\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Update common.py\n\n* Update common.py\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class Conv(nn.Module):",
            "",
            "",
            "class DWConv(Conv):",
            "-    # Depth-wise convolution class",
            "+    # Depth-wise convolution",
            "def __init__(self, c1, c2, k=1, s=1, act=True):  # ch_in, ch_out, kernel, stride, padding, groups",
            "super().__init__(c1, c2, k, s, g=math.gcd(c1, c2), act=act)",
            "",
            "",
            "class DWConvTranspose2d(nn.ConvTranspose2d):",
            "-    # Depth-wise transpose convolution class",
            "+    # Depth-wise transpose convolution",
            "def __init__(self, c1, c2, k=1, s=1, p1=0, p2=0):  # ch_in, ch_out, kernel, stride, padding, padding_out",
            "super().__init__(c1, c2, k, s, p1, p2, groups=math.gcd(c1, c2))"
        ]
    },
    {
        "number": 1021,
        "comments": "format",
        "commit_message": "Fix mnist_synthetic_dataset\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def reduce_per_replica(values, strategy, reduction):",
            "else:",
            "return concat(strategy.experimental_local_results(v))",
            "elif reduction == \"sum\":",
            "-            values = strategy.experimental_local_results(v)",
            "-            return tf.reduce_sum(values)",
            "+            return tf.reduce_sum(strategy.experimental_local_results(v))",
            "else:",
            "raise ValueError(",
            "'`reduction` must be \"first\", \"concat\", \"sum\", or \"auto\". '"
        ]
    },
    {
        "number": 1022,
        "comments": "custom API",
        "commit_message": "[CI] Fix ci  (#21940)\n\n* fix `get_proposal_pos_embed`\n\n* fix order\n\n* style\n\n* zero shot simplify test\n\n* add approximate values for zero shot audio classification\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class DeformableDetrModel(DeformableDetrPreTrainedModel):",
            "scale = 2 * math.pi",
            "",
            "dim_t = torch.arange(num_pos_feats, dtype=torch.float32, device=proposals.device)",
            "-        dim_t = temperature ** (2 * torch.div(dim_t, 2) / num_pos_feats)",
            "+        dim_t = temperature ** (2 * torch_int_div(dim_t, 2) / num_pos_feats)",
            "# batch_size, num_queries, 4",
            "proposals = proposals.sigmoid() * scale",
            "# batch_size, num_queries, 4, 128"
        ]
    },
    {
        "number": 1024,
        "comments": "add API call for shape fix",
        "commit_message": "[RLlib] Fix KL method of MultiCategorial tf distribution (issue #7009). (#7119)\n\n* Fix KL method of MultiCategorial tf distribution.\n\n* Fix KL method of MultiCategorial tf distribution.\n\n* Merge AsyncReplayOptimizer fixes into this branch.\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class MultiCategorical(TFActionDistribution):",
            "",
            "@override(ActionDistribution)",
            "def multi_kl(self, other):",
            "-        return [cat.kl(oth_cat) for cat, oth_cat in zip(self.cats, other.cats)]",
            "+        return tf.stack(",
            "+            [cat.kl(oth_cat) for cat, oth_cat in zip(self.cats, other.cats)],",
            "+            axis=1)",
            "",
            "@override(ActionDistribution)",
            "def kl(self, other):"
        ]
    },
    {
        "number": 1025,
        "comments": "def",
        "commit_message": "Adapt to tf-nightly (#1634)\n\n* fixed tests in keras_layers_test\n\n* fixed image classifier test\n\n* fix bert tokenizer\n\n* multi branch arch adapt preprocessing layers\n\n* depending on tf-nightly\n\n* coverage\n\nCo-authored-by: Haifeng Jin <haifeng-jin@users.noreply.github.com>\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class SelfAttentionMask(tf.keras.layers.Layer):",
            "",
            "return mask  # pragma: no cover",
            "",
            "+    def get_config(self):",
            "+        return super().get_config()",
            "+",
            "",
            "@tf.keras.utils.register_keras_serializable()",
            "class Transformer(tf.keras.layers.Layer):"
        ]
    },
    {
        "number": 1027,
        "comments": "change condition check for type fix",
        "commit_message": "DataParallel fixes (#5733)\n\n* DataParallel fixes:\n\n1. switched to a more precise check\n-        if self.args.n_gpu > 1:\n+        if isinstance(model, nn.DataParallel):\n\n2. fix tests - require the same fixup under DataParallel as the training module\n\n* another fix\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class Trainer:",
            "if self.args.past_index >= 0:",
            "inputs[\"mems\"] = past",
            "# Our model outputs do not work with DataParallel, so forcing return tuple.",
            "-            if self.args.n_gpu > 1:",
            "+            if isinstance(model, nn.DataParallel):",
            "inputs[\"return_tuple\"] = True",
            "",
            "with torch.no_grad():"
        ]
    },
    {
        "number": 1028,
        "comments": "change param for math fix",
        "commit_message": "Dev pylint (#1697)\n\nFix pylint errors\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class PolicyWithValue:",
            "def sample(logits, mask_npinf):",
            "new_logits = tf.math.add(logits, mask_npinf)",
            "u = tf.random_uniform(tf.shape(new_logits), dtype=logits.dtype)",
            "-            return tf.argmax(new_logits - tf.log(-tf.log(u)), axis=-1)",
            "+            return tf.argmax(new_logits - tf.log(-1*tf.log(u)), axis=-1)",
            "",
            "def neglogp(logits, x):",
            "# return tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=x)"
        ]
    },
    {
        "number": 1029,
        "comments": "value change",
        "commit_message": "[X-CLIP] Fix doc tests (#19523)\n\n* Fix XCLIP doc tests\n\n* Add model to doc test list\n\n* Fix tests\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class XCLIPModelIntegrationTest(unittest.TestCase):",
            "torch.Size((inputs.input_ids.shape[0], inputs.pixel_values.shape[0])),",
            ")",
            "",
            "-        expected_logits = torch.tensor([[14.3819, 20.6031, 15.0526]], device=torch_device)",
            "+        expected_logits = torch.tensor([[14.0181, 20.2771, 14.4776]], device=torch_device)",
            "",
            "self.assertTrue(torch.allclose(outputs.logits_per_video, expected_logits, atol=1e-3))"
        ]
    },
    {
        "number": 1030,
        "comments": "add param for resource fix",
        "commit_message": "fix deprecations about casting & initializers in tf1.13\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def Conv2DTranspose(",
            "if get_tf_version_tuple() <= (1, 12):",
            "kernel_initializer = tf.contrib.layers.variance_scaling_initializer(2.0),",
            "else:",
            "-            kernel_initializer = tf.keras.initializers.VarianceScaling(2.0)",
            "+            kernel_initializer = tf.keras.initializers.VarianceScaling(2.0, distribution='untruncated_normal')",
            "",
            "with rename_get_variable({'kernel': 'W', 'bias': 'b'}):",
            "layer = tf.layers.Conv2DTranspose("
        ]
    },
    {
        "number": 1031,
        "comments": "format",
        "commit_message": "enable black in the precommit (#1777)\n\n* enable black in the precommit\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add some fixes\n\n* update libface detection url\n\n* added url from kornia checkpoint\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def binary_focal_loss_with_logits(",
            "",
            "probs_pos = torch.sigmoid(input)",
            "probs_neg = torch.sigmoid(-input)",
            "-    loss_tmp = -alpha * torch.pow(probs_neg, gamma) * target * F.logsigmoid(input) - (",
            "-        1 - alpha",
            "-    ) * torch.pow(probs_pos, gamma) * (1.0 - target) * F.logsigmoid(-input)",
            "+    loss_tmp = -alpha * torch.pow(probs_neg, gamma) * target * F.logsigmoid(input) - (1 - alpha) * torch.pow(",
            "+        probs_pos, gamma",
            "+    ) * (1.0 - target) * F.logsigmoid(-input)",
            "",
            "if reduction == 'none':",
            "loss = loss_tmp"
        ]
    },
    {
        "number": 1033,
        "comments": "remove API call for type fix",
        "commit_message": "data augmentation pipeline\n\nSummary:\nSeq2Seq Data augmentation pipeline based on prefix with the following workflow\n\n{F207337357}\n\nThe pipeline takes about 5 hours to process music domain, the music domain output is about 100k augmented data.\n\nExample:\nf136633622\n\nfix 2 bugs:\n\n* sequence_generator max_len assertion\n* lengths in interactive.py should be python list instead of tensor\n\nReviewed By: myleott\n\nDifferential Revision: D17138089\n\nfbshipit-source-id: eaeeadd5ba81e02930a45f8873069137469925b6\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def make_batches(lines, args, task, max_positions, encode_fn):",
            ").long()",
            "for src_str in lines",
            "]",
            "-    lengths = torch.LongTensor([t.numel() for t in tokens])",
            "+    lengths = [t.numel() for t in tokens]",
            "itr = task.get_batch_iterator(",
            "dataset=task.build_dataset_for_inference(tokens, lengths),",
            "max_tokens=args.max_tokens,"
        ]
    },
    {
        "number": 1034,
        "comments": "add param for math fix",
        "commit_message": "bugfix/3185 transpose (#3252)\n\n* change t() to transpose() as xla devices do not support .t() on 1-dim tensor\n\n* detach tensor before copying\n\n* Revert \"detach tensor before copying\"\n\nThis reverts commit 37cc7bbe\n\n* changed dims\n\n* added test_result_obj_on_tpu\n\n* detach before copying\n\n* detach before copying\n\n* detach before copying\n\n* replace torch.cat with sum\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class EarlyStopping(Callback):",
            "",
            "if trainer.use_tpu:",
            "stop = torch.tensor(int(trainer.should_stop), device=pl_module.device, dtype=torch.int32)",
            "-            stop = xm.mesh_reduce(\"stop_signal\", stop, torch.cat)",
            "+            stop = xm.mesh_reduce(\"stop_signal\", stop, sum)",
            "torch_xla.core.xla_model.rendezvous(\"pl.EarlyStoppingCallback.stop_distributed_training_check\")",
            "trainer.should_stop = int(stop.item()) == trainer.world_size"
        ]
    },
    {
        "number": 1035,
        "comments": "change API call for resource fix",
        "commit_message": "Make most metrics work on GPU (#3851)\n\n* Make most metrics work on GPU\n\n* Make metric tests work on both GPU and CPU\n\n* Add a test for the test utility\n\n* mypy\n\n* Update allennlp/common/testing/test_case.py\n\nCo-Authored-By: Mark Neumann <markn@allenai.org>\n\n* Fix a PR comment\n\n* flake8\n\nCo-authored-by: Mark Neumann <markn@allenai.org>\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class Entropy(Metric):",
            "mask : `torch.Tensor`, optional (default = None).",
            "A masking tensor of shape (batch_size, ...).",
            "\"\"\"",
            "-        logits, mask = self.unwrap_to_tensors(logits, mask)",
            "+        logits, mask = self.detach_tensors(logits, mask)",
            "",
            "if mask is None:",
            "-            mask = torch.ones(logits.size()[:-1])",
            "+            mask = torch.ones(logits.size()[:-1], device=logits.device)",
            "",
            "log_probs = torch.nn.functional.log_softmax(logits, dim=-1)",
            "probabilities = torch.exp(log_probs) * mask.unsqueeze(-1)"
        ]
    },
    {
        "number": 1036,
        "comments": "doc fix",
        "commit_message": "Fixed run resuming summary, deprecated warn -> warning, better keras error message\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "from itertools import chain",
            "if \"keras\" in sys.modules:",
            "if \"tensorflow.python.keras\" in sys.modules:",
            "wandb.termlog(",
            "-            \"WARNING: found both keras and tensorflow.python.keras. Use `from tensorflow import keras` and remove `import keras` to use the latest W&B features.\")",
            "+            \"Found keras and tensorflow.keras. WandbCallback will be configured for keras not tensorflow.keras.\")",
            "import keras",
            "import keras.backend as K",
            "elif \"tensorflow.python.keras\" in sys.modules:"
        ]
    },
    {
        "number": 1039,
        "comments": "add API call for type fix",
        "commit_message": "double down on dual patch norm, fix MAE and Simmim to be compatible with dual patchnorm\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class ImageEmbedder(nn.Module):",
            "",
            "self.to_patch_embedding = nn.Sequential(",
            "Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_size, p2 = patch_size),",
            "+            nn.LayerNorm(patch_dim),",
            "nn.Linear(patch_dim, dim),",
            "+            nn.LayerNorm(dim)",
            ")",
            "",
            "self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))"
        ]
    },
    {
        "number": 1043,
        "comments": "change param for refactor fix",
        "commit_message": "Fixed Style Inconsistency (#3976)\n\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class BertForSequenceClassification(BertPreTrainedModel):",
            "",
            "self.bert = BertModel(config)",
            "self.dropout = nn.Dropout(config.hidden_dropout_prob)",
            "-        self.classifier = nn.Linear(config.hidden_size, self.config.num_labels)",
            "+        self.classifier = nn.Linear(config.hidden_size, config.num_labels)",
            "",
            "self.init_weights()"
        ]
    },
    {
        "number": 1044,
        "comments": "add param for type fix",
        "commit_message": "fix tensorflow's fmod\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def fmod(",
            "out: Optional[Union[tf.Tensor, tf.Variable]] = None,",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "result = tf.math.floormod(x1, x2, name=None)",
            "-    temp = (result, x1)",
            "-    return tf.map_fn(lambda x: x[0] if (x[0] * x[1] >= 0) else (-1 * x[0]), temp)",
            "+    temp = [result, x1]",
            "+    return tf.map_fn(lambda x: x[0] if (x[0] * x[1] >= 0) else (-1 * x[0]), temp, fn_output_signature=result.dtype)",
            "",
            "",
            "def fmax("
        ]
    },
    {
        "number": 1046,
        "comments": "change condition check for type fix",
        "commit_message": "[`T5`] Fix torchquant issue (#21843)\n\n* fix torchquant issue\n\n* add tests\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class MT5DenseGatedActDense(nn.Module):",
            "# To make 8bit quantization work for google/flan-t5-xxl, self.wo is kept in float32.",
            "# See https://github.com/huggingface/transformers/issues/20287",
            "# we also make sure the weights are not in `int8` in case users will force `_keep_in_fp32_modules` to be `None``",
            "-        if hidden_states.dtype != self.wo.weight.dtype and self.wo.weight.dtype != torch.int8:",
            "+        if (",
            "+            isinstance(self.wo.weight, torch.Tensor)",
            "+            and hidden_states.dtype != self.wo.weight.dtype",
            "+            and self.wo.weight.dtype != torch.int8",
            "+        ):",
            "hidden_states = hidden_states.to(self.wo.weight.dtype)",
            "",
            "hidden_states = self.wo(hidden_states)"
        ]
    },
    {
        "number": 1047,
        "comments": "use custom api",
        "commit_message": "Add fp16 support (#963)\n\n* Update imgwarp.py\n\n* update fix\n\n* move _torch_inverse_cast under utils\n\n* fix some tests in pyramid\n\n* fix issues\n\n* implement other autocasting functions\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def _scale_channel(im: torch.Tensor) -> torch.Tensor:",
            "",
            "im = im * 255",
            "# Compute the histogram of the image channel.",
            "-    histo = torch.histc(im, bins=256, min=0, max=255)",
            "+    histo = _torch_histc_cast(im, bins=256, min=0, max=255)",
            "# For the purposes of computing the step, filter out the nonzeros.",
            "nonzero_histo = torch.reshape(histo[histo != 0], [-1])",
            "step = (torch.sum(nonzero_histo) - nonzero_histo[-1]) // 255"
        ]
    },
    {
        "number": 1048,
        "comments": "add param for resource fix",
        "commit_message": "small fixes regarding device placement.\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def vector_to_skew_symmetric_matrix(vector):",
            "a2s = vector_expanded[..., 1:2, :]",
            "a3s = vector_expanded[..., 2:3, :]",
            "# BS x 1 x 1",
            "-    zs = torch.zeros(batch_shape + [1, 1])",
            "+    zs = torch.zeros(batch_shape + [1, 1], device=vector.device)",
            "# BS x 1 x 3",
            "row1 = torch.cat((zs, -a3s, a2s), -1)",
            "row2 = torch.cat((a3s, zs, -a1s), -1)"
        ]
    },
    {
        "number": 1049,
        "comments": "add comment",
        "commit_message": "Fix misstyping\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class YOLOLayer(nn.Module):",
            "w = prediction[..., 2]  # Width",
            "h = prediction[..., 3]  # Height",
            "pred_conf = torch.sigmoid(prediction[..., 4])  # Conf",
            "-        pred_cls = torch.sigmoid(prediction[..., 5:]        ByteTensor = torch.cuda.ByteTensor if x.is_cuda else torch.ByteTensor)  # Cls pred.",
            "+        pred_cls = torch.sigmoid(prediction[..., 5:])  # Cls pred.",
            "",
            "# If grid size does not match current we compute new offsets",
            "if grid_size != self.grid_size:"
        ]
    },
    {
        "number": 1050,
        "comments": "add param for type fix",
        "commit_message": "fix torch.nonzero usage in pytorch 1.5 (#2602)\n\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class PointAssigner(BaseAssigner):",
            "",
            "if gt_labels is not None:",
            "assigned_labels = assigned_gt_inds.new_full((num_points, ), -1)",
            "-            pos_inds = torch.nonzero(assigned_gt_inds > 0).squeeze()",
            "+            pos_inds = torch.nonzero(",
            "+                assigned_gt_inds > 0, as_tuple=False).squeeze()",
            "if pos_inds.numel() > 0:",
            "assigned_labels[pos_inds] = gt_labels[",
            "assigned_gt_inds[pos_inds] - 1]"
        ]
    },
    {
        "number": 1052,
        "comments": "change param for shape fix",
        "commit_message": "fixed att_to_numpy() function for AttCov, AttCovLoc\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def att_to_numpy(att_ws, att):",
            "att_ws = torch.stack([aw[:, -1] for aw in att_ws], dim=1).cpu().numpy()",
            "elif isinstance(att, (AttCov, AttCovLoc)):",
            "# att_ws => list of list of previous attentions",
            "-        att_ws = torch.stack([aw[-1] for aw in att_ws], dim=1).cpu().numpy()",
            "+        att_ws = torch.stack([aw[idx] for idx, aw in enumerate(att_ws)], dim=1).cpu().numpy()",
            "elif isinstance(att, AttLocRec):",
            "# att_ws => list of tuple of attention and hidden states",
            "att_ws = torch.stack([aw[0] for aw in att_ws], dim=1).cpu().numpy()"
        ]
    },
    {
        "number": 1053,
        "comments": "refactor fix",
        "commit_message": "rnn->encoders, removes _th suffix, same filenames for chainer and pytorch, modify tests to not rely on _th\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def main():",
            "# train",
            "logging.info('backend = ' + args.backend)",
            "if args.backend == \"chainer\":",
            "-        from espnet.asr.chain.asr_chainer import train",
            "+        from espnet.asr.chain.asr import train",
            "train(args)",
            "elif args.backend == \"pytorch\":",
            "-        from espnet.asr.pytorch.asr_pytorch import train",
            "+        from espnet.asr.pytorch.asr import train",
            "train(args)",
            "else:",
            "raise ValueError(\"Only chainer and pytorch are supported.\")"
        ]
    },
    {
        "number": 1054,
        "comments": "change param for type fix",
        "commit_message": "Fix default dtype in HJM, HullWhite, and Heston model, as well as in PiecewiseConstant class.\n\nPiperOrigin-RevId: 387639335\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class HestonModel(generic_ito_process.GenericItoProcess):",
            "drift = tf.stack([log_spot_drift, var_drift], -1)",
            "return drift",
            "",
            "-    super(HestonModel, self).__init__(2, _drift_fn, _vol_fn, dtype, name)",
            "+    super(HestonModel, self).__init__(2, _drift_fn, _vol_fn, self._dtype, name)",
            "",
            "def sample_paths(self,",
            "times: types.RealTensor,"
        ]
    },
    {
        "number": 1055,
        "comments": "value change",
        "commit_message": "Test fixes for compatibility with PyTorch master (#1416)\n\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def test_gaussian_mixture_model():",
            "cluster_assignments = dist.Categorical(true_mix_proportions).sample(torch.Size((N,)))",
            "data = dist.Normal(true_cluster_means[cluster_assignments], 1.0).sample()",
            "hmc_kernel = HMC(gmm, trajectory_length=1, adapt_step_size=True, max_iarange_nesting=1)",
            "-    mcmc_run = MCMC(hmc_kernel, num_samples=600, warmup_steps=200).run(data)",
            "+    mcmc_run = MCMC(hmc_kernel, num_samples=300, warmup_steps=100).run(data)",
            "posterior = EmpiricalMarginal(mcmc_run, sites=[\"phi\", \"cluster_means\"]).mean.sort()[0]",
            "assert_equal(posterior[0], true_mix_proportions, prec=0.05)",
            "assert_equal(posterior[1], true_cluster_means, prec=0.2)"
        ]
    },
    {
        "number": 1056,
        "comments": "add API call for type fix",
        "commit_message": "Fix the evaluate() method in the SimilarityLearner class\n\nThe evaluate method does not have a similar function signature as other models, which is causing errors when training it.\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class SimilarityLearner(flair.nn.Model):",
            "epoch_results_str,",
            "detailed_results,",
            "),",
            "-            0,",
            "+            torch.tensor(0),",
            ")",
            "",
            "def _get_state_dict(self):"
        ]
    },
    {
        "number": 1059,
        "comments": "add condition check for null fix",
        "commit_message": "Add final_layer_norm to OPT model (#17785)\n\n* Add final_layer_norm to OPT model\n\n* Add JAX and TF version\n\n* Fix Keras name\n\n* Woops\n\n* Allow for non breaking change\n\n* Apply suggestions from code review\n\n* add tests\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class TFOPTDecoder(tf.keras.layers.Layer):",
            "if output_attentions:",
            "all_self_attns += (layer_self_attn,)",
            "",
            "+        if self.final_layer_norm is not None:",
            "+            hidden_states = self.final_layer_norm(hidden_states)",
            "+",
            "if self.project_out is not None:",
            "hidden_states = self.project_out(hidden_states)"
        ]
    },
    {
        "number": 1060,
        "comments": "change API call for type fix",
        "commit_message": "Fixing python2 logging for extract_features.py\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def convert_examples_to_features(examples, seq_length, tokenizer):",
            "if ex_index < 5:",
            "tf.logging.info(\"*** Example ***\")",
            "tf.logging.info(\"unique_id: %s\" % (example.unique_id))",
            "-      tf.logging.info(\"tokens: %s\" % \" \".join([str(x) for x in tokens]))",
            "+      tf.logging.info(\"tokens: %s\" % \" \".join(",
            "+          [tokenization.printable_text(x) for x in tokens]))",
            "tf.logging.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))",
            "tf.logging.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))",
            "tf.logging.info("
        ]
    },
    {
        "number": 1062,
        "comments": "doc",
        "commit_message": "CDN urls (#4030)\n\n* [file_utils] use_cdn + documentation\n\n* Move to cdn. urls for weights\n\n* [urls] Hotfix for bert-base-japanese\n",
        "label": "no",
        "answer": "no",
        "change": [
            "logger = logging.getLogger(__name__)",
            "# for the pretrained weights provided with the models",
            "####################################################",
            "T5_PRETRAINED_MODEL_ARCHIVE_MAP = {",
            "-    \"t5-small\": \"https://s3.amazonaws.com/models.huggingface.co/bert/t5-small-pytorch_model.bin\",",
            "-    \"t5-base\": \"https://s3.amazonaws.com/models.huggingface.co/bert/t5-base-pytorch_model.bin\",",
            "-    \"t5-large\": \"https://s3.amazonaws.com/models.huggingface.co/bert/t5-large-pytorch_model.bin\",",
            "-    \"t5-3b\": \"https://s3.amazonaws.com/models.huggingface.co/bert/t5-3b-pytorch_model.bin\",",
            "-    \"t5-11b\": \"https://s3.amazonaws.com/models.huggingface.co/bert/t5-11b-pytorch_model.bin\",",
            "+    \"t5-small\": \"https://cdn.huggingface.co/t5-small-pytorch_model.bin\",",
            "+    \"t5-base\": \"https://cdn.huggingface.co/t5-base-pytorch_model.bin\",",
            "+    \"t5-large\": \"https://cdn.huggingface.co/t5-large-pytorch_model.bin\",",
            "+    \"t5-3b\": \"https://cdn.huggingface.co/t5-3b-pytorch_model.bin\",",
            "+    \"t5-11b\": \"https://cdn.huggingface.co/t5-11b-pytorch_model.bin\",",
            "}"
        ]
    },
    {
        "number": 1064,
        "comments": "add param for argument fix",
        "commit_message": "Fixed module resolution for tf.keras optimizers and added unit tests (#1935)\n\nSigned-off-by: Travis Addair <taddair@uber.com>\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def load_model(filepath, custom_optimizers=None, custom_objects=None, compressio",
            "\"\"\"",
            "def wrap_optimizer(cls):",
            "return lambda **kwargs: DistributedOptimizer(cls(**kwargs), compression=compression)",
            "-    return _impl.load_model(keras, wrap_optimizer, filepath, custom_optimizers, custom_objects)",
            "+    optimizer_modules = {keras.optimizers.Optimizer.__module__}",
            "+    return _impl.load_model(keras, wrap_optimizer, optimizer_modules, filepath, custom_optimizers, custom_objects)"
        ]
    },
    {
        "number": 1065,
        "comments": "add API call for shape fix",
        "commit_message": "1. Add time dependent boundary condition to the PDE solver\n2. Remove numpy dependency of the PDE solver implementation.\n2. Fix a bug in 1-d PDE solver for the default boundary to work with a batch of PDEs.\n\nPiperOrigin-RevId: 371112748\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class ParabolicEquationStepperTest(tf.test.TestCase, parameterized.TestCase):",
            "second_order_coeff_fn=second_order_coeff_fn,",
            "inner_first_order_coeff_fn=inner_first_order_coeff_fn)[0]",
            "",
            "-    true_values = tf.math.exp(final_t + grid[0])",
            "+    true_values = tf.expand_dims(tf.math.exp(final_t + grid[0]), axis=0)",
            "self.assertAllClose(",
            "est_values, true_values, atol=1e-2, rtol=1e-2)"
        ]
    },
    {
        "number": 1066,
        "comments": "change API call for math fix",
        "commit_message": "[rllib] Fix atari reward calculations, add LR annealing, explained var stat for A2C / impala (#2700)\n\nChanges needed to reproduce Atari plots in IMPALA / A2C: https://github.com/ray-project/rl-experiments\n\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class LocalMultiGPUOptimizer(PolicyOptimizer):",
            "else:",
            "rnn_inputs = []",
            "self.par_opt = LocalSyncParallelOptimizer(",
            "-                        tf.train.AdamOptimizer(",
            "-                            self.sgd_stepsize), self.devices,",
            "+                        self.policy.optimizer(), self.devices,",
            "[v for _, v in self.policy.loss_inputs()], rnn_inputs,",
            "self.per_device_batch_size, self.policy.copy,",
            "os.getcwd())"
        ]
    },
    {
        "number": 1067,
        "comments": "update API call for version fix",
        "commit_message": "fix multigpu training\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class DistributedReplicatedBuilder(DataParallelBuilder):",
            "return grads",
            "",
            "# Ngpu * Nvar * 2",
            "-        grad_list = self.build_on_multi_tower(",
            "-            get_grads,",
            "+        grad_list = DataParallelBuilder.build_on_towers(",
            "+            self.towers, get_grads,",
            "devices=self.raw_devices,",
            "use_vs=[True] * len(self.towers))  # open vs at each tower",
            "DataParallelBuilder._check_grad_list(grad_list)"
        ]
    },
    {
        "number": 1069,
        "comments": "add API call for resource fix",
        "commit_message": "Fix CIs for PyTorch 1.13 (#20686)\n\n* fix 1\n\n* fix 2\n\n* fix 3\n\n* fix 4\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class OPTForSequenceClassification(OPTPreTrainedModel):",
            "sequence_lengths = -1",
            "else:",
            "if input_ids is not None:",
            "-                sequence_lengths = torch.ne(input_ids, self.config.pad_token_id).sum(-1) - 1",
            "+                sequence_lengths = (torch.ne(input_ids, self.config.pad_token_id).sum(-1) - 1).to(logits.device)",
            "else:",
            "sequence_lengths = -1",
            "logger.warning("
        ]
    },
    {
        "number": 1071,
        "comments": "format",
        "commit_message": "formatting fixes for Array API submodule in TensorFlow backend.\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def vector_norm(x: Tensor,",
            "tn_normalized_vector = tf.linalg.norm(x,p,axis,keepdims)",
            "",
            "if tn_normalized_vector.shape  == tuple():",
            "-        return  tf.expand_dims(tn_normalized_vector, 0)",
            "+        return tf.expand_dims(tn_normalized_vector, 0)",
            "return tn_normalized_vector"
        ]
    },
    {
        "number": 1072,
        "comments": "change API call for math fix",
        "commit_message": "Rename .reshape(s,n) -> .expand_by(s).independent(n) (#1016)\n\n* Start to rename .reshape()\n\n* Remove the .reshape() method entirely\n\n* Add .reshape() with informative error message\n\n* Fix test\n\n* Fix failing test\n\n* Fix failing test\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def test_discrete_parallel(continuous_class):",
            "",
            "def model(data):",
            "weights = pyro.sample('weights', dist.Dirichlet(0.5 * torch.ones(K)))",
            "-        locs = pyro.sample('locs', dist.Normal(0, 10).reshape([K], extra_event_dims=1))",
            "+        locs = pyro.sample('locs', dist.Normal(0, 10).expand_by([K]).independent(1))",
            "scale = pyro.sample('scale', dist.LogNormal(0, 1))",
            "",
            "with pyro.iarange('data', len(data)):"
        ]
    },
    {
        "number": 1073,
        "comments": "refactor",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class FBetaMeasure(Metric):",
            "self._total_sum = torch.zeros(num_classes, device=predictions.device)",
            "",
            "if mask is None:",
            "-            mask = torch.ones_like(gold_labels)",
            "-        mask = mask.to(dtype=torch.bool)",
            "+            mask = torch.ones_like(gold_labels).bool()",
            "gold_labels = gold_labels.float()",
            "",
            "argmax_predictions = predictions.max(dim=-1)[1].float()",
            "-        true_positives = (gold_labels == argmax_predictions) * mask",
            "+        true_positives = (gold_labels == argmax_predictions) & mask",
            "true_positives_bins = gold_labels[true_positives]",
            "",
            "# Watch it:"
        ]
    },
    {
        "number": 1075,
        "comments": "version fix",
        "commit_message": "Fix various docs formatting (#1688)\n\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class RenyiELBO(ELBO):",
            "surrogate_elbo_particles = torch.stack(surrogate_elbo_particles)",
            "",
            "log_weights = (1. - self.alpha) * elbo_particles",
            "-        log_mean_weight = logsumexp(log_weights, dim=0) - math.log(self.num_particles)",
            "+        log_mean_weight = torch.logsumexp(log_weights, dim=0) - math.log(self.num_particles)",
            "elbo = log_mean_weight.sum().item() / (1. - self.alpha)",
            "",
            "# collect parameters to train from model and guide"
        ]
    },
    {
        "number": 1077,
        "comments": "add API call for type fix",
        "commit_message": "Fix compatibility with PyTorch 1.0.x (Fixes #906)\n\nSummary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/910\n\nDifferential Revision: D16536532\n\nPulled By: myleott\n\nfbshipit-source-id: 56bb5570e70b5670ad87c64d9dd20c64c1fa9f5c\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class MaskTokensDataset(BaseWrapperDataset):",
            "if self.mask_whole_words is not None:",
            "mask = np.repeat(mask, word_lens)",
            "new_item = np.full(len(mask), self.pad_idx)",
            "-                new_item[mask] = item[torch.from_numpy(mask)]",
            "+                new_item[mask] = item[torch.from_numpy(mask.astype(np.uint8))]",
            "return torch.from_numpy(new_item)",
            "",
            "# decide unmasking and random replacement"
        ]
    },
    {
        "number": 1078,
        "comments": "doc",
        "commit_message": "Add docstrings of core modules and methods (#3120)\n\n* Add docstrings of core modules and methods\n\n* Update docs and fix comments\n\n* Complete docstrings\n\n* Resolve comments\n\n* reformat docstrings\n\n* resolve comments\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class IoUBalancedNegSampler(RandomSampler):",
            "return sampled_inds",
            "",
            "def _sample_neg(self, assign_result, num_expected, **kwargs):",
            "+        \"\"\"Sample negative boxes",
            "+",
            "+        Args:",
            "+            assign_result (:obj:`AssignResult`): The assigned results of boxes.",
            "+            num_expected (int): The number of expected negative samples",
            "+",
            "+        Returns:",
            "+            Tensor or ndarray: sampled indices.",
            "+        \"\"\"",
            "neg_inds = torch.nonzero(assign_result.gt_inds == 0, as_tuple=False)",
            "if neg_inds.numel() != 0:",
            "neg_inds = neg_inds.squeeze(1)"
        ]
    },
    {
        "number": 1079,
        "comments": "change API call for refactor fix",
        "commit_message": "Add a OneTwoMatching distribution (#2697)\n\n* Implement OneTwoMatching.enumerate_support()\n\n* Clarify docs\n\n* Sketch BP computation of Bethe free energy\n\n* Add large-scale smoke test\n\n* Fix some bugs\n\n* Fix sign error\n\n* Simplify\n\n* Add float32 tests\n\n* Clarify; add debug statements\n\n* Relax test, clamp logits\n\n* Optimize clamping logic\n\n* Simplify Z2 computation\n\n* Avoid splitting logits in half\n\n* Add naive sampling algorithm\n\n* Add a more accurate mean field algorithm\n\n* Fix tests\n\n* Simplify\n\n* Change temperature, tighten tests\n\n* Remove unused algorithms\n\n* Update docs and comments\n\n* Numerically stabilize\n\n* Improve accuraccy\n\n* Remove heuristic\n\n* Reduce memory footprint\n\n* Improve accuracy of free energy formula\n\n* Fuse ops\n\n* Implement OneTwoMatching.mode()\n\n* Simplify\n\n* Simplify test\n\n* Simplify and add a test\n\n* Fix docs\n\n* Add Sinkhorn preconditioner\n\n* Add a .mode() method using lap\n\n* Fix in-place op\n\n* Implement .sample() via perturb-and-map\n\n* Remove bad .sample() implementation\n\n* Add link to docs\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def _sample_coalescent_times(leaf_times):",
            "coal_times.append(t)",
            "coal_times.reverse()",
            "",
            "-    return torch.tensor(coal_times)",
            "+    return proto.new_tensor(coal_times)"
        ]
    },
    {
        "number": 1080,
        "comments": "def",
        "commit_message": "python2 fixes\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def transform(point, center, scale, resolution, invert=False):",
            "return new_point.int()",
            "",
            "",
            "-def crop(image, center, scale, resolution=256):",
            "+def crop(image, center, scale, resolution=256.0):",
            "# Crop around the center point",
            "\"\"\" Crops the image around the center. Input is expected to be an np.ndarray \"\"\"",
            "ul = transform([1, 1], center, scale, resolution, True)",
            "br = transform([resolution, resolution], center, scale, resolution, True)",
            "-    pad = math.ceil(torch.norm((ul - br).float()) / 2 - (br[0] - ul[0]) / 2)",
            "+    pad = math.ceil(torch.norm((ul - br).float()) / 2.0 - (br[0] - ul[0]) / 2.0)",
            "if image.ndim > 2:",
            "newDim = np.array([br[1] - ul[1], br[0] - ul[0],",
            "image.shape[2]], dtype=np.int32)"
        ]
    },
    {
        "number": 1082,
        "comments": "doc",
        "commit_message": "Doc fixes in preparation for the docstyle PR (#8061)\n\n* Fixes in preparation for doc styling\n\n* More fixes\n\n* Better syntax\n\n* Fixes\n\n* Style\n\n* More fixes\n\n* More fixes\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class LongformerEmbeddings(nn.Module):",
            "\"\"\"We are provided embeddings directly. We cannot infer which are padded so just generate",
            "sequential position ids.",
            "",
            "-        :param torch.Tensor inputs_embeds:",
            "-        :return torch.Tensor:",
            "+        Args:",
            "+            inputs_embeds: torch.Tensor inputs_embeds:",
            "+",
            "+        Returns: torch.Tensor",
            "\"\"\"",
            "input_shape = inputs_embeds.size()[:-1]",
            "sequence_length = input_shape[1]"
        ]
    },
    {
        "number": 1083,
        "comments": "add param for resource fix",
        "commit_message": "Make most metrics work on GPU (#3851)\n\n* Make most metrics work on GPU\n\n* Make metric tests work on both GPU and CPU\n\n* Add a test for the test utility\n\n* mypy\n\n* Update allennlp/common/testing/test_case.py\n\nCo-Authored-By: Mark Neumann <markn@allenai.org>\n\n* Fix a PR comment\n\n* flake8\n\nCo-authored-by: Mark Neumann <markn@allenai.org>\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class CategoricalAccuracy(Metric):",
            "# ith entry in gold_labels points to index (0-num_classes) for ith row in max_predictions",
            "# For each row check if index pointed by gold_label is was 1 or not (among max scored classes)",
            "correct = max_predictions_mask[",
            "-                torch.arange(gold_labels.numel()).long(), gold_labels",
            "+                torch.arange(gold_labels.numel(), device=gold_labels.device).long(), gold_labels",
            "].float()",
            "tie_counts = max_predictions_mask.sum(-1)",
            "correct /= tie_counts.float()"
        ]
    },
    {
        "number": 1086,
        "comments": "typo fix",
        "commit_message": "circleci fixes\n\nSummary:\nMisc fixes.\n\n- most important: the mac image is gone so switch to a newer one.\n- torch.concat is new; was used accidentally\n- remove lpips from testing in meta.yaml as it is breaking the conda test. Better to leave the relevant tests failing in OSS.\n- TypedDict usage is breaking implicitron on Python 3.7.\n\nReviewed By: patricklabatut\n\nDifferential Revision: D38458164\n\nfbshipit-source-id: b16c26453a743b9a771e2a6787b9a4d2a52e41c2\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class DoublePoolBatchSampler(Sampler[List[int]]):",
            "torch.randperm(len(self.first_indices), generator=self.generator)",
            "for _ in range(n_copies)",
            "]",
            "-            i_first = torch.concat(raw_indices)[:num_batches]",
            "+            i_first = torch.cat(raw_indices)[:num_batches]",
            "else:",
            "i_first = torch.randperm(len(self.first_indices), generator=self.generator)",
            "first_indices = [self.first_indices[i] for i in i_first]"
        ]
    },
    {
        "number": 1087,
        "comments": "rename",
        "commit_message": "Fix tensorflow implementation\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def proposal_layer_tf(rpn_cls_prob, rpn_bbox_pred, im_info, cfg_key, _feat_strid",
            "proposals = bbox_transform_inv_tf(anchors, rpn_bbox_pred)",
            "proposals = clip_boxes_tf(proposals, im_info[:2])",
            "",
            "-  indices = tf.image.non_max_suppression(rpn_bbox_pred, scores, max_output_size=post_nms_topN, iou_threshold=nms_thresh)",
            "+  indices = tf.image.non_max_suppression(proposals, scores, max_output_size=post_nms_topN, iou_threshold=nms_thresh)",
            "",
            "-  boxes = tf.gather(rpn_bbox_pred, indices)",
            "+  boxes = tf.gather(proposals, indices)",
            "boxes = tf.to_float(boxes)",
            "scores = tf.gather(scores, indices)",
            "scores = tf.reshape(scores, shape=(-1, 1))"
        ]
    },
    {
        "number": 1089,
        "comments": "doc",
        "commit_message": "tl.layers API Refactoring and various modifications (#667)\n\n* Decorators API Refactored\n\n* extra_requires `all`, `all_cpu` and `all_gpu` added\n\n* Error fix\n\n* YAPF Formating Correction\n\n* Test for private method decorator added\n\n* Test Logging Verbosity Fixed to Debug when runned individually\n\n* YAPF corrections applied\n\n* Changelog Added\n\n* Changelog updated\n\n* PR number changed\n\n* First Refactoring Pass done\n\n* cleaning second pass\n\n* Refactoring 3rd pass\n\n* Refactoring 4th Pass\n\n* Code Error fix\n\n* YAPF Formating Fix\n\n* Arguments now using self\n\n* YAPF error correction\n\n* Bug Fix in Decorator\n\n* act name bug fix\n\n* Error Correction\n\n* YAPF formating fix\n\n* Useless tf.identity removed\n\n* Error Fix\n\n* Changelog Updated\n\n* Error fix in tl.activation\n\n* Documentation error fix\n\n* Lazy Import added\n\n* Import Refactoring with LazyImport when necessary\n\n* Changelog Updated\n\n* Gitter Removed\n\n* Fixed proposed by @zsdonghao\n\n* Documentation updated\n\n* Missing requirements added\n\n* Update to TensorLayer 1.8.6rc1\n\n* Requirements error fix\n\n* Docker Files updated\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "class MultiplexerLayer(Layer):",
            ">>> network = tl.layers.ReshapeLayer(net_mux, shape=(-1, 800), name='reshape')",
            ">>> network = tl.layers.DropoutLayer(network, keep=0.5, name='drop3')",
            ">>> # output layer",
            "-    >>> network = tl.layers.DenseLayer(network, n_units=10, act=tf.identity, name='output')",
            "+    >>> network = tl.layers.DenseLayer(network, n_units=10, act=None, name='output')",
            "",
            "\"\"\"",
            "",
            "def __init__(self, layers, name='mux_layer'):",
            "super(MultiplexerLayer, self).__init__(prev_layer=layers, name=name)",
            "+",
            "self.n_inputs = len(layers)",
            "",
            "self.inputs = []",
            "+",
            "for l in layers:",
            "self.inputs.append(l.outputs)",
            "+",
            "try:  # TF1.0",
            "all_inputs = tf.stack(self.inputs, name=name)  # pack means concat a list of tensor in a new dim  # 1.2",
            "except Exception:"
        ]
    },
    {
        "number": 1091,
        "comments": "add param for argument fix",
        "commit_message": "re-organize predict/; fix TF incompatibile change of sparse_softmax_cross_entropy_loss\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class Model(ModelDesc):",
            ".GlobalAvgPooling('gap')",
            ".FullyConnected('linear', 1000, nl=tf.identity)())",
            "",
            "-        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, label)",
            "+        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)",
            "loss = tf.reduce_mean(loss, name='xentropy-loss')",
            "",
            "wrong = prediction_incorrect(logits, label, 1, name='wrong-top1')"
        ]
    },
    {
        "number": 1094,
        "comments": "change API call for version fix",
        "commit_message": "Fix the //third_party/py/keras/distribute:minimize_loss_test that fails on local.\n\nThe compat.v1.layer is populated from keras/legacy_tf_layers, which need to be imported differently.\n\nPiperOrigin-RevId: 399749315\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def batchnorm_example(optimizer_fn,",
            "for z in range(batch_per_epoch)]).repeat()",
            "",
            "optimizer = optimizer_fn()",
            "-  batchnorm = tf.compat.v1.layers.BatchNormalization(",
            "+  batchnorm = normalization.BatchNormalization(",
            "renorm=renorm, momentum=momentum, fused=False)",
            "-  layer = tf.compat.v1.layers.Dense(1, use_bias=False)",
            "+  layer = core.Dense(1, use_bias=False)",
            "",
            "def model_fn(x):",
            "\"\"\"A model that uses batchnorm.\"\"\""
        ]
    },
    {
        "number": 1096,
        "comments": "version fix",
        "commit_message": "[RLlib] Tf2x preparation; part 2 (upgrading `try_import_tf()`). (#9136)\n\n* WIP.\n\n* Fixes.\n\n* LINT.\n\n* WIP.\n\n* WIP.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* WIP.\n\n* Fixes.\n\n* Test\n\n* Fix.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* LINT.\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class TestDistributions(unittest.TestCase):",
            "def test_categorical(self):",
            "\"\"\"Tests the Categorical ActionDistribution (tf only).\"\"\"",
            "num_samples = 100000",
            "-        logits = tf.placeholder(tf.float32, shape=(None, 10))",
            "+        logits = tf1.placeholder(tf.float32, shape=(None, 10))",
            "z = 8 * (np.random.rand(10) - 0.5)",
            "data = np.tile(z, (num_samples, 1))",
            "c = Categorical(logits, {})  # dummy config dict",
            "sample_op = c.sample()",
            "-        sess = tf.Session()",
            "-        sess.run(tf.global_variables_initializer())",
            "+        sess = tf1.Session()",
            "+        sess.run(tf1.global_variables_initializer())",
            "samples = sess.run(sample_op, feed_dict={logits: data})",
            "counts = np.zeros(10)",
            "for sample in samples:"
        ]
    },
    {
        "number": 1097,
        "comments": "value update",
        "commit_message": "Docstring fixes and bugs (#5380)\n\n* docstring and framework bug fixes\n\n* docstring fixes and bugs\n\n* docstring failures fixed and bugs removed\n\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def trace(",
            "[7., 8.]]])",
            ">>> y = ivy.trace(x, offset=1)",
            ">>> print(y)",
            "-    ivy.array([2., 6.])",
            "+    ivy.array([3., 4.])",
            "",
            "With :class:`ivy.NativeArray` inputs:"
        ]
    },
    {
        "number": 1099,
        "comments": "value update",
        "commit_message": "[TFT5, Cache] Add cache to TFT5 (#3772)\n\n* correct gpt2 test inputs\n\n* make style\n\n* delete modeling_gpt2 change in test file\n\n* translate from pytorch\n\n* correct tests\n\n* fix conflicts\n\n* fix conflicts\n\n* fix conflicts\n\n* fix conflicts\n\n* make tensorflow t5 caching work\n\n* make style\n\n* clean reorder cache\n\n* remove unnecessary spaces\n\n* fix test\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class TFGPT2ModelTest(TFModelTesterMixin, unittest.TestCase):",
            "output_from_past_slice = output_from_past[:, 0, random_slice_idx]",
            "",
            "# test that outputs are equal for slice",
            "-            tf.debugging.assert_near(output_from_past_slice, output_from_no_past_slice, rtol=1e-12)",
            "+            tf.debugging.assert_near(output_from_past_slice, output_from_no_past_slice, rtol=1e-6)",
            "",
            "def create_and_check_gpt2_model_attention_mask_past(",
            "self, config, input_ids, input_mask, head_mask, token_type_ids, *args"
        ]
    },
    {
        "number": 1100,
        "comments": "add condition check for state fix",
        "commit_message": "Various fix including:\n- Forbid several Torchhook\n- Fix or alter tests\n- Fix create_pointer()\nTODO: There is an issue with the .get() function (just run unittest to see...)\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class TorchHook(object):",
            "",
            "self._hook_torch_module()",
            "",
            "+        if torch.torch_hooked > 0:",
            "+            raise Exception('Torch was already hooked')",
            "+",
            "def _hook_native_tensors_and_variables(self, tensor_type):",
            "\"\"\"Overloading a given tensor_type\"\"\"",
            "# Overload 'special' methods here"
        ]
    },
    {
        "number": 1101,
        "comments": "doc update",
        "commit_message": "[Pipeline] fix failing bloom `pipeline` test (#20778)\n\nfix failing `pipeline` test\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class TextGenerationPipelineTests(unittest.TestCase, metaclass=PipelineTestCaseM",
            "],",
            ")",
            "",
            "-        # torch_dtype not necessary",
            "+        # torch_dtype will be automatically set to float32 if not provided - check: https://github.com/huggingface/transformers/pull/20602",
            "pipe = pipeline(model=\"hf-internal-testing/tiny-random-bloom\", device_map=\"auto\")",
            "self.assertEqual(pipe.model.device, torch.device(0))",
            "-        self.assertEqual(pipe.model.lm_head.weight.dtype, torch.bfloat16)",
            "+        self.assertEqual(pipe.model.lm_head.weight.dtype, torch.float32)",
            "out = pipe(\"This is a test\")",
            "self.assertEqual(",
            "out,"
        ]
    },
    {
        "number": 1102,
        "comments": "version fix",
        "commit_message": "fixed graclus and following transforms\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class SphericalAdj(object):",
            "phi = torch.acos(direction[:, 2]) / PI",
            "spherical = torch.stack([rho, theta, phi], dim=1)",
            "",
            "-        # Modify data and return.",
            "-        data.adj = SparseTensor(index, spherical, torch.Size([n, n, 3]))",
            "-        return data",
            "+        return SparseTensor(index, spherical, torch.Size([n, n, 3]))"
        ]
    },
    {
        "number": 1103,
        "comments": "no API",
        "commit_message": "Fix TPU testing and collect all tests (#11098)\n\nCo-authored-by: Carlos Mochol\u00ed <carlossmocholi@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\nCo-authored-by: Kaushik B <45285388+kaushikb11@users.noreply.github.com>\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def test_devices_auto_choice_mps():",
            "",
            "@pytest.mark.parametrize(",
            "[\"parallel_devices\", \"accelerator\"],",
            "-    [([torch.device(\"cpu\")], \"cuda\"), ([torch.device(\"cuda\", i) for i in range(8)], (\"tpu\"))],",
            "+    [([torch.device(\"cpu\")], \"cuda\"), ([torch.device(\"cuda\", i) for i in range(8)], \"tpu\")],",
            ")",
            "def test_parallel_devices_in_strategy_confilict_with_accelerator(parallel_devices, accelerator):",
            "with pytest.raises(MisconfigurationException, match=r\"parallel_devices set through\"):"
        ]
    },
    {
        "number": 1104,
        "comments": "change API call for math fix",
        "commit_message": "fixed and improved shape handling for exploration\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class EpsilonDecay(Exploration):",
            "",
            "pred = tf.logical_or(x=(timestep < self.start_timestep),",
            "y=(timestep > self.start_timestep + int(self.timesteps)))",
            "-        return tf.cond(pred=pred, true_fn=true_fn, false_fn=false_fn)",
            "+        return tf.fill(dims=shape, value=tf.cond(pred=pred, true_fn=true_fn, false_fn=false_fn))"
        ]
    },
    {
        "number": 1105,
        "comments": "def",
        "commit_message": "fix https://github.com/facebookresearch/maskrcnn-benchmark/issues/802 (#516)\n\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "import itertools",
            "",
            "import torch",
            "",
            "+def is_cuda_enabled():",
            "+    return torch.version.cuda is not None",
            "+",
            "def get_cuda_version():",
            "return tuple(int(x) for x in torch.version.cuda.split('.'))"
        ]
    },
    {
        "number": 1106,
        "comments": "doc",
        "commit_message": "docstring fix\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def fft(",
            "*,",
            "norm: Optional[str] = \"backward\",",
            "n: Union[int, Tuple[int]] = None,",
            "-    out: Optional[torch.Tensor] = None",
            "+    out: Optional[torch.Tensor] = None,",
            ") -> torch.Tensor:",
            "if not isinstance(dim, int):",
            "raise ivy.exceptions.IvyError(f\"Expecting <class 'int'> instead of {type(dim)}\")",
            "if n is None:",
            "n = x.shape[dim]",
            "-    if n < -len(x.shape) :",
            "+    if n < -len(x.shape):",
            "raise ivy.exceptions.IvyError(",
            "f\"Invalid dim {dim}, expecting ranging\"",
            "\" from {-len(x.shape)} to {len(x.shape)-1}  \"",
            ")",
            "if not isinstance(n, int):",
            "raise ivy.exceptions.IvyError(f\"Expecting <class 'int'> instead of {type(n)}\")",
            "-    if n <= 1 :",
            "+    if n <= 1:",
            "raise ivy.exceptions.IvyError(f\"Invalid data points {n}, expecting more than 1\")",
            "if norm != \"backward\" and norm != \"ortho\" and norm != \"forward\":",
            "raise ivy.exceptions.IvyError(f\"Unrecognized normalization mode {norm}\")"
        ]
    },
    {
        "number": 1108,
        "comments": "def",
        "commit_message": "[RLlib] Offline Type Annotations (#9676)\n\n* Offline Annotations\n\n* Modifications\n\n* Fixed circular dependencies\n\n* Linter fix\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class _QueueRunner(threading.Thread):",
            "self.placeholders = [tf1.placeholder(dtype) for dtype in dtypes]",
            "self.enqueue_op = queue.enqueue(dict(zip(keys, self.placeholders)))",
            "",
            "-    def enqueue(self, batch):",
            "+    def enqueue(self, batch: SampleBatchType):",
            "data = {",
            "self.placeholders[i]: batch[key]",
            "for i, key in enumerate(self.keys)"
        ]
    },
    {
        "number": 1109,
        "comments": "add argument for param",
        "commit_message": "Add xfailing jit tests to many anchor examples (#1296)\n\n* Fix Categorical.enumerate_support to make JitTraceEnum_ELBO work\n* Add xfailing examples that use --jit\n* Enable jit in most SVI examples\n",
        "label": "no",
        "answer": "no",
        "change": [
            "if __name__ == '__main__':",
            "help='number of steps between parameter saves')",
            "parser.add_argument('--cuda', action='store_true', default=False,",
            "help='use cuda')",
            "+    parser.add_argument('--jit', action='store_true', default=False,",
            "+                        help='use PyTorch jit')",
            "parser.add_argument('-t', '--model-steps', type=int, default=3,",
            "help='number of time steps')",
            "parser.add_argument('--rnn-hidden-size', type=int, default=256,"
        ]
    },
    {
        "number": 1110,
        "comments": "format",
        "commit_message": "Fix various typos in error messages.\n\nPiperOrigin-RevId: 410160508\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class Sequential(functional.Functional):",
            "# invalid use case of Sequential, but we tolerate it for backwards",
            "# compatibility.",
            "self._use_legacy_deferred_behavior = True",
            "-        self._build_input_shape = tf.nest.map_structure(_get_shape_tuple, inputs)",
            "+        self._build_input_shape = tf.nest.map_structure(",
            "+            _get_shape_tuple, inputs)",
            "if tf.__internal__.tf2.enabled():",
            "logging.warning('Layers in a Sequential model should only have a '",
            "-                          'single input tensor, but we receive a %s input: %s'",
            "-                          '\\nConsider rewriting this model with the Functional '",
            "-                          'API.' % (type(inputs), inputs))",
            "+                          f'single input tensor. Received: inputs={inputs}. '",
            "+                          'Consider rewriting this model with the Functional '",
            "+                          'API.')",
            "else:",
            "self._build_graph_network_for_inferred_shape(inputs.shape, inputs.dtype)"
        ]
    },
    {
        "number": 1111,
        "comments": "version fix",
        "commit_message": "fixed graclus and following transforms\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class CartesianAdj(object):",
            "cartesian *= 1 / (2 * cartesian.abs().max())",
            "cartesian += 0.5",
            "",
            "-        # Modify data and return.",
            "-        data.adj = SparseTensor(index, cartesian, torch.Size([n, n, dim]))",
            "-        return data",
            "+        return SparseTensor(index, cartesian, torch.Size([n, n, dim]))"
        ]
    },
    {
        "number": 1113,
        "comments": "customize method",
        "commit_message": "Refactoring the TF activations functions (#7150)\n\n* Refactoring the activations functions into a common file\n\n* Apply style\n\n* remove unused import\n\n* fix tests\n\n* Fix tests.\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class TFXLNetFeedForward(tf.keras.layers.Layer):",
            ")",
            "self.dropout = tf.keras.layers.Dropout(config.dropout)",
            "if isinstance(config.ff_activation, str):",
            "-            self.activation_function = ACT2FN[config.ff_activation]",
            "+            self.activation_function = get_tf_activation(config.ff_activation)",
            "else:",
            "self.activation_function = config.ff_activation"
        ]
    },
    {
        "number": 1114,
        "comments": "remove param for math fix",
        "commit_message": "fix\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class Stft(torch.nn.Module, InversibleInterface):",
            "pad = self.n_fft // 2",
            "ilens = ilens + 2 * pad",
            "",
            "-            olens = (",
            "-                torch.div((ilens - self.n_fft), self.hop_length, rounding_mode=\"floor\")",
            "-                + 1",
            "-            )",
            "+            olens = torch.div((ilens - self.n_fft), self.hop_length) + 1",
            "output.masked_fill_(make_pad_mask(olens, output, 1), 0.0)",
            "else:",
            "olens = None"
        ]
    },
    {
        "number": 1117,
        "comments": "def",
        "commit_message": "feat: update openclip loader (#782)\n\n* feat: update openclip loader\n\nto support independent download process and make precision adapted to device to solve VRAM issue\n\n* fix: changes for comments\n\n* fix: error\n\n* fix: error\n\n* fix: use openai loader\n\n* fix: address comments\n\n* fix: openclip compatable\n\nCo-authored-by: numb3r3 <wangfelix87@gmail.com>\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class MultilingualCLIPModel(CLIPModel):",
            "input_ids=input_ids, attention_mask=attention_mask, **kwargs",
            ")",
            "",
            "-    def encode_image(self, pixel_values: torch.Tensor, **kwargs):",
            "+    def encode_image(self, pixel_values: torch.Tensor):",
            "return self._model.encode_image(pixel_values)"
        ]
    },
    {
        "number": 1118,
        "comments": "add param for type fix",
        "commit_message": "[Feat] Enabled Torch1.5.1 cpu support (#796)\n\n* Added py151 support\n\n* Enabled 1.5.1 CI\n\n* Typo fix\n\n* Fixed Equalize\n\n* Update setup.py\n\n* Bug fix\n\n* typo fix\n\n* Fixed mypy check\n\n* Update tests_cpu_versions.yml\n\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def _scale_channel(im: torch.Tensor) -> torch.Tensor:",
            "# and then normalization by step.",
            "lut = (torch.cumsum(histo, 0) + (step // 2)) // step",
            "# Shift lut, prepending with 0.",
            "-        lut = torch.cat([torch.zeros(1, device=lut.device), lut[:-1]])",
            "+        lut = torch.cat([torch.zeros(1, device=lut.device, dtype=lut.dtype), lut[:-1]])",
            "# Clip the counts to be in range.  This is done",
            "# in the C code for image.point.",
            "return torch.clamp(lut, 0, 255)"
        ]
    },
    {
        "number": 1121,
        "comments": "typo fix",
        "commit_message": "fixes for transpilation (#7102)\n\nadd private classes for transpiling operations and minor fix for compilation\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def scatter_nd(",
            "*[",
            "torch.range(0, s - 1)",
            "if idx == slice(None, None, None)",
            "-                                else torch.Tensor([idx % s])",
            "+                                else torch.tensor([idx % s])",
            "for s, idx in zip(shape, index)",
            "],",
            "indexing=\"xy\","
        ]
    },
    {
        "number": 1122,
        "comments": "add param for type fix",
        "commit_message": "[Swin, Swinv2] Fix attn_mask dtype (#18803)\n\n* Add dtype\n\n* Fix Swinv2 as well\n\nCo-authored-by: Niels Rogge <nielsrogge@Nielss-MacBook-Pro.local>\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class DonutSwinLayer(nn.Module):",
            "# partition windows",
            "hidden_states_windows = window_partition(shifted_hidden_states, self.window_size)",
            "hidden_states_windows = hidden_states_windows.view(-1, self.window_size * self.window_size, channels)",
            "-        attn_mask = self.get_attn_mask(height_pad, width_pad)",
            "+        attn_mask = self.get_attn_mask(height_pad, width_pad, dtype=hidden_states.dtype)",
            "if attn_mask is not None:",
            "attn_mask = attn_mask.to(hidden_states_windows.device)"
        ]
    },
    {
        "number": 1125,
        "comments": "no API",
        "commit_message": "fix `unravel_index` test defintion\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def max_value_as_shape_prod(draw):",
            "",
            "",
            "@handle_test(",
            "-    fn_tree=\"functional.ivy.experimental.nanmean\",",
            "+    fn_tree=\"functional.ivy.experimental.unravel_index\",",
            "dtype_x_shape=max_value_as_shape_prod(),",
            "test_gradients=st.just(False),",
            ")"
        ]
    },
    {
        "number": 1126,
        "comments": "add API call for math fix",
        "commit_message": "Fix mode regularized gan loss\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "for it in range(1000000):",
            "D_reg = D(G_sample_reg)",
            "",
            "mse = torch.sum((X - G_sample_reg)**2, 1)",
            "-    E_loss = torch.mean(lam1 * mse + lam2 * D_reg)",
            "+    E_loss = torch.mean(lam1 * mse + lam2 * log(D_reg))",
            "",
            "E_loss.backward()",
            "E_solver.step()"
        ]
    },
    {
        "number": 1127,
        "comments": "change param for resource fix",
        "commit_message": "Fix VisualBert Embeddings (#13017)\n\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class VisualBertEmbeddings(nn.Module):",
            "inputs_embeds = self.word_embeddings(input_ids)",
            "",
            "if token_type_ids is None:",
            "-            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.input_embeds.device)",
            "+            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)",
            "",
            "token_type_embeddings = self.token_type_embeddings(token_type_ids)"
        ]
    },
    {
        "number": 1128,
        "comments": "remove API call for type fix",
        "commit_message": "Fix BCELoss adressing  #1192\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class BCELossMasked(nn.Module):",
            "Returns:",
            "loss: An average loss value in range [0, 1] masked by the length.",
            "\"\"\"",
            "-        # mask: (batch, max_len, 1)",
            "target.requires_grad = False",
            "if length is not None:",
            "-            mask = sequence_mask(sequence_length=length, max_len=target.size(1)).float()",
            "-            x = x * mask",
            "-            target = target * mask",
            "+            # mask: (batch, max_len, 1)",
            "+            mask = sequence_mask(sequence_length=length, max_len=target.size(1))",
            "num_items = mask.sum()",
            "+            loss = functional.binary_cross_entropy_with_logits(x.masked_select(mask), target.masked_select(mask), pos_weight=self.pos_weight, reduction=\"sum\")",
            "else:",
            "+            loss = functional.binary_cross_entropy_with_logits(x, target, pos_weight=self.pos_weight, reduction=\"sum\")",
            "num_items = torch.numel(x)",
            "-        loss = functional.binary_cross_entropy_with_logits(x, target, pos_weight=self.pos_weight, reduction=\"sum\")",
            "loss = loss / num_items",
            "return loss"
        ]
    },
    {
        "number": 1129,
        "comments": "change API call for refactor fix",
        "commit_message": "summaries updated and fixed\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class PrioritizedReplay(Memory):",
            "))",
            "",
            "with tf.control_dependencies(control_inputs=assignments):",
            "-            return tf.no_op()",
            "+            return util.no_operation()",
            "",
            "# These are not supported for prioritized replay currently.",
            "def tf_retrieve_episodes(self, n):"
        ]
    },
    {
        "number": 1130,
        "comments": "no API",
        "commit_message": "[Retiarii] Serializer and experiment status fixes (#3421)\n\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "def get_module_name(cls):",
            "f'please launch the experiment under the directory where \"{main_file_path.name}\" is located.')",
            "module_name = main_file_path.stem",
            "break",
            "+    if module_name == '__main__':",
            "+        warnings.warn('Callstack exhausted but main module still not found. This will probably cause issues that the '",
            "+                      'function/class cannot be imported.')",
            "",
            "# NOTE: this is hacky. As torchscript retrieves LSTM's source code to do something.",
            "# to make LSTM's source code can be found, we should assign original LSTM's __module__ to",
            "# the wrapped LSTM's __module__",
            "# TODO: find out all the modules that have the same requirement as LSTM",
            "-    if f'{cls.__module__}.{cls.__name__}' == 'torch.nn.modules.rnn.LSTM':",
            "-        module_name = cls.__module__",
            "+    if f'{cls_or_func.__module__}.{cls_or_func.__name__}' == 'torch.nn.modules.rnn.LSTM':",
            "+        module_name = cls_or_func.__module__",
            "",
            "return module_name",
            "",
            "",
            "-def get_full_class_name(cls, relocate_module=False):",
            "+def get_importable_name(cls, relocate_module=False):",
            "module_name = get_module_name(cls) if relocate_module else cls.__module__",
            "return module_name + '.' + cls.__name__"
        ]
    },
    {
        "number": 1131,
        "comments": "doc update",
        "commit_message": "fix spelling mistake (#749)\n\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "class OnebitAdam(torch.optim.Optimizer):",
            "self.adam_freeze_key = False",
            "self.initialize = True",
            "print(",
            "-                f\"Finished the initialization step at rant {torch.distributed.get_rank()}\"",
            "+                f\"Finished the initialization step at rank {torch.distributed.get_rank()}\"",
            ")",
            "return loss"
        ]
    },
    {
        "number": 1132,
        "comments": "add param for type fix",
        "commit_message": "\ud83c\udfcb  Fix nan issue in training fastspeech/fastspeech2.\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class TFFastSpeech(tf.keras.Model):",
            "== config.decoder_self_attention_params.hidden_size,",
            "name=\"decoder\",",
            ")",
            "-        self.mel_dense = tf.keras.layers.Dense(units=config.num_mels, name=\"mel_before\")",
            "-        self.postnet = TFTacotronPostnet(config=config, name=\"postnet\")",
            "+        self.mel_dense = tf.keras.layers.Dense(units=config.num_mels, dtype=tf.float32, name=\"mel_before\")",
            "+        self.postnet = TFTacotronPostnet(config=config, dtype=tf.float32, name=\"postnet\")",
            "",
            "self.setup_inference_fn()"
        ]
    },
    {
        "number": 1134,
        "comments": "asset",
        "commit_message": "Fix numerical instability in `GeneralConv` and `neighbor_sample` tests (#4754)\n\n* fix full testing\n\n* changelog\n\n* update\n\n* changelog\n\n* reset\n\n* update\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def test_neighbor_sampler_on_cora(get_dataset):",
            "_, n_id, adjs = next(iter(loader))",
            "out1 = model.batch(data.x[n_id], adjs)",
            "out2 = model.full(data.x, data.edge_index)[batch]",
            "-    assert torch.allclose(out1, out2)",
            "+    assert torch.allclose(out1, out2, atol=1e-7)"
        ]
    },
    {
        "number": 1136,
        "comments": "for loop",
        "commit_message": "model fixes + ipnb fixes\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def main():",
            "",
            "model.eval()",
            "with open(args.output_file, \"w\", encoding='utf-8') as writer:",
            "-        for input_ids, input_mask, segment_ids, example_indices in eval_dataloader:",
            "+        for input_ids, input_mask, example_indices in eval_dataloader:",
            "input_ids = input_ids.to(device)",
            "input_mask = input_mask.float().to(device)",
            "-            segment_ids = segment_ids.to(device)",
            "",
            "-            all_encoder_layers, _ = model(input_ids, segment_ids, input_mask)",
            "+            all_encoder_layers, _ = model(input_ids, token_type_ids=None, attention_mask=input_mask)",
            "",
            "for enc_layers, example_index in zip(all_encoder_layers, example_indices):",
            "feature = features[example_index.item()]"
        ]
    },
    {
        "number": 1140,
        "comments": "add API call for state fix",
        "commit_message": "Fix for restore from checkpoint\n\nResets the buffer index upon restore, ensuring the model is consistent\nwith the agent's behavior - starting a new episode.\n\nThis fixes the following error which occurs on attempt to restore:\n    InvalidArgumentError (see above for traceback): Must have updates.shape = indices.shape + params.shape[1:], got updates.shape [30,1], indices.shape [21], params.shape [100,1]\n  \t[[Node: ppo/observe-timestep/store/ScatterUpdate = ScatterUpdate[T=DT_FLOAT, Tindices=DT_INT32, _class=[\"loc:@ppo/initialize/latest/initialize/state-state\"], use_locking=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](ppo/initialize/latest/initialize/state-state, ppo/observe-timestep/store/mod, ppo/strided_slice, ^ppo/observe-timestep/store/AssignSub)]]\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class Model(object):",
            "#     raise TensorForceError(\"Invalid model directory/file.\")",
            "",
            "self.saver.restore(sess=self.session, save_path=file)",
            "+        self.session.run(self.buffer_index_reset_op)",
            "",
            "def get_components(self):",
            "\"\"\""
        ]
    },
    {
        "number": 1141,
        "comments": "customize API",
        "commit_message": "enh: Implements `InferenceModule` as a pipelined module with separate preprocessor, predictor, and postprocessor modules (#2105)\n\n* Adding inference pipeline with seperate pre-processing, predict and post-processing modules\n\n* Update to flatten outputs from predict consistent to support triton\n\n* inference module refactor\n\n* add back InferenceLudwigModel\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* unify modules into inference.py\n\n* cleaned up inaccurate documentation\n\n* clean up\n\n* clean up type hints and update InferenceLudwigModel\n\n* clean up type hint; passes test_torchscript.py\n\n* added typing to inference module for clarity\n\n* remove inference_module_file_name constant\n\n* unified predict module with postproc\n\n* removed InferencePredictor entirely\n\n* add back the old inference module\n\n* add back training set metadata\n\n* revert change to predict module, move feature filtering to postproc\n\n* cleanup inference_module_v0\n\n* cleanup\n\n* adds device placement to InferenceLudwigModel\n\n* adds ability to save/load torchscript on particular devices\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* allows saving torchscript with dict of devices from api.py\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* correct device inputs\n\n* refactor to expose inference stages (prep for triton refactor)\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* remove magic 'cpu' string\n\n* remove extraneous constants\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add from_directory classmethod for e2e users\n\n* merge\n\n* merge InferenceModule and InferenceLudwigModel\n\n* add comment\n\n* revert small change\n\n* cleanup\n\n* add to_torchscript functionality\n\n* cleanup\n\n* pushes device logic down into inference stages\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* move device placement upstream to inference module to ensure stage modules are performant\n\n* adds logs for device placement experiments\n\n* removes logs\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* remove stage_to_dict\n\n* clean up how we get input device in predictor_forward\n\n* first commit\n\n* wip\n\n* updated interfaces\n\n* postproc GPU\n\n* add intelligent device placement\n\n* clean up device api\n\n* revert flatten op in inference_module_v0\n\n* remove dtype workaround\n\n* benchmarking code\n\n* add DEVICE constant as good default for loading/saving\n\n* added helpful logging and style\n\n* cleanup\n\n* cleanup, adding docstrings\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* docstring\n\nCo-authored-by: Geoffrey Angus <geoffrey@predibase.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "class H3FeatureMixin(BaseFeatureMixin):",
            "):",
            "column = input_df[feature_config[COLUMN]]",
            "if column.dtype == object:",
            "-            column = column.map(int)",
            "-        column = column.map(H3FeatureMixin.h3_to_list)",
            "+            column = backend.df_engine.map_objects(column, int)",
            "+        column = backend.df_engine.map_objects(column, H3FeatureMixin.h3_to_list)",
            "",
            "proc_df[feature_config[PROC_COLUMN]] = backend.df_engine.map_objects(",
            "column, lambda x: np.array(x, dtype=np.uint8)"
        ]
    },
    {
        "number": 1142,
        "comments": "add API call for math fix",
        "commit_message": "Texture Atlas sampling bug fix\n\nSummary: Fixes the index out of bound errors for texture sampling from a texture atlas: when barycentric coordinates are 1.0, the integer index into the (R, R) per face texture map is R (max can only be R-1).\n\nReviewed By: gkioxari\n\nDifferential Revision: D25543803\n\nfbshipit-source-id: 82d0935b981352b49c1d95d5a17f9cc88bad0a82\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class TexturesAtlas(TexturesBase):",
            "# pyre-fixme[16]: `bool` has no attribute `__getitem__`.",
            "mask = (pix_to_face < 0)[..., None]",
            "bary_w01 = torch.where(mask, torch.zeros_like(bary_w01), bary_w01)",
            "-        w_xy = (bary_w01 * R).to(torch.int64)  # (N, H, W, K, 2)",
            "+        # If barycentric coordinates are > 1.0 (in the case of",
            "+        # blur_radius > 0.0), wxy might be > R. We need to clamp this",
            "+        # index to R-1 to index into the texture atlas.",
            "+        w_xy = (bary_w01 * R).to(torch.int64).clamp(max=R - 1)  # (N, H, W, K, 2)",
            "",
            "below_diag = (",
            "bary_w01.sum(dim=-1) * R - w_xy.float().sum(dim=-1)"
        ]
    },
    {
        "number": 1143,
        "comments": "rename",
        "commit_message": "Add binary neural networks. (#418)\n\n* add BinaryDenseLayer SignLayer etc\n\n* add example of binarynet cnn | add BinaryConv2d\n\n* rename scale layer\\\n\n* remove unused code\n\n* remove print params\n\n* rename function name in binarynet example\n\n* update all\n\n* rename sign act name\n\n* rename function\n\n* fix codacy;\n\n* rename sign\n\n* improve docs for sign\n\n* yapf\n\n",
        "label": "no",
        "answer": "no",
        "change": [
            "def sign(x):  # https://github.com/AngusG/tensorflow-xnor-bnn/blob/master/models",
            "",
            "\"\"\"",
            "with tf.get_default_graph().gradient_override_map({\"sign\": \"QuantizeGrad\"}):",
            "-        return tf.sign(x, name='tl_sign')",
            "+        return tf.sign(x, name='sign')",
            "",
            "",
            "# if tf.__version__ > \"1.7\":"
        ]
    },
    {
        "number": 1144,
        "comments": "update API call for version fix",
        "commit_message": "Update to PyTorch 1.9.0 (#2887)\n\n* Update to PyTorch 1.9.0\n\n* Update torch.cholesky, torch.symeig\n\n* Fix torch.tensordot, torch.qr, funsor dependency\n\n* Fix torch import, AutoGuideList usage\n\n* Ignore bug in PyTorch jit\n\n* Ignore tracer warnings\n\n* Replace torch.solve with torch.linalg.solve\n\n* Xfail vectorized markov funsor tests\n\n* Update funsor version\n\n* Switch MNIST mirrors\n\n* Pin pillow version\n\n* Bump torchvision version\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class VariationalSparseGP(GPModel):",
            "M = self.Xu.size(0)",
            "Kuu = self.kernel(self.Xu).contiguous()",
            "Kuu.view(-1)[::M + 1] += self.jitter  # add jitter to the diagonal",
            "-        Luu = Kuu.cholesky()",
            "+        Luu = torch.linalg.cholesky(Kuu)",
            "",
            "zero_loc = self.Xu.new_zeros(self.u_loc.shape)",
            "if self.whiten:"
        ]
    },
    {
        "number": 1146,
        "comments": "",
        "commit_message": "keras converter accepts fixed length sequence in embedding-flatten models\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class NetGraph(object):",
            "self.remove_skip_layers(_KERAS_SKIP_LAYERS) # done 1 pass",
            "self.insert_1d_permute_layers()",
            "self.insert_permute_for_spatial_bn()",
            "+            self.insert_permute_for_embed_flatten()",
            "self.defuse_activation()",
            "self.remove_internal_input_layers()"
        ]
    },
    {
        "number": 1148,
        "comments": "",
        "commit_message": "Update to PyTorch 1.9.0 (#2887)\n\n* Update to PyTorch 1.9.0\n\n* Update torch.cholesky, torch.symeig\n\n* Fix torch.tensordot, torch.qr, funsor dependency\n\n* Fix torch import, AutoGuideList usage\n\n* Ignore bug in PyTorch jit\n\n* Ignore tracer warnings\n\n* Replace torch.solve with torch.linalg.solve\n\n* Xfail vectorized markov funsor tests\n\n* Update funsor version\n\n* Switch MNIST mirrors\n\n* Pin pillow version\n\n* Bump torchvision version\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def _matvecmul(x, y):",
            "",
            "",
            "def _cholesky(x):",
            "-    return x.sqrt() if x.dim() == 1 else x.cholesky()",
            "+    return x.sqrt() if x.dim() == 1 else torch.linalg.cholesky(x)",
            "",
            "",
            "def _transpose(x):"
        ]
    },
    {
        "number": 1149,
        "comments": "",
        "commit_message": "Fix BERT/MobileBERT classifier dropout\n",
        "label": "",
        "answer": "no",
        "change": [
            "class MobileBertForMultipleChoice(MobileBertPreTrainedModel):",
            "super().__init__(config)",
            "",
            "self.mobilebert = MobileBertModel(config)",
            "-        self.dropout = nn.Dropout(config.hidden_dropout_prob)",
            "+        self.dropout = nn.Dropout(config.classifier_dropout)",
            "self.classifier = nn.Linear(config.hidden_size, 1)",
            "",
            "self.init_weights()"
        ]
    },
    {
        "number": 1152,
        "comments": "",
        "commit_message": "Fix TF RNN dynamic behavior\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def rnn(step_function, inputs, initial_states,",
            "new_states = []",
            "",
            "# all this circus is to recover the last vector in the sequence.",
            "-        begin = tf.pack([tf.shape(outputs)[0] - 1] + [0] * (ndim - 1))",
            "-        size = tf.pack([1] + [-1] * (ndim - 1))",
            "-        last_output = tf.slice(outputs, begin, size)",
            "+        slice_begin = tf.pack([tf.shape(outputs)[0] - 1] + [0] * (ndim - 1))",
            "+        slice_size = tf.pack([1] + [-1] * (ndim - 1))",
            "+        last_output = tf.slice(outputs, slice_begin, slice_size)",
            "last_output = tf.squeeze(last_output, [0])",
            "",
            "axes = [1, 0] + list(range(2, len(outputs.get_shape())))"
        ]
    },
    {
        "number": 1155,
        "comments": "",
        "commit_message": "[RLlib] Add all simple learning tests as `framework=tf2`. (#19273)\n\n* Unpin gym and deprecate pendulum v0\n\nMany tests in rllib depended on pendulum v0,\nhowever in gym 0.21, pendulum v0 was deprecated\nin favor of pendulum v1. This may change reward\nthresholds, so will have to potentially rerun\nall of the pendulum v1 benchmarks, or use another\nenvironment in favor. The same applies to frozen\nlake v0 and frozen lake v1\n\nLastly, all of the RLlib tests and Tune tests have\nbeen moved to python 3.7\n\n* fix tune test_sampler::testSampleBoundsAx\n\n* fix re-install ray for py3.7 tests\n\nCo-authored-by: avnishn <avnishn@uw.edu>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def convert_to_numpy(x: TensorStructType, reduce_floats: bool = False):",
            "if torch and isinstance(item, torch.Tensor):",
            "ret = item.cpu().item() if len(item.size()) == 0 else \\",
            "item.detach().cpu().numpy()",
            "-        elif tf and isinstance(item, (tf.Tensor, tf.Variable)):",
            "+        elif tf and isinstance(item, (tf.Tensor, tf.Variable)) and \\",
            "+                hasattr(item, \"numpy\"):",
            "assert tf.executing_eagerly()",
            "ret = item.numpy()",
            "else:"
        ]
    },
    {
        "number": 1156,
        "comments": "",
        "commit_message": "small fix in staging\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class StagingInputWrapper(FeedfreeInput):",
            "",
            "def setup_staging_areas(self):",
            "for idx, device in enumerate(self._devices):",
            "-            inputs = self._input.get_input_tensors()",
            "-            dtypes = [x.dtype for x in inputs]",
            "with tf.device(device):",
            "-                stage = StagingArea(",
            "-                    dtypes, shapes=None)",
            "+                inputs = self._input.get_input_tensors()",
            "+                dtypes = [x.dtype for x in inputs]",
            "+                stage = StagingArea(dtypes, shapes=None)",
            "self._stage_ops.append(stage.put(inputs))",
            "self._areas.append(stage)",
            "outputs = stage.get()"
        ]
    },
    {
        "number": 1157,
        "comments": "",
        "commit_message": "fix error when running with --show-mask-heatmaps (#274)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class COCODemo(object):",
            "\"\"\"",
            "masks = predictions.get_field(\"mask\")",
            "masks_per_dim = self.masks_per_dim",
            "-        masks = torch.nn.functional.interpolate(",
            "+        masks = L.interpolate(",
            "masks.float(), scale_factor=1 / masks_per_dim",
            ").byte()",
            "height, width = masks.shape[-2:]"
        ]
    },
    {
        "number": 1159,
        "comments": "",
        "commit_message": "fix(speedup): make the tensor contiguous before randomizing (#5141)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class ModelSpeedup:",
            "while not visit_queue.empty():",
            "curnode = visit_queue.get()",
            "self.update_indirect_sparsity(curnode)",
            "-            predecessors = self.torch_graph.find_predecessors(",
            "-                curnode.unique_name)",
            "+            predecessors = set(self.torch_graph.find_predecessors(",
            "+                curnode.unique_name))",
            "for predecessor in predecessors:",
            "out_degree[predecessor] -= 1",
            "if out_degree[predecessor] == 0:"
        ]
    },
    {
        "number": 1161,
        "comments": "",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestLegacyAttention(AllenNlpTestCase):",
            "[[0.6, 0.8, 0.1], [0.15, 0.5, 0.2], [0.5, 0.3, 0.2]],",
            "]",
            ")",
            "-        mask = torch.FloatTensor([[1.0, 1.0, 0.0], [0.0, 0.0, 0.0]])",
            "+        mask = torch.BoolTensor([[True, True, False], [False, False, False]])",
            "result = attention(vector, matrix, mask).data.numpy()",
            "assert_almost_equal(result, numpy.array([[0.5, 0.5, 0.0], [0.0, 0.0, 0.0]]))"
        ]
    },
    {
        "number": 1163,
        "comments": "",
        "commit_message": "Fix #1686\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class OpenAIGPTDoubleHeadsModel(OpenAIGPTPreTrainedModel):",
            "tokenizer = OpenAIGPTTokenizer.from_pretrained('openai-gpt')",
            "model = OpenAIGPTDoubleHeadsModel.from_pretrained('openai-gpt')",
            "tokenizer.add_special_tokens({'cls_token': '[CLS]'})  # Add a [CLS] to the vocabulary (we should train it also!)",
            "+        model.resize_token_embeddings(len(tokenizer))",
            "+",
            "choices = [\"Hello, my dog is cute [CLS]\", \"Hello, my cat is cute [CLS]\"]",
            "input_ids = torch.tensor([tokenizer.encode(s) for s in choices]).unsqueeze(0)  # Batch size 1, 2 choices",
            "-        mc_token_ids = torch.tensor([input_ids.size(-1), input_ids.size(-1)]).unsqueeze(0)  # Batch size 1",
            "+        mc_token_ids = torch.tensor([input_ids.size(-1)-1, input_ids.size(-1)-1]).unsqueeze(0)  # Batch size 1",
            "+",
            "outputs = model(input_ids, mc_token_ids=mc_token_ids)",
            "lm_prediction_scores, mc_prediction_scores = outputs[:2]"
        ]
    },
    {
        "number": 1164,
        "comments": "",
        "commit_message": "fix use of deprecated TF functions.\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Model(ModelDesc):",
            "add_moving_summary(tf.reduce_mean(wrong, name='train_error'))",
            "",
            "# weight decay on all W of fc layers",
            "-        wd_cost = tf.mul(0.0004,",
            "-                         regularize_cost('fc.*/W', tf.nn.l2_loss),",
            "-                         name='regularize_loss')",
            "+        wd_cost = regularize_cost('fc.*/W', l2_regularizer(4e-4), name='regularize_loss')",
            "add_moving_summary(cost, wd_cost)",
            "",
            "add_param_summary(('.*/W', ['histogram']))   # monitor W"
        ]
    },
    {
        "number": 1166,
        "comments": "",
        "commit_message": "fix `set` positional and keyword arguments (#2891)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def unique_inverse(",
            "",
            "def unique_values(",
            "x: Union[tf.Tensor, tf.Variable],",
            "+    /,",
            "*,",
            "-    out: Optional[Union[tf.Tensor, tf.Variable]] = None",
            "+    out: Optional[Union[tf.Tensor, tf.Variable]] = None,",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "ret = tf.unique(tf.reshape(x, [-1]))[0]",
            "return tf.sort(ret)"
        ]
    },
    {
        "number": 1167,
        "comments": "",
        "commit_message": "upgrade to pytorch 0.4.0 (#1126)\n\n* bump pytorch to 0.4 + fix sanitize\n\n* remove check for Variable in block_orthogonal\n\n* rename parameter split_size=\n\n* remove checks for Variable\n\n* fix more tests\n\n* fixes\n\n* get tests to pass\n\n* fix warnings\n\n* get rid of some of the Variables\n\n* more tests passing\n\n* more elimination of variables\n\n* finish removing all Variables\n\n* pylint and such\n\n* a few fixes\n\n* move torch.no_grad into model.forward_on_instances\n\n* more pytorch 0.4 changes\n\n* detach() -> data\n\n* pylint\n\n* fix bad tensor creation\n\n* fix types\n\n* remove print statement\n\n* more 0.4 goodness\n\n* factor out is_tensor\n\n* add no_grad to elmo command\n\n* cleanup\n\n* remove TODO\n\n* address PR feedback\n\n* replace all() with item()\n\n* more cleanup\n\n* further cleanup\n\n* remove Variable\n\n* really fix merge conflict\n\n* fix pylint\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestHighway(AllenNlpTestCase):",
            "",
            "def test_forward_works_on_nd_input(self):",
            "highway = Highway(2, 2)",
            "-        input_tensor = Variable(torch.ones(2, 2, 2))",
            "+        input_tensor = torch.ones(2, 2, 2)",
            "output = highway(input_tensor)",
            "assert output.size() == (2, 2, 2)"
        ]
    },
    {
        "number": 1168,
        "comments": "",
        "commit_message": "fix concat + tensorflow out type hints\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def reshape(",
            "shape: Union[ivy.NativeShape, Sequence[int]],",
            "*,",
            "copy: Optional[bool] = None,",
            "-    out: Optional[tf.Tensor] = None,",
            "+    out: Optional[Union[tf.Tensor, tf.Variable]] = None,",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "if copy:",
            "newarr = tf.experimental.numpy.copy(x)"
        ]
    },
    {
        "number": 1169,
        "comments": "",
        "commit_message": "get action state dim fix\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TRPOUpdater(ValueFunction):",
            "",
            "action_means, action_log_stds = self.session.run([self.action_means,",
            "self.action_log_stds],",
            "-                                                         {self.state: state})",
            "+                                                         {self.state: [state]})",
            "",
            "action = action_means + np.exp(action_log_stds) * self.random.randn(*action_log_stds.shape)"
        ]
    },
    {
        "number": 1171,
        "comments": "",
        "commit_message": "Keras: Fix docstring for tf.keras.optimizer. -> tf.keras.optimizers.\n\nPiperOrigin-RevId: 481721074\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def convert_to_legacy_optimizer(optimizer):",
            "",
            "This function takes in a `tf.keras.optimizers.experimental.Optimizer`",
            "instance and converts it to the corresponding",
            "-    `tf.keras.optimizer.legacy.Optimizer` instance.",
            "+    `tf.keras.optimizers.legacy.Optimizer` instance.",
            "For example, `tf.keras.optimizers.experimental.Adam(...)` to",
            "`tf.keras.optimizers.legacy.Adam(...)`."
        ]
    },
    {
        "number": 1172,
        "comments": "",
        "commit_message": "backend fixes (excl. tensorflow) for floor_divide special cases | ivy floor_divide also altered\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def floor_divide(",
            "out: Optional[torch.Tensor] = None",
            ") -> torch.Tensor:",
            "x1, x2 = _cast_for_binary_op(x1, x2)",
            "-    return torch.div(x1, x2, rounding_mode=\"floor\", out=out)",
            "+    return torch.floor(torch.divide(x1, x2, out=out))",
            "",
            "",
            "def bitwise_or("
        ]
    },
    {
        "number": 1175,
        "comments": "",
        "commit_message": "fix mypy and lint checks\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def normalize(data: torch.Tensor, mean: torch.Tensor,",
            "mean = mean[..., :, None, None].to(data.device)",
            "std = std[..., :, None, None].to(data.device)",
            "",
            "-    out = data.sub(mean).div(std)",
            "+    out = (data - mean) / std",
            "",
            "return out",
            "-",
            "-# - denormalise"
        ]
    },
    {
        "number": 1186,
        "comments": "",
        "commit_message": "Use soundfile for mp3 decoding instead of torchaudio (#5573)\n\n* use soundfile for mp3 decoding instead of torchaudio\n\n* fix some tests\n\n* remove torch and torchaudio from library's requirements\n\n* refactor audio decoding, decode everything with soundfile\n\n* remove torchaudio latest test ci stage, remove libsndfile and sox binaries installation\n\n* remove checks for libsndfile in tests since it's bundeled in python library\n\n* remove instructions about installing via package manager since it's misleading\n\n* pin soundfile version to the latest\n\n* update documentation\n\n* fix setup\n\n* Update docs/source/installation.md\n\nCo-authored-by: Mario \u0160a\u0161ko <mariosasko777@gmail.com>\n\n* refactor decoding: move all the code under the main decode_example func\n\n* get audio format with os.path instead of string split\n\n* add module config variables for opus and mp3 support\n\n* apply steven's suggestion to installation docs\n\n* wrap torch.from_numpy in a func to avoid torch.from_numpy pickling error\n\n* Apply suggestions from code review\n\nCo-authored-by: Mario \u0160a\u0161ko <mariosasko777@gmail.com>\n\n* fix code style\n\n* import xsplitext\n\n---------\n\nCo-authored-by: Mario \u0160a\u0161ko <mariosasko777@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Pickler(dill.Pickler):",
            "",
            "@pklregister(obj_type)",
            "def _save_tensor(pickler, obj):",
            "+                        # `torch.from_numpy` is not picklable in `torch>=1.11.0`",
            "+                        def _create_tensor(np_array):",
            "+                            return torch.from_numpy(np_array)",
            "+",
            "dill_log(pickler, f\"To: {obj}\")",
            "args = (obj.detach().cpu().numpy(),)",
            "-                        pickler.save_reduce(torch.from_numpy, args, obj=obj)",
            "+                        pickler.save_reduce(_create_tensor, args, obj=obj)",
            "dill_log(pickler, \"# To\")",
            "return"
        ]
    },
    {
        "number": 1187,
        "comments": "",
        "commit_message": "[T5] Bug correction & Refactor (#8518)\n\n* fix bug\n\n* T5 refactor\n\n* refactor tf\n\n* apply sylvains suggestions\n",
        "label": "",
        "answer": "no",
        "change": [
            "def convert_tf_checkpoint_to_pytorch(tf_checkpoint_path, config_file, pytorch_du",
            "",
            "# Save pytorch-model",
            "print(\"Save PyTorch model to {}\".format(pytorch_dump_path))",
            "-    torch.save(model.state_dict(), pytorch_dump_path)",
            "+    model.save_pretrained(pytorch_dump_path)",
            "",
            "",
            "if __name__ == \"__main__\":"
        ]
    },
    {
        "number": 1190,
        "comments": "",
        "commit_message": "Removes dependency on the overrides package (#5490)\n\n* Removes dependency on the overrides package\n\n* Changelog\n\n* Various fixes for mypy\n\n* Update cached_path dependency\n\n* What happened here?\n\n* Formatting\n\n* Fix more tests\n\n* One more missing overrides\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TorchCheckpointWrapper(CheckpointWrapper):",
            "#  --> https://github.com/facebookresearch/fairscale/blob/1e4a503cda8571851a68effd6e504a192838ab06/fairscale/nn/checkpoint/checkpoint_activations.py#L145-L153  # noqa: E501",
            "# We just patch the forward method to avoid having to proxy all the fields and other methods.",
            "# The use of weakref here is to prevent creating a ref cycle: m -> m.forward -> m.",
            "+",
            "+        assert len(kwargs) == 0  # This way of wrapping only works for positional arguments.",
            "+",
            "module.forward = functools.partial(  # type: ignore[assignment]",
            "_checkpointed_forward, type(module).forward, weakref.ref(module)",
            ")"
        ]
    },
    {
        "number": 1191,
        "comments": "",
        "commit_message": "Speed-up warp_affine and fix bugs in RandomAffine [WIP: to add tests] (#474)\n\n* speed-up warp_affine, rotate, random_crop\n\n* added basic speed test for warp_affine\n\n* fixed centerization for random shear and bug (radians instead of degrees)\n\n* add test versus torchvision\n\n* added convert_affinematrix_to_homography function\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestInvertAffineTransform:",
            "assert_allclose(matrix_inv, expected)",
            "",
            "def test_gradcheck(self, device):",
            "-        matrix = torch.eye(2, 3).to(device)",
            "+        matrix = torch.eye(2, 3).to(device)[None]",
            "matrix = utils.tensor_to_gradcheck_var(matrix)  # to var",
            "assert gradcheck(kornia.invert_affine_transform, (matrix,),",
            "raise_exception=True)"
        ]
    },
    {
        "number": 1192,
        "comments": "",
        "commit_message": "fix deprecation warnings; remove pydoop dependency; update README\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def train(target, dataset, cluster_spec, ctx):",
            "# passing in None for summary_op to avoid a summary_thread being started.",
            "# Running summaries and training operations in parallel could run out of",
            "# GPU memory.",
            "-      summary_writer = tf.train.SummaryWriter(\"tensorboard_%d\" %(ctx.worker_num), graph=tf.get_default_graph())",
            "+      summary_writer = tf.summary.FileWriter(\"tensorboard_%d\" %(ctx.worker_num), graph=tf.get_default_graph())",
            "sv = tf.train.Supervisor(is_chief=is_chief,",
            "logdir=FLAGS.train_dir,",
            "init_op=init_op,"
        ]
    },
    {
        "number": 1193,
        "comments": "",
        "commit_message": "Fix encoders.py\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class RNN(torch.nn.Module):",
            "def __init__(self, idim, elayers, cdim, hdim, dropout, typ=\"blstm\"):",
            "super(RNN, self).__init__()",
            "bidir = typ[0] == \"b\"",
            "-        self.nblstm = torch.nn.LSTM(idim, cdim, elayers, batch_first=True,",
            "-                                    dropout=dropout, bidirectional=bidir) if \"lstm\" in typ \\",
            "+        self.nbrnn = torch.nn.LSTM(idim, cdim, elayers, batch_first=True,",
            "+                                   dropout=dropout, bidirectional=bidir) if \"lstm\" in typ \\",
            "else torch.nn.GRU(idim, cdim, elayers, batch_first=True, dropout=dropout,",
            "bidirectional=bidir)",
            "if bidir:"
        ]
    },
    {
        "number": 1195,
        "comments": "",
        "commit_message": "Fix examples and tutorials to use the updated tensor API (#886)\n\n* Fix examples and tutorials to use the updated tensor API\n\n* small fixes\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class PyroVAEImpl(VAE):",
            "",
            "def model(self, data):",
            "decoder = pyro.module('decoder', self.vae_decoder)",
            "-        z_mean, z_std = ng_zeros([data.size(0), 20]), ng_ones([data.size(0), 20])",
            "+        z_mean, z_std = torch.zeros([data.size(0), 20]), torch.ones([data.size(0), 20])",
            "with pyro.iarange('data', data.size(0)):",
            "z = pyro.sample('latent', Normal(z_mean, z_std).reshape(extra_event_dims=1))",
            "img = decoder.forward(z)"
        ]
    },
    {
        "number": 1196,
        "comments": "",
        "commit_message": "Fix docs links to PyTorch documentation (#856)\n\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "for _name, _Dist in torch.distributions.__dict__.items():",
            "locals()[_name] = _PyroDist",
            "",
            "_PyroDist.__doc__ = '''",
            "-    Wraps :class:`torch.distributions.{}` with",
            "+    Wraps :class:`{}.{}` with",
            ":class:`~pyro.distributions.torch_distribution.TorchDistributionMixin`.",
            "-    '''.format(_Dist.__name__)",
            "+    '''.format(_Dist.__module__, _Dist.__name__)",
            "",
            "__all__.append(_name)"
        ]
    },
    {
        "number": 1197,
        "comments": "",
        "commit_message": "Fix momentum and epsilon values (#19454)\n\nThe momentum value for PyTorch and TensorFlow batch normalization layers is not equivalent. The TensorFlow value should be (1 - pytorch_momentum) in order to ensure the correct updates are applied to the running mean and running variance calculations. We wouldn't observe a difference loading a pretrained model and performing inference, but evaluation outputs would change after some training steps.\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TFData2VecVisionForSemanticSegmentation(TFData2VecVisionPreTrainedModel):",
            "# FPNs",
            "self.fpn1 = [",
            "tf.keras.layers.Conv2DTranspose(config.hidden_size, kernel_size=2, strides=2, name=\"fpn1.0\"),",
            "-            tf.keras.layers.BatchNormalization(name=\"fpn1.1\"),",
            "+            tf.keras.layers.BatchNormalization(name=\"fpn1.1\", momentum=0.9, epsilon=1e-5),",
            "tf.keras.layers.Activation(\"gelu\"),",
            "tf.keras.layers.Conv2DTranspose(config.hidden_size, kernel_size=2, strides=2, name=\"fpn1.3\"),",
            "]"
        ]
    },
    {
        "number": 1198,
        "comments": "",
        "commit_message": "Tutorial fixing (#635)\n\n* TF bug fixing in Tutorials\n\n* Error fix in #476\n\n* Issue with Flags in Tutorials Fixed\n\n* Missing import fixed\n\n* Changelog Update\n\n* VGG19 import error fix\n\n* Error fixing in VGG tutorials\n\n* TFRecord Shape Error Fix\n\n* Sess Initialization Error Fix\n\n* Squeezenet model loading from \"models\" dir\n\n* PTB tutorials import issue fixed\n\n* mobilenet load from dir \"models\"\n\n* YAPF error fix\n\n* Missing Import fixed\n\n* Various Fixes on Tutorials\n\n* YAPF error correct\n\n* Update CHANGELOG.md\n\n* update VGG16 tutorial, auto download model\n\n* Python 3 Unicode Encoding Error\n\n* Deprecation Warning Fix\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def main(_):",
            "optimizer = tf.train.GradientDescentOptimizer(lr)",
            "train_op = optimizer.apply_gradients(zip(grads, tvars))",
            "",
            "-    # sess.run(tf.initialize_all_variables())",
            "+    # sess.run(tf.global_variables_initializer())",
            "tl.layers.initialize_global_variables(sess)",
            "",
            "net.print_params()"
        ]
    },
    {
        "number": 1199,
        "comments": "",
        "commit_message": "[Flax] Fix sample batch size DreamBooth  (#1129)\n\nfix sample batch size\n",
        "label": "",
        "answer": "no",
        "change": [
            "def main():",
            "logger.info(f\"Number of class images to sample: {num_new_images}.\")",
            "",
            "sample_dataset = PromptDataset(args.class_prompt, num_new_images)",
            "-            sample_dataloader = torch.utils.data.DataLoader(sample_dataset, batch_size=args.sample_batch_size)",
            "+            total_sample_batch_size = args.sample_batch_size * jax.local_device_count()",
            "+            sample_dataloader = torch.utils.data.DataLoader(sample_dataset, batch_size=total_sample_batch_size)",
            "",
            "for example in tqdm(",
            "sample_dataloader, desc=\"Generating class images\", disable=not jax.process_index() == 0"
        ]
    },
    {
        "number": 1200,
        "comments": "",
        "commit_message": "Spacy token indexer (#3040)\n\n* add a tokenizer to ud\n\n* add spacy indexer\n\n* allow token_indexers to specify their own type\n\n* dumb hack to allow a whitespace spacy tokenizer...\n\n* pass through token embedder\n\n* add ndarray to TokenType, tests for pass through embedder\n\n* add doc\n\n* remove todo, test\n\n* fix docs\n\n* why is this test flaky\n\n* fix the correct test\n\n* add as_padded_tensor method\n\n* better place for depreciation stuff\n\n* add warning for calling inherited get_padding_token\n\n* ignore type for backward compatability\n\n* mattg comments\n\n* pylint\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TokenCharactersIndexer(TokenIndexer[List[int]]):",
            "# Removes the \"dummy token\".",
            "padded_tokens.pop()",
            "# Truncates all the tokens to the desired length, and return the result.",
            "-        return {key: [list(token[:desired_token_length]) for token in padded_tokens]}",
            "+        return {key: torch.LongTensor([list(token[:desired_token_length])",
            "+                                       for token in padded_tokens])}"
        ]
    },
    {
        "number": 1201,
        "comments": "",
        "commit_message": "Fix tests failing on CUDA (#1834)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class LKJCorrCholesky(TorchDistribution):",
            "Km1 = self._d - 1",
            "",
            "log_diagonals = x.diagonal(offset=0, dim1=-1, dim2=-2)[..., 1:].log()",
            "+        # TODO: Figure out why the `device` kwarg to torch.linspace seems to not work in certain situations,",
            "+        # and a seemingly redundant .to(x.device) is needed below.",
            "values = log_diagonals * torch.linspace(start=Km1 - 1, end=0, steps=Km1,",
            "dtype=x.dtype,",
            "-                                                device=x.device).expand_as(log_diagonals)",
            "+                                                device=x.device).expand_as(log_diagonals).to(x.device)",
            "",
            "values += log_diagonals.mul(eta.mul(2).add(-2.0))",
            "values = values.sum(-1) + lp"
        ]
    },
    {
        "number": 1202,
        "comments": "",
        "commit_message": "Fix redundant normalization of OWL-ViT text embeddings (#19712)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class OwlViTModel(OwlViTPreTrainedModel):",
            "if return_base_image_embeds:",
            "last_hidden_state = vision_outputs[0]",
            "image_embeds = self.vision_model.post_layernorm(last_hidden_state)",
            "+        else:",
            "+            image_embeds = image_embeds_norm",
            "+            text_embeds = text_embeds_norm",
            "",
            "if not return_dict:",
            "output = (logits_per_image, logits_per_text, text_embeds, image_embeds, text_outputs, vision_outputs)"
        ]
    },
    {
        "number": 1204,
        "comments": "",
        "commit_message": "formatting fixes for Array API submodule in backend APIs.\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def isnan(x: torch.Tensor)\\",
            "return torch.isnan(x)",
            "",
            "",
            "-def less(x1: torch.Tensor,x2: torch.Tensor):",
            "-    if hasattr(x1,'dtype') and hasattr(x2,'dtype'):",
            "-        promoted_type = torch.promote_types(x1.dtype,x2.dtype)",
            "+def less(x1: torch.Tensor, x2: torch.Tensor):",
            "+    if hasattr(x1, 'dtype') and hasattr(x2, 'dtype'):",
            "+        promoted_type = torch.promote_types(x1.dtype, x2.dtype)",
            "x1 = x1.to(promoted_type)",
            "x2 = x2.to(promoted_type)",
            "-    return torch.lt(x1,x2)",
            "+    return torch.lt(x1, x2)",
            "",
            "",
            "def cos(x: torch.Tensor)\\"
        ]
    },
    {
        "number": 1205,
        "comments": "",
        "commit_message": "Change rnn-cell to fix #103 (#104)\n\n* Change rnn-cell to fix #103\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Model(ModelDesc):",
            "feat, labelidx, labelvalue, labelshape, seqlen = input_vars",
            "label = tf.SparseTensor(labelidx, labelvalue, labelshape)",
            "",
            "-        cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=HIDDEN)",
            "-        cell = tf.nn.rnn_cell.MultiRNNCell([cell] * NLAYER)",
            "+        cell = tf.contrib.rnn.BasicLSTMCell(num_units=HIDDEN)",
            "+        cell = tf.contrib.rnn.MultiRNNCell([cell] * NLAYER)",
            "",
            "initial = cell.zero_state(tf.shape(feat)[0], tf.float32)"
        ]
    },
    {
        "number": 1208,
        "comments": "",
        "commit_message": "lintfixbot: Auto-commit fixed lint errors in codebase\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "eigh.support_native_out = True",
            "",
            "@with_unsupported_dtypes({\"1.11.0 and below\": (\"float16\", \"bfloat16\")}, backend_version)",
            "def eigvalsh(",
            "-    x: torch.Tensor,",
            "-    /,",
            "-    *,",
            "-    UPLO: Optional[str] = \"L\",",
            "-    out: Optional[torch.Tensor] = None",
            "+    x: torch.Tensor, /, *, UPLO: Optional[str] = \"L\", out: Optional[torch.Tensor] = None",
            ") -> torch.Tensor:",
            "return torch.linalg.eigvalsh(x, UPLO=UPLO, out=out)"
        ]
    },
    {
        "number": 1209,
        "comments": "",
        "commit_message": "Fix broken centered mode in RMSProp and add tests.\n\nPiperOrigin-RevId: 256535597\nChange-Id: I00f58289a75683b3eb83159546e86f070bab3dea\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class RMSProp(base.Module):",
            "ms.assign(tf.square(update) * (1. - decay) + ms * decay)",
            "if self.centered:",
            "mg.assign(update * (1. - decay) + mg * decay)",
            "-          denominator = ms - mg + epsilon",
            "+          denominator = ms - tf.square(mg) + epsilon",
            "else:",
            "denominator = ms + epsilon",
            "mom.assign(momentum * mom + ("
        ]
    },
    {
        "number": 1210,
        "comments": "",
        "commit_message": "fix tf master 'is_sequence' util\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def _linear(args, output_size, bias, bias_start=0.0, weights_init=None,",
            "Raises:",
            "ValueError: if some of the arguments has unspecified or wrong shape.",
            "\"\"\"",
            "-    if args is None or (_rnn_cell._is_sequence(args) and not args):",
            "+    if args is None or (is_sequence(args) and not args):",
            "raise ValueError(\"`args` must be specified\")",
            "-    if not _rnn_cell._is_sequence(args):",
            "+    if not is_sequence(args):",
            "args = [args]",
            "",
            "# Calculate the total size of arguments on dimension 1."
        ]
    },
    {
        "number": 1212,
        "comments": "",
        "commit_message": "Fixed problems with exploration. (#289)\n\n* Cleaned up Agent child and child-classes:\n- Made sure no variables defined  in child classes are required by parent class.\n- Moved some variables shared amongst all types of Agent classes into base Agent class.\n- TODO: Need to a) Move more variables into base Agent class (now defined by each child class, e.g. discount) or b) create intermediary classes e.g. ModelBasedAgent. Needs to be discussed amongst devs.\n- Removed redundant docstring content, e.g. if variable is already defined by a parent class' c'tor and has the exact same functionality there.\n\n* Cleaned up Agent child and child-classes:\n- Made sure no variables defined  in child classes are required by parent class.\n- Moved some variables shared amongst all types of Agent classes into base Agent class.\n- TODO: Need to a) Move more variables into base Agent class (now defined by each child class, e.g. discount) or b) create intermediary classes e.g. ModelBasedAgent. Needs to be discussed amongst devs.\n- Removed redundant docstring content, e.g. if variable is already defined by a parent class' c'tor and has the exact same functionality there.\n\n* Moved `discount` into base Agent class.\n\n* Added LearningAgent class to hold all variables necessary for learning with an optimizable model.\nAll Agent child classes inherit from LearningAgent, except the non-learning ones (RandomAgent and ConstantAgent).\n\nIntroducing the new class (LearningAgent) removed lots of redundant c'tor and initialization code.\n\nObsoleted DDQNAgent (same as DQN with double_q_model==True). Gives an informative warning now (still backwards compatible).\n\n* Cleaned up Agent child and child-classes:\n- Made sure no variables defined  in child classes are required by parent class.\n- Moved some variables shared amongst all types of Agent classes into base Agent class.\n- TODO: Need to a) Move more variables into base Agent class (now defined by each child class, e.g. discount) or b) create intermediary classes e.g. ModelBasedAgent. Needs to be discussed amongst devs.\n- Removed redundant docstring content, e.g. if variable is already defined by a parent class' c'tor and has the exact same functionality there.\n\n* Moved `discount` into base Agent class.\n\n* Added LearningAgent class to hold all variables necessary for learning with an optimizable model.\nAll Agent child classes inherit from LearningAgent, except the non-learning ones (RandomAgent and ConstantAgent).\n\nIntroducing the new class (LearningAgent) removed lots of redundant c'tor and initialization code.\n\nObsoleted DDQNAgent (same as DQN with double_q_model==True). Gives an informative warning now (still backwards compatible).\n\n* Fixed travis import errors LearningAgent.\n\n* - Added LearningAgent properly to agents/__init__.py\n\n* Fixed various pytest failures.\nMoved 'scope' back into base Agent (needed by Random and ConstantAgents).\nRemoved  unnecessary parameters from Random and Constant (non-learning) Agents.\n\n* Fixed action_exploration in `Model` and `Exploration` classes.\n\n* Fixed action_exploration in `Model` and `Exploration` classes.\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class EpsilonDecay(Exploration):",
            "epsilon = self.final_epsilon + (2 ** (-half_life_ratio)) * (self.initial_epsilon - self.final_epsilon)",
            "return epsilon",
            "",
            "-        pred = tf.logical_or(x=(timestep < self.start_timestep), y=(timestep > self.start_timestep + self.timesteps))",
            "+        pred = tf.logical_or(x=(timestep < self.start_timestep),",
            "+                             y=(timestep > self.start_timestep + int(self.timesteps)))",
            "return tf.cond(pred=pred, true_fn=true_fn, false_fn=false_fn)"
        ]
    },
    {
        "number": 1213,
        "comments": "",
        "commit_message": "fix efficientnet\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "ADAM = int(os.getenv(\"ADAM\", 0))",
            "if __name__ == \"__main__\":",
            "print(f\"NUM:{NUM} BS:{BS} CNT:{CNT}\")",
            "model = EfficientNet(NUM, classes=1000, has_se=False, track_running_stats=False)",
            "-  parameters = get_parameters(model)",
            "+  parameters = optim.get_parameters(model)",
            "for p in parameters: p.realize()",
            "if ADAM: optimizer = optim.Adam(parameters, lr=0.001)",
            "else: optimizer = optim.SGD(parameters, lr=0.001)"
        ]
    },
    {
        "number": 1214,
        "comments": "",
        "commit_message": "pylint -> flake8 (#3288)\n\n* pylint\n\n* update pylint\n\n* undo a lot of the raise / else\n\n* add bound on typed-ast\n\n* first mypy fixes\n\n* new flag\n\n* fix mypy errors\n\n* requirements.txt\n\n* pylint -> flake8\n\n* mypy 0.720 -> mypy 0.730\n\n* add back erroneously removed initial newline\n\n* remove .pylintrc\n\n* remove pylintrc from Dockerfile\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestTrain(AllenNlpTestCase):",
            "train_model(params(), serialization_dir=serialization_dir)",
            "archive = load_archive(str(serialization_dir / \"model.tar.gz\"))",
            "model = archive.model",
            "-        assert model.forward(torch.tensor([1, 2, 3]))[\"class\"] == torch.tensor(98) # pylint: disable=not-callable",
            "+        assert model.forward(torch.tensor([1, 2, 3]))[\"class\"] == torch.tensor(98)",
            "assert model.vocab.get_vocab_size() == 9"
        ]
    },
    {
        "number": 1215,
        "comments": "",
        "commit_message": "fix shape error on cuda (#1385)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class EKFState(object):",
            "S = H.mm(P).mm(H.transpose(-1, -2)) + R  # innovation cov",
            "",
            "K_prefix = self._cov.mm(H.transpose(-1, -2))",
            "-        dx = K_prefix.mm(torch.gesv(dz, S)[0]).squeeze(1)  # K*dz",
            "+        dx = K_prefix.mm(torch.gesv(dz.unsqueeze(1), S)[0]).squeeze(1)  # K*dz",
            "x = self._dynamic_model.geodesic_difference(x, -dx)",
            "",
            "I = eye_like(x, self._dynamic_model.dimension)  # noqa: E741"
        ]
    },
    {
        "number": 1216,
        "comments": "",
        "commit_message": "Warn on invalid node and edge type names in `HeteroData` (#5990)\n\nFixes #4150\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_my_conv():",
            "assert torch.allclose(conv((x1, x2), adj.t()), out1)",
            "assert torch.allclose(conv((x1, x2), torch_adj.t()), out1)",
            "assert torch.allclose(conv((x1, None), adj.t()), out2)",
            "-    assert torch.allclose(conv((x1, None), torch_adj.t()), out2)",
            "+    assert torch.allclose(conv((x1, None), torch_adj.t()), out2, atol=1e-6)",
            "conv.fuse = False",
            "assert torch.allclose(conv((x1, x2), adj.t()), out1)",
            "assert torch.allclose(conv((x1, x2), torch_adj.t()), out1)"
        ]
    },
    {
        "number": 1218,
        "comments": "",
        "commit_message": "[Metrics] AUROC error on multilabel + improved testing (#3350)\n\n* error on multilabel\n\n* fix tests\n\n* fix pep8\n\n* changelog\n\n* update doc test\n\n* fix doctest\n\n* fix doctest\n\n* update from suggestion\n\n* Apply suggestions from code review\n\nCo-authored-by: Adrian W\u00e4lchli <aedu.waelchli@gmail.com>\n\n* Update test_classification.py\n\n* Update test_classification.py\n\n* retrigger test\n\n* 'pep8\n\nCo-authored-by: Adrian W\u00e4lchli <aedu.waelchli@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_average_precision(pos_label):",
            "assert isinstance(ap, torch.Tensor)",
            "",
            "",
            "-@pytest.mark.parametrize('pos_label', [1, 2])",
            "+@pytest.mark.parametrize('pos_label', [0, 1])",
            "def test_auroc(pos_label):",
            "auroc = AUROC(pos_label=pos_label)",
            "assert auroc.name == 'auroc'",
            "",
            "-    pred, target = torch.tensor([1, 2, 3, 4]), torch.tensor([1, 2, 0, 1])",
            "+    pred, target = torch.tensor([1, 2, 3, 4]), torch.tensor([1, 1, 0, 1])",
            "area = auroc(pred=pred, target=target, sample_weight=[0.1, 0.2, 0.3, 0.4])",
            "assert isinstance(area, torch.Tensor)"
        ]
    },
    {
        "number": 1219,
        "comments": "",
        "commit_message": "fix docstring\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TransformerLM(nn.Module, LMInterface):",
            "m = subsequent_mask(ys_mask.size(-1), device=ys_mask.device).unsqueeze(0)",
            "return ys_mask.unsqueeze(-2) & m",
            "",
            "-    def forward(self, x: torch.Tensor, t: torch.Tensor):",
            "+    def forward(self, x: torch.Tensor, t: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:",
            "xm = (x != 0)",
            "h, _ = self.encoder(x, self.target_mask(x))",
            "y = self.decoder(h)"
        ]
    },
    {
        "number": 1222,
        "comments": "",
        "commit_message": "upgrade to pytorch 0.4.1 + make work with python 3.7 (but still 3.6 also) (#1543)\n\n* changes for pytorch 0.4.1\n\n* increase tolerance for srl test\n\n* update versions in setup.py\n\n* add script to check requirements.txt vs setup.py + fix setup.py\n\n* loosen bounds on pytorch version\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class NlvrDecoderStep(DecoderStep[NlvrDecoderState]):",
            "action_query = torch.cat([hidden_state, attended_sentence], dim=-1)",
            "# (group_size, action_embedding_dim)",
            "predicted_action_embedding = self._output_projection_layer(action_query)",
            "-        predicted_action_embedding = self._dropout(torch.nn.functional.tanh(predicted_action_embedding))",
            "+        predicted_action_embedding = self._dropout(torch.tanh(predicted_action_embedding))",
            "if state.checklist_state[0] is not None:",
            "embedding_addition = self._get_predicted_embedding_addition(state)",
            "addition = embedding_addition * self._checklist_embedding_multiplier"
        ]
    },
    {
        "number": 1223,
        "comments": "",
        "commit_message": "Bug fix\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class DistributedFusedAdam(torch.optim.Optimizer):",
            "grp = torch.distributed.new_group(ranks=ranks)",
            "if torch.distributed.get_rank() in ranks:",
            "self._ag_pg.append(grp)",
            "-            self._ag_st = [torch.cuda.Stream()]*self._num_ag_pg",
            "+            self._ag_st = [torch.cuda.Stream() for _ in range(self._num_ag_pg)]",
            "self._l2_grad_norm_st = torch.cuda.Stream() if self._compute_L2_grad_norm else None",
            "self._completion_st = torch.cuda.Stream()"
        ]
    },
    {
        "number": 1227,
        "comments": "",
        "commit_message": "Fix notebooks.\n\nPiperOrigin-RevId: 305891180\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "\"#@title Load the Universal Sentence Encoder's TF Hub module\\n\",",
            "\"from absl import logging\\n\",",
            "\"\\n\",",
            "-        \"import tensorflow.compat.v1 as tf\\n\",",
            "-        \"tf.disable_v2_behavior()\\n\",",
            "+        \"import tensorflow as tf\\n\",",
            "\"\\n\",",
            "\"import tensorflow_hub as hub\\n\",",
            "\"import matplotlib.pyplot as plt\\n\","
        ]
    },
    {
        "number": 1228,
        "comments": "",
        "commit_message": "fixes and clean-up\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class ESPnetASRTransducerModel(AbsESPnetModel):",
            "self.use_auxiliary_lm_loss = self.training and auxiliary_lm_loss_weight > 0",
            "",
            "if self.use_auxiliary_ctc:",
            "-            self.ctc_lin = torch.nn.Linear(encoder.output_size(), vocab_size)",
            "+            self.ctc_lin = torch.nn.Linear(encoder.dim_output, vocab_size)",
            "self.ctc_dropout_rate = auxiliary_ctc_dropout_rate",
            "",
            "if self.use_auxiliary_lm_loss:",
            "-            self.lm_lin = torch.nn.Linear(decoder.dunits, vocab_size)",
            "+            self.lm_lin = torch.nn.Linear(decoder.dim_output, vocab_size)",
            "",
            "self.lm_loss_smoothing = auxiliary_lm_loss_smoothing"
        ]
    },
    {
        "number": 1229,
        "comments": "",
        "commit_message": "Fix TFEncoderDecoderModelTest - Pytorch device (#15979)\n\n* fix device\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TFEncoderDecoderMixin:",
            "self.assertEqual(len(tf_outputs_loaded), len(pt_outputs), \"Output lengths differ between TF and PyTorch\")",
            "",
            "for tf_output_loaded, pt_output in zip(tf_outputs_loaded, pt_outputs):",
            "-            self.assert_almost_equals(tf_output_loaded.numpy(), pt_output.numpy(), 1e-3)",
            "+            self.assert_almost_equals(tf_output_loaded.numpy(), pt_output.detach().to(\"cpu\").numpy(), 1e-3)",
            "",
            "def check_equivalence_pt_to_tf(self, config, decoder_config, inputs_dict):"
        ]
    },
    {
        "number": 1230,
        "comments": "",
        "commit_message": "Deprecate prepare_module (#3166)\n\n* Refactor prepare_module\n\n* Add deprecation warning in prepare_module\n\n* Remove prepare_module in inspect\n\n* Remove prepare_module in patching\n\n* Remove prepare_module in dummy_data\n\n* Remove prepare_module in run_beam\n\n* Remove prepare_module in test_dataset_common\n\n* Fix hash in run_beam\n\n* Remove prepare_module from test_load\n\n* Remove prepare_module from test_metric_common\n\n* Remove prepare_module from test_hf_gcp\n\n* Use deprecated function instead\n\n* Add deprecation to docstring\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class patch_submodule:",
            "Examples:",
            "",
            ">>> import importlib",
            "-        >>> from datasets.load import prepare_module",
            "+        >>> from datasets.load import dataset_module_factory",
            ">>> from datasets.streaming import patch_submodule, xjoin",
            ">>>",
            "-        >>> snli_module_path, _ = prepare_module(\"snli\")",
            "-        >>> snli_module = importlib.import_module(snli_module_path)",
            "+        >>> dataset_module = dataset_module_factory(\"snli\")",
            "+        >>> snli_module = importlib.import_module(dataset_module.module_path)",
            ">>> patcher = patch_submodule(snli_module, \"os.path.join\", xjoin)",
            ">>> patcher.start()",
            ">>> assert snli_module.os.path.join is xjoin"
        ]
    },
    {
        "number": 1231,
        "comments": "",
        "commit_message": "feat(graphstore): support `num_nodes`, enabling `Tuple[FeatureStore, GraphStore]` in `LightningLinkData` (#5270)\n\n* init\n\n* test\n\n* fix\n\n* :(\n\n* return type\n\n* changelog\n\n* minor\n\n* major\n\n* Update torch_geometric/data/hetero_data.py\n\n* Update torch_geometric/data/hetero_data.py\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class MyFeatureStore(FeatureStore):",
            "and attr.index == slice(None, None, None)):",
            "return tensor",
            "",
            "-        idx = torch.cat([(index == v).nonzero() for v in attr.index]).view(-1)",
            "+        idx = (torch.cat([(index == v).nonzero() for v in attr.index]).view(-1)",
            "+               if attr.index.numel() > 0 else [])",
            "return tensor[idx]",
            "",
            "def _remove_tensor(self, attr: TensorAttr) -> bool:"
        ]
    },
    {
        "number": 1233,
        "comments": "",
        "commit_message": "GH-2882: fix trainling comma at the end of the sentence\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_transformer_jit_embeddings(results_base_path):",
            "",
            "tensors = base_embeddings.prepare_tensors([sentence])",
            "# ensure that the prepared tensors is what we expect",
            "-    assert sorted(tensors.keys()) == [\"attention_mask\", \"input_ids\", \"overflow_to_sample_mapping\", \"word_ids\"]",
            "+    assert sorted(tensors.keys()) == [",
            "+        \"attention_mask\",",
            "+        \"input_ids\",",
            "+        \"lengths\",",
            "+        \"overflow_to_sample_mapping\",",
            "+        \"word_ids\",",
            "+    ]",
            "",
            "wrapper = JitWrapper(base_embeddings)",
            "parameter_names, parameter_list = TransformerJitWordEmbeddings.parameter_to_list("
        ]
    },
    {
        "number": 1235,
        "comments": "",
        "commit_message": "[Docs] Models (#416)\n\n* docs for attention\n\n* types for embeddings\n\n* unet2d docstrings\n\n* UNet2DConditionModel docstrings\n\n* fix typos\n\n* style and vq-vae docstrings\n\n* docstrings  for VAE\n\n* Update src/diffusers/models/unet_2d.py\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* make style\n\n* added inherits from sentence\n\n* docstring to forward\n\n* make style\n\n* Apply suggestions from code review\n\nCo-authored-by: Pedro Cuenca <pedro@huggingface.co>\n\n* finish model docs\n\n* up\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\nCo-authored-by: Pedro Cuenca <pedro@huggingface.co>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Timesteps(nn.Module):",
            "class GaussianFourierProjection(nn.Module):",
            "\"\"\"Gaussian Fourier embeddings for noise levels.\"\"\"",
            "",
            "-    def __init__(self, embedding_size=256, scale=1.0):",
            "+    def __init__(self, embedding_size: int = 256, scale: float = 1.0):",
            "super().__init__()",
            "self.weight = nn.Parameter(torch.randn(embedding_size) * scale, requires_grad=False)"
        ]
    },
    {
        "number": 1236,
        "comments": "",
        "commit_message": "Fix tests on CUDA (#2098)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "from kornia.testing import assert_close",
            "class TestOneHot:",
            "def test_smoke(self, device, dtype):",
            "num_classes = 4",
            "-        labels = torch.zeros(2, 2, 1, dtype=torch.int64)",
            "+        labels = torch.zeros(2, 2, 1, dtype=torch.int64, device=device)",
            "labels[0, 0, 0] = 0",
            "labels[0, 1, 0] = 1",
            "labels[1, 0, 0] = 2"
        ]
    },
    {
        "number": 1237,
        "comments": "",
        "commit_message": "Black preview (#17217)\n\n* Black preview\n\n* Fixup too!\n\n* Fix check copies\n\n* Use the same version as the CI\n\n* Bump black\n",
        "label": "",
        "answer": "no",
        "change": [
            "class CanineModelTest(ModelTesterMixin, unittest.TestCase):",
            "torch.allclose(",
            "set_nan_tensor_to_zero(tuple_object), set_nan_tensor_to_zero(dict_object), atol=1e-5",
            "),",
            "-                            msg=f\"Tuple and dict output are not equal. Difference: {torch.max(torch.abs(tuple_object - dict_object))}. Tuple has `nan`: {torch.isnan(tuple_object).any()} and `inf`: {torch.isinf(tuple_object)}. Dict has `nan`: {torch.isnan(dict_object).any()} and `inf`: {torch.isinf(dict_object)}.\",",
            "+                            msg=(",
            "+                                \"Tuple and dict output are not equal. Difference:\"",
            "+                                f\" {torch.max(torch.abs(tuple_object - dict_object))}. Tuple has `nan`:\"",
            "+                                f\" {torch.isnan(tuple_object).any()} and `inf`: {torch.isinf(tuple_object)}. Dict has\"",
            "+                                f\" `nan`: {torch.isnan(dict_object).any()} and `inf`: {torch.isinf(dict_object)}.\"",
            "+                            ),",
            ")",
            "",
            "recursive_check(tuple_output, dict_output)"
        ]
    },
    {
        "number": 1241,
        "comments": "",
        "commit_message": "revert padding bug fix for now\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def sample_autoregressive(partial_sequences,",
            "if has_partial_sequences and remove_partial_sequences:",
            "# remove partial sequences from outputs",
            "partial_length = mtf.reduce_sum(",
            "-            mtf.to_int32(mtf.not_equal(partial_sequences, padding_id)),",
            "+            mtf.to_int32(mtf.not_equal(partial_sequences, 0)),",
            "reduced_dim=length_dim)",
            "outputs = mtf.dynamic_shift(",
            "outputs, -partial_length, length_dim, wrap=False)"
        ]
    },
    {
        "number": 1242,
        "comments": "",
        "commit_message": "Fix flake8 format problems.\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class ConvolutionBlock(nn.Module):",
            "self.act = activation",
            "",
            "def forward(self, x):",
            "+        \"\"\"Compute Covolution Block",
            "+",
            "+        :param torch.Tensor x: (batch, time, size)",
            "+        :return torch.Tensor: convoluted `value` (batch, time, d_model)",
            "+        \"\"\"",
            "# exchange the temporal dimension and the feature dimension",
            "# pad the input from (batch, len, dim) to (batch, dim, len+(k-1))",
            "x = self.pad_left(x.transpose(1, 2))"
        ]
    },
    {
        "number": 1243,
        "comments": "",
        "commit_message": "Add hypernetwork support to split cross attention v1\n\n* Add hypernetwork support to split_cross_attention_forward_v1\n* Fix device check in esrgan_model.py to use devices.device_esrgan instead of shared.device\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class UpscalerESRGAN(Upscaler):",
            "print(\"Unable to load %s from %s\" % (self.model_path, filename))",
            "return None",
            "",
            "-        pretrained_net = torch.load(filename, map_location='cpu' if shared.device.type == 'mps' else None)",
            "+        pretrained_net = torch.load(filename, map_location='cpu' if devices.device_esrgan.type == 'mps' else None)",
            "crt_model = arch.RRDBNet(3, 3, 64, 23, gc=32)",
            "",
            "pretrained_net = fix_model_layers(crt_model, pretrained_net)"
        ]
    },
    {
        "number": 1244,
        "comments": "",
        "commit_message": "fix test for v0.3\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "for epoch in range(num_epochs):",
            "if j % embedding_log == 0:",
            "print(\"loss_value:{}\".format(loss_value.data[0]))",
            "#we need 3 dimension for tensor to visualize it!",
            "-            out = torch.cat((out, torch.ones(len(out), 1)), 1)",
            "-            writer.add_embedding(out.data, metadata=label_batch.data, label_img=data_batch.data, global_step=n_iter)",
            "+            out = torch.cat((out.data, torch.ones(len(out), 1)), 1)",
            "+            writer.add_embedding(out, metadata=label_batch.data, label_img=data_batch.data, global_step=n_iter)",
            "",
            "writer.close()"
        ]
    },
    {
        "number": 1245,
        "comments": "",
        "commit_message": "CDN urls (#4030)\n\n* [file_utils] use_cdn + documentation\n\n* Move to cdn. urls for weights\n\n* [urls] Hotfix for bert-base-japanese\n",
        "label": "",
        "answer": "no",
        "change": [
            "from .modeling_utils import PreTrainedModel",
            "logger = logging.getLogger(__name__)",
            "",
            "TRANSFO_XL_PRETRAINED_MODEL_ARCHIVE_MAP = {",
            "-    \"transfo-xl-wt103\": \"https://s3.amazonaws.com/models.huggingface.co/bert/transfo-xl-wt103-pytorch_model.bin\",",
            "+    \"transfo-xl-wt103\": \"https://cdn.huggingface.co/transfo-xl-wt103-pytorch_model.bin\",",
            "}"
        ]
    },
    {
        "number": 1248,
        "comments": "",
        "commit_message": "Abstract accelerator (step 3) (#2677)\n\n* Integrate accelerator abstraction interface into deepspeed/\n\n* Fix error message in fp16/fused_optimizer\n\n* fix error message in fp16/unfused_optimizer.py\n\n* assign get_accelerator().pin_memory() result to input Tensor name\n\n* no need to check cuda and whether nvtx supported\n\n* move try-except into inner most block\n\n* call Event() and Stream() in get_accelerator() for data type\n\n* Make Stream and Event as properties of abstract interface so they can be used as data type in deepspeed\n\n* Apply op_builder backend api change from #2705 from @jeffra\n\n* fix tests where Builder NAME is used\n\n* keep original ...Builder.NAME interface instead of ...Builder().NAME interface\n\n* fix builder closure for installation\n\n* fix randomltd builder\n\n* add comments to clarify create_op_builder and get_op_builder\n\n* fix compatibility with pip install -e\n\nCo-authored-by: Cheng Li <pistasable@gmail.com>\nCo-authored-by: Olatunji Ruwase <olruwase@microsoft.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class DeepSpeedZeRoOffload(object):",
            "self._prefetch_bucket_sz = int(prefetch_bucket_size)",
            "self._max_reuse_distance_in_numel = int(max_reuse_distance)",
            "self._max_available_parameters_in_numel = int(max_live_parameters)",
            "-        self.__allgather_stream = Stream(",
            "-        ) if overlap_comm else torch.cuda.default_stream()",
            "+        self.__allgather_stream = get_accelerator().Stream(",
            "+        ) if overlap_comm else get_accelerator().default_stream()",
            "",
            "self.forward_hooks = []",
            "self.backward_hooks = []"
        ]
    },
    {
        "number": 1249,
        "comments": "",
        "commit_message": "Fix some comments and tokenizer outputs from P-Tuning to RLHF\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "if __name__ == \"__main__\":",
            "rl_training = False",
            "actor_training = False",
            "",
            "-    device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")",
            "+    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")",
            "+    # place here the path to the config.yaml file",
            "config_path = \"/home/pierpaolo/Documents/optimapi/ptuning/config.yaml\"",
            "",
            "if reward_training:"
        ]
    },
    {
        "number": 1250,
        "comments": "",
        "commit_message": "fix example about dropout\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "MIN_AFTER_DEQUEUE = int(50000 * 0.4)",
            "CAPACITY = MIN_AFTER_DEQUEUE + 3 * BATCH_SIZE",
            "",
            "def get_model(inputs, is_training):",
            "-    #keep_prob = tf.constant(0.5 if is_training else 1.0)",
            "+    #keep_prob = tf.constant(0.5 if is_training else 0.0)",
            "",
            "image, label = inputs"
        ]
    },
    {
        "number": 1251,
        "comments": "",
        "commit_message": "Improve reproduceability 2/3 (#1906)\n\n* [Repro] Correct reproducability\n\n* up\n\n* up\n\n* uP\n\n* up\n\n* need better image\n\n* allow conversion from no state dict checkpoints\n\n* up\n\n* up\n\n* up\n\n* up\n\n* check tensors\n\n* check tensors\n\n* check tensors\n\n* check tensors\n\n* next try\n\n* up\n\n* up\n\n* better name\n\n* up\n\n* up\n\n* Apply suggestions from code review\n\n* correct more\n\n* up\n\n* replace all torch randn\n\n* fix\n\n* correct\n\n* correct\n\n* finish\n\n* fix more\n\n* up\n",
        "label": "",
        "answer": "no",
        "change": [
            "class LDMPipeline(DiffusionPipeline):",
            "True, otherwise a `tuple. When returning a tuple, the first element is a list with the generated images.",
            "\"\"\"",
            "",
            "-        latents = torch.randn(",
            "+        latents = randn_tensor(",
            "(batch_size, self.unet.in_channels, self.unet.sample_size, self.unet.sample_size),",
            "generator=generator,",
            ")"
        ]
    },
    {
        "number": 1252,
        "comments": "",
        "commit_message": "Release 0.1.0 (#609)\n\n* fix: download token file path (#556)\n\n* fix: new metrics usage in paramsearch and cross_validation (#553)\n\n* fix: new metrics usage in paramsearch and cross_validation\n\n* fix: remove incorrect import\n\n* feat: simple vocab refactored so it supports recursivity (#534)\n\n* feat: recursivity added to zero padding, check_str_batch added\n\n* feat: simple vocab refactored so it supports recursivity\n\n* fix: change check of type for numpy arrays to support uints\n\n* fix: fix get_dimensions for string batches\n\n* feat: add flatten_str_batch to utils\n\n* feat: add flattening to fit of the simple_vocab\n\n* fix: debug prints removed, chain bug fixed\n\n* fix: zero padding in recursion bug fixed\n\n* fix: remove char vocab from registry\n\n* fix: zero batch size issue fixed\n\n* fix: fix previous fix\n\n* feat: use simple_vocab in go_bot\n\n* fix: ner configs fix old char vocab\n\n* style: pep 8 imports order in core/data/utils.py\n\n* docs: remove outdated vocabs from docs\n\n* fix: remove outdated vocabs from registry\n\n* refactor: moved server/utils configs to designated folder (#546)\n\n* Moved configs to configs folder\n\n* Added default configs\n\n* Backup\n\n* Implemented cofig handling\n\n* Fix in paths.py\n\n* Added stateful and multi_instance params to MS Bot Framework config\n\n* Hotfix in default agent\n\n* Fixed utterances ids handling in default agent\n\n* Minor refactoring in MS BF server.py\n\n* Added MS Bot Framework app_id and app_secret params to server_config\n\n* Slighly fixed deep.py, stateful and multi_instance params moved to common_defaults section of config\n\n* Added https, key, cert params to server config\n\n* Added telegram token param to server config\n\n* Fixed config path retrival in test_quick_start.py\n\n* Changed download token file path to ~/.deeppavlov/token\n\n* Small fixes in get download token function\n\n* Fixed default server config\n\n* Minor fixes in paths.py\n\n* moved configs dir to utils/configs\n\n* Small fixes in configs\n\n* Updated readme and docs\n\n* Added ssh cert and key files check in rise api\n\n* Fixed configuration file path getting in alice.py\n\n* Changed naming of DP configuration to settings\n\n* Added https config params handling to alice.py\n\n* Fixed download token file path\n\n* fix: removed potential cycle dependencies at log.py (#563)\n\n* feat: notebook tutorial for classification (#558)\n\n* feature: notebook tutorial for classification\n\n* fix: fixes for classification tutorial\n\n* fix: fixes in classification notebook\n\n* fix: fixed some reviewed mistakes\n\n* feat: telegram agent (#561)\n\n* Moved configs to configs folder\n\n* Added default configs\n\n* Backup\n\n* Implemented cofig handling\n\n* Fix in paths.py\n\n* Added stateful and multi_instance params to MS Bot Framework config\n\n* Hotfix in default agent\n\n* Fixed utterances ids handling in default agent\n\n* Minor refactoring in MS BF server.py\n\n* Added MS Bot Framework app_id and app_secret params to server_config\n\n* Slighly fixed deep.py, stateful and multi_instance params moved to common_defaults section of config\n\n* Added https, key, cert params to server config\n\n* Added telegram token param to server config\n\n* Fixed config path retrival in test_quick_start.py\n\n* Changed download token file path to ~/.deeppavlov/token\n\n* Small fixes in get download token function\n\n* Fixed default server config\n\n* Minor fixes in paths.py\n\n* moved configs dir to utils/configs\n\n* Small fixes in configs\n\n* Updated readme and docs\n\n* Added ssh cert and key files check in rise api\n\n* Telegram wrapper (interactbot mode) now uses Agent entity. Also fixed multi model.out_params in default skill\n\n* Fixed configuration file path getting in alice.py\n\n* Changed naming of DP configuration to settings\n\n* Added https config params handling to alice.py\n\n* Fixed download token file path\n\n* refactor: python api (#551)\n\n* refactor: remove unnecessary utility functions\n\n* fix: inherit RedirectedPrints from contextlib.redirect_stdout\n\n* refactor: CosineSimilarityClassifier uses Serializable.__init__()\n\n* style: add typing for download.py\n\n* refactor: deep_download accepts config as dict\n\n* fix: fix errors found with code inspection\n\n* refactor: move find_config function away to core.common.file\n\n* feat: allow to download config's file requirements with build_model_from_config function\n\n* refactor: rename build_model_from_config to build_model\n\n* feat: add a config tree object\n\n* refactor: rename Sturct's _to_dict method to _asdict\nto conform with namedtuple's naming\n\n* fix: correct root directory for configs to search\n\n* feat: allow to import configs Struct and build_model fn from deeppavlov\n\n* docs: update python usage examples in model's docs\n\n* fix: do not raise in deeppavlov.__init__ because of absent requirements\n\n* fix: resolve merge conflict of recursive imports\n\n* feat: add pretty representation of Struct objects in iPython\n\n* feat: add download parameter to train_evaluate_model_from_config\n\n* feat: import train_evaluate_model_from_config in __init__ of deeppavlov as train_model\n\n* feat: add `train_model` sugar function\n\n* Fix quadratic complexity in ner_f1 (#564)\n\n* feat: iPython's tab-completion shows only directories and names for configs object (#566)\n\n* feat: iPython's tab-completion shows only directories and names for configs object\n\n* style: add typing to configs Struct methods\n\n* fix: show y_predicted in train logs examples instead of output names (#568)\n\n* feat: dialog logging (#560)\n\n* Moved configs to configs folder\n\n* Added default configs\n\n* Backup\n\n* Implemented cofig handling\n\n* Fix in paths.py\n\n* Added stateful and multi_instance params to MS Bot Framework config\n\n* Hotfix in default agent\n\n* Fixed utterances ids handling in default agent\n\n* Minor refactoring in MS BF server.py\n\n* Added MS Bot Framework app_id and app_secret params to server_config\n\n* Slighly fixed deep.py, stateful and multi_instance params moved to common_defaults section of config\n\n* Added https, key, cert params to server config\n\n* Added telegram token param to server config\n\n* Backup commit\n\n* Fixed config path retrival in test_quick_start.py\n\n* Implemented agent conversations logger\n\n* Changed download token file path to ~/.deeppavlov/token\n\n* Small fixes in get download token function\n\n* Fixed default server config\n\n* Minor fixes in paths.py\n\n* Added docstring and minor fixes to dialog logger\n\n* Added dialog logger apiref\n\n* moved configs dir to utils/configs\n\n* Small fixes in configs\n\n* Updated readme and docs\n\n* Added ssh cert and key files check in rise api\n\n* Moved dialog_logger configs\n\n* Added dialog logging to riseapi mode\n\n* Fixed configuration file path getting in alice.py\n\n* Changed naming of DP configuration to settings\n\n* Added https config params handling to alice.py\n\n* Fixed download token file path\n\n* Moved dialog logger config from utils/configs to utils/settings\n\n* Added Settings and dialog logging development guide\n\n* Changed typing Any -> Hashable in dialog_logger.py\n\n* Logging dir creation minor refactoring in dialog_logger.py\n\n* Small fixes in docs/devguides/settings.rst\n\n* Update docs/devguides/settings.rst\n\nCo-Authored-By: litinsky <alitinsky@gmail.com>\n\n* Updated settings devguide: made dialog logging defaults description clearer\n\n* Moved .deeppavlov file handling from dialog_logger.py to deeppavlov/__init__.py\n\n* Change naming of all DeepPavlov configuration tools: config -> settings\n\n* Fixed settings path handling in log.py\n\n* Fixed dialog_logging.py according PEP\n\nCo-Authored-By: litinsky <alitinsky@gmail.com>\n\n* Fixed dialog_logging.py according PEP\n\nCo-Authored-By: litinsky <alitinsky@gmail.com>\n\n* Fixed dialog_logging.py according PEP\n\nCo-Authored-By: litinsky <alitinsky@gmail.com>\n\n* Fixed typos in devguides/settings.rst\n\nCo-Authored-By: litinsky <alitinsky@gmail.com>\n\n* fix: making copy of config in read_data_by_config (#570)\n\n* fix: making copy of config in read_data_by_config\n\n* fix: get instead of pop\n\n* fix: reader_config is a copy\n\n* fix: remove unused import\n\n* Updated README.md with DeepPavlov dockerhub reference (#573)\n\n* Updated README.md with DeepPavlov dockerhub reference\n\n* docs: change header\n\n* fix: add fasttext to requirements to ranking configs instead of gensim (#571)\n\n* fix: add fasttext to requirements to ranking configs instead of gensim\n\n* fix: add fasttext to requirements to test ranking configs instead of gensim\n\n* Typo and grammar fix\n\n* feat: added ensure_ascii mode to dialog logging feature (#579)\n\n* Added ensure_ascii mode to dialog logging feature\n\n* Minor fixes in dialog logging fix\n\n* Update deeppavlov/core/agent/dialog_logger.py\n\nCo-Authored-By: litinsky <alitinsky@gmail.com>\n\n* feat: few shot SVM NER (#565)\n\n* fix: add svm tagger to registry\n\n* feat: template for ner-few shot iterator\n\n* feat: svm tagger added to ner models\n\n* chore: add svm ner to registry\n\n* fix: load loading in constructor, add checking existence of saved model\n\n* feat: bio converter added\n\n* fix: add bio restorer empty init\n\n* chore: add bio stuff to registry\n\n* feat: brand new few-shot iterator\n\n* fix: remove temporary acceleration slice\n\n* refactor: remove unnecessary utility functions\n\n* fix: inherit RedirectedPrints from contextlib.redirect_stdout\n\n* refactor: CosineSimilarityClassifier uses Serializable.__init__()\n\n* style: add typing for download.py\n\n* refactor: deep_download accepts config as dict\n\n* fix: fix errors found with code inspection\n\n* refactor: move find_config function away to core.common.file\n\n* feat: allow to download config's file requirements with build_model_from_config function\n\n* refactor: rename build_model_from_config to build_model\n\n* feat: add a config tree object\n\n* refactor: rename Sturct's _to_dict method to _asdict\nto conform with namedtuple's naming\n\n* fix: correct root directory for configs to search\n\n* feat: allow to import configs Struct and build_model fn from deeppavlov\n\n* docs: update python usage examples in model's docs\n\n* fix: do not raise in deeppavlov.__init__ because of absent requirements\n\n* fix: resolve merge conflict of recursive imports\n\n* feat: add pretty representation of Struct objects in iPython\n\n* feat: add download parameter to train_evaluate_model_from_config\n\n* feat: config for russian few-shot ner added\n\n* feat: import train_evaluate_model_from_config in __init__ of deeppavlov as train_model\n\n* feat: add `train_model` sugar function\n\n* docs: add few-shot ner doc\n\n* chore: setup paths, rename configs\n\n* docs: add few-shot ner doc\n\n* feat: add tests\n\n* Update docs/components/ner.rst\n\nCo-Authored-By: mu-arkhipov <arkhipov@yahoo.com>\n\n* fix: add elmo requirements\n\nCo-Authored-By: mu-arkhipov <arkhipov@yahoo.com>\n\n* docs: add docs for SVM ner\n\n* docs: add docs for FewShotNER iterator\n\n* refactor: parent init reused for ner-few-shot-iterator\n\n* fix: fix returned typing\n\n* fix: fix returned typing for gen_batches\n\n* chore: indents\n\nCo-Authored-By: mu-arkhipov <arkhipov@yahoo.com>\n\n* chore: typing\n\nCo-Authored-By: mu-arkhipov <arkhipov@yahoo.com>\n\n* refactor: clearer bio markup restorer\n\n* chore: import order\n\n* chore: add typing to svm model __call__\n\n* chore: return typing\n\nCo-Authored-By: mu-arkhipov <arkhipov@yahoo.com>\n\n* chore: return save typing\n\nCo-Authored-By: mu-arkhipov <arkhipov@yahoo.com>\n\n* chore: return typing\n\nCo-Authored-By: mu-arkhipov <arkhipov@yahoo.com>\n\n* docs: lingua fix\n\nCo-Authored-By: mu-arkhipov <arkhipov@yahoo.com>\n\n* fix: add requirements elmo\n\n* fix: add typings for svm ner\n\n* fix: add typings for bio\n\n* refactor: add path features usage\n\n* fix: fix shuffle, remove unnecessary imports\n\n* style: add blank ending line\n\n* chore: remove unnecessary import\n\n* fix: shuffle argument for gen_batches during the test\n\n* refactor: add typings change shuffle behaviour\n\n* style: typings\n\n* style: import order\n\n* fix: remove interact pretrained model from tests\n\n* fix: return predicted tags\n\n* feat: russian sentiment with elmo embeddings (#580)\n\n* feature: config for new rusentiment\n\n* feature: config for new rusentiment\n\n* docs: info about new elmo model\n\n* fix: add tests\n\n* fixed errors and updated docstrings\n\n* feat: skip downloads when files with the same hashes already exist (#582)\n\n* feat: add a utility function to generate .md5 files for archives\n\n* feat: add a function for computing archives hashes\n\n* feat: try and check md5 hashes before files downloads\n\n* tests: add a test for .md5 files existence on deeppavlov share\n\n* tests: remove urlib3 logs from tests of hashes existence\n\n* chore: fix imports order\n\n* chore: correct licence block\n\n* Apply suggestions from code review\n\nadd fastText import to docstring\n\nCo-Authored-By: Guitaricet <Guitaricet@gmail.com>\n\n* Update deeppavlov/models/embedders/tfidf_weighted_embedder.py\n\nCo-Authored-By: Guitaricet <Guitaricet@gmail.com>\n\n* feat: config variables (#574)\n\n* feat: first steps to using variables in configs\n\n* fix: remove set_deeppavlov_root function usage\n\n* feat: store metavariables in a single JSON-object\n\n* refactor: rename _parse_item to _parse_config_property\n\n* feat: parse subconfigs when building chainers\n\n* feat: parse_config before training\n\n* docs: update `expand_path`'s docstring\n\n* docs: add docstring to the `parse_config` function\n\n* style: pep8 in file.py\n\n* feat: use parse_config in every endpoint from deep.py\n\n* feat: adapted paramsearch to configs variables\n\n* feat: somewhat adapted evolution script to configs variables\n\n* chore: rename DEEPPAVLOV_ROOT to DEEPPAVLOV_PATH\n\n* feat: both evolution configs are conforming with config variables\n\n* feature: evolution use only MODELS_PATH\n\n* feature: replace MODELS_PATH to MODELS_SAVE_PATH and MODELS_LOAD_PATH\n\n* style: remove direct type comparison in favor of isinstance\n\n* fix: paths in evolve.py\n\n* style: remove excessive path casting\n\n* chore: rename result_table.csv in evolution to result_table.tsv\n\n* feat: adapt all classifiers configs to config variables\n\n* feat: adapt cross-validation config to config variables\n\n* feat: adapt all ranking configs to config variables\n\n* feat: adapt ecommerce config to config variables\n\n* fix: correct argument name in run_ms_bf_default_agent call\n\n* feat: adapt elmo configs to config variables\n\n* feat: adapt some faq configs to config variables\n\n* feat: adapt the rest of faq configs to config variables\n\n* feat: adapt gobot configs to config variables\n\n* feat: adapt morphotagger configs to config variables\n\n* fix: correct telegram_utils labeling for morphotagger configs\n\n* feat: adapt ner configs to config variables\n\n* feat: adapt odqa configs to config variables\n\n* feat: adapt odqa configs to config variables\n\n* feat: adapt paramsearch config to config variables\n\n* feat: adapt some ranking configs to config variables\n\n* feat: adapt ranking configs to config variables\n\n* feat: adapt bot_kvret configs to config variables\n\n* feat: adapt speller configs to config variables\n\n* feat: adapt squad configs to config variables\n\n* feat: adapt vectorizer config to config variables\n\n* chore: remove trailing / in download subdirs in configs\n\n* feat: create `CONFIGS_PATH` config variable for testing purposes\n\n* tests: adapt tests to config variables model\n\n* feat: merge `name` and `class` into `class_name`\n\n* test: correctly copy referenced configs\n\n* fix: small fixes in evolution\n\n* docs: classification, hypersearch docs for renamed ``class_name``\n\n* feat: merge `name` and `class` into `class_name` in test configs\n\n* fix: adapt to inplace config update in cross-validation\n\n* fix: keras_classification config description notebook\n\n* fix: remove keras_classification config description notebook\n\n* docs: replace `name` and `class` description with `class_name` description\n\n* docs: typing in evolution_param_generator.py\n\n* fix: upd chitchat notebook\n\n* docs: add readme block for config variables\n\n* docs: update config variables example in documentation to include usage of `DEEPPAVLOV_PATH`\n\n* fix: few_shot ner with new config format\n\n* fix: metrics import in ner notebook\n\n* docs: fix typos in MorphoTagger configs\n\n* feat: update gobot tutorial\n\n* feat: add links to collabs\n\n* feat: russian sentiment with elmo embeddings (#580)\n\n* feature: config for new rusentiment\n\n* feature: config for new rusentiment\n\n* docs: info about new elmo model\n\n* fix: add tests\n\n* fix: new rusentiment model is adapted to new config format\n\n* feat: language=python3 in tutorials\n\n* fix: sentiment config format\n\n* fix: add MODELS_PATH to the tensorboard log_dir\n\n* fix: MODELS_PATH for all tensorboard logdirs\n\n* fix: remove extra downloads from test classifiers configs\n\n* chore: add tests for classifiers projection layer\n\n* fix: do not load whole files from tar archives into memory when generationg .md5 files (#584)\n\n* feat: eCommerce bot with tfidf retrieve (#567)\n\n* fix: change agent's UI\n\n* fix: separate two bots\n\n* fix: change data reader\n\n* fix: add registry\n\n* fix: change def name\n\n* fix: code refactoring\n\n* fix: code refactoring\n\n* fix: add configs\n\n* fix: change config\n\n* fix: code refactoring\n\n* fix: change if\n\n* fix: edit docs\n\n* fix: fix name\n\n* feat: add tests\n\n* fix: change input\n\n* fix: add csr to list\n\n* fix: spaces\n\n* fix: minor changes\n\n* fix: omit testing the model\n\n* fix: simplify appearance\n\n* fix: change state\n\n* fix: types\n\n* fix: code refactoring\n\n* fix: code refactoring\n\n* fix: add all modes\n\n* fix: disable testing\n\n* fix: change def parameters\n\n* feat: improve naming\n\n* fix: docs\n\n* fix: clean files\n\n* fix: typo\n\n* fix: line length\n\n* fix: file name\n\n* fix: typo\n\n* fix: change input\n\n* fix: change input in config\n\n* fix: change format\n\n* fix: change input\n\n* fix: change docstring\n\n* doc: add resources note\n\n* doc: rename skill\n\n* doc: space requirement\n\n* fix: change test name\n\n* feat: adapt ecommerce configs to the new format\n\n* feat: allow validating every n batches (#589)\n\n* feat: allow validating every n batches\n\n* fix: tensorboard log for batches-validation logs batches instead epochs\n\n* feat: allow recursive training with a `--recursive` parameter (#590)\n\n* fix: change variables for save and load paths of main model in intents_snips (#594)\n\n* fix: utils.prepare.registry will not skip already imported packages (#595)\n\n* feat: Yahoo Conversational vs informational model (#593)\n\n* feature: basic dataset reader\n\n* fix: basic_dataset_reader for x and y lists\n\n* feature: data_sum_operator\n\n* chore: config\n\n* chore: config\n\n* feature: masking layer\n\n* feature: gru with masking for classification\n\n* fix: masking over outputs not hiddens\n\n* chore: adding maxpool over masking\n\n* chore: configs\n\n* feature: basic dataset reader\n\n* chore: configs\n\n* feature; dataset reader dealing with list input\n\n* chore: configs\n\n* chore: metrics\n\n* chore: yahoo answers config\n\n* chore: yahoo answers config for fulltext\n\n* fix: union to import, new metrics for yahoo\n\n* chore: config new model\n\n* chore: config for yahoo\n\n* feature: gru with self mult att and masking\n\n* chore: config for yahoo\n\n* chore: config experiment\n\n* chore: config experiment\n\n* chore: config experiment\n\n* chore: concat of max, aver and state\n\n* chore: concat of max, aver and state\n\n* fix: concat of max, aver and state\n\n* fix: concat of max, aver and state\n\n* chore: config\n\n* chore: config\n\n* chore: configs to new deeppavlov classification format\n\n* chore: configs\n\n* chore: config model_v5\n\n* chore: config model_v6 higher batch size\n\n* feature: notebook with pseudo-labeling for classification\n\n* fix: notebook fixes\n\n* chore: configs to compare embeddings\n\n* chore: number of epochs\n\n* chore: config with restore lr for pseudo labeling\n\n* chore: yahoo answers\n\n* fix: elmo version\n\n* chore: config for elmo classification\n\n* chore: config for elmo classification v14\n\n* chore: config for elmo classification v14\n\n* chore: config for elmo classification v15\n\n* chore: back to v14\n\n* chore: remove fasttext configs\n\n* chore: two models for yahoo elmo pretrained\n\n* chore: config for elmo pretrained final\n\n* chore: configs for experiments on yahoo questions\n\n* chore: configs for experiments on yahoo questions\n\n* chore: configs for experiments on yahoo questions\n\n* chore: configs for experiments on yahoo questions\n\n* chore: configs for experiments on yahoo questions\n\n* chore: configs for experiments on yahoo questions\n\n* chore: configs for experiments on yahoo questions\n\n* fix: configs for experiment with elmo\n\n* chore:  configs for elmo google on cnn and gru\n\n* chore:  configs for elmo google on cnn and gru\n\n* chore:  configs for elmo google on cnn and gru\n\n* chore: config and notebook for pseudo-labelling\n\n* chore: config for next experiment on yahoo pseudo labeling\n\n* new run of pseudo-labeling\n\n* fix: change models downloads path to models path in main model paths\n\n* chore: remove masking from keras classification model\n\n* chore: config for convers_vs_info model\n\n* chore: convers_vs_info config\n\n* chore: new model params, pre-trained model prepared\n\n* chore: empty outputs for pseudo-labeling notebook\n\n* feature: config and docs for yahoo_L31 conversational vs informational\n\n* chore: another output of chainer\n\n* chore: tests, remove dataset_reader\n\n* fix: dir for downloading pre-trained model\n\n* fix: moved requirements\n\n* fix: rmeove basic dataset reader\n\n* chore: revert intents_snips fix. will be separate pull request\n\n* chore: remove dataset_iterator from yahoo model\n\n* chore: new links\n\n* chore: quick start in the beginning of docs\n\n* fix: remove masking function\n\n* fix: remove masking function\n\n* chore: remove train parameters except of metrics\n\n* fix: extra import\n\n* fix: example of usage\n\n* fix: units_gru instead of units_lstm\n\n* fix: do not load from saved when evaluating model without training (#592)\n\n* fix: do not load from saved when evaluating model without training\n\n* fix: new load_path logic in go_bot\n\n* fix: set matplotlib logging level to WARNING (#598)\n\n* feat: ODQA with squad noans (best ODQA) (#599)\n\n* feat: odqa with squad noans\n\n* docs: update scores for noans, add R3 scores\n\n* docs: update features.rst with new odqa scores\n\n* fix: correct url for morpho_en model (#601)\n\n* feat: allow recursive training even if the parent config has no reader (#597)\n\n* feat: Added a learning ELMo model (#569)\n\n* new(elmo): add  model\n\n* new(elmo): update  model\n\n* wip (elmo): all sub components\n\n* wip (elmo): pipline is  ready\n\n* wip (elmo): update docs, remove usless files.\n\n* wip (elmo): add download.\n\n* wip (elmo): add tests.\n\n* fix (elmo): fix the elmo embedder.\n\n* wip (elmo): cleaning after debug.\n\n* Change train.py a option of a start epoch\n\nCo-Authored-By: kudep <kuznetsov.den.p@gmail.com>\n\n* updated ELMO docs\nminor code style fixes\n\n* fixed rendering\n\n* typo fix\n\nCo-Authored-By: Guitaricet <Guitaricet@gmail.com>\n\n* wip (train): fix length of line\n\n* wip (simple_vocab): update simple_vocab and fix a path checking.\n\n* wip (file_paths_iterator): fix bugs\n\n* wip (iterator, reader, deep): fix style and doc\n\n* wip (chunk_generator): fix style, add chunk_generator to core utils\n\n* wip (file_paths_iterator): update docs\n\n* wip (file_paths_iterators): update docs,  add inheritance from file_paths_iterator\n\n* wip (deep): remove unused imports\n\n* wip (elmo): update style\n\n* wip (elmo): fix docs typo\n\n* Update file_paths_iterator.py\n\n* wip (elmo): fix bag of PR\n\n* wip (simple_vocab): fix style\n\n* Update deeppavlov/dataset_iterators/elmo_file_paths_iterator.py\n\nfix (elmo_file_paths_iterator): rm empty line\n\nCo-Authored-By: kudep <kuznetsov.den.p@gmail.com>\n\n* Update deeppavlov/dataset_iterators/elmo_file_paths_iterator.py\n\nfix (style): rm empty line\n\nCo-Authored-By: kudep <kuznetsov.den.p@gmail.com>\n\n* Update deeppavlov/dataset_iterators/file_paths_iterator.py\n\nfix (style): rm empty line\n\nCo-Authored-By: kudep <kuznetsov.den.p@gmail.com>\n\n* Update deeppavlov/dataset_iterators/file_paths_iterator.py\n\nfix (style): rm empty line\n\nCo-Authored-By: kudep <kuznetsov.den.p@gmail.com>\n\n* Update deeppavlov/models/elmo/train_utils.py\n\nfix (style): rm empty line\n\nCo-Authored-By: kudep <kuznetsov.den.p@gmail.com>\n\n* Update deeppavlov/dataset_readers/file_paths_reader.py\n\nfix (style): rm empty line\n\nCo-Authored-By: kudep <kuznetsov.den.p@gmail.com>\n\n* fix (style): fix file_paths_iterator\n\n* fix (style): fix file_paths_iterator\n\n* fix (style): style correction\n\n* Update deeppavlov/models/elmo/elmo.py\n\nfix (style): style correction\n\nCo-Authored-By: kudep <kuznetsov.den.p@gmail.com>\n\n* Update deeppavlov/models/preprocessors/str_token_reverser.py\n\nfix (style): style correction\n\nCo-Authored-By: kudep <kuznetsov.den.p@gmail.com>\n\n* Update deeppavlov/models/elmo/elmo.py\n\nfix (style): style correction\n\nCo-Authored-By: kudep <kuznetsov.den.p@gmail.com>\n\n* Update deeppavlov/models/elmo/elmo.py\n\nfix (style): style correction\n\nCo-Authored-By: kudep <kuznetsov.den.p@gmail.com>\n\n* Update deeppavlov/models/elmo/bilm_model.py\n\nfix (style): style correction\n\nCo-Authored-By: kudep <kuznetsov.den.p@gmail.com>\n\n* Update deeppavlov/models/preprocessors/str_utf8_encoder.py\n\nfix (style): style correction\n\nCo-Authored-By: kudep <kuznetsov.den.p@gmail.com>\n\n* Update deeppavlov/models/elmo/bilm_model.py\n\nfix (style): style correction\n\nCo-Authored-By: kudep <kuznetsov.den.p@gmail.com>\n\n* wip (elmo): new elmo iterator\n\n* wip (elmo): impl 2 graphs\n\n* wip (elmo): ready 2 graph inside of elmo, honest ppl\n\n* wip (elmo): add a dumping\n\n* wip (elmo): update tfhub wrapper\n\n* fix (elmo): a bug of copy of options\n\n* fix (elmo): a bug of th_hub wrapper\n\n* wip (elmo): update configs and requirements\n\n* wip (elmo): update configs and requirements of elmo_embedder\n\n* wip (elmo): rm usless config\n\n* fix (elmo): a bug of the simple_vocab\n\n* wip (elmo): rm useless code of the simple_vocab\n\n* wip (elmo): change requirements of elmo configs.\n\n* wip (elmo): change dirs of elmo configs. Update the  registry.\n\n* wip (elmo): update simple_vocab.py and file_paths_iterator.py.\n\n* wip (elmo): fix style and docs\n\n* wip (elmo): replace weight_layers\n\n* wip (elmo): update a style\n\n* wip (elmo): update docs\n\n* wip (elmo): fix a bug of title of docs\n\n* wip (elmo): update docs, config paths, elmo2tfhub func signature.\n\n* wip (elmo): change download urls of config.\n\n* wip (elmo): fix configs after a merge\n\n* wip (elmo): fix doc typo\n\n* wip (elmo): update glob of file_paths_reader\n\n* wip (elmo): update a style\n\n* wip (elmo): fix loss and style\n\n* wip (elmo): rm debug print\n\n* wip (doc) update doc string\n\n* fix: padding keras model (#603)\n\n* fix: hot fix of padding type in keras_classification_model\n\n* fix: pre and post padding acceptable\n\n* fix: lear_rate to learning_rate\n\n* chore: config for yahoo with reader, iterator\n\n* docs: docstring on padding\n\n* docs: docs on yahoo in classifiers.rst\n\n* docs: add info about pre trained models classifiers.rst\n\n* chore: params for yahoo\n\n* chore: params for yahoo\n\n* chore: params for yahoo\n\n* chore: params for yahoo\n\n* chore: params for yahoo\n\n* fix: padding is optional parameter\n\n* fix: reinit of optional params\n\n* fix: new score for yahoo\n\n* fix: scores\n\n* chore: remove reader iterator from yahoo\n\n* fix: sort order (#604)\n\n* docs: usage examples and file sizes (#586)\n\n* chore: update version number\n\n* docs: remove obsolete note about odqa's GPU requirement\n\n* docs: add a warning in README about downloads location\n\n* docs: add a note about on disc space requirement for spelling correction models\n\n* doc: update memory notes and db size info\n\n* docs: add download sizes for classifiers\n\n* docs: add section about tf-idf ranking and update downloads info\n\n* docs: add note about disk space\n\n* docs: fix table in features.rst\n\n* docs: add download file sizes info\n\n* docs: fix config path in spellers usage example\n\n* fix: classifiers info about pre-trained in python\n\n* fix: change deep.py to deeppavlov/deep.py\n\n* fix: deep.py to -m deeppavlov\n\n* docs: fix indents in gobot's memory note\n\n* docs: replace `deep.py` call everywhere in docs with `-m deeppavlov`\n\n* docs: fix configs links in bash deeppavlov calls\n\n* docs: replace single graves with doubles\n\n* docs: add rst citations to NER documentation\n\n* docs: documents links as headers in features list\n\n* docs: add warning about config changes\n\n* docs: add memory requirement note for spelling correction in features\n\n* docs: add download file sizes for paraphraser and ranking\n\n* docs: add kvret_bot disk requirements info\n\n* docs: python api for go_bots\n\n* docs: update tables headers style in features list\n\n* docs: mock russian_tagsets package\n\n* docs: small fixes\n\n* docs: information about the model use in python api\n\n* fix: bugs in docs\n\n* doc: add \"quick start\" section to tfidf ranker and odqa\n\n* feat: rm extinguished run_model script\n\n* chore: merged docs for classifiers\n\n* doc: add space requirements\n\n* doc: add quick start section\n\n* docs: minor changes in neural_ranking doc\n\n* docs: add python example and download info to features docpage\n\n* docs: fix title underline\n\n* docs: ner docs updated\n\n* docs: update breaking changes warning\n\n* docs: move back few-shot docs\n\n* docs: prettify odqa scores table\n\n* docs: prettify ranker and squad results tables\n\n* docs: prettify classifiers table\n\n* docs: fix headers and beautify citations links in go-bot documentation\n\n* docs: beautify classification table in features list\n\n* docs: add a basic documentation for pattern_matching_skill\n\n* docs: update morphotagger docs\n\n* update: add processing of config variables in morphotagger 'predict' mode\n\n* refactor: update paths in morphotagger configs\n\n* fix: typo in morphotagger config\n\n* docs: update a basic documentation for pattern_matching_skill\n\n* docs: update headers in ecommerce.rst\n\n* examples: morphotagger Python API\n\n* docs: prettify morphotagger doc\n\n* wip (elmo): add tech requirements, update README\n\n* docs: seq2seq doc citation links\n\n* fix: properly return y_true for test in MorphotaggerDatasetReader\n\n* docs: add install instruction\n\n* docs: add installation of gobot and seq2seq_go_bot\n\n* docs: remove extra install command\n\n* docs: add an instruction for installing requirements for spelling\n\n* docs: add info about 'install' command\n\n* docs: add quick start info\n\n* fix: Unexpected indentation\n\n* docs: add install docs to classifiers\n\n* fix: indentation\n\n* fix: indentation\n\n* docs: minor updates in NER documentation\n\n* docs: add an instruction for installing requirements for morphotagger\n\n* docs: add an instruction for installing requirements for tfidf_ranking\n\n* docs: add an instruction for installing requirements for odqa\n\n* docs: add an instruction for installing requirements for squad\n\n* docs: add an instruction for installing requirements for ranking\n\n* docs: fix a minor typo\n\n* wip (docs): add install docs to elmo_model/elmo_embedder. (#607)\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class GoalOrientedBotNetwork(TFModel):",
            "self.sess.run(tf.global_variables_initializer())",
            "",
            "super().__init__(**kwargs)",
            "-        if tf.train.checkpoint_exists(str(self.save_path.resolve())):",
            "+        if tf.train.checkpoint_exists(str(self.load_path.resolve())):",
            "log.info(\"[initializing `{}` from saved]\".format(self.__class__.__name__))",
            "self.load()",
            "else:"
        ]
    },
    {
        "number": 1253,
        "comments": "",
        "commit_message": "fix LinearWrap imports. use varreplace in DoReFa.\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Model(ModelDesc):",
            ".BatchNorm('bnfc1')",
            ".apply(nonlin)",
            ".FullyConnected('fct', 1000, use_bias=True)())",
            "-        tf.get_variable = old_get_variable",
            "",
            "prob = tf.nn.softmax(logits, name='output')"
        ]
    },
    {
        "number": 1254,
        "comments": "",
        "commit_message": "[Torchscript] Add Vector preprocessing and postprocessing (#2160)\n\n* Adds torchscript-compatible Vector pre-/post- processing\n\n* finish merge\n\n* add vector input/output feature\n\n* type annotation\n\n* cleanup\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* workaround for py37\n\n* added return type hint\n\n* added return type hint\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class _SetPreprocessing(torch.nn.Module):",
            "self.unit_to_id = metadata[\"str2idx\"]",
            "self.is_bag = is_bag",
            "",
            "-    def forward(self, v: TorchscriptPreprocessingInput):",
            "+    def forward(self, v: TorchscriptPreprocessingInput) -> torch.Tensor:",
            "\"\"\"Takes a list of strings and returns a tensor of counts for each token.\"\"\"",
            "if not torch.jit.isinstance(v, List[str]):",
            "raise ValueError(f\"Unsupported input: {v}\")"
        ]
    },
    {
        "number": 1256,
        "comments": "",
        "commit_message": "Fix backprop bug when using non-zero hard loss weight.\n\nSummary:\nGot `RuntimeError: grad can be implicitly created only for scalar outputs` on f224839642.\n\nAfter some digging, found that we don't mean() the has_answer hard_loss so the final loss object becomes a tensor instead of scalar.\n\nAlso change cross_entropy loss to nll_loss per comment on Line 42 of loss.py\n\nReviewed By: stanvp\n\nDifferential Revision: D24349613\n\nfbshipit-source-id: 4109b72a6bab3d53fa5b93e6010f1ce7dcd06dde\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class KLDivergenceCELoss(Loss):",
            "soft_loss *= self.t ** 2  # See https://arxiv.org/pdf/1503.02531.pdf",
            "hard_loss = 0.0",
            "if self.hard_weight > 0.0:",
            "-            hard_loss = F.cross_entropy(",
            "-                logits,",
            "+            hard_loss = F.nll_loss(",
            "+                F.log_softmax(logits, 1, dtype=torch.float32),",
            "hard_targets,",
            "-                reduction=\"mean\" if reduce else \"none\",",
            "weight=self.weight,",
            "+                reduction=\"mean\" if reduce else \"none\",",
            ")",
            "",
            "return ("
        ]
    },
    {
        "number": 1257,
        "comments": "",
        "commit_message": "added record-and-pretrain example, documentation improvements, fixes\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Module(tf.Module):",
            "elif initializer == 'ones':",
            "initializer = tf_util.ones(shape=spec.shape, dtype=spec.type)",
            "elif initializer == 'constant':",
            "-            initializer = tf_util.fill(dims=spec.shape, value=self.initialization_scale)",
            "+            initializer = tf.fill(",
            "+                dims=spec.shape, value=tf_util.constant(value=initialization_scale, dtype=spec.type)",
            "+            )",
            "",
            "# Variable",
            "variable = tf.Variable("
        ]
    },
    {
        "number": 1258,
        "comments": "",
        "commit_message": "fixed unused code, error prone and code style  (#352)\n\n* raise not implemented error instead of pass\n\n* fix unused code\n\n* fix pylint issues.\n\n* fixed some assert error\n\n* fixed unused code\n\n* fixed all error prone\n\n* fixed most of the code style\n\n* yapf\n\n* yapf tests\n\n* fixed suggestion\n\n* yapf\n\n* remove unused code\n\n* remove unused code\n\n* fix\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def unstack_layer(layer, num=None, axis=0, name='unstack'):",
            "",
            "\"\"\"",
            "inputs = layer.outputs",
            "-    with tf.variable_scope(name) as vs:",
            "+    with tf.variable_scope(name):",
            "outputs = tf.unstack(inputs, num=num, axis=axis)",
            "",
            "logging.info(\"UnStackLayer %s: num: %s axis: %d, n_outputs: %d\" % (name, num, axis, len(outputs)))"
        ]
    },
    {
        "number": 1259,
        "comments": "",
        "commit_message": "Fixed some test cases\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TrainTest(unittest.TestCase):",
            "'--lfw_nrof_folds', '2' ]",
            "args = facenet_train.parse_arguments(argv)",
            "model_dir = facenet_train.main(args)",
            "+",
            "+",
            "model_file = os.path.join(model_dir, 'model.ckpt-1')",
            "# Check that the trained model can be loaded",
            "+        tf.reset_default_graph()",
            "argv = ['--model_file', model_file,",
            "'--lfw_pairs', self.lfw_pairs_file,",
            "'--lfw_dir', self.dataset_dir,"
        ]
    },
    {
        "number": 1262,
        "comments": "",
        "commit_message": "Reinforcement learning fix opt (#999)\n\n* change readme\n\n* Add files via upload\n\n* fix opt and make format\n\n* readme\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class PPO(object):",
            "",
            "self.update_old_pi()",
            "adv = self.cal_adv(s, r)",
            "-        # adv = (adv - adv.mean())/(adv.std()+1e-6)     # sometimes helpful",
            "+        # adv = (adv - adv.mean())/(adv.std()+1e-6)  # sometimes helpful",
            "",
            "# update actor",
            "if METHOD['name'] == 'kl_pen':"
        ]
    },
    {
        "number": 1263,
        "comments": "",
        "commit_message": "delaunay fix + planetoid mask is now a bool\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def edge_index_from_dict(graph_dict, num_nodes=None):",
            "",
            "",
            "def sample_mask(index, num_nodes):",
            "-    mask = torch.zeros((num_nodes, ), dtype=torch.uint8)",
            "+    mask = torch.zeros((num_nodes, ), dtype=torch.bool)",
            "mask[index] = 1",
            "return mask"
        ]
    },
    {
        "number": 1264,
        "comments": "",
        "commit_message": "fix optimizer for restored model\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def main(args):",
            "checkpoint = torch.load(args.restore_path)",
            "model.load_state_dict(checkpoint['model'])",
            "optimizer.load_state_dict(checkpoint['optimizer'])",
            "-        print(\"\\n > Model restored from step %d\\n\" % checkpoint['step'])",
            "+        print(\" > Model restored from step %d\" % checkpoint['step'])",
            "start_epoch = checkpoint['step'] // len(train_loader)",
            "best_loss = checkpoint['linear_loss']",
            "start_epoch = 0",
            "args.restore_step = checkpoint['step']",
            "else:",
            "args.restore_step = 0",
            "-        print(\"\\n > Starting a new training\")",
            "+        print(\" > Starting a new training\")",
            "",
            "if use_cuda:",
            "-        model = nn.DataParallel(model.cuda())",
            "+        print(\" > Using CUDA.\")",
            "+        model = nn.DataParallel(model).cuda()",
            "",
            "num_params = count_parameters(model)",
            "print(\" | > Model has {} parameters\".format(num_params))"
        ]
    },
    {
        "number": 1265,
        "comments": "",
        "commit_message": "Fix typos in torchscript tests.\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_transformer_conv():",
            "",
            "t = '(PairTensor, SparseTensor, NoneType) -> Tensor'",
            "jit = torch.jit.script(conv.jittable(t))",
            "-    assert torch.allclose(conv((x1, x2), adj.t()), out, atol=1e-6)",
            "+    assert torch.allclose(jit((x1, x2), adj.t()), out, atol=1e-6)"
        ]
    },
    {
        "number": 1266,
        "comments": "",
        "commit_message": "Fix doctest CI (#21166)\n\n* fix\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "PT_SEQUENCE_CLASSIFICATION_SAMPLE = r\"\"\"",
            "...     \"{checkpoint}\", num_labels=num_labels, problem_type=\"multi_label_classification\"",
            "... )",
            "",
            "-    >>> labels = torch.nn.functional.one_hot(torch.tensor(predicted_class_ids), num_classes=num_labels).to(torch.float)",
            "+    >>> labels = torch.sum(",
            "+    ...     torch.nn.functional.one_hot(predicted_class_ids[None, :].clone(), num_classes=num_labels), dim=1",
            "+    ... ).to(torch.float)",
            ">>> loss = model(**inputs, labels=labels).loss",
            "```",
            "\"\"\""
        ]
    },
    {
        "number": 1267,
        "comments": "",
        "commit_message": "rnn->encoders, removes _th suffix, same filenames for chainer and pytorch, modify tests to not rely on _th\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_lecun_init_torch():",
            "torch.manual_seed(nseed)",
            "numpy.random.seed(nseed)",
            "os.environ[\"CHAINER_SEED\"] = str(nseed)",
            "-    import espnet.nets.pytorch.e2e_asr_th as m",
            "+    import espnet.nets.pytorch.e2e_asr as m",
            "model = m.Loss(m.E2E(40, 5, args), 0.5)",
            "b = model.predictor.ctc.ctc_lo.bias.data.numpy()",
            "assert numpy.all(b == 0.0)"
        ]
    },
    {
        "number": 1270,
        "comments": "",
        "commit_message": "Fixed nightly errors (#886)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestNormalizeLAF:",
            "laf = torch.tensor([[1, 0, 1], [0, 1, 1]]).float()",
            "laf = laf.view(1, 1, 2, 3)",
            "img = torch.rand(1, 3, h, w)",
            "-        expected = torch.tensor([[0.2, 0, 0.1], [0, 0.2, 0.2]]).float()",
            "+        expected = torch.tensor([[[[0.2, 0, 0.1], [0, 0.2, 0.2]]]]).float()",
            "lafn = kornia.feature.normalize_laf(laf, img)",
            "assert_allclose(lafn, expected)"
        ]
    },
    {
        "number": 1271,
        "comments": "",
        "commit_message": "Update to PyTorch 1.9.0 (#2887)\n\n* Update to PyTorch 1.9.0\n\n* Update torch.cholesky, torch.symeig\n\n* Fix torch.tensordot, torch.qr, funsor dependency\n\n* Fix torch import, AutoGuideList usage\n\n* Ignore bug in PyTorch jit\n\n* Ignore tracer warnings\n\n* Replace torch.solve with torch.linalg.solve\n\n* Xfail vectorized markov funsor tests\n\n* Update funsor version\n\n* Switch MNIST mirrors\n\n* Pin pillow version\n\n* Bump torchvision version\n",
        "label": "",
        "answer": "no",
        "change": [
            "def _PositiveDefinite_check(self, value):",
            "matrix_shape = value.shape[-2:]",
            "batch_shape = value.shape[:-2]",
            "flattened_value = value.reshape((-1,) + matrix_shape)",
            "-    return torch.stack([v.symeig(eigenvectors=False)[0][:1] > 0.0",
            "+    return torch.stack([torch.linalg.eigvalsh(v)[:1] > 0.0",
            "for v in flattened_value]).view(batch_shape)"
        ]
    },
    {
        "number": 1272,
        "comments": "",
        "commit_message": "Fix typos in torchscript tests.\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_dna_conv():",
            "",
            "t = '(Tensor, SparseTensor, OptTensor) -> Tensor'",
            "jit = torch.jit.script(conv.jittable(t))",
            "-    assert torch.allclose(conv(x, adj1.t()), out1, atol=1e-6)",
            "-    assert torch.allclose(conv(x, adj2.t()), out2, atol=1e-6)",
            "+    assert torch.allclose(jit(x, adj1.t()), out1, atol=1e-6)",
            "+    assert torch.allclose(jit(x, adj2.t()), out2, atol=1e-6)",
            "",
            "conv.cached = True",
            "conv(x, edge_index)"
        ]
    },
    {
        "number": 1273,
        "comments": "",
        "commit_message": "Fix typos in torchscript tests.\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_gat_conv():",
            "",
            "t = '(OptPairTensor, SparseTensor, Size, NoneType) -> Tensor'",
            "jit = torch.jit.script(conv.jittable(t))",
            "-    assert torch.allclose(conv((x1, x2), adj.t()), out1, atol=1e-6)",
            "-    assert torch.allclose(conv((x1, None), adj.t()), out2, atol=1e-6)",
            "+    assert torch.allclose(jit((x1, x2), adj.t()), out1, atol=1e-6)",
            "+    assert torch.allclose(jit((x1, None), adj.t()), out2, atol=1e-6)"
        ]
    },
    {
        "number": 1274,
        "comments": "",
        "commit_message": "Add usage of batch norm in conv test and fix usage of is_training collection\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def batch_normalize(tensor_in, epsilon=1e-5, convnet=True, decay=0.9,",
            "\"\"\"Internal function that updates mean and variance during training\"\"\"",
            "with tf.control_dependencies([ema_assign_op]):",
            "return tf.identity(assign_mean), tf.identity(assign_var)",
            "-        IS_TRAINING = tf.get_collection(\"IS_TRAINING\")[-1]",
            "-        mean, variance = control_flow_ops.cond(IS_TRAINING,",
            "-                                               update_mean_var,",
            "-                                               lambda: (ema_mean, ema_var))",
            "+        is_training = tf.squeeze(tf.get_collection(\"IS_TRAINING\"))",
            "+        mean, variance = tf.python.control_flow_ops.cond(",
            "+            is_training, update_mean_var, lambda: (ema_mean, ema_var))",
            "return tf.nn.batch_norm_with_global_normalization(",
            "tensor_in, mean, variance, beta, gamma, epsilon,",
            "scale_after_normalization=scale_after_normalization)"
        ]
    },
    {
        "number": 1275,
        "comments": "",
        "commit_message": "Fix docs links to PyTorch documentation (#856)\n\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def torch_multinomial(input, num_samples, replacement=False):",
            "",
            "def torch_sign(value):",
            "\"\"\"",
            "-    Like ``torch.sign()`` but also works for numbers.",
            "+    Like :func:`torch.sign`` but also works for numbers.",
            "\"\"\"",
            "if isinstance(value, numbers.Number):",
            "return (value > 0) - (value < 0)"
        ]
    },
    {
        "number": 1278,
        "comments": "",
        "commit_message": "bug fix\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class LJSpeechDataset(Dataset):",
            "linear = torch.FloatTensor(linear)",
            "mel = torch.FloatTensor(mel)",
            "mel_lengths = torch.LongTensor(mel_lengths)",
            "-            stop_targets = torch.FloatTensor(stop_targets).squeeze()",
            "+            stop_targets = torch.FloatTensor(stop_targets)",
            "",
            "return text, text_lenghts, linear, mel, mel_lengths, stop_targets, item_idxs[0]"
        ]
    },
    {
        "number": 1279,
        "comments": "",
        "commit_message": "improved global tensor handling, various other fixes\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Synchronization(Optimizer):",
            "return deltas",
            "",
            "do_sync = (time - self.last_sync >= self.sync_frequency)",
            "-        return tf.cond(pred=do_sync, true_fn=sync, false_fn=no_sync)",
            "+        return self.cond(pred=do_sync, true_fn=sync, false_fn=no_sync)"
        ]
    },
    {
        "number": 1282,
        "comments": "",
        "commit_message": "Fix special values overflow/underflow for amp (#3901)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def multi_perspective_match_pairwise(",
            "norm_value = vector1_norm * vector2_norm.transpose(2, 3)",
            "",
            "# (batch, seq_len1, seq_len2, num_perspectives)",
            "-    return (mul_result / norm_value.clamp(min=eps)).permute(0, 2, 3, 1)",
            "+    return (mul_result / norm_value.clamp(min=tiny_value_of_dtype(norm_value.dtype))).permute(",
            "+        0, 2, 3, 1",
            "+    )",
            "",
            "",
            "class BiMpmMatching(nn.Module, FromParams):"
        ]
    },
    {
        "number": 1283,
        "comments": "",
        "commit_message": "Fix module dict in base finetuning (#8170)\n\n* Fix module dict in base finetuning\n\n* Update CHANGELOG.md\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_complex_nested_model():",
            "assert len(BaseFinetuning.flatten_modules(model)) == 10",
            "",
            "BaseFinetuning.freeze(model.encoder, train_bn=True)",
            "-    assert not model.encoder[0].conv.weight.requires_grad  # Validate a leaf module parameter is frozen",
            "+    assert not model.encoder[0].module_dict[\"conv\"].weight.requires_grad  # Validate a leaf module parameter is frozen",
            "assert not model.encoder[0].parent_param.requires_grad  # Validate the parent module parameter is frozen",
            "assert model.encoder[0].bn.weight.requires_grad"
        ]
    },
    {
        "number": 1289,
        "comments": "",
        "commit_message": "fix attention bug. Attention needs to pass on encoder outputs\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TFTacotronLocationSensitiveAttention(tf.keras.layers.Layer):",
            "",
            "def get_initial_attention(self, batch_size):",
            "\"\"\"Get initial attention.\"\"\"",
            "-        return tf.zeros(shape=[batch_size, self.config.attention_dim], dtype=tf.float32)",
            "+        return tf.zeros(shape=[batch_size, self.config.encoder_lstm_units * 2], dtype=tf.float32)",
            "",
            "",
            "class TFTacotronPrenet(tf.keras.layers.Layer):"
        ]
    },
    {
        "number": 1290,
        "comments": "",
        "commit_message": "torch.from_tensor() bug fix\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def train(hyp, opt, device, tb_writer=None):",
            "if rank != -1:",
            "indices = torch.zeros([dataset.n], dtype=torch.int)",
            "if rank == 0:",
            "-                    indices[:] = torch.from_tensor(dataset.indices, dtype=torch.int)",
            "+                    indices[:] = torch.tensor(dataset.indices, dtype=torch.int)",
            "dist.broadcast(indices, 0)",
            "if rank != 0:",
            "dataset.indices = indices.cpu().numpy()"
        ]
    },
    {
        "number": 1292,
        "comments": "",
        "commit_message": "Fixed formatting\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class BaseWorker(AbstractWorker, ObjectStorage):",
            "",
            "response = command(*args, **kwargs)",
            "",
            "-            #Temporary fix for websockets when returning a tuple of tensors from an LSTM cell",
            "+            # Temporary fix for websockets when returning a tuple of tensors from an LSTM cell",
            "if command_name == \"torch.lstm_cell\":",
            "response = torch.stack(response)",
            "",
            "-            #Temporary fix for websockets when returning a tuple of tensors from torch.sort()",
            "+            # Temporary fix for websockets when returning a tuple of tensors from torch.sort()",
            "if command_name == \"torch.sort\":",
            "Alpha_Tensor_Fixed = (response[0].float(), response[1].float())",
            "response = torch.stack(Alpha_Tensor_Fixed)"
        ]
    },
    {
        "number": 1294,
        "comments": "",
        "commit_message": "fix SCEModule.sSE (#113)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class SCSEModule(nn.Module):",
            "nn.Conv2d(in_channels // reduction, in_channels, 1),",
            "nn.Sigmoid(),",
            ")",
            "-        self.sSE = nn.Sequential(nn.Conv2d(in_channels, in_channels, 1), nn.Sigmoid())",
            "+        self.sSE = nn.Sequential(nn.Conv2d(in_channels, 1, 1), nn.Sigmoid())",
            "",
            "def forward(self, x):",
            "return x * self.cSE(x) + x * self.sSE(x)"
        ]
    },
    {
        "number": 1296,
        "comments": "",
        "commit_message": "Fixed a problem with jax interpolate where it got into an infinite recursive loop\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def interpolate(",
            "size = [x.shape[0], *size, x.shape[1]]",
            "",
            "if align_corners or mode == \"area\":",
            "-        return ivy.interpolate(",
            "+        return ivy.functional.experimental.interpolate(",
            "x, size, mode=mode, align_corners=align_corners, antialias=antialias",
            ")",
            "x = jnp.transpose(x, (0, *range(2, dims + 2), 1))"
        ]
    },
    {
        "number": 1298,
        "comments": "",
        "commit_message": "fix typo in last commit (fix #1202)\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Model(ModelDesc):",
            "logits = M(image)",
            "if ctx.is_main_training_tower:",
            "for op in M.updates:",
            "-                tf.add_to_collection(tf.GraphKeys.UPDATE_OPS)",
            "+                tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, op)",
            "",
            "# build cost function by tensorflow",
            "cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)"
        ]
    },
    {
        "number": 1300,
        "comments": "",
        "commit_message": "Moving allennlp.nn.decoding to allennlp.state_machines (#1714)\n\n* Moving allennlp.nn.decoding to allennlp.state_machines\n\n* Fixing docs\n\n* Updated TODO\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "import numpy as np",
            "from numpy.testing import assert_almost_equal",
            "",
            "from allennlp.common.testing import AllenNlpTestCase",
            "-from allennlp.nn.decoding.decoder_trainers import ExpectedRiskMinimization",
            "-from ..simple_transition_system import SimpleDecoderState, SimpleDecoderStep",
            "+from allennlp.state_machines.trainers import ExpectedRiskMinimization",
            "+from ..simple_transition_system import SimpleState, SimpleTransitionFunction",
            "",
            "",
            "class TestExpectedRiskMinimization(AllenNlpTestCase):",
            "def setUp(self):",
            "super().setUp()",
            "-        self.initial_state = SimpleDecoderState([0], [[0]], [torch.Tensor([0.0])])",
            "-        self.decoder_step = SimpleDecoderStep()",
            "+        self.initial_state = SimpleState([0], [[0]], [torch.Tensor([0.0])])",
            "+        self.decoder_step = SimpleTransitionFunction()",
            "# Cost is the number of odd elements in the action history.",
            "self.supervision = lambda state: torch.Tensor([sum([x%2 != 0 for x in",
            "state.action_history[0]])])"
        ]
    },
    {
        "number": 1302,
        "comments": "",
        "commit_message": "fix memory bloat on restore\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def main(args):  # pylint: disable=redefined-outer-name",
            "pos_weight=torch.tensor(10)) if c.stopnet else None",
            "",
            "if args.restore_path:",
            "-        checkpoint = torch.load(args.restore_path)",
            "+        checkpoint = torch.load(args.restore_path, map_location='cpu')",
            "try:",
            "# TODO: fix optimizer init, model.cuda() needs to be called before",
            "# optimizer restore"
        ]
    },
    {
        "number": 1303,
        "comments": "",
        "commit_message": "Fix wrong mps selection below MasOS 12.3\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def get_optimal_device():",
            "else:",
            "return torch.device(\"cuda\")",
            "",
            "-    if has_mps:",
            "+    if has_mps():",
            "return torch.device(\"mps\")",
            "",
            "return cpu"
        ]
    },
    {
        "number": 1305,
        "comments": "",
        "commit_message": "[Train] Strip \"module.\" from state dict (#30705)\n\nThis PR adds logic to automatically strip the \"module.\" prefix from a user-saved state dict in TorchCheckpoint, which is present if a user obtains the state dict from a DistributedDataParallel module directly. We already obtain the underlying module if a user saves the model object, so this merely makes the logic consistent.\n\nThis PR also edits our examples to remove instances where this operation was conducted in the example itself. This led to issues if train.torch.prepare_model was used with num_workers=1 (eg. on Google Colab), as the module was not wrapped around, thus leading to the .module attribute being missing.\n\nSigned-off-by: Antoni Baum <antoni.baum@protonmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_torch_e2e_state_dict(ray_start_4_cpus):",
            "assert predictions.count() == 3",
            "",
            "",
            "+# We can't really test for prepare_model here as we can't detect what the user",
            "+# has saved without loading (and thus triggering the exception anyway)",
            "def test_torch_e2e_dir(ray_start_4_cpus, tmpdir):",
            "def train_func():",
            "model = torch.nn.Linear(3, 1)"
        ]
    },
    {
        "number": 1307,
        "comments": "",
        "commit_message": "upgrade to pytorch 0.4.0 (#1126)\n\n* bump pytorch to 0.4 + fix sanitize\n\n* remove check for Variable in block_orthogonal\n\n* rename parameter split_size=\n\n* remove checks for Variable\n\n* fix more tests\n\n* fixes\n\n* get tests to pass\n\n* fix warnings\n\n* get rid of some of the Variables\n\n* more tests passing\n\n* more elimination of variables\n\n* finish removing all Variables\n\n* pylint and such\n\n* a few fixes\n\n* move torch.no_grad into model.forward_on_instances\n\n* more pytorch 0.4 changes\n\n* detach() -> data\n\n* pylint\n\n* fix bad tensor creation\n\n* fix types\n\n* remove print statement\n\n* more 0.4 goodness\n\n* factor out is_tensor\n\n* add no_grad to elmo command\n\n* cleanup\n\n* remove TODO\n\n* address PR feedback\n\n* replace all() with item()\n\n* more cleanup\n\n* further cleanup\n\n* remove Variable\n\n* really fix merge conflict\n\n* fix pylint\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class BiattentiveClassificationNetwork(Model):",
            "\"\"\"",
            "Parameters",
            "-        tokens : Dict[str, Variable], required",
            "+        tokens : Dict[str, torch.LongTensor], required",
            "The output of ``TextField.as_array()``.",
            "-        label : Variable, optional (default = None)",
            "+        label : torch.LongTensor, optional (default = None)",
            "A variable representing the label for each instance in the batch.",
            "Returns"
        ]
    },
    {
        "number": 1308,
        "comments": "",
        "commit_message": "[Test] Fix count_nonzero\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def count_nonzero(",
            "def _dtype_count_nonzero(a, axis, dtype):",
            "if dtype is None:",
            "return torch.count_nonzero(a, dim=axis)",
            "-        return torch.tensor(torch.count_nonzero(a, dim=axis), dtype=dtype)",
            "+        return torch.tensor(torch.count_nonzero(a, dim=axis),",
            "+                            dtype=ivy.as_native_dtype(dtype))",
            "",
            "x = _dtype_count_nonzero(a, axis, dtype)",
            "if not keepdims:"
        ]
    },
    {
        "number": 1309,
        "comments": "",
        "commit_message": "Add Image feature (#3163)\n\n* Initial commit\n\n* Add basic decoding\n\n* Replace features.Audio with Audio\n\n* Add Image to package reference\n\n* Use np.array\n\n* Update error msg\n\n* Add mode and channel decoding\n\n* Fix return value\n\n* Finish decoding\n\n* Make CI happy\n\n* Some more fixes\n\n* Minor doc fix\n\n* Remove animated option\n\n* Pin version\n\n* Remove unused imports in setup.py\n\n* Add vision requirements to setup.py\n\n* Add initial tests\n\n* Delete other formats\n\n* Make Image feature hashable\n\n* Add more tests\n\n* Support numpy array in alter data check in TypedSequence\n\n* Fix TypedSequence converion\n\n* Finish tests\n\n* Update Image - add ImageExtensionType and supporting functions\n\n* Update encoding functions\n\n* Add support in TypedSequence for ImageExtensionType\n\n* Add tests\n\n* Remove unused import\n\n* Fix doc and style\n\n* Fix doc indentation\n\n* Improve comment\n\n* Return single image instead of dict\n\n* Return PIL Image and not dict\n\n* Encode dict\n\n* Update tests\n\n* Style\n\n* np.ndarray encoding/decoding\n\n* Minor improvements\n\n* PIL Image support in cast_to_python_objects\n\n* Test cast\n\n* Doc fix\n\n* Extension type fixes\n\n* Style\n\n* Use types_mapper in Dataset.to_pandas\n\n* Add pandas extension array for image type\n\n* Update tests\n\n* image type inference\n\n* Remvoe cast_to_python test after Quentin's change\n\n* Improve tests\n\n* Add storage type\n\n* Improve tests\n\n* Test map that returns np.ndarray\n\n* Rename functions\n\n* Add streaming test\n\n* Use image struct in all situations\n\n* Update src/datasets/features/image.py - encode_example type hint\n\nCo-authored-by: Quentin Lhoest <42851186+lhoestq@users.noreply.github.com>\n\n* Update src/datasets/features/image.py -list_image_compression_formats type hint\n\nCo-authored-by: Quentin Lhoest <42851186+lhoestq@users.noreply.github.com>\n\n* Support str in encode_objects_to_image_dicts\n\n* Update src/datasets/features/image.py - objects_to_list_of_image_dicts type hint\n\nCo-authored-by: Quentin Lhoest <42851186+lhoestq@users.noreply.github.com>\n\n* Style\n\nCo-authored-by: Quentin Lhoest <lhoest.q@gmail.com>\nCo-authored-by: Quentin Lhoest <42851186+lhoestq@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class LibrispeechASR(datasets.GeneratorBasedBuilder):",
            "features=datasets.Features(",
            "{",
            "\"file\": datasets.Value(\"string\"),",
            "-                    \"audio\": datasets.features.Audio(sampling_rate=16_000),",
            "+                    \"audio\": datasets.Audio(sampling_rate=16_000),",
            "\"text\": datasets.Value(\"string\"),",
            "\"speaker_id\": datasets.Value(\"int64\"),",
            "\"chapter_id\": datasets.Value(\"int64\"),"
        ]
    },
    {
        "number": 1311,
        "comments": "",
        "commit_message": "fix typing callable in load storage (#1768)\n\n* fix typing callable in load storage\n\n* missing import\n",
        "label": "",
        "answer": "no",
        "change": [
            "class MKDDescriptor(nn.Module):",
            "",
            "",
            "def load_whitening_model(kernel_type: str, training_set: str) -> Dict:",
            "-    whitening_models = torch.hub.load_state_dict_from_url(urls[kernel_type], map_location=lambda storage, loc: storage)",
            "+    storage_fcn: Callable = lambda storage, loc: storage",
            "+    whitening_models = torch.hub.load_state_dict_from_url(",
            "+        urls[kernel_type], map_location=storage_fcn",
            "+    )",
            "whitening_model = whitening_models[training_set]",
            "return whitening_model"
        ]
    },
    {
        "number": 1312,
        "comments": "",
        "commit_message": "changed all asset to if/raise to prevent disable of assert during PYTHONOPTIMISE env (#4655)\n\n* changed all asset to if/raise to prevent disable of assert during PYTHONOPTIMISE env\n\n* minor fix for proper inverson of assert condition\n\n* Changed to AssertionError which is handled at multiple places\n\n* Changed to AssertionError which is handled at multiple places\n\n* added simple test case in test_string to increase test coverage\n\n* added simple test cases in test_string to increase test coverage\n\n* added simple test cases to increase test coverage\n\n* Either None OR More than One worker result found\n\n* changes for review comments\n\n* removed comments, minor changes\n\nCo-authored-by: Vivek Pothina <vivek.pothina@ninjacart.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TorchTensor(AbstractTensor):",
            "",
            "\"\"\"",
            "",
            "-        assert isinstance(self.child, PointerTensor)",
            "+        if not isinstance(self.child, PointerTensor):",
            "+            raise TypeError(\"child should be a PointerTensor\")",
            "",
            "ps = list(pointers)",
            "ps.append(self)"
        ]
    },
    {
        "number": 1313,
        "comments": "",
        "commit_message": "fix the unravel_index's torch backend as it returned a tuple rather than a torch.tensor\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def unravel_index(",
            "for dim in reversed(shape):",
            "output.append(temp % dim)",
            "temp = temp // dim",
            "-    return tuple(reversed(output))",
            "+    return torch.tensor(reversed(output))",
            "",
            "",
            "unravel_index.support_native_out = False"
        ]
    },
    {
        "number": 1315,
        "comments": "",
        "commit_message": "Not use -1e4 as attn mask (#17306)\n\n* Use torch.finfo(self.dtype).min\n\n* for GPTNeoX\n\n* for Albert\n\n* For Splinter\n\n* Update src/transformers/models/data2vec/modeling_data2vec_audio.py\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* fix -inf used in Bart-like models\n\n* Fix a few remaining -inf\n\n* more fix\n\n* clean up\n\n* For CLIP\n\n* For FSMT\n\n* clean up\n\n* fix test\n\n* Add dtype argument and use it for LayoutLMv3\n\n* update FlaxLongT5Attention\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class CanineSelfAttention(nn.Module):",
            "# Since attention_mask is 1.0 for positions we want to attend and 0.0 for",
            "# masked positions, this operation will create a tensor which is 0.0 for",
            "# positions we want to attend and -10000.0 for masked positions.",
            "-                attention_mask = (1.0 - attention_mask.float()) * -10000.0",
            "+                attention_mask = (1.0 - attention_mask.float()) * torch.finfo(attention_scores.dtype).min",
            "# Apply the attention mask (precomputed for all layers in CanineModel forward() function)",
            "attention_scores = attention_scores + attention_mask"
        ]
    },
    {
        "number": 1317,
        "comments": "",
        "commit_message": "Fix FlauBERT GPU test (#6142)\n\n* Fix GPU test\n\n* Remove legacy constructor\n",
        "label": "",
        "answer": "no",
        "change": [
            "class FlaubertModel(XLMModel):",
            "# if self.is_decoder and src_enc is not None:",
            "#     src_mask = torch.arange(src_len.max(), dtype=torch.long, device=lengths.device) < src_len[:, None]",
            "",
            "-        device = input_ids.device if input_ids is not None else inputs_embeds.device",
            "-",
            "# position_ids",
            "if position_ids is None:",
            "position_ids = torch.arange(slen, dtype=torch.long, device=device)"
        ]
    },
    {
        "number": 1321,
        "comments": "",
        "commit_message": "fix dtype, device in `sum`, `prod`, `to_dev` (#1358)\n\n* fix dtype, device in `sum`, `prod`, `to_dev`\n\n* make `device` have `None` as default\n\n* add `dtype = ivy.as_native_dtype(dtype)` and make `copy` positional\n\n* `to_dev` conform to array API\n\n* `astype` fixes to signature\n\n* black\n",
        "label": "",
        "answer": "no",
        "change": [
            "def prod(",
            "dtype = tf.int64",
            "elif x.dtype == tf.uint64:",
            "dtype = tf.uint64",
            "+    dtype = ivy.as_native_dtype(dtype)",
            "return tf.experimental.numpy.prod(x, axis, dtype, keepdims)"
        ]
    },
    {
        "number": 1323,
        "comments": "",
        "commit_message": "feat: models parameters check for ner\n\n* feat: parameters check added to ner\n\n* feat: parameters check added to slotfill\n\n* chore: minor clean-up\n\n* fix: fix conll-2003 model file names and archive names\n\n* refactor: remove blank line\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TFModel(NNModel, metaclass=TfModelMeta):",
            "opt_scope = tf.variable_scope(optimizer_scope_name)",
            "with opt_scope:",
            "if learnable_scopes is None:",
            "-                variables_to_train = tf.trainable_variables()",
            "+                variables_to_train = tf.global_variables()",
            "else:",
            "variables_to_train = []",
            "for scope_name in learnable_scopes:",
            "-                    for var in tf.trainable_variables():",
            "+                    for var in tf.global_variables():",
            "if scope_name in var.name:",
            "variables_to_train.append(var)"
        ]
    },
    {
        "number": 1324,
        "comments": "",
        "commit_message": "Fix axis specification in TF.\n",
        "label": "",
        "answer": "no",
        "change": [
            "def abs(x):",
            "",
            "",
            "def sqrt(x):",
            "-    x = tf.clip_by_value(x, _EPSILON, np.inf)",
            "+    x = tf.clip_by_value(x, 0., np.inf)",
            "return tf.sqrt(x)"
        ]
    },
    {
        "number": 1328,
        "comments": "",
        "commit_message": "[release] fix pytorch pbt failure test. (#31791)\n\nThe regression is introduced by #30705.\n\nAlso added some documentation into TorchTrainer so users know there is quite some magic happening :)\n\nTested manually in workspace.\nFollow-up PR to add more strict assertions to the test.\n\nSigned-off-by: xwjiang2010 <xwjiang2010@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def train_func(config):",
            "checkpoint_epoch = checkpoint_dict[\"epoch\"]",
            "starting_epoch = checkpoint_epoch + 1",
            "",
            "+    model = train.torch.prepare_model(model)",
            "+",
            "# Load in training and validation data.",
            "transform_train = transforms.Compose(",
            "["
        ]
    },
    {
        "number": 1329,
        "comments": "",
        "commit_message": "fixed calculation of ctc loss in TFWav2Vec2ForCTC (#18014)\n\nCo-authored-by: Sreyan-G@NVIDIA <sreyang@nvidia.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TFWav2Vec2ForCTC(TFWav2Vec2PreTrainedModel):",
            "loss = tf.reduce_sum(loss)",
            "if self.config.ctc_loss_reduction == \"mean\":",
            "loss = tf.reduce_mean(loss)",
            "+",
            "+            loss = tf.reshape(loss, (1,))",
            "else:",
            "loss = None"
        ]
    },
    {
        "number": 1331,
        "comments": "",
        "commit_message": "lint fixes\n\nSummary:\nRan the linter.\nTODO: need to update the linter as per D21353065.\n\nReviewed By: bottler\n\nDifferential Revision: D21362270\n\nfbshipit-source-id: ad0e781de0a29f565ad25c43bc94a19b1828c020\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestPointMeshDistance(TestCaseMixin, unittest.TestCase):",
            "self.assertClose(loss_op, loss_naive)",
            "",
            "# Compare backward pass",
            "-        rand_val = torch.rand((1)).item()",
            "+        rand_val = torch.rand(1).item()",
            "grad_dist = torch.tensor(rand_val, dtype=torch.float32, device=device)",
            "",
            "loss_naive.backward(grad_dist)"
        ]
    },
    {
        "number": 1332,
        "comments": "",
        "commit_message": "Fix //examples/text_embeddings_v2.\n\nDuring refactor to public APIs, it ended up using the wrong filename.\n\nPiperOrigin-RevId: 268192870\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TextEmbeddingModel(tf.train.Checkpoint):",
            "# Assign the table initializer to this instance to ensure the asset",
            "# it depends on is saved with the SavedModel.",
            "self._table_initializer = tf.lookup.TextFileInitializer(",
            "-        vocab_file_path, tf.string, tf.lookup.TextFileIndex.WHOLE_LINE,",
            "+        write_vocabulary_file(self._vocabulary),",
            "+        tf.string, tf.lookup.TextFileIndex.WHOLE_LINE,",
            "tf.int64, tf.lookup.TextFileIndex.LINE_NUMBER)",
            "self._table = tf.lookup.StaticVocabularyTable(",
            "self._table_initializer, num_oov_buckets=oov_buckets)"
        ]
    },
    {
        "number": 1333,
        "comments": "",
        "commit_message": "RANSAC improvements (#1435)\n\n- Added LU solver for RANSAC-homography, it speeds-up the minimal-solver part x10\n- Now find_fundamental weights is optional, as it was for homography\n- Fix test precision and made one xFail test to be compulsory\n- Fixed tests for fundamental matrix, as one have to have >= 8 points (also added to docstring)\n- updates deprecated torch.svd to torch.linalg.svd\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def decompose_essential_matrix(E_mat: torch.Tensor) -> Tuple[torch.Tensor, torch",
            "raise AssertionError(E_mat.shape)",
            "",
            "# decompose matrix by its singular values",
            "-    U, _, V = torch.svd(E_mat)",
            "+    U, _, V = _torch_svd_cast(E_mat)",
            "Vt = V.transpose(-2, -1)",
            "",
            "mask = torch.ones_like(E_mat)"
        ]
    },
    {
        "number": 1335,
        "comments": "",
        "commit_message": "[Examples] Replicates the new --log_level feature to all trainer-based pytorch (#12359)\n\n* added log_level\n\n* fix comment\n\n* fixed log_level\n\n* Trigger CI\n\n* Unfied logging\n\n* simplified args for log_level\n",
        "label": "",
        "answer": "no",
        "change": [
            "def main():",
            "",
            "model.resize_token_embeddings(len(tokenizer))",
            "",
            "-    # Preprocessing the raw_datasets.",
            "+    # Preprocessing the datasets.",
            "# First we tokenize all the texts.",
            "padding = \"max_length\" if args.pad_to_max_length else False"
        ]
    },
    {
        "number": 1338,
        "comments": "",
        "commit_message": "Fixed weight init for fused weight matrices in fused MHA by adding correct gain factor.\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class EncdecMultiheadAttn(nn.Module):",
            "",
            "def reset_parameters(self):",
            "nn.init.xavier_uniform_(self.in_proj_weight_q)",
            "-        nn.init.xavier_uniform_(self.in_proj_weight_kv)",
            "+        # in_proj_weight_kv has shape [2 * hidden, hidden] but it should be",
            "+        # initialized like a [hidden, hidden] matrix.",
            "+        # sqrt(6 / (hidden + hidden)) / sqrt(6 / (2 * hidden + hidden)) = sqrt(1.5)",
            "+        # therefore xavier_uniform gain should be set to sqrt(1.5).",
            "+        nn.init.xavier_uniform_(self.in_proj_weight_kv, gain=math.sqrt(1.5))",
            "nn.init.xavier_uniform_(self.out_proj_weight)",
            "if self.bias:",
            "nn.init.constant_(self.in_proj_bias_q, 0.)"
        ]
    },
    {
        "number": 1341,
        "comments": "",
        "commit_message": "fixed to already implemented fucntion\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class FeedForwardTransformer(TTSInterface, torch.nn.Module):",
            "spembs = None",
            "",
            "# get option",
            "-        alpha = getattr(inference_args, \"fastspeech_alpha\", None)",
            "+        alpha = getattr(inference_args, \"fastspeech_alpha\", 1.0)",
            "",
            "# inference",
            "_, outs, _ = self._forward("
        ]
    },
    {
        "number": 1343,
        "comments": "",
        "commit_message": "small bug fixes for torch gradients module.\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def adam_update(ws, dcdws, lr, mw, vw, step, beta1=0.9, beta2=0.999, epsilon=1e-",
            "",
            "def stop_gradient(x, preserve_type=True):",
            "is_var = is_variable(x)",
            "-    # ToDo: work out why _torch.tensor() wrapping is necessary in certain cases, presumably .detach() should be enough.",
            "-    x = _torch.tensor(x.detach())",
            "+    x = x.detach()",
            "if is_var and preserve_type:",
            "-        return variable(x)",
            "+        return x.requires_grad_()",
            "return x"
        ]
    },
    {
        "number": 1344,
        "comments": "",
        "commit_message": "rename log_softmax, support dim, fix onnx Softmax\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class SpeedyResNet(nn.Module):",
            "])",
            "self.lin = nn.Linear(512, num_classes, bias=False)",
            "",
            "-  # note, pytorch just uses https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html instead of logsoftmax",
            "+  # note, pytorch just uses https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html instead of log_softmax",
            "def forward(self, x):",
            "x = self.ic(x)",
            "x = self.ib(x)"
        ]
    },
    {
        "number": 1346,
        "comments": "",
        "commit_message": "Drop torch 1.6 testing (#10390)\n\n* Drop torch 1.6 support\n\n* Drop 1.6 support\n\n* Update CHANGELOG\n\n* Fixes\n\n* Split change\n\n* Undo change\n\n* 1.7 -> 1.7.1\n\nhttps://github.com/pytorch/pytorch/issues/47354\n\n* Force trigger nightly\n\n* Update .github/workflows/events-nightly.yml\n\nCo-authored-by: Aki Nitta <nitta@akihironitta.com>\n\n* Revert 1.7.1 change - try wildcard\n\n* Update adjust versions and test it\n\n* Undo test changes\n\n* Revert \"Undo test changes\"\n\nThis reverts commit 3a6acadd115e86f02d83a788f1978372ab6764f3.\n\n* Update CHANGELOG.md\n\nCo-authored-by: Aki Nitta <nitta@akihironitta.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def reset_deterministic_algorithm():",
            "yield",
            "if _TORCH_GREATER_EQUAL_1_8:",
            "torch.use_deterministic_algorithms(False)",
            "-    elif _TORCH_GREATER_EQUAL_1_7:",
            "+    else:",
            "torch.set_deterministic(False)",
            "-    else:  # the minimum version Lightning supports is PyTorch 1.6",
            "-        torch._set_deterministic(False)",
            "",
            "",
            "@pytest.fixture"
        ]
    },
    {
        "number": 1347,
        "comments": "",
        "commit_message": "fix (#1174)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class LocalSyncParallelOptimizer(object):",
            "",
            "# Then setup the per-device loss graphs that use the shared weights",
            "self._batch_index = tf.placeholder(tf.int32)",
            "-        data_splits = zip(",
            "-            *[tf.split(ph, len(devices)) for ph in input_placeholders])",
            "+",
            "+        # Split on the CPU in case the data doesn't fit in GPU memory.",
            "+        with tf.device(\"/cpu:0\"):",
            "+            data_splits = zip(",
            "+                *[tf.split(ph, len(devices)) for ph in input_placeholders])",
            "+",
            "self._towers = []",
            "for device, device_placeholders in zip(self.devices, data_splits):",
            "self._towers.append(self._setup_device(device,"
        ]
    },
    {
        "number": 1349,
        "comments": "",
        "commit_message": "refactor BaseWorker.send_command to accept explicit arguments. (#3487)\n\n* syft: refactor BaseWorker.send_command to accept explicit arguments.\n\nsyft: remove unused variables\n\n* syft: fix linter issues.\n\n* syft: fix argument type of args_ and update docstring for send_command of BaseWoker class.\n\n* syft, pointers: fix int type to tuple conversion.\n\nCo-authored-by: Shubham Gupta <shubamgupta3121@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TorchHook(FrameworkHook):",
            "@wraps(attr)",
            "def overloaded_attr(self_torch, *args, **kwargs):",
            "ptr = hook_self.local_worker.send_command(",
            "-                recipient=self_torch.worker(), message=(f\"{'torch'}.{attr}\", None, args, kwargs)",
            "+                recipient=self_torch.worker(),",
            "+                cmd_name=f\"{'torch'}.{attr}\",",
            "+                args_=args,",
            "+                kwargs_=kwargs,",
            ")",
            "",
            "return ptr.wrap()"
        ]
    },
    {
        "number": 1352,
        "comments": "",
        "commit_message": "word_language_model: Fix Transformer init_weights\n\nModel was not getting initialized property since it was using the\ndecoder object instead of decoder weight to initialize zeros.\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TransformerModel(nn.Module):",
            "def init_weights(self):",
            "initrange = 0.1",
            "nn.init.uniform_(self.encoder.weight, -initrange, initrange)",
            "-        nn.init.zeros_(self.decoder)",
            "+        nn.init.zeros_(self.decoder.weight)",
            "nn.init.uniform_(self.decoder.weight, -initrange, initrange)",
            "",
            "def forward(self, src, has_mask=True):"
        ]
    },
    {
        "number": 1354,
        "comments": "",
        "commit_message": "Implement RGB to LUV (#442)\n\n* add rgb <-> luv and doc update\n\n* add gamma nonlinearity in luv\n\n* increase precision on xyz->rgb coeff\n\n* finish tests\n\n* update docs and add citation\n\n* add comments\n\n* fix docs\n\n* update test and correct docs\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def xyz_to_rgb(image: torch.Tensor) -> torch.Tensor:",
            "y: torch.Tensor = image[..., 1, :, :]",
            "z: torch.Tensor = image[..., 2, :, :]",
            "",
            "-    r: torch.Tensor = 3.240479 * x + -1.53715 * y + -0.498535 * z",
            "-    g: torch.Tensor = -0.969256 * x + 1.875991 * y + 0.041556 * z",
            "-    b: torch.Tensor = 0.055648 * x + -0.204043 * y + 1.057311 * z",
            "+    r: torch.Tensor = 3.2404813432005266 * x + -1.5371515162713185 * y + -0.4985363261688878 * z",
            "+    g: torch.Tensor = -0.9692549499965682 * x + 1.8759900014898907 * y + 0.0415559265582928 * z",
            "+    b: torch.Tensor = 0.0556466391351772 * x + -0.2040413383665112 * y + 1.0573110696453443 * z",
            "",
            "out: torch.Tensor = torch.stack((r, g, b), dim=-3)"
        ]
    },
    {
        "number": 1356,
        "comments": "",
        "commit_message": "[MRG] Image regressor Added (#92)\n\n* move softmax to loss\n\n* image regressor class created\n\n* metric loss implemented\n\n* util bug fixed\n\n* regressor tested\n\n* bug fix\n\n* refactor bo\n\n* rename\n\n* test fixed\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def set_stub_weight_to_torch(stub_layer, torch_layer):",
            "",
            "",
            "def set_stub_weight_to_keras(stub_layer, keras_layer):",
            "-    stub_layer.export_weights_keras(keras_layer)",
            "\\ No newline at end of file",
            "+    stub_layer.export_weights_keras(keras_layer)"
        ]
    },
    {
        "number": 1357,
        "comments": "",
        "commit_message": "fix reshape error\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class MultiHeadAttention(nn.Module):",
            "# perform attention, result size = (n_head * mb_size) x len_q x d_v",
            "outputs, attns = self.attention(q_s, k_s, v_s, attn_mask=attn_mask.repeat(n_head, 1, 1))",
            "",
            "-        # back to original mb_size batch",
            "-        outputs = outputs.view(mb_size, len_q, -1)            # mb_size x len_q x (n_head*d_v)",
            "+        # back to original mb_size batch, result size = mb_size x len_q x (n_head*d_v)",
            "+        outputs = torch.cat(torch.split(outputs, mb_size, dim=0), dim=-1)",
            "",
            "# project back to residual size",
            "outputs = self.proj(outputs)"
        ]
    },
    {
        "number": 1360,
        "comments": "",
        "commit_message": "upgrade to pytorch 0.4.0 (#1126)\n\n* bump pytorch to 0.4 + fix sanitize\n\n* remove check for Variable in block_orthogonal\n\n* rename parameter split_size=\n\n* remove checks for Variable\n\n* fix more tests\n\n* fixes\n\n* get tests to pass\n\n* fix warnings\n\n* get rid of some of the Variables\n\n* more tests passing\n\n* more elimination of variables\n\n* finish removing all Variables\n\n* pylint and such\n\n* a few fixes\n\n* move torch.no_grad into model.forward_on_instances\n\n* more pytorch 0.4 changes\n\n* detach() -> data\n\n* pylint\n\n* fix bad tensor creation\n\n* fix types\n\n* remove print statement\n\n* more 0.4 goodness\n\n* factor out is_tensor\n\n* add no_grad to elmo command\n\n* cleanup\n\n* remove TODO\n\n* address PR feedback\n\n* replace all() with item()\n\n* more cleanup\n\n* further cleanup\n\n* remove Variable\n\n* really fix merge conflict\n\n* fix pylint\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestTokenCharactersEncoder(AllenNlpTestCase):",
            "",
            "def test_forward_applies_embedding_then_encoder(self):",
            "numpy_tensor = numpy.random.randint(6, size=(3, 4, 7))",
            "-        inputs = Variable(torch.from_numpy(numpy_tensor))",
            "+        inputs = torch.from_numpy(numpy_tensor)",
            "encoder_output = self.encoder(inputs)",
            "reshaped_input = inputs.view(12, 7)",
            "embedded = self.embedding(reshaped_input)"
        ]
    },
    {
        "number": 1361,
        "comments": "",
        "commit_message": "Add tests to Trainer (#6605)\n\n* Add tests to Trainer\n\n* Test if removing long breaks everything\n\n* Remove ugly hack\n\n* Fix distributed test\n\n* Use float for number of epochs\n",
        "label": "",
        "answer": "no",
        "change": [
            "def default_data_collator(features: List[InputDataClass]) -> Dict[str, torch.Ten",
            "if isinstance(v, torch.Tensor):",
            "batch[k] = torch.stack([f[k] for f in features])",
            "else:",
            "-                batch[k] = torch.tensor([f[k] for f in features], dtype=torch.long)",
            "+                batch[k] = torch.tensor([f[k] for f in features])",
            "",
            "return batch"
        ]
    },
    {
        "number": 1363,
        "comments": "",
        "commit_message": "Optimize Stable Diffusion (#371)\n\n* initial commit\n\n* make UNet stream capturable\n\n* try to fix noise_pred value\n\n* remove cuda graph and keep NB\n\n* non blocking unet with PNDMScheduler\n\n* make timesteps np arrays for pndm scheduler\nbecause lists don't get formatted to tensors in `self.set_format`\n\n* make max async in pndm\n\n* use channel last format in unet\n\n* avoid moving timesteps device in each unet call\n\n* avoid memcpy op in `get_timestep_embedding`\n\n* add `channels_last` kwarg to `DiffusionPipeline.from_pretrained`\n\n* update TODO\n\n* replace `channels_last` kwarg with `memory_format` for more generality\n\n* revert the channels_last changes to leave it for another PR\n\n* remove non_blocking when moving input ids to device\n\n* remove blocking from all .to() operations at beginning of pipeline\n\n* fix merging\n\n* fix merging\n\n* model can run in other precisions without autocast\n\n* attn refactoring\n\n* Revert \"attn refactoring\"\n\nThis reverts commit 0c70c0e189cd2c4d8768274c9fcf5b940ee310fb.\n\n* remove restriction to run conv_norm in fp32\n\n* use `baddbmm` instead of `matmul`for better in attention for better perf\n\n* removing all reshapes to test perf\n\n* Revert \"removing all reshapes to test perf\"\n\nThis reverts commit 006ccb8a8c6bc7eb7e512392e692a29d9b1553cd.\n\n* add shapes comments\n\n* hardcore whats needed for jitting\n\n* Revert \"hardcore whats needed for jitting\"\n\nThis reverts commit 2fa9c698eae2890ac5f8e367ca80532ecf94df9a.\n\n* Revert \"remove restriction to run conv_norm in fp32\"\n\nThis reverts commit cec592890c32da3d1b78d38b49e4307aedf459b9.\n\n* revert using baddmm in attention's forward\n\n* cleanup comment\n\n* remove restriction to run conv_norm in fp32. no quality loss was noticed\n\nThis reverts commit cc9bc1339c998ebe9e7d733f910c6d72d9792213.\n\n* add more optimizations techniques to docs\n\n* Revert \"add shapes comments\"\n\nThis reverts commit 31c58eadb8892f95478cdf05229adf678678c5f4.\n\n* apply suggestions\n\n* make quality\n\n* apply suggestions\n\n* styling\n\n* `scheduler.timesteps` are now arrays so we dont need .to()\n\n* remove useless .type()\n\n* use mean instead of max in `test_stable_diffusion_inpaint_pipeline_k_lms`\n\n* move scheduler timestamps to correct device if tensors\n\n* add device to `set_timesteps` in LMSD scheduler\n\n* `self.scheduler.set_timesteps` now uses device arg for schedulers that accept it\n\n* quick fix\n\n* styling\n\n* remove kwargs from schedulers `set_timesteps`\n\n* revert to using max in K-LMS inpaint pipeline test\n\n* Revert \"`self.scheduler.set_timesteps` now uses device arg for schedulers that accept it\"\n\nThis reverts commit 00d5a51e5c20d8d445c8664407ef29608106d899.\n\n* move timesteps to correct device before loop in SD pipeline\n\n* apply previous fix to other SD pipelines\n\n* UNet now accepts tensor timesteps even on wrong device, to avoid errors\n- it shouldnt affect performance if timesteps are alrdy on correct device\n- it does slow down performance if they're on the wrong device\n\n* fix pipeline when timesteps are arrays with strides\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def get_timestep_embedding(",
            "assert len(timesteps.shape) == 1, \"Timesteps should be a 1d-array\"",
            "",
            "half_dim = embedding_dim // 2",
            "-    exponent = -math.log(max_period) * torch.arange(start=0, end=half_dim, dtype=torch.float32)",
            "+    exponent = -math.log(max_period) * torch.arange(",
            "+        start=0, end=half_dim, dtype=torch.float32, device=timesteps.device",
            "+    )",
            "exponent = exponent / (half_dim - downscale_freq_shift)",
            "",
            "-    emb = torch.exp(exponent).to(device=timesteps.device)",
            "+    emb = torch.exp(exponent)",
            "emb = timesteps[:, None].float() * emb[None, :]",
            "",
            "# scale embeddings"
        ]
    },
    {
        "number": 1364,
        "comments": "",
        "commit_message": "PaillierTensor and Arg Type Matching (experimental) (#2740)\n\n* Cleanup directory names\n\nWith syft.frameworks.torch, the directory names have becomee disorganized\nand don't seem to follow any set standard. Also, some are simply too long\nand cumbersome to be typing out all the time (such as differential_privacy).\n\nI have replaced long names with their shorthand options, federated leearning\nto simply fl, differential_privacy to dp. I also changed crypto to mpc\nsince all of the algorithms contained within the folder exclusively\nrelated to secure multi-party computation. If and when we implement\na wider varitey of crypto algorithms, we can reconsider this name. However,\nat present, I would like for us to use a more descriptive directory name.\n\n* Revert changed notbooks\n\nI testeed all the notebooks to ensure that the work with the\nrenamd directories but don't wish to actually change them\n\n* Revert changed notbooks\n\nI testeed all the notebooks to ensure that the work with the\nrenamd directories but don't wish to actually change them\n\n* Revert setup.py change\n\n* Revert docs change\n\n* Revert docs change\n\n* Revert docs change\n\n* Init boilerplate example tensor\n\nI am beginning this project by copy-pasting LoggingTensor as a boilerplate\nexample of how to create a custom tensor type in PySyft. I then changed\nall instances of LoggingTensor to PaillierTensor\n\n* Init experimental notebook\n\nI'll be doing my experimenting and development/testing in this notebook\n\n* Add PaillierTensor to hook_args\n\nIn this commit, I added PaillierTensor to the hook_args\nconfiguration file\n\n* Add support for encrypt, decrypt, and __add__\n\nI added support for basic encryption, decryption, and addition to PaillierTensor.\nData within the tensor is stored as a numpy array of phe scalars, where phe\nis the python package for paillier homomorphic encryption. This will allow\nus to use many of the desirable numpy operations as needed\n\n* Remove unused boilerplate\n\n* Parallelize homomorphic encryption and decryption\n\nSince the encryption and decryption step was quite slow, I parallelizd it\nusing the multiprocessing library. I'm pretty sure I'll be able to do this\nfor all of the paillier operations, greatly increasing the speed of the lib.\n\n* Move thread pool to syft.\n\nI don't want to have to re-initialize the thread pool over and over, so I'm\nadding it as a global syft variable\n\n* Hook args automatically\n\nSince PyTorch has a lot of similar methods to Numpy, by defaeult se can use\nthe normal hooking logic for most methods\n\n* Add experimental support for mis-matching args\n\nOne thing we want to be able to do is to add a tensor of scalars to a tensor of\nencrypted valus and vise versa. This was very tricky to do and we need to\nlook closer to make sure I didn't add any security issues but I think\nit looks right.\n\n* Add comment\n\n* Remove unused code\n\n* Add support for encrypted matrix multiplication\n\n* Fix bug in arithmetic\n\n* Fix bug in matmul\n\n* Fix bug in test\n\n* Remove verbose comments\n\n* Run black\n\n* Add phe to requirements.txt\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_section_1_differential_privacy():",
            "query_result = np.argmax(counts)",
            "query_result",
            "",
            "-    from syft.frameworks.torch.differential_privacy import pate",
            "+    from syft.frameworks.torch.dp import pate",
            "",
            "num_teachers, num_examples, num_labels = (100, 100, 10)",
            "preds = (np.random.rand(num_teachers, num_examples) * num_labels).astype(int)  # fake preds"
        ]
    },
    {
        "number": 1365,
        "comments": "",
        "commit_message": "[ENHANCE] removed deprecated codes for v6.0 (#1281)\n\n* removed deprecation warning and deprecated codes\n\n* Update\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix\n\n* fix\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestNormalize:",
            "f = kornia.enhance.Normalize(mean=mean, std=std)",
            "data = torch.ones(2, 3, 256, 313)",
            "if isinstance(mean, float):",
            "-            expected = (data - torch.tensor(mean)) / torch.tensor(std)",
            "+            expected = (data - torch.as_tensor(mean)) / torch.as_tensor(std)",
            "else:",
            "-            expected = (data - torch.tensor(mean[0])) / torch.tensor(std[0])",
            "+            expected = (data - torch.as_tensor(mean[0])) / torch.as_tensor(std[0])",
            "assert_close(f(data), expected)",
            "",
            "@staticmethod"
        ]
    },
    {
        "number": 1368,
        "comments": "",
        "commit_message": "fixed problem with multi-state/action networks and scoping\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Categorical(Distribution):",
            "logits = [log(prob) for _ in range(util.prod(shape)) for prob in probabilities]",
            "action_size = util.prod(self.shape) * self.num_actions",
            "",
            "-        with tf.name_scope(name=scope):",
            "-            self.logits = Linear(size=action_size, bias=logits, scope='logits')",
            "+        self.logits = Linear(size=action_size, bias=logits, scope='logits')",
            "",
            "super(Categorical, self).__init__(scope, summary_labels)"
        ]
    },
    {
        "number": 1371,
        "comments": "",
        "commit_message": "lint fixes\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Data(object):",
            "return self",
            "",
            "def cuda(self, props=None):",
            "-        func = lambda x: x.cuda() if torch.cuda.is_available() else x  # noqa",
            "+        def func(x):",
            "+            return x.cuda() if torch.cuda.is_available() else x",
            "+",
            "return self._transer(func, props)",
            "",
            "def cpu(self, props=None):"
        ]
    },
    {
        "number": 1372,
        "comments": "",
        "commit_message": "GH-48: fix warning: flatten parameters in language model\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class LanguageModel(nn.Module):",
            "encoded = self.encoder(input)",
            "emb = self.drop(encoded)",
            "",
            "+        self.rnn.flatten_parameters()",
            "+",
            "output, hidden = self.rnn(emb, hidden)",
            "",
            "if self.proj is not None:"
        ]
    },
    {
        "number": 1373,
        "comments": "",
        "commit_message": "[Train] Fix accuracy calculation for CIFAR example (#22292)\n\nSame as #21689 except for cifar\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def train_func(config):",
            "train_dataset = Subset(train_dataset, list(range(64)))",
            "validation_dataset = Subset(validation_dataset, list(range(64)))",
            "",
            "-    train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"])",
            "-    validation_loader = DataLoader(validation_dataset, batch_size=config[\"batch_size\"])",
            "+    worker_batch_size = config[\"batch_size\"] // train.world_size()",
            "+",
            "+    train_loader = DataLoader(train_dataset, batch_size=worker_batch_size)",
            "+    validation_loader = DataLoader(validation_dataset, batch_size=worker_batch_size)",
            "",
            "train_loader = train.torch.prepare_data_loader(train_loader)",
            "validation_loader = train.torch.prepare_data_loader(validation_loader)"
        ]
    },
    {
        "number": 1374,
        "comments": "",
        "commit_message": "Fix test_model_parallelization (#17249)\n\n* Fix test_model_parallelization\n\n* Modify\n",
        "label": "",
        "answer": "no",
        "change": [
            "class ModelTesterMixin:",
            "memory_after_parallelization = get_current_gpu_memory_use()",
            "",
            "# Assert that the memory use on all devices is higher than it was when loaded only on CPU",
            "-            for n in range(torch.cuda.device_count()):",
            "+            for n in range(len(model.device_map.keys())):",
            "self.assertGreater(memory_after_parallelization[n], memory_at_start[n])",
            "",
            "# Assert that the memory use of device 0 is lower than it was when the entire model was loaded on it"
        ]
    },
    {
        "number": 1377,
        "comments": "",
        "commit_message": "Style fixes\n",
        "label": "",
        "answer": "no",
        "change": [
            "def deconv2d(x, kernel, output_shape, strides=(1, 1),",
            "x = _preprocess_conv2d_input(x, dim_ordering)",
            "output_shape = _preprocess_deconv_output_shape(output_shape, dim_ordering)",
            "kernel = _preprocess_conv2d_kernel(kernel, dim_ordering)",
            "-    kernel = tf.transpose(kernel, (0, 1, 3, 2))  # tranpose kernel chanels",
            "+    kernel = tf.transpose(kernel, (0, 1, 3, 2))",
            "padding = _preprocess_border_mode(border_mode)",
            "strides = (1,) + strides + (1,)"
        ]
    },
    {
        "number": 1378,
        "comments": "",
        "commit_message": "[Bert, et al] fix early device assignment (#14447)\n\n* fix early device assignment\n\n* more models\n",
        "label": "",
        "answer": "no",
        "change": [
            "class FNetEmbeddings(nn.Module):",
            "if version.parse(torch.__version__) > version.parse(\"1.6.0\"):",
            "self.register_buffer(",
            "\"token_type_ids\",",
            "-                torch.zeros(self.position_ids.size(), dtype=torch.long, device=self.position_ids.device),",
            "+                torch.zeros(self.position_ids.size(), dtype=torch.long),",
            "persistent=False,",
            ")"
        ]
    },
    {
        "number": 1380,
        "comments": "",
        "commit_message": "fixing bug for #822\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Conv2dSubsampling(torch.nn.Module):",
            "torch.nn.ReLU()",
            ")",
            "self.out = torch.nn.Sequential(",
            "-            torch.nn.Linear(odim * (idim // 4), odim),",
            "+            torch.nn.Linear(odim * ((idim - 1)// 4), odim),",
            "PositionalEncoding(odim, dropout_rate)",
            ")"
        ]
    },
    {
        "number": 1383,
        "comments": "",
        "commit_message": "[Feat] Affine scale with non-isotropic values (#646)\n\n* Updated non-isotropic scaling\n\n* Fixed tracerWarning\n\n* Updated 3D non-isotropic scaling\n\n* Refined code\n\n* Added some comments\n\n* Fixed a weird module import error\n\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestInvertAffineTransform:",
            "",
            "def test_rot90_batch(self, device):",
            "angle = torch.tensor([90.]).to(device)",
            "-        scale = torch.tensor([1.]).to(device)",
            "+        scale = torch.tensor([[1., 1.]]).to(device)",
            "center = torch.tensor([[0., 0.]]).to(device)",
            "expected = torch.tensor([[",
            "[0., -1., 0.],"
        ]
    },
    {
        "number": 1384,
        "comments": "",
        "commit_message": "Reproducibility 3/3 (#1924)\n\n* make tests deterministic\n\n* run slow tests\n\n* prepare for testing\n\n* finish\n\n* refactor\n\n* add print statements\n\n* finish more\n\n* correct some test failures\n\n* more fixes\n\n* set up to correct tests\n\n* more corrections\n\n* up\n\n* fix more\n\n* more prints\n\n* add\n\n* up\n\n* up\n\n* up\n\n* uP\n\n* uP\n\n* more fixes\n\n* uP\n\n* up\n\n* up\n\n* up\n\n* up\n\n* fix more\n\n* up\n\n* up\n\n* clean tests\n\n* up\n\n* up\n\n* up\n\n* more fixes\n\n* Apply suggestions from code review\n\nCo-authored-by: Suraj Patil <surajp815@gmail.com>\n\n* make\n\n* correct\n\n* finish\n\n* finish\n\nCo-authored-by: Suraj Patil <surajp815@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class LDMSuperResolutionPipelineIntegrationTests(unittest.TestCase):",
            "ldm.to(torch_device)",
            "ldm.set_progress_bar_config(disable=None)",
            "",
            "-        generator = torch.Generator(device=torch_device).manual_seed(0)",
            "+        generator = torch.manual_seed(0)",
            "image = ldm(image=init_image, generator=generator, num_inference_steps=20, output_type=\"numpy\").images",
            "",
            "image_slice = image[0, -3:, -3:, -1]",
            "",
            "assert image.shape == (1, 256, 256, 3)",
            "-        expected_slice = np.array([0.7418, 0.7472, 0.7424, 0.7422, 0.7463, 0.726, 0.7382, 0.7248, 0.6828])",
            "+        expected_slice = np.array([0.7644, 0.7679, 0.7642, 0.7633, 0.7666, 0.7560, 0.7425, 0.7257, 0.6907])",
            "+",
            "assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-2"
        ]
    },
    {
        "number": 1386,
        "comments": "",
        "commit_message": "use assertClose\n\nSummary: use assertClose in some tests, which enforces shape equality. Fixes some small problems, including graph_conv on an empty graph.\n\nReviewed By: nikhilaravi\n\nDifferential Revision: D20556912\n\nfbshipit-source-id: 60a61eafe3c03ce0f6c9c1a842685708fb10ac5b\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestMeshEdgeLoss(unittest.TestCase):",
            "loss = mesh_edge_loss(meshes, target_length=target_length)",
            "",
            "predloss = TestMeshEdgeLoss.mesh_edge_loss_naive(meshes, target_length)",
            "-        self.assertTrue(torch.allclose(loss, predloss))",
            "+        self.assertClose(loss, predloss)",
            "",
            "@staticmethod",
            "def mesh_edge_loss("
        ]
    },
    {
        "number": 1387,
        "comments": "",
        "commit_message": "Remove tl.layers.initialize_global_variables(sess) (#931)\n\n* update sampling layers\n\n* upadte zoom\n\n* fix bug zoom\n\n* typo\n\n* fix bug affine_transform_cv2 x and y\n\n* fix bug crop when crop size equal to image size\n\n* fix file docs typo\n\n* fix bug instance norm\n\n* fix docs\n\n* update examples , init variables\n\n* changelog\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "lambd = .99  # decay factor",
            "e = 0.1  # e-Greedy Exploration, the larger the more random",
            "num_episodes = 10000",
            "with tf.Session() as sess:",
            "-    tl.layers.initialize_global_variables(sess)",
            "+    sess.run(tf.global_variables_initializer())",
            "for i in range(num_episodes):",
            "## Reset environment and get first new observation",
            "episode_time = time.time()"
        ]
    },
    {
        "number": 1390,
        "comments": "",
        "commit_message": "[IMPORTANT] release test script for layers | basic layer update (#389)\n\n* release test script for layers\n* update basic layer \n* fixed averaged embedding layer for python2\n",
        "label": "",
        "answer": "no",
        "change": [
            "sess = tf.InteractiveSession()",
            "",
            "batch_size = 128",
            "x = tf.placeholder(tf.float32, shape=[None, 784])",
            "-y_ = tf.placeholder(",
            "-    tf.int64, shape=[",
            "-        None,",
            "-    ])",
            "+y_ = tf.placeholder(tf.int64, shape=[None])",
            "",
            "",
            "def keras_block(x):"
        ]
    },
    {
        "number": 1392,
        "comments": "",
        "commit_message": "fix lint and docstring failures\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def ceil(x: torch.Tensor, *, out: Optional[torch.Tensor] = None) -> torch.Tensor",
            "return torch.ceil(x, out=out)",
            "",
            "",
            "-def floor(x: torch.Tensor,",
            "-          *,",
            "-          out: Optional[torch.Tensor] = None",
            "-          ) -> torch.Tensor:",
            "+def floor(x: torch.Tensor, *, out: Optional[torch.Tensor] = None) -> torch.Tensor:",
            "if \"int\" in str(x.dtype):",
            "if ivy.exists(out):",
            "return ivy.inplace_update(out, x)"
        ]
    },
    {
        "number": 1393,
        "comments": "",
        "commit_message": "Squashed commit of the following:\n\ncommit 047d0c474c18a87c205e566948410be16787e477\nMerge: 9396ed37d bfe7bca3a\nAuthor: Shinji Watanabe <sw005320@gmail.com>\nDate:   Thu May 19 09:50:02 2022 -0400\n\n    Merge pull request #4378 from akreal/fix-check_short_utt\n\n    Fix minimum input length for Conv2dSubsampling2 in check_short_utt\n\ncommit bfe7bca3a98da52714e1c45906cf826704464b7c\nAuthor: Pavel Denisov <pavel.denisov@ims.uni-stuttgart.de>\nDate:   Thu May 19 13:41:59 2022 +0200\n\n    Fix minimum input length for Conv2dSubsampling2 in check_short_utt\n\ncommit 9396ed37deb8b101fd064d46c85975ad9047bf87\nMerge: c54b585c1 e047156ec\nAuthor: Naoyuki Kamo <naoyuki.kamo829@gmail.com>\nDate:   Thu May 19 14:50:56 2022 +0900\n\n    Merge pull request #4376 from kamo-naoyuki/libsndfile\n\n    Remove the restriction for libsndfile version\n\ncommit c54b585c1ca6693ae7ba7e299a48af762eda6adf\nMerge: 9ca49caed 88465607c\nAuthor: Tomoki Hayashi <hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp>\nDate:   Thu May 19 12:29:02 2022 +0900\n\n    Merge pull request #4374 from YosukeHiguchi/master\n\n    Minor fixes for the intermediate loss usage and Mask-CTC decoding\n\ncommit e047156ec8df3266259aed03742ac798e365f648\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Thu May 19 10:11:08 2022 +0900\n\n    remove version restiction for libsndfile\n\ncommit 9ca49caed98410cd7d2c71e4781819a1e92b35d9\nMerge: b008ac7d5 2952c3bca\nAuthor: Naoyuki Kamo <naoyuki.kamo829@gmail.com>\nDate:   Thu May 19 09:38:33 2022 +0900\n\n    Merge pull request #4375 from espnet/kamo-naoyuki-patch-1\n\n    Update .mergify.yml\n\ncommit 88465607cf5e899b8ce1b93c5c9fe09b69a2ab83\nAuthor: Yosuke Higuchi <wasapon.dev@gmail.com>\nDate:   Thu May 19 07:05:29 2022 +0900\n\n    fix for test\n\ncommit 2952c3bca26a70723094d5a160387b7936f71769\nAuthor: Naoyuki Kamo <naoyuki.kamo829@gmail.com>\nDate:   Thu May 19 06:59:02 2022 +0900\n\n    Update .mergify.yml\n\ncommit b008ac7d58e9ced1a9f8c89cc85ee69d9e9461ab\nMerge: 3c96908ed 4203c9c9c\nAuthor: Naoyuki Kamo <naoyuki.kamo829@gmail.com>\nDate:   Thu May 19 06:32:44 2022 +0900\n\n    Merge pull request #4372 from kamo-naoyuki/isort\n\n    Add isort checking to the CI tests\n\ncommit 4de7aa562f74c596e5b616fd8278a50a707d0198\nAuthor: Yosuke Higuchi <wasapon.dev@gmail.com>\nDate:   Thu May 19 06:19:20 2022 +0900\n\n    fix for test\n\ncommit 9c83ddb46404334914764a8e4356ea8a4c3c806c\nAuthor: Yosuke Higuchi <wasapon.dev@gmail.com>\nDate:   Thu May 19 05:05:01 2022 +0900\n\n    support gpu decoding for mask-ctc\n\ncommit 49100e4f1b3fc389c5672dc2ca17973525c4bf02\nAuthor: Yosuke Higuchi <wasapon.dev@gmail.com>\nDate:   Thu May 19 05:03:29 2022 +0900\n\n    fix bug for returning intermediate states\n\ncommit 4203c9c9c9d5a68cd13d464290cead3738ed003d\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Wed May 18 17:47:22 2022 +0900\n\n    apply isort\n\ncommit d0f2eac70a5521adf59618ba3ce6603e2863f0c5\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Wed May 18 17:46:47 2022 +0900\n\n    modified for isort options\n\ncommit 8f73b73d23d34bf5f3e8ed2f625dca1916ea8683\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Wed May 18 16:38:34 2022 +0900\n\n    apply black\n\ncommit 6974dd4efc11e465d4a3d1a34190c7ed782dacee\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Wed May 18 16:35:15 2022 +0900\n\n    Add descriptions for isort\n\ncommit 24c3676a8d4c2e60d2726e9bcd9bdbed740610e0\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Wed May 18 16:16:53 2022 +0900\n\n    Apply isort\n\ncommit 3c96908edc5c592c9c99bba0640428613dc7c3cb\nMerge: c173c3093 aa5d6ffff\nAuthor: Jiatong <728307998@qq.com>\nDate:   Tue May 17 18:00:40 2022 -0700\n\n    Merge pull request #4341 from chintu619/st_bugfix\n\n    bug fixes in ST recipes\n\ncommit c173c30930631731e6836c274a591ad571749741\nMerge: e0e0620ac d38188cc3\nAuthor: Naoyuki Kamo <naoyuki.kamo829@gmail.com>\nDate:   Tue May 17 15:20:31 2022 +0900\n\n    Merge pull request #4371 from espnet/kamo-naoyuki-patch-1\n\n    Update .mergify.yml\n\ncommit d38188cc30af6cffc4ad0233e7e705e93511c11d\nAuthor: Naoyuki Kamo <naoyuki.kamo829@gmail.com>\nDate:   Tue May 17 13:43:40 2022 +0900\n\n    Update .mergify.yml\n\ncommit e0e0620acca0df345cf317a13c839d7d4d5c773f\nMerge: df053b8c1 2cfbbd337\nAuthor: Tomoki Hayashi <hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp>\nDate:   Tue May 17 13:01:02 2022 +0900\n\n    Merge pull request #4369 from kan-bayashi/minor_fix_jets\n\ncommit 2cfbbd337d64f68e1f937e37feeb544d972c4e0b\nAuthor: kan-bayashi <hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp>\nDate:   Tue May 17 11:06:00 2022 +0900\n\n    updated jets test\n\ncommit 17ab7747fe7e0d4d6885847f2c738253a859dedf\nAuthor: kan-bayashi <hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp>\nDate:   Tue May 17 11:05:52 2022 +0900\n\n    updated README\n\ncommit 6ec8c27815c6fded4c13b01b8d2707016e9e8e95\nAuthor: kan-bayashi <hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp>\nDate:   Tue May 17 09:25:41 2022 +0900\n\n    updated README\n\ncommit b1e6c752b0d94f3209593e0cdbd5b43d79e8076d\nAuthor: kan-bayashi <hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp>\nDate:   Tue May 17 09:19:54 2022 +0900\n\n    shorten jets test\n\ncommit df053b8c13c26fe289fc882751801fd781e9d43e\nMerge: afa8f8ec5 5aa543a9f\nAuthor: Tomoki Hayashi <hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp>\nDate:   Tue May 17 08:13:36 2022 +0900\n\n    Merge pull request #4364 from imdanboy/master\n\n    add e2e tts model: JETS\n\ncommit 5aa543a9ff6c329f5fc601f3aa053ffd4afb19ba\nAuthor: Tomoki Hayashi <hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp>\nDate:   Mon May 16 21:13:30 2022 +0900\n\n    minor fix of docstrings and comments\n\ncommit a82e78d18aca9c00bcf8f378c42e78a0de24940e\nAuthor: imdanboy <imdanboy@gmail.com>\nDate:   Fri May 13 22:28:31 2022 +0900\n\n    JETS; e2e tts model\n\ncommit afa8f8ec5b8ec77deb1a3c1531915ebbee7b80e6\nMerge: fffb3444f cd77501a8\nAuthor: Shinji Watanabe <sw005320@gmail.com>\nDate:   Fri May 13 17:36:30 2022 -0400\n\n    Merge pull request #4349 from pyf98/quantization\n\n    Add quantization in ESPnet2 for asr inference\n\ncommit fffb3444fe4d8ef2630a22dd145d6f1fb0caab46\nMerge: f840b8114 5331890e6\nAuthor: Naoyuki Kamo <naoyuki.kamo829@gmail.com>\nDate:   Fri May 13 20:36:39 2022 +0900\n\n    Merge pull request #4361 from espnet/kamo-naoyuki-patch-1\n\n    Update README.md\n\ncommit aa5d6ffff67079f2cbe6a7e1eba852e459f0f6a4\nAuthor: Chaitanya Narisetty <cnariset@andrew.cmu.edu>\nDate:   Fri May 13 05:15:32 2022 -0400\n\n    fix lm tag names\n\ncommit 3cac7bb7f732a694f4b87007271d394a9ee3838e\nAuthor: Chaitanya Narisetty <cnariset@andrew.cmu.edu>\nDate:   Fri May 13 05:07:55 2022 -0400\n\n    resolve conflicts and fix lm_train filenames\n\ncommit ea44663e8a24ebfcaa03f3bba149e561e970fdf3\nAuthor: Chaitanya Narisetty <cnariset@andrew.cmu.edu>\nDate:   Fri May 13 04:43:18 2022 -0400\n\n    review suggested changes\n\ncommit 650c733437da32627f88fe369555ce1955536087\nMerge: 6d1bd3a8e f840b8114\nAuthor: Chaitanya Narisetty <cnariset@andrew.cmu.edu>\nDate:   Fri May 13 03:18:08 2022 -0400\n\n    Merge branch 'espnet_master' into st_bugfix\n\ncommit 5331890e6a6a61a3006e5e2c13d47172f5587a29\nAuthor: Naoyuki Kamo <naoyuki.kamo829@gmail.com>\nDate:   Fri May 13 13:15:40 2022 +0900\n\n    Update README.md\n\ncommit f840b8114452b4803b8fb25c1f22a93da146e9ba\nMerge: 1b1241040 9cfd6af64\nAuthor: Naoyuki Kamo <naoyuki.kamo829@gmail.com>\nDate:   Fri May 13 13:13:34 2022 +0900\n\n    Merge pull request #4348 from kamo-naoyuki/1.11.0\n\n    Add pytorch=1.10.2 and 1.11.0 to ci configurations\n\ncommit 9cfd6af64a28237019196cd495fbd2943790ce21\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Fri May 13 09:58:04 2022 +0900\n\n    fix\n\ncommit 2625be71a722e7eb030dff4f71d8dc9599a33844\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Fri May 13 03:46:24 2022 +0900\n\n    remove warning\n\ncommit 9a2001fac56dddf5ba1c2eaec092cb420f83f7c9\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Fri May 13 03:44:11 2022 +0900\n\n    fix for pytorch1.11 (+= became inplace op)\n\ncommit 5518b6ba0af0bba9e9d59d6c47607656f49c9988\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Thu May 12 22:04:42 2022 +0900\n\n    fix import order\n\ncommit 98689a5f0bfd88efffdbbcdd5d924e186d563a91\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Thu May 12 21:17:35 2022 +0900\n\n    change to show the error logs when jobs are failed\n\ncommit bb0d0aaa9e9f9076ac88aad425ad2f2caef369a7\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Thu May 12 20:40:39 2022 +0900\n\n    fix code style\n\ncommit 934b161f1f714637c3d7d47c14f8c810a9df6fe2\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Thu May 12 20:33:58 2022 +0900\n\n    change to show the error logs when jobs are failed\n\ncommit 5c474b96c543c3d26e95b432355bcfd2bf8dc116\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Thu May 12 20:20:18 2022 +0900\n\n    remove verbosity options\n\ncommit 005aad11b37acf388c6b70143ab40a5231bc7a39\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Thu May 12 20:04:57 2022 +0900\n\n    fix\n\ncommit 5c4b966a957062e4de298bcb69fe8cf6f1365fd1\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Thu May 12 19:36:11 2022 +0900\n\n    remove tests for python=3.10.0 temporary\n\ncommit 809ac3741814b7d9ebdd351b9e0e9343e236977c\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Thu May 12 19:27:20 2022 +0900\n\n    fix\n\ncommit 86186b744fb2bfc259909c49cc906fb0856d15bf\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Thu May 12 19:10:18 2022 +0900\n\n    add installation for packaging\n\ncommit 8fbac77268906075043cbecfb3e1c5625b145fce\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Thu May 12 18:59:17 2022 +0900\n\n    fix\n\ncommit b0050d97da3d0545b62a5d21b029ddd016ce6ca1\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Thu May 12 18:56:52 2022 +0900\n\n    fix\n\ncommit 6e9035d42eea31cad87a7c8b87fc79635a6df7c2\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Thu May 12 18:32:33 2022 +0900\n\n    fix\n\ncommit 1c344a95ceb83b4b44675aee5326afeb9284d8e8\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Thu May 12 18:25:35 2022 +0900\n\n    change LooseVersion to parse\n\ncommit f899a05768436cc38fb432d6f002ab667983abbd\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Thu May 12 18:09:33 2022 +0900\n\n    fix\n\ncommit 7d5242212403e740c4d5b8ebd9a346a991ea50a9\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Thu May 12 18:09:15 2022 +0900\n\n    fix\n\ncommit b7cfdd9a70559271e45de103e242228f94e837ff\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Thu May 12 18:05:41 2022 +0900\n\n    Change LooseVersion to parse\n\ncommit d234b9ab30bbc2bb6fd42d6335421a6f8a9ed637\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Thu May 12 17:10:40 2022 +0900\n\n    fix\n\ncommit 1b1241040e1e30e575a182b6be8b8e4602badeb8\nMerge: 39bae01e4 52c238d02\nAuthor: Shinji Watanabe <sw005320@gmail.com>\nDate:   Wed May 11 13:00:13 2022 -0400\n\n    Merge pull request #4352 from espnetUser/master\n\n    Add unit test to streaming ASR inference\n\ncommit 52c238d02d50fcfb2c4e2a5058c743c7db913eec\nAuthor: espnetUser <81252087+espnetUser@users.noreply.github.com>\nDate:   Wed May 11 16:10:04 2022 +0200\n\n    Applied black formating to test_asr_inference.py for PR\n\ncommit 87c7573874aeec096dd1e902478d3dd6e2c83ad2\nAuthor: espnetUser <81252087+espnetUser@users.noreply.github.com>\nDate:   Wed May 11 15:43:01 2022 +0200\n\n    Update asr_inference_streaming.py\n\n    Fix CI error on mismatch in Tensor dtypes\n\ncommit 39bae01e4a132da69b9b0d025da8c579a5f38b77\nMerge: dd24d7d41 71f3c8813\nAuthor: Tomoki Hayashi <hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp>\nDate:   Wed May 11 17:53:04 2022 +0900\n\n    Merge pull request #4355 from kan-bayashi/fix_lid_in_gan_tts\n\ncommit dd24d7d41517202b308afb186f466c8006ae4c14\nMerge: 2dde7734b f7b390582\nAuthor: Tomoki Hayashi <hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp>\nDate:   Wed May 11 17:52:09 2022 +0900\n\n    Merge pull request #4206 from WeiGodHorse/master\n\ncommit 2dde7734bade874d4f8cfe7df4be069e64259fd5\nMerge: beb336027 ec7e2b07b\nAuthor: Tomoki Hayashi <hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp>\nDate:   Wed May 11 16:27:55 2022 +0900\n\n    Merge pull request #4356 from kan-bayashi/fix_mixed_precision_vits\n\n    fix loss = NaN in VITS with mixed precision\n\ncommit 7a590ccd0da4897ef283486776f134eabe865ce0\nAuthor: espnetUser <81252087+espnetUser@users.noreply.github.com>\nDate:   Wed May 11 09:25:03 2022 +0200\n\n    Applied black formating to test_asr_inference.py for PR\n\ncommit ec7e2b07bfa85c8a2292de7a2edbf1c2cd956d99\nAuthor: kan-bayashi <hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp>\nDate:   Wed May 11 14:48:36 2022 +0900\n\n    fixed black\n\ncommit 2be9ddc5a2c0a7c4aad2b155fa1450222ca0c7a3\nAuthor: kan-bayashi <hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp>\nDate:   Wed May 11 14:28:05 2022 +0900\n\n    fixed mixed_precision NaN (#4236)\n\ncommit 71f3c88133c7a29db54baa7eaa3b4fdf329cbdf5\nAuthor: kan-bayashi <hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp>\nDate:   Wed May 11 13:39:59 2022 +0900\n\n    fixed optional data names for TTS\n\ncommit ee57ff94dfa2c3ced30c1b103076b4ae18fa9199\nAuthor: espnetUser <81252087+espnetUser@users.noreply.github.com>\nDate:   Tue May 10 22:37:18 2022 +0200\n\n    Update asr_inference_streaming.py\n\n    Fix dtype CI error\n\ncommit 272d5d015f89f1520c82c31bd309fdce89d88f50\nAuthor: espnetUser <81252087+espnetUser@users.noreply.github.com>\nDate:   Tue May 10 21:52:21 2022 +0200\n\n    Update test_asr_inference.py\n\n    Remove streaming=true parameter\n\ncommit c96e0d7f79e6e94e568b22156eb61004d5d8cf8c\nAuthor: espnetUser <81252087+espnetUser@users.noreply.github.com>\nDate:   Tue May 10 21:25:57 2022 +0200\n\n    Aplied black formating to test_asr_inference.py for PR\n\ncommit cd77501a8f09b5b11bf5422b0e24b8316820af77\nAuthor: Yifan Peng <pengyf21@gmail.com>\nDate:   Tue May 10 12:02:07 2022 -0400\n\n    fix error for rnn encoders flatten_parameters\n\ncommit 3aafdb9d92c8c61d62be72f0907da957d177aa8c\nAuthor: espnetUser <81252087+espnetUser@users.noreply.github.com>\nDate:   Tue May 10 17:05:48 2022 +0200\n\n    Update asr_inference_streaming.py\n\n    Bugfix in streaming inference #4216\n\ncommit 61b50138b7e8828506a18067cc2f482e745e83d7\nAuthor: espnetUser <81252087+espnetUser@users.noreply.github.com>\nDate:   Tue May 10 16:58:14 2022 +0200\n\n    Update test_asr_inference.py\n\n    Added edge test case for streaming asr unit test and increased execution time out\n\ncommit 052dd603900362048675f65058b7a6f4bd94bc7d\nAuthor: Yifan Peng <pengyf21@gmail.com>\nDate:   Mon May 9 23:27:41 2022 -0400\n\n    fix ci\n\ncommit 06e2a7a16a06cda326035d03c84734d18c852cd3\nAuthor: Yifan Peng <pengyf21@gmail.com>\nDate:   Mon May 9 23:10:14 2022 -0400\n\n    apply black\n\ncommit a48423fda5ab75d1205396ca5f744dc8ca98df00\nAuthor: Yifan Peng <pengyf21@gmail.com>\nDate:   Mon May 9 22:59:57 2022 -0400\n\n    add test for espnet2 quantization\n\ncommit acb24c886f47fec7a00063cb66423e7bd52ea0bc\nAuthor: Yifan Peng <pengyf21@gmail.com>\nDate:   Mon May 9 22:59:39 2022 -0400\n\n    add quantization to asr_inference\n\ncommit b98fc861939310b73b50f959bc45176da10ef493\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Tue May 10 11:52:27 2022 +0900\n\n    fix\n\ncommit 3428f032d58c73902b5e6fe80307eb08cfc64ff6\nMerge: 4ff2ce124 beb336027\nAuthor: Naoyuki Kamo <naoyuki.kamo829@gmail.com>\nDate:   Tue May 10 11:42:23 2022 +0900\n\n    Merge branch 'master' into 1.11.0\n\ncommit 4ff2ce1244e0af72439deaa59226eba434a70618\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Tue May 10 11:34:31 2022 +0900\n\n    add pytorch=1.10.1, 1.11.0 to ci configurations\n\ncommit beb3360276aa9ff65fe84f4c5e99c0c063c2a6be\nMerge: 537f9b6c1 79cda74ba\nAuthor: Shinji Watanabe <sw005320@gmail.com>\nDate:   Mon May 9 16:27:37 2022 -0400\n\n    Merge pull request #4347 from YosukeHiguchi/espnet2_maskctc2\n\n    Minor fix for Mask-CTC forward function\n\ncommit 79cda74ba20f0b795251e23a9cb9fd624e2be02d\nAuthor: Yosuke Higuchi <wasapon.dev@gmail.com>\nDate:   Mon May 9 22:43:29 2022 +0900\n\n    add kwargs in forward argument\n\ncommit 537f9b6c14ab195cdcd21c404656c8534295f15d\nMerge: 793b999a5 9e8e75315\nAuthor: Shinji Watanabe <sw005320@gmail.com>\nDate:   Sun May 8 17:34:55 2022 -0400\n\n    Merge pull request #4343 from Emrys365/complex_support\n\n    Fix a bug in stats aggregation when PITSolver is used\n\ncommit 9e8e753154f5f71c9cb26217483427adb278759c\nAuthor: Wangyou Zhang <C0me_On@163.com>\nDate:   Sat May 7 13:16:35 2022 +0800\n\n    Apply black\n\ncommit 5ea4e087a311ab7c798950e68ae92e10b1bb41d8\nAuthor: Wangyou Zhang <C0me_On@163.com>\nDate:   Sat May 7 12:05:49 2022 +0800\n\n    Fix a bug in stats aggregation when PITSolver is used\n\ncommit 6d1bd3a8ef695a75358d019cc1b33100817c0dad\nMerge: eb6dc2d55 793b999a5\nAuthor: Chaitanya Narisetty <cnariset@andrew.cmu.edu>\nDate:   Fri May 6 10:51:14 2022 -0400\n\n    Merge branch 'espnet:master' into st_bugfix\n\ncommit eb6dc2d55faac7e62742d0b7791d8f3a991e91d1\nAuthor: Chaitanya Narisetty <cnariset@andrew.cmu.edu>\nDate:   Fri May 6 10:08:19 2022 -0400\n\n    typo fix\n\ncommit 8c56ee817867358f2a8130372fd914c136bd7a5b\nAuthor: Chaitanya Narisetty <cnariset@andrew.cmu.edu>\nDate:   Fri May 6 08:59:26 2022 -0400\n\n    bug fixes in ST recipes\n\n    * Change sampling frequency in `fbank.conf` and `pitch.conf` in Covost2 recipe\n    * In `run.sh`, if language is low resource, then have more speed perturbations. Fix typos for test sets\n    * In `st.sh`\n      * fix directory naming issues to avoid replacement for different language pairs\n      * Replace `>>` with `>` to replace previous inference results\n      * Fix removing of empty text in stage 4\n      * When removing utterance-ID in `ref.trn.org` or `hyp.trn.org`, the current implementation removes all words in parenthesis instead of removing just the utterance-ID from the end of each line. Fixed this by changing `perl -pe 's/\\([^\\)]+\\)//g;'` to `perl -pe 's/\\([^\\)]+\\)$//g;'`\n\ncommit f7b390582d2d77b113a92a5e52f907d5832d6f04\nAuthor: \u9b4f\u5baa\u8c6a <weixianhao@bytedance.com>\nDate:   Fri May 6 20:18:05 2022 +0800\n\n    change a test file to conform new pypinyin package\n\ncommit b83128fafc913e775a49d37a5cad24a893718020\nAuthor: \u9b4f\u5baa\u8c6a <weixianhao@bytedance.com>\nDate:   Fri May 6 17:54:20 2022 +0800\n\n    Fix missing punctuation\n\ncommit 931fd226babe69b35c6e3a6a288e5e0c901736a1\nAuthor: \u9b4f\u5baa\u8c6a <weixianhao@bytedance.com>\nDate:   Fri May 6 16:54:31 2022 +0800\n\n    reformat\n\ncommit 793b999a50af484a5eaf6227ef7556b48514ef15\nMerge: 4f41a1a06 6d0672882\nAuthor: Shinji Watanabe <sw005320@gmail.com>\nDate:   Thu May 5 21:54:27 2022 -0400\n\n    Merge pull request #4330 from pyf98/show_translation_result\n\n    Update show_translation_result.sh to show all decoding results under the given exp directory\n\ncommit 4f41a1a06ecd96af567bc73d1d6734531dd3cb44\nMerge: a49cc60cd f0d7cc2bf\nAuthor: Shinji Watanabe <sw005320@gmail.com>\nDate:   Thu May 5 21:53:10 2022 -0400\n\n    Merge pull request #4329 from roshansh-cmu/wandb\n\n    Wandb Minor Fix for Model Resume\n\ncommit a49cc60cda690e448d925c3e2bfdc5a85b3f5cd3\nMerge: de624ed58 21fba33c6\nAuthor: Shinji Watanabe <sw005320@gmail.com>\nDate:   Thu May 5 21:51:43 2022 -0400\n\n    Merge pull request #4338 from espnet/ftshijt-patch-1\n\n    Fix typo\n\ncommit 21fba33c69d9199c6897ffc6da8433ab94b7051d\nAuthor: Jiatong <728307998@qq.com>\nDate:   Thu May 5 21:25:10 2022 -0400\n\n    Fix typo\n\ncommit de624ed58953d17907fb241c5cb6514f27510162\nMerge: b757b89d4 fe288000d\nAuthor: Shinji Watanabe <sw005320@gmail.com>\nDate:   Thu May 5 16:10:44 2022 -0400\n\n    Merge pull request #4332 from simpleoier/chime6\n\n    add chime6 recipe\n\ncommit c504336661fa3cefa60b2214da39fbf0118fce49\nMerge: 50269e8b4 b757b89d4\nAuthor: \u9b4f\u5baa\u8c6a <weixianhao@bytedance.com>\nDate:   Wed May 4 21:58:43 2022 +0800\n\n    Merge remote-tracking branch 'upstream/master'\n\ncommit fe288000dbde339b4c386408af488af4bac423b6\nAuthor: simpleoier <netnetchangxk@gmail.com>\nDate:   Tue May 3 17:51:36 2022 -0400\n\n    add egs2/chime6/asr1 recipe\n\ncommit 6d06728820576ed96a729b3477a29ccab12542f1\nAuthor: Yifan Peng <pengyf21@gmail.com>\nDate:   Sat Apr 30 20:53:52 2022 -0400\n\n    fix ci\n\ncommit 72333a892d16ef913633111120f159008812795e\nAuthor: Yifan Peng <pengyf21@gmail.com>\nDate:   Sat Apr 30 20:34:06 2022 -0400\n\n    fix ci\n\ncommit f15e6adaafaca380ea152cf2b38d604eea3603d3\nAuthor: Yifan Peng <pengyf21@gmail.com>\nDate:   Sat Apr 30 18:54:37 2022 -0400\n\n    quote expansion\n\ncommit f6731cd97565bf4108f1064a83f1fffea4ca351b\nAuthor: Yifan Peng <pengyf21@gmail.com>\nDate:   Sat Apr 30 18:43:49 2022 -0400\n\n    update mt.sh\n\ncommit 552060a1d5670d0fd838bd8e10fc9e47a1122346\nAuthor: Yifan Peng <pengyf21@gmail.com>\nDate:   Sat Apr 30 18:41:41 2022 -0400\n\n    update show translation result\n\ncommit f0d7cc2bfbc8f68c42820262a8ca6e4906f3818b\nAuthor: Roshan S Sharma <36464960+roshansh-cmu@users.noreply.github.com>\nDate:   Fri Apr 29 20:57:18 2022 -0400\n\n    Delete resnet.py\n\ncommit 79c071e9ecd268a1963e8ca3863a2f5eaf34a525\nAuthor: roshansh-cmu <roshansh@andrew.cmu.edu>\nDate:   Fri Apr 29 20:54:37 2022 -0400\n\n    Wandb minor fix for model resume\n\ncommit ffe7c58ac8a255769f6952b8c7225a5158a00068\nMerge: 835033c70 b757b89d4\nAuthor: Roshan S Sharma <36464960+roshansh-cmu@users.noreply.github.com>\nDate:   Fri Apr 29 20:45:47 2022 -0400\n\n    Merge branch 'espnet:master' into master\n\ncommit b757b89d45d5574cebf44e225cbe32e3e9e4f522\nMerge: 930b380de 664414c8f\nAuthor: Tomoki Hayashi <hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp>\nDate:   Fri Apr 29 16:11:56 2022 +0900\n\n    Merge pull request #4320 from cadia-lvl/add-progress-bar\n\ncommit 930b380de02b31f8d2da4144d471e60ed41d70fc\nMerge: 2a48371b8 de81cf979\nAuthor: Shinji Watanabe <sw005320@gmail.com>\nDate:   Thu Apr 28 16:30:34 2022 -0400\n\n    Merge pull request #4316 from simpleoier/enh_s2t\n\n    add egs2/chime4/enh_asr1 recipe and results\n\ncommit de81cf979fd61ab13e0ab0fe0432fbbaa4776be3\nAuthor: simpleoier <netnetchangxk@gmail.com>\nDate:   Thu Apr 28 11:54:10 2022 -0400\n\n    update egs2/chime4/enh_asr1/README.md and related enh1, asr1 configs.\n\ncommit 664414c8f27d5148377ffa733c7f8369eaf7ebd4\nAuthor: kan-bayashi <hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp>\nDate:   Thu Apr 28 21:31:45 2022 +0900\n\n    fixed flake8\n\ncommit 2a48371b8ceffd4899dc08f2fc5df092ed1d8a93\nMerge: 72c1d8f2b 5a9178236\nAuthor: Shinji Watanabe <sw005320@gmail.com>\nDate:   Thu Apr 28 07:40:31 2022 -0400\n\n    Merge pull request #4243 from D-Keqi/master\n\n    Add streaming ST/SLU\n\ncommit 72c1d8f2bde996febde895c603722dba1634cf20\nMerge: b7f0a5a6f 406656cdc\nAuthor: Shinji Watanabe <sw005320@gmail.com>\nDate:   Thu Apr 28 07:37:23 2022 -0400\n\n    Merge pull request #4110 from earthmanylf/dpclanddan\n\n    Merge Deep Clustering and Deep Attractor Network to enh separator\n\ncommit b7f0a5a6fc227049c1b8735d8ac4362c27333022\nMerge: 44971ff96 2d950f962\nAuthor: Shinji Watanabe <sw005320@gmail.com>\nDate:   Thu Apr 28 07:33:11 2022 -0400\n\n    Merge pull request #4328 from Emrys365/egs2_aishell4\n\n    Rename egs2/clarity21/enh_2021 to egs2/clarity21/enh1\n\ncommit 2d950f96223fd4823203b6a4e9afdc86b2357e7e\nAuthor: Wangyou Zhang <C0me_On@163.com>\nDate:   Thu Apr 28 16:58:26 2022 +0800\n\n    Rename egs2/clarity21/enh_2021/\n\ncommit 2b663318cd1773fb8685b1e03295b6bc6889c283\nAuthor: simpleoier <netnetchangxk@gmail.com>\nDate:   Thu Apr 28 00:59:22 2022 -0400\n\n    fix small bugs and add CHiME4 enh_asr1 recipe & results\n\ncommit 406656cdcb668a77910074b4382b557b6f845c54\nAuthor: earthmanylf <411214987@qq.com>\nDate:   Thu Apr 28 11:10:11 2022 +0800\n\n    Add custom name in __init__ in tf_domain.py; Merge test_dpcl_loss.py to test_tf_domain.py\n\ncommit 5a9178236bc1a7a4a5db82ad84773d9c43199c81\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 28 10:31:29 2022 +0800\n\n    use the another st_inference\n\ncommit 9e4bb7fa88e8c63e69712e77c5b783c64181fbc2\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 28 10:13:59 2022 +0800\n\n    fix conflict\n\ncommit 21d2ac6331ec0779b8ec2d3265ccdfabfaacbd61\nMerge: b801ddc96 44971ff96\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 28 10:12:15 2022 +0800\n\n    Merge pull request #17 from espnet/master\n\n    merge the latest espnet\n\ncommit b801ddc96aedd2a9b4e63d2e3612c3cf7417799a\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 28 10:11:11 2022 +0800\n\n    Add files via upload\n\ncommit 316cf02340a627548b71317ba04afac457f68101\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 28 10:04:29 2022 +0800\n\n    fix conflict\n\ncommit 9b33b791d7c7b509f514b7540a8ec5dd7fff9d0b\nAuthor: earthmanylf <411214987@qq.com>\nDate:   Wed Apr 27 23:22:22 2022 +0800\n\n    Fix format\n\ncommit 346a42467881e5bbd9414200dd3c915935eb56dd\nAuthor: earthmanylf <411214987@qq.com>\nDate:   Wed Apr 27 22:37:22 2022 +0800\n\n    Fix format\n\ncommit 44971ff962aae30c962226f1ba3d87de057ac00e\nMerge: 0ae377389 c4b93e8fd\nAuthor: Jiatong <728307998@qq.com>\nDate:   Wed Apr 27 10:13:03 2022 -0400\n\n    Merge pull request #4324 from ftshijt/master\n\n    Add Test Functions for ST Train and Inference\n\ncommit 0d3be31602306650fee44c367cbc788e0b0462db\nAuthor: earthmanylf <411214987@qq.com>\nDate:   Wed Apr 27 22:09:12 2022 +0800\n\n    Fix format\n\ncommit b24d108b0d7d501b2faa1971feca5a281198d351\nMerge: 4c679c061 f1312a8b2\nAuthor: earthmanylf <411214987@qq.com>\nDate:   Wed Apr 27 21:29:33 2022 +0800\n\n    Fix conflict\n\ncommit 4c679c061c1a0be411f613bdbdeb7849af19edf4\nMerge: a90e2ecef 0ae377389\nAuthor: earthmanylf <411214987@qq.com>\nDate:   Wed Apr 27 21:15:33 2022 +0800\n\n    Fix conflict\n\ncommit 10e6c7ea2e5783442631213dfc20dd7b9543839d\nAuthor: Gunnar Thor <ornolfsson@gmail.com>\nDate:   Wed Apr 27 09:30:47 2022 +0000\n\n    split docstring to conform with linter\n\ncommit c4b93e8fd870954ec2649abc3fc6172d78d92166\nAuthor: ftshijt <728307998@qq.com>\nDate:   Wed Apr 27 01:49:00 2022 -0400\n\n    apply black\n\ncommit 04d0cd84878701a0ff5e09933581c98ef7e0adac\nMerge: 72b6b21d5 4a12ab320\nAuthor: ftshijt <728307998@qq.com>\nDate:   Wed Apr 27 01:27:36 2022 -0400\n\n    Merge branch 'master' of https://github.com/ftshijt/espnet\n\ncommit 72b6b21d509a26d30a454525811c3530ee6b297b\nAuthor: ftshijt <728307998@qq.com>\nDate:   Wed Apr 27 01:27:09 2022 -0400\n\n    add st unit test\n\ncommit d1e8ac3d8717f8717fb645592c25ee8cafc4060c\nAuthor: ftshijt <728307998@qq.com>\nDate:   Wed Apr 27 01:15:18 2022 -0400\n\n    update test\n\ncommit 5fb7dd619293dcd1cc02c6371c4079c22a40a23b\nAuthor: ftshijt <728307998@qq.com>\nDate:   Wed Apr 27 00:53:46 2022 -0400\n\n    remove requirement for src_token_list\n\ncommit 4118b1b21f25fc7d8aa56658cd7ff691684884be\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Wed Apr 27 10:31:42 2022 +0800\n\n    fix conflict\n\ncommit 5436784241eaa4f60e0990627758a841e7927651\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Wed Apr 27 10:06:19 2022 +0800\n\n    Update test_integration_espnet2.sh\n\ncommit 469168b4451b4922306b3393598d199a514acd50\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Wed Apr 27 10:04:56 2022 +0800\n\n    fix issue\n\ncommit 06ddfe19a346f1ea8b620e4eb5bf61bfdcfc3309\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Wed Apr 27 10:01:38 2022 +0800\n\n    fix conflict\n\ncommit 5a81f91ce6734745272e6d960261797cfcb3dd41\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Wed Apr 27 09:57:18 2022 +0800\n\n    fix conflict\n\ncommit 91d48d920c229af3902fc05c361ba1b5f1636c67\nAuthor: Gunnar Thor <ornolfsson@gmail.com>\nDate:   Tue Apr 26 22:21:13 2022 +0000\n\n    applied black\n\ncommit ec518ccc74b85e3b50304ab70ae5a1f069df0038\nAuthor: Gunnar Thor <ornolfsson@gmail.com>\nDate:   Wed Feb 23 11:31:56 2022 +0000\n\n    Add progress bar to phonemization\n\ncommit f1312a8b2eeecf57f740b963b832dc4a806ac5f8\nAuthor: earthmanylf <43513215+earthmanylf@users.noreply.github.com>\nDate:   Mon Apr 25 10:37:19 2022 +0800\n\n    Update README.md\n\n    Co-authored-by: Wangyou Zhang <C0me_On@163.com>\n\ncommit a90e2ecef4854884dc525345a466f33fce79bd0a\nAuthor: earthmanylf <411214987@qq.com>\nDate:   Sun Apr 24 22:55:54 2022 +0800\n\n    Fix format problems\n\ncommit be0112bf99c7caf787feba50c7dbc47a1879dbfb\nAuthor: earthmanylf <411214987@qq.com>\nDate:   Sun Apr 24 22:06:45 2022 +0800\n\n    Fix format problems\n\ncommit 16acdadb6dba56d0f91a3132b540a01c9bd25c89\nMerge: feb28baf9 f6a2522ad\nAuthor: earthmanylf <411214987@qq.com>\nDate:   Sun Apr 24 21:14:02 2022 +0800\n\n    Fix conflict\n\ncommit 95be28ab0e48415922677a92639833d648f3844c\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Sat Apr 23 14:47:11 2022 +0800\n\n    Fix CI\n\ncommit a0966f61701041228c96924359b8e6678960a31a\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Sat Apr 23 14:46:10 2022 +0800\n\n    Fix CI\n\ncommit 1daecd4570f477da905e4365ff30e4c0be53ca44\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Sat Apr 23 14:44:21 2022 +0800\n\n    fix CI\n\ncommit 7261735b82173ae5ac377844fad2f3b9289e08ec\nMerge: 809106e2a f6a2522ad\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Sat Apr 23 14:21:06 2022 +0800\n\n    Merge pull request #15 from espnet/master\n\n    Merging the latest ESPnet\n\ncommit 809106e2a512990b30fd1afcf2c7bf897d185d58\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Sat Apr 23 12:33:18 2022 +0800\n\n    show the log result\n\ncommit 65b53563cac0fdc09d653112f85dd735313cb650\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Sat Apr 23 11:10:41 2022 +0800\n\n    show the error report in the log\n\ncommit 36bdfcbfd0731e543db130b6fb756e140f9f2cb2\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 21 15:21:07 2022 +0800\n\n    fix ci\n\ncommit c8e05efd90ea4c9f775b149916d05f0f74092157\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 21 11:30:54 2022 +0800\n\n    fix ci\n\ncommit 4831a6671728e52f0b2a0766a7c4cb60dd3d470f\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Wed Apr 20 20:34:26 2022 +0800\n\n    fix CI\n\ncommit 26fc7e1b41c57dc5c6a6882fe20a8847ee5a055c\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Wed Apr 20 16:37:29 2022 +0800\n\n    Add files via upload\n\ncommit b7c7bf13f9df6d9c09888c21c5c071c15f1023bc\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Wed Apr 20 15:19:37 2022 +0800\n\n    fix ci\n\ncommit 2b1b6bbef15553a11862a9c74352bed95412337d\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Wed Apr 20 11:33:40 2022 +0800\n\n    fix fbank_pitch issue\n\ncommit 0d5736fc393332465ae49a620392735a22312c97\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Wed Apr 20 11:33:21 2022 +0800\n\n    fix fbank_pitch issue\n\ncommit 835033c70cb2821340481b6e3f695d3afe6cbcd0\nMerge: fcf13c412 42eb3108a\nAuthor: Roshan S Sharma <36464960+roshansh-cmu@users.noreply.github.com>\nDate:   Tue Apr 19 07:36:09 2022 -0400\n\n    Merge branch 'espnet:master' into master\n\ncommit 70c1980b7c8d396bd5d05d8eba50bf90a84bff55\nAuthor: D-Keqi <462975470@qq.com>\nDate:   Tue Apr 19 19:01:41 2022 +0800\n\n    fix CI\n\ncommit fabb3a1fd17b10cbcf252240e0c40243a8c2f971\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Tue Apr 19 16:39:39 2022 +0800\n\n    update the test_integration_espnet2\n\ncommit c08e023e429ad90399f3722d825ccaa33c84b291\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Tue Apr 19 16:36:09 2022 +0800\n\n    Update and rename tmp to path.sh\n\ncommit 838d2ecfa767585a3df0161388f5dd5de426695a\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Tue Apr 19 16:35:08 2022 +0800\n\n    Add files via upload\n\ncommit 62162ae8938d71f0f9040ee1e27eb40c83882808\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Tue Apr 19 16:33:31 2022 +0800\n\n    Create tmp\n\ncommit 9a5585e282b68d44921879385f5a3796bacd1fdb\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Tue Apr 19 16:33:00 2022 +0800\n\n    Delete t\n\ncommit 349f4ab3498bc296d46ad4b42a77fda25d5e2286\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Tue Apr 19 16:31:43 2022 +0800\n\n    add conf\n\ncommit e3486d24210cb53491518d913df2268a2f03eded\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Tue Apr 19 16:28:12 2022 +0800\n\n    Create t\n\ncommit 652cf1774dd442d55082652713bbadbc4b6946a6\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Tue Apr 19 16:27:47 2022 +0800\n\n    Delete tmp\n\ncommit 48fcab7a8d8b0ad1a97798fa823d315aa7708d3d\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Tue Apr 19 16:27:12 2022 +0800\n\n    add st1 of mini_an4\n\ncommit 1800b0be298111842ab2a3cf5f39a9ac79c3a86f\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Tue Apr 19 16:25:21 2022 +0800\n\n    Create tmp\n\ncommit 0a1d05b61d611ca8a7b7ca1815ae089781cbdfde\nMerge: 73ca6e4e4 952a70a70\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Wed Apr 13 10:20:46 2022 +0800\n\n    Merge pull request #14 from espnet/master\n\n    Merge the latest ESPnet\n\ncommit 73ca6e4e4baddd5f3fb6075788ed3e902021b9c8\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 7 17:59:52 2022 +0800\n\n    fix ci\n\n    fix ci\n\ncommit acd3e0acdc4d4c6eadfa531711906aa29ffb01a0\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 7 17:58:34 2022 +0800\n\n    fix CI\n\n    fix CI\n\ncommit e6da9baea12c6383282bdb716745060be5011a08\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 7 17:16:45 2022 +0800\n\n    Add files via upload\n\ncommit fc45fa368bc55b92f94e9ae6f9a6953728f3c894\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 7 17:13:53 2022 +0800\n\n    Delete README.md\n\ncommit 5b8c0b567f6b172e2112c5460c45e44b934478a6\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 7 17:13:11 2022 +0800\n\n    Delete egs2/chime4/asr1/exp/asr_train_asr_streaming_transformer_raw_en_char/decode_asr_streamindt05_real_beamformit_2micsg_lm_lm_train_lm_en_char_valid.loss.ave_asr_model_valid.acc.ave directory\n\ncommit 87ac110aaf70e2c339bac6ed7c5b60a856acc535\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 7 17:10:14 2022 +0800\n\n    streaming slu\n\ncommit 7b7fde9752cd9cd4905d642996215a158bf8d026\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 7 17:09:27 2022 +0800\n\n    streaming slu\n\ncommit fcd129620bbbc063dd918b83961d568ad694e45a\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 7 17:08:55 2022 +0800\n\n    streaming st\n\ncommit 17fe79ca89b496e4f9b6b4caaa2497816d4855b3\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 7 17:07:28 2022 +0800\n\n    streaming st\n\ncommit 812a527bb836a2fbd12ceb6d3bcabcc728d88427\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 7 17:06:31 2022 +0800\n\n    streaming st\n\ncommit e69a6d8efcd1ae57aca6315d70a20e484d360f7f\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 7 17:05:25 2022 +0800\n\n    streaming st\n\ncommit e488037b8d9b3e46476874f62b095ae5b7323e19\nMerge: 9fb445053 189e1593d\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 7 15:32:57 2022 +0800\n\n    Merge pull request #13 from espnet/master\n\n    Update lastest espnet\n\ncommit fcf13c412842d57cf48580dd89ff0d1fc5e6c3e0\nMerge: 39700a054 c4aba12f9\nAuthor: Roshan S Sharma <36464960+roshansh-cmu@users.noreply.github.com>\nDate:   Wed Apr 6 13:35:13 2022 -0400\n\n    Merge branch 'espnet:master' into master\n\ncommit feb28baf9dd6af564fe30920c1c6e70c2258e0de\nMerge: 3e6167c51 c4aba12f9\nAuthor: earthmanylf <411214987@qq.com>\nDate:   Wed Apr 6 19:24:06 2022 +0800\n\n    Add deep clustering end-to-end training method\n\ncommit 50269e8b4dd0696d02e5da9f70c2d7952a26f392\nAuthor: WeiGodHorse <weigodhorse@gmail.com>\nDate:   Fri Mar 25 22:58:41 2022 +0800\n\n    fix a bug in Mandarin pypinyin_g2p_phone\n\ncommit 39700a054ac5ed718a1eb74cef9b64b2144b727c\nMerge: aa706c512 14c635069\nAuthor: Roshan S Sharma <36464960+roshansh-cmu@users.noreply.github.com>\nDate:   Thu Mar 24 17:42:11 2022 -0400\n\n    Merge branch 'espnet:master' into master\n\ncommit aa706c5122391feee57d4db121a403dfd8ea0ab0\nMerge: ab2fa25af 350af365f\nAuthor: Roshan S Sharma <36464960+roshansh-cmu@users.noreply.github.com>\nDate:   Wed Mar 23 23:34:17 2022 -0400\n\n    Merge branch 'espnet:master' into master\n\ncommit ab2fa25af6dffce3ecdf3e92adaa171d3d156d50\nMerge: de5e7139b cb8181a99\nAuthor: Roshan S Sharma <36464960+roshansh-cmu@users.noreply.github.com>\nDate:   Tue Mar 8 16:03:38 2022 -0500\n\n    Merge branch 'espnet:master' into master\n\ncommit de5e7139b65549adfcac58cb0ee23c32c50634ea\nMerge: 5ef36bcae 1bac0f080\nAuthor: Roshan S Sharma <36464960+roshansh-cmu@users.noreply.github.com>\nDate:   Tue Mar 8 15:09:20 2022 -0500\n\n    Merge branch 'espnet:master' into master\n\ncommit 5ef36bcae3fac1792ccc2aae6b7dbab715f094fe\nMerge: 597cd7bd8 0c246e23c\nAuthor: Roshan S Sharma <36464960+roshansh-cmu@users.noreply.github.com>\nDate:   Tue Mar 8 13:35:27 2022 -0500\n\n    Merge branch 'espnet:master' into master\n\ncommit 597cd7bd8a0efbe82733d19774297ab90f5c659f\nMerge: 6625f9056 f16e579e2\nAuthor: Roshan S Sharma <36464960+roshansh-cmu@users.noreply.github.com>\nDate:   Mon Mar 7 21:54:06 2022 -0500\n\n    Merge branch 'espnet:master' into master\n\ncommit 6625f9056b5087aeb13a2214c770d586c067f5e3\nMerge: 5f237866b 5e070668e\nAuthor: Roshan S Sharma <36464960+roshansh-cmu@users.noreply.github.com>\nDate:   Mon Mar 7 13:35:03 2022 -0500\n\n    Merge branch 'espnet:master' into master\n\ncommit 3e6167c51df23b7629d7830e81e8cf4ea52032fc\nAuthor: earthmanylf <411214987@qq.com>\nDate:   Mon Mar 7 20:03:31 2022 +0800\n\n    Fixed format in some files\n\ncommit 294373a121cf0766efe623dc56b12d0990a77c93\nAuthor: earthmanylf <411214987@qq.com>\nDate:   Mon Mar 7 18:26:49 2022 +0800\n\n    Update code and add comments in separator\n\ncommit 5f86c1104cbce4275043e11050b69191834ddbc0\nMerge: 7aa90b584 6f429608b\nAuthor: earthmanylf <411214987@qq.com>\nDate:   Mon Mar 7 18:06:10 2022 +0800\n\n    Add experiment result in egs2/wsj0_2mix/enh1/README.md; Update code in some files\n\ncommit 5f237866b360028676c7b9e903d15839cdaa0113\nMerge: 66c1a798d 6f429608b\nAuthor: Roshan S Sharma <36464960+roshansh-cmu@users.noreply.github.com>\nDate:   Sun Mar 6 19:26:35 2022 -0500\n\n    Merge branch 'espnet:master' into master\n\ncommit 66c1a798d15f531b4c4b4c1e02cfd1eda6813f92\nMerge: 5c5eb0292 a04a98c98\nAuthor: Roshan S Sharma <36464960+roshansh-cmu@users.noreply.github.com>\nDate:   Thu Mar 3 18:14:47 2022 -0500\n\n    Merge branch 'espnet:master' into master\n\ncommit 7aa90b5844ba1d0050cfd737b2a2fabe9abd5d62\nMerge: 5f7e2e714 b274c4ea6\nAuthor: earthmanylf <411214987@qq.com>\nDate:   Thu Mar 3 16:20:25 2022 +0800\n\n    Merge branch 'master' of github.com:espnet/espnet into dpclanddan\n\ncommit 5c5eb0292e28c19345fc71d456348f6353f2e2a4\nMerge: bd8e400fa 9863980d2\nAuthor: Roshan S Sharma <36464960+roshansh-cmu@users.noreply.github.com>\nDate:   Wed Mar 2 12:13:35 2022 -0500\n\n    Merge branch 'espnet:master' into master\n\ncommit bd8e400fa37ebc1b77f7a938ae9275bb18de6fe5\nMerge: 58aec432d 7999009d5\nAuthor: Roshan S Sharma <36464960+roshansh-cmu@users.noreply.github.com>\nDate:   Mon Feb 28 20:37:32 2022 -0500\n\n    Merge branch 'espnet:master' into master\n\ncommit 5f7e2e7140cc7204acecda90a6ff1d5379967da6\nMerge: d3acdcc3b 637d8c333\nAuthor: earthmanylf <411214987@qq.com>\nDate:   Sun Feb 27 13:19:45 2022 +0800\n\n    Merge branch 'master' of github.com:espnet/espnet into dpclanddan\n\ncommit d3acdcc3bd537cf3f50c8d5c4642dfc488daa656\nAuthor: earthmanylf <411214987@qq.com>\nDate:   Fri Feb 25 18:32:30 2022 +0800\n\n    fix bugs of test_dan_separator.py\n\ncommit c54d9a4087106b56ab5ce4ec9758aeb74bca0b4c\nAuthor: earthmanylf <411214987@qq.com>\nDate:   Fri Feb 25 16:00:30 2022 +0800\n\n    add subs to the abs_separator.py\n\ncommit c1d9be5f4f9eb32bc75fb7a8b2fe406aa997946c\nAuthor: earthmanylf <411214987@qq.com>\nDate:   Fri Feb 25 15:30:46 2022 +0800\n\n    update for dpcl and dan\n\ncommit 58aec432d97300ec12494676a19900a08a950827\nMerge: 23a537e2a 9c24b3add\nAuthor: Roshan S Sharma <36464960+roshansh-cmu@users.noreply.github.com>\nDate:   Wed Feb 23 16:17:09 2022 -0500\n\n    Merge branch 'espnet:master' into master\n\ncommit 23a537e2ad1ee9af7e8016054208d5ce1cc572fd\nAuthor: roshansh-cmu <roshansh@andrew.cmu.edu>\nDate:   Tue Feb 22 06:50:03 2022 -0500\n\n    black fix\n\ncommit 8572a57af47ef72e9f010601483b31eb96baf03f\nMerge: 969b333d9 650472b45\nAuthor: roshansh-cmu <roshansh@andrew.cmu.edu>\nDate:   Mon Feb 21 22:35:49 2022 -0500\n\n    Mergefix\n\ncommit ee20e18a5f0eef55c8b0709e1e6b9bcddf10e4e6\nMerge: 63f88c02b a3e1543e9\nAuthor: earthmanylf <43513215+earthmanylf@users.noreply.github.com>\nDate:   Wed Feb 16 14:29:36 2022 +0800\n\n    Merge pull request #1 from espnet/master\n\n    Merge from upstream\n\ncommit 9fb445053f999b64350e5e7a56a1699a727ed125\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Wed Sep 15 00:30:05 2021 +0800\n\n    Update README.md\n\ncommit 8c6d3e1614a247b78f1b17ff2c6ef3b3725b166a\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Wed Sep 15 00:29:31 2021 +0800\n\n    Update README.md\n\ncommit 2411dbb82b08aee182df0738a47d7f6f44bdcea8\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Mon Sep 13 13:08:52 2021 +0800\n\n    Update README.md\n\ncommit 3edc1a6d816428b3e4e099271dc51c117b9c8d3b\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Mon Sep 13 13:08:25 2021 +0800\n\n    Update README.md\n\ncommit d4d4b7e450992867bc0ee91ffb467ec38ad6981c\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Sat Sep 11 23:11:39 2021 +0800\n\n    Update README.md\n\ncommit 885ab0552dc26076b0b581eb88813f426179fdcb\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Sat Sep 4 10:48:05 2021 +0800\n\n    add results\n\ncommit dfba960da5e60cd9d78c439b7fa0e400332fbe46\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Sat Sep 4 10:43:36 2021 +0800\n\n    create exp\n\ncommit 391d7c78f310313ca78abc1b3341183a15336579\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Sat Sep 4 10:40:23 2021 +0800\n\n    steaming results\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class CTC(torch.nn.Module):",
            "self.probs = None  # for visualization",
            "",
            "# In case of Pytorch >= 1.7.0, CTC will be always builtin",
            "-        self.ctc_type = (",
            "-            ctc_type",
            "-            if LooseVersion(torch.__version__) < LooseVersion(\"1.7.0\")",
            "-            else \"builtin\"",
            "-        )",
            "+        self.ctc_type = ctc_type if V(torch.__version__) < V(\"1.7.0\") else \"builtin\"",
            "",
            "if ctc_type != self.ctc_type:",
            "logging.warning(f\"CTC was set to {self.ctc_type} due to PyTorch version.\")"
        ]
    },
    {
        "number": 1396,
        "comments": "",
        "commit_message": "Removes dependency on the overrides package (#5490)\n\n* Removes dependency on the overrides package\n\n* Changelog\n\n* Various fixes for mypy\n\n* Update cached_path dependency\n\n* What happened here?\n\n* Formatting\n\n* Fix more tests\n\n* One more missing overrides\n",
        "label": "",
        "answer": "no",
        "change": [
            "class BLEU(Metric):",
            "self._prediction_lengths += dist_reduce_sum(_prediction_lengths)",
            "self._reference_lengths += dist_reduce_sum(_reference_lengths)",
            "",
            "-    @overrides",
            "def get_metric(self, reset: bool = False) -> Dict[str, float]:",
            "",
            "brevity_penalty = self._get_brevity_penalty()"
        ]
    },
    {
        "number": 1397,
        "comments": "",
        "commit_message": "Fixed audio/bag/binary feature tests to work on GPU (#1600)\n\n* Fixed audio/bag/binary feature tests to work on GPU\n\n* Added timeseries feature test (#1601)\n\n* Removed device transfer calls from encoders to features\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def binary_config():",
            "def test_binary_input_feature(binary_config: Dict, encoder: str) -> None:",
            "binary_config.update({\"encoder\": encoder})",
            "binary_input_feature = BinaryInputFeature(binary_config)",
            "-    binary_tensor = torch.randn([SEQ_SIZE, BINARY_W_SIZE], dtype=torch.float32)",
            "+    binary_tensor = torch.randn([SEQ_SIZE, BINARY_W_SIZE], dtype=torch.float32).to(DEVICE)",
            "encoder_output = binary_input_feature(binary_tensor)",
            "assert encoder_output[\"encoder_output\"].shape[1:] == binary_input_feature.output_shape"
        ]
    },
    {
        "number": 1399,
        "comments": "",
        "commit_message": "Drop JIT support for `core.check`, `Boxes`, and others (#2219)\n\n* Drop JIT support for `core.check` API\n\n- Consequently for this, we drop support of JIT on the following items: (in of dynamo)\n  - enhance\n    - AdjustSigmoid\n    - AdjustLog\n    - AddWeighted\n  - geometry\n    - UndistortPoints\n    - bbox and Boxes - follow up on #2218\n    - EuclideanDistance\n    - TransformPoints\n    - HomographyWarper\n    - WarpPerspective\n    - UpscaleDouble\n  - losses\n\n* Update typing with pyupgrade\n* drop all jit related from bbox and boxes\n\nfrom #2218\n* fix/skip failing dynamo tests\n* fix loss hd\n* fix typing\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TestEulerFromQuaternion(BaseTester):",
            "def test_module(self, device, dtype):",
            "pass",
            "",
            "-    def test_jit(self, device, dtype):",
            "+    def test_dynamo(self, device, dtype, torch_optimizer):",
            "q = Quaternion.random(batch_size=1)",
            "q = q.to(device, dtype)",
            "op = euler_from_quaternion",
            "-        op_jit = torch.jit.script(op)",
            "-        assert_close(op(q.w, q.x, q.y, q.z), op_jit(q.w, q.x, q.y, q.z))",
            "+        op_optimized = torch_optimizer(op)",
            "+        assert_close(op(q.w, q.x, q.y, q.z), op_optimized(q.w, q.x, q.y, q.z))",
            "",
            "def test_forth_and_back(self, device, dtype):",
            "q = Quaternion.random(batch_size=2)"
        ]
    },
    {
        "number": 1401,
        "comments": "",
        "commit_message": "fix CI error\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class AdadeltaFactory(OptimizerFactoryInterface):",
            "",
            "\"\"\"",
            "return torch.optim.Adadelta(",
            "-            target,",
            "-            rho=args.rho,",
            "-            eps=args.eps,",
            "-            weight_decay=args.weight_decay,",
            "+            target, rho=args.rho, eps=args.eps, weight_decay=args.weight_decay",
            ")"
        ]
    },
    {
        "number": 1402,
        "comments": "",
        "commit_message": "fix\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class ESPnetSVSModel(AbsESPnetModel):",
            "midi_score_lengths = torch.tensor([len(midi_score)])",
            "tempo_score_lengths = torch.tensor([len(tempo_score)])",
            "beat_score_phn_lengths = torch.tensor([len(beat_score_phn)])",
            "-        beat_score_syb_lengths = torch.tensor([len(beat_score_syb)])",
            "assert (",
            "label_score_lengths == midi_score_lengths",
            "and label_score_lengths == tempo_score_lengths"
        ]
    },
    {
        "number": 1403,
        "comments": "",
        "commit_message": "bugfix in GST\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class StyleTokenLayer(nn.Module):",
            "self.key_dim = embedding_dim // num_heads",
            "self.style_tokens = nn.Parameter(",
            "torch.FloatTensor(num_style_tokens, self.key_dim))",
            "-        nn.init.orthogonal_(self.style_tokens)",
            "+        nn.init.normal_(self.style_tokens, mean=0, std=0.5)",
            "self.attention = MultiHeadAttention(",
            "query_dim=self.query_dim,",
            "key_dim=self.key_dim,"
        ]
    },
    {
        "number": 1405,
        "comments": "",
        "commit_message": "Detect.py supports running against a Triton container (#9228)\n\n* update coco128-seg comments\n\n* Enables detect.py to use Triton for inference\n\nTriton Inference Server is an open source inference serving software\nthat streamlines AI inferencing.\nhttps://github.com/triton-inference-server/server\n\nThe user can now provide a \"--triton-url\" argument to detect.py to use\na local or remote Triton server for inference.\nFor e.g., http://localhost:8000 will use http over port 8000\nand grpc://localhost:8001 will use grpc over port 8001.\nNote, it is not necessary to specify a weights file to use Triton.\n\nA Triton container can be created by first exporting the Yolov5 model\nto a Triton supported runtime. Onnx, Torchscript, TensorRT are\nsupported by both Triton and the export.py script.\n\nThe exported model can then be containerized via the OctoML CLI.\nSee https://github.com/octoml/octo-cli#getting-started for a guide.\n\n* added triton client to requirements\n\n* fixed support for TFSavedModels in Triton\n\n* reverted change\n\n* Test CoreML update\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Update ci-testing.yml\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Use pathlib\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Refacto DetectMultiBackend to directly accept triton url as --weights http://...\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Deploy category\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Update detect.py\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Update common.py\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Update common.py\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Update predict.py\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Update predict.py\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Update predict.py\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Update triton.py\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Update triton.py\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Add printout and requirements check\n\n* Cleanup\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* triton fixes\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fixed triton model query over grpc\n\n* Update check_requirements('tritonclient[all]')\n\n* group imports\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix likely remote URL bug\n\n* update comment\n\n* Update is_url()\n\n* Fix 2x download attempt on http://path/to/model.pt\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\nCo-authored-by: glennjocher <glenn.jocher@ultralytics.com>\nCo-authored-by: Gaz Iqbal <giqbal@octoml.ai>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def run(",
            "seen, windows, dt = 0, [], (Profile(), Profile(), Profile())",
            "for path, im, im0s, vid_cap, s in dataset:",
            "with dt[0]:",
            "-            im = torch.Tensor(im).to(device)",
            "+            im = torch.Tensor(im).to(model.device)",
            "im = im.half() if model.fp16 else im.float()  # uint8 to fp16/32",
            "if len(im.shape) == 3:",
            "im = im[None]  # expand for batch dim"
        ]
    },
    {
        "number": 1407,
        "comments": "",
        "commit_message": "Add Cuda tests for pyro.distributions; fix errors (#297)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Gamma(Distribution):",
            "alpha = alpha.expand_as(x)",
            "beta = beta.expand_as(x)",
            "ll_1 = - beta * x",
            "-        ll_2 = (alpha - pyro.ones(x.size())) * torch.log(x)",
            "+        ll_2 = (alpha - 1.0) * torch.log(x)",
            "ll_3 = alpha * torch.log(beta)",
            "ll_4 = - log_gamma(alpha)",
            "return ll_1 + ll_2 + ll_3 + ll_4"
        ]
    },
    {
        "number": 1409,
        "comments": "",
        "commit_message": "[Deterministic torch randn] Allow tensors to be generated on CPU (#1902)\n\n* [Deterministic torch randn] Allow tensors to be generated on CPU\n\n* fix more\n\n* up\n\n* fix more\n\n* up\n\n* Update src/diffusers/utils/torch_utils.py\n\nCo-authored-by: Anton Lozhkov <anton@huggingface.co>\n\n* Apply suggestions from code review\n\n* up\n\n* up\n\n* Apply suggestions from code review\n\nCo-authored-by: Pedro Cuenca <pedro@huggingface.co>\n\nCo-authored-by: Anton Lozhkov <anton@huggingface.co>\nCo-authored-by: Pedro Cuenca <pedro@huggingface.co>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class UnCLIPPipelineIntegrationTests(unittest.TestCase):",
            "pipeline = pipeline.to(torch_device)",
            "pipeline.set_progress_bar_config(disable=None)",
            "",
            "-        generator = torch.Generator(device=torch_device).manual_seed(0)",
            "+        generator = torch.Generator(device=\"cpu\").manual_seed(0)",
            "output = pipeline(",
            "\"horse\",",
            "num_images_per_prompt=1,"
        ]
    },
    {
        "number": 1412,
        "comments": "",
        "commit_message": "Black fix\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Reporter:",
            "if LooseVersion(torch.__version__) >= LooseVersion(\"1.4.0\"):",
            "if torch.cuda.is_initialized():",
            "stats[\"gpu_max_cached_mem_GB\"] = (",
            "-                    torch.cuda.max_memory_reserved() / 2**30",
            "+                    torch.cuda.max_memory_reserved() / 2 ** 30",
            ")",
            "else:",
            "if torch.cuda.is_available() and torch.cuda.max_memory_cached() > 0:",
            "-                stats[\"gpu_cached_mem_GB\"] = torch.cuda.max_memory_cached() / 2**30",
            "+                stats[\"gpu_cached_mem_GB\"] = torch.cuda.max_memory_cached() / 2 ** 30",
            "",
            "self.stats.setdefault(self.epoch, {})[sub_reporter.key] = stats",
            "sub_reporter.finished()"
        ]
    },
    {
        "number": 1413,
        "comments": "",
        "commit_message": "Fix returning a proper rotation in levelling; supporting batches and default centroid\n\nSummary:\n`get_rotation_to_best_fit_xy` is useful to expose externally, however there was a bug (which we probably did not care about for our use case): it could return a rotation matrix with det(R) == \u22121.\nThe diff fixes that, and also makes centroid optional (it can be computed from points).\n\nReviewed By: bottler\n\nDifferential Revision: D39926791\n\nfbshipit-source-id: 5120c7892815b829f3ddcc23e93d4a5ec0ca0013\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def fit_circle_in_3d(",
            "Circle3D object",
            "\"\"\"",
            "centroid = points.mean(0)",
            "-    r = _get_rotation_to_best_fit_xy(points, centroid)",
            "+    r = get_rotation_to_best_fit_xy(points, centroid)",
            "normal = r[:, 2]",
            "rotated_points = (points - centroid) @ r",
            "result_2d = fit_circle_in_2d("
        ]
    },
    {
        "number": 1415,
        "comments": "",
        "commit_message": "Abstract accelerator (step 3) (#2677)\n\n* Integrate accelerator abstraction interface into deepspeed/\n\n* Fix error message in fp16/fused_optimizer\n\n* fix error message in fp16/unfused_optimizer.py\n\n* assign get_accelerator().pin_memory() result to input Tensor name\n\n* no need to check cuda and whether nvtx supported\n\n* move try-except into inner most block\n\n* call Event() and Stream() in get_accelerator() for data type\n\n* Make Stream and Event as properties of abstract interface so they can be used as data type in deepspeed\n\n* Apply op_builder backend api change from #2705 from @jeffra\n\n* fix tests where Builder NAME is used\n\n* keep original ...Builder.NAME interface instead of ...Builder().NAME interface\n\n* fix builder closure for installation\n\n* fix randomltd builder\n\n* add comments to clarify create_op_builder and get_op_builder\n\n* fix compatibility with pip install -e\n\nCo-authored-by: Cheng Li <pistasable@gmail.com>\nCo-authored-by: Olatunji Ruwase <olruwase@microsoft.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class SwapBufferManager(object):",
            "self.count = count",
            "self.dtype = dtype",
            "self.all_buffers = [",
            "-            torch.zeros(num_elems,",
            "-                        device='cpu',",
            "-                        dtype=dtype).pin_memory() for _ in range(count)",
            "+            get_accelerator().pin_memory(",
            "+                torch.zeros(num_elems,",
            "+                            device='cpu',",
            "+                            dtype=dtype)) for _ in range(count)",
            "]",
            "self.free_buffer_index = [i for i in range(count)]",
            "self.used_buffer_index = {}"
        ]
    },
    {
        "number": 1416,
        "comments": "",
        "commit_message": "Fix PT TF ViTMAE (#16766)\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class ViTMAEDecoder(nn.Module):",
            "[ViTMAELayer(decoder_config) for _ in range(config.decoder_num_hidden_layers)]",
            ")",
            "",
            "-        self.decoder_norm = nn.LayerNorm(config.decoder_hidden_size)",
            "+        self.decoder_norm = nn.LayerNorm(config.decoder_hidden_size, eps=config.layer_norm_eps)",
            "self.decoder_pred = nn.Linear(",
            "config.decoder_hidden_size, config.patch_size**2 * config.num_channels, bias=True",
            ")  # encoder to decoder"
        ]
    },
    {
        "number": 1421,
        "comments": "",
        "commit_message": "fixed a bug\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def main():",
            "n_vocab = len(char_list)",
            "",
            "# for debug, small data",
            "-    # train = train[:100000]",
            "-    # valid = valid[:100]",
            "+    train = train[:100000]",
            "+    valid = valid[:100]",
            "",
            "# for debug, ptb data",
            "# train, valid, _ = chainer.datasets.get_ptb_words()"
        ]
    },
    {
        "number": 1422,
        "comments": "",
        "commit_message": "New logger connector code (#7882)\n\n* New logger connector code\n\n* Update CHANGELOG\n\n* Update requirements\n\n* Fix import path\n\n* Add new suffix\n\n* Tests\n\n* Minor changes\n\n* Rename and reorder\n\n* Fix import\n\n* Formatting\n\n* Fix with seed_everything?\n\n* Fix test?\n\n* Fix test?\n\n* Minor change\n\n* Minor changes\n\n* Minor changes\n\n* Force float\n\n* Fix minimal bug\n\n* Fix minimal bug\n\n* Update with latest changes\n\n* Fix import\n\n* bad merge\n\n* update typing\n\nCo-authored-by: tchaton <thomas@grid.ai>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def _setup_ddp(rank, worldsize):",
            "def _ddp_test_fn(rank, worldsize):",
            "_setup_ddp(rank, worldsize)",
            "tensor = torch.tensor([1.0])",
            "-    actual = LightningModule._LightningModule__sync(tensor, sync_dist=True, sync_dist_op=torch.distributed.ReduceOp.SUM)",
            "+    sync = _Sync(sync_ddp_if_available, should=True, op=torch.distributed.ReduceOp.SUM)",
            "+    actual = sync(tensor)",
            "assert actual.item() == dist.get_world_size(), \"Result-Log does not work properly with DDP and Tensors\""
        ]
    },
    {
        "number": 1425,
        "comments": "",
        "commit_message": "fixed\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class ESPnetTTSModel(AbsESPnetModel):",
            ")[0][0]",
            "if self.energy_normalize is not None:",
            "energy = self.energy_normalize(energy[None])[0][0]",
            "-            kwargs[\"energy\"] = energy",
            "+            if energy is not None:",
            "+                kwargs[\"energy\"] = energy",
            "",
            "if spembs is not None:",
            "kwargs[\"spembs\"] = spembs"
        ]
    },
    {
        "number": 1426,
        "comments": "",
        "commit_message": "[style] consistent nn. and nn.functional (#12124)\n\n* consistent nn. and nn.functional\n\n* fix glitch\n\n* fix glitch #2\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TFFlaubertMainLayer(tf.keras.layers.Layer):",
            "# encoder attention (for decoder only)",
            "# if self.is_decoder and src_enc is not None:",
            "#     attn = self.encoder_attn[i](tensor, src_mask, kv=src_enc, cache=cache)",
            "-            #     attn = F.dropout(attn, p=self.dropout, training=self.training)",
            "+            #     attn = nn.functional.dropout(attn, p=self.dropout, training=self.training)",
            "#     tensor = tensor + attn",
            "#     tensor = self.layer_norm15[i](tensor)"
        ]
    },
    {
        "number": 1427,
        "comments": "",
        "commit_message": "Fix typos in torchscript tests.\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def test_gcn2_conv():",
            "",
            "t = '(Tensor, Tensor, SparseTensor, OptTensor) -> Tensor'",
            "jit = torch.jit.script(conv.jittable(t))",
            "-    assert torch.allclose(conv(x, x_0, adj1.t()), out1, atol=1e-6)",
            "-    assert torch.allclose(conv(x, x_0, adj2.t()), out2, atol=1e-6)",
            "+    assert torch.allclose(jit(x, x_0, adj1.t()), out1, atol=1e-6)",
            "+    assert torch.allclose(jit(x, x_0, adj2.t()), out2, atol=1e-6)",
            "",
            "conv.cached = True",
            "conv(x, x_0, edge_index)"
        ]
    },
    {
        "number": 1428,
        "comments": "",
        "commit_message": "Fix NameError: name 'dist' is not defined (#763)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def mpi_discovery(distributed_port=TORCH_DISTRIBUTED_DEFAULT_PORT, verbose=True)",
            "os.environ['MASTER_PORT']))",
            "",
            "if torch.distributed.is_initialized():",
            "-        assert torch.distributed.get_rank() == rank, \"MPI rank {} does not match torch rank {}\".format(rank, dist.get_rank())",
            "+        assert torch.distributed.get_rank() == rank, \"MPI rank {} does not match torch rank {}\".format(",
            "+            rank, torch.distributed.get_rank())",
            "assert torch.distributed.get_world_size() == world_size, \"MPI world size {} does not match torch world size {}\".format(",
            "world_size, torch.distributed.get_world_size())"
        ]
    },
    {
        "number": 1430,
        "comments": "",
        "commit_message": "re-organize predict/; fix TF incompatibile change of sparse_softmax_cross_entropy_loss\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Model(ModelDesc):",
            ".FullyConnected('fc1', 512, nl=tf.nn.relu) \\",
            ".FullyConnected('linear', out_dim=self.cifar_classnum, nl=tf.identity)()",
            "",
            "-        cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, label)",
            "+        cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)",
            "cost = tf.reduce_mean(cost, name='cross_entropy_loss')",
            "",
            "wrong = symbf.prediction_incorrect(logits, label)"
        ]
    },
    {
        "number": 1431,
        "comments": "",
        "commit_message": "remove unused variables and fix typo\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def get_wordlm():",
            "char_dict = {x: i for i, x in enumerate(char_list)}",
            "word_dict = {x: i for i, x in enumerate(word_list)}",
            "",
            "-    rnnlm = lm_pytorch.ClassifierWithState(",
            "+    word_rnnlm = lm_pytorch.ClassifierWithState(",
            "lm_pytorch.RNNLM(len(word_list), n_layers, n_units)",
            ")",
            "word_rnnlm = lm_pytorch.ClassifierWithState("
        ]
    },
    {
        "number": 1433,
        "comments": "",
        "commit_message": "[RLlib] Tf2x preparation; part 2 (upgrading `try_import_tf()`). (#9136)\n\n* WIP.\n\n* Fixes.\n\n* LINT.\n\n* WIP.\n\n* WIP.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* WIP.\n\n* Fixes.\n\n* Test\n\n* Fix.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* LINT.\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TFModelV2(ModelV2):",
            "name,",
            "framework=\"tf\")",
            "self.var_list = []",
            "-        if tf.executing_eagerly():",
            "+        if tf1.executing_eagerly():",
            "self.graph = None",
            "else:",
            "-            self.graph = tf.get_default_graph()",
            "+            self.graph = tf1.get_default_graph()",
            "",
            "def context(self):",
            "\"\"\"Returns a contextmanager for the current TF graph.\"\"\""
        ]
    },
    {
        "number": 1434,
        "comments": "",
        "commit_message": "[RLlib] Minor cleanup in preparation to tf2.x support. (#9130)\n\n* WIP.\n\n* Fixes.\n\n* LINT.\n\n* Fixes.\n\n* Fixes and LINT.\n\n* WIP.\n",
        "label": "",
        "answer": "no",
        "change": [
            "def stats(policy, train_batch):",
            "\"cur_lr\": tf.cast(policy.cur_lr, tf.float64),",
            "\"policy_loss\": policy.loss.pi_loss,",
            "\"policy_entropy\": policy.loss.entropy,",
            "-        \"var_gnorm\": tf.global_norm(list(policy.model.trainable_variables())),",
            "+        \"var_gnorm\": tf.linalg.global_norm(",
            "+            list(policy.model.trainable_variables())),",
            "\"vf_loss\": policy.loss.vf_loss,",
            "}",
            "",
            "",
            "def grad_stats(policy, train_batch, grads):",
            "return {",
            "-        \"grad_gnorm\": tf.global_norm(grads),",
            "+        \"grad_gnorm\": tf.linalg.global_norm(grads),",
            "\"vf_explained_var\": explained_variance(",
            "train_batch[Postprocessing.VALUE_TARGETS],",
            "policy.model.value_function()),"
        ]
    },
    {
        "number": 1435,
        "comments": "",
        "commit_message": "Add load_dataset_builder (#2500)\n\n* Add load_dataset_builder\n\n* Fix\n\n* Add docstring\n\n* Remove _return_resolved_file_path arg\n\n* Improve camel-case/snake-case conversion\n\n* Add test\n\n* Fix test\n\n* Fix packaged_modules\n\n* Improve test\n\n* Fix prepare_module test\n\n* Doc improvement\n\n* Mention load_dataset_builder in docs\n\n* Remove replacements in packaged_module\n\n* Try to trigger CI\n\n* mention dataset_builder.info in the docs\n\nCo-authored-by: Quentin Lhoest <42851186+lhoestq@users.noreply.github.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def test_load_dataset_streaming(dataset_loading_script_dir, data_dir):",
            "def test_loading_from_the_datasets_hub():",
            "with tempfile.TemporaryDirectory() as tmp_dir:",
            "dataset = load_dataset(SAMPLE_DATASET_IDENTIFIER, cache_dir=tmp_dir)",
            "-        assert len(dataset[\"train\"]), 2",
            "-        assert len(dataset[\"validation\"]), 3",
            "+        assert len(dataset[\"train\"]) == 2",
            "+        assert len(dataset[\"validation\"]) == 3",
            "del dataset"
        ]
    },
    {
        "number": 1436,
        "comments": "",
        "commit_message": "leaf Variable inplace bug fix (#1619)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Model(nn.Module):",
            "m = self.model[-1]  # Detect() module",
            "for mi, s in zip(m.m, m.stride):  # from",
            "b = mi.bias.view(m.na, -1)  # conv.bias(255) to (3,85)",
            "-            b[:, 4] += math.log(8 / (640 / s) ** 2)  # obj (8 objects per 640 image)",
            "-            b[:, 5:] += math.log(0.6 / (m.nc - 0.99)) if cf is None else torch.log(cf / cf.sum())  # cls",
            "+            b.data[:, 4] += math.log(8 / (640 / s) ** 2)  # obj (8 objects per 640 image)",
            "+            b.data[:, 5:] += math.log(0.6 / (m.nc - 0.99)) if cf is None else torch.log(cf / cf.sum())  # cls",
            "mi.bias = torch.nn.Parameter(b.view(-1), requires_grad=True)",
            "",
            "def _print_biases(self):"
        ]
    },
    {
        "number": 1438,
        "comments": "",
        "commit_message": "[sgd] Fixes TrainingOperator wrap model incorrect (#14353)\n\nCo-authored-by: Amog Kamsetty <amogkamsetty@yahoo.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TrainingOperator:",
            "",
            "logger.debug(\"Registering optimizers.\")",
            "self._optimizers = optimizers",
            "-        if not isinstance(self._optimizers, Iterable):",
            "+        if isinstance(self._optimizers, torch.optim.Optimizer):",
            "self._optimizers = [self._optimizers]",
            "",
            "if schedulers:"
        ]
    },
    {
        "number": 1441,
        "comments": "",
        "commit_message": "pylint -> flake8 (#3288)\n\n* pylint\n\n* update pylint\n\n* undo a lot of the raise / else\n\n* add bound on typed-ast\n\n* first mypy fixes\n\n* new flag\n\n* fix mypy errors\n\n* requirements.txt\n\n* pylint -> flake8\n\n* mypy 0.720 -> mypy 0.730\n\n* add back erroneously removed initial newline\n\n* remove .pylintrc\n\n* remove pylintrc from Dockerfile\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class CnnHighwayEncoder(Seq2VecEncoder):",
            "``encoding``:",
            "Shape ``(batch_size, projection_dim)`` tensor with context-insensitive token representations.",
            "\"\"\"",
            "-        # pylint: disable=arguments-differ",
            "-",
            "# convolutions want (batch_size, embedding_dim, num_characters)",
            "inputs = inputs.transpose(1, 2)"
        ]
    },
    {
        "number": 1442,
        "comments": "",
        "commit_message": "[rllib] Also refactor DQN to use shared RLlib models (#730)\n\n* wip\n\n* works with cartpole\n\n* lint\n\n* fix pg\n\n* comment\n\n* action dist rename\n\n* preprocessor\n\n* fix test\n\n* typo\n\n* fix the action[0] nonsense\n\n* revert\n\n* satisfy the lint\n\n* wip\n\n* works with cartpole\n\n* lint\n\n* fix pg\n\n* comment\n\n* action dist rename\n\n* preprocessor\n\n* fix test\n\n* typo\n\n* fix the action[0] nonsense\n\n* revert\n\n* satisfy the lint\n\n* Minor indentation changes.\n\n* fix merge\n\n* add humanoid\n\n* initial dqn refactor\n\n* remove tfutil\n\n* fix calls\n\n* fix tf errors 1\n\n* closer\n\n* runs now\n\n* lint\n\n* tensorboard graph\n\n* fix linting\n\n* more 4 space\n\n* fix\n\n* fix linT\n\n* more lint\n\n* oops\n\n* es parity\n\n* remove example.py\n\n* fix training bug\n\n* add cartpole demo\n\n* try fixing cartpole\n\n* allow model options, configure cartpole\n\n* debug\n\n* simplify\n\n* no dueling\n\n* avoid out of file handles\n\n* Test dqn in jenkins.\n\n* Minor formatting.\n\n* fix issue\n\n* fix another\n\n* Fix problem in which we log to a directory that hasn't been created.\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class VisionNetwork(Model):",
            "conv2, 512, [10, 10], padding=\"VALID\", scope=\"fc1\")",
            "fc2 = slim.conv2d(fc1, num_outputs, [1, 1], activation_fn=None,",
            "normalizer_fn=None, scope=\"fc2\")",
            "-            return tf.squeeze(fc2, [1, 2])",
            "+            return tf.squeeze(fc2, [1, 2]), tf.squeeze(fc1, [1, 2])"
        ]
    },
    {
        "number": 1443,
        "comments": "",
        "commit_message": "Fx support for multiple model architectures (#17393)\n\n* Support for Bart and LayoutLM, and partial support for XLNet\n\n* Support for mbart\n\n* A lot of new models supported\n\n* Support for other models\n\n* LayoutLM fix\n\n* Use strings instead of classes\n",
        "label": "",
        "answer": "no",
        "change": [
            "class LayoutLMModel(LayoutLMPreTrainedModel):",
            "token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)",
            "",
            "if bbox is None:",
            "-            bbox = torch.zeros(tuple(list(input_shape) + [4]), dtype=torch.long, device=device)",
            "+            bbox = torch.zeros(input_shape + (4,), dtype=torch.long, device=device)",
            "",
            "extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)"
        ]
    },
    {
        "number": 1445,
        "comments": "",
        "commit_message": "Fix nn.DataParallel compatibility in PyTorch 1.5 (#4300)\n\n* Test case for #3936\n\n* multigpu tests pass on pytorch 1.4.0\n\n* Fixup\n\n* multigpu tests pass on pytorch 1.5.0\n\n* Update src/transformers/modeling_utils.py\n\n* Update src/transformers/modeling_utils.py\n\n* rename multigpu to require_multigpu\n\n* mode doc\n",
        "label": "",
        "answer": "no",
        "change": [
            "class BertModel(BertPreTrainedModel):",
            "",
            "# We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]",
            "# ourselves in which case we just need to make it broadcastable to all heads.",
            "-        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(",
            "-            attention_mask, input_shape, self.device",
            "-        )",
            "+        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape, device)",
            "",
            "# If a 2D ou 3D attention mask is provided for the cross-attention",
            "# we need to make broadcastabe to [batch_size, num_heads, seq_length, seq_length]"
        ]
    },
    {
        "number": 1446,
        "comments": "",
        "commit_message": "fixed example\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def batch_average(input, slice):",
            "\"\"\"Averages ``input`` features in the node dimension. Batch information is",
            "given by ``slice``.",
            "",
            "-    Example::",
            "+    Example:",
            "",
            "-        >>>> input = torch.FloatTensor([[1, 2], [3, 4], [5, 6], [7, 8]])",
            "-        >>>> slice = torch.LongTensor([2, 4])",
            "-        >>>> output = batch_average(input, slice)",
            "-        >>>> # [[2, 3], [6, 7]]",
            "+        >>> input = torch.FloatTensor([[1, 2], [3, 4], [5, 6], [7, 8]])",
            "+        >>> slice = torch.LongTensor([2, 4])",
            "+        >>> output = batch_average(input, slice)",
            "+        >>> # [[2, 3], [6, 7]]",
            "\"\"\"",
            "",
            "last_index = 0"
        ]
    },
    {
        "number": 1447,
        "comments": "",
        "commit_message": "tl.layers API Refactoring and various modifications (#667)\n\n* Decorators API Refactored\n\n* extra_requires `all`, `all_cpu` and `all_gpu` added\n\n* Error fix\n\n* YAPF Formating Correction\n\n* Test for private method decorator added\n\n* Test Logging Verbosity Fixed to Debug when runned individually\n\n* YAPF corrections applied\n\n* Changelog Added\n\n* Changelog updated\n\n* PR number changed\n\n* First Refactoring Pass done\n\n* cleaning second pass\n\n* Refactoring 3rd pass\n\n* Refactoring 4th Pass\n\n* Code Error fix\n\n* YAPF Formating Fix\n\n* Arguments now using self\n\n* YAPF error correction\n\n* Bug Fix in Decorator\n\n* act name bug fix\n\n* Error Correction\n\n* YAPF formating fix\n\n* Useless tf.identity removed\n\n* Error Fix\n\n* Changelog Updated\n\n* Error fix in tl.activation\n\n* Documentation error fix\n\n* Lazy Import added\n\n* Import Refactoring with LazyImport when necessary\n\n* Changelog Updated\n\n* Gitter Removed\n\n* Fixed proposed by @zsdonghao\n\n* Documentation updated\n\n* Missing requirements added\n\n* Update to TensorLayer 1.8.6rc1\n\n* Requirements error fix\n\n* Docker Files updated\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class SpatialTransformer2dAffineLayer(Layer):",
            "# 4. Get all parameters",
            "variables = tf.get_collection(TF_GRAPHKEYS_VARIABLES, scope=vs.name)",
            "",
            "-        # # fixed",
            "-        # self.all_layers = list(layer.all_layers)",
            "-        # self.all_params = list(layer.all_params)",
            "-        # self.all_drop = dict(layer.all_drop)",
            "-        #",
            "# # theta_layer",
            "# self.all_layers.extend(theta_layer.all_layers)",
            "# self.all_params.extend(theta_layer.all_params)",
            "# self.all_drop.update(theta_layer.all_drop)",
            "",
            "-        # this layer",
            "self.all_layers.append(self.outputs)",
            "self.all_params.extend(variables)"
        ]
    },
    {
        "number": 1448,
        "comments": "",
        "commit_message": "[WIP] - Documentation Cleaning and Coding Style (#516)\n\n* Activation Cleaning Docstring Test\n\n* Requirements Pinned with range to insure tested versions are used. Range are used to prevent updating requirements all the time.\n\n* setup.cfg file added with PEP8 configuration\n\n* activation.py refactored\n\n* docstring fixed - ready for documentation unittest\n\n* Yapf correction for max_line_length: 120\n\n* test yapf refactored\n\n* Requirements conflict solved\n\n* Yapf Style modified and merged in file \"setup.cfg\"\n\n* Yapf Confiuguration Updated\n\n* Code Refactored with new YAPF formating style\n\n* Code Refactored with new YAPF formating style\n\n* Code Refactored with new YAPF formating style\n\n* tl.layers.pooling YAPF reformat\n\n* yapf updated\n\n* gitignore updated\n\n* YAPF Style Fixing Attempt\n\n* Space Error Fix\n\n* Style Correction\n\n* Assertion Codacy Errors Corrected\n\n* Error Fix\n\n* Assertion Refactored\n\n* YAPF Style Applied to Master\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "import tensorlayer as tl",
            "def model(x, is_train=True, reuse=False, name_scope=\"env1\"):",
            "with tf.variable_scope(name_scope, reuse=reuse):",
            "net = tl.layers.InputLayer(x, name='input')",
            "-        net = tl.layers.TimeDistributedLayer(net, layer_class=tl.layers.DenseLayer, args={'n_units': 50, 'name': 'dense'}, name='time_dense')",
            "+        net = tl.layers.TimeDistributedLayer(",
            "+            net, layer_class=tl.layers.DenseLayer, args={",
            "+                'n_units': 50,",
            "+                'name': 'dense'",
            "+            }, name='time_dense'",
            "+        )",
            "return net",
            "",
            "",
            "class Layer_Time_Distributed_Test(CustomTestCase):",
            "+",
            "@classmethod",
            "def setUpClass(cls):"
        ]
    },
    {
        "number": 1449,
        "comments": "",
        "commit_message": "embed sized setting in default lm bug fix\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class RNNLM(nn.Module):",
            "super(RNNLM, self).__init__()",
            "self.embed = nn.Embedding(n_vocab, n_embed)",
            "if typ == \"lstm\":",
            "-            self.rnn = nn.ModuleList( [nn.LSTMCell(n_embed, n_units)] + [nn.LSTMCell(n_units, n_units) for _ in range(n_layers - 1)] )",
            "+            self.rnn = nn.ModuleList(",
            "+                [nn.LSTMCell(n_embed, n_units)] + [nn.LSTMCell(n_units, n_units) for _ in range(n_layers - 1)]",
            "+                )",
            "else:",
            "-            self.rnn = nn.ModuleList( [nn.GRUCell(n_embed, n_units)] + [nn.GRUCell(n_units, n_units) for _ in range(n_layers - 1)] )",
            "-",
            "+            self.rnn = nn.ModuleList(",
            "+                [nn.GRUCell(n_embed, n_units)] + [nn.GRUCell(n_units, n_units) for _ in range(n_layers - 1)]",
            "+                )",
            "+",
            "self.dropout = nn.ModuleList(",
            "[nn.Dropout(dropout_rate) for _ in range(n_layers + 1)])",
            "self.lo = nn.Linear(n_units, n_vocab)"
        ]
    },
    {
        "number": 1450,
        "comments": "",
        "commit_message": "Fix loss\n\nPlease review @thomwolf but i think this is equivqlent (and it mimics the loss computation of the original loss)\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class BertForQuestionAnswering(nn.Module):",
            "",
            "def compute_loss(logits, positions):",
            "max_position = positions.max().item()",
            "-                one_hot = torch.FloatTensor(batch_size, max(max_position, seq_length) +1, device=input_ids.device).zero_()",
            "+                one_hot = torch.FloatTensor(batch_size, max(max_position, seq_length) +1).zero_()",
            "one_hot = one_hot.scatter(1, positions.cpu(), 1) # Second argument need to be LongTensor and not cuda.LongTensor",
            "-                one_hot = one_hot[:, :seq_length]",
            "+                one_hot = one_hot[:, :seq_length].to(input_ids.device)",
            "log_probs = nn.functional.log_softmax(logits, dim = -1).view(batch_size, seq_length)",
            "loss = -torch.mean(torch.sum(one_hot*log_probs), dim = -1)",
            "return loss"
        ]
    },
    {
        "number": 1451,
        "comments": "",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class BagOfEmbeddingsEncoder(Seq2VecEncoder):",
            "summed = summed / lengths.unsqueeze(-1).float()",
            "",
            "if length_mask is not None:",
            "-                summed = summed * (length_mask > 0).float().unsqueeze(-1)",
            "+                summed = summed * (length_mask > 0).unsqueeze(-1)",
            "",
            "return summed"
        ]
    },
    {
        "number": 1459,
        "comments": "",
        "commit_message": "enable `disallow_incomplete_defs` on mypy (#2094)\n\n* enable `disallow_incomplete_defs` on mypy\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix `blur_pool2d` doc\n\n* finish v1: works on torch 1.13.1\n\n- Remove JIT support for Boxes3D\n\n* rip off the np typing\n\n* replace `Size` with `Tuple[int, ...]` on augs\n\n* add `Dtype` to kornia.filters.kernels\n\n* minor fix after rebase\n\n* Remove old torch from typing CI\n\n---------\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def vec_like(n: int, tensor: torch.Tensor, shared_memory: bool = False):",
            "if len(tensor.shape) < 1:",
            "raise AssertionError(tensor.shape)",
            "",
            "-    vec = torch.zeros(n, 1, device=tensor.device, dtype=tensor.dtype)",
            "+    vec = zeros(n, 1, device=tensor.device, dtype=tensor.dtype)",
            "return vec[None].expand(tensor.shape[0], n, 1) if shared_memory else vec[None].repeat(tensor.shape[0], 1, 1)"
        ]
    },
    {
        "number": 1462,
        "comments": "",
        "commit_message": "Scope TokenIndexer output by indexer name (#3597)\n\n* Indexer tests are now passing, at least\n\n* Fixed some masking issues, and padding keys for one config file\n\n* TextFieldEmbedder tests pass\n\n* Fixing fixtures, some more tests passing\n\n* Fixed weird ordering bug\n\n* All TokenEmbedder tests passing?\n\n* Update fixtures\n\n* Fix more hard-coded references\n\n* fix field tests\n\n* fix dataset reader tests\n\n* Fix iterator tests\n\n* More tests passing\n\n* fix hotflip and some other tests\n\n* more tests\n\n* more test fixes\n\n* more tests\n\n* most tests passing; I think the remaining ones are spacy model changes\n\n* hard-code POS tag test\n\n* last test, I think\n\n* black\n\n* updated black\n\n* flake8\n\n* mypy\n\n* black again\n\n* fix training configs\n\n* remove reference to embedder_to_indexer_map\n\n* Other fixes from PR comments\n\n* fix breakage from incorrect merge during rebase\n\n* flake, some docstring formatting\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class LanguageModel(Model):",
            "return_dict = {}",
            "",
            "# If we have target tokens, calculate the loss.",
            "-        token_ids = source.get(\"tokens\")",
            "-        if token_ids is not None:",
            "+        token_id_dict = source.get(\"tokens\")",
            "+        if token_id_dict is not None:",
            "+            token_ids = token_id_dict[\"tokens\"]",
            "assert isinstance(contextual_embeddings, torch.Tensor)",
            "",
            "# Use token_ids to compute targets"
        ]
    },
    {
        "number": 1463,
        "comments": "",
        "commit_message": "Fix SummaryWritter error and similar deprecated warnings (#248)\n\nTF1.0 advises to use FileWriter and similarly pathed functions to replace deprecated ones.\n  All changes here are related to fixing deprecated errors and warnings.\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def main():",
            "",
            "# Save the result as an audio summary.",
            "datestring = str(datetime.now()).replace(' ', 'T')",
            "-    writer = tf.train.SummaryWriter(logdir)",
            "-    tf.audio_summary('generated', decode, wavenet_params['sample_rate'])",
            "-    summaries = tf.merge_all_summaries()",
            "+    writer = tf.summary.FileWriter(logdir)",
            "+    tf.summary.audio('generated', decode, wavenet_params['sample_rate'])",
            "+    summaries = tf.summary.merge_all()",
            "summary_out = sess.run(summaries,",
            "feed_dict={samples: np.reshape(waveform, [-1, 1])})",
            "writer.add_summary(summary_out)"
        ]
    },
    {
        "number": 1465,
        "comments": "",
        "commit_message": "Fix CI with change of name of nlp (#7054)\n\n* nlp -> datasets\n\n* More nlp -> datasets\n\n* Woopsie\n\n* More nlp -> datasets\n\n* One last\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def load_indexes():",
            "",
            "@st.cache(allow_output_mutation=True)",
            "def load_train_data():",
            "-    eli5 = nlp.load_dataset(\"eli5\", name=\"LFQA_reddit\")",
            "+    eli5 = datasets.load_dataset(\"eli5\", name=\"LFQA_reddit\")",
            "eli5_train = eli5[\"train_eli5\"]",
            "eli5_train_q_reps = np.memmap(",
            "\"eli5_questions_reps.dat\", dtype=\"float32\", mode=\"r\", shape=(eli5_train.num_rows, 128)"
        ]
    },
    {
        "number": 1466,
        "comments": "",
        "commit_message": "Fixed failing test for depthwise_conv2d (#9229)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def depthwise_conv2d(",
            "dilations: Optional[Union[int, Tuple[int, int]]] = 1,",
            "out: Optional[torch.Tensor] = None,",
            ") -> torch.Tensor:",
            "-    x = torch.tensor(x)",
            "-    filters = torch.tensor(filters)",
            "+    x = torch.as_tensor(x)",
            "+    filters = torch.as_tensor(filters)",
            "strides = [strides] * 2 if isinstance(strides, int) else strides",
            "strides = [strides[1], strides[2]] if len(strides) == 4 else strides",
            "dilations = [dilations] * 2 if isinstance(dilations, int) else dilations",
            "-    filters = ivy.squeeze(filters, 3) if filters.ndim == 4 else filters",
            "+    filters = ivy.squeeze(filters, 3).to_native() if filters.ndim == 4 else filters",
            "",
            "f_w_after_dilation = filters.shape[1] + (",
            "(dilations[1] - 1) * (filters.shape[1] - 1)"
        ]
    },
    {
        "number": 1468,
        "comments": "",
        "commit_message": "small fix\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class SequenceTagger(flair.nn.Model):",
            "lengths: List[int] = [len(sentence.tokens) for sentence in sentences]",
            "longest_token_sequence_in_batch: int = max(lengths)",
            "",
            "-        pre_allocated_zero_tensor = t = torch.zeros(",
            "+        pre_allocated_zero_tensor = torch.zeros(",
            "self.embeddings.embedding_length * longest_token_sequence_in_batch,",
            "dtype=torch.float,",
            "device=flair.device,"
        ]
    },
    {
        "number": 1469,
        "comments": "",
        "commit_message": "Fix test suite when running on MPS-enabled hardware (#14708)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_lite_dataloader_device_placement(src_device_str, dest_device_str):",
            "iterator = iter(lite_dataloader)",
            "",
            "batch0 = next(iterator)",
            "-    # TODO: This should be torch.equal, but not supported on MPS at this time (torch 1.12)",
            "-    assert torch.allclose(batch0, torch.tensor([0, 1], device=dest_device))",
            "+    # TODO: torch.equal is not supported on MPS at this time (torch 1.12)",
            "+    assert torch.equal(batch0, torch.tensor([0, 1], device=dest_device))",
            "",
            "batch1 = next(iterator)",
            "-    # TODO: This should be torch.equal, but not supported on MPS at this time (torch 1.12)",
            "-    assert torch.allclose(batch1[\"data\"], torch.tensor([2, 3], device=dest_device))",
            "+    # TODO: torch.equal is not supported on MPS at this time (torch 1.12)",
            "+    assert torch.equal(batch1[\"data\"], torch.tensor([2, 3], device=dest_device))",
            "",
            "",
            "def test_lite_optimizer_wraps():"
        ]
    },
    {
        "number": 1470,
        "comments": "",
        "commit_message": "Quick fix :) (#606)\n\n* Changing the name\n\n* style + quality\n\n* update doc and logo\n\n* clean up\n\n* circle-CI on the branche for now\n\n* fix daily dialog dataset\n\n* fix urls\n\nCo-authored-by: Quentin Lhoest <lhoest.q@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "Returns:",
            "\"\"\"",
            "",
            "",
            "-class Sacrebleu(nlp.Metric):",
            "+class Sacrebleu(datasets.Metric):",
            "def _info(self):",
            "-        return nlp.MetricInfo(",
            "+        return datasets.MetricInfo(",
            "description=_DESCRIPTION,",
            "citation=_CITATION,",
            "homepage=\"https://github.com/mjpost/sacreBLEU\",",
            "inputs_description=_KWARGS_DESCRIPTION,",
            "-            features=nlp.Features(",
            "+            features=datasets.Features(",
            "{",
            "-                    \"predictions\": nlp.Value(\"string\", id=\"sequence\"),",
            "-                    \"references\": nlp.Sequence(nlp.Value(\"string\", id=\"sequence\"), id=\"references\"),",
            "+                    \"predictions\": datasets.Value(\"string\", id=\"sequence\"),",
            "+                    \"references\": datasets.Sequence(datasets.Value(\"string\", id=\"sequence\"), id=\"references\"),",
            "}",
            "),",
            "codebase_urls=[\"https://github.com/mjpost/sacreBLEU\"],"
        ]
    }
]