{"number": 4, "change": "class TrainerIntegrationTest(unittest.TestCase):\n\n# Adding one column not used by the model should have no impact\nz = np.random.normal(size=(64,)).astype(np.float32)\n-        train_dataset = nlp.Dataset.from_dict({\"input_x\": x, \"label\": y, \"extra\": z})\n+        train_dataset = datasets.Dataset.from_dict({\"input_x\": x, \"label\": y, \"extra\": z})\nmodel = RegressionModel()\ntrainer = Trainer(model, args, train_dataset=train_dataset)\ntrainer.train()\n", "fix_pattern": "In the condition of using the `datasets` module, if the function `from_dict` is detected, then change it to `Dataset.from_dict` to fix the API misuse."}
{"number": 7, "change": "def test_quantile():\n\n\ndef test_pi():\n-    x = torch.empty(1000).log_normal_(0, 1)\n+    x = torch.randn(1000).exp()\nassert_equal(pi(x, prob=0.8), quantile(x, probs=[0.1, 0.9]))\n", "fix_pattern": "Fix_pattern: \nIn the condition of 'calling the log_normal_ method on an empty tensor', if 'torch.empty(1000).log_normal_(0, 1)' is detected, then change the code to 'torch.randn(1000).exp()' to fix the API misuse."}
{"number": 8, "change": "class TPUAccelerator(Accelerator):\nReturn:\nA tensor of shape (world_size, batch, ...)\n\"\"\"\n-        return xm.all_gather(tensor, group=group, sync_grads=sync_grads)\n+        # todo: Add support for backward with all_gather\n+        if torch.distributed.is_initialized():\n+            return xm.all_gather(tensor, group=group, sync_grads=sync_grads)\n+        return tensor\n", "fix_pattern": "In the condition of \"if torch.distributed.is_initialized()\", if the pattern of missing support for backward with all_gather is detected, then add the code to support backward with all_gather."}
{"number": 9, "change": "class Swinv2SelfAttention(nn.Module):\nquery_layer = self.transpose_for_scores(mixed_query_layer)\n\n# cosine attention\n-        attention_scores = F.normalize(query_layer, dim=-1) @ F.normalize(key_layer, dim=-1).transpose(-2, -1)\n+        attention_scores = nn.functional.normalize(query_layer, dim=-1) @ nn.functional.normalize(\n+            key_layer, dim=-1\n+        ).transpose(-2, -1)\nlogit_scale = torch.clamp(self.logit_scale, max=math.log(1.0 / 0.01)).exp()\nattention_scores = attention_scores * logit_scale\nrelative_position_bias_table = self.continuous_position_bias_mlp(self.relative_coords_table).view(\n", "fix_pattern": "In the condition of using the `normalize` function from the `nn.functional` module, if the code is using the `F.normalize` function instead, change the `F.normalize` to `nn.functional.normalize` to fix the API misuse."}
{"number": 12, "change": "def get_rotation_matrix2d(\n\n# create output tensor\nbatch_size: int = center.shape[0]\n-    one = torch.tensor(1.).to(center.device)\n+    one = torch.tensor(1., device=center.device, dtype=center.dtype)\nM: torch.Tensor = torch.zeros(\nbatch_size, 2, 3, device=center.device, dtype=center.dtype)\nM[..., 0:2, 0:2] = scaled_rotation\n", "fix_pattern": "In the condition of assigning a value to a tensor, if the API misuse pattern of not specifying the device and dtype is detected, then add the device and dtype arguments to fix the API misuse."}
{"number": 15, "change": "class Trainer(TrainerBase):\nbreak\nsys.stdout.flush()\n\n-        model.load_state_dict(torch.load(best_model_path))\n+        if rank == 0:\n+            model.load_state_dict(torch.load(best_model_path))\nreturn model, best_metric\n\ndef _run_epoch(\n", "fix_pattern": "in the condition of checking rank == 0, if the pattern of not loading the model is detected, then add the code to load the model to fix the API misuse."}
{"number": 16, "change": "def test_preprocess_weights_for_loading_gru_incompatible():\n\ndef assert_not_compatible(src, dest, message):\nwith pytest.raises(ValueError) as ex:\n-            keras.engine.topology.preprocess_weights_for_loading(\n+            keras.engine.saving.preprocess_weights_for_loading(\ndest, initialize_weights(src).get_weights())\nassert message in ex.value.message\n", "fix_pattern": "In the condition of using the `assert_not_compatible` function, if the code `keras.engine.topology.preprocess_weights_for_loading` is detected, then change it to `keras.engine.saving.preprocess_weights_for_loading` to fix the API misuse."}
{"number": 17, "change": "Returns:\n\"\"\"\n\n\n-class BERTScore(nlp.Metric):\n+class BERTScore(datasets.Metric):\ndef _info(self):\n-        return nlp.MetricInfo(\n+        return datasets.MetricInfo(\ndescription=_DESCRIPTION,\ncitation=_CITATION,\nhomepage=\"https://github.com/Tiiiger/bert_score\",\ninputs_description=_KWARGS_DESCRIPTION,\n-            features=nlp.Features(\n+            features=datasets.Features(\n{\n-                    \"predictions\": nlp.Value(\"string\", id=\"sequence\"),\n-                    \"references\": nlp.Sequence(nlp.Value(\"string\", id=\"sequence\"), id=\"references\"),\n+                    \"predictions\": datasets.Value(\"string\", id=\"sequence\"),\n+                    \"references\": datasets.Sequence(datasets.Value(\"string\", id=\"sequence\"), id=\"references\"),\n}\n),\ncodebase_urls=[\"https://github.com/Tiiiger/bert_score\"],\n", "fix_pattern": "in the condition of changing the base class from nlp.Metric to datasets.Metric, if the codebase uses nlp.Features and nlp.Value, then change them to datasets.Features and datasets.Value to fix the API misuse."}
{"number": 18, "change": "class CoarseMaskHead(FCNMaskHead):\nfor i in range(num_fcs):\nfc_in_channels = (\nlast_layer_dim if i == 0 else self.fc_out_channels)\n-            self.fcs.append(nn.Linear(fc_in_channels, self.fc_out_channels))\n+            self.fcs.append(Linear(fc_in_channels, self.fc_out_channels))\nlast_layer_dim = self.fc_out_channels\noutput_channels = self.num_classes * self.output_area\n-        self.fc_logits = nn.Linear(last_layer_dim, output_channels)\n+        self.fc_logits = Linear(last_layer_dim, output_channels)\n\ndef init_weights(self):\nfor m in self.fcs.modules():\n", "fix_pattern": "In the condition of creating the linear layers, if an incorrect API usage is detected (i.e. nn.Linear), then the code should be updated to use the correct API (i.e. Linear) to fix the API misuse."}
{"number": 21, "change": "class SequenceTagger(flair.nn.DefaultClassifier):\nfor sentence in batch:\nsentence.remove_labels(label_name)\n\n-            loss = self._calculate_loss(features, gold_labels)\n-\nif return_loss:\n+                loss = self._calculate_loss(features, gold_labels)\noverall_loss += loss[0]\nlabel_count += loss[1]\n", "fix_pattern": "In the condition of \"if return_loss\", if the pattern of \"self._calculate_loss(features, gold_labels)\" is detected, then add the code \"loss = self._calculate_loss(features, gold_labels)\" to fix the API misuse."}
{"number": 23, "change": "class TacotronTrainTest(unittest.TestCase):\noptimizer = optim.Adam(model.parameters(), lr=c.lr)\nfor i in range(5):\nmel_out, linear_out, align, stop_tokens = model.forward(\n-                input, input_lengths, mel_spec)\n+                input, input_lengths, mel_spec, speaker_ids)\noptimizer.zero_grad()\nloss = criterion(mel_out, mel_spec, mel_lengths)\nstop_loss = criterion_st(stop_tokens, stop_targets)\n", "fix_pattern": "In the condition of \"model.forward()\", if the pattern \"speaker_ids\" is detected, then add \"speaker_ids\" as an argument to the function call in order to fix the API misuse."}
{"number": 24, "change": "def evaluate(args, model, tokenizer, prefix=\"\", test=False):\neval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n\n# multi-gpu evaluate\n-        if args.n_gpu > 1:\n+        if args.n_gpu > 1 and not isinstance(model, torch.nn.DataParallel):\nmodel = torch.nn.DataParallel(model)\n\n# Eval!\n", "fix_pattern": "In the condition of \"args.n_gpu > 1\", if the pattern of \"not isinstance(model, torch.nn.DataParallel)\" is detected, then add the code \"model = torch.nn.DataParallel(model)\" to fix the API misuse."}
{"number": 31, "change": "class StableDiffusionProcessingTxt2Img(StableDiffusionProcessing):\nreturn samples\n\nx = create_random_tensors([opt_C, self.firstphase_height // opt_f, self.firstphase_width // opt_f], seeds=seeds, subseeds=subseeds, subseed_strength=self.subseed_strength, seed_resize_from_h=self.seed_resize_from_h, seed_resize_from_w=self.seed_resize_from_w, p=self)\n-        samples = self.sampler.sample(self, x, conditioning, unconditional_conditioning, image_conditioning=self.create_dummy_mask(x))\n+        samples = self.sampler.sample(self, x, conditioning, unconditional_conditioning, image_conditioning=self.create_dummy_mask(x, first_phase=True))\n\nsamples = samples[:, :, self.truncate_y//2:samples.shape[2]-self.truncate_y//2, self.truncate_x//2:samples.shape[3]-self.truncate_x//2]\n", "fix_pattern": "In the condition of creating a dummy mask for an image conditioning, if the first_phase attribute is not provided, then the code \"create_dummy_mask(x)\" is changed to \"create_dummy_mask(x, first_phase=True)\" to fix the API misuse."}
{"number": 32, "change": "def sort_batch_by_length(tensor: torch.autograd.Variable, sequence_lengths: torc\nsorted_tensor = tensor.index_select(0, permutation_index)\n# This is the equivalent of zipping with index, sorting by the original\n# sequence lengths and returning the now sorted indices.\n-    index_range = Variable(torch.range(0, len(sequence_lengths) - 1).long())\n+    index_range = Variable(torch.arange(0, len(sequence_lengths)).long())\n_, reverse_mapping = permutation_index.sort(0, descending=False)\nrestoration_indices = index_range.index_select(0, reverse_mapping)\nreturn sorted_tensor, sorted_sequence_lengths, restoration_indices\n", "fix_pattern": "In the condition of comparing the lengths of two sequences, if the function \"torch.range()\" is used, then change it to \"torch.arange()\" to fix the API misuse."}
{"number": 40, "change": "def makenp(x, modality=None):\n\ndef pytorch_np(x, modality):\nimport torch\n-    if isinstance(x, torch.autograd.variable.Variable):\n+    if isinstance(x, torch.autograd.Variable):\nx = x.data\nx = x.cpu().numpy()\nif modality == 'IMG':\n", "fix_pattern": "In the condition of \"isinstance(x, torch.autograd.variable.Variable)\", if \"torch.autograd.variable\" is detected, then change \"torch.autograd.variable\" to \"torch.autograd.Variable\" to fix the API misuse."}
{"number": 43, "change": "class TrainingTypePlugin(ABC):\nself.lr_schedulers = schedulers\n\ndef _move_optimizer_state(self, device: Optional[torch.device] = None) -> None:\n-        \"\"\"Moves the state of the optimizers to the GPU if needed.\"\"\"\n-        device = device or self.root_device\n+        \"\"\"Moves the state of the optimizers to the appropriate device if needed.\"\"\"\nfor opt in self.optimizers:\nfor p, v in opt.state.items():\n-                opt.state[p] = apply_to_collection(v, torch.Tensor, move_data_to_device, device)\n+                # `self.root_device` would raise error if called outside the spawn process\n+                # while training on 8 and more cores.\n+                opt.state[p] = apply_to_collection(v, torch.Tensor, move_data_to_device, device or self.root_device)\n\ndef optimizer_state(self, optimizer: Optimizer) -> Dict[str, Tensor]:\n\"\"\"Returns state of an optimizer.\n", "fix_pattern": "In the condition of `if self.root_device` being called outside the spawn process while training on 8 and more cores, the fix pattern is to change the code `device = device or self.root_device` to `device or self.root_device` in order to fix the API misuse."}
{"number": 50, "change": "class GraphConv(MessagePassing):\nself.lin.reset_parameters()\n\ndef forward(self, x, edge_index):\n+        if isinstance(x, Tensor):\n+            x = (x, x)\nreturn self.propagate(edge_index, x=(self.lin(x[0]), x[1]))\n", "fix_pattern": "In the condition of \"if isinstance(x, Tensor)\", if the pattern \"x = (x, x)\" is detected, then add the code \"x = (x, x)\" to fix the API misuse."}
{"number": 53, "change": "class Trainer:\n).to(self.args.device)\n\nelif is_sagemaker_dp_enabled():\n-            model = DDP(model, device_ids=[dist.get_local_rank()], broadcast_buffers=False)\n+            model = nn.parallel.DistributedDataParallel(\n+                model, device_ids=[int(os.getenv(\"SMDATAPARALLEL_LOCAL_RANK\"))]\n+            )\nelif self.args.local_rank != -1:\nkwargs = {}\nif self.args.ddp_find_unused_parameters is not None:\n", "fix_pattern": "In the condition of `is_sagemaker_dp_enabled()`, if the pattern `DDP` is detected, then change the code to use `nn.parallel.DistributedDataParallel` to fix the API misuse."}
{"number": 54, "change": "class Network(object):\n\"\"\"\n@layer\ndef softmax(self, target, axis, name=None):\n-        max_axis = tf.reduce_max(target, axis, keepdims=True)\n+        max_axis = tf.reduce_max(target, axis, keep_dims=True)\ntarget_exp = tf.exp(target-max_axis)\n-        normalize = tf.reduce_sum(target_exp, axis, keepdims=True)\n+        normalize = tf.reduce_sum(target_exp, axis, keep_dims=True)\nsoftmax = tf.div(target_exp, normalize, name)\nreturn softmax\n", "fix_pattern": "In the condition of using the `reduce_max` and `reduce_sum` functions on a tensor, if the `keepdims` parameter is used with an incorrect spelling, then change the spelling of `keepdims` to `keep_dims` to fix the API misuse."}
{"number": 55, "change": "class RNN(torch.nn.Module):\nif not isinstance(ilens, torch.Tensor):\nilens = torch.tensor(ilens)\nxs_pack = pack_padded_sequence(xs_pad, ilens.cpu(), batch_first=True)\n-        self.nbrnn.flatten_parameters()\n+        if self.training:\n+            self.nbrnn.flatten_parameters()\nif prev_state is not None and self.nbrnn.bidirectional:\n# We assume that when previous state is passed,\n# it means that we're streaming the input\n", "fix_pattern": "Fix pattern: \nIn the condition of \"self.training\", if \"self.nbrnn.flatten_parameters()\" is detected, then add \"if self.training:\" before \"self.nbrnn.flatten_parameters()\" to fix the API misuse."}
{"number": 56, "change": "class TFXLNetModelTest(TFModelTesterMixin, unittest.TestCase):\n# Send to model\nloss = model(tuple_input[:-1])[0]\n\n-                self.assertEqual(loss.shape, [loss_size])\n+                self.assertEqual(loss.shape.as_list(), expected_loss_size)\n\n\n@require_tf\n", "fix_pattern": "In the condition of comparing the shape of a tensor, if `shape` is used instead of `shape.as_list()`, then change the code to `shape.as_list()` to fix the API misuse."}
{"number": 58, "change": "def sigmoid_example(design):\ntorch.tensor([[-1.5, 0.5], [1.5, 0.]])\n),\n(\n-        known_covariance_linear_model(torch.tensor([1., -1.]), torch.tensor(10.), torch.tensor(1.)),\n+        known_covariance_linear_model(torch.tensor([1., -1.]), torch.tensor([10., 10.]), torch.tensor(1.)),\nnz_lm_2p_10_10_1,\ntorch.tensor([[-1., 0.5], [2.5, -2.]])\n),\n", "fix_pattern": "In the condition of passing a tensor with a single value as argument to the \"known_covariance_linear_model\" function, if a pattern of passing a tensor with multiple values is detected, then changing the tensor argument from a single value to a tensor with multiple values fixes the API misuse."}
{"number": 59, "change": "class DetaModel(DetaPreTrainedModel):\nscale = 2 * math.pi\n\ndim_t = torch.arange(num_pos_feats, dtype=torch.float32, device=proposals.device)\n-        dim_t = temperature ** (2 * (dim_t // 2) / num_pos_feats)\n+        dim_t = temperature ** (2 * torch.div(dim_t, 2) / num_pos_feats)\n# batch_size, num_queries, 4\nproposals = proposals.sigmoid() * scale\n# batch_size, num_queries, 4, 128\n", "fix_pattern": "in the condition of assignment operation, if the pattern of using integer division operator \"//\" is detected, then change it to the pattern of using the division operator \"/\" to fix the API misuse."}
{"number": 61, "change": "class LxmertAttention(nn.Module):\nattention_scores = attention_scores + attention_mask\n\n# Normalize the attention scores to probabilities.\n-        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n+        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n\n# This is actually dropping out entire tokens to attend to, which might\n# seem a bit unusual, but is taken from the original Transformer paper.\n", "fix_pattern": "in the condition of using the nn.Softmax() function, if the function is used with the dim parameter being -1, then change the function to nn.functional.softmax() to fix the API misuse."}
{"number": 66, "change": "class DynamicConvolution2D(nn.Module):\nweight = self.linear_weight(x)  # B x T x kH\nweight = F.dropout(weight, self.dropout_rate, training=self.training)\nweight = weight.view(B, T, H, k).transpose(1, 2).contiguous()  # B x H x T x k\n-        weight_new = torch.zeros(B * H * T * (T + k - 1)).view(B, H, T, T + k - 1).fill_(float('-inf'))\n+        weight_new = torch.zeros(B * H * T * (T + k - 1), dtype=weight.dtype)\n+        weight_new = weight_new.view(B, H, T, T + k - 1).fill_(float('-inf'))\nweight_new = weight_new.to(x.device)  # B x H x T x T+k-1\nweight_new.as_strided((B, H, T, k), ((T + k - 1) * T * H, (T + k - 1) * T, T + k, 1)).copy_(weight)\nweight_new = weight_new.narrow(-1, int((k - 1) / 2), T)  # B x H x T x T(k)\n", "fix_pattern": "In the condition of using torch.zeros to initialize a tensor, if dtype is missing, then add dtype=weight.dtype to fix the API misuse."}
{"number": 68, "change": "class Gru(TransformationBase):\n\ndef tf_apply(self, x, sequence_length=None):\nx, state = tf.nn.dynamic_rnn(\n-            cell=self.cell, inputs=x, sequence_length=sequence_length, dtype=tf.float32,\n+            cell=self.cell, inputs=x, sequence_length=sequence_length, initial_state=None,\n+            dtype=util.tf_dtype(dtype='float'),\n# Weird TensorFlow behavior? (https://github.com/tensorflow/tensorflow/issues/15874)\nparallel_iterations=(self.input_spec['shape'][0] + 1)\n)\n", "fix_pattern": "In the condition of an API call to `tf.nn.dynamic_rnn`, if the parameter `dtype=tf.float32` is detected, then change it to `dtype=util.tf_dtype(dtype='float')` to fix the API misuse."}
{"number": 69, "change": "class Optimizer(Component):\nFor those we treat model as max_norm.\neg. optimizer.clip_grad_norm(max_norm)\n\"\"\"\n-            return torch.nn.utils.clip_grad_norm_(self.params, max_norm)\n+            return clip_grad_norm_(self.params, max_norm)\nelse:\n-            return torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n+            return clip_grad_norm_(model.parameters(), max_norm)\n\ndef pre_export(self, model):\npass\n", "fix_pattern": "In the condition of \"if model is self.params\", if the method \"torch.nn.utils.clip_grad_norm_\" is called, then remove \"torch.nn.utils.\" to fix the API misuse."}
{"number": 76, "change": "def main(args):\nbob_decision = Marginal(Search(bob))\n\n# Here Alice and Bob slightly prefer one location over the other a priori\n-    shared_preference = Variable(torch.Tensor([args.preference]))\n+    shared_preference = torch.tensor([args.preference])\n\nbob_depth = args.depth\nnum_samples = args.num_samples\n", "fix_pattern": "In the condition of assigning a value to a variable, if the code contains \"Variable(torch.Tensor)\" as the assignment, then change it to \"torch.tensor\" to fix the API misuse."}
{"number": 81, "change": "class Planetoid(Dataset):\n# Create unweighted sparse adjacency matrix.\nweight = torch.ones(index.size(1))\nn = input.size(0)\n-        adj = torch.sparse.FloatTensor(index, weight, torch.Size([n, n]))\n+        adj = SparseTensor(index, weight, torch.Size([n, n]))\n\n# Bundle graph to data object.\n-        self.data = Data(input, adj, position=None, target=target)\n+        self.data = Data(input, adj, position=None, target=target.long())\n\ndef __getitem__(self, index):\ndata = self.data\n", "fix_pattern": "in the condition of creating a sparse adjacency matrix, if SparseTensor is detected, then change the code to use SparseTensor instead of torch.sparse.FloatTensor to fix the API misuse."}
{"number": 85, "change": "\"import sys\\n\",\n\"sys.path.append(f'{os.getcwd()}/SentEval')\\n\",\n\"\\n\",\n-        \"import tensorflow_hub as hub\\n\",\n\"import tensorflow as tf\\n\",\n+        \"\\n\",\n+        \"# Prevent TF from claiming all GPU memory so there is some left for pytorch.\\n\",\n+        \"gpus = tf.config.list_physical_devices('GPU')\\n\",\n+        \"if gpus:\\n\",\n+        \"  # Memory growth needs to be the same across GPUs.\\n\",\n+        \"  for gpu in gpus:\\n\",\n+        \"    tf.config.experimental.set_memory_growth(gpu, True)\\n\",\n+        \"\\n\",\n+        \"import tensorflow_hub as hub\\n\",\n\"import tensorflow_text\\n\",\n\"import senteval\\n\",\n\"import time\\n\",\n", "fix_pattern": "In the condition of using TensorFlow with GPUs, if there is a need to allocate GPU memory manually, then the code \"tf.config.experimental.set_memory_growth(gpu, True)\" should be added to enable dynamic memory growth for each GPU."}
{"number": 88, "change": "def create_checkerboard(h, w, nw):\n\n\n# TODO: Isn't this function duplicated with eye_like?\n-def create_eye_batch(batch_size, eye_size):\n+def create_eye_batch(batch_size, eye_size, device=None, dtype=None):\n\"\"\"Creates a batch of identity matrices of shape Bx3x3\n\"\"\"\n-    return torch.eye(eye_size).view(\n+    return torch.eye(eye_size, device=device, dtype=dtype).view(\n1, eye_size, eye_size).expand(batch_size, -1, -1)\n", "fix_pattern": "In the condition of checking for the presence of a device and dtype in the function arguments, if the pattern of missing arguments is detected, then add the missing arguments 'device=None, dtype=None' to fix the API misuse."}
{"number": 93, "change": "class DNAConv(MessagePassing):\nnum_edges = edge_index.size(1)\n\nedge_index, edge_weight = gcn_norm(edge_index, x.size(self.node_dim),\n-                                           edge_weight, self.improved, x.dtype)\n+                                           edge_weight, dtype=x.dtype)\n\nif self.cached:\nself._cache = (num_edges, edge_index, edge_weight)\n", "fix_pattern": "In the condition of \"if self.cached:\", if \"x.dtype\" is detected, then remove the \"x.dtype\" argument in the \"gcn_norm\" function call to fix the API misuse."}
{"number": 94, "change": "class T5Attention(nn.Module):\nposition_bias = position_bias + mask  # (batch_size, n_heads, seq_length, key_length)\n\nscores += position_bias\n-        attn_weights = F.softmax(scores.float(), dim=-1).type_as(\n+        attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(\nscores\n)  # (batch_size, n_heads, seq_length, key_length)\n-        attn_weights = F.dropout(\n+        attn_weights = nn.functional.dropout(\nattn_weights, p=self.dropout, training=self.training\n)  # (batch_size, n_heads, seq_length, key_length)\n", "fix_pattern": "In the condition of using the softmax function and dropout function from the nn.functional module, if the code is using the softmax and dropout functions from the F module instead, then replace them with the equivalent functions from the nn.functional module to fix the API misuse."}
{"number": 97, "change": "def _get_ort_session_options() -> ort.SessionOptions:\nif not torch.cuda.is_available():\nsess_options.execution_mode = ort.ExecutionMode.ORT_PARALLEL\nsess_options.inter_op_num_threads = 1\n-        sess_options.intra_op_num_threads = max(torch.get_num_threads(), 1)\n+        sess_options.intra_op_num_threads = max(\n+            int(\n+                os.environ.get(\"NEBULLVM_THREADS_PER_MODEL\")\n+                or torch.get_num_threads()\n+            ),\n+            1,\n+        )\nreturn sess_options\n", "fix_pattern": "in the condition of \"if not torch.cuda.is_available()\", if the pattern of setting the \"intra_op_num_threads\" to the maximum of \"NEBULLVM_THREADS_PER_MODEL\" environment variable or the number of threads from \"torch.get_num_threads()\" is detected, then change the code to set the \"intra_op_num_threads\" to the maximum value, ensuring correct API usage."}
{"number": 98, "change": "def save_best_model(model, optimizer, model_loss, best_loss, out_path,\ndef check_update(model, grad_clip, grad_top):\nr'''Check model gradient against unexpected jumps and failures'''\nskip_flag = False\n-    grad_norm = torch.nn.utils.clip_grad_norm(model.parameters(), grad_clip)\n+    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\nif np.isinf(grad_norm):\nprint(\" | > Gradient is INF !!\")\nskip_flag = True\n", "fix_pattern": "In the condition of detecting if the gradient norm is infinite, the code was changed to use the correct API function \"clip_grad_norm_\" instead of \"clip_grad_norm\" to fix the API misuse."}
{"number": 99, "change": "def quaternion_exp_to_log(quaternion: torch.Tensor,\n>>> kornia.quaternion_exp_to_log(quaternion)\ntensor([0., 0., 0.])\n\"\"\"\n-    if not torch.is_tensor(quaternion):\n+    if not isinstance(quaternion, torch.Tensor):\nraise TypeError(\"Input type is not a torch.Tensor. Got {}\".format(\ntype(quaternion)))\n", "fix_pattern": "In the condition of checking if the input is a torch.Tensor, if the pattern of using 'torch.is_tensor' is detected, then the code should be changed to 'isinstance(quaternion, torch.Tensor)' to fix the API misuse."}
{"number": 103, "change": "class ReformerLayer(nn.Module):\n\"\"\"\n# randomize seeds\n# use cuda generator if available\n-        if len(torch.cuda.default_generators) > 0:\n+        if hasattr(torch.cuda, \"default_generators\") and len(torch.cuda.default_generators) > 0:\n# GPU\ndevice_idx = torch.cuda.current_device()\nself.feed_forward_seed = torch.cuda.default_generators[device_idx].seed()\n", "fix_pattern": "in the condition of checking if the attribute exists for \"default_generators\" in the torch.cuda module, if a pattern of checking the length of \"default_generators\" is detected, then the code is changed to add the \"hasattr\" check before checking the length to fix the API misuse."}
{"number": 104, "change": "def get_projective_transform(center: torch.Tensor, angles: torch.Tensor) -> torc\n\n# create rotation matrix\nangle_axis_rad: torch.Tensor = K.deg2rad(angles)\n-    rmat: torch.Tensor = K.angle_axis_to_rotation_matrix(angle_axis_rad)  # Bx3x3\n+    rmat: torch.Tensor = K.angle_axis_to_rotation_matrix(angle_axis_rad) * scales.view(-1, 1, 1)  # Bx3x3\n\n# define matrix to move forth and back to origin\nfrom_origin_mat = torch.eye(4)[None].repeat(rmat.shape[0], 1, 1).type_as(center)  # Bx4x4\n", "fix_pattern": "In the condition of calculating the rotation matrix, if the scales parameter is available, then add the scales.view(-1, 1, 1) code to fix the API misuse in the code."}
{"number": 105, "change": "class FQETorchModel:\nq_values, _ = self.q_model({\"obs\": obs}, [], None)\nif actions is not None:\nactions = torch.tensor(actions, device=self.device, dtype=int)\n-            q_values = torch.gather(q_values, -1, actions.unsqueeze(-1)).squeeze()\n+            q_values = torch.gather(q_values, -1, actions.unsqueeze(-1)).squeeze(-1)\nreturn q_values.detach()\n\ndef estimate_v(\n", "fix_pattern": "In the condition of checking if `actions` is not None, if the pattern of calling `squeeze` without any argument is detected, then change the code to call `squeeze(-1)` to fix the API misuse."}
{"number": 107, "change": "class ModelCatalog:\nmodel_name (str): Name to register the model under.\nmodel_class (type): Python class of the model.\n\"\"\"\n-        if issubclass(model_class, tf.keras.Model):\n-            deprecation_warning(old=\"register_custom_model\", error=False)\n+        if tf is not None:\n+            if issubclass(model_class, tf.keras.Model):\n+                deprecation_warning(old=\"register_custom_model\", error=False)\n_global_registry.register(RLLIB_MODEL, model_name, model_class)\n\n@staticmethod\n", "fix_pattern": "In the condition of checking if TensorFlow is not None, if the pattern of issubclass(model_class, tf.keras.Model) is detected, then add the code to fix the API misuse by registering the model with the global registry."}
{"number": 109, "change": "class DLA(nn.Module):\nif self.drop_rate > 0.:\nx = F.dropout(x, p=self.drop_rate, training=self.training)\nx = self.fc(x)\n-        if not self.global_pool.is_identity():\n-            x = x.flatten(1)  # conv classifier, flatten if pooling isn't pass-through (disabled)\n+        x = self.flatten(x)\nreturn x\n", "fix_pattern": "in the condition of \"if self.drop_rate > 0.:\", if the pattern of \"x = x.flatten(1)\" is detected, then the code \"x = self.flatten(x)\" is added to fix the API misuse."}
{"number": 112, "change": "class Trainer(\n\nresults = self.predict_loop.on_predict_epoch_end()\nself.predict_loop.on_predict_end()\n+\n+        # re-enable grads\n+        torch.set_grad_enabled(True)\n+\nreturn results\n\ndef run_sanity_check(self, ref_model):\n", "fix_pattern": "in the condition of \"missing grad_enabled\", if \"missing torch.set_grad_enabled(True)\" is detected, then add \"torch.set_grad_enabled(True)\" to fix the API misuse."}
{"number": 113, "change": "def filter2d(\ninput = input.view(-1, tmp_kernel.size(0), input.size(-2), input.size(-1))\n\n# convolve the tensor with the kernel.\n-    output = F.conv2d(input, tmp_kernel, groups=tmp_kernel.size(0), padding=0, stride=1)\n+    # NOTE: type(...) to fix getting `torch.bfloat16` type.\n+    # TODO: @johnnv1, fix it through the Augmentation Base.\n+    output = F.conv2d(input, tmp_kernel, groups=tmp_kernel.size(0), padding=0, stride=1).type(input.dtype)\n\nif padding == 'same':\nout = output.view(b, c, h, w)\n", "fix_pattern": "In the condition of padding being 'same', if an API misuse of not properly setting the data type of a tensor is detected, then add .type(input.dtype) to fix the issue."}
{"number": 114, "change": "def remainder(\nres_floored = torch.where(res >= 0, torch.floor(res), torch.ceil(res))\ndiff = res - res_floored\ndiff, x2 = ivy.promote_types_of_inputs(diff, x2)\n-        return torch.mul(diff, x2, out=out)\n+        return torch.mul(diff, x2, out=out).to(x1.dtype)\nreturn torch.remainder(x1, x2, out=out)\n", "fix_pattern": "In the condition of \"return torch.mul(diff, x2, out=out)\", if \"out=out\" is present, then add \".to(x1.dtype)\" to fix the API misuse."}
{"number": 116, "change": "class EpsilonGreedy(Exploration):\ntorch.multinomial(random_valid_action_logits, 1), axis=1)\n# Pick either random or greedy.\naction = torch.where(\n-                torch.empty((batch_size, )).uniform_() < epsilon,\n+                torch.empty(\n+                    (batch_size, )).uniform_().to(self.device) < epsilon,\nrandom_actions, exploit_action)\n\nreturn action, action_logp\n", "fix_pattern": "Fix_pattern: \n\nIn the condition of checking if a random number is less than epsilon, if the torch.empty() function does not have the \"to()\" method called on the result, then add \".to(self.device)\" to fix the API misuse."}
{"number": 118, "change": "class Delta(TorchDistribution):\n\ndef expand(self, batch_shape):\nvalidate_args = self.__dict__.get('_validate_args')\n+        batch_shape = torch.Size(batch_shape)\nv = self.v.expand(batch_shape + self.event_shape)\nlog_density = self.log_density.expand(batch_shape)\nreturn Delta(v, log_density, self.event_dim, validate_args=validate_args)\n", "fix_pattern": "In the condition of checking for the existence of '_validate_args', if a pattern of missing code assignment is detected, then add the code assignment of 'batch_shape = torch.Size(batch_shape)' to fix the API misuse."}
{"number": 119, "change": "def main():\n\npruner = AGP_Pruner(model, configure_list)\nmodel = pruner.compress()\n-\n+    model = model.to(device)\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\nfor epoch in range(10):\npruner.update_epoch(epoch)\nprint('# Epoch {} #'.format(epoch))\ntrain(model, device, train_loader, optimizer)\ntest(model, device, test_loader)\n-    pruner.export_model('model.pth', 'mask.pth', 'model.onnx', [1, 1, 28, 28])\n+    pruner.export_model('model.pth', 'mask.pth', 'model.onnx', [1, 1, 28, 28], device)\n\n\nif __name__ == '__main__':\n", "fix_pattern": "In the condition of the main function, if the pattern of missing device assignment is detected, then the code is changed to add a device assignment to fix the API misuse."}
{"number": 121, "change": "from ray.air.config import ScalingConfig\n\n\ndef mnist_dataset(batch_size: int) -> tf.data.Dataset:\n-    (x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n+    with FileLock(os.path.expanduser(\"~/.mnist_lock\")):\n+        (x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n# The `x` arrays are in uint8 and have values in the [0, 255] range.\n# You need to convert them to float32 with values in the [0, 1] range.\nx_train = x_train / np.float32(255)\n", "fix_pattern": "In the condition of acquiring a file lock, if there is missing code to lock the file, then add the code `with FileLock(os.path.expanduser(\"~/.mnist_lock\"))` to fix the API misuse."}
{"number": 126, "change": "def main_fun(argv, ctx):\ngrads = average_gradients(tower_grads)\n\n# Add a summary to track the learning rate.\n-      summaries.append(tf.scalar_summary('learning_rate', lr))\n+      summaries.append(tf.summary.scalar('learning_rate', lr))\n\n# Add histograms for gradients.\nfor grad, var in grads:\nif grad is not None:\nsummaries.append(\n-              tf.histogram_summary(var.op.name + '/gradients', grad))\n+              tf.summary.histogram(var.op.name + '/gradients', grad))\n\n# Apply the gradients to adjust the shared variables.\napply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n\n# Add histograms for trainable variables.\nfor var in tf.trainable_variables():\n-        summaries.append(tf.histogram_summary(var.op.name, var))\n+        summaries.append(tf.summary.histogram(var.op.name, var))\n\n# Track the moving averages of all trainable variables.\nvariable_averages = tf.train.ExponentialMovingAverage(\n", "fix_pattern": "In the condition of `if grad is not None`, if the pattern `tf.histogram_summary()` is detected, then the code should be changed to `tf.summary.histogram()` to fix the API misuse."}
{"number": 127, "change": "class Trainer(TrainerBase):\n\n@timing.time(\"Trainer.test\")\ndef test(self, test_iter, model, metric_reporter: MetricReporter):\n+        if cuda.CUDA_ENABLED:\n+            model = model.cuda()\n+\nmodel.eval()\nwith torch.no_grad():\ntest_metric = self._run_epoch(\n", "fix_pattern": "in the condition of `CUDA_ENABLED`, if `model.cuda()` is detected, then add the code `model = model.cuda()` to fix the API misuse."}
{"number": 128, "change": "class TrainingArguments:\n@torch_required\ndef _setup_devices(self) -> \"torch.device\":\nlogger.info(\"PyTorch: setting up devices\")\n-        if torch.distributed.is_initialized() and self.local_rank == -1:\n+        if torch.distributed.is_available() and torch.distributed.is_initialized() and self.local_rank == -1:\nlogger.warning(\n\"torch.distributed process group is initialized, but local_rank == -1. \"\n\"In order to use Torch DDP, launch your script with `python -m torch.distributed.launch\"\n", "fix_pattern": "In the condition of checking if Torch DDP is initialized, if the pattern of checking for `torch.distributed.is_initialized()` is detected, then the code should be updated to also check for `torch.distributed.is_available()` to fix the API misuse."}
{"number": 131, "change": "with tf.Graph().as_default():\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"model\")\nif not os.path.exists(checkpoint_dir):\nos.makedirs(checkpoint_dir)\n-        saver = tf.train.Saver(tf.all_variables())\n+        saver = tf.train.Saver(tf.global_variables())\n\n# Write vocabulary\nvocab_processor.save(os.path.join(out_dir, \"vocab\"))\n\n# Initialize all variables\n-        sess.run(tf.initialize_all_variables())\n+        sess.run(tf.global_variables_initializer())\n\ndef train_step(x_batch, y_batch):\n\"\"\"\n", "fix_pattern": "In the condition of checking if a directory exists, if the old method of using \"tf.all_variables()\" is used to define the saver or initializing variables, then the code should be changed to use the new method \"tf.global_variables()\" or \"tf.global_variables_initializer()\" to fix the API misuse."}
{"number": 132, "change": "class CLIPTextTransformer(nn.Module):\nattentions=encoder_outputs.attentions,\n)\n\n-    def _build_causal_attention_mask(self, bsz, seq_len):\n+    def _build_causal_attention_mask(self, bsz, seq_len, dtype):\n# lazily create causal attention mask, with full attention between the vision tokens\n# pytorch uses additive attention mask; fill with -inf\n-        mask = torch.empty(bsz, seq_len, seq_len)\n-        mask.fill_(torch.tensor(float(\"-inf\")))\n+        mask = torch.empty(bsz, seq_len, seq_len, dtype=dtype)\n+        mask.fill_(torch.tensor(torch.finfo(dtype).min))\nmask.triu_(1)  # zero out the lower diagonal\nmask = mask.unsqueeze(1)  # expand mask\nreturn mask\n", "fix_pattern": "In the condition of `_build_causal_attention_mask` function, if the `dtype` parameter is missing, then add `dtype=dtype` to fix the API misuse."}
{"number": 135, "change": "def get_mini_batch(mini_batch_indices, sequences, seq_lengths, cuda=False):\n# get mask for mini-batch\nmini_batch_mask = get_mini_batch_mask(mini_batch, sorted_seq_lengths)\n\n-    # wrap in PyTorch Variables\n-    mini_batch = Variable(torch.Tensor(mini_batch))\n-    mini_batch_reversed = Variable(torch.Tensor(mini_batch_reversed))\n-    mini_batch_mask = Variable(torch.Tensor(mini_batch_mask))\n+    # wrap in PyTorch Tensors\n+    mini_batch = torch.tensor(mini_batch)\n+    mini_batch_reversed = torch.tensor(mini_batch_reversed)\n+    mini_batch_mask = torch.tensor(mini_batch_mask)\n\n# cuda() here because need to cuda() before packing\nif cuda:\n", "fix_pattern": "Fix_pattern: \nIn the condition of \"cuda=True\", if the pattern of using \"Variable(torch.Tensor())\" is detected, then change it to \"torch.tensor()\" to fix the API misuse."}
{"number": 137, "change": "class FlopsProfiler(object):\nstart_time_hook)\n\ndef end_time_hook(module, input, output):\n-                torch.cuda.synchronize()\n+                get_accelerator().synchronize()\nmodule.__duration__ += time.time() - module.__start_time__\n\nif not hasattr(module, \"__end_time_hook_handle__\"):\n", "fix_pattern": "In the condition of \"if not hasattr(module, \"__end_time_hook_handle__\"):\", if the pattern \"torch.cuda.synchronize()\" is detected, then change the code to \"get_accelerator().synchronize()\" to fix the API misuse."}
{"number": 140, "change": "class PaintByExample(DiffusionInpaintModel):\nmask: [H, W, 1] 255 means area to repaint\nreturn: BGR IMAGE\n\"\"\"\n-        set_seed(config.paint_by_example_seed)\n-\noutput = self.model(\nimage=PIL.Image.fromarray(image),\nmask_image=PIL.Image.fromarray(mask[:, :, -1], mode=\"L\"),\nexample_image=config.paint_by_example_example_image,\nnum_inference_steps=config.paint_by_example_steps,\noutput_type='np.array',\n+            generator=torch.manual_seed(config.paint_by_example_seed)\n).images[0]\n\noutput = (output * 255).round().astype(\"uint8\")\n", "fix_pattern": "In the condition of initializing the generator for paint-by-example, if a seed is given in the configuration, then add the generator=torch.manual_seed(config.paint_by_example_seed) parameter to fix the API misuse."}
{"number": 141, "change": "class BigBirdPegasusBlockSparseAttention(nn.Module):\nnum_indices_to_gather = indices.shape[-2] * indices.shape[-1]\nnum_indices_to_pick_from = params.shape[2]\n\n-        indices_shift = (\n-            torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)\n-            // num_indices_to_gather\n-            * num_indices_to_pick_from\n-        )\n+        shift = torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)\n+        indices_shift = torch_int_div(shift, num_indices_to_gather) * num_indices_to_pick_from\n\nflattened_indices = indices.view(-1) + indices_shift\nflattened_params = params.reshape(-1, params.shape[-2], params.shape[-1])\n", "fix_pattern": "In the condition of creating a shifted index for gathering, if the pattern of dividing by the number of indices to gather and then multiplying by the number of indices to pick from is detected, then remove the \"// num_indices_to_gather\" operation and change it to use a separate shift variable to fix the API misuse."}
{"number": 151, "change": "def main(args):\n# CNN will transform a high dimension image into a low dimension 2D tensors for RBF kernel.\n# This kernel accepts inputs are inputs of CNN and gives outputs are covariance matrix of RBF on\n# outputs of CNN.\n-    kernel = gp.kernels.RBF(input_dim=10, lengthscale=torch.ones(10)).warp(iwarping_fn=cnn_fn)\n+    kernel = gp.kernels.Warp(gp.kernels.RBF(input_dim=10, lengthscale=torch.ones(10)),\n+                             iwarping_fn=cnn_fn)\n\n# init inducing points (taken randomly from dataset)\nXu = next(iter(train_loader))[0][:args.num_inducing]\n", "fix_pattern": "In the condition of instantiating the RBF kernel with the `gp.kernels.RBF` class, if the `warp` method is called with the argument `iwarping_fn=cnn_fn`, then replace it with `gp.kernels.Warp(gp.kernels.RBF(...), iwarping_fn=cnn_fn)` to fix the API misuse."}
{"number": 153, "change": "def linspace_helper(start, stop, num, axis=None, *, device):\nelse:\nres = [linspace_method(start, stp, num, device=device) for stp in stop]\nelse:\n-        return linspace_method(start, stop, num, device=device)\n+        return linspace_method(start, stop, num, dtype=torch.float64, device=device)\nres = torch.cat(res, -1).reshape(sos_shape + [num])\nif axis is not None:\nres = torch.transpose(res, axis, -1)\n", "fix_pattern": "in the condition of missing dtype argument, if linspace_method() is called, then add dtype=torch.float64 to fix the API misuse."}
{"number": 159, "change": "class PatchAffineShapeEstimator(nn.Module):\n\"input shape should be must be [Bx1x{}x{}]. \"\n\"Got {}\".format(self.patch_size, self.patch_size, patch.size()))\nself.weighting = self.weighting.to(patch.dtype).to(patch.device)\n-        grads: torch.Tensor = self.gradient(patch)\n+        grads: torch.Tensor = self.gradient(patch) * self.weighting\n# unpack the edges\ngx: torch.Tensor = grads[:, :, 0]\ngy: torch.Tensor = grads[:, :, 1]\n", "fix_pattern": "In the condition of \"when calculating gradients using the 'gradient' function\", if the pattern of \"multiplying the gradients by the 'weighting' tensor\" is detected, then add the \"* self.weighting\" expression to fix the API misuse."}
{"number": 163, "change": "def non_max_suppression(prediction, conf_thres=0.1, iou_thres=0.6, classes=None,\nmerge = False  # use merge-NMS\n\nt = time.time()\n-    output = [torch.zeros(0, 6)] * prediction.shape[0]\n+    output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\nfor xi, x in enumerate(prediction):  # image index, image inference\n# Apply constraints\n# x[((x[..., 2:4] < min_wh) | (x[..., 2:4] > max_wh)).any(1), 4] = 0  # width-height\n", "fix_pattern": "In the condition of initializing the output tensor, if the shape of the tensor is directly passed as an argument, then add parentheses around the shape argument to fix the API misuse."}
{"number": 166, "change": "class DartsTrainer(BaseOneShotTrainer):\np += e * d\n\n_, loss = self._logits_and_loss(trn_X, trn_y)\n-            dalphas.append(torch.autograd.grad(loss, [c.alpha for c in self.nas_modules]))\n+            dalphas.append(torch.autograd.grad(loss, [c.alpha for _, c in self.nas_modules]))\n\ndalpha_pos, dalpha_neg = dalphas  # dalpha { L_trn(w+) }, # dalpha { L_trn(w-) }\nhessian = [(p - n) / (2. * eps) for p, n in zip(dalpha_pos, dalpha_neg)]\n", "fix_pattern": "In the condition of passing a list comprehension to the torch.autograd.grad() function, if the variable before the comma is not needed, then remove it to fix the API misuse."}
{"number": 167, "change": "def subtract(\nout: Optional[Union[tf.Tensor, tf.Variable]] = None,\n) -> Union[tf.Tensor, tf.Variable]:\nx1, x2 = ivy.promote_types_of_inputs(x1, x2)\n-    return tf.subtract(x1, x2)\n+    return tf.experimental.numpy.subtract(x1, x2)\n\n\ndef tan(\n", "fix_pattern": "in the condition of \"calling the tf.subtract() function\", if \"importing tf.experimental.numpy\" is detected, then \"change the code from tf.subtract() to tf.experimental.numpy.subtract() to fix the API misuse.\""}
{"number": 169, "change": "class XDropout(torch.autograd.Function):\n# Once https://github.com/pytorch/pytorch/issues/78391 is fixed, do something like:\n# if opset_version < 12:\n#   return torch.onnx.symbolic_opset9.dropout(g, input, dropout_p, train)\n-        return torch.onnx.symbolic_opset12.dropout(g, input, dropout_p, train)\n+        return symbolic_opset12.dropout(g, input, dropout_p, train)\n\n\n# Copied from transformers.models.deberta.modeling_deberta.StableDropout\n", "fix_pattern": "In the condition of \"opset_version < 12\", if the pattern \"torch.onnx.symbolic_opset12.dropout(g, input, dropout_p, train)\" is detected, then remove \"torch.\" from the code to fix the API misuse."}
{"number": 177, "change": "class IvyModule(ivy.Module):\nif ivy.array_mode():\na, kw = ivy.args_to_native(*a, **kw)\n# noinspection PyUnresolvedReferences\n-        params_hk = self._native_module.init(ivy.functional.core.random.RNG, *a, **kw)\n+        params_hk = self._native_module.init(ivy.random.RNG, *a, **kw)\nparams_dict = _hk_flat_map_to_dict(params_hk)\nself._hk_params = ivy.Container(params_dict)\nparam_iterator = self._hk_params.to_iterator()\n", "fix_pattern": "in the condition of `ivy.array_mode()`, if `ivy.functional.core.random.RNG` is detected, then change the `self._native_module.init` code to `ivy.random.RNG` to fix the API misuse."}
{"number": 181, "change": "class TensorFlowEstimator(BaseEstimator):\nraise NotFittedError()\npredict_data_feeder = setup_predict_data_feeder(X)\npreds = []\n-        dropouts = tf.get_collection(DROPOUTS)\n-        feed_dict = {prob: 0.0 for prob in dropouts}\n+        dropouts = self._graph.get_collection(DROPOUTS)\n+        feed_dict = {prob: 1.0 for prob in dropouts}\nfor data in predict_data_feeder:\nfeed_dict[self._inp] = data\npreds.append(self._session.run(\n", "fix_pattern": "In the condition of setting up the feed_dict dictionary, if the pattern of setting all dropout probabilities to 0.0 is detected, then the code is changed to set all dropout probabilities to 1.0 to fix the API misuse."}
{"number": 182, "change": "class GradientsTest(tf.test.TestCase):\nself.assertAllClose(eager_result, function_result)\nbackprop_result, numeric_result = tf.test.compute_gradient(\nm, [inp], delta=1e-3)\n-    self.assertAllClose(numeric_result, backprop_result, rtol=1e-2)\n+    self.assertAllClose(numeric_result, backprop_result, atol=1e-3)\nself.assertAllClose(tf.reshape(numeric_result, [-1]),\n-                        tf.reshape(eager_result, [-1]), rtol=1e-2)\n+                        tf.reshape(eager_result, [-1]), atol=1e-3)\n\ndef testEmbeddingLookupGradientsHaveKnownShape(self):\n", "fix_pattern": "In the condition of `testEmbeddingLookupGradientsHaveKnownShape`, if an API misuse of `self.assertAllClose` is detected with the `rtol` argument, then the code should be changed to use the `atol` argument to fix the issue."}
{"number": 187, "change": "class Pix2PixModel(BaseModel):\ndef backward_D(self):\n# Fake\n# stop backprop to the generator by detaching fake_B\n-        fake_AB = self.fake_AB_pool.query(torch.cat((self.real_A, self.fake_B), 1))\n+        fake_AB = self.fake_AB_pool.query(torch.cat((self.real_A, self.fake_B), 1).data)\npred_fake = self.netD.forward(fake_AB.detach())\nself.loss_D_fake = self.criterionGAN(pred_fake, False)\n\n# Real\nreal_AB = torch.cat((self.real_A, self.real_B), 1)\npred_real = self.netD.forward(real_AB)\n-        self.loss_D_real = self.criterionGAN(self.pred_real, True)\n+        self.loss_D_real = self.criterionGAN(pred_real, True)\n\n# Combined loss\nself.loss_D = (self.loss_D_fake + self.loss_D_real) * 0.5\n", "fix_pattern": "Fix_pattern: \nIn the condition of accessing the data of a tensor, if the pattern of using <tensor>.data is detected, then remove the .data to fix the API misuse."}
{"number": 193, "change": "class tensorflow_extractor(base_extractor):\nwriter.close()\nsess.run(init)\nsaver = tf.train.Saver()\n+            tf.train.export_meta_graph(\"kit.meta\", as_text=True)\nsaver.restore(sess, path + cls.architecture_map[architecture]['filename'])\nsave_path = saver.save(sess, path + \"imagenet_{}.ckpt\".format(architecture))\nprint(\"Model saved in file: %s\" % save_path)\n", "fix_pattern": "in the condition of adding TensorFlow extraction meta graph, if the pattern to export meta graph is detected, then add the code to export the meta graph using the tf.train.export_meta_graph function to fix the API misuse."}
{"number": 194, "change": "def test_auto_diagonal_gaussians(auto_class, Elbo):\nguide = auto_class(model, rank=1)\nelse:\nguide = auto_class(model)\n-    adam = optim.Adam({\"lr\": .001, \"betas\": (0.95, 0.999)})\n+    adam = optim.ClippedAdam({\"lr\": .01, \"betas\": (0.95, 0.999),\n+                              \"lrd\": 0.1 ** (1 / n_steps)})\nsvi = SVI(model, guide, adam, loss=Elbo())\n\nfor k in range(n_steps):\n", "fix_pattern": "In the condition of the else statement, if an incorrect instantiation of the optim.Adam class is detected, then change it to an instantiation of the optim.ClippedAdam class to fix the API misuse."}
{"number": 195, "change": "def test_frontend_backward_multi_channel(train, use_wpe, use_beamformer):\nfrontend.train()\nelse:\nfrontend.eval()\n+    torch.random.manual_seed(14)\nx = torch.randn(2, 1000, 2, requires_grad=True)\nx_lengths = torch.LongTensor([1000, 980])\ny, y_lengths = frontend(x, x_lengths)\n", "fix_pattern": "In the condition of \"use_beamformer\", if the pattern \"torch.random.manual_seed(14)\" is detected, then add the code to set the random seed to 14 to fix the API misuse."}
{"number": 197, "change": "class TestGradientScaling(unittest.TestCase):\noptimizer = FP16Optimizer.build_optimizer(self.namespace_dls, params)\n\nself.run_iter(model, params, optimizer)\n-        self.assertTrue(torch.all(optimizer.fp32_params.eq(torch.tensor([3.1000, 5.1000], device='cuda:0', requires_grad=True))))\n+        self.assertTrue(all(\n+            torch.all(fp32_params.eq(torch.tensor([3.1000, 5.1000], device='cuda:0', requires_grad=True)))\n+            for fp32_params in optimizer.fp32_params.values()\n+        ))\n\ndef test_memory_efficient(self):\nmodel = copy.deepcopy(self.model)\n", "fix_pattern": "In the condition of iterating through `optimizer.fp32_params.values()`, if the pattern of comparing equality between each `fp32_params` and a tensor is detected, then change the code from `self.assertTrue(torch.all(optimizer.fp32_params.eq(torch.tensor([3.1000, 5.1000], device='cuda:0', requires_grad=True))))` to `self.assertTrue(all(torch.all(fp32_params.eq(torch.tensor([3.1000, 5.1000], device='cuda:0', requires_grad=True))) for fp32_params in optimizer.fp32_params.values()))` to fix the API misuse."}
{"number": 198, "change": "class TestLuvToRgb(BaseTester):\n[0.06325728, 0.78878325, 0.74280596, 0.99514300, 0.47176042]]\n]], device=device, dtype=dtype)\n\n-        assert_allclose(kornia.color.luv_to_rgb(data), expected)\n+        assert_allclose(kornia.color.luv_to_rgb(data), expected, rtol=1e-4, atol=1e-4)\n\ndef test_forth_and_back(self, device, dtype):\ndata = torch.rand(3, 4, 5, device=device, dtype=dtype)\n", "fix_pattern": "In the condition of `assert_allclose`, if there is a missing tolerance argument, then add the tolerance arguments `rtol` and `atol` with appropriate values to fix the API misuse."}
{"number": 199, "change": "class EulerSamplingTest(tf.test.TestCase, parameterized.TestCase):\ntimes=times,\nnum_samples=num_samples,\ninitial_state=x0,\n-            random_type=tff.math.random.RandomType.SOBOL,\n+            random_type=tff.math.random.RandomType.HALTON,\ntime_step=0.01,\n-            seed=12134))\n+            seed=12134,\n+            skip=100,\n+            dtype=tf.float32))\n\n-    self.assertAllClose(paths.shape, (num_samples, 5, 2), atol=0)\n+    self.assertAllClose(paths.shape, (num_samples, 3, 2), atol=0)\nmeans = np.mean(paths, axis=0)\ntimes = np.reshape(times, [-1, 1])\nexpected_means = x0 + (2.0 / 3.0) * mu * np.power(times, 1.5)\n", "fix_pattern": "Fix_pattern: \n\nIn the condition of initializing the random generator, if the 'SOBOL' random type is detected, then change it to 'HALTON' to fix the API misuse."}
{"number": 203, "change": "class BatchNorm(TransformModule):\nif self.training:\nmean, var = y.mean(0), y.var(0)\n\n-            # NOTE: The momentum variable agrees with the definition in e.g. `torch.nn.BatchNorm1d`\n-            self.moving_mean.mul_(1 - self.momentum).add_(mean * self.momentum)\n-            self.moving_variance.mul_(1 - self.momentum).add_(var * self.momentum)\n+            with torch.no_grad():\n+                # NOTE: The momentum variable agrees with the definition in e.g. `torch.nn.BatchNorm1d`\n+                self.moving_mean.mul_(1 - self.momentum).add_(mean * self.momentum)\n+                self.moving_variance.mul_(1 - self.momentum).add_(var * self.momentum)\n\n# During test time, use smoothed averages rather than the sample ones\nelse:\n", "fix_pattern": "In the condition of \"self.training\", if \"torch.no_grad()\" is not present, then add \"with torch.no_grad()\" before the code to fix the API misuse."}
{"number": 210, "change": "def train_model(params: Params, serialization_dir: str) -> Model:\n\nlogger.info(\"Creating a vocabulary using %s data.\", \", \".join(datasets_for_vocab_creation))\nvocab = Vocabulary.from_params(params.pop(\"vocabulary\", {}),\n-                                   Dataset([instance for key, dataset in all_datasets.items()\n-                                            for instance in dataset.instances\n-                                            if key in datasets_for_vocab_creation]))\n+                                   (instance for key, dataset in all_datasets.items()\n+                                    for instance in dataset\n+                                    if key in datasets_for_vocab_creation))\nvocab.save_to_files(os.path.join(serialization_dir, \"vocabulary\"))\n\nmodel = Model.from_params(vocab, params.pop('model'))\n", "fix_pattern": "In the condition of iterating over instances in the dataset, if the pattern of iterating over dataset.items() is detected, then change the code to iterate over dataset directly. This fix is done to fix the API misuse."}
{"number": 214, "change": "class KerasBackend(AbstractBackend):\nreturn keras\n\ndef einsum(self, pattern, *x):\n-        return self.tf.einsum(pattern, *x)\n+        return self.tf.vectorized_map(\n+            functools.partial(self.tf.einsum, pattern),\n+            *x\n+        )\n\n\nclass OneFlowBackend(AbstractBackend):\n", "fix_pattern": "In the condition of using the `einsum` method, if the `vectorized_map` function is detected, then the code is changed from `self.tf.einsum(pattern, *x)` to `self.tf.vectorized_map(functools.partial(self.tf.einsum, pattern), *x)` to fix the API misuse."}
{"number": 215, "change": "def run_modelmerger(primary_model_name, secondary_model_name, teritary_model_nam\nt2 = theta_2.get(key, torch.zeros_like(theta_1[key]))\ntheta_1[key] = theta_func1(theta_1[key], t2)\nelse:\n-                    theta_1[key] = 0\n+                    theta_1[key] = torch.zeros_like(theta_1[key])\ndel theta_2, teritary_model\n\nfor key in tqdm.tqdm(theta_0.keys()):\n", "fix_pattern": "In the condition of `else`, if `theta_1[key]` is set to 0, then change the code to set `theta_1[key]` to `torch.zeros_like(theta_1[key])` to fix the API misuse."}
{"number": 216, "change": "class DefaultClassifier(Classifier):\n\ndef _calculate_loss(self, scores, labels):\n\n-        if len(labels) == 0: return torch.tensor(0., requires_grad=True), 1\n+        if len(labels) == 0: return torch.tensor(0., requires_grad=True, device=flair.device), 1\n\nif self.multi_label:\nlabels = torch.tensor([[1 if l in all_labels_for_point else 0 for l in self.label_dictionary.get_items()]\n", "fix_pattern": "In the condition of checking if the length of labels is 0, if the API misuse of the `requires_grad` parameter is detected, the code is modified to include the `device` parameter to fix the issue."}
{"number": 217, "change": "class EmbeddingLayer(nn.Module):\ntorch.empty(weight_shape[0],\nweight_shape[1],\ndtype=dtype,\n-                        device=torch.cuda.current_device()))\n+                        device=get_accelerator().current_device_name()))\n\ndef forward(self, input):\nreturn F.embedding(input, self.weight)\n", "fix_pattern": "Fix_pattern: In the condition of API misuse, if the code contains an incorrect method call or argument, then change the code to the correct method call or argument to fix the issue."}
{"number": 218, "change": "class MultiActionDistribution(ActionDistribution):\n\ndef logp(self, x):\n\"\"\"The log-likelihood of the action distribution.\"\"\"\n-        split_list = self.reshaper.split_tensor(x)\n+        split_list = tf.split(x, len(self.input_lens), axis=1)\nfor i, distribution in enumerate(self.child_distributions):\n# Remove extra categorical dimension\nif isinstance(distribution, Categorical):\n", "fix_pattern": "In the condition of checking if the distribution is an instance of Categorical, the fix pattern is to change the code from using the \"reshaper.split_tensor(x)\" to using \"tf.split(x, len(self.input_lens), axis=1)\" to fix the API misuse."}
{"number": 219, "change": "class CategoricalOneHotPolicy(StochasticPolicy):\ndef __init__(self, network, session, state, random, action_count=1, scope='policy'):\nwith tf.variable_scope(scope):\naction_layer = linear(layer_input=network.output, config={'num_outputs': action_count}, scope='outputs')\n+            action_layer = tf.reshape(action_layer, [-1, action_count])\n+\ndistribution = tf.nn.softmax(action_layer)\nsample = tf.multinomial(distribution, 1)\n", "fix_pattern": "in the condition of tf.variable_scope(scope), if the API misuse of not reshaping the action_layer variable is detected, then add the code \"action_layer = tf.reshape(action_layer, [-1, action_count])\" to fix the issue."}
{"number": 221, "change": "class SingleRoIExtractor(nn.Module):\nout_size = self.roi_layers[0].out_size\nnum_levels = len(feats)\ntarget_lvls = self.map_roi_levels(rois, num_levels)\n-        roi_feats = torch.cuda.FloatTensor(rois.size()[0], self.out_channels,\n-                                           out_size, out_size).fill_(0)\n+        roi_feats = feats[0].new_zeros(rois.size()[0], self.out_channels,\n+                                       out_size, out_size)\nfor i in range(num_levels):\ninds = target_lvls == i\nif inds.any():\n", "fix_pattern": "In the condition of 'inds.any()', if 'torch.cuda.FloatTensor().fill_(0)' is detected, then change the code to 'feats[0].new_zeros()' to fix the API misuse."}
{"number": 223, "change": "class MobileNetV3LargeEncoder(MobileNetV3):\n)\n\nif pretrained:\n-            self.load_state_dict(load_state_dict_from_url(\n+            self.load_state_dict(torch.hub.load_state_dict_from_url(\n'https://download.pytorch.org/models/mobilenet_v3_large-8738ca79.pth'))\n\ndel self.avgpool\n", "fix_pattern": "In the condition of loading a pretrained model, if an unsupported function for loading the state dictionary is detected, then add the correct function call to fix the API misuse."}
{"number": 224, "change": "def make_non_pad_mask(lengths):\n\"\"\"\nbs = int(len(lengths))\nmaxlen = int(max(lengths))\n-    mask = torch.zeros(bs, maxlen).byte()\n+    mask = torch.zeros(bs, maxlen, dtype=torch.uint8)\nfor i, l in enumerate(lengths):\nmask[i, :l] = 1\n", "fix_pattern": "In the condition of creating a torch tensor with zeros, if the API misuse of using the 'byte' type is detected, then the code should be changed to use the 'dtype=torch.uint8' type to fix the issue."}
{"number": 229, "change": "class TFXGLMPreTrainedModel(TFPreTrainedModel):\n@tf.function(\ninput_signature=[\n{\n-                \"input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"input_ids\"),\n-                \"attention_mask\": tf.TensorSpec((None, None), tf.int32, name=\"attention_mask\"),\n+                \"input_ids\": tf.TensorSpec((None, None), tf.int64, name=\"input_ids\"),\n+                \"attention_mask\": tf.TensorSpec((None, None), tf.int64, name=\"attention_mask\"),\n}\n]\n)\n", "fix_pattern": "in the condition of changing the data type of the input tensors from tf.int32 to tf.int64, if the pattern of \"input_ids\" and \"attention_mask\" is detected, then change the code from tf.TensorSpec((None, None), tf.int32) to tf.TensorSpec((None, None), tf.int64) to fix the API misuse."}
{"number": 231, "change": "class SpeedyResNet:\nnn.Linear(512, num_classes, bias=False)\n]\n\n-  # note, pytorch just uses https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html instead of logsoftmax\n-  def __call__(self, x): return x.sequential(self.net).logsoftmax()\n+  # note, pytorch just uses https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html instead of log_softmax\n+  def __call__(self, x): return x.sequential(self.net).log_softmax()\n\nfrom extra.jit import TinyJit\n@TinyJit\n", "fix_pattern": "In the condition of using the `logsoftmax()` function from PyTorch, if the old function name `log_softmax()` is detected, then change it to `logsoftmax()` to fix the API misuse."}
{"number": 234, "change": "class EulerAncestralDiscreteScheduler(SchedulerMixin, ConfigMixin):\n\nprev_sample = sample + derivative * dt\n\n-        device = model_output.device if torch.is_tensor(model_output) else torch.device(\"cpu\")\n+        device = model_output.device\nif device.type == \"mps\":\n# randn does not work reproducibly on mps\nnoise = torch.randn(model_output.shape, dtype=model_output.dtype, device=\"cpu\", generator=generator).to(\n", "fix_pattern": "In the condition of checking the device type, if the pattern of inappropriate code usage is detected, then remove the unnecessary argument from the code to fix the API misuse."}
{"number": 236, "change": "def _create_fc(num_features, num_classes, use_conv=False):\nelif use_conv:\nfc = nn.Conv2d(num_features, num_classes, 1, bias=True)\nelse:\n-        # NOTE: using my Linear wrapper that fixes AMP + torchscript casting issue\n-        fc = Linear(num_features, num_classes, bias=True)\n+        fc = nn.Linear(num_features, num_classes, bias=True)\nreturn fc\n", "fix_pattern": "in the condition of `use_conv`, if the pattern `Linear` is detected, then change the `Linear` code to `nn.Linear` to fix the API misuse."}
{"number": 241, "change": "class GenerationMixin:\ncontinue  # don't waste resources running the code we don't need\n\nnext_token_logits = outputs.logits[:, -1, :]\n-\n-            # hack: adjust tokens for Marian. For Marian we have to make sure that the `pad_token_id`\n-            # cannot be generated both before and after the `nn.functional.log_softmax` operation.\n-            next_token_logits = outputs.logits[:, -1, :]\n# hack: adjust tokens for Marian. For Marian we have to make sure that the `pad_token_id`\n# cannot be generated both before and after the `nn.functional.log_softmax` operation.\nnext_token_logits = self.adjust_logits_during_generation(next_token_logits, cur_len=cur_len)\n", "fix_pattern": "In the condition of \"if 'hack: adjust tokens for Marian' is detected\", then remove the duplicated assignment of 'next_token_logits' and fix the API misuse by calling the 'adjust_logits_during_generation' method."}
{"number": 244, "change": "def test_runway_sd_1_5_negative_prompt(sd_device, strategy, sampler):\ndef test_cv2(strategy, cv2_flag, cv2_radius):\nmodel = ModelManager(\nname=\"cv2\",\n-        device=device,\n+        device=torch.device(device),\n)\ncfg = get_config(strategy, cv2_flag=cv2_flag, cv2_radius=cv2_radius)\nassert_equal(\n", "fix_pattern": "In the condition of initializing a ModelManager, if the 'device' parameter is not of type torch.device, then the fix is to add 'torch.device()' around the 'device' parameter to fix the API misuse."}
{"number": 245, "change": "class Model(ModelDesc):\nsummary.add_moving_summary(self.cost)\n\ndef _get_optimizer(self):\n-        lr = tf.get_variable('learning_rate', initializer=1e-3, trainable=False)\n+        lr = tf.get_variable('learning_rate', initializer=self.learning_rate, trainable=False)\nopt = tf.train.AdamOptimizer(lr, epsilon=1e-3)\nreturn optimizer.apply_grad_processors(\nopt, [gradproc.GlobalNormClip(10), gradproc.SummaryGradient()])\n", "fix_pattern": "In the condition of initializing the learning rate, if the usage of a hardcoded value is detected, then change the code to use the assigned value of self.learning_rate to fix the API misuse."}
{"number": 246, "change": "class Network(object):\nweights = self.make_var('weights', shape=[dim, num_out])\nbiases = self.make_var('biases', [num_out])\nop = tf.nn.relu_layer if relu else tf.nn.xw_plus_b\n-            fc = op(feed_in, weights, biases, name=scope.name)\n+            #fc = op(feed_in, weights, biases, name=scope.name)\n+            fc = op(feed_in, weights, biases, name=name)\nreturn fc\n", "fix_pattern": "In the condition of tf.nn.relu_layer or tf.nn.xw_plus_b being used, if the variable 'name' is specified instead of 'scope.name', the code is changed to fix the API misuse."}
{"number": 250, "change": "def train_cnn_and_compute_accuracy(params, steps, train_images, train_labels, va\n# Do the training and evaluation.\nwith tf.Session() as sess:\n# Initialize the network weights.\n-    sess.run(tf.initialize_all_variables())\n+    sess.run(tf.global_variables_initializer())\nfor i in range(1, steps + 1):\n# Fetch the next batch of data.\nimage_batch = get_batch(train_images, i, batch_size)\n", "fix_pattern": "In the condition of \"initializing network weights using tf.initialize_all_variables()\", if \"tf.global_variables_initializer()\" is detected, then change the code to \"sess.run(tf.global_variables_initializer())\" to fix the API misuse."}
{"number": 252, "change": "class DeepSpeedDataLoader(object):\nelse:\nif data_sampler is None:\ndata_sampler = RandomSampler(dataset)\n-                device_count = torch.cuda.device_count()\n+                device_count = get_accelerator().device_count()\nbatch_size *= device_count\n\nif num_local_io_workers is None:\n", "fix_pattern": "In the condition of \"if data_sampler is None\", if the pattern of using the variable \"device_count\" is detected, then change the code from \"device_count = torch.cuda.device_count()\" to \"device_count = get_accelerator().device_count()\" to fix the API misuse."}
{"number": 257, "change": "def asin(x: Union[tf.Tensor, tf.Variable]) -> Union[tf.Tensor, tf.Variable]:\nreturn tf.asin(x)\n\n\n-def asinh(\n-        x: Union[tf.Tensor, tf.Variable]\n-) -> Union[tf.Tensor, tf.Variable]:\n-    x = tf.cast(x, tf.float32)\n+def asinh(x: Union[tf.Tensor, tf.Variable]) -> Union[tf.Tensor, tf.Variable]:\nreturn tf.asinh(x)\n", "fix_pattern": "In the condition of \"having an unnecessary type conversion\", if the pattern of type conversion is detected, then remove the unnecessary type conversion code to fix the API misuse."}
{"number": 259, "change": "class RagTokenForGeneration(RagPreTrainedModel):\nn_docs = n_docs if n_docs is not None else self.config.n_docs\n\n# RAG-token marginalization\n-        seq_logprobs = torch.nn.functional.log_softmax(seq_logits, dim=-1).view(\n+        seq_logprobs = nn.functional.log_softmax(seq_logits, dim=-1).view(\nseq_logits.shape[0] // n_docs, n_docs, -1, seq_logits.size(-1)\n)\ndoc_logprobs = torch.log_softmax(doc_scores, dim=1)\n", "fix_pattern": "In the condition of using a Torch module, if the code contains the incorrect module name \"torch.nn.functional\", then change it to the correct module name \"nn.functional\" to fix the API misuse."}
{"number": 266, "change": "class TestStackedSelfAttention(AllenNlpTestCase):\nfeedforward_hidden_dim=5,\nnum_layers=3,\nnum_attention_heads=3)\n-        inputs = Variable(torch.randn([3, 5, 9]))\n+        inputs = torch.randn([3, 5, 9])\nencoder_output = encoder(inputs, None)\nassert list(encoder_output.size()) == [3, 5, 12]\n", "fix_pattern": "In the condition that a `Variable` is used to wrap a tensor, if the `Variable` is unnecessary and causing code to break, remove the `Variable` and use the tensor directly to fix the API misuse."}
{"number": 267, "change": "def qr(A: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:  # pragma: no cove\n\"\"\"\nLike torch.linalg.qr.\n\"\"\"\n-    if hasattr(torch.linalg, \"qr\"):\n+    if hasattr(torch, \"linalg\") and hasattr(torch.linalg, \"qr\"):\n# PyTorch version >= 1.9\nreturn torch.linalg.qr(A)\nreturn torch.qr(A)\n", "fix_pattern": "in the condition of `hasattr(torch, \"linalg\") and hasattr(torch.linalg, \"qr\")`, if `hasattr(torch.linalg, \"qr\")` is detected, then remove `torch.` and change it to `torch.linalg.qr(A)` to fix the API misuse."}
{"number": 268, "change": "def prepare_bart_inputs_dict(\nif decoder_attention_mask is None:\ndecoder_attention_mask = decoder_input_ids.ne(config.pad_token_id)\nif head_mask is None:\n-        head_mask = torch.ones(config.encoder_layers, config.encoder_attention_heads)\n+        head_mask = torch.ones(config.encoder_layers, config.encoder_attention_heads, device=torch_device)\nif decoder_head_mask is None:\n-        decoder_head_mask = torch.ones(config.decoder_layers, config.decoder_attention_heads)\n+        decoder_head_mask = torch.ones(config.decoder_layers, config.decoder_attention_heads, device=torch_device)\nreturn {\n\"input_ids\": input_ids,\n\"decoder_input_ids\": decoder_input_ids,\n", "fix_pattern": "Fix_pattern: \nIn the condition of \"if head_mask is None\", if a pattern of missing device assignment for a tensor is detected, then add the device assignment to fix the API misuse."}
{"number": 269, "change": "class PNDMSchedulerTest(SchedulerCommonTest):\nscheduler_config = self.get_scheduler_config(steps_offset=1)\nscheduler = scheduler_class(**scheduler_config)\nscheduler.set_timesteps(10)\n-        assert np.equal(\n+        assert torch.equal(\nscheduler.timesteps,\n-            np.array([901, 851, 851, 801, 801, 751, 751, 701, 701, 651, 651, 601, 601, 501, 401, 301, 201, 101, 1]),\n-        ).all()\n+            torch.LongTensor(\n+                [901, 851, 851, 801, 801, 751, 751, 701, 701, 651, 651, 601, 601, 501, 401, 301, 201, 101, 1]\n+            ),\n+        )\n\ndef test_betas(self):\nfor beta_start, beta_end in zip([0.0001, 0.001], [0.002, 0.02]):\n", "fix_pattern": "In the condition of \"assert np.equal(scheduler.timesteps, np.array([901, 851, 851, 801, 801, 751, 751, 701, 701, 651, 651, 601, 601, 501, 401, 301, 201, 101, 1]),).all()\", if the pattern \"np.equal\" is detected, then change the code to \"torch.equal\" to fix the API misuse."}
{"number": 272, "change": "class GradTTS(DiffusionPipeline):\nmu_y = mu_y.transpose(1, 2)\n\n# Sample latent representation from terminal distribution N(mu_y, I)\n-        z = mu_y + torch.randn_like(mu_y, device=mu_y.device) / temperature\n+        z = mu_y + torch.randn(mu_y.shape, device=mu_y.device, generator=generator) / temperature\n\nxt = z * y_mask\nh = 1.0 / num_inference_steps\n", "fix_pattern": "Fix_pattern: \nin the condition of using `torch.randn_like`, if `torch.randn` is used instead, then add `mu_y.shape` and `generator` arguments to fix the API misuse."}
{"number": 273, "change": "class NanDetector:\ngradients = {}\nfor name, param in self.named_parameters:\nif param.grad is not None:\n-                grad_norm = torch.norm(param.grad.data, p=2, dtype=torch.float32)\n+                grad_norm = torch.norm(param.grad.data.float(), p=2)\nnorm[name] = grad_norm.item()\nif torch.isnan(grad_norm).any() or torch.isinf(grad_norm).any():\ngradients[name] = param.grad.data\n", "fix_pattern": "Fix_pattern: In the condition of checking for NaN or infinite values using torch.isnan() or torch.isinf(), if the gradient norm calculation is not explicitly converted to a float using the .float() method, then add the .float() method to fix the API misuse."}
{"number": 277, "change": "def fpn_map_rois_to_levels(boxes):\nBe careful that the returned tensor could be empty.\n\"\"\"\nsqrtarea = tf.sqrt(tf_area(boxes))\n-    level = tf.to_int32(tf.floor(\n-        4 + tf.log(sqrtarea * (1. / 224) + 1e-6) * (1.0 / np.log(2))))\n+    level = tf.cast(tf.floor(\n+        4 + tf.log(sqrtarea * (1. / 224) + 1e-6) * (1.0 / np.log(2))), tf.int32)\n\n# RoI levels range from 2~5 (not 6)\nlevel_ids = [\n", "fix_pattern": "In the condition of using TensorFlow's tf.cast() function, if the returned value needs to be converted to an integer type, the tf.to_int32() function should be replaced with tf.cast() using tf.int32 as the target type, to fix the API misuse."}
{"number": 278, "change": "def mesh_laplacian_smoothing(meshes, method: str = \"uniform\"):\nelif method == \"cot\":\nloss = L.mm(verts_packed) * norm_w - verts_packed\nelif method == \"cotcurv\":\n-        loss = (L.mm(verts_packed) - verts_packed) * norm_w\n+        loss = (L.mm(verts_packed) - L_sum * verts_packed) * norm_w\nloss = loss.norm(dim=1)\n\nloss = loss * weights\n", "fix_pattern": "In the condition of `method == \"cotcurv\"`, if the pattern `(L.mm(verts_packed) - verts_packed)` is detected, then change the code to `(L.mm(verts_packed) - L_sum * verts_packed)` to fix the API misuse."}
{"number": 281, "change": "def test_hub_oneshot(space_type, strategy_type):\nNDS_SPACES = ['amoeba', 'darts', 'pnas', 'enas', 'nasnet']\nif strategy_type == 'proxyless':\nif 'width' in space_type or 'depth' in space_type or \\\n-                any(space_type.startswith(prefix) for prefix in NDS_SPACES + ['proxylessnas', 'mobilenetv3']):\n+                any(space_type.startswith(prefix) for prefix in NDS_SPACES + ['proxylessnas', 'mobilenetv3', 'autoformer']):\npytest.skip('The space has used unsupported APIs.')\nif strategy_type in ['darts', 'gumbel'] and space_type == 'mobilenetv3':\npytest.skip('Skip as it consumes too much memory.')\n", "fix_pattern": "In the condition of \"if 'width' in space_type or 'depth' in space_type or any(space_type.startswith(prefix) for prefix in NDS_SPACES + ['proxylessnas', 'mobilenetv3'])\", if the pattern 'autoformer' is detected, then add 'autoformer' to the list of prefixes to fix the API misuse."}
{"number": 282, "change": "class GCNConv(MessagePassing):\nx = torch.matmul(x, self.weight)\n\nif not self.cached or self.cached_result is None:\n-            edge_index, norm = GCNConv.norm(edge_index,\n-                                            x.size(0), edge_weight,\n+            edge_index, norm = GCNConv.norm(edge_index, x.size(0), edge_weight,\nself.improved, x.dtype)\nself.cached_result = edge_index, norm\n", "fix_pattern": "In the condition of \"if not self.cached or self.cached_result is None\", if the pattern \"(remove/add/change)\" is detected, then the \"self.cached_result = edge_index, norm\" code should be removed/added/changed to fix the API misuse."}
{"number": 283, "change": "class CTC(torch.nn.Module):\nif self.ctc_type == \"builtin\":\nolens = to_device(ys_hat, torch.LongTensor([len(s) for s in ys]))\nhlens = hlens.long()\n+            ys_pad = torch.cat(ys)  # without this the code breaks for asr_mix\nself.loss = self.loss_fn(ys_hat, ys_pad, hlens, olens)\nelse:\nself.loss = None\n", "fix_pattern": "in the condition of \"self.ctc_type == \"builtin\", if \"ys_pad\" is not concatenated, then add the code \"ys_pad = torch.cat(ys)\" to fix the API misuse."}
{"number": 296, "change": "class DeformableDetrModelIntegrationTests(unittest.TestCase):\nresults = feature_extractor.post_process_object_detection(\noutputs, threshold=0.3, target_sizes=[image.size[::-1]]\n)[0]\n-        expected_scores = torch.tensor([0.7999, 0.7894, 0.6331, 0.4720, 0.4382])\n+        expected_scores = torch.tensor([0.7999, 0.7894, 0.6331, 0.4720, 0.4382]).to(torch_device)\nexpected_labels = [17, 17, 75, 75, 63]\n-        expected_slice_boxes = torch.tensor([16.5028, 52.8390, 318.2544, 470.7841])\n+        expected_slice_boxes = torch.tensor([16.5028, 52.8390, 318.2544, 470.7841]).to(torch_device)\n\nself.assertEqual(len(results[\"scores\"]), 5)\nself.assertTrue(torch.allclose(results[\"scores\"], expected_scores, atol=1e-4))\n", "fix_pattern": "In the condition of \"performing computations on tensors\", if \"the tensor is not on the correct device\", then \"add or change the .to(torch_device) method to fix the device mismatch\"."}
{"number": 298, "change": "class TensorflowBackendInferenceLearner(TensorflowBaseInferenceLearner):\nmetadata = LearnerMetadata.read(path)\nnetwork_parameters = ModelParams(**metadata.network_parameters)\ninput_tfms = metadata.input_tfms\n-        model = nebullvm.operations.inference_learners.utils.load_model(\n+        model = tf.keras.models.load_model(\npath / TENSORFLOW_BACKEND_FILENAMES[\"tf_model\"]\n)\ndevice = Device(metadata.device)\n", "fix_pattern": "In the condition of using the nebullvm.operations.inference_learners.utils.load_model() function, if the pattern of loading a TensorFlow model is detected, then change the code to use the tf.keras.models.load_model() function instead to fix the API misuse."}
{"number": 302, "change": "def rmsle(\n>>> x = torch.tensor([0., 1, 2, 3])\n>>> y = torch.tensor([0., 1, 2, 2])\n>>> rmsle(x, y)\n-        tensor(0.0207)\n+        tensor(0.1438)\n\n\"\"\"\n-    rmsle = mse(torch.log(pred + 1), torch.log(target + 1), reduction=reduction)\n+    rmsle = rmse(torch.log(pred + 1), torch.log(target + 1), reduction=reduction)\nreturn rmsle\n", "fix_pattern": "In the condition of calculating the root mean squared logarithmic error (rmsle), if the function call to mse() is detected, then the code is changed to call rmse() instead to fix the API misuse."}
{"number": 305, "change": "def ones_like(x, name=None):\n[ 1.,  1.,  1.]], dtype=float32)\n```\n\"\"\"\n-    return tf.ones_like(x, name=name)\n+    return tf.ones_like(x, dtype=dtype, name=name)\n\n\ndef random_uniform_variable(shape, low, high, dtype=None,\n", "fix_pattern": "In the condition of using the `ones_like` function, if the `name` parameter is not provided, the fix is to change the order of the parameters and pass `dtype` as the first parameter and `name` as the second parameter to fix the API misuse."}
{"number": 306, "change": "class Ensemble(nn.ModuleList):\nreturn y, None  # inference, train output\n\n\n-def attempt_load(weights, map_location=None, inplace=True, fuse=True):\n+def attempt_load(weights, device=None, inplace=True, fuse=True):\nfrom models.yolo import Detect, Model\n\n# Loads an ensemble of models weights=[a,b,c] or a single model weights=[a] or weights=a\nmodel = Ensemble()\nfor w in weights if isinstance(weights, list) else [weights]:\n-        ckpt = torch.load(attempt_download(w), map_location=map_location)  # load\n-        ckpt = (ckpt.get('ema') or ckpt['model']).float()  # FP32 model\n+        ckpt = torch.load(attempt_download(w))\n+        ckpt = (ckpt.get('ema') or ckpt['model']).to(device).float()  # FP32 model\nmodel.append(ckpt.fuse().eval() if fuse else ckpt.eval())  # fused or un-fused model in eval mode\n\n# Compatibility updates\n", "fix_pattern": "In the condition of `weights` being a list or not, if the pattern of converting the model to the `float()` datatype is detected, then the code `ckpt = (ckpt.get('ema') or ckpt['model']).to(device).float()` is added to fix the API misuse."}
{"number": 307, "change": "from allennlp.common.params import Params\n\nclass TestStackedBidirectionalLstm(AllenNlpTestCase):\ndef test_stacked_bidirectional_lstm_completes_forward_pass(self):\n-        input_tensor = torch.autograd.Variable(torch.rand(4, 5, 3))\n+        input_tensor = torch.rand(4, 5, 3)\ninput_tensor[1, 4:, :] = 0.\ninput_tensor[2, 2:, :] = 0.\ninput_tensor[3, 1:, :] = 0.\n", "fix_pattern": "In the condition of initializing a variable, if the code using 'torch.autograd.Variable' is detected, then remove this code to fix the API misuse."}
{"number": 309, "change": "class TFCTRLMainLayer(tf.keras.layers.Layer):\ntoken_type_embeds = 0\nposition_ids = tf.reshape(position_ids, [-1, shape_list(position_ids)[-1]])\n\n-        inputs_embeds = self.w(input_ids)\n+        inputs_embeds = self.w(input_ids, mode='embedding')\n# x = embedded.unsqueeze(0) if len(input_ids.shape)<2 else embedded\nseq_len = input_shape[-1]\nmask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n", "fix_pattern": "In the condition of using the `w()` method in `inputs_embeds = self.w(input_ids)`, if the desired mode for embedding is missing, then change the code to `inputs_embeds = self.w(input_ids, mode='embedding')` to fix the API misuse."}
{"number": 310, "change": "def _preprocess_conv3d_input(x, data_format):\nA tensor.\n\"\"\"\n# tensorflow doesn't support float64 for conv layer before 1.8.0\n-    if (dtype(x) == 'float64'\n-            and StrictVersion(tf.__version__) < StrictVersion('1.8.0')):\n+    if (dtype(x) == 'float64' and\n+            StrictVersion(tf.__version__.split('-')[0]) < StrictVersion('1.8.0')):\nx = tf.cast(x, 'float32')\ntf_data_format = 'NDHWC'\nif data_format == 'channels_first':\n", "fix_pattern": "In the condition of checking if the TensorFlow version is less than 1.8.0, if a pattern of splitting and comparing the version numbers is detected, then change the code by adding the split operation and comparing the first part of the version number with 1.8.0 to fix the API misuse."}
{"number": 314, "change": "class _EagerVariableStore(tf.Module):\nlayer = create_layer_method()\nself._layers[name] = layer\nif isinstance(layer, base_layer.Layer):\n-        self._regularizers[name] = lambda: layer.losses\n+        self._regularizers[name] = lambda: tf.math.reduce_sum(layer.losses)\nreturn self._layers[name]\n\ndef add_regularizer(self, var, regularizer):\n", "fix_pattern": "In the condition of checking if the layer is an instance of `base_layer.Layer`, if the pattern of calling `tf.math.reduce_sum` on `layer.losses` is detected, the code is changed from using `lambda: layer.losses` to `lambda: tf.math.reduce_sum(layer.losses)` to fix the API misuse."}
{"number": 315, "change": "class TFWav2Vec2MainLayer(tf.keras.layers.Layer):\nif inputs[\"attention_mask\"] is not None:\n# compute real output lengths according to convolution formula\noutput_lengths = self._get_feat_extract_output_lengths(tf.reduce_sum(inputs[\"attention_mask\"], -1))\n-            attention_mask = tf.sequence_mask(output_lengths, dtype=hidden_states.dtype)\n+\n+            attention_mask = tf.sequence_mask(\n+                output_lengths, maxlen=shape_list(hidden_states)[1], dtype=hidden_states.dtype\n+            )\n\nhidden_states = self.feature_projection(hidden_states, training=inputs[\"training\"])\n", "fix_pattern": "In the condition of \"if inputs['attention_mask'] is not None\", if the pattern of \"tf.sequence_mask(output_lengths, dtype=hidden_states.dtype)\" is detected, then change the code to \"tf.sequence_mask(output_lengths, maxlen=shape_list(hidden_states)[1], dtype=hidden_states.dtype)\" to fix the API misuse."}
{"number": 317, "change": "class DiagNormal(Distribution):\n# when the data is a ragged tensor. also useful for KL annealing. this entire logic\n# will likely be done in a better/cleaner way in the future\nif log_pdf_mask is not None:\n-            # TODO fix this to broadcasting as below, e.g. by instead:\n-            # log_pxs *= log_pdf_mask  # Then continue with broadcasting logic below.\n-            return torch.sum(log_pdf_mask * log_pxs, -1)\n+            log_pxs = log_pxs * log_pdf_mask\nbatch_log_pdf = torch.sum(log_pxs, -1)\nbatch_log_pdf_shape = x.size()[:-1] + (1,)\nreturn batch_log_pdf.contiguous().view(batch_log_pdf_shape)\n", "fix_pattern": "In the condition where `log_pdf_mask` is not None, if the pattern of multiplying `log_pdf_mask` with `log_pxs` is detected, the code is changed to multiply `log_pxs` with `log_pdf_mask` to fix the API misuse."}
{"number": 323, "change": "class BaseModel():\nsave_filename = '%s_net_%s.pth' % (which_epoch, name)\nsave_path = os.path.join(self.save_dir, save_filename)\nnet = getattr(self, 'net' + name)\n-                net.load_state_dict(torch.load(save_path))\n+                net.module.load_state_dict(torch.load(save_path))\n\n# print network information\ndef print_networks(self, verbose):\n", "fix_pattern": "In the condition of accessing the network module, if 'load_state_dict' is called on the 'net' object, then 'module' needs to be added before calling the 'load_state_dict' function to fix the API misuse."}
{"number": 324, "change": "class SpeedsterRootOp(Operation):\n) -> List[BaseInferenceLearner]:\nif self.orig_latency_measure_op.get_result() is not None:\nmodel_outputs = self.orig_latency_measure_op.get_result()[0]\n-            if isinstance(model, Module):\n+            if isinstance(model, torch.nn.Module):\noptimization_op = self.torch_optimization_op\nelif isinstance(model, tf.Module) and model is not None:\noptimization_op = self.tensorflow_optimization_op\n", "fix_pattern": "Fix_pattern:\nin the condition of isinstance(model, Module), if isinstance(model, torch.nn.Module) is detected, then change the code from self.torch_optimization_op to self.tensorflow_optimization_op to fix the API misuse."}
{"number": 325, "change": "def run(\n):\n# PyTorch model\nim = torch.zeros((batch_size, 3, *imgsz))  # BCHW image\n-    model = attempt_load(weights, map_location=torch.device('cpu'), inplace=True, fuse=False)\n+    model = attempt_load(weights, device=torch.device('cpu'), inplace=True, fuse=False)\n_ = model(im)  # inference\nmodel.info()\n", "fix_pattern": "In the condition of calling the \"attempt_load\" function, if the \"map_location\" argument is detected, then change it to \"device\" to fix the API misuse."}
{"number": 328, "change": "class UNet2DConditionModel(ModelMixin, ConfigMixin):\nif not torch.is_tensor(timesteps):\ntimesteps = torch.tensor([timesteps], dtype=torch.long, device=sample.device)\nelif torch.is_tensor(timesteps) and len(timesteps.shape) == 0:\n-            timesteps = timesteps[None].to(sample.device)\n+            timesteps = timesteps.to(dtype=torch.float32)\n+            timesteps = timesteps[None].to(device=sample.device)\n\n# broadcast to batch dimension in a way that's compatible with ONNX/Core ML\ntimesteps = timesteps.expand(sample.shape[0])\n", "fix_pattern": "In the condition of checking whether timesteps is a tensor, if the pattern of timesteps being a tensor with a shape of 0 is detected, then change the code to convert the dtype to float32 before assigning it to timesteps and then assign it to the sample device."}
{"number": 335, "change": "class AdaptiveEmbedding(nn.Module):\n\ninp_i = inp_flat.index_select(0, indices_i) - l_idx\nemb_i = self.emb_layers[i](inp_i)\n-                emb_i = F.linear(emb_i, self.emb_projs[i])\n+                emb_i = nn.functional.linear(emb_i, self.emb_projs[i])\n\nemb_flat.index_copy_(0, indices_i, emb_i)\n", "fix_pattern": "In the condition of using the `AdaptiveEmbedding` class, if a `F.linear` function call is detected, then change it to `nn.functional.linear` to fix the API misuse."}
{"number": 341, "change": "for m in model_list:\ndata_root=os.environ.get('IMAGENET_DIR', './imagenet')\n)\n\n+    torch.cuda.empty_cache()\n+\n", "fix_pattern": "In the condition of iterating through the model list, if the pattern of not clearing the CUDA cache is detected, then add the code to clear the cache using `torch.cuda.empty_cache()` to fix the API misuse."}
{"number": 348, "change": "def main():\nmodel = MMDataParallel(model, device_ids=[0])\noutputs = single_gpu_test(model, data_loader, args.show)\nelse:\n-        model = MMDistributedDataParallel(model.cuda())\n+        model = MMDistributedDataParallel(\n+            model.cuda(),\n+            device_ids=[torch.cuda.current_device()],\n+            broadcast_buffers=False)\noutputs = multi_gpu_test(model, data_loader, args.tmpdir,\nargs.gpu_collect)\n", "fix_pattern": "In the condition of \"else\", if the pattern \"model.cuda()\" is detected, then add the code \"device_ids=[torch.cuda.current_device()], broadcast_buffers=False\" to fix the API misuse."}
{"number": 357, "change": "class Tester(unittest.TestCase):\n# generate input data\nbatch_size = 1\ncenter = torch.zeros(batch_size, 2)\n-        angle = torch.ones(batch_size, 1)\n-        scale = torch.ones(batch_size, 1)\n+        angle = torch.ones(batch_size)\n+        scale = torch.ones(batch_size)\n\ncenter = utils.tensor_to_gradcheck_var(center)  # to var\nangle = utils.tensor_to_gradcheck_var(angle)  # to var\n", "fix_pattern": "In the condition of initializing the angle variable, if a single value tensor is passed, then change the shape of the tensor to match the expected shape in order to fix the API misuse."}
{"number": 360, "change": "class Trainer:\nreturn type(data)(self._prepare_input(v) for v in data)\nelif isinstance(data, torch.Tensor):\nkwargs = {\"device\": self.args.device}\n-            if self.deepspeed and data.dtype != torch.int64:\n-                # NLP models inputs are int64 and those get adjusted to the right dtype of the\n+            if self.deepspeed and (torch.is_floating_point(data) or torch.is_complex(data)):\n+                # NLP models inputs are int/uint and those get adjusted to the right dtype of the\n# embedding. Other models such as wav2vec2's inputs are already float and thus\n# may need special handling to match the dtypes of the model\nkwargs.update({\"dtype\": self.args.hf_deepspeed_config.dtype()})\n", "fix_pattern": "Fix_pattern: \nin the condition of checking if self.deepspeed is true, if the data is of floating point type or complex type, then the code is changed to fix the API misuse."}
{"number": 362, "change": "class DeformableDetrImageProcessor(BaseImageProcessor):\nimg_w = torch.Tensor([i[1] for i in target_sizes])\nelse:\nimg_h, img_w = target_sizes.unbind(1)\n-        scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1)\n+        scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1).to(boxes.device)\nboxes = boxes * scale_fct[:, None, :]\n\nresults = []\n", "fix_pattern": "in the condition of \"else\", if the pattern \"torch.stack([img_w, img_h, img_w, img_h], dim=1)\" is detected, then add \".to(boxes.device)\" to fix the API misuse."}
{"number": 363, "change": "class SpanBasedF1Test(AllenNlpTestCase):\ngold_indices = [self.vocab.get_token_index(x, \"tags\") for x in bio_tags]\ngold_tensor = torch.tensor([gold_indices], device=device)\nprediction_tensor = torch.rand([1, 6, self.vocab.get_vocab_size(\"tags\")], device=device)\n-        mask = torch.tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]], device=device)\n+        mask = torch.BoolTensor(\n+            [[True, True, True, True, True, True, True, True, True]], device=device\n+        )\n\n# Make prediction so that it is exactly correct.\nfor i, tag_index in enumerate(gold_indices):\n", "fix_pattern": "In the condition of initializing the mask tensor, if a pattern of using a torch tensor of integer values is detected, then change the code to use a torch tensor of boolean values to fix the API misuse."}
{"number": 366, "change": "class DistributedFusedLAMB(torch.optim.Optimizer):\nl2_norm = torch.zeros(size=[self._model_params_num], dtype=torch.float32, device='cuda')\nlocal_contrib_l2_norm = multi_tensor_applier(self.multi_tensor_l2norm, self._overflow_buf, [self._contrib_update_frag_for_norm], True)[1] ** 2\nl2_norm.masked_scatter_(self._model_param_is_contrib, local_contrib_l2_norm)\n-        torch.distributed.allreduce(l2_norm, group=self._ag_pg[0])\n+        torch.distributed.all_reduce(l2_norm, group=self._ag_pg[0])\nreturn l2_norm.masked_select(self._model_param_is_contrib)\n\ndef _pipeline_step(self):\n", "fix_pattern": "Fix_pattern: In the condition of calling the function \"allreduce\" in the torch.distributed module, if the code calls \"allreduce\" with a capital \"R\", then change it to \"all_reduce\" to fix the API misuse."}
{"number": 367, "change": "class _BinaryPostprocessing(torch.nn.Module):\npredictions = [self.bool2str.get(pred, self.bool2str[0]) for pred in predictions]\n\nprobs = preds[self.probabilities_key]\n-        probs = torch.dstack(1 - probs, probs)\n+        probs = torch.stack([1 - probs, probs], dim=-1)\n\nreturn {\nself.predictions_key: predictions,\n", "fix_pattern": "Fix_pattern: \n\nIn the condition of checking the probability values, if the pattern of stacking two tensors with a dimension is detected, then change the code from using torch.dstack() to torch.stack([1 - probs, probs], dim=-1) to fix the API misuse."}
{"number": 368, "change": "if __name__ == \"__main__\":\nexp = get_exp(args.exp_file, args.name)\nexp.merge(args.opts)\n\n-    num_gpu = get_num_devices() if args.devices is None else args.devices\n-    assert num_gpu <= get_num_devices()\n+    num_gpu = torch.cuda.device_count() if args.devices is None else args.devices\n+    assert num_gpu <= torch.cuda.device_count()\n\ndist_url = \"auto\" if args.dist_url is None else args.dist_url\nlaunch(\n", "fix_pattern": "In the condition of checking the number of devices available, if the `get_num_devices()` function is used, then change it to `torch.cuda.device_count()` to fix the API misuse."}
{"number": 372, "change": "class Critic(object):\nself.train_op = tf.train.AdamOptimizer(self.lr).minimize(self.loss)\n\nwith tf.variable_scope('a_grad'):\n-            self.a_grads = tf.gradients(self.q, a)[0]   # tensor of gradients of each sample (None, a_dim)\n+            self.a_grads = tf.gradients(self.q, self.a)[0]   # tensor of gradients of each sample (None, a_dim)\n\nif self.replacement['name'] == 'hard':\nself.t_replace_counter = 0\n", "fix_pattern": "in the condition of if self.replacement['name'] == 'hard', if the variable 'a' is detected, then change the code \"self.a_grads = tf.gradients(self.q, a)[0]\" to \"self.a_grads = tf.gradients(self.q, self.a)[0]\" to fix the API misuse."}
{"number": 375, "change": "def corr2d(X, K):  #@save\n\n# Defined in file: ./chapter_convolutional-neural-networks/lenet.md\ndef evaluate_accuracy_gpu(net, data_iter, device=None): #@save\n+    net.eval()  # Set the model to evaluation mode\nif not device:\ndevice = next(iter(net.parameters())).device\nmetric = d2l.Accumulator(2)  # num_corrected_examples, num_examples\n", "fix_pattern": "Fix_pattern: In the condition of \"if not device\", if the pattern \"net.eval()\" is not detected, then add the code \"net.eval()\" to fix the API misuse."}
{"number": 376, "change": "class VideoSequential(ImageSequential):\n# Size of T\nframe_num = input.size(self._temporal_channel)\n# Got param generation shape to (B, C, H, W). Ignoring T.\n-        batch_shape = self.__infer_channel_exclusive_batch_shape__(input)\n+        batch_shape = self.__infer_channel_exclusive_batch_shape__(input, self._temporal_channel)\ninput = self._input_shape_convert_in(input)\ninput = input.reshape(-1, *batch_shape[1:])\nif not self.same_on_frame:\n", "fix_pattern": "In the code, if the condition \"not self.same_on_frame\" is met, the \"self._temporal_channel\" parameter was missing in the function call \"__infer_channel_exclusive_batch_shape__(input)\". The fix was to add \"self._temporal_channel\" as a parameter to the function call \"__infer_channel_exclusive_batch_shape__(input, self._temporal_channel)\". This fixed the API misuse."}
{"number": 377, "change": "def create_dataloader(path, imgsz, batch_size, stride, single_cls=False, hyp=Non\nprefix=prefix)\n\nbatch_size = min(batch_size, len(dataset))\n-    nw = min([os.cpu_count() // DEVICE_COUNT, batch_size if batch_size > 1 else 0, workers])  # number of workers\n+    nd = torch.cuda.device_count()  # number of CUDA devices\n+    nw = min([os.cpu_count() // max(nd, 1), batch_size if batch_size > 1 else 0, workers])  # number of workers\nsampler = None if rank == -1 else distributed.DistributedSampler(dataset, shuffle=shuffle)\nloader = DataLoader if image_weights else InfiniteDataLoader  # only DataLoader allows for attribute updates\nreturn loader(dataset,\n", "fix_pattern": "In the condition of determining the number of workers, if the pattern \"os.cpu_count() // DEVICE_COUNT\" is detected, then change it to \"os.cpu_count() // max(nd, 1)\" to fix the API misuse."}
{"number": 381, "change": "if dependency_check.crypten_available:\n\nframework_packages[\"crypten\"] = crypten\nframework_tensors.append(crypten.mpc.MPCTensor)\n+    framework_tensors.append(crypten.nn.Module)\n+\n\nframework_tensors = tuple(framework_tensors)\nFrameworkTensorType = Union[framework_tensors]\n", "fix_pattern": "In the condition of \"if dependency_check.crypten_available\", if the pattern \"framework_tensors.append(crypten.mpc.MPCTensor)\" is detected, then add the code \"framework_tensors.append(crypten.nn.Module)\" to fix the API misuse."}
{"number": 384, "change": "def array(object_in, dtype: Optional[str] = None, dev: Optional[str] = None):\ndev = default_device(dev)\ndtype = dtype_from_str(default_dtype(dtype, object_in))\nif isinstance(object_in, np.ndarray):\n-        return _torch.Tensor(object_in).to(dev_from_str(dev))\n+        return torch.Tensor(object_in).to(dev_from_str(dev))\nif dtype is not None:\n-        return _torch.tensor(object_in, dtype=dtype, device=dev_from_str(dev))\n-    elif isinstance(object_in, _torch.Tensor):\n+        return torch.tensor(object_in, dtype=dtype, device=dev_from_str(dev))\n+    elif isinstance(object_in, torch.Tensor):\nreturn object_in.to(dev_from_str(dev))\nelse:\n-        return _torch.tensor(object_in, device=dev_from_str(dev))\n+        return torch.tensor(object_in, device=dev_from_str(dev))\n\nasarray = array\n", "fix_pattern": "In the condition of checking if the object is an instance of np.ndarray, if the pattern \"_torch\" is detected, then change it to \"torch\" to fix the API misuse."}
{"number": 390, "change": "def _calculate_expected_result(\naggregation_op_only_probs = gumbel_dist.sample()\nelse:\n# <float32>[batch_size, num_aggregation_labels - 1]\n-        aggregation_op_only_probs = torch.nn.functional.softmax(\n+        aggregation_op_only_probs = nn.functional.softmax(\nlogits_aggregation[:, 1:] / config.aggregation_temperature, dim=-1\n)\n", "fix_pattern": "Fix pattern:\n\nIn the condition of checking if the aggregation_op_only_probs is empty, if an API misuse is detected where the torch.nn.functional.softmax is used instead of nn.functional.softmax, then the code should be changed to nn.functional.softmax to fix the API misuse."}
{"number": 392, "change": "class TensorforceModel(Model):\ndiscounts = tf.math.pow(x=discount, y=exponent)\nif not self.predict_terminal_values:\ndiscounts = tf.where(\n-                    condition=tf.math.greater(x=_terminal, y=one),\n-                    x=discounts, y=tf.zeros_like(input=discounts)\n+                    condition=tf.math.equal(x=_terminal, y=one),\n+                    x=tf.zeros_like(input=discounts), y=discounts\n)\n\n-            reward += discounts * horizon_values\n+            reward = reward + discounts * horizon_values\n\ndependencies = [reward]\nif self.summaries == 'all' or 'reward' in self.summaries:\n", "fix_pattern": "In the condition of `tf.where`, if `tf.math.greater` is detected, then change `x=discounts, y=tf.zeros_like(input=discounts)` to `x=tf.zeros_like(input=discounts), y=discounts` to fix the API misuse."}
{"number": 395, "change": "def test_cgcnn_conv():\nedge_index = torch.tensor([[0, 0, 0, 1, 2, 3], [1, 2, 3, 0, 0, 0]])\nnum_nodes = edge_index.max().item() + 1\nx = torch.randn((num_nodes, node_dim))\n-    pseudo = torch.rand((edge_index.size(1), 3))\n+    pseudo = torch.rand((edge_index.size(1), edge_dim))\n\nconv = CGCNNConv(node_dim, edge_dim)\nassert conv.__repr__() == 'CGCNNConv(16, 16)'\n", "fix_pattern": "In the condition of \"edge_dim\", if \"pseudo\" is detected, then change the code from \"3\" to \"edge_dim\" to fix the API misuse."}
{"number": 408, "change": "\"        # compute the gating function and one minus the gating function\\n\",\n\"        gate_intermediate = self.relu(self.lin_gate_z_to_hidden(z_t_1))\\n\",\n\"        gate = self.sigmoid(self.lin_gate_hidden_to_z(gate_intermediate))\\n\",\n-    \"        one_minus_gate = ng_ones(gate.size()).type_as(gate) - gate\\n\",\n+    \"        one_minus_gate = torch.ones(gate.size()).type_as(gate) - gate\\n\",\n\"        # compute the 'proposed mean'\\n\",\n\"        proposed_mean_intermediate = self.relu(self.lin_proposed_mean_z_to_hidden(z_t_1))\\n\",\n\"        proposed_mean = self.lin_proposed_mean_hidden_to_z(proposed_mean_intermediate)\\n\",\n", "fix_pattern": "In the condition of checking for the creation of a tensor, if the API function `ng_ones()` is detected, then change it to `torch.ones()` to fix the API misuse."}
{"number": 412, "change": "def crop_by_boxes(tensor, src_box, dst_box,\ndst_trans_src = dst_trans_src.expand(tensor.shape[0], -1, -1)\n\nbbox = _infer_bounding_box(dst_box)\n-    patches: torch.Tensor = warp_perspective(\n-        tensor, dst_trans_src, (int(bbox[0].int().data.item()), int(bbox[1].int().data.item())))\n+    patches: torch.Tensor = warp_affine(\n+        tensor, dst_trans_src[:, :2, :], (int(bbox[0].int().data.item()), int(bbox[1].int().data.item())))\n\n# return in the original shape\nif is_unbatched:\n", "fix_pattern": "In the condition of using warp_perspective/warp_affine function, if warp_perspective is detected, then change it to warp_affine to fix the API misuse."}
{"number": 417, "change": "class Model(ModelDesc):\nif get_current_tower_context().is_training:\nwd_w = tf.train.exponential_decay(2e-4, get_global_step_var(),\n80000, 0.7, True)\n-            wd_cost = tf.mul(wd_w, regularize_cost('.*/W', tf.nn.l2_loss), name='wd_cost')\n+            wd_cost = tf.multiply(wd_w, regularize_cost('.*/W', tf.nn.l2_loss), name='wd_cost')\ncosts.append(wd_cost)\n\nadd_param_summary(('.*/W', ['histogram']))   # monitor W\n", "fix_pattern": "in the condition of checking if the current tower is training, if the pattern of using tf.mul() is detected, then change the code to use tf.multiply() to fix the API misuse."}
{"number": 418, "change": "if __name__ == '__main__':\nloss_values.clear()\naccuracies.clear()\nif step % 100 == 0:\n-            vis.draw_projections(embeds.detach().cpu().numpy(), utterances_per_speaker, step)\n+            vis.draw_projections(embeds.detach().numpy(), utterances_per_speaker, step)\n", "fix_pattern": "in the condition of <step % 100 == 0>, if <embeds.detach().cpu()> is detected, then remove .cpu() from the code to fix the API misuse."}
{"number": 419, "change": "class ScoreSdeVpScheduler(SchedulerMixin, ConfigMixin):\nx_mean = x + drift * dt\n\n# add noise\n-        noise = torch.randn(x.shape, layout=x.layout, generator=generator).to(x.device)\n+        noise = randn_tensor(x.shape, layout=x.layout, generator=generator, device=x.device, dtype=x.dtype)\nx = x_mean + diffusion * math.sqrt(-dt) * noise\n\nreturn x, x_mean\n", "fix_pattern": "In the condition of calling the function \"randn_tensor()\", if the pattern \"torch.randn()\" is detected, then change the code to \"randn_tensor()\" to fix the API misuse."}
{"number": 422, "change": "def get_keras_model():\nM.add(KL.Conv2D(32, 3, padding='same', activation='relu'))\nM.add(KL.Flatten())\nM.add(KL.Dense(512, activation='relu', kernel_regularizer=keras.regularizers.l2(1e-5)))\n-        M.add(KL.Dropout(0.5))\n+        M.add(KL.Dropout(rate=0.5))\nM.add(KL.Dense(10, activation=None, kernel_regularizer=keras.regularizers.l2(1e-5)))\nreturn M\n", "fix_pattern": "In the condition of using the Keras Dropout layer, if the parameter \"rate\" is used instead of \"dropout\", then change the code from \"M.add(KL.Dropout(0.5))\" to \"M.add(KL.Dropout(rate=0.5))\" to fix the API misuse."}
{"number": 424, "change": "def main(parsed_args):\n\ndef cli_main():\nparser = options.get_eval_lm_parser()\n+    add_distributed_training_args(parser)\nargs = options.parse_args_and_arch(parser)\n-    main(args)\n+    distributed_utils.call_main(args, main)\n\n\nif __name__ == '__main__':\n", "fix_pattern": "In the condition of \"__name__ == '__main__'\", if the pattern \"options.parse_args_and_arch(parser)\" is detected, then add the code \"add_distributed_training_args(parser)\" before the detection pattern to fix the API misuse."}
{"number": 425, "change": "def degree(index, num_nodes=None, dtype=None, device=None):\ntensor([3., 1., 1.])\n\"\"\"\nnum_nodes = maybe_num_nodes(index, num_nodes)\n-    out = torch.zeros((num_nodes), dtype=dtype, device=device)\n+    out = torch.zeros((num_nodes), dtype=dtype, device=index.device)\nreturn out.scatter_add_(0, index, out.new_ones((index.size(0))))\n", "fix_pattern": "in the condition of \"device=None\", if missing the \"device=index.device\" assignment, then add the assignment \"device=index.device\" to fix the API misuse."}
{"number": 429, "change": "class RandomThinPlateSpline(AugmentationBase2D):\n\ndef generate_parameters(self, shape: torch.Size) -> Dict[str, Tensor]:\nB, _, _, _ = shape\n-        src = torch.tensor([[[-1.0, -1.0], [-1.0, 1.0], [1.0, -1.0], [1.0, 1.0], [0.0, 0.0]]]).expand(B, 5, 2)  # Bx5x2\n+        src = tensor([[[-1.0, -1.0], [-1.0, 1.0], [1.0, -1.0], [1.0, 1.0], [0.0, 0.0]]]).expand(B, 5, 2)  # Bx5x2\ndst = src + self.dist.rsample(src.shape)\nreturn dict(src=src, dst=dst)\n", "fix_pattern": "In the condition of \"using the tensor() function from the torch module\", if a pattern with \"torch.\" before the function call is detected, then remove the \"torch.\" to fix the API misuse."}
{"number": 432, "change": "class PNDMScheduler(SchedulerMixin, ConfigMixin):\n::-1\n].copy()  # we copy to avoid having negative strides which are not supported by torch.from_numpy\n\n-        self.timesteps = np.concatenate([self.prk_timesteps, self.plms_timesteps]).astype(np.int64)\n+        timesteps = np.concatenate([self.prk_timesteps, self.plms_timesteps]).astype(np.int64)\n+        self.timesteps = torch.from_numpy(timesteps).to(device)\n\nself.ets = []\nself.counter = 0\n", "fix_pattern": "In the condition of copying numpy array into a torch tensor, if the pattern of converting np.array to torch.from_numpy().to(device) is detected, then add the code to fix the API misuse."}
{"number": 433, "change": "def HomographyRegressionApp():\n[-1, 1],  # top-right\n]]).to(dst_homo_src.device)\n# transform points\n-            pts_dst = dgm.transform_points(dgm.inverse(dst_homo_src), pts_src)\n+            pts_dst = dgm.transform_points(torch.inverse(dst_homo_src), pts_src)\n\ndef compute_factor(size):\nreturn 1.0 * size / 2\n", "fix_pattern": "In the condition of `API misuse`, if `dgm.inverse(dst_homo_src)` is detected, then remove `dgm.inverse` to fix the API misuse."}
{"number": 436, "change": "class Highway(torch.nn.Module):\n# above, too.\nnonlinear_part, gate = projected_input.chunk(2, dim=-1)\nnonlinear_part = self._activation(nonlinear_part)\n-            gate = torch.nn.functional.sigmoid(gate)\n+            gate = torch.sigmoid(gate)\ncurrent_input = gate * linear_part + (1 - gate) * nonlinear_part\nreturn current_input\n", "fix_pattern": "in the condition of using torch.nn.functional.sigmoid, if it is detected, then change it to torch.sigmoid to fix the API misuse."}
{"number": 437, "change": "class Model(object):\n\"It should be either Tensor or a list of Tensor.\"\n)\nfor idx in range(len(check_argu)):\n-                        if not isinstance(check_argu[idx], tf_ops._TensorLike) or not tf_ops.is_dense_tensor_like(\n+                        if not isinstance(check_argu[idx], [tf.Tensor, tf.SparseTensor, tf.Variable]) or not tf_ops.is_dense_tensor_like(\ncheck_argu[idx]):\nraise TypeError(\n\"The argument `%s` should be either Tensor or a list of Tensor \" % (check_order[co]) +\n", "fix_pattern": "In the condition of checking the argument type, if the pattern of checking for Tensor or list of Tensor is not detected, then the code is changed to specifically check for tf.Tensor, tf.SparseTensor, or tf.Variable to fix the API misuse."}
{"number": 439, "change": "class TestBasicTextFieldEmbedder(AllenNlpTestCase):\n})\ntoken_embedder = BasicTextFieldEmbedder.from_params(self.vocab, params)\ninputs = {\n-                'words': Variable(torch.rand(3, 4, 5, 6) * 20).long(),\n-                'characters': Variable(torch.rand(3, 4, 5, 6, 7) * 15).long(),\n+                'words': (torch.rand(3, 4, 5, 6) * 20).long(),\n+                'characters': (torch.rand(3, 4, 5, 6, 7) * 15).long(),\n}\nassert token_embedder(inputs, num_wrapping_dims=2).size() == (3, 4, 5, 6, 12)\n", "fix_pattern": "In the condition of creating inputs for the token_embedder, if the inputs are a dictionary with keys 'words' and 'characters', then the pattern of wrapping the values in 'Variable()' needs to be changed to '( )' to fix the API misuse."}
{"number": 446, "change": "class Csv(datasets.ArrowBasedBuilder):\nif schema is not None\nelse None\n)\n-        for file_idx, file in enumerate(files):\n+        for file_idx, file in enumerate(itertools.chain.from_iterable(files)):\ncsv_file_reader = pd.read_csv(file, iterator=True, dtype=dtype, **self.config.read_csv_kwargs)\ntry:\nfor batch_idx, df in enumerate(csv_file_reader):\n", "fix_pattern": "In the condition of iterating over files, if the `files` list is being iterated using `enumerate`, then change the code to use `itertools.chain.from_iterable` to flatten the list and iterate over its elements individually to fix the API misuse."}
{"number": 448, "change": "class ARMAConv(MessagePassing):\nif self.bias is not None:\nout += self.bias[0 if self.shared_weights else t]\n\n-            if t < self.num_layers - 1:\n+            if self.act is not None and t < self.num_layers - 1:\nout = self.act(out)\n\nreturn out.mean(dim=-3)\n", "fix_pattern": "in the condition of checking if the activation function is not None, if the pattern of checking if t is less than the number of layers minus 1 is detected, then add the code to fix the API misuse by assigning the result to the variable \"out\"."}
{"number": 449, "change": "class DependencyParser(flair.nn.Model):\nsentence_tensor = self.word_dropout(sentence_tensor)\n\nif self.use_rnn:\n-            sentence_tensor = pack_padded_sequence(sentence_tensor, lengths, True, False)\n+            sentence_sequence = pack_padded_sequence(sentence_tensor, torch.IntTensor(lengths), True, False)\n\n-            sentence_tensor, _ = self.lstm(sentence_tensor)\n-            sentence_tensor, _ = pad_packed_sequence(sentence_tensor, True, total_length=seq_len)\n+            sentence_sequence, _ = self.lstm(sentence_sequence)\n+            sentence_tensor, _ = pad_packed_sequence(sentence_sequence, True, total_length=seq_len)\n\n# apply MLPs for arc and relations to the BiLSTM output states\narc_h = self.mlp_arc_h(sentence_tensor)\n", "fix_pattern": "In the condition of using an RNN, if there is a need to pack the padded sequence, the API 'pack_padded_sequence' is called with the correct arguments to fix the API misuse."}
{"number": 451, "change": "def testtanh():\n\nPtensor = PolynomialTensor()\n\n-    x = torch.linspace(-3, 3, steps=10)\n+    x = torch.tensor(np.linspace(-3, 3, 10))\nexpected = torch.tensor(\n[\n-3.3883e02,\n", "fix_pattern": "in the condition of calling the torch.linspace() function, if numpy array is used directly as the input, then change the code to use the torch.tensor(np.linspace()) function instead to fix the API misuse."}
{"number": 452, "change": "class BartTranslationTests(unittest.TestCase):\nwith torch.no_grad():\nlogits, *other_stuff = model(**self.net_input)\n\n-        expected_slice = torch.tensor([9.0078, 10.1113, 14.4787])\n+        expected_slice = torch.tensor([9.0078, 10.1113, 14.4787], device=torch_device)\nresult_slice = logits[0][0][:3]\nself.assertTrue(torch.allclose(expected_slice, result_slice, atol=TOLERANCE))\n", "fix_pattern": "In the condition of API misuse, if a missing device argument is detected, then add `device=torch_device` to fix the issue."}
{"number": 453, "change": "def test_dc_crn_separator_invalid_type():\ndef test_dc_crn_separator_output():\nreal = torch.rand(2, 10, 17)\nimag = torch.rand(2, 10, 17)\n-    x = ComplexTensor(real, imag) if is_torch_1_9_plus else torch.complex(real, imag)\n+    x = torch.complex(real, imag) if is_torch_1_9_plus else ComplexTensor(real, imag)\nx_lens = torch.tensor([10, 8], dtype=torch.long)\n\nfor num_spk in range(1, 3):\n", "fix_pattern": "In the condition of checking the torch version, if the pattern \"ComplexTensor(real, imag)\" is detected, then change the code to \"torch.complex(real, imag)\" to fix the API misuse."}
{"number": 457, "change": "def multilevel_roi_align(features, rcnn_boxes, resolution):\nall_rois = tf.concat(all_rois, axis=0)  # NCHW\n# Unshuffle to the original order, to match the original samples\nlevel_id_perm = tf.concat(level_ids, axis=0)  # A permutation of 1~N\n-    level_id_invert_perm = tf.invert_permutation(level_id_perm)\n+    level_id_invert_perm = tf.math.invert_permutation(level_id_perm)\nall_rois = tf.gather(all_rois, level_id_invert_perm, name=\"output\")\nreturn all_rois\n", "fix_pattern": "In the condition of using the `tf.invert_permutation` function from TensorFlow, if the pattern `tf.invert_permutation(<arguments>)` is detected, then change the code to `tf.math.invert_permutation(<arguments>)` to fix the API misuse."}
{"number": 459, "change": "def _preprocess_deconv_output_shape(x, shape, dim_ordering):\nshape = (shape[0], shape[2], shape[3], shape[1])\n\nif shape[0] is None:\n-        shape = (tf.shape(x)[0], ) + shape[1:]\n+        shape = (tf.shape(x)[0], ) + tuple(shape[1:])\nreturn shape\n", "fix_pattern": "In the condition of the if statement, if a tuple conversion is missing for the 'shape' variable, then add the 'tuple' function to convert it to a tuple. This fixes the API misuse."}
{"number": 462, "change": "class RGCNConv(MessagePassing):\nreturn out if edge_norm is None else out * edge_norm.view(-1, 1)\n\ndef update(self, aggr_out, x):\n-        if x.dtype == torch.long:\n+        if x is None:\nout = aggr_out + self.root\nelse:\nout = aggr_out + torch.matmul(x, self.root)\n", "fix_pattern": "Fix_pattern: \n\nIn the condition of checking if x is None, if the pattern of x being None is detected, then the code is removed to fix the API misuse."}
{"number": 470, "change": "class DenseGCNConv(torch.nn.Module):\nidx = torch.arange(N, dtype=torch.long, device=adj.device)\nadj[:, idx, idx] = 1 if not self.improved else 2\n\n-        out = self.lin(x)\n+        out = torch.matmul(x, self.weight)\ndeg_inv_sqrt = adj.sum(dim=-1).clamp(min=1).pow(-0.5)\n\nadj = deg_inv_sqrt.unsqueeze(-1) * adj * deg_inv_sqrt.unsqueeze(-2)\n", "fix_pattern": "In the condition of `if not self.improved`, if the pattern `self.lin(x)` is detected, then change the code to `torch.matmul(x, self.weight)` to fix the API misuse."}
{"number": 472, "change": "class GradientsTest(tf.test.TestCase):\n\n\nif __name__ == \"__main__\":\n-  tf.test.main()\n+  if tf.__internal__.tf2.enabled():\n+    tf.test.main()\n", "fix_pattern": "In the condition of \"__name__ == \"__main__\", if \"tf.__internal__.tf2.enabled()\" is detected, then add the code \"tf.test.main()\" to fix the API misuse."}
{"number": 473, "change": "class Model(ModelDesc):\ncost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)\ncost = tf.reduce_mean(cost, name='cross_entropy_loss')\n\n-        acc = tf.to_float(tf.nn.in_top_k(logits, label, 1))\n+        acc = tf.cast(tf.nn.in_top_k(logits, label, 1), tf.float32)\n\nacc = tf.reduce_mean(acc, name='accuracy')\nsummary.add_moving_summary(acc)\n", "fix_pattern": "In the condition of converting a boolean output to float, if tf.to_float() function is used, then change it to tf.cast() to fix the API misuse."}
{"number": 475, "change": "def block(params, scope, layer_num, bias, sequence_dim, memory_length_dim, varia\nif use_moe:\nmoe_params = mtf.transformer.moe.HParams()\nmtf.transformer.moe.set_default_moe_hparams(moe_params)\n+\n+                # override defaults\nfor k, v in params[\"moe_params\"].items():\nmoe_params.add_hparam(k, v)\n-                mtf.transformer.moe.set_default_moe_hparams(moe_params)\n+\nmoe_train = params[\"mode\"] == \"train\"\n\nm, aux_loss = mtf.transformer.moe.transformer_moe_layer_v1(res_x, x.shape[-1], moe_params,\n", "fix_pattern": "in the condition of \"use_moe\", if a pattern of adding MOE parameters to \"moe_params\" is detected, then add the MOE parameters to \"moe_params\" using \"add_hparam\" to fix the API misuse."}
{"number": 476, "change": "class TFCoreModelTesterMixin:\n\nself.assertIsNotNone(outputs)\n\n-        tf.keras.mixed_precision.experimental.set_policy(\"float32\")\n+        tf.keras.mixed_precision.set_global_policy(\"float32\")\n\n@slow\ndef test_train_pipeline_custom_model(self):\n", "fix_pattern": "In the condition of creating a custom model tester class, if the pattern \"tf.keras.mixed_precision.experimental.set_policy(\"float32\")\" is detected, then it should be changed to \"tf.keras.mixed_precision.set_global_policy(\"float32\")\" to fix the API misuse."}
{"number": 479, "change": "class TFMBartForConditionalGeneration(TFMBartPreTrainedModel, TFCausalLanguageMo\nif labels is not None:\nlabels = tf.where(\nlabels == self.config.pad_token_id,\n-                tf.fill(shape_list(labels), -100),\n+                tf.cast(tf.fill(shape_list(labels), -100), labels.dtype),\nlabels,\n)\nuse_cache = False\n", "fix_pattern": "In the condition of checking if labels is not None, if the pattern of comparing labels to self.config.pad_token_id is detected, then change the code for filling the labels to also cast it to the same datatype as labels to fix the API misuse."}
{"number": 480, "change": "class TFModelTesterMixin:\ndef _prepare_for_class(self, inputs_dict, model_class, return_labels=False):\nif model_class in TF_MODEL_FOR_MULTIPLE_CHOICE_MAPPING.values():\ninputs_dict = {\n-                k: tf.tile(tf.expand_dims(v, 1), (1, self.model_tester.num_choices, 1))\n-                if isinstance(v, tf.Tensor) and v.ndim != 0\n+                k: tf.tile(tf.expand_dims(v, 1), (1, self.model_tester.num_choices) + (1,) * (v.ndim - 1))\n+                if isinstance(v, tf.Tensor) and v.ndim > 0\nelse v\nfor k, v in inputs_dict.items()\n}\n", "fix_pattern": "in the condition of \"if model_class in TF_MODEL_FOR_MULTIPLE_CHOICE_MAPPING.values()\", if the pattern \"isinstance(v, tf.Tensor) and v.ndim != 0\" is detected, then change the code to \"tf.tile(tf.expand_dims(v, 1), (1, self.model_tester.num_choices) + (1,) * (v.ndim - 1))\" to fix the API misuse."}
{"number": 482, "change": "class Pandas(datasets.ArrowBasedBuilder):\nreturn pa_table\n\ndef _generate_tables(self, files):\n-        for i, file in enumerate(files):\n+        for i, file in enumerate(itertools.chain.from_iterable(files)):\nwith open(file, \"rb\") as f:\npa_table = pa.Table.from_pandas(pd.read_pickle(f))\nyield i, self._cast_table(pa_table)\n", "fix_pattern": "In the condition of iterating over files, if the pattern of using \"itertools.chain.from_iterable\" is detected, then the code is changed by adding it to properly iterate over the files. This fixes the API misuse and ensures proper file iteration."}
{"number": 483, "change": "class DiceLoss(nn.Module):\ncardinality = torch.sum(input_soft + target_one_hot, dims)\n\ndice_score = 2. * intersection / (cardinality + self.eps)\n-        return torch.mean(1. - dice_score)\n+        return torch.mean(torch.tensor(1.) - dice_score)\n\n\n######################\n", "fix_pattern": "In the condition of a method calculating the Dice Loss, if the pattern of subtracting a scalar value from a tensor is detected, then change the subtraction code to subtract the tensor from the scalar instead to fix the API misuse."}
{"number": 488, "change": "class SageMakerTrainingArguments(TrainingArguments):\n# Here, we'll use torch.distributed.\n# Initializes the distributed backend which will take care of synchronizing nodes/GPUs\nif not torch.distributed.is_initialized():\n-                torch.distributed.init_process_group(backend=\"nccl\")\n+                torch.distributed.init_process_group(backend=\"nccl\", timeout=self.ddp_timeout_delta)\ndevice = torch.device(\"cuda\", self.local_rank)\nself._n_gpu = 1\n", "fix_pattern": "in the condition of `if not torch.distributed.is_initialized():`, if `pattern` is detected, then add `timeout=self.ddp_timeout_delta` to the `torch.distributed.init_process_group` code to fix the API misuse."}
{"number": 497, "change": "class EvalbBracketingScorer(Metric):\nshutil.rmtree(tempdir)\n\nif is_distributed():\n-            # Setting the device to CPU since this metric is not expected to run on GPUs.\n-            device = torch.device(\"cpu\")\n+            device = torch.device(\"cuda\" if dist.get_backend() == \"nccl\" else \"cpu\")\ncorrect_predicted_brackets = torch.tensor(_correct_predicted_brackets).to(device)\npredicted_brackets = torch.tensor(_predicted_brackets).to(device)\ngold_brackets = torch.tensor(_gold_brackets).to(device)\n", "fix_pattern": "In the condition of `is_distributed()`, if `dist.get_backend() == \"nccl\"` is detected, then change `device = torch.device(\"cpu\")` to `device = torch.device(\"cuda\" if dist.get_backend() == \"nccl\" else \"cpu\")` to fix the API misuse."}
{"number": 501, "change": "class E2E(torch.nn.Module):\n# Neither CPUTensor nor float/int value can be used\n# because NCCL communicates between GPU devices.\ndevice = next(self.parameters()).device\n-        acc = torch.tensor([acc], device=device)\n+\n+        acc = torch.tensor([acc], device=device) if acc is not None else None\ncer = torch.tensor([cer], device=device)\nwer = torch.tensor([wer], device=device)\nreturn self.loss, loss_ctc, loss_att, acc, cer, wer\n", "fix_pattern": "In the condition of checking if the variable \"acc\" is not None, if the pattern of assigning a tensor to \"acc\" is detected, then add a condition to assign None to \"acc\" to fix the API misuse."}
{"number": 502, "change": "class DeepQNetwork(ValueFunction):\n\"\"\"\n\n# Compute estimated future value\n-        float_terminals = tf.to_float(batch['terminals'])\n+        float_terminals = batch['terminals'].astype(float)\nq_targets = batch['rewards'] + (1. - float_terminals) \\\n* self.gamma * self.get_target_values(batch['next_states'])\n", "fix_pattern": "In the condition of converting a variable to float, if the pattern of converting using tf.to_float() is detected, then change the code to using the astype(float) method to fix the API misuse."}
{"number": 508, "change": "class DistributedFusedAdam(torch.optim.Optimizer):\ngrp = torch.distributed.new_group(ranks=ranks)\nif torch.distributed.get_rank() in ranks:\nself._rs_pg.append(grp)\n-            if self._compute_L2_grad_norm and torch.distributed.get_rank() in ranks:\n-                self._l2_grad_norm_pg = torch.distributed.new_group(ranks=ranks)\n-                torch.distributed.all_reduce(self._overflow_buf,group=self._l2_grad_norm_pg)\n+            if self._compute_L2_grad_norm:\n+                l2_grad_norm_pg = torch.distributed.new_group(ranks=ranks)\n+                if torch.distributed.get_rank() in ranks:\n+                    self._l2_grad_norm_pg = l2_grad_norm_pg\n+                    torch.distributed.all_reduce(self._overflow_buf,group=self._l2_grad_norm_pg)\nself._rs_st = [torch.cuda.Stream() for _ in range(self._num_rs_pg)]\nfor rs_pg in self._rs_pg:\ntorch.distributed.all_reduce(self._overflow_buf,group=rs_pg)\n", "fix_pattern": "In the condition of \"if self._compute_L2_grad_norm and torch.distributed.get_rank() in ranks\", if the pattern \"torch.distributed.new_group(ranks=ranks)\" is detected, then remove \"self._l2_grad_norm_pg = torch.distributed.new_group(ranks=ranks)\" from the code to fix the API misuse."}
{"number": 510, "change": "def cartesian_product_of_parameters(**possible_parameters):\n\n\ndef default_with_one_parameter_changed(*, default={}, **possible_parameters):\n-    assert isinstance(default, dict), f\"default should be a dict not a {type(default)}\"\n+    if not isinstance(default, dict):\n+        raise AssertionError(f\"default should be a dict not a {type(default)}\")\n\nfor parameter_name, possible_values in possible_parameters.items():\nfor v in possible_values:\n", "fix_pattern": "In the condition of checking the type of \"default\", if it is not a dictionary, then the code is changed from an assertion to a raise of AssertionError to fix the API misuse."}
{"number": 516, "change": "class Speech2Text2SinusoidalPositionalEmbedding(nn.Module):\nemb = torch.cat([emb, torch.zeros(num_embeddings, 1)], dim=1)\nif padding_idx is not None:\nemb[padding_idx, :] = 0\n-        return emb\n+        return emb.to(torch.get_default_dtype())\n\n@torch.no_grad()\ndef forward(self, input_ids: torch.Tensor, past_key_values_length: int = 0):\n", "fix_pattern": "In the condition of \"if padding_idx is not None\", if the pattern \"emb.to(torch.get_default_dtype())\" is detected, then add \"emb.to(torch.get_default_dtype())\" to fix the API misuse."}
{"number": 518, "change": "class EulerAncestralDiscreteScheduler(SchedulerMixin, ConfigMixin):\n\nprev_sample = sample + derivative * dt\n\n-        device = model_output.device if torch.is_tensor(model_output) else \"cpu\"\n-        if str(device) == \"mps\":\n+        device = model_output.device if torch.is_tensor(model_output) else torch.device(\"cpu\")\n+        if device.type == \"mps\":\n# randn does not work reproducibly on mps\nnoise = torch.randn(model_output.shape, dtype=model_output.dtype, device=\"cpu\", generator=generator).to(\ndevice\n", "fix_pattern": "Fix pattern: In the condition of checking the device type, if the pattern of using 'str(device)' is detected, then change it to 'device.type' to fix the API misuse."}
{"number": 519, "change": "class AutoRegressiveNN(nn.Module):\n\nif permutation is None:\n# By default set a random permutation of variables, which is important for performance with multiple steps\n-            self.permutation = torch.randperm(input_dim)\n+            self.permutation = torch.randperm(input_dim, device='cpu').to(torch.Tensor().device)\nelse:\n# The permutation is chosen by the user\nself.permutation = permutation.type(dtype=torch.int64)\n", "fix_pattern": "in the condition of \"permutation is None\", if the pattern \"torch.randperm(input_dim)\" is detected, then change the code to \"torch.randperm(input_dim, device='cpu').to(torch.Tensor().device)\" to fix the API misuse."}
{"number": 522, "change": "class StableDiffusionInpaintPipeline(DiffusionPipeline):\nelse:\nraise ImportError(\"Please install accelerate via `pip install accelerate`\")\n\n-        device = torch.device(\"cuda\")\n+        device = torch.device(f\"cuda:{gpu_id}\")\n\nfor cpu_offloaded_model in [self.unet, self.text_encoder, self.vae, self.safety_checker]:\nif cpu_offloaded_model is not None:\n", "fix_pattern": "In the condition of \"if gpu_id is not None\", if the pattern \"device = torch.device(\"cuda\")\" is detected, then change the code to \"device = torch.device(f\"cuda:{gpu_id}\") to fix the API misuse."}
{"number": 524, "change": "class TwoStageDetector(BaseDetector, RPNTestMixin, BBoxTestMixin,\nif self.with_rpn:\nrpn_outs = self.rpn_head(x)\nouts = outs + (rpn_outs, )\n-        proposals = torch.randn(1000, 4).cuda()\n+        proposals = torch.randn(1000, 4).to(device=img.device)\n# bbox head\nrois = bbox2roi([proposals])\nif self.with_bbox:\n", "fix_pattern": "In the condition of \"if self.with_bbox:\", if the pattern \"cuda()\" is detected, then change the code to \"to(device=img.device)\" to fix the API misuse."}
{"number": 526, "change": "class PGModel(Model):\nactions = np.concatenate([path['actions'] for path in batch])\nbatch_advantage = np.concatenate([path[\"advantage\"] for path in batch])\nbatch_advantage = zero_mean_unit_variance(batch_advantage)\n+        batch_advantage = np.expand_dims(batch_advantage, axis=1)\nstates = np.concatenate([path['states'] for path in batch])\n\nreturn action_log_stds, action_means, actions, batch_advantage, states\n", "fix_pattern": "In the condition of having a batch_advantage variable, if the pattern of not having an axis parameter in np.expand_dims() is detected, then add axis=1 to the np.expand_dims() function to fix the API misuse."}
{"number": 529, "change": "class Categorical(Distribution):\nelif one_hot:\nboolean_mask = x\nelse:\n-                boolean_mask = torch.zeros(ps.size()).scatter_(-1, x.data.long(), 1)\n+                boolean_mask = torch_zeros_like(ps.data).scatter_(-1, x.data.long(), 1)\n# apply log function to masked probability tensor\nreturn torch.log(ps.masked_select(boolean_mask.byte()).contiguous().view(*batch_pdf_size))\n", "fix_pattern": "In the condition of checking if the variable \"one_hot\" is true, if the pattern of using \"torch_zeros_like\" instead of \"torch.zeros\" is detected, then change the code from \"torch.zeros\" to \"torch_zeros_like\" to fix the API misuse."}
{"number": 530, "change": "class ViTMAEModelIntegrationTest(unittest.TestCase):\n\n# forward pass\nwith torch.no_grad():\n-            outputs = model(**inputs, noise=torch.from_numpy(noise))\n+            outputs = model(**inputs, noise=torch.from_numpy(noise).to(device=torch_device))\n\n# verify the logits\nexpected_shape = torch.Size((1, 196, 768))\n", "fix_pattern": "In the condition of calling the torch.from_numpy() function with an argument, if the code does not include the \".to()\" method for specifying the device, then add \".to(device=torch_device)\" to fix the API misuse."}
{"number": 532, "change": "def initialize_vocabulary(vocabulary_path):\nrev_vocab = []\nwith gfile.GFile(vocabulary_path, mode=\"rb\") as f:\nrev_vocab.extend(f.readlines())\n-    rev_vocab = [line.strip() for line in rev_vocab]\n+    rev_vocab = [tf.compat.as_bytes(line.strip()) for line in rev_vocab]\nvocab = dict([(x, y) for (y, x) in enumerate(rev_vocab)])\nreturn vocab, rev_vocab\nelse:\n", "fix_pattern": "In the condition of using `gfile.GFile` to read lines from a file, if `tf.compat.as_bytes` is not used on the lines before creating the vocabulary dictionary, then the code should be modified to include `tf.compat.as_bytes` to fix the API misuse."}
{"number": 534, "change": "class ConformerSeparator(AbsSeparator):\n\"\"\"\n\n# if complex spectrum,\n-        if isinstance(input, ComplexTensor):\n+        if isinstance(input, ComplexTensor) or (\n+            is_torch_1_8_plus and torch.is_complex(input)\n+        ):\nfeature = abs(input)\nelse:\nfeature = input\n", "fix_pattern": "In the condition of \"if isinstance(input, ComplexTensor)\", if an additional pattern of \"is_torch_1_8_plus and torch.is_complex(input)\" is detected, then add \"or (is_torch_1_8_plus and torch.is_complex(input))\" to fix the API misuse."}
{"number": 537, "change": "def batch_flatten(x):\n'''Turn a n-D tensor into a 2D tensor where\nthe first dimension is conserved.\n'''\n-    x = tf.reshape(x, [-1, prod(shape(x)[1:])])\n+    x = tf.reshape(x, tf.pack([-1, prod(shape(x)[1:])]))\nreturn x\n", "fix_pattern": "In the condition of using tf.reshape(), if a single value is being passed as the second argument, then change it to tf.pack([ ]) to fix the API misuse."}
{"number": 538, "change": "def distributed_train(local_rank: int, main_address: str, main_port: int, num_no\n# 2. PREPARE DISTRIBUTED MODEL\nmodel = torch.nn.Linear(32, 2)\ndevice = torch.device(f\"cuda:{local_rank}\") if torch.cuda.is_available() else torch.device(\"cpu\")\n-    model = DistributedDataParallel(model, device_ids=[local_rank]).to(device)\n+    model = DistributedDataParallel(model, device_ids=[local_rank] if torch.cuda.is_available() else None).to(device)\n\n# 3. SETUP LOSS AND OPTIMIZER\ncriterion = torch.nn.MSELoss()\n", "fix_pattern": "In the condition of \"torch.cuda.is_available()\", if the pattern \"device_ids=[local_rank]\" is detected, then change the code to \"device_ids=[local_rank] if torch.cuda.is_available() else None\" to fix the API misuse."}
{"number": 548, "change": "class VisionTransformer(nn.Module):\n\ndef forward(self, x):\nx = self.forward_features(x)\n-        if isinstance(x, tuple):\n-            x, x_dist = self.head(x[0]), self.head_dist(x[1])\n+        if self.head_dist is not None:\n+            x, x_dist = self.head(x[0]), self.head_dist(x[1])  # x must be a tuple\nif self.training and not torch.jit.is_scripting():\n# during inference, return the average of both classifier predictions\nreturn x, x_dist\n", "fix_pattern": "In the condition of `if self.head_dist is not None`, if the pattern of `x` being a tuple is detected, then the code `x, x_dist = self.head(x[0]), self.head_dist(x[1])` is added to fix the API misuse."}
{"number": 549, "change": "class TFKerasUtil(object):\n\ndataset = dataset.batch(batch_size).map(prep_data_tf_keras)\nreturn dataset\n-        return fn\n+        return tf.autograph.experimental.do_not_convert(fn) if _HAS_AUTOGRAPH else fn\n\n@staticmethod\ndef get_horovod():\n", "fix_pattern": "In the condition of tf.autograph.experimental.do_not_convert being available (_HAS_AUTOGRAPH), if an API misuse is detected, then add tf.autograph.experimental.do_not_convert() to fix the code."}
{"number": 554, "change": "with tf.device('/cpu:0'):\nnet = FlattenLayer(net, name='flatten')\nnet = DenseLayer(net, 384, act=tf.nn.relu, W_init=W_init2, b_init=b_init2, name='d1relu')\nnet = DenseLayer(net, 192, act=tf.nn.relu, W_init=W_init2, b_init=b_init2, name='d2relu')\n-            net = DenseLayer(net, n_units=10, act=tf.identity, W_init=W_init2, name='output')\n+            net = DenseLayer(net, n_units=10, act=None, W_init=W_init2, name='output')\ny = net.outputs\n\nce = tl.cost.cross_entropy(y, y_, name='cost')\n", "fix_pattern": "in the condition of `net` being initialized as a dense layer with `n_units=10`, if the activation function is set to `tf.identity`, then change it to `None` to fix the API misuse."}
{"number": 557, "change": "class up(nn.Module):\nif bilinear:\nself.up = nn.UpsamplingBilinear2d(scale_factor=2)\nelse:\n-            self.up = nn.ConvTranspose2d(in_ch, out_ch, 2, stride=2)\n+            self.up = nn.ConvTranspose2d(in_ch//2, in_ch//2, 2, stride=2)\n\nself.conv = double_conv(in_ch, out_ch)\n", "fix_pattern": "In the condition of \"if bilinear\", if \"in_ch\" is detected, then change \"in_ch\" to \"in_ch//2\" to fix the API misuse."}
{"number": 559, "change": "class Metric(nn.Module, ABC):\nAutomatically calls ``update()``. Returns the metric value over inputs if ``compute_on_step`` is True.\n\"\"\"\n# add current step\n-        self.update(*args, **kwargs)\n+        with torch.no_grad():\n+            self.update(*args, **kwargs)\nself._forward_cache = None\n\nif self.compute_on_step:\n", "fix_pattern": "Fix_pattern: In the condition of `if self.compute_on_step`, if the pattern of missing `torch.no_grad()` is detected, then add `with torch.no_grad():` to fix the API misuse."}
{"number": 560, "change": "temperature = max(args.temperature, 1e-3)\nwith open(args.outf, 'w') as outf:\nfor i in range(args.nwords):\n\n-        output, hidden = model(Variable(input, requires_grad=False), hidden)\n-        gen = torch.multinomial(output[0].data.cpu().div(temperature).exp(), 1)[0][0] # FIXME: no multinomial on GPU?\n+        output, hidden = model(Variable(input, volatile=True), hidden)\n+        gen = torch.multinomial(output[0].data.div(temperature).exp().cpu(), 1)[0][0] # FIXME: multinomial is only for CPU\ninput.fill_(gen)\nword = corpus.dic.idx2word[gen]\noutf.write(word)\n", "fix_pattern": "In the condition of \"running multinomial on GPU\", if \"multinomial is called on GPU\", then change \"multinomial\" to \"multinomial(...).cpu()\" to fix the API misuse."}
{"number": 561, "change": "class TFFunnelForMultipleChoice(TFFunnelPreTrainedModel, TFMultipleChoiceLoss):\n@tf.function(\ninput_signature=[\n{\n-                \"input_ids\": tf.TensorSpec((None, None), tf.int64, name=\"input_ids\"),\n+                \"input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"input_ids\"),\n\"attention_mask\": tf.TensorSpec((None, None), tf.float32, name=\"attention_mask\"),\n-                \"token_type_ids\": tf.TensorSpec((None, None), tf.int64, name=\"token_type_ids\"),\n+                \"token_type_ids\": tf.TensorSpec((None, None), tf.int32, name=\"token_type_ids\"),\n}\n]\n)\n", "fix_pattern": "in the condition of checking the input signature for the TF function, if the data type of the \"input_ids\" and \"token_type_ids\" tensors is \"int64\", then change it to \"int32\" to fix the API misuse."}
{"number": 566, "change": "class ModelSaver(Callback):\nself.var_collections = var_collections\nif checkpoint_dir is None:\ncheckpoint_dir = logger.get_logger_dir()\n-        assert checkpoint_dir is not None\n-        if not tf.gfile.IsDirectory(checkpoint_dir):\n-            tf.gfile.MakeDirs(checkpoint_dir)\n+        if checkpoint_dir is not None:\n+            if not tf.gfile.IsDirectory(checkpoint_dir):\n+                tf.gfile.MakeDirs(checkpoint_dir)\nself.checkpoint_dir = checkpoint_dir\n\ndef _setup_graph(self):\n+        assert self.checkpoint_dir is not None, \\\n+            \"ModelSaver() doesn't have a valid checkpoint directory.\"\nvars = []\nfor key in self.var_collections:\nvars.extend(tf.get_collection(key))\n", "fix_pattern": "In the condition of checking if the checkpoint directory is None, if the pattern of nested if statements with a validation check is detected, then remove the unnecessary if statement to fix the API misuse."}
{"number": 571, "change": "class ModelCheckpoint(Callback):\nself.best_k_models.pop(del_filepath)\n\n# do not save nan, replace with +/- inf\n-        if torch.isnan(current):\n+        if isinstance(current, torch.Tensor) and torch.isnan(current):\ncurrent = torch.tensor(float('inf' if self.mode == \"min\" else '-inf'))\n\nfilepath = self._get_metric_interpolated_filepath_name(ckpt_name_metrics, epoch, step, del_filepath)\n", "fix_pattern": "In the condition of checking if the current variable is NaN, if the current variable is a torch.Tensor and is NaN, the code was modified to add an isinstance check before the torch.isnan() check to fix the API misuse."}
{"number": 574, "change": "class Graph(kerastuner.HyperModel, serializable.Serializable):\n\ndef build(self, hp):\n\"\"\"Build the HyperModel into a Keras Model.\"\"\"\n-        tf.keras.backend.clear_session()\nself._register_hps(hp)\nself.compile()\nreal_nodes = {}\n", "fix_pattern": "In the condition of extending a class and overriding a method, if unnecessary code is detected in the method, then remove the unnecessary code to fix the API misuse."}
{"number": 578, "change": "class GroupViTVisionTransformer(nn.Module):\n\nself.embeddings = GroupViTVisionEmbeddings(config)\nself.encoder = GroupViTVisionEncoder(config)\n-        self.layernorm = nn.LayerNorm(embed_dim)\n+        self.layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n\n@add_start_docstrings_to_model_forward(GROUPVIT_VISION_INPUTS_DOCSTRING)\n@replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=GroupViTVisionConfig)\n", "fix_pattern": "In the condition of initializing an instance of the `LayerNorm` class, if the argument `eps` is not provided, then add the argument `eps=config.layer_norm_eps` to fix the API misuse."}
{"number": 585, "change": "class TestTrainSampleHook(tf.test.TestCase):\npred_dict = {}\npred_dict[\"predicted_tokens\"] = tf.constant([[\"Hello\", \"World\", \"\u7b11w\"]])\npred_dict[\"labels.target_tokens\"] = tf.constant([[\"Hello\", \"World\", \"\u7b11w\"]])\n-    pred_dict[\"labels.target_len\"] = tf.constant([2]),\n+    pred_dict[\"labels.target_len\"] = tf.constant(2),\ngraph_utils.add_dict_to_collection(pred_dict, \"predictions\")\n\ndef tearDown(self):\n", "fix_pattern": "In the condition of instantiating a constant tensor with tf.constant(), if the pattern of passing a list with a single element as the value is detected, then change the code to pass the single element directly to the constant() function to fix the API misuse."}
{"number": 587, "change": "class VonMises(TorchDistribution):\n\"\"\"\nshape = self._extended_shape(sample_shape)\nx = torch.empty(shape, dtype=self.loc.dtype, device=self.loc.device)\n-        done = torch.zeros(shape, dtype=self.loc.dtype, device=self.loc.device).byte()\n+        done = torch.zeros(shape, dtype=self.loc.dtype, device=self.loc.device).bool()\nwhile not done.all():\nu = torch.rand((3,) + shape, dtype=self.loc.dtype, device=self.loc.device)\nu1, u2, u3 = u.unbind()\n", "fix_pattern": "In the condition of while loop, if the pattern of using \".byte()\" to convert a tensor to a boolean tensor is detected, then change it to \".bool()\" to fix the API misuse."}
{"number": 588, "change": "class GridTest(TestCase):\nassert_equal(adj.to_dense().numpy(), expected_adj)\n\ndef test_grid_with_connectivity_8(self):\n-        adj = grid(torch.Size([3, 2]), connectivity=8)\n+        adj = grid_3x3(torch.Size([3, 2]), connectivity=8)\n\nexpected_adj = [\n[0, 1, 1, 2, 0, 0],\n", "fix_pattern": "In the condition of changing the function name from \"grid\" to \"grid_3x3\", if the expected_adj test data remains the same, then the code change fixes the API misuse by correctly calling the updated function."}
{"number": 591, "change": "class Model(ModelDesc):\nself.last_state = tf.identity(last_state, 'last_state')\n\n# seqlen x (Bxrnnsize)\n-        output = tf.reshape(tf.concat(1, outputs), [-1, param.rnn_size])  # (Bxseqlen) x rnnsize\n+        output = tf.reshape(tf.concat_v2(outputs, 1), [-1, param.rnn_size])  # (Bxseqlen) x rnnsize\nlogits = FullyConnected('fc', output, param.vocab_size, nl=tf.identity)\nself.prob = tf.nn.softmax(logits / param.softmax_temprature)\n", "fix_pattern": "in the condition of using tf.concat function, if the function tf.concat is called with arguments (outputs, 1), then change tf.concat to tf.concat_v2 to fix the API misuse."}
{"number": 595, "change": "def fit_line(points: Tensor, weights: Optional[Tensor] = None) -> ParametrizedLi\nA = A.transpose(-2, -1) @ A\n\n# NOTE: not optimal for 2d points, but for now works for other dimensions\n-    _, _, V = torch.linalg.svd(A)\n+    _, _, V = _torch_svd_cast(A)\n+    V = V.transpose(-2, -1)\n\n# the first left eigenvector is the direction on the fited line\ndirection = V[..., 0, :]  # BxD\n", "fix_pattern": "in the condition of `torch.linalg.svd` function call, if incorrect transpose operation order is detected, then change the order of transpose operation to fix the API misuse."}
{"number": 596, "change": "def ndim(x):\n'''Returns the number of axes in a tensor, as an integer.\n'''\nif is_sparse(x):\n-        return int(x.shape.get_shape()[0])\n+        return x._dims\n\ndims = x.get_shape()._dims\nif dims is not None:\n", "fix_pattern": "Fix_pattern: \nin the condition of `is_sparse(x)`, if `x.shape.get_shape()[0]` is detected, then remove `int()` from the return statement to fix the API misuse."}
{"number": 601, "change": "class Model(ModelDesc):\ninput_list = tf.unstack(input_feature, axis=1)  # seqlen x (Bxrnnsize)\n\n# seqlen is 1 in inference. don't need loop_function\n-        outputs, last_state = tf.nn.rnn(cell, input_list, initial, scope='rnnlm')\n+        outputs, last_state = tf.contrib.rnn.static_rnn(cell, input_list, initial, scope='rnnlm')\nself.last_state = tf.identity(last_state, 'last_state')\n\n# seqlen x (Bxrnnsize)\n", "fix_pattern": "In the condition of using the `rnn` function in TensorFlow, if the `tf.nn.rnn` function is detected, then change it to `tf.contrib.rnn.static_rnn` to fix the API misuse."}
{"number": 611, "change": "class Attention(nn.Module):\nquery, processed_inputs)\n# apply masking\nif mask is not None:\n-            attention.data.masked_fill_(torch.bitwise_not(mask), self._mask_value)\n+            attention.data.masked_fill_(~mask, self._mask_value)\n# apply windowing - only in eval mode\nif not self.training and self.windowing:\nattention = self.apply_windowing(attention, inputs)\n", "fix_pattern": "In the condition of checking for a mask, if the bitwise_not operator is used, then change it to the logical not operator to fix the API misuse."}
{"number": 613, "change": "class EarlyStopping(Callback):\nf\" {self.monitor} = {current} {self.order_dict[self.mode]} {self.divergence_threshold}.\"\n\" Signaling Trainer to stop.\"\n)\n-        elif self.monitor_op(current - self.min_delta, self.best_score):\n+        elif self.monitor_op(current - self.min_delta, self.best_score.to(trainer.lightning_module.device)):\nshould_stop = False\nreason = self._improvement_message(current)\nself.best_score = current\n", "fix_pattern": "In the condition of \"elif self.monitor_op(current - self.min_delta, self.best_score)\", if an API misuse is detected, then the code \"self.best_score\" is changed to \"self.best_score.to(trainer.lightning_module.device)\" to fix the API misuse."}
{"number": 614, "change": "class LSTM(Model):\nlast_layer = add_time_dimension(features, self.seq_lens)\n\n# Setup the LSTM cell\n-        lstm = rnn.BasicLSTMCell(cell_size, state_is_tuple=True)\n+        lstm = tf.nn.rnn_cell.LSTMCell(cell_size, state_is_tuple=True)\nself.state_init = [\nnp.zeros(lstm.state_size.c, np.float32),\nnp.zeros(lstm.state_size.h, np.float32)\n", "fix_pattern": "In the condition of an LSTM cell setup, if the usage of `rnn.BasicLSTMCell` is detected, then change the code to use `tf.nn.rnn_cell.LSTMCell` instead in order to fix the API misuse."}
{"number": 615, "change": "class XGLMModel(XGLMPreTrainedModel):\n\nhidden_states = inputs_embeds + positions\n\n-        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n+        hidden_states = nn.functional.dropout(hidden_states, p=float(self.dropout), training=self.training)\n\n# decoder layers\nall_hidden_states = () if output_hidden_states else None\n", "fix_pattern": "in the condition of checking the dropout value, if the value is not a float, then change the code from using the value directly to converting it to float to fix the API misuse."}
{"number": 618, "change": "class CategoricalAccuracy(Metric):\ncorrect.unsqueeze_(-1)\n\nif mask is not None:\n-            correct *= mask.view(-1, 1).float()\n+            correct *= mask.view(-1, 1)\nself.total_count += mask.sum()\nelse:\nself.total_count += gold_labels.numel()\n", "fix_pattern": "In the condition of \"mask is not None\", if the pattern of \"float()\" is detected, then remove it to fix the API misuse."}
{"number": 621, "change": "def conditional(\nif f_scale_tril is not None:\npack = torch.cat((pack, f_scale_tril_2D), dim=1)\n\n-        Lffinv_pack = pack.triangular_solve(Lff, upper=False)[0]\n+        Lffinv_pack = torch.linalg.solve_triangular(Lff, pack, upper=False)\n# unpack\nv_2D = Lffinv_pack[:, : f_loc_2D.size(1)]\nW = Lffinv_pack[:, f_loc_2D.size(1) : f_loc_2D.size(1) + M].t()\n", "fix_pattern": "In the condition of checking if the variable \"f_scale_tril\" is not None, if the pattern of using \"pack.triangular_solve\" is detected, then change the code to use \"torch.linalg.solve_triangular\" to fix the API misuse."}
{"number": 623, "change": "class Model(ModelDesc):\nwrong = prediction_incorrect(logits, label, 5, name='wrong-top5')\nadd_moving_summary(tf.reduce_mean(wrong, name='train-error-top5'))\n\n-        wd_cost = tf.mul(1e-4, regularize_cost('.*/W', tf.nn.l2_loss), name='l2_regularize_loss')\n+        wd_cost = regularize_cost('.*/W', l2_regularizer(1e-4), name='l2_regularize_loss')\nadd_moving_summary(loss, wd_cost)\nself.cost = tf.add_n([loss, wd_cost], name='cost')\n", "fix_pattern": "In the condition of the l2 regularization cost calculation, if the regularize_cost() function is being used with the wrong parameters, then change the code to use the l2_regularizer() function instead to fix the API misuse."}
{"number": 627, "change": "class Optimizer:\ng = [dev_grads[dev][var_idx][0] for dev in devices]\n\nif np.prod(grad_shape):  # nccl does not support zero-sized tensors\n-                            g = tf.contrib.nccl.all_sum(g)\n+                            g = nccl_ops.all_sum(g)\n\nfor dev, gg in zip(devices, g):\ndev_grads[dev][var_idx] = (gg, dev_grads[dev][var_idx][1])\n", "fix_pattern": "In the condition of \"if np.prod(grad_shape):\", if the pattern \"tf.contrib.nccl.all_sum\" is detected, then change the \"tf.contrib.nccl.all_sum\" to \"nccl_ops.all_sum\" to fix the API misuse."}
{"number": 639, "change": "class LinearModel(object):\nreturn self.sess.run(self.cross_entropy_grads, feed_dict={self.x: xs, self.y_: ys})\n\ndef net_initialization():\n-  return LinearModel([784,10])\n+  with tf.Graph().as_default():\n+    return LinearModel([784,10])\n\n# By default, when an environment variable is used by a remote function, the\n# initialization code will be rerun at the end of the remote task to ensure\n", "fix_pattern": "In the condition of instantiating a TensorFlow graph and returning an object of a LinearModel class, if the code is missing the graph initialization, then add the line \"with tf.Graph().as_default():\" before the LinearModel instantiation to fix the API misuse."}
{"number": 640, "change": "class EpochResultStore:\n# attach capture batch_size\nResult.attach_batch_size(self._batch_size, hook_result)\n\n-            hook_result.detach()\n+            hook_result = hook_result.detach()\nif self.trainer.move_metrics_to_cpu:\n-                hook_result.cpu()\n+                hook_result = hook_result.cpu()\nelif self.trainer._distrib_type == DistributedType.DP:\n-                hook_result.to(torch.device(\"cuda\", self.trainer.root_gpu))\n+                hook_result = hook_result.to(torch.device(\"cuda\", self.trainer.root_gpu))\n\nself._internals[fx_name].append(hook_result, info)\n", "fix_pattern": "In the condition of \"if self.trainer.move_metrics_to_cpu\", if the code \"hook_result.cpu()\" is detected, then change the code to \"hook_result = hook_result.cpu()\" to fix the API misuse. Additionally, in the condition of \"elif self.trainer._distrib_type == DistributedType.DP\", if the code \"hook_result.to(torch.device(\"cuda\", self.trainer.root_gpu))\" is detected, then change the code to \"hook_result = hook_result.to(torch.device(\"cuda\", self.trainer.root_gpu))\" to fix the API misuse."}
{"number": 641, "change": "class LinearRegression(d2l.Module):\ndef __init__(self, lr):\nsuper().__init__()\nself.save_hyperparameters()\n-        self.net = tf.keras.layers.Dense(1)\n+        initializer = tf.initializers.RandomNormal(stddev=0.01)\n+        self.net = tf.keras.layers.Dense(1, kernel_initializer=initializer)\n\ndef forward(self, X):\n\"\"\"The linear regression model.\n", "fix_pattern": "In the condition of initializing a dense layer in a linear regression model, if the initialization of the kernel is missing, then add the code \"kernel_initializer=initializer\" to fix the API misuse."}
{"number": 643, "change": "class TestPretrainedTransformerEmbedder(AllenNlpTestCase):\ndef test_xlnet_token_type_ids(self):\ntoken_embedder = PretrainedTransformerEmbedder(\"xlnet-base-cased\")\ntoken_ids = torch.LongTensor([[1, 2, 3], [2, 3, 4]])\n-        mask = torch.ones_like(token_ids)\n+        mask = torch.ones_like(token_ids).bool()\ntype_ids = torch.zeros_like(token_ids)\ntype_ids[1, 1] = 1\ntoken_embedder(token_ids, mask, type_ids)\n", "fix_pattern": "In the condition of \"making a tensor of ones like another tensor\", if \".bool()\" is not present, add \".bool()\" to convert the tensor to a boolean type to fix the API misuse."}
{"number": 645, "change": "class Block(Layer):\nlayer_counter[layer_type] += 1\n\n# layer_name = self.name + '-' + layer_name\n-            self.layers[n] = self.submodule(\n+            layer = self.submodule(\nname=layer_name, module=layer_spec, modules=tensorforce.core.layer_modules,\ninput_spec=self._input_spec\n)\n-            self._input_spec = self.layers[n].output_spec()\n-\n+            self.layers.append(layer)\n+            self._input_spec = layer.output_spec()\n\nreturn self.layers[0].input_spec.copy()\n", "fix_pattern": "In the condition of adding layers to a block, if the goal is to append the layer to the block's list of layers, then change the code from setting the specific index of the list to appending the layer to the list. This fixes the API misuse of setting an index that may not exist."}
{"number": 648, "change": "def model():\n\nif sd_vae_approx_model is None:\nsd_vae_approx_model = VAEApprox()\n-        sd_vae_approx_model.load_state_dict(torch.load(os.path.join(paths.models_path, \"VAE-approx\", \"model.pt\")))\n+        sd_vae_approx_model.load_state_dict(torch.load(os.path.join(paths.models_path, \"VAE-approx\", \"model.pt\"), map_location='cpu' if devices.device.type != 'cuda' else None))\nsd_vae_approx_model.eval()\nsd_vae_approx_model.to(devices.device, devices.dtype)\n", "fix_pattern": "In the condition of checking if a variable is None, if an API misuse pattern is detected (lack of an argument), then add the missing argument to the code to fix the API misuse."}
{"number": 655, "change": "class {{cookiecutter.camelcase_modelname}}EncoderLayer(nn.Module):\nhidden_states = residual + hidden_states\nhidden_states = self.final_layer_norm(hidden_states)\n\n-        if torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any():\n+        if hidden_states.dtype == torch.float16 and (torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any()):\nclamp_value = torch.finfo(hidden_states.dtype).max - 1000\nhidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n", "fix_pattern": "In the condition of hidden_states.dtype == torch.float16, if the pattern of torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any() is detected, then the code is changed to add the check for hidden_states.dtype == torch.float16 to fix the API misuse."}
{"number": 656, "change": "class LabelSmoother:\n\ndef __call__(self, model_output, labels):\nlogits = model_output[\"logits\"] if isinstance(model_output, dict) else model_output[0]\n-        log_probs = -torch.nn.functional.log_softmax(logits, dim=-1)\n+        log_probs = -nn.functional.log_softmax(logits, dim=-1)\nif labels.dim() == log_probs.dim() - 1:\nlabels = labels.unsqueeze(-1)\n", "fix_pattern": "In the condition of checking the dimensions of labels and log_probs, if a pattern of incorrect API usage is detected (using the torch.nn.functional module instead of nn.functional), the code is changed to fix the API misuse by removing \"torch.\" from the code."}
{"number": 661, "change": "class Model(ModelDesc):\n.apply(fg)\n.BatchNorm('bn5').apply(activate)\n# 5\n-                      .tf.nn.dropout(0.5 if is_training else 1.0)\n+                      .Dropout(rate=0.5 if is_training else 0.0)\n.Conv2D('conv6', 512, 5, padding='VALID')\n.apply(fg).BatchNorm('bn6')\n.apply(nonlin)\n", "fix_pattern": "In the condition of 'is_training', if the pattern 'tf.nn.dropout' is detected, then change the code to 'Dropout(rate=0.5 if is_training else 0.0)' to fix the API misuse."}
{"number": 665, "change": "class BLEU(Metric):\nreturn math.exp(1.0 - self._reference_lengths / self._prediction_lengths)\n\ndef _get_valid_tokens_mask(self, tensor: torch.LongTensor) -> torch.ByteTensor:\n-        valid_tokens_mask = torch.ones(tensor.size(), dtype=torch.uint8)\n+        valid_tokens_mask = torch.ones(tensor.size(), dtype=torch.bool)\nfor index in self._exclude_indices:\nvalid_tokens_mask = valid_tokens_mask & (tensor != index)\nreturn valid_tokens_mask\n", "fix_pattern": "in the condition of changing the data type of a tensor to torch.bool, if a pattern of using torch.uint8 is detected, then change the code to use torch.bool to fix the API misuse."}
{"number": 672, "change": "class MessagePassing(torch.nn.Module):\nthe_size: List[Optional[int]] = [None, None]\n\nif isinstance(edge_index, Tensor):\n-            assert edge_index.dtype == torch.long\n-            assert edge_index.dim() == 2\n-            assert edge_index.size(0) == 2\n+            assert edge_index.dtype == torch.long, \\\n+                \"edge_index.dtype is not of torch.long\"\n+            assert edge_index.dim() == 2, \\\n+                \"edge_index.dim() is not equal to 2\"\n+            assert edge_index.size(0) == 2, \\\n+                \"edge_index.size(0) is not equal to 2\"\nif size is not None:\nthe_size[0] = size[0]\nthe_size[1] = size[1]\n", "fix_pattern": "In the condition of checking if `edge_index` is an instance of `Tensor`, if the pattern `assert <condition>, <message>` is detected, then change the code to include the error message in the assertion to fix the API misuse."}
{"number": 674, "change": "class DecoderLayer(nn.Module):\nif self.normalize_before:\nx = self.norm2(x)\nif self.concate_after:\n-            x_concat = torch.cat(x, self.src_attn(x, memory, memory, memory_mask))\n+            x_concat = torch.cat((x, self.src_attn(x, memory, memory, memory_mask)), dim=-1)\nx = residual + self.concate_linear2(x_concat)\nelse:\nx = residual + self.dropout(self.src_attn(x, memory, memory, memory_mask))\n", "fix_pattern": "Fix pattern: \n\nIn the condition of concate_after, if pattern of torch.cat(x, ...) is detected, then change the code to torch.cat((x, ...), dim=-1) to fix the API misuse."}
{"number": 677, "change": "def clip(\n*,\nout: Optional[torch.Tensor] = None,\n) -> torch.Tensor:\n-    assert torch.all(torch.less(x_min, x_max)), \"Min value must be less than max.\"\n+    assert torch.all(\n+        torch.less(torch.tensor(x_min), x_max)\n+    ), \"Min value must be less than max.\"\nif hasattr(x_min, \"dtype\"):\npromoted_type = torch.promote_types(x_min.dtype, x_max.dtype)\npromoted_type = torch.promote_types(promoted_type, x.dtype)\n", "fix_pattern": "In the condition of checking if all the values in a tensor are less than a certain value, if the pattern of passing a variable directly instead of wrapping it in `torch.tensor()` is detected, then replace the code to wrap the variable in `torch.tensor()` to fix the API misuse."}
{"number": 681, "change": "class LayoutLMv3Model(LayoutLMv3PreTrainedModel):\nposition_ids = position_ids.expand_as(input_ids)\nfinal_position_ids = position_ids\n\n-        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, None, device)\n+        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(\n+            attention_mask, None, device, dtype=embedding_output.dtype\n+        )\n\n# Prepare head mask if needed\n# 1.0 in head_mask indicate we keep the head\n", "fix_pattern": "in the condition of getting an extended attention mask, if the dtype of the embedding output is not specified, then add the dtype argument to the get_extended_attention_mask() function to fix the API misuse."}
{"number": 683, "change": "class StableDiffusionKDiffusionPipeline(DiffusionPipeline):\nreturn_tensors=\"pt\",\n)\ntext_input_ids = text_inputs.input_ids\n-        untruncated_ids = self.tokenizer(prompt, padding=\"max_length\", return_tensors=\"pt\").input_ids\n+        untruncated_ids = self.tokenizer(prompt, padding=\"longest\", return_tensors=\"pt\").input_ids\n\n-        if not torch.equal(text_input_ids, untruncated_ids):\n+        if untruncated_ids.shape[-1] >= text_input_ids.shape[-1] and not torch.equal(text_input_ids, untruncated_ids):\nremoved_text = self.tokenizer.batch_decode(untruncated_ids[:, self.tokenizer.model_max_length - 1 : -1])\nlogger.warning(\n\"The following part of your input was truncated because CLIP can only handle sequences up to\"\n", "fix_pattern": "In the condition of checking the shape of untruncated_ids and text_input_ids, if the shape of untruncated_ids is larger or equal to text_input_ids and they are not equal, then the code was changed from padding=\"max_length\" to padding=\"longest\" to fix the API misuse."}
{"number": 693, "change": "class Ensemble(nn.ModuleList):\n\n\ndef attempt_load(weights, device=None, inplace=True, fuse=True):\n+    # Loads an ensemble of models weights=[a,b,c] or a single model weights=[a] or weights=a\nfrom models.yolo import Detect, Model\n\n-    # Loads an ensemble of models weights=[a,b,c] or a single model weights=[a] or weights=a\nmodel = Ensemble()\nfor w in weights if isinstance(weights, list) else [weights]:\n-        ckpt = torch.load(attempt_download(w), map_location=device)\n-        ckpt = (ckpt.get('ema') or ckpt['model']).float()  # FP32 model\n+        ckpt = torch.load(attempt_download(w), map_location='cpu')  # load\n+        ckpt = (ckpt.get('ema') or ckpt['model']).to(device).float()  # FP32 model\nmodel.append(ckpt.fuse().eval() if fuse else ckpt.eval())  # fused or un-fused model in eval mode\n\n# Compatibility updates\n", "fix_pattern": "in the condition of checking if weights is an instance of list, if the map_location argument in torch.load() is set to device variable, then change it to 'cpu' to fix the API misuse."}
{"number": 707, "change": "class TFLiteBackendInferenceLearner(TensorflowBaseInferenceLearner):\nself.interpreter.set_tensor(i, input_tensor)\nself.interpreter.invoke()\nreturn tuple(\n-            self.interpreter.get_tensor(output_detail[\"index\"])\n+            tf.convert_to_tensor(\n+                self.interpreter.get_tensor(output_detail[\"index\"])\n+            )\nfor output_detail in output_details\n)\n", "fix_pattern": "In the condition of calling the `interpreter.get_tensor` method, if the output should be converted to a TensorFlow tensor, the pattern is to add the `tf.convert_to_tensor` function around the call to `interpreter.get_tensor` to fix the API misuse."}
{"number": 710, "change": "class Model(ModelDesc):\n.Conv2D('conv3.1', filters=128, padding='VALID') \\\n.Conv2D('conv3.2', filters=128, padding='VALID') \\\n.FullyConnected('fc0', 1024 + 512, activation=tf.nn.relu) \\\n-                .tf.nn.dropout(keep_prob) \\\n+                .Dropout(rate=drop_rate) \\\n.FullyConnected('fc1', 512, activation=tf.nn.relu) \\\n.FullyConnected('linear', out_dim=self.cifar_classnum)()\n", "fix_pattern": "In the condition of using the Dropout function, if the \"keep_prob\" argument is used, then the code should be changed to \"rate=drop_rate\" to fix the API misuse."}
{"number": 720, "change": "class CustomConverter(object):\nxs_pad = pad_list([torch.from_numpy(x).float() for x in xs], 0).to(device, dtype=self.dtype)\n\nilens = torch.from_numpy(ilens).to(device)\n-        # NOTE: this is for multi-task learning (e.g., speech translation)\n-        ys_pad = pad_list([torch.from_numpy(np.array(y[0]) if isinstance(y, tuple) else y).long()\n+        # NOTE: this is for multi-output (e.g., speech translation)\n+        ys_pad = pad_list([torch.from_numpy(np.array(y[0][:]) if isinstance(y, tuple) else y).long()\nfor y in ys], self.ignore_id).to(device)\n\nreturn xs_pad, ilens, ys_pad\n", "fix_pattern": "In the condition of \"if isinstance(y, tuple)\", if the pattern \"np.array(y[0])\" is detected, then change the code to \"np.array(y[0][:])\" to fix the API misuse."}
{"number": 725, "change": "def run_benchmark(state):\n\n\ndef on_state_reset():\n-    tf.keras.backend.set_value(model.optimizer.lr, lr * hvd.size())\n+    opt.lr.assign(lr * hvd.size())\n\n\nstate = hvd.elastic.TensorFlowKerasState(model, opt, img_secs=[], iter=0, batch=0, warm=False)\n", "fix_pattern": "In the condition of `on_state_reset()`, if the pattern `tf.keras.backend.set_value(model.optimizer.lr, lr * hvd.size())` is detected, then change the code to `opt.lr.assign(lr * hvd.size())` to fix the API misuse."}
{"number": 728, "change": "def vector_to_skew_symmetric_matrix(\na2s = vector_expanded[..., 1:2, :]\na3s = vector_expanded[..., 2:3, :]\n# BS x 1 x 1\n-    zs = torch.zeros(batch_shape + [1, 1], device=vector.device)\n+    zs = torch.zeros(batch_shape + [1, 1], device=vector.device, dtype=vector.dtype)\n# BS x 1 x 3\nrow1 = torch.cat((zs, -a3s, a2s), -1)\nrow2 = torch.cat((a3s, zs, -a1s), -1)\n", "fix_pattern": "In the condition of creating a zeros tensor, if the dtype is not specified, then add the parameter dtype=<variable>."}
{"number": 730, "change": "from pyro.ops.einsum import contract\ndef _finfo(tensor):\n# This can be replaced with torch.finfo once it is available\n# https://github.com/pytorch/pytorch/issues/10742\n-    return np.finfo(torch.empty(torch.Size(), dtype=tensor.dtype).numpy().dtype)\n+    return np.finfo(torch.empty(torch.Size(), dtype=tensor.dtype, device=\"cpu\").numpy().dtype)\n\n\ndef _check_batch_dims_are_sensible(output_dims, nonoutput_ordinal):\n", "fix_pattern": "In the condition of creating a tensor using `torch.empty()`, if the `device` parameter was not specified, add `device=\"cpu\"` to fix the API misuse."}
{"number": 742, "change": "def stats(policy, train_batch):\n\"cur_lr\": tf.cast(policy.cur_lr, tf.float64),\n\"policy_loss\": policy.loss.pi_loss,\n\"entropy\": policy.loss.entropy,\n-        \"var_gnorm\": tf.global_norm(policy.model.trainable_variables()),\n+        \"var_gnorm\": tf.linalg.global_norm(policy.model.trainable_variables()),\n\"vf_loss\": policy.loss.vf_loss,\n\"vf_explained_var\": explained_variance(\ntf.reshape(policy.loss.value_targets, [-1]),\n", "fix_pattern": "Fix_pattern: In the condition of calling tf.global_norm(), if the API tf.linalg.global_norm() is detected, then replace tf.global_norm() with tf.linalg.global_norm() to fix the API misuse."}
{"number": 743, "change": "def test_tensorrt_torch(\nres_orig = tuple(model(*inputs_example))\nassert all(\n[\n-                    torch.allclose(res_tensor, res_orig_tensor, rtol=1e-01)\n+                    torch.allclose(\n+                        res_tensor.float(), res_orig_tensor, rtol=1e-01\n+                    )\nfor (res_tensor, res_orig_tensor) in zip(res, res_orig)\n]\n)\n", "fix_pattern": "Fix_pattern: \nIn the condition of testing tensorrt torch, if a float() conversion is missing in the res_tensor, then add the float() conversion to res_tensor in order to fix the API misuse."}
{"number": 744, "change": "class Attention(nn.Module):\n# Apply the attention mask\nw = w + attention_mask\n\n-        w = nn.Softmax(dim=-1)(w)\n+        w = nn.functional.softmax(w, dim=-1)\nw = self.attn_dropout(w)\n\n# Mask heads if we want to\n", "fix_pattern": "In the condition of using the Softmax function from the nn module, if the Softmax function is detected, then replace it with the softmax function from the functional module to fix the API misuse."}
{"number": 752, "change": "class DistributedGroupSampler(Sampler):\nif size > 0:\nindice = np.where(self.flag == i)[0]\nassert len(indice) == size\n-                indice = indice[list(torch.randperm(int(size),\n-                                                    generator=g))].tolist()\n+                # add .numpy() to avoid bug when selecting indice in parrots.\n+                # TODO: check whether torch.randperm() can be replaced by\n+                # numpy.random.permutation().\n+                indice = indice[list(\n+                    torch.randperm(int(size), generator=g).numpy())].tolist()\nextra = int(\nmath.ceil(\nsize * 1.0 / self.samples_per_gpu / self.num_replicas)\n", "fix_pattern": "In the condition of \"if size > 0\", if an API misuse pattern of selecting indices is detected using torch.randperm(), then \".numpy()\" is added to torch.randperm() to fix the API misuse."}
{"number": 759, "change": "def test_pair_norm(scale_individually):\nassert out1.size() == (100, 16)\n\nout2 = norm(torch.cat([x, x], dim=0), torch.cat([batch, batch + 1], dim=0))\n-    assert torch.allclose(out1, out2[:100])\n-    assert torch.allclose(out1, out2[100:])\n+    assert torch.allclose(out1, out2[:100], atol=1e-6)\n+    assert torch.allclose(out1, out2[100:], atol=1e-6)\n", "fix_pattern": "Fix_pattern: In the condition of comparing tensor values using `torch.allclose()`, if the `atol` argument is not provided, then add the `atol` argument with a specified value to fix the API misuse."}
{"number": 764, "change": "class BlenderbotSmallEncoderLayer(nn.Module):\nhidden_states = residual + hidden_states\nhidden_states = self.final_layer_norm(hidden_states)\n\n-        if torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any():\n+        if hidden_states.dtype == torch.float16 and (\n+            torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any()\n+        ):\nclamp_value = torch.finfo(hidden_states.dtype).max - 1000\nhidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n", "fix_pattern": "Fix_pattern:\nIn the condition of checking the data type of the hidden_states variable, if there is an occurrence of infinity or NaN values, then the code was modified to also check if the data type is torch.float16 to fix the API misuse."}
{"number": 774, "change": "class QuantLinear(nn.Module):\nx_int = x / prev_act_scaling_factor\n\nreturn (\n-            F.linear(x_int, weight=self.weight_integer, bias=self.bias_integer) * bias_scaling_factor,\n+            nn.functional.linear(x_int, weight=self.weight_integer, bias=self.bias_integer) * bias_scaling_factor,\nbias_scaling_factor,\n)\n", "fix_pattern": "Fix_pattern: In the condition of using the nn.Module class, if the usage of the F.linear function is detected, then change the code to use the nn.functional.linear function to fix the API misuse."}
{"number": 779, "change": "class Importance(TracePosterior):\n\"\"\"\nif self.log_weights:\nlog_w_norm = self.get_normalized_weights(log_scale=True)\n-            ess = torch.exp(-logsumexp(2*log_w_norm, 0))\n+            ess = torch.exp(-torch.logsumexp(2*log_w_norm, 0))\nelse:\nwarnings.warn(\"The log_weights list is empty, effective sample size is zero.\")\ness = 0\n", "fix_pattern": "In the condition of \"self.log_weights\", if the pattern \"logsumexp(2*log_w_norm, 0)\" is detected, then change the code to \"torch.logsumexp(2*log_w_norm, 0)\" to fix the API misuse."}
{"number": 782, "change": "def dnn(tensor_in, hidden_units, activation=tf.nn.relu, keep_prob=None):\nwith tf.variable_scope('dnn'):\nfor i, n_units in enumerate(hidden_units):\nwith tf.variable_scope('layer%d' % i):\n-                tensor_in = linear.linear(tensor_in, n_units, True)\n+                tensor_in = linear(tensor_in, n_units, True)\ntensor_in = activation(tensor_in)\nif keep_prob:\ntensor_in = tf.nn.dropout(tensor_in, keep_prob)\n", "fix_pattern": "In the condition of checking if the `keep_prob` is not None, the fix pattern is to remove `linear.linear` and replace it with `linear` to fix the API misuse."}
{"number": 783, "change": "class DSClipEncoder(torch.nn.Module):\nseq_len,\nseq_len,\ndtype=dtype,\n-                           device=torch.cuda.current_device())\n+                           device=get_accelerator().current_device_name())\nmask.fill_(torch.tensor(torch.finfo(dtype).min))\nmask.triu_(1)\nmask = mask.unsqueeze(1)\n", "fix_pattern": "Fix_pattern: \nin the condition of \"using the device argument in torch.cuda.current_device()\", if \"torch.cuda.current_device()\" is detected, then change the code to \"get_accelerator().current_device_name()\" to fix the API misuse."}
{"number": 789, "change": "class AttentionDecoder(DecoderBase):\n])\nelse:\nattention_context = output.attention_context\n-    return tf.concat(1, [next_input, attention_context])\n+    return tf.concat_v2([next_input, attention_context], 1)\n\ndef _pad_att_scores(self, scores):\n\"\"\"Pads attention scores to fixed length. This is a hack because raw_rnn\n", "fix_pattern": "in the condition of 'else', if the pattern 'tf.concat(1, [next_input, attention_context])' is detected, then change the code to 'tf.concat_v2([next_input, attention_context], 1)' to fix the API misuse."}
{"number": 791, "change": "def test_graph_saint():\nassert sample.node_norm.numel() == sample.num_nodes\nassert sample.edge_norm.numel() == sample.num_edges\n\n+    torch.manual_seed(12345)\nloader = GraphSAINTRandomWalkSampler(data, batch_size=2, walk_length=1,\n-                                         num_steps=4, log=False)\n+                                         num_steps=4, sample_coverage=10,\n+                                         log=False)\n\nfor sample in loader:\nassert len(sample) == 4\n", "fix_pattern": "in the condition of \"test_graph_saint function\", if there are missing arguments in the GraphSAINTRandomWalkSampler constructor, then add the missing arguments in the constructor to fix the API misuse."}
{"number": 792, "change": "class GPTJAttention(nn.Module):\n):\n# compute causal mask from causal mask buffer\nquery_length, key_length = query.size(-2), key.size(-2)\n-        causal_mask = self.bias[:, :, key_length - query_length : key_length, :key_length].to(torch.bool)\n+        causal_mask = self.bias[:, :, key_length - query_length : key_length, :key_length]\n\n# Keep the attention weights computation in fp32 to avoid overflow issues\nquery = query.to(torch.float32)\n", "fix_pattern": "Fix_pattern: \nIn the condition of computing attention weights, if the bias is already of type torch.bool, then remove the \".to(torch.bool)\" method call to fix the API misuse"}
{"number": 800, "change": "class GeneralizedChannelPermute(ConditionedGeneralizedChannelPermute, TransformM\nself.__delattr__('permutation')\n\n# Sample a random orthogonal matrix\n-        W, _ = torch.qr(torch.randn(channels, channels))\n+        W, _ = torch.linalg.qr(torch.randn(channels, channels))\n\n# Construct the partially pivoted LU-form and the pivots\nLU, pivots = W.lu()\n", "fix_pattern": "In the condition of using the `torch.qr()` function, if the `torch.linalg.qr()` function is detected, then change the code to `torch.linalg.qr()` to fix the API misuse."}
{"number": 803, "change": "def model(x, is_train, reuse):\n# nt = Conv2d(nin, 16, (3, 3), (2, 2), act=tf.nn.relu, padding='SAME', name='tc1')\n# nt = Conv2d(nt, 8, (3, 3), (2, 2), act=tf.nn.relu, padding='SAME', name='tc2')\n## 2. Spatial transformer module (sampler)\n-        n = tl.layers.SpatialTransformer2dAffineLayer(nin, theta_layer=nt, out_size=[40, 40], name='spatial')\n+        n = tl.layers.SpatialTransformer2dAffineLayer(nin, theta_layer=nt, out_size=(40, 40), name='spatial')\ns = n\n## 3. Classifier\nn = tl.layers.Conv2d(\n", "fix_pattern": "In the condition of mapping out a spatial transformer module, if the pattern for setting the out_size as a list is detected, then change the code to set it as a tuple in order to fix the API misuse."}
{"number": 804, "change": "class DeiTPreTrainedModel(PreTrainedModel):\ndef _init_weights(self, module: Union[nn.Linear, nn.Conv2d, nn.LayerNorm]) -> None:\n\"\"\"Initialize the weights\"\"\"\nif isinstance(module, (nn.Linear, nn.Conv2d)):\n-            # Slightly different from the TF version which uses truncated_normal for initialization\n-            # cf https://github.com/pytorch/pytorch/pull/5617\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.data = nn.init.trunc_normal_(module.weight.data, mean=0.0, std=self.config.initializer_range)\nif module.bias is not None:\nmodule.bias.data.zero_()\nelif isinstance(module, nn.LayerNorm):\n", "fix_pattern": "In the condition of `isinstance(module, (nn.Linear, nn.Conv2d))`, if the pattern `module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)` is detected, then change the code to `module.weight.data = nn.init.trunc_normal_(module.weight.data, mean=0.0, std=self.config.initializer_range)` to fix the API misuse."}
{"number": 808, "change": "class DisentangledSelfAttention(nn.Module):\ndim=-1,\nindex=p2c_pos.squeeze(0).expand([query_layer.size(0), key_layer.size(-2), key_layer.size(-2)]),\n).transpose(-1, -2)\n-            score += p2c_att / scale\n+            score += p2c_att / torch.tensor(scale, dtype=p2c_att.dtype)\n\nreturn score\n", "fix_pattern": "In the condition of API misuse, if a constant value is used as a divisor in a division operation, then the API should be fixed by changing the divisor to a tensor with the correct data type."}
{"number": 812, "change": "def evaluate(model, data_loader, device):\nimage = list(img.to(device) for img in image)\ntargets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n-        torch.cuda.synchronize(device)\n+        # \u5f53\u4f7f\u7528CPU\u65f6\uff0c\u8df3\u8fc7GPU\u76f8\u5173\u6307\u4ee4\n+        if device != torch.device(\"cpu\"):\n+            torch.cuda.synchronize(device)\n+\nmodel_time = time.time()\noutputs = model(image)\n", "fix_pattern": "In the condition of checking if the device is not set to CPU, the code was added to synchronize the GPU. This fix pattern is used to fix the API misuse in handling GPU-related instructions."}
{"number": 816, "change": "def dnn(tensor_in, hidden_units, activation=tf.nn.relu, keep_prob=None):\nfor i, n_units in enumerate(hidden_units):\nwith tf.variable_scope('layer%d' % i):\ntensor_in = linear(tensor_in, n_units, True)\n-            tensor_in = activation(tensor_in)\n-            if keep_prob:\n-                tensor_in = tf.nn.dropout(tensor_in, keep_prob)\n+                tensor_in = activation(tensor_in)\n+                if keep_prob:\n+                    tensor_in = skflow.ops.dropout(tensor_in, keep_prob)\nreturn tensor_in\n", "fix_pattern": "In the condition of \"if keep_prob\", if \"tf.nn.dropout\" is detected, then replace it with \"skflow.ops.dropout\" to fix the API misuse."}
{"number": 828, "change": "def main():\nif utils.is_primary(args):\n_logger.info('Using NVIDIA APEX AMP. Training in mixed precision.')\nelif use_amp == 'native':\n-        amp_autocast = partial(torch.autocast, device_type=device.type, dtype=amp_dtype)\n-        if device.type == 'cuda':\n+        try:\n+            amp_autocast = partial(torch.autocast, device_type=device.type, dtype=amp_dtype)\n+        except (AttributeError, TypeError):\n+            # fallback to CUDA only AMP for PyTorch < 1.10\n+            assert device.type == 'cuda'\n+            amp_autocast = torch.cuda.amp.autocast\n+        if device.type == 'cuda' and amp_dtype == torch.float16:\n+            # loss scaler only used for float16 (half) dtype, bfloat16 does not need it\nloss_scaler = NativeScaler()\nif utils.is_primary(args):\n_logger.info('Using native Torch AMP. Training in mixed precision.')\n", "fix_pattern": "in the condition of `use_amp == 'native'`, if the pattern `amp_autocast = partial(torch.autocast, device_type=device.type, dtype=amp_dtype)` is detected, then add a try-except block to handle the case when the torch.autocast API is not available in older versions of PyTorch, and fallback to using `torch.cuda.amp.autocast` instead."}
{"number": 837, "change": "class Encoder(torch.nn.Module):\npos_enc_class(attention_dim, positional_dropout_rate),\n)\nelif input_layer is None:\n-            self.embed = pos_enc_class(attention_dim, positional_dropout_rate)\n+            self.embed = torch.nn.Sequential(\n+                pos_enc_class(attention_dim, positional_dropout_rate)\n+            )\nelse:\nraise ValueError(\"unknown input_layer: \" + input_layer)\nself.normalize_before = normalize_before\n", "fix_pattern": "in the condition of `elif input_layer is None`, if the pattern `self.embed = pos_enc_class(attention_dim, positional_dropout_rate)` is detected, then change the `self.embed` assignment to `self.embed = torch.nn.Sequential(pos_enc_class(attention_dim, positional_dropout_rate))` to fix the API misuse."}
{"number": 838, "change": "class ChineseCLIPVisionTransformer(nn.Module):\nembed_dim = config.hidden_size\n\nself.embeddings = ChineseCLIPVisionEmbeddings(config)\n-        self.pre_layrnorm = nn.LayerNorm(embed_dim)\n+        self.pre_layrnorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\nself.encoder = ChineseCLIPVisionEncoder(config)\n-        self.post_layernorm = nn.LayerNorm(embed_dim)\n+        self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n\n@add_start_docstrings_to_model_forward(CHINESE_CLIP_VISION_INPUTS_DOCSTRING)\n@replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=ChineseCLIPVisionConfig)\n", "fix_pattern": "In the condition of initializing a LayerNorm module, if the \"eps\" argument is not provided, add it with the value from the \"config.layer_norm_eps\" attribute to fix the API misuse."}
{"number": 840, "change": "class TestSolveCast:\n\nclass TestSolveWithMask:\ndef test_smoke(self, device, dtype):\n+        torch.manual_seed(0)  # issue kornia#2027\nA = torch.randn(2, 3, 1, 4, 4, device=device, dtype=dtype)\nB = torch.randn(2, 3, 1, 4, 6, device=device, dtype=dtype)\n", "fix_pattern": "Fix_pattern: In the condition of \"test_smoke\" function, if the pattern of missing torch.manual_seed() is detected, then add the line \"torch.manual_seed(0)\" to fix the API misuse."}
{"number": 847, "change": "def _add_gradients_summaries(grads_and_vars):\ngrad_values = grad.values\nelse:\ngrad_values = grad\n-      summaries.append(tf.histogram_summary(var.op.name + ':gradient',\n+      summaries.append(tf.summary.histogram(var.op.name + ':gradient',\ngrad_values))\n-      summaries.append(tf.histogram_summary(var.op.name + ':gradient_norm',\n+      summaries.append(tf.summary.histogram(var.op.name + ':gradient_norm',\ntf.global_norm([grad_values])))\nelse:\ntf.logging.info('Var %s has no gradient', var.op.name)\n", "fix_pattern": "In the condition of checking whether grad_values is a dictionary or not, if the pattern tf.histogram_summary() is detected, then it should be replaced with tf.summary.histogram() to fix the API misuse."}
{"number": 849, "change": "class QM9(InMemoryDataset):\nedge_type += 2 * [self.bonds[bond.GetBondType()]]\n\nedge_index = torch.tensor([row, col], dtype=torch.long)\n-            edge_type = torch.tensor(edge_type)\n-            edge_attr = F.one_hot(torch.tensor(edge_type),\n+            edge_type = torch.tensor(edge_type, dtype=torch.long)\n+            edge_attr = F.one_hot(edge_type,\nnum_classes=len(self.bonds)).to(torch.float)\n\nperm = (edge_index[0] * N + edge_index[1]).argsort()\n", "fix_pattern": "in the condition of \"assigning a tensor to edge_type\", if \"incorrect data type in torch.tensor()\" is detected, then \"change the data type to dtype=torch.long in torch.tensor()\" to fix the API misuse."}
{"number": 854, "change": "class MinSaver(Callback):\nnewname = os.path.join(logger.LOG_DIR,\nself.filename or\n('max-' + self.monitor_stat if self.reverse else 'min-' + self.monitor_stat))\n-        files_to_copy = glob.glob(path + '*')\n+        files_to_copy = tf.gfile.Glob(path + '*')\nfor file_to_copy in files_to_copy:\n-            shutil.copy(file_to_copy, file_to_copy.replace(path, newname))\n+            tf.gfile.Copy(file_to_copy, file_to_copy.replace(path, newname), overwrite=True)\nlogger.info(\"Model with {} '{}' saved.\".format(\n'maximum' if self.reverse else 'minimum', self.monitor_stat))\n", "fix_pattern": "in the condition of using glob.glob(), if tf.gfile.Glob() is detected, then change shutil.copy() to tf.gfile.Copy() to fix the API misuse."}
{"number": 855, "change": "class Evaluator(object):\nThe mean average result per tensor over the entire dataset.\n\n\"\"\"\n+        tflearn.is_training(False, self.session)\ncoord = tf.train.Coordinator()\ninputs = tf.get_collection(tf.GraphKeys.INPUTS)\n# Data Preprocessing\n", "fix_pattern": "In the condition of calling the \"is_training\" method from the tflearn module with a False value for the training parameter, if the pattern of not providing the session parameter is detected, then add the session parameter to fix the API misuse."}
{"number": 857, "change": "class Lamb(Optimizer):\nglobal_grad_norm.add_(grad.pow(2).sum())\n\nglobal_grad_norm = torch.sqrt(global_grad_norm)\n-        max_grad_norm = self.defaults['max_grad_norm']\n+        # FIXME it'd be nice to remove explicit tensor conversion of scalars when torch.where promotes\n+        # scalar types properly https://github.com/pytorch/pytorch/issues/9190\n+        max_grad_norm = torch.tensor(self.defaults['max_grad_norm'], device=device)\nclip_global_grad_norm = torch.where(\nglobal_grad_norm > max_grad_norm,\nglobal_grad_norm / max_grad_norm,\n", "fix_pattern": "in the condition of <global_grad_norm > max_grad_norm>, if <explicit tensor conversion of scalars> is detected, then(add) the <torch.tensor(self.defaults['max_grad_norm'], device=device)> to fix the API misuse."}
{"number": 860, "change": "class ModelCatalogTest(unittest.TestCase):\ndef testCustomModel(self):\nray.init()\nModelCatalog.register_custom_model(\"foo\", CustomModel)\n-        p1 = ModelCatalog.get_model(1, 5, {\"custom_model\": \"foo\"})\n+        p1 = ModelCatalog.get_model(\n+            tf.constant([1, 2, 3]), 5, {\"custom_model\": \"foo\"})\nself.assertEqual(str(type(p1)), str(CustomModel))\n", "fix_pattern": "In the condition of \"calling ModelCatalog.get_model()\", if the pattern \"passing a tf.constant as the first argument\" is detected, then change the code to pass an integer instead of tf.constant to fix the API misuse."}
{"number": 862, "change": "class _netD(nn.Module):\n\ndef forward(self, input):\ngpu_ids = None\n-        if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > 1:\n+        if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > =1:\ngpu_ids = range(self.ngpu)\noutput = nn.parallel.data_parallel(self.main, input, gpu_ids)\nreturn output.view(-1, 1)\n", "fix_pattern": "in the condition of isinstance(input.data, torch.cuda.FloatTensor), if self.ngpu >= 1 is detected, then change the code from self.ngpu > 1 to self.ngpu >= 1 to fix the API misuse."}
{"number": 865, "change": "from tests import utils\ndef test_image_classifier(tmp_path):\ntrain_x = utils.generate_data(num_instances=320, shape=(32, 32, 3))\ntrain_y = utils.generate_one_hot_labels(num_instances=320, num_classes=10)\n-    clf = ak.ImageClassifier(directory=tmp_path, max_trials=2, seed=utils.SEED)\n+    clf = ak.ImageClassifier(\n+        directory=tmp_path,\n+        max_trials=2,\n+        seed=utils.SEED,\n+        distribution_strategy=tf.distribute.MirroredStrategy(),\n+    )\nclf.fit(train_x, train_y, epochs=1, validation_split=0.2)\nkeras_model = clf.export_model()\nclf.evaluate(train_x, train_y)\n", "fix_pattern": "in the condition of using AutoKeras ImageClassifier, if the distribution strategy is not specified, then the code should be changed to include the MirroredStrategy distribution strategy to fix the API misuse."}
{"number": 867, "change": "def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\noutput_shape[3],\noutput_shape[1])\nif output_shape[0] is None:\n-        output_shape = (tf.shape(x)[0],) + tuple(output_shape[1:])\n-        output_shape = tf.stack(list(output_shape))\n+        output_shape = (shape(x)[0],) + tuple(output_shape[1:])\n+\n+    output_shape = tf.stack(list(output_shape))\n\npadding = _preprocess_padding(padding)\nif tf_data_format == 'NHWC':\n", "fix_pattern": "In the condition of checking if output_shape[0] is None, if the pattern \"shape(x)\" is detected, then change the code to \"tf.shape(x)\" to fix the API misuse."}
{"number": 868, "change": "class TestRasterizeMeshes(TestCaseMixin, unittest.TestCase):\n], dtype=torch.int64, device=device)\n# fmt: on\n\n-        pix_to_face_padded = -torch.ones_like(pix_to_face_frontface)\n+        pix_to_face_padded = -(torch.ones_like(pix_to_face_frontface))\n# Run with and without culling\n# Without culling, for k=0, the front face (i.e. face 2) is\n# rasterized and for k=1, the back face (i.e. face 3) is\n", "fix_pattern": "in the condition of initializing a variable using `-torch.ones_like()`, if `-(torch.ones_like())` is detected, then change the initialization code to `-(torch.ones_like())` to fix the API misuse."}
{"number": 875, "change": "class SparkKerasTests(tf.test.TestCase):\n\ndef test_fit_model_multiclass(self):\nmodel = create_mnist_model()\n-        if version.parse(tf.keras.__version__) < version.parse(\"2.11\"):\n+        if version.parse(tf.keras.__version__.replace(\"-tf\", \"+tf\")) < version.parse(\"2.11\"):\noptimizer = tf.keras.optimizers.Adadelta(1.0)\nelse:\noptimizer = tf.keras.optimizers.legacy.Adadelta(1.0)\n", "fix_pattern": "In the condition of the if statement, if the pattern \"replace(\"-tf\", \"+tf\")\" is detected in the version string, then replace the code \"tf.keras.__version__\" with \"tf.keras.__version__.replace(\"-tf\", \"+tf\")\" to fix the API misuse."}
{"number": 878, "change": "def train(args):\ndtype = torch.float32\nmodel = model_class(args.n_vocab, args).to(dtype=dtype)\nif args.ngpu > 0:\n-        model.to(\"cuda:0\")\n+        model.to(\"cuda\")\ngpu_id = list(range(args.ngpu))\nelse:\ngpu_id = [-1]\n", "fix_pattern": "In the condition of \"args.ngpu > 0\", if \"cuda:0\" is detected, then change the \"cuda:0\" to \"cuda\" to fix the API misuse."}
{"number": 883, "change": "def triangular_solve(x, y, upper=False, transpose=False):\n\n\ndef precision_to_scale_tril(P):\n-    Lf = torch.cholesky(torch.flip(P, (-2, -1)))\n+    Lf = torch.linalg.cholesky(torch.flip(P, (-2, -1)))\nL_inv = torch.transpose(torch.flip(Lf, (-2, -1)), -2, -1)\nL = torch.triangular_solve(torch.eye(P.shape[-1], dtype=P.dtype, device=P.device),\nL_inv, upper=False)[0]\n", "fix_pattern": "In the condition of using the `torch.cholesky` function, if the pattern `torch.cholesky` is detected, the code should be modified to use `torch.linalg.cholesky` instead to fix the API misuse."}
{"number": 884, "change": "class NaturalGradient(Optimizer):\napplied = self.apply_step(variables=variables, diffs=estimated_diffs)\n\nwith tf.control_dependencies(control_inputs=(applied,)):\n-                return [tf.identity(input=estimated_diff) for estimated_diff in estimated_diffs]\n+                return [estimated_diff + 0.0 for estimated_diff in estimated_diffs]\n\ndef false_fn():\nreturn [tf.zeros_like(tensor=diff) for diff in diffs]\n", "fix_pattern": "Fix_pattern: In the condition of using tf.control_dependencies, if the pattern of adding 0.0 to each element in a list is detected, then change the code to fix the API misuse."}
{"number": 886, "change": "def argsort(\nret = tf.argsort(\ntf.convert_to_tensor(x), axis=axis, direction=\"ASCENDING\", stable=stable\n)\n-    return ret\n+    return tf.cast(ret, dtype=tf.int64)\n\n\ndef sort(\n", "fix_pattern": "In the condition of returning a sorted array, if the code is missing the data type casting, then add the code \"tf.cast(ret, dtype=tf.int64)\" to fix the API misuse."}
{"number": 887, "change": "def _to_ivy(x: Any) -> Any:\n\n\ndef _to_ivy_array(x: Any) -> ivy.Array:\n-    if isinstance(x, (torch.Tensor, tf.Tensor, jnp.numpy.DeviceArray, numpy.ndarray)):\n+    if isinstance(x, (torch.Tensor, tf.Tensor, jnp.DeviceArray, numpy.ndarray)):\nreturn ivy.array(numpy.array(x))\nreturn x\n", "fix_pattern": "Fix_pattern: \nIn the condition of `isinstance(x, (torch.Tensor, tf.Tensor, jnp.DeviceArray, numpy.ndarray))`, if `jnp.numpy.DeviceArray` is detected, then remove the `numpy` to fix the API misuse."}
{"number": 888, "change": "def vecdot(\nout: Optional[Union[tf.Tensor, tf.Variable]] = None,\n) -> Union[tf.Tensor, tf.Variable]:\ndtype = ivy.as_native_dtype(ivy.promote_types(x1.dtype, x2.dtype))\n-    x1, x2 = tf.cast(x1, tf.float32), tf.cast(x2, tf.float32)\n+    if dtype != \"float64\":\n+        x1, x2 = tf.cast(x1, tf.float32), tf.cast(x2, tf.float32)\nret = tf.cast(tf.tensordot(x1, x2, axes=(axis, axis)), dtype)\nreturn ret\n", "fix_pattern": "In the condition of \"dtype != 'float64'\", if the pattern of unnecessary casting of 'x1' and 'x2' to float32 is detected, then remove the casting code to fix the API misuse."}
{"number": 902, "change": "class PipelineEngine(DeepSpeedEngine):\nmem_cached = new_cached\nmem_alloced = new_alloced\n\n-        max_alloced = torch.cuda.max_memory_allocated()\n-        max_cached = torch.cuda.max_memory_cached()\n+        max_alloced = get_accelerator().max_memory_allocated()\n+        max_cached = get_accelerator().max_memory_cached()\n\n# convert to GB for printing\nnew_alloced /= 1024**3\n", "fix_pattern": "In the condition of using the `max_memory_allocated()` and `max_memory_cached()` functions, if the `get_accelerator()` is missing, then adding `get_accelerator()` before the function calls helps fix the API misuse."}
{"number": 903, "change": "def torch_multinomial(input, num_samples, replacement=False):\nDoes not support keyword argument `out`.\n\"\"\"\nif input.is_cuda:\n-        return torch_multinomial(input.cpu(), num_samples, replacement).cuda()\n+        return torch.multinomial(input.cpu(), num_samples, replacement).cuda(input.get_device())\nelse:\nreturn torch.multinomial(input, num_samples, replacement)\n", "fix_pattern": "Fix_pattern: \nIn the condition of `if input.is_cuda`, if `pattern` is detected, then add `.cuda(input.get_device())` to the code to fix the API misuse."}
{"number": 904, "change": "def test_delete_entire_dataset(domain_owner, cleanup_storage):\nassert domain_owner.datasets[0].name == \"Dataset_1\"\nassert domain_owner.datasets[1].name == \"Dataset_2\"\n\n-    domain_owner.datasets.delete(dataset_id=domain_owner.datasets[0].id)\n+    domain_owner.datasets.delete(\n+        dataset_id=domain_owner.datasets[0].id, skip_checks=True\n+    )\n\n# Check if the number of available datasets has been decreased\nassert len(domain_owner.datasets) == 1\n", "fix_pattern": "In the condition of \"deleting an entire dataset\", if the pattern of \"not skipping checks\" is detected, then the code is modified to include the \"skip_checks\" flag to fix the API misuse."}
{"number": 908, "change": "def pack(\ntry:\nimport torch\n\n-        meta_objs.update(torch=torch.__version__)\n+        meta_objs.update(torch=str(torch.__version__))\nexcept ImportError:\npass\ntry:\n", "fix_pattern": "In the condition of \"ImportError\", if \"torch\" is detected, then change the code from \"meta_objs.update(torch=torch.__version__)\" to \"meta_objs.update(torch=str(torch.__version__))\" to fix the API misuse."}
{"number": 910, "change": "class Schedule(metaclass=ABCMeta):\nraise NotImplementedError\n\ndef value(self, t):\n-        if self.framework == \"tf\" and tf.executing_eagerly() is False:\n+        if self.framework == \"tf\":\nreturn tf.cast(\n-                tf.py_func(self._value, [t], tf.float64),\n+                tf.py_function(self._value, [t], tf.float64),\ntf.float32,\n-                name=\"schedule-value\")\n+                name=\"schedule_value\")\nreturn self._value(t)\n\ndef __call__(self, t):\n", "fix_pattern": "In the condition of \"if self.framework == \"tf\"\", if \"tf.py_func\" is detected, then change it to \"tf.py_function\" to fix the API misuse."}
{"number": 912, "change": "class FeedForwardTransformer(TTSInterface, torch.nn.Module):\n\n# concat speaker embedding\nif self.spk_embed_dim is not None:\n-            spembs = torch.nn.functional.normalize(spembs).unsqueeze(1).expand(-1, hs.size(1), -1)\n-            hs = self.projection(torch.cat([hs, spembs], dim=-1))\n+            spembs_ = F.normalize(spembs).unsqueeze(1).expand(-1, hs.size(1), -1)\n+            hs = self.projection(torch.cat([hs, spembs_], dim=-1))\n\n# forward duration predictor and length regulator\nd_masks = make_pad_mask(ilens).to(xs.device)\n", "fix_pattern": "in the condition of \"if self.spk_embed_dim is not None\", if the pattern of using \"torch.nn.functional.normalize\" is detected, then change it to \"F.normalize\" to fix the API misuse."}
{"number": 919, "change": "def image_histogram2d(\nhist = hist.squeeze()\nelif image.dim() == 3:\nhist = hist.squeeze(0)\n-    return hist, torch.zeros_like(hist, dtype=hist.dtype, device=device)\n+    return hist, torch.zeros_like(hist, dtype=hist.dtype, device=hist.device)\n", "fix_pattern": "In the condition of checking the image dimension, if the pattern of incorrect device argument is detected in the torch.zeros_like function, then change the device argument to match the histogram device to fix the API misuse."}
{"number": 920, "change": "class FeedForwardEncoder(Seq2SeqEncoder):\nreturn self._feedforward(inputs)\nelse:\noutputs = self._feedforward(inputs)\n-            return outputs * mask.unsqueeze(dim=-1).float()\n+            return outputs * mask.unsqueeze(dim=-1)\n", "fix_pattern": "In the condition of \"else\", if the pattern \"mask.unsqueeze(dim=-1).float()\" is detected, then remove \".float()\" from the code to fix the API misuse."}
{"number": 929, "change": "def rand_like_with_shape(shape, ori_t):\nhigher_bound = torch.max(ori_t)\n\nif dtype in [torch.uint8, torch.int16, torch.short, torch.int16, torch.long, torch.bool]:\n-        return torch.randint(lower_bound, higher_bound+1, shape, dtype=dtype, device=device)\n+        return torch.randint(lower_bound.long(), higher_bound.long() + 1, shape, dtype=dtype, device=device)\nelse:\nreturn torch.rand(shape, dtype=dtype, device=device, requires_grad=require_grad)\n", "fix_pattern": "In the condition of checking the data type, if the pattern of calling the \"long()\" method on the bounds is detected, then the code should be changed to fix the API misuse by calling the \"long()\" method on the bounds."}
{"number": 930, "change": "def map_data_vector_model(subsample_size):\npyro.sample(\"x\", dist.normal, mu[batch], sigma[batch])\nreturn batch\n\n-    ind = Variable(torch.LongTensor(range(20)))\n+    LongTensor = torch.cuda.LongTensor if torch.Tensor.is_cuda else torch.LongTensor\n+    ind = Variable(LongTensor(range(20)))\nbatch = pyro.map_data('mapdata', ind, local_model, batch_size=subsample_size)\nreturn list(batch.data)\n", "fix_pattern": "In the condition of torch.Tensor.is_cuda, if torch.cuda.LongTensor is not already defined, then add the line `LongTensor = torch.cuda.LongTensor` to fix the API misuse."}
{"number": 933, "change": "class SelfAttnFunc(torch.autograd.Function):\nvalues_grads   = torch.bmm(dropout_results.transpose(1,2), output_lin_grads, out=values_grads.transpose(0,1))\n\n# Mask and Scaling for Dropout (not a publically documented op)\n-        dropout_grads = torch._masked_scale(matmul2_dgrad1, dropout_mask, dropout_prob_t[0])\n+        dropout_grads = torch._masked_scale(matmul2_dgrad1, dropout_mask, 1.0/(1.0-dropout_prob_t[0]))\n\n# Softmax Grad (not a publically documented op)\nsoftmax_grads = torch._softmax_backward_data(dropout_grads, softmax_results, -1, softmax_results)\n", "fix_pattern": "In the condition of dropout scaling, if the code to mask and scale is using a dropout probability value, then change the code to calculate the scaling factor using 1.0 divided by (1.0 minus the dropout probability value) to fix the API misuse."}
{"number": 935, "change": "class AutoShape(nn.Module):\n#   multiple:        = [Image.open('image1.jpg'), Image.open('image2.jpg'), ...]  # list of images\n\nt = [time_sync()]\n-        p = next(self.model.parameters()) if self.pt else torch.zeros(1)  # for device and type\n+        p = next(self.model.parameters()) if self.pt else torch.zeros(1, device=self.model.device)  # for device, type\nautocast = self.amp and (p.device.type != 'cpu')  # Automatic Mixed Precision (AMP) inference\nif isinstance(imgs, torch.Tensor):  # torch\nwith amp.autocast(autocast):\n", "fix_pattern": "In the condition of initializing the variable \"p\", if the pattern \"self.pt\" is detected, then change the code from \"torch.zeros(1)\" to \"torch.zeros(1, device=self.model.device)\" to fix the API misuse."}
{"number": 936, "change": "class CycleDiffusionPipeline(DiffusionPipeline):\n\ndevice = torch.device(f\"cuda:{gpu_id}\")\n\n-        for cpu_offloaded_model in [self.unet, self.text_encoder, self.vae, self.safety_checker]:\n+        for cpu_offloaded_model in [self.unet, self.text_encoder, self.vae]:\nif cpu_offloaded_model is not None:\ncpu_offload(cpu_offloaded_model, device)\n\n+        if self.safety_checker is not None:\n+            # TODO(Patrick) - there is currently a bug with cpu offload of nn.Parameter in accelerate\n+            # fix by only offloading self.safety_checker for now\n+            cpu_offload(self.safety_checker.vision_model)\n+\n@property\n# Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline._execution_device\ndef _execution_device(self):\n", "fix_pattern": "In the condition of \"if self.safety_checker is not None\", if the pattern of only offloading \"self.safety_checker\" is detected, then add the code \"cpu_offload(self.safety_checker.vision_model)\" to fix the API misuse."}
{"number": 937, "change": "def shift_tokens_right(input_ids: tf.Tensor, pad_token_id: int, decoder_start_to\n\nif tf.executing_eagerly():\n# \"Verify that `labels` has only positive values and -100\"\n-        assert_gte0 = tf.debugging.assert_greater_equal(shifted_input_ids, tf.constant(0))\n+        assert_gte0 = tf.debugging.assert_greater_equal(shifted_input_ids, tf.constant(0, dtype=input_ids.dtype))\n\n# Make sure the assertion op is called by wrapping the result in an identity no-op\nwith tf.control_dependencies([assert_gte0]):\n", "fix_pattern": "In the condition of `tf.executing_eagerly()`, if the pattern `tf.constant(0)` is detected, then change the dtype of the constant to `input_ids.dtype` to fix the API misuse."}
{"number": 940, "change": "def arange(start, stop=None, step=1, dtype=None, dev=None):\nif dtype in [torch.int8, torch.uint8, torch.int16]:\nreturn torch.arange(start, stop, step=step, dtype=torch.int64, device=dev).to(dtype)\nelse:\n-            return torch.range(start, stop, step=step, dtype=dtype, device=dev)\n+            return torch.arange(start, stop, step=step, dtype=dtype, device=dev)\n", "fix_pattern": "Fix_pattern: \nIn the condition of dtype check, if the pattern of torch.range is detected, then change the code to use torch.arange to fix the API misuse."}
{"number": 944, "change": "def _apply_affine(input: torch.Tensor,\n\nheight, width = x_data.shape[-2:]\ntransform: torch.Tensor = params['transform'].to(device, dtype)\n-\n-    out_data: torch.Tensor = warp_perspective(x_data, transform, (height, width))\n+    out_data: torch.Tensor = warp_affine(x_data, transform[:, :2, :], (height, width))\n\nif return_transform:\nreturn out_data.view_as(input), transform\n", "fix_pattern": "Fix_pattern: The fix pattern is to change the code from using the 'warp_perspective' function to using the 'warp_affine' function in order to fix an API misuse."}
{"number": 946, "change": "class GPTNeoAttentionMixin:\nelse:\nraise ValueError(f\"Input tensor rank should be one of [2, 3], but is: {len(tensor.shape)}\")\n\n-        padded_tensor = F.pad(tensor, padding_side, value=pad_value)\n+        padded_tensor = nn.functional.pad(tensor, padding_side, value=pad_value)\npadded_tensor = padded_tensor.unfold(dimension=1, size=window_size + block_length, step=block_length)\n\nif is_key_value:\n", "fix_pattern": "In the condition of using the `pad` function from the `torch.nn.functional` module, if the pattern of using `F.pad` is detected, then change the code to use `nn.functional.pad` to fix the API misuse."}
{"number": 953, "change": "def test_tagged_corpus_downsample():\n\nassert 10 == len(corpus.train)\n\n-    corpus.downsample(percentage=0.3, only_downsample_train=True)\n+    corpus.downsample(percentage=0.3, downsample_dev=False, downsample_test=False)\n\nassert 3 == len(corpus.train)\n", "fix_pattern": "In the condition of `only_downsample_train=True`, if the pattern `downsample_dev=False, downsample_test=False` is detected, then add the code `corpus.downsample(percentage=0.3, downsample_dev=False, downsample_test=False)` to fix the API misuse."}
{"number": 962, "change": "class DecisionTransformerGPT2Attention(nn.Module):\n# Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.\n# Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`\nmask_value = torch.full([], mask_value, dtype=attn_weights.dtype).to(attn_weights.device)\n-            attn_weights = torch.where(causal_mask, attn_weights, mask_value)\n+            attn_weights = torch.where(causal_mask, attn_weights.to(attn_weights.dtype), mask_value)\n\nif attention_mask is not None:\n# Apply the attention mask\n", "fix_pattern": "In the condition of \"if attention_mask is not None\", if the pattern \"attn_weights\" is detected, then change the code from \"attn_weights = torch.where(causal_mask, attn_weights, mask_value)\" to \"attn_weights = torch.where(causal_mask, attn_weights.to(attn_weights.dtype), mask_value)\" to fix the API misuse."}
{"number": 963, "change": "def load_tf_graph(graph_file):\n\"\"\"\n# We load the protobuf file from the disk and parse it to retrieve the\n# unserialized graph_def\n-    with tf.gfile.GFile(graph_file, \"rb\") as f:\n-        graph_def = tf.GraphDef()\n+    with tf.io.gfile.GFile(graph_file, \"rb\") as f:\n+        graph_def = tf.compat.v1.GraphDef()\ngraph_def.ParseFromString(f.read())\n\n# Then, we import the graph_def into a new Graph and returns it\n", "fix_pattern": "In the condition of using the TensorFlow I/O library, if there is a call to tf.gfile.GFile, then change it to tf.io.gfile.GFile to fix the API misuse."}
{"number": 967, "change": "class TestScalarMix(AllenNlpTestCase):\ntensors = [torch.randn([3, 4, 5]) for _ in range(3)]\nnumpy_mask = numpy.ones((3, 4), dtype=\"int32\")\nnumpy_mask[1, 2:] = 0\n-        mask = torch.from_numpy(numpy_mask)\n+        mask = torch.from_numpy(numpy_mask).bool()\n\nweights = [0.1, 0.2, 0.3]\nfor k in range(3):\n", "fix_pattern": "In the condition of creating a torch tensor from a NumPy array, if the tensor requires a boolean data type, then change the code to include the \".bool()\" method to fix the API misuse."}
{"number": 969, "change": "class SequenceFeatureExtractionTestMixin(FeatureExtractionSavingTestMixin):\ninput_np = feat_extract.pad(processed_features, padding=\"longest\", return_tensors=\"np\")[input_name]\ninput_tf = feat_extract.pad(processed_features, padding=\"longest\", return_tensors=\"tf\")[input_name]\n\n-        self.assertTrue(abs(input_np.sum() - input_tf.numpy().sum()) < 1e-2)\n+        self.assertTrue(abs(input_np.astype(np.float32).sum() - input_tf.numpy().sum()) < 1e-2)\n\ndef test_attention_mask(self):\nfeat_dict = self.feat_extract_dict\n", "fix_pattern": "in the condition of using the `self.assertTrue()` function, if the pattern of using the `input_np.sum()` function is detected, then change the `input_np.sum()` to `input_np.astype(np.float32).sum()` to fix the API misuse."}
{"number": 971, "change": "class ScoreSdeVeScheduler(SchedulerMixin, ConfigMixin):\n\"\"\"\nsampling_eps = sampling_eps if sampling_eps is not None else self.config.sampling_eps\n\n-        self.timesteps = torch.linspace(1, sampling_eps, num_inference_steps)\n+        self.timesteps = torch.linspace(1, sampling_eps, num_inference_steps, device=device)\n\ndef set_sigmas(\nself, num_inference_steps: int, sigma_min: float = None, sigma_max: float = None, sampling_eps: float = None\n", "fix_pattern": "In the condition of checking if a variable is None, if the variable is detected to be None, then add the missing argument to fix the API misuse."}
{"number": 972, "change": "class SSIM(nn.Module):\nssim_map = ((2 * mu1_mu2 + self.C1) * (2 * sigma12 + self.C2)) / \\\n((mu1_sq + mu2_sq + self.C1) * (sigma1_sq + sigma2_sq + self.C2))\n\n-        loss = torch.clamp(1. - ssim_map, min=0, max=1) / 2.\n+        loss = torch.clamp(torch.tensor(1.) - ssim_map, min=0, max=1) / 2.\n\nif self.reduction == 'mean':\nloss = torch.mean(loss)\n", "fix_pattern": "In the condition of 'self.reduction == 'mean', if 'torch.clamp(1. - ssim_map, min=0, max=1) / 2.' is detected, then change 'torch.clamp(1. - ssim_map, min=0, max=1) / 2.' to 'torch.clamp(torch.tensor(1.) - ssim_map, min=0, max=1) / 2.' to fix the API misuse."}
{"number": 974, "change": "def rpn_losses(anchor_labels, anchor_boxes, label_logits, box_logits):\nplaceholder = 1.\nlabel_loss = tf.nn.sigmoid_cross_entropy_with_logits(\nlabels=tf.to_float(valid_anchor_labels), logits=valid_label_logits)\n-    label_loss = label_loss * (1. / config.RPN_BATCH_PER_IM)\n+    label_loss = tf.reduce_sum(label_loss) * (1. / config.RPN_BATCH_PER_IM)\nlabel_loss = tf.where(tf.equal(nr_valid, 0), placeholder, label_loss, name='label_loss')\n\npos_anchor_boxes = tf.boolean_mask(anchor_boxes, pos_mask)\n", "fix_pattern": "In the condition of tf.equal(nr_valid, 0), if tf.reduce_sum(label_loss) pattern is detected, then remove (1. / config.RPN_BATCH_PER_IM) from label_loss assignment to fix the API misuse."}
{"number": 976, "change": "from allennlp.common.testing import AllenNlpTestCase\n\nclass TestElmoLstmCell(AllenNlpTestCase):\ndef test_elmo_lstm(self):\n-        input_tensor = Variable(torch.rand(4, 5, 3))\n+        input_tensor = torch.rand(4, 5, 3)\ninput_tensor[1, 4:, :] = 0.\ninput_tensor[2, 2:, :] = 0.\ninput_tensor[3, 1:, :] = 0.\n-        mask = Variable(torch.ones([4, 5]))\n+        mask = torch.ones([4, 5])\nmask[1, 4:] = 0.\nmask[2, 2:] = 0.\nmask[3, 1:] = 0.\n", "fix_pattern": "In the condition of using the `Variable` class from the `torch` module, if the pattern of using `Variable(torch.<function>)` is detected, then remove the `Variable()` wrapper to fix the API misuse."}
{"number": 977, "change": "class TpuStrategyTest(tf.test.TestCase):\nserving_fn = create_serving_signature(model)\n\nsaved_model_dir = tempfile.mkdtemp(dir=self.get_temp_dir())\n-      tf.saved_model.save(\n-          model, saved_model_dir, signatures={\"serving_default\": serving_fn})\n+      model.save(saved_model_dir, save_format=\"tf\",\n+                 signatures={\"serving_default\": serving_fn})\n\n# Test the saved_model.\nloaded_serving_fn = tf.keras.models.load_model(\n", "fix_pattern": "In the condition of saving a TensorFlow model with `tf.saved_model.save()`, if the intention is to save the model in the \"tf\" format, then the `tf.saved_model.save()` function should be replaced with `model.save()` while specifying the `save_format` argument as \"tf\" to fix the API misuse."}
{"number": 984, "change": "def _preprocess_conv3d_input(x, data_format):\n# Returns\nA tensor.\n\"\"\"\n-    if dtype(x) == 'float64':\n+    # tensorflow doesn't support float64 for conv layer before 1.8.0\n+    if (dtype(x) == 'float64'\n+            and StrictVersion(tf.__version__) < StrictVersion('1.8.0')):\nx = tf.cast(x, 'float32')\ntf_data_format = 'NDHWC'\nif data_format == 'channels_first':\n", "fix_pattern": "in the condition of 'dtype(x) == 'float64' and StrictVersion(tf.__version__) < StrictVersion('1.8.0')', if 'dtype(x) == 'float64'' is detected, then add 'x = tf.cast(x, 'float32')' to fix the API misuse."}
{"number": 988, "change": "def main(args):\naccelerator.clip_grad_norm_(params_to_clip, args.max_grad_norm)\noptimizer.step()\nlr_scheduler.step()\n-                optimizer.zero_grad()\n+                optimizer.zero_grad(set_to_none=args.set_grads_to_none)\n\n# Checks if the accelerator has performed an optimization step behind the scenes\nif accelerator.sync_gradients:\n", "fix_pattern": "in the condition of accelerator.sync_gradients, if set_grads_to_none parameter is specified, then add optimizer.zero_grad(set_to_none=args.set_grads_to_none) to fix the API misuse."}
{"number": 989, "change": "class Tagger(nn.Module):\n# criterion\nself.crit = nn.CrossEntropyLoss(ignore_index=0) # ignore padding\n\n-        self.drop = Dropout(args['dropout'])\n+        self.drop = nn.Dropout(args['dropout'])\nself.worddrop = WordDropout(args['word_dropout'])\n\ndef forward(self, word, word_mask, wordchars, wordchars_mask, upos, xpos, ufeats, pretrained, word_orig_idx, sentlens, wordlens):\n", "fix_pattern": "Fix pattern: In the condition of instantiating a dropout layer, if \"Dropout\" is used instead of \"nn.Dropout\", then change the code to \"nn.Dropout\" to fix the API misuse."}
{"number": 990, "change": "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\ndef train():\nmodel.train()\noptimizer.zero_grad()\n-    pos_z, neg_z, summary = model(data.x, data.edge_index, data.edge_attr)\n-    loss = model.loss(pos_z, neg_z, summary)\n+    y = model(data.x, data.edge_index, data.edge_attr)\n+    loss = torch.sum(y) #TODO: actual loss function\nloss.backward()\noptimizer.step()\nreturn loss.item()\n", "fix_pattern": "in the condition of invoking the loss function, if the pattern of using the model.loss() method is detected, then remove the invocation of model.loss() and replace it with the actual loss function to fix the API misuse."}
{"number": 991, "change": "def build_linear_positions(index_dims, output_range=(-1.0, 1.0)):\nreturn torch.linspace(start=output_range[0], end=output_range[1], steps=n_xels_per_dim, dtype=torch.float32)\n\ndim_ranges = [_linspace(n_xels_per_dim) for n_xels_per_dim in index_dims]\n-    array_index_grid = torch.meshgrid(*dim_ranges)\n+    array_index_grid = meshgrid(*dim_ranges, indexing=\"ij\")\n\nreturn torch.stack(array_index_grid, dim=-1)\n", "fix_pattern": "In the condition of calling the `meshgrid` function, if the indexing argument is missing, then add the `indexing=\"ij\"` argument to fix the API misuse."}
{"number": 1007, "change": "class GPT2Attention(nn.Module):\n# Apply the attention mask\nattn_weights = attn_weights + attention_mask\n\n-        attn_weights = nn.Softmax(dim=-1)(attn_weights)\n+        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n\n# Downcast (if necessary) back to V's dtype (if in mixed-precision) -- No-Op if otherwise\nif attn_weights.dtype != torch.float32:\n", "fix_pattern": "in the condition of checking the dtype of the attention weights, if the Softmax function is detected, then change it to nn.functional.softmax to fix the API misuse."}
{"number": 1008, "change": "class BidirectionalEndpointSpanExtractor(SpanExtractor):\nsequence_lengths = util.get_lengths_from_binary_sequence_mask(sequence_mask)\nelse:\n# shape (batch_size), filled with the sequence length size of the sequence_tensor.\n-            sequence_lengths = util.ones_like(sequence_tensor[:, 0, 0]).long() * sequence_tensor.size(1)\n+            sequence_lengths = (torch.ones_like(sequence_tensor[:, 0, 0], dtype=torch.long) *\n+                                sequence_tensor.size(1))\n\n# shape (batch_size, num_spans, 1)\nend_sentinel_mask = (exclusive_span_ends == sequence_lengths.unsqueeze(-1)).long().unsqueeze(-1)\n", "fix_pattern": "In the condition of checking if statement, if the pattern of utilizing the 'ones_like' function is detected, then change the code to use the 'torch.ones_like' function instead to fix the API misuse."}
{"number": 1016, "change": "class LanguageModel(nn.Module):\n\nfor i in range(number_of_characters):\n\n-                if torch.cuda.is_available():\n-                    input = input.cuda()\n+                input = input.to(flair.device)\n\n# get predicted weights\nprediction, _, hidden = self.forward(input, hidden)\n", "fix_pattern": "In the condition of checking if torch.cuda.is_available(), if the pattern of using input.cuda() is detected, then change it to input.to(flair.device) to fix the API misuse."}
{"number": 1030, "change": "def Conv2DTranspose(\nif get_tf_version_tuple() <= (1, 12):\nkernel_initializer = tf.contrib.layers.variance_scaling_initializer(2.0),\nelse:\n-            kernel_initializer = tf.keras.initializers.VarianceScaling(2.0)\n+            kernel_initializer = tf.keras.initializers.VarianceScaling(2.0, distribution='untruncated_normal')\n\nwith rename_get_variable({'kernel': 'W', 'bias': 'b'}):\nlayer = tf.layers.Conv2DTranspose(\n", "fix_pattern": "In the condition of checking the TensorFlow version, if the pattern of tf.contrib.layers.variance_scaling_initializer is detected, then add distribution='untruncated_normal' to the tf.keras.initializers.VarianceScaling to fix the API misuse."}
{"number": 1033, "change": "def make_batches(lines, args, task, max_positions, encode_fn):\n).long()\nfor src_str in lines\n]\n-    lengths = torch.LongTensor([t.numel() for t in tokens])\n+    lengths = [t.numel() for t in tokens]\nitr = task.get_batch_iterator(\ndataset=task.build_dataset_for_inference(tokens, lengths),\nmax_tokens=args.max_tokens,\n", "fix_pattern": "Fix_pattern: \nIn the condition of iterating over a list of tokens, if the pattern of using torch.LongTensor() is detected, then change it to use a list comprehension to fix the API misuse."}
{"number": 1034, "change": "class EarlyStopping(Callback):\n\nif trainer.use_tpu:\nstop = torch.tensor(int(trainer.should_stop), device=pl_module.device, dtype=torch.int32)\n-            stop = xm.mesh_reduce(\"stop_signal\", stop, torch.cat)\n+            stop = xm.mesh_reduce(\"stop_signal\", stop, sum)\ntorch_xla.core.xla_model.rendezvous(\"pl.EarlyStoppingCallback.stop_distributed_training_check\")\ntrainer.should_stop = int(stop.item()) == trainer.world_size\n", "fix_pattern": "In the condition of \"if trainer.use_tpu\", if the pattern of \"torch.cat\" is detected, then change the code to \"sum\" to fix the API misuse."}
{"number": 1035, "change": "class Entropy(Metric):\nmask : `torch.Tensor`, optional (default = None).\nA masking tensor of shape (batch_size, ...).\n\"\"\"\n-        logits, mask = self.unwrap_to_tensors(logits, mask)\n+        logits, mask = self.detach_tensors(logits, mask)\n\nif mask is None:\n-            mask = torch.ones(logits.size()[:-1])\n+            mask = torch.ones(logits.size()[:-1], device=logits.device)\n\nlog_probs = torch.nn.functional.log_softmax(logits, dim=-1)\nprobabilities = torch.exp(log_probs) * mask.unsqueeze(-1)\n", "fix_pattern": "Fix pattern: \nIn the condition of mask being None, if torch.ones() is detected, then add device=logits.device to the torch.ones() function to fix the API misuse."}
{"number": 1044, "change": "def fmod(\nout: Optional[Union[tf.Tensor, tf.Variable]] = None,\n) -> Union[tf.Tensor, tf.Variable]:\nresult = tf.math.floormod(x1, x2, name=None)\n-    temp = (result, x1)\n-    return tf.map_fn(lambda x: x[0] if (x[0] * x[1] >= 0) else (-1 * x[0]), temp)\n+    temp = [result, x1]\n+    return tf.map_fn(lambda x: x[0] if (x[0] * x[1] >= 0) else (-1 * x[0]), temp, fn_output_signature=result.dtype)\n\n\ndef fmax(\n", "fix_pattern": "In the condition of `if x[0] * x[1] >= 0`, if the pattern `[result, x1]` is detected, then add `fn_output_signature=result.dtype` to the code to fix the API misuse."}
{"number": 1046, "change": "class MT5DenseGatedActDense(nn.Module):\n# To make 8bit quantization work for google/flan-t5-xxl, self.wo is kept in float32.\n# See https://github.com/huggingface/transformers/issues/20287\n# we also make sure the weights are not in `int8` in case users will force `_keep_in_fp32_modules` to be `None``\n-        if hidden_states.dtype != self.wo.weight.dtype and self.wo.weight.dtype != torch.int8:\n+        if (\n+            isinstance(self.wo.weight, torch.Tensor)\n+            and hidden_states.dtype != self.wo.weight.dtype\n+            and self.wo.weight.dtype != torch.int8\n+        ):\nhidden_states = hidden_states.to(self.wo.weight.dtype)\n\nhidden_states = self.wo(hidden_states)\n", "fix_pattern": "In the condition of checking the data type of `self.wo.weight`, if three conditions are met (1) `self.wo.weight` is a tensor, (2) the data type of `hidden_states` is different from the data type of `self.wo.weight`, and (3) the data type of `self.wo.weight` is not `torch.int8`, then the code is modified to fix the API misuse by converting `hidden_states` to the data type of `self.wo.weight`."}
{"number": 1048, "change": "def vector_to_skew_symmetric_matrix(vector):\na2s = vector_expanded[..., 1:2, :]\na3s = vector_expanded[..., 2:3, :]\n# BS x 1 x 1\n-    zs = torch.zeros(batch_shape + [1, 1])\n+    zs = torch.zeros(batch_shape + [1, 1], device=vector.device)\n# BS x 1 x 3\nrow1 = torch.cat((zs, -a3s, a2s), -1)\nrow2 = torch.cat((a3s, zs, -a1s), -1)\n", "fix_pattern": "in the condition of accessing the device of a tensor, if a device is not specified, then add the device argument to the function call to fix the API misuse."}
{"number": 1050, "change": "class PointAssigner(BaseAssigner):\n\nif gt_labels is not None:\nassigned_labels = assigned_gt_inds.new_full((num_points, ), -1)\n-            pos_inds = torch.nonzero(assigned_gt_inds > 0).squeeze()\n+            pos_inds = torch.nonzero(\n+                assigned_gt_inds > 0, as_tuple=False).squeeze()\nif pos_inds.numel() > 0:\nassigned_labels[pos_inds] = gt_labels[\nassigned_gt_inds[pos_inds] - 1]\n", "fix_pattern": "In the condition of checking if the assigned_gt_inds are greater than 0, if the pattern of using the as_tuple=False argument is detected, then the code is changed to fix the API misuse."}
{"number": 1052, "change": "def att_to_numpy(att_ws, att):\natt_ws = torch.stack([aw[:, -1] for aw in att_ws], dim=1).cpu().numpy()\nelif isinstance(att, (AttCov, AttCovLoc)):\n# att_ws => list of list of previous attentions\n-        att_ws = torch.stack([aw[-1] for aw in att_ws], dim=1).cpu().numpy()\n+        att_ws = torch.stack([aw[idx] for idx, aw in enumerate(att_ws)], dim=1).cpu().numpy()\nelif isinstance(att, AttLocRec):\n# att_ws => list of tuple of attention and hidden states\natt_ws = torch.stack([aw[0] for aw in att_ws], dim=1).cpu().numpy()\n", "fix_pattern": "in the condition of checking whether `att` is an instance of `AttCov` or `AttCovLoc`, if `aw` is a list and `aw` variable needs access through an index, then the code has been changed from accessing `aw[-1]` to `aw[idx]` to fix the API misuse."}
{"number": 1059, "change": "class TFOPTDecoder(tf.keras.layers.Layer):\nif output_attentions:\nall_self_attns += (layer_self_attn,)\n\n+        if self.final_layer_norm is not None:\n+            hidden_states = self.final_layer_norm(hidden_states)\n+\nif self.project_out is not None:\nhidden_states = self.project_out(hidden_states)\n", "fix_pattern": "In the condition of `if self.final_layer_norm is not None`, if `self.final_layer_norm` is detected, then add the line `hidden_states = self.final_layer_norm(hidden_states)` to fix the API misuse."}
{"number": 1065, "change": "class ParabolicEquationStepperTest(tf.test.TestCase, parameterized.TestCase):\nsecond_order_coeff_fn=second_order_coeff_fn,\ninner_first_order_coeff_fn=inner_first_order_coeff_fn)[0]\n\n-    true_values = tf.math.exp(final_t + grid[0])\n+    true_values = tf.expand_dims(tf.math.exp(final_t + grid[0]), axis=0)\nself.assertAllClose(\nest_values, true_values, atol=1e-2, rtol=1e-2)\n", "fix_pattern": "In the condition of calculating and comparing true values with estimated values, if there is a need to expand dimensions of the true values, then the code should be modified to include the 'tf.expand_dims' function to fix the API misuse."}
{"number": 1066, "change": "class LocalMultiGPUOptimizer(PolicyOptimizer):\nelse:\nrnn_inputs = []\nself.par_opt = LocalSyncParallelOptimizer(\n-                        tf.train.AdamOptimizer(\n-                            self.sgd_stepsize), self.devices,\n+                        self.policy.optimizer(), self.devices,\n[v for _, v in self.policy.loss_inputs()], rnn_inputs,\nself.per_device_batch_size, self.policy.copy,\nos.getcwd())\n", "fix_pattern": "in the condition of the class definition, if an incorrect API method is detected as the optimizer argument in the LocalSyncParallelOptimizer initialization, then change the optimizer argument to self.policy.optimizer() to fix the API misuse."}
{"number": 1067, "change": "class DistributedReplicatedBuilder(DataParallelBuilder):\nreturn grads\n\n# Ngpu * Nvar * 2\n-        grad_list = self.build_on_multi_tower(\n-            get_grads,\n+        grad_list = DataParallelBuilder.build_on_towers(\n+            self.towers, get_grads,\ndevices=self.raw_devices,\nuse_vs=[True] * len(self.towers))  # open vs at each tower\nDataParallelBuilder._check_grad_list(grad_list)\n", "fix_pattern": "In the condition of using the DataParallelBuilder class, if the method build_on_multi_tower is being called, then the pattern is to change it to build_on_towers to fix the API misuse."}
{"number": 1069, "change": "class OPTForSequenceClassification(OPTPreTrainedModel):\nsequence_lengths = -1\nelse:\nif input_ids is not None:\n-                sequence_lengths = torch.ne(input_ids, self.config.pad_token_id).sum(-1) - 1\n+                sequence_lengths = (torch.ne(input_ids, self.config.pad_token_id).sum(-1) - 1).to(logits.device)\nelse:\nsequence_lengths = -1\nlogger.warning(\n", "fix_pattern": "in the condition of \"input_ids is not None\", if the pattern \"(torch.ne(input_ids, self.config.pad_token_id).sum(-1) - 1)\" is detected, then add \".to(logits.device)\" to fix the API misuse."}
{"number": 1072, "change": "def test_discrete_parallel(continuous_class):\n\ndef model(data):\nweights = pyro.sample('weights', dist.Dirichlet(0.5 * torch.ones(K)))\n-        locs = pyro.sample('locs', dist.Normal(0, 10).reshape([K], extra_event_dims=1))\n+        locs = pyro.sample('locs', dist.Normal(0, 10).expand_by([K]).independent(1))\nscale = pyro.sample('scale', dist.LogNormal(0, 1))\n\nwith pyro.iarange('data', len(data)):\n", "fix_pattern": "In the condition of expanding an event dimension, if the API function \"reshape\" is detected, then change it to \"expand_by\" to fix the API misuse."}
{"number": 1075, "change": "class RenyiELBO(ELBO):\nsurrogate_elbo_particles = torch.stack(surrogate_elbo_particles)\n\nlog_weights = (1. - self.alpha) * elbo_particles\n-        log_mean_weight = logsumexp(log_weights, dim=0) - math.log(self.num_particles)\n+        log_mean_weight = torch.logsumexp(log_weights, dim=0) - math.log(self.num_particles)\nelbo = log_mean_weight.sum().item() / (1. - self.alpha)\n\n# collect parameters to train from model and guide\n", "fix_pattern": "In the condition where the logsumexp function is being used, the fix pattern involves removing the incorrect use of math.log and replacing it with torch.logsumexp to correct the API misuse."}
{"number": 1077, "change": "class MaskTokensDataset(BaseWrapperDataset):\nif self.mask_whole_words is not None:\nmask = np.repeat(mask, word_lens)\nnew_item = np.full(len(mask), self.pad_idx)\n-                new_item[mask] = item[torch.from_numpy(mask)]\n+                new_item[mask] = item[torch.from_numpy(mask.astype(np.uint8))]\nreturn torch.from_numpy(new_item)\n\n# decide unmasking and random replacement\n", "fix_pattern": "in the condition of the existence of `self.mask_whole_words`, if the array `mask` is used as an index, then change `torch.from_numpy(mask)` to `torch.from_numpy(mask.astype(np.uint8))` to fix the API misuse."}
{"number": 1083, "change": "class CategoricalAccuracy(Metric):\n# ith entry in gold_labels points to index (0-num_classes) for ith row in max_predictions\n# For each row check if index pointed by gold_label is was 1 or not (among max scored classes)\ncorrect = max_predictions_mask[\n-                torch.arange(gold_labels.numel()).long(), gold_labels\n+                torch.arange(gold_labels.numel(), device=gold_labels.device).long(), gold_labels\n].float()\ntie_counts = max_predictions_mask.sum(-1)\ncorrect /= tie_counts.float()\n", "fix_pattern": "Fix_pattern:\nin the condition of \"accessing tensor elements using indexing\", if \"using an index tensor created with torch.arange()\" is detected, then \"add ', device=gold_labels.device' to the indexing tensor\" to fix the API misuse."}
{"number": 1091, "change": "class Model(ModelDesc):\n.GlobalAvgPooling('gap')\n.FullyConnected('linear', 1000, nl=tf.identity)())\n\n-        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, label)\n+        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)\nloss = tf.reduce_mean(loss, name='xentropy-loss')\n\nwrong = prediction_incorrect(logits, label, 1, name='wrong-top1')\n", "fix_pattern": "Fix_pattern: in the condition of API misuse, if incorrectly referenced arguments are detected, then change the arguments in the code to fix the API misuse."}
{"number": 1094, "change": "def batchnorm_example(optimizer_fn,\nfor z in range(batch_per_epoch)]).repeat()\n\noptimizer = optimizer_fn()\n-  batchnorm = tf.compat.v1.layers.BatchNormalization(\n+  batchnorm = normalization.BatchNormalization(\nrenorm=renorm, momentum=momentum, fused=False)\n-  layer = tf.compat.v1.layers.Dense(1, use_bias=False)\n+  layer = core.Dense(1, use_bias=False)\n\ndef model_fn(x):\n\"\"\"A model that uses batchnorm.\"\"\"\n", "fix_pattern": "In the condition of using the `tf.compat.v1.layers` module for batch normalization, if the code contains `tf.compat.v1.layers.BatchNormalization`, then change it to `normalization.BatchNormalization` to fix the API misuse. Similarly, change `tf.compat.v1.layers.Dense` to `core.Dense` when creating a layer."}
{"number": 1100, "change": "class TorchHook(object):\n\nself._hook_torch_module()\n\n+        if torch.torch_hooked > 0:\n+            raise Exception('Torch was already hooked')\n+\ndef _hook_native_tensors_and_variables(self, tensor_type):\n\"\"\"Overloading a given tensor_type\"\"\"\n# Overload 'special' methods here\n", "fix_pattern": "In the condition of `if torch.torch_hooked > 0`, if the pattern of `Torch was already hooked` is detected, then the code is added to raise an exception in order to fix the API misuse."}
{"number": 1104, "change": "class EpsilonDecay(Exploration):\n\npred = tf.logical_or(x=(timestep < self.start_timestep),\ny=(timestep > self.start_timestep + int(self.timesteps)))\n-        return tf.cond(pred=pred, true_fn=true_fn, false_fn=false_fn)\n+        return tf.fill(dims=shape, value=tf.cond(pred=pred, true_fn=true_fn, false_fn=false_fn))\n", "fix_pattern": "Fix pattern: \nin the condition of tf.cond, if the value returned is used as a fill value, then add tf.fill(dims=shape, value=) to fix the API misuse."}
{"number": 1118, "change": "def _scale_channel(im: torch.Tensor) -> torch.Tensor:\n# and then normalization by step.\nlut = (torch.cumsum(histo, 0) + (step // 2)) // step\n# Shift lut, prepending with 0.\n-        lut = torch.cat([torch.zeros(1, device=lut.device), lut[:-1]])\n+        lut = torch.cat([torch.zeros(1, device=lut.device, dtype=lut.dtype), lut[:-1]])\n# Clip the counts to be in range.  This is done\n# in the C code for image.point.\nreturn torch.clamp(lut, 0, 255)\n", "fix_pattern": "Fix_pattern: In the condition of passing device and dtype arguments to torch.zeros(), if the dtype is detected, then add the dtype argument to fix the API misuse."}
{"number": 1122, "change": "class DonutSwinLayer(nn.Module):\n# partition windows\nhidden_states_windows = window_partition(shifted_hidden_states, self.window_size)\nhidden_states_windows = hidden_states_windows.view(-1, self.window_size * self.window_size, channels)\n-        attn_mask = self.get_attn_mask(height_pad, width_pad)\n+        attn_mask = self.get_attn_mask(height_pad, width_pad, dtype=hidden_states.dtype)\nif attn_mask is not None:\nattn_mask = attn_mask.to(hidden_states_windows.device)\n", "fix_pattern": "in the condition of `attn_mask is not None`, if `self.get_attn_mask` returns `dtype=hidden_states.dtype`, then add `dtype=hidden_states.dtype` to fix the API misuse."}
{"number": 1127, "change": "class VisualBertEmbeddings(nn.Module):\ninputs_embeds = self.word_embeddings(input_ids)\n\nif token_type_ids is None:\n-            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.input_embeds.device)\n+            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)\n\ntoken_type_embeddings = self.token_type_embeddings(token_type_ids)\n", "fix_pattern": "In the condition of the variable being None, if the API misuse pattern of using the wrong device is detected, then change the code to fix the API misuse by using the correct device."}
{"number": 1128, "change": "class BCELossMasked(nn.Module):\nReturns:\nloss: An average loss value in range [0, 1] masked by the length.\n\"\"\"\n-        # mask: (batch, max_len, 1)\ntarget.requires_grad = False\nif length is not None:\n-            mask = sequence_mask(sequence_length=length, max_len=target.size(1)).float()\n-            x = x * mask\n-            target = target * mask\n+            # mask: (batch, max_len, 1)\n+            mask = sequence_mask(sequence_length=length, max_len=target.size(1))\nnum_items = mask.sum()\n+            loss = functional.binary_cross_entropy_with_logits(x.masked_select(mask), target.masked_select(mask), pos_weight=self.pos_weight, reduction=\"sum\")\nelse:\n+            loss = functional.binary_cross_entropy_with_logits(x, target, pos_weight=self.pos_weight, reduction=\"sum\")\nnum_items = torch.numel(x)\n-        loss = functional.binary_cross_entropy_with_logits(x, target, pos_weight=self.pos_weight, reduction=\"sum\")\nloss = loss / num_items\nreturn loss\n", "fix_pattern": "in the condition of <if length is not None>, if <mask> is detected, then(remove) the <x = x * mask, target = target * mask> to fix the API misuse."}
{"number": 1129, "change": "class PrioritizedReplay(Memory):\n))\n\nwith tf.control_dependencies(control_inputs=assignments):\n-            return tf.no_op()\n+            return util.no_operation()\n\n# These are not supported for prioritized replay currently.\ndef tf_retrieve_episodes(self, n):\n", "fix_pattern": "in the condition of having an unsupported operation for prioritized replay, the fix pattern is to change the code from `tf.no_op()` to `util.no_operation()` to fix the API misuse."}
{"number": 1132, "change": "class TFFastSpeech(tf.keras.Model):\n== config.decoder_self_attention_params.hidden_size,\nname=\"decoder\",\n)\n-        self.mel_dense = tf.keras.layers.Dense(units=config.num_mels, name=\"mel_before\")\n-        self.postnet = TFTacotronPostnet(config=config, name=\"postnet\")\n+        self.mel_dense = tf.keras.layers.Dense(units=config.num_mels, dtype=tf.float32, name=\"mel_before\")\n+        self.postnet = TFTacotronPostnet(config=config, dtype=tf.float32, name=\"postnet\")\n\nself.setup_inference_fn()\n", "fix_pattern": "In the condition of initializing a tf.keras.layers.Dense layer, if the dtype is not specified, add dtype=tf.float32 to fix the API misuse."}
{"number": 1141, "change": "class H3FeatureMixin(BaseFeatureMixin):\n):\ncolumn = input_df[feature_config[COLUMN]]\nif column.dtype == object:\n-            column = column.map(int)\n-        column = column.map(H3FeatureMixin.h3_to_list)\n+            column = backend.df_engine.map_objects(column, int)\n+        column = backend.df_engine.map_objects(column, H3FeatureMixin.h3_to_list)\n\nproc_df[feature_config[PROC_COLUMN]] = backend.df_engine.map_objects(\ncolumn, lambda x: np.array(x, dtype=np.uint8)\n", "fix_pattern": "in the condition of \"column.dtype == object\", if \"column.map(int)\" is detected, then the code is changed to \"backend.df_engine.map_objects(column, int)\" to fix the API misuse."}
{"number": 1144, "change": "class VariationalSparseGP(GPModel):\nM = self.Xu.size(0)\nKuu = self.kernel(self.Xu).contiguous()\nKuu.view(-1)[::M + 1] += self.jitter  # add jitter to the diagonal\n-        Luu = Kuu.cholesky()\n+        Luu = torch.linalg.cholesky(Kuu)\n\nzero_loc = self.Xu.new_zeros(self.u_loc.shape)\nif self.whiten:\n", "fix_pattern": "In the condition of using a cholesky function, if the cholesky function from the torch module is detected, then replace the cholesky function from the previous module with the torch cholesky function to fix the API misuse."}
{"number": 1148, "change": "def _matvecmul(x, y):\n\n\ndef _cholesky(x):\n-    return x.sqrt() if x.dim() == 1 else x.cholesky()\n+    return x.sqrt() if x.dim() == 1 else torch.linalg.cholesky(x)\n\n\ndef _transpose(x):\n", "fix_pattern": "In the condition of `x.dim() == 1`, if the pattern `x.cholesky()` is detected, then the code `torch.linalg.cholesky(x)` should be added to fix the API misuse."}
{"number": 1155, "change": "def convert_to_numpy(x: TensorStructType, reduce_floats: bool = False):\nif torch and isinstance(item, torch.Tensor):\nret = item.cpu().item() if len(item.size()) == 0 else \\\nitem.detach().cpu().numpy()\n-        elif tf and isinstance(item, (tf.Tensor, tf.Variable)):\n+        elif tf and isinstance(item, (tf.Tensor, tf.Variable)) and \\\n+                hasattr(item, \"numpy\"):\nassert tf.executing_eagerly()\nret = item.numpy()\nelse:\n", "fix_pattern": "Fix_pattern:  In the condition of checking if the item is an instance of either tf.Tensor or tf.Variable, if the item has the attribute \"numpy\", then the code is changed to item.numpy() to fix the API misuse."}
{"number": 1163, "change": "class OpenAIGPTDoubleHeadsModel(OpenAIGPTPreTrainedModel):\ntokenizer = OpenAIGPTTokenizer.from_pretrained('openai-gpt')\nmodel = OpenAIGPTDoubleHeadsModel.from_pretrained('openai-gpt')\ntokenizer.add_special_tokens({'cls_token': '[CLS]'})  # Add a [CLS] to the vocabulary (we should train it also!)\n+        model.resize_token_embeddings(len(tokenizer))\n+\nchoices = [\"Hello, my dog is cute [CLS]\", \"Hello, my cat is cute [CLS]\"]\ninput_ids = torch.tensor([tokenizer.encode(s) for s in choices]).unsqueeze(0)  # Batch size 1, 2 choices\n-        mc_token_ids = torch.tensor([input_ids.size(-1), input_ids.size(-1)]).unsqueeze(0)  # Batch size 1\n+        mc_token_ids = torch.tensor([input_ids.size(-1)-1, input_ids.size(-1)-1]).unsqueeze(0)  # Batch size 1\n+\noutputs = model(input_ids, mc_token_ids=mc_token_ids)\nlm_prediction_scores, mc_prediction_scores = outputs[:2]\n", "fix_pattern": "In the condition of initializing the model and tokenizer, if the issue of tokenizing special tokens is detected, then the code is modified to resize the token embeddings to match the tokenizer length to fix the API misuse."}
{"number": 1164, "change": "class Model(ModelDesc):\nadd_moving_summary(tf.reduce_mean(wrong, name='train_error'))\n\n# weight decay on all W of fc layers\n-        wd_cost = tf.mul(0.0004,\n-                         regularize_cost('fc.*/W', tf.nn.l2_loss),\n-                         name='regularize_loss')\n+        wd_cost = regularize_cost('fc.*/W', l2_regularizer(4e-4), name='regularize_loss')\nadd_moving_summary(cost, wd_cost)\n\nadd_param_summary(('.*/W', ['histogram']))   # monitor W\n", "fix_pattern": "In the condition of \"weight decay on all W of fc layers\", if the pattern \"tf.mul(0.0004, regularize_cost('fc.*/W', tf.nn.l2_loss), name='regularize_loss')\" is detected, then remove the \"tf.mul(0.0004,\" and replace it with \"l2_regularizer(4e-4),\" to fix the API misuse."}
{"number": 1168, "change": "def reshape(\nshape: Union[ivy.NativeShape, Sequence[int]],\n*,\ncopy: Optional[bool] = None,\n-    out: Optional[tf.Tensor] = None,\n+    out: Optional[Union[tf.Tensor, tf.Variable]] = None,\n) -> Union[tf.Tensor, tf.Variable]:\nif copy:\nnewarr = tf.experimental.numpy.copy(x)\n", "fix_pattern": "In the condition of \"if copy:\", if the pattern \"out: Optional[tf.Tensor] = None,\" is detected, then change the code to \"out: Optional[Union[tf.Tensor, tf.Variable]] = None,\" to fix the API misuse."}
{"number": 1191, "change": "class TestInvertAffineTransform:\nassert_allclose(matrix_inv, expected)\n\ndef test_gradcheck(self, device):\n-        matrix = torch.eye(2, 3).to(device)\n+        matrix = torch.eye(2, 3).to(device)[None]\nmatrix = utils.tensor_to_gradcheck_var(matrix)  # to var\nassert gradcheck(kornia.invert_affine_transform, (matrix,),\nraise_exception=True)\n", "fix_pattern": "In the condition of a test_gradcheck function, if the condition to check the size of a tensor is detected, then add \"[None]\" to fix the API misuse."}
{"number": 1193, "change": "class RNN(torch.nn.Module):\ndef __init__(self, idim, elayers, cdim, hdim, dropout, typ=\"blstm\"):\nsuper(RNN, self).__init__()\nbidir = typ[0] == \"b\"\n-        self.nblstm = torch.nn.LSTM(idim, cdim, elayers, batch_first=True,\n-                                    dropout=dropout, bidirectional=bidir) if \"lstm\" in typ \\\n+        self.nbrnn = torch.nn.LSTM(idim, cdim, elayers, batch_first=True,\n+                                   dropout=dropout, bidirectional=bidir) if \"lstm\" in typ \\\nelse torch.nn.GRU(idim, cdim, elayers, batch_first=True, dropout=dropout,\nbidirectional=bidir)\nif bidir:\n", "fix_pattern": "In the condition of \"bidir\", if \"lstm\" is detected, then change the code from \"nblstm\" to \"nbrnn\" to fix the API misuse."}
{"number": 1197, "change": "class TFData2VecVisionForSemanticSegmentation(TFData2VecVisionPreTrainedModel):\n# FPNs\nself.fpn1 = [\ntf.keras.layers.Conv2DTranspose(config.hidden_size, kernel_size=2, strides=2, name=\"fpn1.0\"),\n-            tf.keras.layers.BatchNormalization(name=\"fpn1.1\"),\n+            tf.keras.layers.BatchNormalization(name=\"fpn1.1\", momentum=0.9, epsilon=1e-5),\ntf.keras.layers.Activation(\"gelu\"),\ntf.keras.layers.Conv2DTranspose(config.hidden_size, kernel_size=2, strides=2, name=\"fpn1.3\"),\n]\n", "fix_pattern": "Fix pattern: \nIn the condition of using BatchNormalization in keras layers, if the pattern of missing momentum and epsilon values is detected, then add momentum=0.9 and epsilon=1e-5 to fix the API misuse."}
{"number": 1199, "change": "def main():\nlogger.info(f\"Number of class images to sample: {num_new_images}.\")\n\nsample_dataset = PromptDataset(args.class_prompt, num_new_images)\n-            sample_dataloader = torch.utils.data.DataLoader(sample_dataset, batch_size=args.sample_batch_size)\n+            total_sample_batch_size = args.sample_batch_size * jax.local_device_count()\n+            sample_dataloader = torch.utils.data.DataLoader(sample_dataset, batch_size=total_sample_batch_size)\n\nfor example in tqdm(\nsample_dataloader, desc=\"Generating class images\", disable=not jax.process_index() == 0\n", "fix_pattern": "Fix_pattern: In the condition of initializing the DataLoader, if the batch size is being adjusted based on the number of devices, the code is changed to multiply the batch size by the number of devices to fix the API misuse."}
{"number": 1200, "change": "class TokenCharactersIndexer(TokenIndexer[List[int]]):\n# Removes the \"dummy token\".\npadded_tokens.pop()\n# Truncates all the tokens to the desired length, and return the result.\n-        return {key: [list(token[:desired_token_length]) for token in padded_tokens]}\n+        return {key: torch.LongTensor([list(token[:desired_token_length])\n+                                       for token in padded_tokens])}\n", "fix_pattern": "Fix_pattern: \n\nin the condition where the code is returning a dictionary containing lists of tokens, if the code is returning a list of lists of integers, replace the code with a torch LongTensor to fix the API misuse."}
{"number": 1257, "change": "class Module(tf.Module):\nelif initializer == 'ones':\ninitializer = tf_util.ones(shape=spec.shape, dtype=spec.type)\nelif initializer == 'constant':\n-            initializer = tf_util.fill(dims=spec.shape, value=self.initialization_scale)\n+            initializer = tf.fill(\n+                dims=spec.shape, value=tf_util.constant(value=initialization_scale, dtype=spec.type)\n+            )\n\n# Variable\nvariable = tf.Variable(\n", "fix_pattern": "In the condition of \"if initializer == 'constant'\", if the pattern \"tf_util.fill(dims=spec.shape, value=self.initialization_scale)\" is detected, then change the code to \"tf.fill(dims=spec.shape, value=tf_util.constant(value=initialization_scale, dtype=spec.type))\" to fix the API misuse."}
{"number": 1265, "change": "def test_transformer_conv():\n\nt = '(PairTensor, SparseTensor, NoneType) -> Tensor'\njit = torch.jit.script(conv.jittable(t))\n-    assert torch.allclose(conv((x1, x2), adj.t()), out, atol=1e-6)\n+    assert torch.allclose(jit((x1, x2), adj.t()), out, atol=1e-6)\n", "fix_pattern": "In the condition of testing the `conv` function, if the `jit` was not called with the correct parameters, then change the code from `conv(...)` to `jit(...)`. This fixes the API misuse by ensuring that the correct function is called with the appropriate parameters."}
{"number": 1266, "change": "PT_SEQUENCE_CLASSIFICATION_SAMPLE = r\"\"\"\n...     \"{checkpoint}\", num_labels=num_labels, problem_type=\"multi_label_classification\"\n... )\n\n-    >>> labels = torch.nn.functional.one_hot(torch.tensor(predicted_class_ids), num_classes=num_labels).to(torch.float)\n+    >>> labels = torch.sum(\n+    ...     torch.nn.functional.one_hot(predicted_class_ids[None, :].clone(), num_classes=num_labels), dim=1\n+    ... ).to(torch.float)\n>>> loss = model(**inputs, labels=labels).loss\n```\n\"\"\"\n", "fix_pattern": "In the condition of sequence classification, if the variable \"predicted_class_ids\" needs to be one-hot encoded, then change the code from \"torch.nn.functional.one_hot(torch.tensor(predicted_class_ids), num_classes=num_labels).to(torch.float)\" to \"torch.sum(torch.nn.functional.one_hot(predicted_class_ids[None, :].clone(), num_classes=num_labels), dim=1).to(torch.float)\" to fix the API misuse."}
{"number": 1273, "change": "def test_gat_conv():\n\nt = '(OptPairTensor, SparseTensor, Size, NoneType) -> Tensor'\njit = torch.jit.script(conv.jittable(t))\n-    assert torch.allclose(conv((x1, x2), adj.t()), out1, atol=1e-6)\n-    assert torch.allclose(conv((x1, None), adj.t()), out2, atol=1e-6)\n+    assert torch.allclose(jit((x1, x2), adj.t()), out1, atol=1e-6)\n+    assert torch.allclose(jit((x1, None), adj.t()), out2, atol=1e-6)\n", "fix_pattern": "In the condition of calling the function `conv`, if the `jit` function is used instead, then change the code from `conv((x1, x2), adj.t())` to `jit((x1, x2), adj.t())` to fix the API misuse."}
{"number": 1282, "change": "def multi_perspective_match_pairwise(\nnorm_value = vector1_norm * vector2_norm.transpose(2, 3)\n\n# (batch, seq_len1, seq_len2, num_perspectives)\n-    return (mul_result / norm_value.clamp(min=eps)).permute(0, 2, 3, 1)\n+    return (mul_result / norm_value.clamp(min=tiny_value_of_dtype(norm_value.dtype))).permute(\n+        0, 2, 3, 1\n+    )\n\n\nclass BiMpmMatching(nn.Module, FromParams):\n", "fix_pattern": "In the condition of \"clamping the norm_value\", if the \"min\" argument is not provided, then add the \"tiny_value_of_dtype(norm_value.dtype)\" argument to fix the API misuse."}
{"number": 1290, "change": "def train(hyp, opt, device, tb_writer=None):\nif rank != -1:\nindices = torch.zeros([dataset.n], dtype=torch.int)\nif rank == 0:\n-                    indices[:] = torch.from_tensor(dataset.indices, dtype=torch.int)\n+                    indices[:] = torch.tensor(dataset.indices, dtype=torch.int)\ndist.broadcast(indices, 0)\nif rank != 0:\ndataset.indices = indices.cpu().numpy()\n", "fix_pattern": "Fix_pattern: In the condition of \"rank == 0\", if \"torch.from_tensor\" is detected, then change the code to \"torch.tensor\" to fix the API misuse."}
{"number": 1294, "change": "class SCSEModule(nn.Module):\nnn.Conv2d(in_channels // reduction, in_channels, 1),\nnn.Sigmoid(),\n)\n-        self.sSE = nn.Sequential(nn.Conv2d(in_channels, in_channels, 1), nn.Sigmoid())\n+        self.sSE = nn.Sequential(nn.Conv2d(in_channels, 1, 1), nn.Sigmoid())\n\ndef forward(self, x):\nreturn x * self.cSE(x) + x * self.sSE(x)\n", "fix_pattern": "In the condition of \"incorrect number of channels used in nn.Conv2d\", if \"input channels are changed from in_channels to 1\", then \"change the input channels in nn.Conv2d to fix the API misuse.\""}
{"number": 1296, "change": "def interpolate(\nsize = [x.shape[0], *size, x.shape[1]]\n\nif align_corners or mode == \"area\":\n-        return ivy.interpolate(\n+        return ivy.functional.experimental.interpolate(\nx, size, mode=mode, align_corners=align_corners, antialias=antialias\n)\nx = jnp.transpose(x, (0, *range(2, dims + 2), 1))\n", "fix_pattern": "in the condition of align_corners or mode == \"area\", if ivy.interpolate is used, then change it to ivy.functional.experimental.interpolate to fix the API misuse."}
{"number": 1308, "change": "def count_nonzero(\ndef _dtype_count_nonzero(a, axis, dtype):\nif dtype is None:\nreturn torch.count_nonzero(a, dim=axis)\n-        return torch.tensor(torch.count_nonzero(a, dim=axis), dtype=dtype)\n+        return torch.tensor(torch.count_nonzero(a, dim=axis),\n+                            dtype=ivy.as_native_dtype(dtype))\n\nx = _dtype_count_nonzero(a, axis, dtype)\nif not keepdims:\n", "fix_pattern": "In the condition of \"dtype is None\", if the pattern \"torch.tensor(torch.count_nonzero(a, dim=axis), dtype=dtype)\" is detected, then change the \"dtype\" argument of the \"torch.tensor\" function call to \"ivy.as_native_dtype(dtype)\" to fix the API misuse."}
{"number": 1312, "change": "class TorchTensor(AbstractTensor):\n\n\"\"\"\n\n-        assert isinstance(self.child, PointerTensor)\n+        if not isinstance(self.child, PointerTensor):\n+            raise TypeError(\"child should be a PointerTensor\")\n\nps = list(pointers)\nps.append(self)\n", "fix_pattern": "In the condition of \"child is not a PointerTensor\", if \"pattern\" (incorrect usage of assert) is detected, then remove the assert statement and raise a TypeError instead to fix the API misuse."}
{"number": 1313, "change": "def unravel_index(\nfor dim in reversed(shape):\noutput.append(temp % dim)\ntemp = temp // dim\n-    return tuple(reversed(output))\n+    return torch.tensor(reversed(output))\n\n\nunravel_index.support_native_out = False\n", "fix_pattern": "In the condition of using the unravel_index function, if the code is missing a correct return type, then add \"torch.tensor\" to fix the API misuse."}
{"number": 1315, "change": "class CanineSelfAttention(nn.Module):\n# Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n# masked positions, this operation will create a tensor which is 0.0 for\n# positions we want to attend and -10000.0 for masked positions.\n-                attention_mask = (1.0 - attention_mask.float()) * -10000.0\n+                attention_mask = (1.0 - attention_mask.float()) * torch.finfo(attention_scores.dtype).min\n# Apply the attention mask (precomputed for all layers in CanineModel forward() function)\nattention_scores = attention_scores + attention_mask\n", "fix_pattern": "Fix_pattern: \nIn the condition of incorrect API usage, if a specific pattern is detected, then change the code to fix the API misuse by using the appropriate method."}
{"number": 1323, "change": "class TFModel(NNModel, metaclass=TfModelMeta):\nopt_scope = tf.variable_scope(optimizer_scope_name)\nwith opt_scope:\nif learnable_scopes is None:\n-                variables_to_train = tf.trainable_variables()\n+                variables_to_train = tf.global_variables()\nelse:\nvariables_to_train = []\nfor scope_name in learnable_scopes:\n-                    for var in tf.trainable_variables():\n+                    for var in tf.global_variables():\nif scope_name in var.name:\nvariables_to_train.append(var)\n", "fix_pattern": "In the condition of `if learnable_scopes is None`, if the pattern `tf.trainable_variables()` is detected, then change the code to `tf.global_variables()` to fix the API misuse."}
{"number": 1324, "change": "def abs(x):\n\n\ndef sqrt(x):\n-    x = tf.clip_by_value(x, _EPSILON, np.inf)\n+    x = tf.clip_by_value(x, 0., np.inf)\nreturn tf.sqrt(x)\n", "fix_pattern": "In the condition of \"sqrt\" function, if the pattern of using \"_EPSILON\" as the clipping threshold is detected, then change the code to use \"0.0\" as the clipping threshold to fix the API misuse."}
{"number": 1346, "change": "def reset_deterministic_algorithm():\nyield\nif _TORCH_GREATER_EQUAL_1_8:\ntorch.use_deterministic_algorithms(False)\n-    elif _TORCH_GREATER_EQUAL_1_7:\n+    else:\ntorch.set_deterministic(False)\n-    else:  # the minimum version Lightning supports is PyTorch 1.6\n-        torch._set_deterministic(False)\n\n\n@pytest.fixture\n", "fix_pattern": "In the condition of checking the PyTorch version, if the pattern `_TORCH_GREATER_EQUAL_1_7` is detected, then the code `torch.set_deterministic(False)` should be replaced with `torch.use_deterministic_algorithms(False)` to fix the API misuse."}
{"number": 1357, "change": "class MultiHeadAttention(nn.Module):\n# perform attention, result size = (n_head * mb_size) x len_q x d_v\noutputs, attns = self.attention(q_s, k_s, v_s, attn_mask=attn_mask.repeat(n_head, 1, 1))\n\n-        # back to original mb_size batch\n-        outputs = outputs.view(mb_size, len_q, -1)            # mb_size x len_q x (n_head*d_v)\n+        # back to original mb_size batch, result size = mb_size x len_q x (n_head*d_v)\n+        outputs = torch.cat(torch.split(outputs, mb_size, dim=0), dim=-1)\n\n# project back to residual size\noutputs = self.proj(outputs)\n", "fix_pattern": "In the condition of resizing the outputs to the original mb_size batch, if the pattern of using the view() function is detected, then change the code to use the torch.cat() and torch.split() functions to correctly resize the outputs. This fixes the API misuse of the view() function."}
{"number": 1363, "change": "def get_timestep_embedding(\nassert len(timesteps.shape) == 1, \"Timesteps should be a 1d-array\"\n\nhalf_dim = embedding_dim // 2\n-    exponent = -math.log(max_period) * torch.arange(start=0, end=half_dim, dtype=torch.float32)\n+    exponent = -math.log(max_period) * torch.arange(\n+        start=0, end=half_dim, dtype=torch.float32, device=timesteps.device\n+    )\nexponent = exponent / (half_dim - downscale_freq_shift)\n\n-    emb = torch.exp(exponent).to(device=timesteps.device)\n+    emb = torch.exp(exponent)\nemb = timesteps[:, None].float() * emb[None, :]\n\n# scale embeddings\n", "fix_pattern": "In the condition of checking the number of dimensions, if the incorrect device assignment is detected when creating a range, then the device argument should be added to fix the API misuse."}
{"number": 1365, "change": "class TestNormalize:\nf = kornia.enhance.Normalize(mean=mean, std=std)\ndata = torch.ones(2, 3, 256, 313)\nif isinstance(mean, float):\n-            expected = (data - torch.tensor(mean)) / torch.tensor(std)\n+            expected = (data - torch.as_tensor(mean)) / torch.as_tensor(std)\nelse:\n-            expected = (data - torch.tensor(mean[0])) / torch.tensor(std[0])\n+            expected = (data - torch.as_tensor(mean[0])) / torch.as_tensor(std[0])\nassert_close(f(data), expected)\n\n@staticmethod\n", "fix_pattern": "In the condition of \"isinstance(mean, float)\", if \"torch.tensor\" is detected, then change it to \"torch.as_tensor\" to fix the API misuse."}
{"number": 1373, "change": "def train_func(config):\ntrain_dataset = Subset(train_dataset, list(range(64)))\nvalidation_dataset = Subset(validation_dataset, list(range(64)))\n\n-    train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"])\n-    validation_loader = DataLoader(validation_dataset, batch_size=config[\"batch_size\"])\n+    worker_batch_size = config[\"batch_size\"] // train.world_size()\n+\n+    train_loader = DataLoader(train_dataset, batch_size=worker_batch_size)\n+    validation_loader = DataLoader(validation_dataset, batch_size=worker_batch_size)\n\ntrain_loader = train.torch.prepare_data_loader(train_loader)\nvalidation_loader = train.torch.prepare_data_loader(validation_loader)\n", "fix_pattern": "in the condition of using distributed training with multiple workers, if the batch size is divided by the number of workers, then change the batch size in the DataLoader to worker_batch_size to fix the API misuse."}
{"number": 1378, "change": "class FNetEmbeddings(nn.Module):\nif version.parse(torch.__version__) > version.parse(\"1.6.0\"):\nself.register_buffer(\n\"token_type_ids\",\n-                torch.zeros(self.position_ids.size(), dtype=torch.long, device=self.position_ids.device),\n+                torch.zeros(self.position_ids.size(), dtype=torch.long),\npersistent=False,\n)\n", "fix_pattern": "In the condition of checking the torch version, if there is a call to torch.zeros with a specified device, then the fix is to remove the device argument in order to fix the API misuse."}
{"number": 1383, "change": "class TestInvertAffineTransform:\n\ndef test_rot90_batch(self, device):\nangle = torch.tensor([90.]).to(device)\n-        scale = torch.tensor([1.]).to(device)\n+        scale = torch.tensor([[1., 1.]]).to(device)\ncenter = torch.tensor([[0., 0.]]).to(device)\nexpected = torch.tensor([[\n[0., -1., 0.],\n", "fix_pattern": "In the condition of updating the scale parameter, if a single value is replaced with a 2D tensor, then add the missing brackets in the code to fix the API misuse."}
{"number": 1384, "change": "class LDMSuperResolutionPipelineIntegrationTests(unittest.TestCase):\nldm.to(torch_device)\nldm.set_progress_bar_config(disable=None)\n\n-        generator = torch.Generator(device=torch_device).manual_seed(0)\n+        generator = torch.manual_seed(0)\nimage = ldm(image=init_image, generator=generator, num_inference_steps=20, output_type=\"numpy\").images\n\nimage_slice = image[0, -3:, -3:, -1]\n\nassert image.shape == (1, 256, 256, 3)\n-        expected_slice = np.array([0.7418, 0.7472, 0.7424, 0.7422, 0.7463, 0.726, 0.7382, 0.7248, 0.6828])\n+        expected_slice = np.array([0.7644, 0.7679, 0.7642, 0.7633, 0.7666, 0.7560, 0.7425, 0.7257, 0.6907])\n+\nassert np.abs(image_slice.flatten() - expected_slice).max() < 1e-2\n", "fix_pattern": "In the condition of initializing a torch generator, if the code is using `torch.Generator(device=torch_device)`, then remove the `device=torch_device` argument to fix the API misuse."}
{"number": 1397, "change": "def binary_config():\ndef test_binary_input_feature(binary_config: Dict, encoder: str) -> None:\nbinary_config.update({\"encoder\": encoder})\nbinary_input_feature = BinaryInputFeature(binary_config)\n-    binary_tensor = torch.randn([SEQ_SIZE, BINARY_W_SIZE], dtype=torch.float32)\n+    binary_tensor = torch.randn([SEQ_SIZE, BINARY_W_SIZE], dtype=torch.float32).to(DEVICE)\nencoder_output = binary_input_feature(binary_tensor)\nassert encoder_output[\"encoder_output\"].shape[1:] == binary_input_feature.output_shape\n", "fix_pattern": "In the condition of using a torch tensor, if the code does not specify a device, then add \".to(DEVICE)\" to fix the API misuse."}
{"number": 1409, "change": "class UnCLIPPipelineIntegrationTests(unittest.TestCase):\npipeline = pipeline.to(torch_device)\npipeline.set_progress_bar_config(disable=None)\n\n-        generator = torch.Generator(device=torch_device).manual_seed(0)\n+        generator = torch.Generator(device=\"cpu\").manual_seed(0)\noutput = pipeline(\n\"horse\",\nnum_images_per_prompt=1,\n", "fix_pattern": "In the condition of instantiating the torch.Generator, if the device parameter is detected to be set to the wrong value, then change the device parameter to \"cpu\" to fix the API misuse."}
{"number": 1415, "change": "class SwapBufferManager(object):\nself.count = count\nself.dtype = dtype\nself.all_buffers = [\n-            torch.zeros(num_elems,\n-                        device='cpu',\n-                        dtype=dtype).pin_memory() for _ in range(count)\n+            get_accelerator().pin_memory(\n+                torch.zeros(num_elems,\n+                            device='cpu',\n+                            dtype=dtype)) for _ in range(count)\n]\nself.free_buffer_index = [i for i in range(count)]\nself.used_buffer_index = {}\n", "fix_pattern": "In the condition of \"API misuse\", if the pattern of incorrectly specifying the device and dtype is detected, then add the \"get_accelerator().pin_memory()\" function to fix the issue."}
{"number": 1422, "change": "def _setup_ddp(rank, worldsize):\ndef _ddp_test_fn(rank, worldsize):\n_setup_ddp(rank, worldsize)\ntensor = torch.tensor([1.0])\n-    actual = LightningModule._LightningModule__sync(tensor, sync_dist=True, sync_dist_op=torch.distributed.ReduceOp.SUM)\n+    sync = _Sync(sync_ddp_if_available, should=True, op=torch.distributed.ReduceOp.SUM)\n+    actual = sync(tensor)\nassert actual.item() == dist.get_world_size(), \"Result-Log does not work properly with DDP and Tensors\"\n", "fix_pattern": "In the condition of API misuse with `LightningModule.__sync`, if the pattern `LightningModule._LightningModule__sync` is detected, then change the code to use the `_Sync` class for synchronization."}
{"number": 1427, "change": "def test_gcn2_conv():\n\nt = '(Tensor, Tensor, SparseTensor, OptTensor) -> Tensor'\njit = torch.jit.script(conv.jittable(t))\n-    assert torch.allclose(conv(x, x_0, adj1.t()), out1, atol=1e-6)\n-    assert torch.allclose(conv(x, x_0, adj2.t()), out2, atol=1e-6)\n+    assert torch.allclose(jit(x, x_0, adj1.t()), out1, atol=1e-6)\n+    assert torch.allclose(jit(x, x_0, adj2.t()), out2, atol=1e-6)\n\nconv.cached = True\nconv(x, x_0, edge_index)\n", "fix_pattern": "In the condition of calling the jit function, if the conv function is called instead of the jit function, then change the code to call the jit function to fix the API misuse."}
{"number": 1438, "change": "class TrainingOperator:\n\nlogger.debug(\"Registering optimizers.\")\nself._optimizers = optimizers\n-        if not isinstance(self._optimizers, Iterable):\n+        if isinstance(self._optimizers, torch.optim.Optimizer):\nself._optimizers = [self._optimizers]\n\nif schedulers:\n", "fix_pattern": "In the condition of checking the optimizers, if an instance of `torch.optim.Optimizer` is detected, then change the assignment of `_optimizers` to a list containing the optimizer."}
{"number": 1463, "change": "def main():\n\n# Save the result as an audio summary.\ndatestring = str(datetime.now()).replace(' ', 'T')\n-    writer = tf.train.SummaryWriter(logdir)\n-    tf.audio_summary('generated', decode, wavenet_params['sample_rate'])\n-    summaries = tf.merge_all_summaries()\n+    writer = tf.summary.FileWriter(logdir)\n+    tf.summary.audio('generated', decode, wavenet_params['sample_rate'])\n+    summaries = tf.summary.merge_all()\nsummary_out = sess.run(summaries,\nfeed_dict={samples: np.reshape(waveform, [-1, 1])})\nwriter.add_summary(summary_out)\n", "fix_pattern": "In the condition of using TensorFlow, if the tf.train.SummaryWriter() and tf.audio_summary() functions are called, they should be replaced with tf.summary.FileWriter() and tf.summary.audio() respectively to fix the API misuse."}
